<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#25972;&#21512;LLM&#26234;&#33021;&#20307;&#34892;&#21160;&#65292;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#39640;&#36798;20%&#12290;</title><link>https://rss.arxiv.org/abs/2402.01030</link><description>&lt;p&gt;
&#21487;&#25191;&#34892;&#20195;&#30721;&#34892;&#21160;&#33021;&#22815;&#28608;&#21457;&#26356;&#20986;&#33394;&#30340;LLM&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Executable Code Actions Elicit Better LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01030
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#25972;&#21512;LLM&#26234;&#33021;&#20307;&#34892;&#21160;&#65292;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#39640;&#36798;20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26234;&#33021;&#20307;&#20855;&#22791;&#25191;&#34892;&#24191;&#27867;&#34892;&#21160;&#30340;&#33021;&#21147;&#65292;&#22914;&#35843;&#29992;&#24037;&#20855;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#31561;&#65292;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;LLM&#26234;&#33021;&#20307;&#36890;&#24120;&#36890;&#36807;&#29983;&#25104;JSON&#25110;&#25991;&#26412;&#30340;&#39044;&#23450;&#20041;&#26684;&#24335;&#26469;&#20135;&#29983;&#34892;&#21160;&#65292;&#36825;&#36890;&#24120;&#21463;&#38480;&#20110;&#21463;&#38480;&#21046;&#30340;&#34892;&#21160;&#31354;&#38388;&#65288;&#20363;&#22914;&#65292;&#39044;&#23450;&#20041;&#24037;&#20855;&#30340;&#33539;&#22260;&#65289;&#21644;&#21463;&#38480;&#30340;&#28789;&#27963;&#24615;&#65288;&#20363;&#22914;&#65292;&#26080;&#27861;&#32452;&#21512;&#22810;&#20010;&#24037;&#20855;&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#23558;LLM&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#25972;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#65288;CodeAct&#65289;&#12290;CodeAct&#19982;Python&#35299;&#37322;&#22120;&#38598;&#25104;&#65292;&#21487;&#20197;&#25191;&#34892;&#20195;&#30721;&#34892;&#21160;&#65292;&#24182;&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#22312;&#26032;&#30340;&#35266;&#23519;&#20013;&#21160;&#24577;&#20462;&#35746;&#20808;&#21069;&#30340;&#34892;&#21160;&#25110;&#21457;&#20986;&#26032;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#23545;17&#20010;LLM&#22312;API-Bank&#21644;&#26032;&#32534;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;CodeAct&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#65288;&#25104;&#21151;&#29575;&#39640;&#20986;20%&#65289;&#12290;CodeAct&#30340;&#20196;&#20154;&#40723;&#33310;&#30340;&#34920;&#29616;&#28608;&#21169;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-sourc
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.12027</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#27934;&#23519;: &#22312;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12027
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;&#20197;&#22270;&#34920;&#24418;&#24335;&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#65292;&#25552;&#20379;&#20851;&#38190;&#27934;&#23519;&#24182;&#24110;&#21161;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#38543;&#30528;&#36817;&#24180;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38761;&#21629;&#65292;&#24182;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#20219;&#21153;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12027v1 Announce Type: cross  Abstract: Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models (LLMs), have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks. In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics a
&lt;/p&gt;</description></item><item><title>FlexCap&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22270;&#20687;&#20013;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#21306;&#22495;&#25551;&#36848;&#65292;&#22312;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12026</link><description>&lt;p&gt;
FlexCap&#65306;&#22312;&#22270;&#20687;&#20013;&#29983;&#25104;&#20016;&#23500;&#12289;&#26412;&#22320;&#21270;&#21644;&#28789;&#27963;&#30340;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
FlexCap: Generating Rich, Localized, and Flexible Captions in Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12026
&lt;/p&gt;
&lt;p&gt;
FlexCap&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22270;&#20687;&#20013;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#21306;&#22495;&#25551;&#36848;&#65292;&#22312;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;$\textit{&#28789;&#27963;&#23383;&#24149;}$&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#38271;&#24230;&#19981;&#21516;&#30340;&#29305;&#23450;&#21306;&#22495;&#25551;&#36848;&#12290;&#35813;&#27169;&#22411;FlexCap&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20026;&#36755;&#20837;&#30340;&#36793;&#30028;&#26694;&#29983;&#25104;&#38271;&#24230;&#26465;&#20214;&#30340;&#23383;&#24149;&#65292;&#20174;&#32780;&#21487;&#20197;&#25511;&#21046;&#20854;&#36755;&#20986;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#25551;&#36848;&#33539;&#22260;&#20174;&#31616;&#27905;&#30340;&#23545;&#35937;&#26631;&#31614;&#21040;&#35814;&#32454;&#30340;&#23383;&#24149;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20174;&#24102;&#23383;&#24149;&#30340;&#22270;&#20687;&#24320;&#22987;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;&#21306;&#22495;&#25551;&#36848;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#23383;&#24149;&#21151;&#33021;&#26377;&#20960;&#20010;&#23453;&#36149;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;FlexCap&#22312;Visual Genome&#25968;&#25454;&#38598;&#19978;&#30340;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;FlexCap&#29983;&#25104;&#26412;&#22320;&#21270;&#25551;&#36848;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#26469;&#26500;&#24314;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#31995;&#32479;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31995;&#32479;&#22312;&#35768;&#22810;VQ&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12026v1 Announce Type: cross  Abstract: We introduce a versatile $\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications.   First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset. Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model. The resulting system achieves state-of-the-art zero-shot performance on a number of VQ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20581;&#24247;&#20844;&#24179;&#21361;&#23475;&#21644;&#20559;&#35265;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#65292;&#36827;&#34892;&#20102;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#20154;&#31867;&#35780;&#20272;LLM&#29983;&#25104;&#31572;&#26696;&#20559;&#35265;&#30340;&#22810;&#22240;&#23376;&#26694;&#26550;&#20197;&#21450;EquityMedQA&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.12025</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20581;&#24247;&#20844;&#24179;&#21361;&#23475;&#21644;&#20559;&#35265;&#30340;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12025
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20581;&#24247;&#20844;&#24179;&#21361;&#23475;&#21644;&#20559;&#35265;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#65292;&#36827;&#34892;&#20102;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#20154;&#31867;&#35780;&#20272;LLM&#29983;&#25104;&#31572;&#26696;&#20559;&#35265;&#30340;&#22810;&#22240;&#23376;&#26694;&#26550;&#20197;&#21450;EquityMedQA&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#30528;&#20026;&#22797;&#26434;&#30340;&#20581;&#24247;&#20449;&#24687;&#38656;&#27714;&#25552;&#20379;&#26381;&#21153;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#21516;&#26102;&#20063;&#26377;&#21487;&#33021;&#24341;&#20837;&#21361;&#23475;&#24182;&#21152;&#21095;&#20581;&#24247;&#19981;&#24179;&#31561;&#12290;&#21487;&#38752;&#22320;&#35780;&#20272;&#19982;&#20844;&#24179;&#30456;&#20851;&#30340;&#27169;&#22411;&#22833;&#28789;&#26159;&#21457;&#23637;&#20419;&#36827;&#20581;&#24247;&#20844;&#24179;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#25581;&#31034;&#21487;&#33021;&#23548;&#33268;LLM&#29983;&#25104;&#30340;&#38271;&#31687;&#31572;&#26696;&#20013;&#30340;&#20844;&#24179;&#30456;&#20851;&#21361;&#23475;&#30340;&#20559;&#35265;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Med-PaLM 2&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22312;&#35813;&#39046;&#22495;&#36827;&#34892;&#30340;&#26368;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#29992;&#20110;&#20154;&#31867;&#35780;&#20272;LLM&#29983;&#25104;&#31572;&#26696;&#20559;&#35265;&#30340;&#22810;&#22240;&#23376;&#26694;&#26550;&#65292;&#20197;&#21450;EquityMedQA&#65292;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#26032;&#21457;&#24067;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#65292;&#20854;&#20013;&#26082;&#21253;&#25324;&#25163;&#21160;&#31574;&#21010;&#21448;&#21253;&#25324;LLM&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20016;&#23500;&#20102;&#23545;&#25239;&#24615;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#20154;&#31867;&#35780;&#20272;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#35774;&#35745;&#36807;&#31243;&#37117;&#26681;&#26893;&#20110;&#23454;&#38469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12025v1 Announce Type: cross  Abstract: Large language models (LLMs) hold immense promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities. Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity. In this work, we present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and then conduct an empirical case study with Med-PaLM 2, resulting in the largest human evaluation study in this area to date. Our contributions include a multifactorial framework for human assessment of LLM-generated answers for biases, and EquityMedQA, a collection of seven newly-released datasets comprising both manually-curated and LLM-generated questions enriched for adversarial queries. Both our human assessment framework and dataset design process are grounde
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457;&#21452;&#35821;&#32763;&#35793;&#27169;&#22411;&#65292;&#25506;&#32034;&#21488;&#28286;&#31119;&#24314;&#35805;&#21644;&#32321;&#20307;&#20013;&#25991;/&#33521;&#25991;&#20043;&#38388;&#30340;&#32763;&#35793;&#65292;&#24341;&#20837;&#38480;&#21046;&#21333;&#35821;&#35821;&#26009;&#24211;&#24182;&#23558;&#25152;&#26377;&#21488;&#28286;&#31119;&#24314;&#35805;&#25991;&#23383;&#31995;&#32479;&#35268;&#33539;&#20026;&#31119;&#24314;&#35805;&#27721;&#23383;&#65292;&#20174;&#32780;&#25552;&#39640;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12024</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#35268;&#33539;&#22235;&#31181;&#25991;&#23383;&#31995;&#32479;&#65292;&#22686;&#24378;&#31119;&#24314;&#35805;&#30340;&#21452;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Enhancing Hokkien Dual Translation by Exploring and Standardizing of Four Writing Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12024
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457;&#21452;&#35821;&#32763;&#35793;&#27169;&#22411;&#65292;&#25506;&#32034;&#21488;&#28286;&#31119;&#24314;&#35805;&#21644;&#32321;&#20307;&#20013;&#25991;/&#33521;&#25991;&#20043;&#38388;&#30340;&#32763;&#35793;&#65292;&#24341;&#20837;&#38480;&#21046;&#21333;&#35821;&#35821;&#26009;&#24211;&#24182;&#23558;&#25152;&#26377;&#21488;&#28286;&#31119;&#24314;&#35805;&#25991;&#23383;&#31995;&#32479;&#35268;&#33539;&#20026;&#31119;&#24314;&#35805;&#27721;&#23383;&#65292;&#20174;&#32780;&#25552;&#39640;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12024v1 &#20844;&#21578;&#31867;&#22411;: &#26032; &#25552;&#35201;: &#26426;&#22120;&#32763;&#35793;&#20027;&#35201;&#38598;&#20013;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;HRLs&#65289;&#65292;&#32780;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;LRLs&#65289;&#22914;&#21488;&#28286;&#31119;&#24314;&#35805;&#30456;&#23545;&#26410;&#34987;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#21488;&#28286;&#31119;&#24314;&#35805;&#19982;&#32321;&#20307;&#20013;&#25991;&#21644;&#33521;&#25991;&#20043;&#38388;&#30340;&#21452;&#35821;&#32763;&#35793;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#32321;&#20307;&#20013;&#25991;&#30340;&#39044;&#35757;&#32451;LLaMA2-7B&#27169;&#22411;&#26469;&#21033;&#29992;&#21488;&#28286;&#31119;&#24314;&#35805;&#27721;&#23383;&#19982;&#32321;&#20307;&#20013;&#25991;&#20043;&#38388;&#30340;&#25340;&#38899;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#23454;&#39564;&#28041;&#21450;&#21488;&#28286;&#31119;&#24314;&#35805;&#21508;&#31181;&#25991;&#23383;&#31995;&#32479;&#20043;&#38388;&#30340;&#32763;&#35793;&#20219;&#21153;&#65292;&#20197;&#21450;&#21488;&#28286;&#31119;&#24314;&#35805;&#19982;&#20854;&#20182;&#39640;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#26377;&#38480;&#30340;&#21333;&#35821;&#35821;&#26009;&#24211;&#36824;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#27169;&#22411;&#23545;&#21488;&#28286;&#31119;&#24314;&#35805;&#30340;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#32763;&#35793;&#27169;&#22411;&#23558;&#25152;&#26377;&#21488;&#28286;&#31119;&#24314;&#35805;&#25991;&#23383;&#31995;&#32479;&#35268;&#33539;&#20026;&#31119;&#24314;&#35805;&#27721;&#23383;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12024v1 Announce Type: new  Abstract: Machine translation focuses mainly on high-resource languages (HRLs), while low-resource languages (LRLs) like Taiwanese Hokkien are relatively under-explored. This study aims to address this gap by developing a dual translation model between Taiwanese Hokkien and both Traditional Mandarin Chinese and English. We employ a pre-trained LLaMA2-7B model specialized in Traditional Mandarin Chinese to leverage the orthographic similarities between Taiwanese Hokkien Han and Traditional Mandarin Chinese. Our comprehensive experiments involve translation tasks across various writing systems of Taiwanese Hokkien and between Taiwanese Hokkien and other HRLs. We find that the use of a limited monolingual corpus also further improve the model's Taiwanese Hokkien capabilities. We then utilize our translation model to standardize all Taiwanese Hokkien writing systems into Hokkien Han, resulting in further performance improvements. Additionally, we intr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#35265;&#35299;&#32467;&#21512;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#28436;&#31034;&#25968;&#25454;&#38598;&#23545;&#40784;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.12017</link><description>&lt;p&gt;
&#30417;&#30563;&#24494;&#35843;&#20316;&#20026;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised Fine-Tuning as Inverse Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#35265;&#35299;&#32467;&#21512;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#28436;&#31034;&#25968;&#25454;&#38598;&#23545;&#40784;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#40784;&#30340;&#20027;&#27969;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20154;&#31867;&#25110;AI&#21453;&#39304;&#65292;&#24182;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#29305;&#23450;&#31867;&#22411;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#21508;&#31181;&#24773;&#26223;&#19979;&#19982;&#19987;&#23478;&#28436;&#31034;&#23545;&#40784;&#26356;&#20026;&#29616;&#23454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#26694;&#26550;&#65292;&#20197;&#28436;&#31034;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#26469;&#35268;&#21010;&#23545;&#40784;LLMs&#30340;&#38382;&#39064;&#12290;&#20511;&#37492;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;LLM&#23545;&#40784;&#20219;&#21153;&#20013;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;&#36825;&#20123;&#19981;&#21516;&#26041;&#27861;&#30340;&#35206;&#30422;&#29575;&#21644;&#23547;&#25214;&#27169;&#24335;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#32463;&#20856;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#30340;&#21033;&#24330;&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#19981;&#21516;&#26041;&#27861;&#34920;&#29616;&#31361;&#20986;&#30340;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12017v1 Announce Type: cross  Abstract: The prevailing approach to aligning Large Language Models (LLMs) typically relies on human or AI feedback and assumes access to specific types of preference datasets. In our work, we question the efficacy of such datasets and explore various scenarios where alignment with expert demonstrations proves more realistic. We build a sequential decision-making framework to formulate the problem of aligning LLMs using demonstration datasets. Drawing insights from inverse reinforcement learning and imitation learning, we introduce various approaches for divergence minimization in the LLM alignment tasks. Our analysis highlights the mass-covering and mode-seeking behaviors of these different approaches. Inclusively, we examine the pros and cons of the classical supervised fine-tuning method, elaborating on scenarios where different methods shine.
&lt;/p&gt;</description></item><item><title>EnvGen&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#33258;&#36866;&#24212;&#21019;&#24314;&#35757;&#32451;&#29615;&#22659;&#65292;&#24110;&#21161;&#23567;&#22411;&#20855;&#36523;&#20307;RL&#20195;&#29702;&#22312;&#24369;&#28857;&#26041;&#38754;&#23398;&#20064;&#26377;&#29992;&#25216;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12014</link><description>&lt;p&gt;
EnvGen: &#36890;&#36807;LLMs&#29983;&#25104;&#21644;&#35843;&#25972;&#29615;&#22659;&#20197;&#35757;&#32451;&#20855;&#36523;&#20307;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12014
&lt;/p&gt;
&lt;p&gt;
EnvGen&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#33258;&#36866;&#24212;&#21019;&#24314;&#35757;&#32451;&#29615;&#22659;&#65292;&#24110;&#21161;&#23567;&#22411;&#20855;&#36523;&#20307;RL&#20195;&#29702;&#22312;&#24369;&#28857;&#26041;&#38754;&#23398;&#20064;&#26377;&#29992;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#20851;&#36890;&#36807;&#20114;&#21160;&#36827;&#34892;&#20855;&#36523;&#20307;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#30452;&#25509;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20195;&#29702;&#65292;&#20197;&#30830;&#23450;&#29615;&#22659;&#20013;&#30340;&#19979;&#19968;&#27493;&#12290;LLM&#20195;&#29702;&#30001;&#20110;&#20854;&#19990;&#30028;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#27604;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20197;&#24448;&#36739;&#23567;&#30340;&#20195;&#29702;&#34920;&#29616;&#26356;&#24378;&#65307;&#20294;&#39057;&#32321;&#35843;&#29992;LLMs&#36895;&#24230;&#24930;&#19988;&#26114;&#36149;&#12290;&#25105;&#20204;&#25552;&#20986;EnvGen&#65292;&#19968;&#20010;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#31034;&#19968;&#20010;LLM&#29983;&#25104;&#35757;&#32451;&#29615;&#22659;&#65292;&#20351;&#20195;&#29702;&#21487;&#20197;&#24555;&#36895;&#24182;&#34892;&#23398;&#20064;&#19981;&#21516;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLM&#33719;&#24471;&#20219;&#21153;&#25551;&#36848;&#21644;&#27169;&#25311;&#22120;&#30446;&#26631;&#65292;&#28982;&#21518;&#34987;&#35201;&#27714;&#29983;&#25104;&#19968;&#32452;&#29615;&#22659;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12014v1 Announce Type: cross  Abstract: Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel. Concretely, the LLM is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.11996</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#30693;&#35782;&#25552;&#21462;&#12289;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#21644;&#22810;&#27169;&#24577;&#26234;&#33021;&#22270;&#25512;&#29702;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11996
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#25105;&#20204;&#23558;&#19968;&#32452;&#28041;&#21450;&#29983;&#29289;&#26448;&#26009;&#39046;&#22495;&#30340;1,000&#31687;&#31185;&#23398;&#35770;&#25991;&#36716;&#21270;&#20026;&#35814;&#32454;&#30340;&#26412;&#20307;&#30693;&#35782;&#22270;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#26080;&#26631;&#24230;&#29305;&#24615;&#12290;&#36890;&#36807;&#22522;&#20110;&#33410;&#28857;&#30456;&#20284;&#24615;&#21644;&#20171;&#25968;&#20013;&#24515;&#24615;&#30340;&#32452;&#21512;&#25490;&#21517;&#65292;&#25506;&#27979;&#19981;&#21516;&#27010;&#24565;&#20043;&#38388;&#30340;&#22270;&#36941;&#21382;&#36335;&#24452;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#28145;&#20837;&#30340;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#26597;&#35810;&#65292;&#35782;&#21035;&#30693;&#35782;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#21069;&#25152;&#26410;&#35265;&#30340;&#26448;&#26009;&#35774;&#35745;&#21450;&#20854;&#34892;&#20026;&#12290;&#19968;&#39033;&#27604;&#36739;&#25581;&#31034;&#20102;&#29983;&#29289;&#26448;&#26009;&#21644;&#36125;&#22810;&#33452;&#31532;&#20061;&#20132;&#21709;&#26354;&#20043;&#38388;&#30340;&#35814;&#32454;&#32467;&#26500;&#30456;&#20284;&#20043;&#22788;&#65292;&#31361;&#26174;&#20102;&#36890;&#36807;&#21516;&#26500;&#26144;&#23556;&#20849;&#20139;&#22797;&#26434;&#24615;&#27169;&#24335;&#12290;&#35813;&#31639;&#27861;&#36827;&#19968;&#27493;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#20998;&#32423;&#33740;&#19997;&#20307;&#30340;&#22797;&#21512;&#26448;&#26009;&#65292;&#23558;&#22270;&#37319;&#26679;&#30340;&#32852;&#21512;&#21512;&#25104;&#21407;&#29702;&#19982;&#24247;&#23450;&#26031;&#22522;&#12298;&#31532;&#19971;&#32452;&#25104;&#12299;&#20013;&#25552;&#21462;&#30340;&#21407;&#21017;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11996v1 Announce Type: cross  Abstract: Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#25945;&#23398;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#21453;&#39304;&#20013;&#25552;&#21462;&#12289;&#23884;&#20837;&#12289;&#32858;&#31867;&#21644;&#24635;&#32467;SETs&#12290;</title><link>https://arxiv.org/abs/2403.11984</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#25991;&#26412;&#27169;&#22411;&#20026;&#25945;&#23398;&#35780;&#20272;&#21019;&#24314;&#23450;&#24615;&#20195;&#30721;&#25163;&#20876;
&lt;/p&gt;
&lt;p&gt;
Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#25945;&#23398;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#21453;&#39304;&#20013;&#25552;&#21462;&#12289;&#23884;&#20837;&#12289;&#32858;&#31867;&#21644;&#24635;&#32467;SETs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#39304;&#26159;&#25913;&#36827;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#24403;&#26377;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#22823;&#37327;&#21453;&#39304;&#26102;&#65292;&#23558;&#20449;&#24687;&#25552;&#28860;&#25104;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20998;&#26512;&#25945;&#23398;&#35780;&#20272;&#65288;SETs&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#19968;&#25152;&#22823;&#22411;&#20844;&#31435;&#22823;&#23398;&#30340;5000&#20010;SETs&#35821;&#26009;&#24211;&#26469;&#28436;&#31034;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11984v1 Announce Type: cross  Abstract: Feedback is a critical aspect of improvement. Unfortunately, when there is a lot of feedback from multiple sources, it can be difficult to distill the information into actionable insights. Consider student evaluations of teaching (SETs), which are important sources of feedback for educators. They can give instructors insights into what worked during a semester. A collection of SETs can also be useful to administrators as signals for courses or entire programs. However, on a large scale as in high-enrollment courses or administrative records over several years, the volume of SETs can render them difficult to analyze. In this paper, we discuss a novel method for analyzing SETs using natural language processing (NLP) and large language models (LLMs). We demonstrate the method by applying it to a corpus of 5,000 SETs from a large public university. We show that the method can be used to extract, embed, cluster, and summarize the SETs to id
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#28436;&#21270;&#30340;&#35745;&#31639;&#27169;&#25311;&#65292;&#20197;&#21450;&#20854;&#23545;&#20110;&#27169;&#25311;&#35821;&#35328;&#28436;&#21270;&#30340;&#24110;&#21161;&#21644;&#23616;&#38480;&#24615;&#65292;&#36866;&#29992;&#20110;&#35821;&#35328;&#23398;&#23478;&#21644;&#35748;&#30693;&#31185;&#23398;&#23478;&#12290;</title><link>https://arxiv.org/abs/2403.11958</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#35328;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Language Evolution with Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11958
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#28436;&#21270;&#30340;&#35745;&#31639;&#27169;&#25311;&#65292;&#20197;&#21450;&#20854;&#23545;&#20110;&#27169;&#25311;&#35821;&#35328;&#28436;&#21270;&#30340;&#24110;&#21161;&#21644;&#23616;&#38480;&#24615;&#65292;&#36866;&#29992;&#20110;&#35821;&#35328;&#23398;&#23478;&#21644;&#35748;&#30693;&#31185;&#23398;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#24314;&#27169;&#22312;&#35821;&#35328;&#28436;&#21270;&#30740;&#31350;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26088;&#22312;&#27169;&#25311;&#35302;&#21457;&#32467;&#26500;&#21270;&#35821;&#35328;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#20986;&#29616;&#30340;&#26465;&#20214;&#21644;&#23398;&#20064;&#36807;&#31243;&#12290;&#26412;&#31456;&#25506;&#35752;&#20102;&#21478;&#19968;&#31867;&#35745;&#31639;&#27169;&#22411;&#8212;&#8212;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#26412;&#31456;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#23427;&#20204;&#22312;&#27169;&#25311;&#35821;&#35328;&#28436;&#21270;&#26041;&#38754;&#30340;&#24110;&#21161;&#12290;&#36824;&#35752;&#35770;&#20102;&#20851;&#38190;&#21457;&#29616;&#12289;&#23616;&#38480;&#24615;&#21644;&#26368;&#36817;&#23581;&#35797;&#26500;&#24314;&#36924;&#30495;&#27169;&#25311;&#30340;&#24037;&#20316;&#12290;&#26412;&#31456;&#20027;&#35201;&#38754;&#21521;&#23547;&#27714;&#23558;&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#30740;&#31350;&#35821;&#35328;&#28436;&#21270;&#30340;&#24037;&#20855;&#30340;&#35821;&#35328;&#23398;&#23478;&#21644;&#35748;&#30693;&#31185;&#23398;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11958v1 Announce Type: new  Abstract: Computational modeling plays an essential role in the study of language emergence. It aims to simulate the conditions and learning processes that could trigger the emergence of a structured language within a simulated controlled environment. Several methods have been used to investigate the origin of our language, including agent-based systems, Bayesian agents, genetic algorithms, and rule-based systems. This chapter explores another class of computational models that have recently revolutionized the field of machine learning: deep learning models. The chapter introduces the basic concepts of deep and reinforcement learning methods and summarizes their helpfulness for simulating language emergence. It also discusses the key findings, limitations, and recent attempts to build realistic simulations. This chapter targets linguists and cognitive scientists seeking an introduction to deep learning as a tool to investigate language evolution.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#36827;&#34892;&#33258;&#36866;&#24212;&#21452;&#35821;&#23545;&#40784;&#30340;&#31995;&#32479;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#20854;&#22312;&#22788;&#29702;&#29255;&#27573;&#24615;&#24179;&#34892;&#25991;&#26412;&#26102;&#34920;&#29616;&#20986;&#19982;&#20043;&#31561;&#25928;&#30340;&#32467;&#26524;&#65292;&#19988;&#20855;&#26377;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.11921</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#36827;&#34892;&#33258;&#36866;&#24212;&#21452;&#35821;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Adaptative Bilingual Aligning Using Multilingual Sentence Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11921
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#36827;&#34892;&#33258;&#36866;&#24212;&#21452;&#35821;&#23545;&#40784;&#30340;&#31995;&#32479;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#20854;&#22312;&#22788;&#29702;&#29255;&#27573;&#24615;&#24179;&#34892;&#25991;&#26412;&#26102;&#34920;&#29616;&#20986;&#19982;&#20043;&#31561;&#25928;&#30340;&#32467;&#26524;&#65292;&#19988;&#20855;&#26377;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#21452;&#25991;&#26412;&#23545;&#40784;&#31995;&#32479;&#65292;&#21517;&#20026;AIlign&#12290;&#36825;&#20010;&#23545;&#40784;&#22120;&#20381;&#36182;&#20110;&#21477;&#23376;&#23884;&#20837;&#26469;&#25552;&#21462;&#21487;&#38752;&#30340;&#38170;&#28857;&#65292;&#36825;&#20123;&#38170;&#28857;&#21487;&#20197;&#25351;&#23548;&#23545;&#40784;&#36335;&#24452;&#65292;&#21363;&#20351;&#23545;&#20110;&#20854;&#24179;&#34892;&#24615;&#26159;&#29255;&#27573;&#24615;&#30340;&#12289;&#19981;&#20005;&#26684;&#21333;&#35843;&#30340;&#25991;&#26412;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AIlign&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#25216;&#26415;&#31561;&#25928;&#30340;&#32467;&#26524;&#65292;&#19988;&#20855;&#26377;&#20934;&#32447;&#24615;&#30340;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;AIlign&#33021;&#22815;&#22788;&#29702;&#37027;&#20123;&#20165;&#22312;&#23616;&#37096;&#28385;&#36275;&#24179;&#34892;&#24615;&#21644;&#21333;&#35843;&#24615;&#23646;&#24615;&#30340;&#25991;&#26412;&#65292;&#36825;&#19982;&#26368;&#36817;&#30340;&#31995;&#32479;&#22914;Vecalign&#25110;Bertalign&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11921v1 Announce Type: new  Abstract: In this paper, we present an adaptive bitextual alignment system called AIlign. This aligner relies on sentence embeddings to extract reliable anchor points that can guide the alignment path, even for texts whose parallelism is fragmentary and not strictly monotonic. In an experiment on several datasets, we show that AIlign achieves results equivalent to the state of the art, with quasi-linear complexity. In addition, AIlign is able to handle texts whose parallelism and monotonicity properties are only satisfied locally, unlike recent systems such as Vecalign or Bertalign.
&lt;/p&gt;</description></item><item><title>Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11905</link><description>&lt;p&gt;
Tur[k]ingBench&#65306;&#29992;&#20110;&#32593;&#32476;&#20195;&#29702;&#30340;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench: A Challenge Benchmark for Web Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11905
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#22312;&#21407;&#22987;&#25991;&#26412;&#24418;&#24335;&#19979;&#29702;&#35299;&#21644;&#20132;&#27969;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19990;&#30028;&#19978;&#19981;&#20165;&#20165;&#26159;&#21407;&#22987;&#25991;&#26412;&#12290;&#20363;&#22914;&#65292;&#20154;&#20204;&#22312;&#32593;&#39029;&#19978;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#22312;&#36825;&#20123;&#32593;&#39029;&#19978;&#65292;&#25991;&#26412;&#19982;&#20854;&#20182;&#24418;&#24335;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#24182;&#20197;&#21508;&#31181;&#22797;&#26434;&#20114;&#21160;&#30340;&#24418;&#24335;&#23436;&#25104;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25512;&#24191;&#21040;&#36825;&#31181;&#22797;&#26434;&#30340;&#39046;&#22495;&#21602;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TurkingBench&#65292;&#19968;&#20010;&#30001;&#21253;&#21547;&#22810;&#27169;&#24577;&#32972;&#26223;&#30340;&#25991;&#26412;&#35828;&#26126;&#21046;&#23450;&#30340;&#20219;&#21153;&#22522;&#20934;&#12290;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#20154;&#24037;&#21512;&#25104;&#30340;&#32593;&#39029;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#36825;&#37324;&#25105;&#20204;&#20351;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21508;&#31181;&#27880;&#37322;&#30446;&#30340;&#30340;&#33258;&#28982;HTML&#39029;&#38754;&#12290;&#27599;&#20010;&#20219;&#21153;&#30340;HTML&#35828;&#26126;&#20063;&#34987;&#23454;&#20363;&#21270;&#20026;&#21508;&#31181;&#20540;&#65288;&#20174;&#20247;&#21253;&#20219;&#21153;&#33719;&#24471;&#65289;&#20197;&#24418;&#25104;&#20219;&#21153;&#30340;&#26032;&#23454;&#20363;&#12290;&#36825;&#20010;&#22522;&#20934;&#21253;&#21547;32.2K&#20010;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11905v1 Announce Type: new  Abstract: Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains?   To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instanc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CICLe&#26694;&#26550;&#65292;&#22522;&#20110;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2403.11904</link><description>&lt;p&gt;
CICLe: &#36866;&#24212;&#19978;&#19979;&#25991;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11904
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CICLe&#26694;&#26550;&#65292;&#22522;&#20110;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#27745;&#26579;&#25110;&#25530;&#20551;&#39135;&#21697;&#23545;&#20154;&#31867;&#20581;&#24247;&#26500;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#22312;&#32473;&#23450;&#20102;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#35760;&#32593;&#32476;&#25991;&#26412;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#33258;&#21160;&#26816;&#27979;&#36825;&#31181;&#39118;&#38505;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;7,546&#20010;&#25551;&#36848;&#20844;&#20849;&#39135;&#21697;&#21484;&#22238;&#20844;&#21578;&#30340;&#30701;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25991;&#26412;&#37117;&#32463;&#36807;&#25163;&#21160;&#26631;&#35760;&#65292;&#20998;&#20026;&#20004;&#20010;&#31890;&#24230;&#32423;&#21035;&#65288;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#21484;&#22238;&#23545;&#24212;&#30340;&#39135;&#21697;&#20135;&#21697;&#21644;&#21361;&#23475;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25968;&#25454;&#38598;&#24182;&#23545;&#26420;&#32032;&#12289;&#20256;&#32479;&#21644;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#22522;&#20110;tf-idf&#34920;&#31034;&#30340;&#36923;&#36753;&#22238;&#24402;&#22312;&#25903;&#25345;&#36739;&#20302;&#30340;&#31867;&#21035;&#19978;&#20248;&#20110;RoBERTa&#21644;XLM-R&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#19982;&#26222;&#36890;&#25552;&#31034;&#30456;&#27604;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11904v1 Announce Type: new  Abstract: Contaminated or adulterated food poses a substantial risk to human health. Given sets of labeled web texts for training, Machine Learning and Natural Language Processing can be applied to automatically detect such risks. We publish a dataset of 7,546 short texts describing public food recall announcements. Each text is manually labeled, on two granularity levels (coarse and fine), for food products and hazards that the recall corresponds to. We describe the dataset and benchmark naive, traditional, and Transformer models. Based on our analysis, Logistic Regression based on a tf-idf representation outperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss different prompting strategies and present an LLM-in-the-loop framework, based on Conformal Prediction, which boosts the performance of the base classifier while reducing energy consumption compared to normal prompting.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22768;&#26126;&#20998;&#35299;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#35780;&#20272;&#25991;&#26412;&#25903;&#25345;&#26102;&#65292;&#20998;&#35299;&#26041;&#27861;&#23545;&#32467;&#26524;&#20855;&#26377;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;DecompScore&#21644;&#22522;&#20110;LLM&#30340;&#29983;&#25104;&#20998;&#35299;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11903</link><description>&lt;p&gt;
&#23545;&#22768;&#26126;&#20998;&#35299;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Claim Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11903
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22768;&#26126;&#20998;&#35299;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#35780;&#20272;&#25991;&#26412;&#25903;&#25345;&#26102;&#65292;&#20998;&#35299;&#26041;&#27861;&#23545;&#32467;&#26524;&#20855;&#26377;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;DecompScore&#21644;&#22522;&#20110;LLM&#30340;&#29983;&#25104;&#20998;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#25991;&#26412;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#65292;&#35780;&#20272;&#27492;&#31867;&#25991;&#26412;&#22312;&#22806;&#37096;&#30693;&#35782;&#26469;&#28304;&#25903;&#25345;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#24378;&#12290;&#35768;&#22810;&#35780;&#20272;&#25991;&#26412;&#25903;&#25345;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26576;&#31181;&#23558;&#25991;&#26412;&#20998;&#35299;&#20026;&#20854;&#21508;&#20010;&#23376;&#22768;&#26126;&#24182;&#23558;&#20854;&#19982;&#21487;&#20449;&#21442;&#32771;&#36827;&#34892;&#35780;&#20998;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#22768;&#26126;&#20998;&#35299;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#65292;&#22914;&#20309;&#24433;&#21709;&#26368;&#36817;&#25552;&#20986;&#30340;FActScore&#31561;&#35780;&#20272;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#21457;&#29616;&#23427;&#23545;&#25152;&#20351;&#29992;&#30340;&#20998;&#35299;&#26041;&#27861;&#24456;&#25935;&#24863;&#12290;&#36825;&#31181;&#25935;&#24863;&#24615;&#30340;&#20986;&#29616;&#26159;&#22240;&#20026;&#36825;&#26679;&#30340;&#24230;&#37327;&#25351;&#26631;&#23558;&#24635;&#20307;&#25991;&#26412;&#25903;&#25345;&#24402;&#22240;&#20110;&#29983;&#25104;&#25991;&#26412;&#30340;&#27169;&#22411;&#65292;&#23613;&#31649;&#20986;&#38169;&#20063;&#21487;&#33021;&#26469;&#33258;&#20110;&#24230;&#37327;&#30340;&#20998;&#35299;&#27493;&#39588;&#12290;&#20026;&#20102;&#34913;&#37327;&#20998;&#35299;&#36136;&#37327;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#24212;&#20110;FActScore&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;DecompScore&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#29983;&#25104;&#20998;&#35299;&#26041;&#27861;&#65292;&#21463;&#21040;&#20271;&#29305;&#20848;&#24503;&#183;&#32599;&#32032;&#20851;&#20110;&#36923;&#36753;&#21407;&#23376;&#24615;&#30340;&#29702;&#35770;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11903v1 Announce Type: new  Abstract: As generated text becomes more commonplace, it is increasingly important to evaluate how well-supported such text is by external knowledge sources. Many approaches for evaluating textual support rely on some method for decomposing text into its individual subclaims which are scored against a trusted reference. We investigate how various methods of claim decomposition -- especially LLM-based methods -- affect the result of an evaluation approach such as the recently proposed FActScore, finding that it is sensitive to the decomposition method used. This sensitivity arises because such metrics attribute overall textual support to the model that generated the text even though error can also come from the metric's decomposition step. To measure decomposition quality, we introduce an adaptation of FActScore, which we call DecompScore. We then propose an LLM-based approach to generating decompositions inspired by Bertrand Russell's theory of lo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;&#32763;&#35793;&#25216;&#26415;&#65292;&#27604;&#36739;&#20116;&#31181;&#20013;&#38388;&#35821;&#35328;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#25351;&#26631;&#35780;&#20272;&#32763;&#35793;&#20013;&#38544;&#21547;&#30340;&#24615;&#21035;&#20559;&#35265;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.11896</link><description>&lt;p&gt;
&#35843;&#26597;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#26631;&#35760;&#21644;&#39537;&#21160;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Investigating Markers and Drivers of Gender Bias in Machine Translations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11896
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;&#32763;&#35793;&#25216;&#26415;&#65292;&#27604;&#36739;&#20116;&#31181;&#20013;&#38388;&#35821;&#35328;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#25351;&#26631;&#35780;&#20272;&#32763;&#35793;&#20013;&#38544;&#21547;&#30340;&#24615;&#21035;&#20559;&#35265;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38544;&#21547;&#24615;&#21035;&#20559;&#35265;&#26159;&#19968;&#20010;&#26377;&#20805;&#20998;&#25991;&#29486;&#25903;&#25345;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#24341;&#20837;&#24615;&#21035;&#21487;&#33021;&#20250;&#24310;&#32493;&#29616;&#23454;&#19990;&#30028;&#30340;&#20559;&#35265;&#12290;&#26377;&#20123;LLMs&#20351;&#29992;&#21551;&#21457;&#24335;&#25110;&#21518;&#22788;&#29702;&#26469;&#25513;&#30422;&#36825;&#31181;&#20559;&#35265;&#65292;&#20351;&#35843;&#26597;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#26469;&#30740;&#31350;LLMs&#20013;&#30340;&#20559;&#35265;&#65292;&#20351;&#29992;DeepL&#32763;&#35793;API&#26469;&#35843;&#26597;&#37325;&#22797;&#32763;&#35793;&#19968;&#32452;56&#20010;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#26102;&#25152;&#23637;&#29616;&#30340;&#20559;&#35265;&#12290;&#27599;&#20010;&#38472;&#36848;&#20197; 'she' &#24320;&#22987;&#65292;&#24182;&#39318;&#20808;&#32763;&#35793;&#20026;&#19968;&#20010; '&#26080;&#24615;&#21035;' &#20013;&#38388;&#35821;&#35328;&#65292;&#28982;&#21518;&#20877;&#32763;&#35793;&#22238;&#33521;&#35821;&#65307;&#28982;&#21518;&#25105;&#20204;&#26816;&#26597;&#20102;&#21453;&#21521;&#32763;&#35793;&#25991;&#26412;&#20013;&#30340;&#20195;&#35789;&#36873;&#25321;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#65306;&#65288;1&#65289;&#27604;&#36739;&#20102;&#20116;&#31181;&#20013;&#38388;&#35821;&#35328;&#65288;&#33452;&#20848;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#29233;&#27801;&#23612;&#20122;&#35821;&#12289;&#22303;&#32819;&#20854;&#35821;&#21644;&#21256;&#29273;&#21033;&#35821;&#65289;&#30340;&#32467;&#26524;&#65307;&#65288;2&#65289;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#37325;&#22797;&#32763;&#35793;&#20013;&#25152;&#26263;&#31034;&#30340;&#24615;&#21035;&#21464;&#21270;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11896v1 Announce Type: new  Abstract: Implicit gender bias in Large Language Models (LLMs) is a well-documented problem, and implications of gender introduced into automatic translations can perpetuate real-world biases. However, some LLMs use heuristics or post-processing to mask such bias, making investigation difficult. Here, we examine bias in LLMss via back-translation, using the DeepL translation API to investigate the bias evinced when repeatedly translating a set of 56 Software Engineering tasks used in a previous study. Each statement starts with 'she', and is translated first into a 'genderless' intermediate language then back into English; we then examine pronoun- choice in the back-translated texts. We expand prior research in the following ways: (1) by comparing results across five intermediate languages, namely Finnish, Indonesian, Estonian, Turkish and Hungarian; (2) by proposing a novel metric for assessing the variation in gender implied in the repeated tran
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.11894</link><description>&lt;p&gt;
&#20174;&#21487;&#35299;&#37322;&#21040;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#29616;&#23454;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36890;&#36807;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#21307;&#30103;&#20445;&#20581;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;DL&#30340;NLP&#26041;&#27861;&#26085;&#30410;&#22797;&#26434;&#65292;&#38656;&#35201;&#36879;&#26126;&#30340;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#25110;&#33267;&#23569;&#26159;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#36827;&#34892;&#21487;&#38752;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#26412;&#25991;&#23545;&#21307;&#30103;&#20581;&#24247;NLP&#20013;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;DL&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#33539;&#22260;&#23457;&#26597;&#12290;&#24341;&#20837;&#20102;&#26415;&#35821;&#8220;XIAI&#8221;&#65288;eXplainable&#21644;Interpretable Artificial Intelligence&#65289;&#20197;&#21306;&#20998;XAI&#21644;IAI&#12290;&#26041;&#27861;&#26681;&#25454;&#20854;&#21151;&#33021;&#65288;&#27169;&#22411;&#12289;&#36755;&#20837;&#12289;&#36755;&#20986;&#20026;&#22522;&#30784;&#65289;&#21644;&#33539;&#22260;&#65288;&#23616;&#37096;&#12289;&#20840;&#23616;&#65289;&#36827;&#19968;&#27493;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27880;&#24847;&#26426;&#21046;&#26159;&#26368;&#20027;&#35201;&#30340;&#26032;&#20852;IAI&#12290;&#27492;&#22806;&#65292;IAI&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#23545;&#25239;XAI&#12290;&#30830;&#23450;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22823;&#22810;&#25968;XIAI&#19981;&#25506;&#32034;&#8220;&#20840;&#23616;&#8221;&#24314;&#27169;&#36807;&#31243;&#65292;&#32570;&#20047;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#19988;&#38656;&#35201;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11894v1 Announce Type: cross  Abstract: Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore "global" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks. Importan
&lt;/p&gt;</description></item><item><title>QueryAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#21453;&#39304;&#30340;&#33258;&#25105;&#26657;&#27491;&#26041;&#27861;ERASER&#65292;&#22312;&#35821;&#20041;&#35299;&#26512;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#39640;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.11886</link><description>&lt;p&gt;
QueryAgent&#65306;&#19968;&#31181;&#20855;&#26377;&#29615;&#22659;&#21453;&#39304;&#30340;&#21487;&#38752;&#39640;&#25928;&#25512;&#29702;&#26694;&#26550;&#21450;&#33258;&#25105;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11886
&lt;/p&gt;
&lt;p&gt;
QueryAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#21453;&#39304;&#30340;&#33258;&#25105;&#26657;&#27491;&#26041;&#27861;ERASER&#65292;&#22312;&#35821;&#20041;&#35299;&#26512;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#39640;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35821;&#20041;&#35299;&#26512;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#22312;&#36935;&#21040;&#24187;&#35273;&#26102;&#29616;&#26377;&#26041;&#27861;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;QueryAgent&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#20915;&#38382;&#39064;&#24182;&#36827;&#34892;&#36880;&#27493;&#33258;&#25105;&#26657;&#27491;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#21453;&#39304;&#30340;&#33258;&#25105;&#26657;&#27491;&#26041;&#27861;ERASER&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;ERASER&#21033;&#29992;&#20013;&#38388;&#27493;&#39588;&#20013;&#30340;&#20016;&#23500;&#29615;&#22659;&#21453;&#39304;&#65292;&#22312;&#24517;&#35201;&#26102;&#20165;&#36827;&#34892;&#36873;&#25321;&#24615;&#21644;&#24046;&#24322;&#21270;&#30340;&#33258;&#25105;&#26657;&#27491;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QueryAgent&#22312;GrailQA&#21644;GraphQ&#19978;&#20165;&#20351;&#29992;&#19968;&#20010;&#20363;&#23376;&#23601;&#27604;&#25152;&#26377;&#20808;&#21069;&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#21462;&#24471;&#20102;7.0&#21644;15.0&#30340;F1&#20540;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#21253;&#25324;&#36816;&#34892;&#26102;&#38388;&#12289;&#26597;&#35810;&#24320;&#38144;&#21644;API&#35843;&#29992;&#25104;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;ERASER&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11886v1 Announce Type: cross  Abstract: Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs. By leveraging ERASER, we
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#21327;&#21516;&#35757;&#32451;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#29983;&#25104;&#24335;&#23545;&#35805;&#24335;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#26377;&#38480;&#12289;&#23384;&#22312;&#22122;&#22768;&#21644;&#35821;&#35328;&#39118;&#26684;&#36716;&#21464;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11873</link><description>&lt;p&gt;
CO3: &#20302;&#36164;&#28304;&#23545;&#27604;&#21327;&#21516;&#35757;&#32451;&#29992;&#20110;&#29983;&#25104;&#23545;&#35805;&#24335;&#26597;&#35810;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
CO3: Low-resource Contrastive Co-training for Generative Conversational Query Rewrite
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11873
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#21327;&#21516;&#35757;&#32451;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#29983;&#25104;&#24335;&#23545;&#35805;&#24335;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#26377;&#38480;&#12289;&#23384;&#22312;&#22122;&#22768;&#21644;&#35821;&#35328;&#39118;&#26684;&#36716;&#21464;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11873v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#29983;&#25104;&#24335;&#26597;&#35810;&#37325;&#20889;&#21033;&#29992;&#23545;&#35805;&#21382;&#21490;&#29983;&#25104;&#37325;&#20889;&#26597;&#35810;&#65292;&#20294;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#37329;&#26631;&#37325;&#20889;&#23545;&#65292;&#26368;&#36817;&#65292;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#32780;&#36825;&#20123;&#26041;&#27861;&#23545;&#30001;&#20110;&#25968;&#25454;&#35268;&#27169;&#26377;&#38480;&#32780;&#20135;&#29983;&#30340;&#22266;&#26377;&#22122;&#22768;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#24403;&#35757;&#32451;&#21644;&#27979;&#35797;&#24773;&#20917;&#20043;&#38388;&#23384;&#22312;&#35821;&#35328;&#39118;&#26684;&#36716;&#21464;&#26102;&#65292;&#36825;&#20004;&#31181;&#23581;&#35797;&#37117;&#20250;&#38754;&#20020;&#24615;&#33021;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#22122;&#22768;&#21644;&#35821;&#35328;&#39118;&#26684;&#36716;&#21464;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20302;&#36164;&#28304;&#29983;&#25104;&#24335;&#23545;&#35805;&#24335;&#26597;&#35810;&#37325;&#20889;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#36890;&#36807;&#23545;&#27604;&#24335;&#21327;&#21516;&#35757;&#32451;&#33539;&#24335;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21516;&#26102;&#35757;&#32451;&#20004;&#20010;&#23545;&#20598;&#27169;&#22411;&#65288;&#20998;&#21035;&#20026;Rewriter&#21644;Simplifier&#65289;&#65292;&#20351;&#24471;&#23427;&#20204;&#20013;&#30340;&#27599;&#19968;&#20010;&#37117;&#36890;&#36807;&#20266;&#26631;&#35760;&#20026;&#21478;&#19968;&#20010;&#25552;&#20379;&#39069;&#22806;&#25351;&#23548;&#65292;&#20174;&#32780;&#20197;&#36845;&#20195;&#30340;&#26041;&#24335;&#22686;&#24378;&#21478;&#19968;&#20010;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#19982;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11873v1 Announce Type: new  Abstract: Generative query rewrite generates reconstructed query rewrites using the conversation history while rely heavily on gold rewrite pairs that are expensive to obtain. Recently, few-shot learning is gaining increasing popularity for this task, whereas these methods are sensitive to the inherent noise due to limited data size. Besides, both attempts face performance degradation when there exists language style shift between training and testing cases. To this end, we study low-resource generative conversational query rewrite that is robust to both noise and language style shift. The core idea is to utilize massive unlabeled data to make further improvements via a contrastive co-training paradigm. Specifically, we co-train two dual models (namely Rewriter and Simplifier) such that each of them provides extra guidance through pseudo-labeling for enhancing the other in an iterative manner. We also leverage contrastive learning with data augmen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20892;&#19994;&#23475;&#34411;&#31649;&#29702;&#20013;&#21033;&#29992;GPT-4&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#36136;&#37327;</title><link>https://arxiv.org/abs/2403.11858</link><description>&lt;p&gt;
GPT-4&#20316;&#20026;&#35780;&#20272;&#22120;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20892;&#19994;&#23475;&#34411;&#31649;&#29702;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11858
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20892;&#19994;&#23475;&#34411;&#31649;&#29702;&#20013;&#21033;&#29992;GPT-4&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20892;&#19994;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#22312;&#23475;&#34411;&#31649;&#29702;&#26041;&#38754;&#30340;&#24212;&#29992;&#20173;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#12290;&#25105;&#20204;&#26088;&#22312;&#35777;&#26126;&#36890;&#36807;&#35780;&#20272;&#30001;&#24320;&#25918;AI&#30340;Generative Pre-trained Transformer&#65288;GPT&#65289;&#31995;&#21015;&#21644;&#35895;&#27468;&#30340;FLAN&#31995;&#21015;&#29983;&#25104;&#30340;&#23475;&#34411;&#31649;&#29702;&#24314;&#35758;&#30340;&#20869;&#23481;&#30340;&#21487;&#34892;&#24615;&#12290;&#32771;&#34385;&#21040;&#20892;&#19994;&#24314;&#35758;&#30340;&#29305;&#23450;&#24773;&#22659;&#23646;&#24615;&#65292;&#33258;&#21160;&#34913;&#37327;&#25110;&#37327;&#21270;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#25104;&#20026;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;GPT-4&#20316;&#20026;&#35780;&#20272;&#22120;&#65292;&#23545;&#29983;&#25104;&#30340;&#20869;&#23481;&#22312;&#36830;&#36143;&#24615;&#12289;&#36923;&#36753;&#19968;&#33268;&#24615;&#12289;&#27969;&#30021;&#24615;&#12289;&#30456;&#20851;&#24615;&#12289;&#21487;&#29702;&#35299;&#24615;&#21644;&#20840;&#38754;&#24615;&#19978;&#36827;&#34892;&#25171;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38598;&#25104;&#20102;&#22522;&#20110;&#20316;&#29289;&#38408;&#20540;&#25968;&#25454;&#30340;&#19987;&#23478;&#31995;&#32479;&#20316;&#20026;&#22522;&#20934;&#65292;&#20197;&#33719;&#21462;&#26377;&#20851;&#20316;&#29289;&#30000;&#38388;&#21457;&#29616;&#23475;&#34411;&#26159;&#21542;&#38656;&#35201;&#37319;&#21462;&#31649;&#29702;&#25514;&#26045;&#30340;&#23454;&#38469;&#20934;&#30830;&#24615;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11858v1 Announce Type: new  Abstract: In the rapidly evolving field of artificial intelligence (AI), the application of large language models (LLMs) in agriculture, particularly in pest management, remains nascent. We aimed to prove the feasibility by evaluating the content of the pest management advice generated by LLMs, including the Generative Pre-trained Transformer (GPT) series from OpenAI and the FLAN series from Google. Considering the context-specific properties of agricultural advice, automatically measuring or quantifying the quality of text generated by LLMs becomes a significant challenge. We proposed an innovative approach, using GPT-4 as an evaluator, to score the generated content on Coherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and Exhaustiveness. Additionally, we integrated an expert system based on crop threshold data as a baseline to obtain scores for Factual Accuracy on whether pests found in crop fields should take management act
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Guide-Align&#65292;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21046;&#23450;&#29305;&#23450;&#25351;&#21335;&#65292;&#20174;&#32780;&#24314;&#31435;&#20840;&#38754;&#30340;&#25351;&#23548;&#24211;&#65292;&#29992;&#20110;&#25351;&#23548;LLMs&#29983;&#25104;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2403.11838</link><description>&lt;p&gt;
&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#65306;&#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#24211;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11838
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Guide-Align&#65292;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21046;&#23450;&#29305;&#23450;&#25351;&#21335;&#65292;&#20174;&#32780;&#24314;&#31435;&#20840;&#38754;&#30340;&#25351;&#23548;&#24211;&#65292;&#29992;&#20110;&#25351;&#23548;LLMs&#29983;&#25104;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#20559;&#35265;&#20869;&#23481;&#29983;&#25104;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#39118;&#38505;&#12290;&#24403;&#21069;&#30340;&#23545;&#40784;&#25216;&#26415;&#20043;&#19968;&#21253;&#25324;&#22522;&#20110;&#21407;&#21017;&#30340;&#38598;&#25104;&#65292;&#20294;&#38754;&#20020;&#30001;&#20110;&#25163;&#24037;&#21046;&#23450;&#35268;&#21017;&#30340;&#19981;&#31934;&#30830;&#24615;&#21644;&#26410;&#32463;&#23433;&#20840;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#39118;&#38505;&#24863;&#30693;&#19981;&#36275;&#32780;&#20135;&#29983;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Guide-Align&#65292;&#36825;&#26159;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#26368;&#21021;&#65292;&#19968;&#20010;&#32463;&#36807;&#23433;&#20840;&#35757;&#32451;&#30340;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#20026;&#21508;&#31181;&#36755;&#20837;&#21046;&#23450;&#20855;&#20307;&#25351;&#21335;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#25351;&#21335;&#24211;&#21644;&#29992;&#20110;&#36755;&#20837;&#25351;&#21335;&#26816;&#32034;&#30340;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#26816;&#32034;&#27169;&#22411;&#23558;&#26032;&#36755;&#20837;&#19982;&#30456;&#20851;&#25351;&#21335;&#30456;&#20851;&#32852;&#65292;&#24341;&#23548;LLMs&#22312;&#21709;&#24212;&#29983;&#25104;&#20013;&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#65292;&#20174;&#32780;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#12290;&#21478;&#19968;&#20010;&#39069;&#22806;&#21487;&#36873;&#38454;&#27573;&#28041;&#21450;&#20351;&#29992;&#32463;&#36807;&#32454;&#33268;&#23545;&#40784;&#30340;&#26032;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11838v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training. To address these, we introduce Guide-Align, a two-stage approach. Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, thereby establishing a comprehensive library of guidelines and models for input-guidelines retrieval. Subsequently, the retrieval model correlates new inputs with pertinent guidelines, guiding LLMs in response generation to ensure safe and high-quality outputs, thus aligning with human values. An additional optional stage involves fine-tuning a model with new well-aligned datasets generated through the
&lt;/p&gt;</description></item><item><title>&#24378;&#21046;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#33021;&#26377;&#21161;&#20110;&#20419;&#36827;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26500;&#25104;&#27010;&#25324;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.11834</link><description>&lt;p&gt;
&#25506;&#32034;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#26500;&#25104;&#27010;&#25324;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Relationship between In-context Learning and Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11834
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21046;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#33021;&#26377;&#21161;&#20110;&#20419;&#36827;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26500;&#25104;&#27010;&#25324;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#26500;&#25104;&#27010;&#25324;&#21407;&#21017;&#65292;&#22797;&#26434;&#34920;&#36798;&#30340;&#21547;&#20041;&#21487;&#20197;&#29702;&#35299;&#20026;&#20854;&#37096;&#20998;&#21547;&#20041;&#21450;&#23427;&#20204;&#22914;&#20309;&#32452;&#21512;&#30340;&#20989;&#25968;&#12290;&#36825;&#19968;&#21407;&#21017;&#23545;&#20110;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#65292;&#21487;&#20197;&#35828;&#23545;&#20110;&#38754;&#23545;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;NLP&#27169;&#22411;&#20063;&#26159;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21253;&#25324;Transformer&#65292;&#22312;&#26500;&#25104;&#27010;&#25324;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20551;&#35774;&#24378;&#21046;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#24402;&#32435;&#20559;&#35265;&#20197;&#20419;&#36827;&#26500;&#25104;&#27010;&#25324;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20351;&#26222;&#36890;&#23398;&#20064;&#38750;&#24120;&#22256;&#38590;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#20102;&#19968;&#20010;&#22240;&#26524;Transformer&#65306;&#25105;&#20204;&#21521;&#20854;&#25552;&#20379;&#19981;&#21516;&#25490;&#24207;&#30340;&#35757;&#32451;&#23454;&#20363;&#24182;&#27927;&#29260;&#23454;&#20363;&#26631;&#31614;&#12290;&#36825;&#30456;&#24403;&#20110;&#22312;&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#27169;&#22411;&#35299;&#20915;&#25152;&#26377;&#21487;&#33021;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#12290;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20219;&#21153;&#65292;&#28982;&#32780;&#65292;&#36890;&#36807;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11834v1 Announce Type: new  Abstract: According to the principle of compositional generalization, the meaning of a complex expression can be understood as a function of the meaning of its parts and of how they are combined. This principle is crucial for human language processing and also, arguably, for NLP models in the face of out-of-distribution data. However, many neural network models, including Transformers, have been shown to struggle with compositional generalization. In this paper, we hypothesize that forcing models to in-context learn can provide an inductive bias to promote compositional generalization. To test this hypothesis, we train a causal Transformer in a setting that renders ordinary learning very difficult: we present it with different orderings of the training instance and shuffle instance labels. This corresponds to training the model on all possible few-shot learning problems attainable from the dataset. The model can solve the task, however, by utilizi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SSCAE&#30340;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#35821;&#20041;&#12289;&#21477;&#27861;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#25239;&#26679;&#26412;&#65292;&#25552;&#20986;&#20102;&#21160;&#24577;&#38408;&#20540;&#21644;&#26412;&#22320;&#36138;&#23146;&#25628;&#32034;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#23545;&#25239;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.11833</link><description>&lt;p&gt;
SSCAE -- &#35821;&#20041;&#12289;&#21477;&#27861;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#28982;&#35821;&#35328;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SSCAE&#30340;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#35821;&#20041;&#12289;&#21477;&#27861;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#25239;&#26679;&#26412;&#65292;&#25552;&#20986;&#20102;&#21160;&#24577;&#38408;&#20540;&#21644;&#26412;&#22320;&#36138;&#23146;&#25628;&#32034;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;AEs&#65289;&#30340;&#24433;&#21709;&#12290;&#29992;AEs&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#20854;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24320;&#21457;&#39640;&#36136;&#37327;AEs&#30340;&#27169;&#22411;&#27604;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#35201;&#24930;&#24471;&#22810;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SSCAE&#30340;&#23454;&#29992;&#21644;&#39640;&#25928;&#30340;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#20041;&#12289;&#21477;&#27861;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#28982;&#35821;&#35328;AE&#29983;&#25104;&#22120;&#12290;SSCAE&#35782;&#21035;&#37325;&#35201;&#21333;&#35789;&#24182;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26089;&#26399;&#26367;&#25442;&#38598;&#12290;&#25509;&#30528;&#65292;&#20351;&#29992;&#20004;&#20010;&#33879;&#21517;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#35780;&#20272;&#21021;&#22987;&#38598;&#21512;&#30340;&#35821;&#20041;&#21644;&#21477;&#27861;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#65288;1&#65289;&#21160;&#24577;&#38408;&#20540;&#26469;&#25429;&#33719;&#26356;&#39640;&#25928;&#30340;&#25200;&#21160;&#20197;&#21450;&#65288;2&#65289;&#26412;&#22320;&#36138;&#23146;&#25628;&#32034;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11833v1 Announce Type: new  Abstract: Machine learning models are vulnerable to maliciously crafted Adversarial Examples (AEs). Training a machine learning model with AEs improves its robustness and stability against adversarial attacks. It is essential to develop models that produce high-quality AEs. Developing such models has been much slower in natural language processing (NLP) than in areas such as computer vision. This paper introduces a practical and efficient adversarial attack model called SSCAE for \textbf{S}emantic, \textbf{S}yntactic, and \textbf{C}ontext-aware natural language \textbf{AE}s generator. SSCAE identifies important words and uses a masked language model to generate an early set of substitutions. Next, two well-known language models are employed to evaluate the initial set in terms of semantic and syntactic characteristics. We introduce (1) a dynamic threshold to capture more efficient perturbations and (2) a local greedy search to generate high-qualit
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#24067;&#20102;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38544;&#21947;&#29702;&#35299;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;10k&#20010;&#38544;&#21947;&#21477;&#30340;&#37322;&#20041;&#20197;&#21450;1.5k&#20010;&#19981;&#24688;&#24403;&#37322;&#20041;&#23454;&#20363;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#24037;&#20855;&#26469;&#26816;&#39564;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#23454;&#29616;&#20840;&#38754;&#38544;&#21947;&#35299;&#37322;&#32780;&#38750;&#20165;&#20381;&#36182;&#35789;&#27719;&#30456;&#20284;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11810</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#21947;&#29702;&#35299;&#25361;&#25112;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Metaphor Understanding Challenge Dataset for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11810
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#24067;&#20102;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38544;&#21947;&#29702;&#35299;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;10k&#20010;&#38544;&#21947;&#21477;&#30340;&#37322;&#20041;&#20197;&#21450;1.5k&#20010;&#19981;&#24688;&#24403;&#37322;&#20041;&#23454;&#20363;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#24037;&#20855;&#26469;&#26816;&#39564;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#23454;&#29616;&#20840;&#38754;&#38544;&#21947;&#35299;&#37322;&#32780;&#38750;&#20165;&#20381;&#36182;&#35789;&#27719;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#38544;&#21947;&#26159;&#22522;&#26412;&#35748;&#30693;&#36807;&#31243;&#65288;&#22914;&#31867;&#27604;&#25512;&#29702;&#21644;&#20998;&#31867;&#65289;&#30340;&#21453;&#26144;&#65292;&#24182;&#28145;&#28145;&#26681;&#26893;&#20110;&#26085;&#24120;&#20132;&#27969;&#12290;&#22240;&#27492;&#65292;&#38544;&#21947;&#29702;&#35299;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#38544;&#21947;&#29702;&#35299;&#25361;&#25112;&#25968;&#25454;&#38598;&#65288;MUNCH&#65289;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#38544;&#21947;&#29702;&#35299;&#33021;&#21147;&#12290;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#21253;&#21547;&#38544;&#21947;&#29992;&#27861;&#30340;&#21477;&#23376;&#30340;&#36229;&#36807;10k&#20010;&#37322;&#20041;&#65292;&#20197;&#21450;&#21253;&#21547;1.5k&#20010;&#19981;&#24688;&#24403;&#37322;&#20041;&#30340;&#23454;&#20363;&#12290;&#36825;&#20123;&#19981;&#24688;&#24403;&#37322;&#20041;&#32463;&#36807;&#31934;&#24515;&#25361;&#36873;&#65292;&#26088;&#22312;&#20316;&#20026;&#25511;&#21046;&#26469;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#30830;&#23454;&#36827;&#34892;&#20840;&#38754;&#30340;&#38544;&#21947;&#35299;&#37322;&#65292;&#36824;&#26159;&#20165;&#20165;&#20381;&#36182;&#35789;&#27719;&#30456;&#20284;&#24615;&#12290;&#25152;&#26377;&#24688;&#24403;&#21644;&#19981;&#24688;&#24403;&#30340;&#37322;&#20041;&#37117;&#32463;&#36807;&#25163;&#21160;&#27880;&#37322;&#12290;&#36825;&#20123;&#38544;&#21947;&#21477;&#28085;&#30422;&#20102;4&#31181;&#27969;&#27966;&#65288;&#23398;&#26415;&#12289;&#26032;&#38395;&#12289;&#23567;&#35828;&#21644;&#23545;&#35805;&#65289;&#20013;&#30340;&#33258;&#28982;&#38544;&#21947;&#29992;&#27861;&#65292;&#24182;&#23637;&#31034;&#19981;&#21516;&#31243;&#24230;&#30340;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11810v1 Announce Type: new  Abstract: Metaphors in natural language are a reflection of fundamental cognitive processes such as analogical reasoning and categorisation, and are deeply rooted in everyday communication. Metaphor understanding is therefore an essential task for large language models (LLMs). We release the Metaphor Understanding Challenge Dataset (MUNCH), designed to evaluate the metaphor understanding capabilities of LLMs. The dataset provides over 10k paraphrases for sentences containing metaphor use, as well as 1.5k instances containing inapt paraphrases. The inapt paraphrases were carefully selected to serve as control to determine whether the model indeed performs full metaphor interpretation or rather resorts to lexical similarity. All apt and inapt paraphrases were manually annotated. The metaphorical sentences cover natural metaphor uses across 4 genres (academic, news, fiction, and conversation), and they exhibit different levels of novelty. Experiments
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11807</link><description>&lt;p&gt;
LLM&#30340;&#20915;&#31574;&#27700;&#24179;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#31350;&#31455;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#21508;&#31181;&#33021;&#21147;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26497;&#22909;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#35270;&#35282;&#25506;&#31350;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25903;&#25345;&#22810;&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#21442;&#19982;&#30340;&#28216;&#25103;&#65292;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;GAMA-Bench&#65292;&#21253;&#25324;&#20843;&#20010;&#32463;&#20856;&#30340;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20998;&#26041;&#26696;&#65292;&#23450;&#37327;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;GAMA-Bench&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#31283;&#20581;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#22686;&#24378;&#31574;&#30053;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;GPT-3.5&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#20854;&#27867;&#21270;&#33021;&#21147;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#19968;&#20123;&#26041;&#27861;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11807v1 Announce Type: new  Abstract: Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates LLMs' decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model's performance in these games quantitatively. Through GAMA-Bench, we investigate LLMs' robustness, generalizability, and enhancement strategies. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other mod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Counting-Stars&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;GPT-4 Turbo&#21644;Kimi Chat&#22312;&#27492;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.11802</link><description>&lt;p&gt;
Counting-Stars&#65306;&#19968;&#31181;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11802
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Counting-Stars&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;GPT-4 Turbo&#21644;Kimi Chat&#22312;&#27492;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#20855;&#26377;&#24378;&#22823;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#35780;&#20272;&#31574;&#30053;&#65292;&#23545;&#39046;&#20808;&#30340;LLMs&#65288;&#20363;&#22914;ChatGPT&#21644;KimiChat&#65289;&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#20102;&#35299;&#29978;&#23569;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#35780;&#20272;&#31574;&#30053;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;Counting-Stars&#12290;Counting-Stars&#26088;&#22312;&#35201;&#27714;LLMs&#20805;&#20998;&#29702;&#35299;&#21644;&#25429;&#25417;&#38271;&#19978;&#19979;&#25991;&#20013;&#30340;&#38271;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#33021;&#22815;&#25910;&#38598;&#36328;&#36234;&#25972;&#20010;&#19978;&#19979;&#25991;&#30340;&#22810;&#20010;&#35777;&#25454;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#22522;&#20110;Counting-Stars&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#20102;&#20004;&#20010;&#39046;&#20808;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#65292;&#21363;GPT-4 Turbo&#21644;Kimi Chat&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4 Turbo&#21644;Kimi Chat&#22312;Counting-Stars&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11802v1 Announce Type: new  Abstract: While recent research endeavors have concentrated on developing Large Language Models (LLMs) with robust long-context capabilities, due to the lack of appropriate evaluation strategies, relatively little is known about how well the long-context processing abilities and performance of leading LLMs (e.g., ChatGPT and KimiChat). To address this gap, we propose a simple, efficient, and reasonable strategy for evaluating long-context LLMs as a new benchmark, named Counting-Stars. The Counting-Stars is designed to require LLMs to fully understand and capture long dependencies in long contexts and be able to collect inter-dependency across multiple pieces of evidence spanning the entire context to finish the task. Based on the Counting-Stars, we conduct experiments to evaluate the two leading long-context LLMs, i.e., GPT-4 Turbo and Kimi Chat. The experimental results indicate that GPT-4 Turbo and Kimi Chat achieve significant performance in th
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#25968;&#25454;&#38598;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36739;&#24369;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36923;&#36753;&#36830;&#36143;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#65292;&#23454;&#39564;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#20986;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#30340;&#21457;&#23637;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.11793</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65306;&#23545;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11793
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#25968;&#25454;&#38598;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36739;&#24369;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36923;&#36753;&#36830;&#36143;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#65292;&#23454;&#39564;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#20986;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#30340;&#21457;&#23637;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#29702;&#33021;&#21147;&#30340;&#29616;&#26377;&#26041;&#27861;&#20197;&#32467;&#26524;&#20026;&#20013;&#24515;&#65292;&#20351;&#24471;&#35780;&#20272;&#25512;&#29702;&#36807;&#31243;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#25968;&#25454;&#38598;&#20197;&#36807;&#31243;&#20026;&#20013;&#24515;&#30340;&#26041;&#24335;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#12290;ARC&#35201;&#27714;&#35299;&#20915;&#38382;&#39064;&#26102;&#20855;&#26377;&#20005;&#35880;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#19968;&#20010;&#33021;&#22815;&#20419;&#36827;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#30340;&#22522;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36739;&#24369;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36923;&#36753;&#36830;&#36143;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#31361;&#26174;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#30340;&#21457;&#23637;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11793v1 Announce Type: cross  Abstract: The existing methods for evaluating the inference abilities of Large Language Models (LLMs) have been results-centric, making it difficult to assess the inference process. We introduce a new approach using the Abstract and Reasoning Corpus (ARC) dataset to evaluate the inference and contextual understanding abilities of large language models in a process-centric manner. ARC demands rigorous logical structures for problem-solving, making it a benchmark that facilitates the comparison of model inference abilities with humans. Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity. Our experiments highlight the reasoning capabilities of LLMs, proposing development paths for achieving human-level reasoning.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;OpenAI&#30340;GPT-3.5&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25104;&#21151;&#25552;&#21462;&#20102;&#36229;&#20851;&#32852;&#30693;&#35782;&#65292;&#23613;&#31649;&#31934;&#30830;&#29575;&#36739;&#20302;&#65292;&#20294;&#32467;&#26524;&#26174;&#31034;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.11786</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#36229;&#20851;&#32852;&#30693;&#35782;&#22270;
&lt;/p&gt;
&lt;p&gt;
Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11786
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;OpenAI&#30340;GPT-3.5&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25104;&#21151;&#25552;&#21462;&#20102;&#36229;&#20851;&#32852;&#30693;&#35782;&#65292;&#23613;&#31649;&#31934;&#30830;&#29575;&#36739;&#20302;&#65292;&#20294;&#32467;&#26524;&#26174;&#31034;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21462;&#36229;&#20851;&#31995;&#23545;&#20110;&#26500;&#24314;&#20840;&#38754;&#30340;&#30693;&#35782;&#22270;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#30417;&#30563;&#26041;&#27861;&#26377;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;OpenAI&#30340;GPT-3.5&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#36229;&#20851;&#32852;&#30693;&#35782;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#21484;&#22238;&#29575;&#36798;&#21040;&#20102;0.77&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#31934;&#30830;&#29575;&#30446;&#21069;&#36739;&#20302;&#65292;&#20294;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#35814;&#32454;&#20998;&#26512;&#25581;&#31034;&#20102;&#26410;&#26469;&#30740;&#31350;&#39046;&#22495;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11786v1 Announce Type: cross  Abstract: Extracting hyper-relations is crucial for constructing comprehensive knowledge graphs, but there are limited supervised methods available for this task. To address this gap, we introduce a zero-shot prompt-based method using OpenAI's GPT-3.5 model for extracting hyper-relational knowledge from text. Comparing our model with a baseline, we achieved promising results, with a recall of 0.77. Although our precision is currently lower, a detailed analysis of the model outputs has uncovered potential pathways for future research in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#39044;&#27979;&#21463;&#35797;&#32773;&#27491;&#22312;&#30475;&#21040;&#30340;&#21050;&#28608;&#30340;&#21333;&#19968;&#35299;&#30721;&#22120;&#65292;&#26080;&#35770;&#21050;&#28608;&#26159;&#20197;&#20309;&#31181;&#27169;&#24577;&#21576;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#27169;&#24577;&#26080;&#20851;&#35299;&#30721;&#22120;&#34920;&#29616;&#19982;&#27169;&#24577;&#29305;&#23450;&#35299;&#30721;&#22120;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.11771</link><description>&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#27169;&#24577;&#26080;&#20851;fMRI&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Modality-Agnostic fMRI Decoding of Vision and Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#39044;&#27979;&#21463;&#35797;&#32773;&#27491;&#22312;&#30475;&#21040;&#30340;&#21050;&#28608;&#30340;&#21333;&#19968;&#35299;&#30721;&#22120;&#65292;&#26080;&#35770;&#21050;&#28608;&#26159;&#20197;&#20309;&#31181;&#27169;&#24577;&#21576;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#27169;&#24577;&#26080;&#20851;&#35299;&#30721;&#22120;&#34920;&#29616;&#19982;&#27169;&#24577;&#29305;&#23450;&#35299;&#30721;&#22120;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#23558;&#21463;&#35797;&#32773;&#35266;&#30475;&#22270;&#20687;&#30340;&#22823;&#33041;&#28608;&#27963;&#25968;&#25454;&#26144;&#23556;&#21040;&#35270;&#35273;&#27169;&#22411;&#65288;&#27169;&#24577;&#29305;&#23450;&#35299;&#30721;&#65289;&#20197;&#21450;&#35821;&#35328;&#27169;&#22411;&#65288;&#36328;&#27169;&#24577;&#35299;&#30721;&#65289;&#30340;&#29305;&#24449;&#34920;&#31034;&#31354;&#38388;&#20013;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#24182;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;fMRI&#25968;&#25454;&#38598;&#65288;&#27599;&#20010;&#21463;&#35797;&#32773;&#32422;8500&#20010;&#35797;&#39564;&#65289;&#65292;&#35753;&#20154;&#20204;&#26082;&#35266;&#30475;&#22270;&#20687;&#21448;&#35266;&#30475;&#36825;&#20123;&#22270;&#20687;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#36825;&#19968;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#23454;&#29616;&#20102;&#27169;&#24577;&#26080;&#20851;&#35299;&#30721;&#22120;&#30340;&#24320;&#21457;&#65306;&#19968;&#31181;&#21333;&#19968;&#35299;&#30721;&#22120;&#21487;&#20197;&#39044;&#27979;&#21463;&#35797;&#32773;&#27491;&#22312;&#30475;&#21040;&#30340;&#21050;&#28608;&#65292;&#32780;&#19981;&#32771;&#34385;&#21050;&#28608;&#20197;&#20309;&#31181;&#27169;&#24577;&#65288;&#22270;&#20687;&#25110;&#25991;&#26412;&#65289;&#21576;&#29616;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#36825;&#31867;&#35299;&#30721;&#22120;&#65292;&#20197;&#23558;&#22823;&#33539;&#22260;&#20844;&#24320;&#21487;&#29992;&#30340;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#65288;&#35270;&#35273;+&#35821;&#35328;&#65289;&#27169;&#22411;&#20013;&#30340;&#21050;&#28608;&#34920;&#31034;&#26144;&#23556;&#21040;&#22823;&#33041;&#20449;&#21495;&#19978;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#65288;1&#65289;&#27169;&#24577;&#26080;&#20851;&#35299;&#30721;&#22120;&#34920;&#29616;&#24471;&#19982;&#65288;&#26377;&#26102;&#29978;&#33267;&#26356;&#22909;&#65289;&#27169;&#24577;&#29305;&#23450;&#35299;&#30721;&#19968;&#26679;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11771v1 Announce Type: cross  Abstract: Previous studies have shown that it is possible to map brain activation data of subjects viewing images onto the feature representation space of not only vision models (modality-specific decoding) but also language models (cross-modal decoding). In this work, we introduce and use a new large-scale fMRI dataset (~8,500 trials per subject) of people watching both images and text descriptions of such images. This novel dataset enables the development of modality-agnostic decoders: a single decoder that can predict which stimulus a subject is seeing, irrespective of the modality (image or text) in which the stimulus is presented. We train and evaluate such decoders to map brain signals onto stimulus representations from a large range of publicly available vision, language and multimodal (vision+language) models. Our findings reveal that (1) modality-agnostic decoders perform as well as (and sometimes even better than) modality-specific dec
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#38901;&#24459;&#35799;&#21644;&#35799;&#27468;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20934;&#30830;&#29575;&#20026;97%&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32416;&#27491;&#20102;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.11752</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#32463;&#20856;&#65306;&#35782;&#21035;&#21644;&#32416;&#27491;&#38901;&#24459;&#35799;&#20013;&#30340;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting The Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#38901;&#24459;&#35799;&#21644;&#35799;&#27468;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20934;&#30830;&#29575;&#20026;97%&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32416;&#27491;&#20102;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38901;&#24459;&#35799;&#26159;&#20256;&#36882;&#25991;&#21270;&#35268;&#33539;&#21644;&#31038;&#20250;&#35282;&#33394;&#30340;&#24378;&#22823;&#23186;&#20171;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20316;&#21697;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#24310;&#32493;&#20102;&#26377;&#20559;&#35265;&#30340;&#35748;&#30693;&#65292;&#24182;&#38480;&#21046;&#20102;&#20010;&#20307;&#36523;&#20221;&#30340;&#33539;&#22260;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#38901;&#24459;&#35799;&#21644;&#35799;&#27468;&#25968;&#25454;&#38598;&#65292;&#35782;&#21035;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20934;&#30830;&#29575;&#20026;97%&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#24615;&#21035;&#20559;&#35265;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32416;&#27491;&#20102;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#65292;&#24182;&#22312;&#19982;&#20154;&#31867;&#25945;&#32946;&#32773;&#32416;&#27491;&#31574;&#30053;&#30340;&#27604;&#36739;&#35843;&#26597;&#20013;&#35780;&#20272;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#24635;&#20043;&#65292;&#36825;&#39033;&#24037;&#20316;&#31361;&#26174;&#20102;&#25991;&#23398;&#20316;&#21697;&#20013;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30340;&#26222;&#36941;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32416;&#27491;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11752v1 Announce Type: new  Abstract: Rhymes and poems are a powerful medium for transmitting cultural norms and societal roles. However, the pervasive existence of gender stereotypes in these works perpetuates biased perceptions and limits the scope of individuals' identities. Past works have shown that stereotyping and prejudice emerge in early childhood, and developmental research on causal mechanisms is critical for understanding and controlling stereotyping and prejudice. This work contributes by gathering a dataset of rhymes and poems to identify gender stereotypes and propose a model with 97\% accuracy to identify gender bias. Gender stereotypes were rectified using a Large Language Model (LLM) and its effectiveness was evaluated in a comparative survey against human educator rectifications. To summarize, this work highlights the pervasive nature of gender stereotypes in literary works and reveals the potential of LLMs to rectify gender stereotypes. This study raises 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;EMBER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#23558;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#30452;&#25509;&#23884;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#21516;&#26102;&#25991;&#26412;&#29983;&#25104;&#21644;&#20449;&#24687;&#25552;&#21462;&#65292;&#20351;&#24471;&#35299;&#30721;&#22120;-only&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.11747</link><description>&lt;p&gt;
&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#30340;&#23884;&#20837;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Embedded Named Entity Recognition using Probing Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11747
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;EMBER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#23558;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#30452;&#25509;&#23884;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#21516;&#26102;&#25991;&#26412;&#29983;&#25104;&#21644;&#20449;&#24687;&#25552;&#21462;&#65292;&#20351;&#24471;&#35299;&#30721;&#22120;-only&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#26159;&#33258;&#21160;&#20107;&#23454;&#26816;&#26597;&#25110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31561;&#24212;&#29992;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#30446;&#21069;&#65292;&#36825;&#35201;&#27714;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#21333;&#29420;&#30340;&#27169;&#22411;&#65292;&#36825;&#20250;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#65292;&#25110;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30772;&#22351;&#24615;&#24494;&#35843;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#30452;&#25509;&#23558;&#20449;&#24687;&#25552;&#21462;&#21151;&#33021;&#23884;&#20837;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#21516;&#26102;&#25991;&#26412;&#29983;&#25104;&#21644;&#20449;&#24687;&#25552;&#21462;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EMBER&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#23427;&#20351;&#35299;&#30721;&#22120;-only&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#38468;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#26497;&#23567;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-2&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EMBER&#22312;&#27969;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#20445;&#25345;&#39640;&#20196;&#29260;&#29983;&#25104;&#36895;&#29575;&#65292;&#19982;43.64%&#30456;&#27604;&#65292;&#20854;&#36895;&#24230;&#20165;&#30053;&#24494;&#19979;&#38477;&#32422;1%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11747v1 Announce Type: new  Abstract: Extracting semantic information from generated text is a useful tool for applications such as automated fact checking or retrieval augmented generation. Currently, this requires either separate models during inference, which increases computational cost, or destructive fine-tuning of the language model. Instead, we propose directly embedding information extraction capabilities into pre-trained language models using probing classifiers, enabling efficient simultaneous text generation and information extraction. For this, we introduce an approach called EMBER and show that it enables named entity recognition in decoder-only language models without fine-tuning them and while incurring minimal additional computational cost at inference time. Specifically, our experiments using GPT-2 show that EMBER maintains high token generation rates during streaming text generation, with only a negligible decrease in speed of around 1% compared to a 43.64
&lt;/p&gt;</description></item><item><title>NeFT&#26159;&#19968;&#31181;&#31070;&#32463;&#20803;&#32423;&#24494;&#35843;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#21644;&#35745;&#31639;&#39640;&#25928;&#22320;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11621</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#20851;&#27880;&#31070;&#32463;&#20803;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#20803;&#32423;&#30417;&#30563;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11621
&lt;/p&gt;
&lt;p&gt;
NeFT&#26159;&#19968;&#31181;&#31070;&#32463;&#20803;&#32423;&#24494;&#35843;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#21644;&#35745;&#31639;&#39640;&#25928;&#22320;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#23637;&#31034;&#21508;&#31181;&#34892;&#20026;&#21644;&#35282;&#33394;&#30340;&#31070;&#32463;&#20803;&#32452;&#25104;&#65292;&#22312;&#27169;&#22411;&#25193;&#23637;&#30340;&#21516;&#26102;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#22810;&#26679;&#21270;&#30340;&#29305;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#24182;&#38750;&#25152;&#26377;&#31070;&#32463;&#20803;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#37117;&#27963;&#36291;&#65292;&#32780;&#36825;&#31181;&#31232;&#30095;&#24615;&#19982;&#20219;&#21153;&#29305;&#23450;&#33021;&#21147;&#21576;&#27491;&#30456;&#20851;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#27169;&#22411;&#21098;&#26525;&#21644;&#35757;&#32451;&#25928;&#29575;&#30340;&#21457;&#23637;&#12290;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#28041;&#21450;LLMs&#30340;&#25152;&#26377;&#21442;&#25968;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#19988;&#21487;&#33021;&#24182;&#38750;&#24517;&#35201;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#26088;&#22312;&#26368;&#23567;&#21270;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#20294;&#23427;&#20204;&#20173;&#22312;&#30456;&#23545;&#23439;&#35266;&#30340;&#23610;&#24230;&#19978;&#25805;&#20316;&#65288;&#20363;&#22914;&#65292;&#23618;&#32423;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#20803;&#32423;&#24494;&#35843;&#65288;NeFT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#21442;&#25968;&#35757;&#32451;&#30340;&#31890;&#24230;&#32454;&#21270;&#21040;&#20010;&#20307;&#31070;&#32463;&#20803;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31934;&#30830;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#27169;&#22411;&#26356;&#26032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NeFT&#19981;&#20165;&#33021;&#22312;&#20960;&#20046;&#30456;&#21516;&#31934;&#24230;&#19979;&#26174;&#33879;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#36824;&#33021;&#22312;&#27169;&#22411;&#26356;&#26032;&#26041;&#38754; &#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11621v1 Announce Type: new  Abstract: Large Language Models (LLMs) are composed of neurons that exhibit various behaviors and roles, which become increasingly diversified as models scale. Recent studies have revealed that not all neurons are active across different datasets, and this sparsity correlates positively with the task-specific ability, leading to advancements in model pruning and training efficiency. Traditional fine-tuning methods engage all parameters of LLMs, which is computationally expensive and may not be necessary. In contrast, Parameter-Efficient Fine-Tuning (PEFT) approaches aim to minimize the number of trainable parameters, yet they still operate at a relatively macro scale (e.g., layer-level). We introduce Neuron-Level Fine-Tuning (NeFT), a novel approach that refines the granularity of parameter training down to the individual neuron, enabling more precise and computationally efficient model updates. The experimental results show that NeFT not only exc
&lt;/p&gt;</description></item><item><title>Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11585</link><description>&lt;p&gt;
Linguacodus&#65306;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#20013;&#36827;&#34892;&#21464;&#38761;&#24615;&#20195;&#30721;&#29983;&#25104;&#30340;&#21327;&#21516;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11585
&lt;/p&gt;
&lt;p&gt;
Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26080;&#32541;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#20195;&#30721;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Linguacodus&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#37096;&#32626;&#19968;&#20010;&#21160;&#24577;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#39640;&#32423;&#25968;&#25454;&#22609;&#24418;&#25351;&#20196;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36845;&#20195;&#22320;&#36716;&#25442;&#20026;&#20195;&#30721;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;Linguacodus&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33021;&#22815;&#35780;&#20272;&#21508;&#31181;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20026;&#29305;&#23450;&#20219;&#21153;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#31934;&#32454;&#35843;&#25972;&#36807;&#31243;&#65292;&#24182;&#38416;&#26126;&#20102;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#21151;&#33021;&#24615;&#20195;&#30721;&#12290;Linguacodus&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#37325;&#22823;&#39134;&#36291;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#20219;&#21153;&#25551;&#36848;&#21644;&#21487;&#25191;&#34892;&#20195;&#30721;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#23545;&#25512;&#36827;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11585v1 Announce Type: cross  Abstract: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across div
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TOLE&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#35760;&#32423;&#21035;&#22870;&#21169;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;"&#20808;&#37327;&#23376;&#21270;&#65292;&#28982;&#21518;&#21152;&#22122;&#22768;"&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#21487;&#20197;&#22312;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#19979;&#28789;&#27963;&#25193;&#23637;&#65292;&#36991;&#20813;&#20102;&#36807;&#25311;&#21512;&#21644;&#35821;&#20041;&#23849;&#28291;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11558</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#35760;&#32423;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Token-level Feedback for Controllable Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11558
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TOLE&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#35760;&#32423;&#21035;&#22870;&#21169;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;"&#20808;&#37327;&#23376;&#21270;&#65292;&#28982;&#21518;&#21152;&#22122;&#22768;"&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#21487;&#20197;&#22312;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#19979;&#28789;&#27963;&#25193;&#23637;&#65292;&#36991;&#20813;&#20102;&#36807;&#25311;&#21512;&#21644;&#35821;&#20041;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#28385;&#36275;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29983;&#25104;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24341;&#20837;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65292;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65288;&#24494;&#35843;&#22522;&#30784;&#26041;&#27861;&#65289;&#25110;&#35821;&#20041;&#23849;&#28291;&#65288;&#21518;&#22788;&#29702;&#26041;&#27861;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;RL&#26041;&#27861;&#36890;&#24120;&#26159;&#30001;&#31895;&#31890;&#24230;&#65288;&#21477;&#23376;/&#27573;&#33853;&#32423;&#21035;&#65289;&#30340;&#21453;&#39304;&#24341;&#23548;&#30340;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#21477;&#23376;&#20869;&#35821;&#20041;&#25197;&#26354;&#25110;&#36827;&#23637;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#27425;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TOLE&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21046;&#23450;&#20102;TOken-LEvel&#22870;&#21169;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65292;&#24182;&#37319;&#29992;&#8220;&#20808;&#37327;&#23376;&#21270;&#65292;&#28982;&#21518;&#21152;&#22122;&#22768;&#8221;&#33539;&#24335;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;TOLE&#21487;&#20197;&#28789;&#27963;&#22320;&#25193;&#23637;&#21040;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#65292;&#35745;&#31639;&#25104;&#26412;&#24456;&#20302;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;al
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11558v1 Announce Type: cross  Abstract: To meet the requirements of real-world applications, it is essential to control generations of large language models (LLMs). Prior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods). However, current RL methods are generally guided by coarse-grained (sentence/paragraph-level) feedback, which may lead to suboptimal performance owing to semantic twists or progressions within sentences. To tackle that, we propose a novel reinforcement learning algorithm named TOLE which formulates TOken-LEvel rewards for controllable text generation, and employs a "first-quantize-then-noise" paradigm to enhance the robustness of the RL algorithm.Furthermore, TOLE can be flexibly extended to multiple constraints with little computational expense. Experimental results show that our al
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;DEE&#65292;&#19968;&#20010;&#21452;&#38454;&#27573;&#21487;&#35299;&#37322;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.11509</link><description>&lt;p&gt;
DEE: &#25991;&#26412;&#29983;&#25104;&#30340;&#21452;&#38454;&#27573;&#21487;&#35299;&#37322;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DEE: Dual-stage Explainable Evaluation Method for Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11509
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;DEE&#65292;&#19968;&#20010;&#21452;&#38454;&#27573;&#21487;&#35299;&#37322;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#26041;&#27861;&#23545;&#20110;&#29983;&#25104;&#31995;&#32479;&#30340;&#24212;&#29992;&#26085;&#30410;&#24191;&#27867;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#24456;&#38590;&#35299;&#37322;&#24615;&#65292;&#21457;&#20986;&#19968;&#20010;&#23396;&#31435;&#30340;&#25968;&#23383;&#35780;&#20998;&#26469;&#34920;&#31034;&#35780;&#20272;&#32467;&#26524;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#35797;&#22270;&#36890;&#36807;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#38169;&#35823;&#20998;&#26512;&#26469;&#32531;&#35299;&#36825;&#19968;&#23616;&#38480;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#20173;&#21463;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22312;&#20840;&#38754;&#35206;&#30422;&#38169;&#35823;&#21644;&#24555;&#36895;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#24037;&#19994;&#32972;&#26223;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DEE&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#21452;&#38454;&#27573;&#21487;&#35299;&#37322;&#35780;&#20272;&#26041;&#27861;&#12290;&#22522;&#20110;Llama 2&#26500;&#24314;&#30340;DEE&#36981;&#24490;&#19968;&#20010;&#21452;&#38454;&#27573;&#21407;&#21017;&#65292;&#26681;&#25454;&#21508;&#38454;&#27573;&#30340;&#25351;&#23548;&#36827;&#34892;&#39640;&#25928;&#35782;&#21035;&#21021;&#22987;&#38454;&#27573;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#38543;&#21518;&#30528;&#25163;&#25552;&#20379;&#20840;&#38754;&#30340;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11509v1 Announce Type: new  Abstract: Automatic methods for evaluating machine-generated texts hold significant importance due to the expanding applications of generative systems. Conventional methods tend to grapple with a lack of explainability, issuing a solitary numerical score to signify the assessment outcome. Recent advancements have sought to mitigate this limitation by incorporating large language models (LLMs) to offer more detailed error analyses, yet their applicability remains constrained, particularly in industrial contexts where comprehensive error coverage and swift detection are paramount. To alleviate these challenges, we introduce DEE, a Dual-stage Explainable Evaluation method for estimating the quality of text generation. Built upon Llama 2, DEE follows a dual-stage principle guided by stage-specific instructions to perform efficient identification of errors in generated texts in the initial stage and subsequently delves into providing comprehensive diag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39034;&#24207;&#37325;&#24314;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#20851;&#20110;&#35789;&#24207;&#30340;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#33539;&#22260;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;ChatGPT&#20381;&#36182;&#20110;&#35789;&#24207;&#36827;&#34892;&#25512;&#26029;&#65292;&#20294;&#19981;&#33021;&#26126;&#30830;&#25903;&#25345;&#25110;&#21542;&#23450;&#35789;&#24207;&#19982;&#35789;&#27719;&#35821;&#20041;&#20043;&#38388;&#30340;&#20887;&#20313;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.11473</link><description>&lt;p&gt;
&#35789;&#24207;&#30340;&#24433;&#21709;&#65306;&#37325;&#26032;&#25490;&#24207;&#21644;&#29983;&#25104;&#20998;&#26512;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
Word Order's Impacts: Insights from Reordering and Generation Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39034;&#24207;&#37325;&#24314;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#20851;&#20110;&#35789;&#24207;&#30340;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#33539;&#22260;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;ChatGPT&#20381;&#36182;&#20110;&#35789;&#24207;&#36827;&#34892;&#25512;&#26029;&#65292;&#20294;&#19981;&#33021;&#26126;&#30830;&#25903;&#25345;&#25110;&#21542;&#23450;&#35789;&#24207;&#19982;&#35789;&#27719;&#35821;&#20041;&#20043;&#38388;&#30340;&#20887;&#20313;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#35789;&#35821;&#39034;&#24207;&#30340;&#24433;&#21709;&#12290;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#25171;&#20081;&#21407;&#22987;&#35789;&#24207;&#26469;&#21019;&#24314;&#19968;&#20010;&#28151;&#20081;&#30340;&#24207;&#21015;&#36827;&#34892;&#20998;&#26512;&#65292;&#28982;&#21518;&#27604;&#36739;&#27169;&#22411;&#22312;&#21407;&#22987;&#24207;&#21015;&#21644;&#28151;&#20081;&#24207;&#21015;&#20043;&#38388;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#19968;&#20123;&#23567;&#24133;&#19979;&#38477;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#20851;&#20110;&#35789;&#24207;&#30340;&#20551;&#35774;&#65292;&#21253;&#25324;&#8220;&#35789;&#24207;&#19982;&#35789;&#27719;&#35821;&#20041;&#37325;&#22797;&#8221;&#21644;&#8220;&#27169;&#22411;&#19981;&#20381;&#36182;&#20110;&#35789;&#24207;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;&#19968;&#20010;&#39034;&#24207;&#37325;&#24314;&#30340;&#35270;&#35282;&#65292;&#36873;&#25321;&#19981;&#21516;&#33539;&#22260;&#30340;&#25968;&#25454;&#38598;&#26469;&#37325;&#26032;&#23457;&#35270;&#19978;&#36848;&#20551;&#35774;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36873;&#25321;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#39034;&#24207;&#37325;&#24314;&#21644;&#36830;&#32493;&#29983;&#25104;&#20219;&#21153;&#12290;&#23454;&#35777;&#30740;&#31350;&#25903;&#25345;ChatGPT&#20381;&#36182;&#20110;&#35789;&#24207;&#36827;&#34892;&#25512;&#26029;&#65292;&#20294;&#26080;&#27861;&#25903;&#25345;&#25110;&#21542;&#23450;&#35789;&#24207;&#19982;&#35789;&#27719;&#35821;&#20041;&#20043;&#38388;&#30340;&#20887;&#20313;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11473v1 Announce Type: cross  Abstract: Existing works have studied the impacts of the order of words within natural text. They usually analyze it by destroying the original order of words to create a scrambled sequence, and then comparing the models' performance between the original and scrambled sequences. The experimental results demonstrate marginal drops. Considering this findings, different hypothesis about word order is proposed, including ``the order of words is redundant with lexical semantics'', and ``models do not rely on word order''. In this paper, we revisit the aforementioned hypotheses by adding a order reconstruction perspective, and selecting datasets of different spectrum. Specifically, we first select four different datasets, and then design order reconstruction and continuing generation tasks. Empirical findings support that ChatGPT relies on word order to infer, but cannot support or negate the redundancy relations between word order lexical semantics.
&lt;/p&gt;</description></item><item><title>HateCOT&#25968;&#25454;&#38598;&#36890;&#36807;GPT-3.5-Turbo&#29983;&#25104;&#35299;&#37322;&#65292;&#23558;52,000&#20010;&#26679;&#26412;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#19979;&#30340;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.11456</link><description>&lt;p&gt;
HateCOT&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27867;&#21270;&#25915;&#20987;&#24615;&#35328;&#35770;&#26816;&#27979;&#30340;&#35299;&#37322;&#22686;&#24378;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11456
&lt;/p&gt;
&lt;p&gt;
HateCOT&#25968;&#25454;&#38598;&#36890;&#36807;GPT-3.5-Turbo&#29983;&#25104;&#35299;&#37322;&#65292;&#23558;52,000&#20010;&#26679;&#26412;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#19979;&#30340;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#23545;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#21487;&#38752;&#39640;&#25928;&#26816;&#27979;&#30340;&#38656;&#27714;&#65292;&#20026;&#20102;&#38480;&#21046;&#20854;&#26377;&#23475;&#24433;&#21709;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#19982;&#26816;&#27979;&#25915;&#20987;&#24615;&#20869;&#23481;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;HateCOT&#65292;&#36825;&#26159;&#20174;&#22810;&#26679;&#21270;&#29616;&#26377;&#26469;&#28304;&#20013;&#25277;&#21462;&#30340;5.2&#19975;&#20010;&#26679;&#26412;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30001;GPT-3.5-Turbo&#21644;&#20154;&#24037;&#31934;&#24515;&#21046;&#20316;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;HateCOT&#19978;&#20026;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#38646;-shot&#21644;few-shot&#35774;&#32622;&#19979;&#26174;&#33879;&#25913;&#36827;&#20102;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#23613;&#31649;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11456v1 Announce Type: cross  Abstract: The ubiquitousness of social media has led to the need for reliable and efficient detection of offensive content to limit harmful effects. This has led to a proliferation of datasets and models related to detecting offensive content. While sophisticated models have attained strong performance on individual datasets, these models often do not generalize due to differences between how "offensive content" is conceptualized, and the resulting differences in how these datasets are labeled. In this paper, we introduce HateCOT, a dataset of 52,000 samples drawn from diverse existing sources with explanations generated by GPT-3.5-Turbo and human-curated. We show that pre-training models for the detection of offensive content on HateCOT significantly boots open-sourced Language Models on three benchmark datasets in both zero and few-shot settings, despite differences in domain and task.} We further find that HateCOT enables effective K-shot fin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;38&#31181;&#39118;&#26684;&#30340;&#39118;&#26684;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;StyleEval&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#26684;&#21270;&#23545;&#35805;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;LLMs&#22312;&#39118;&#26684;&#21270;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.11439</link><description>&lt;p&gt;
StyleChat: &#22312;LLMs&#20013;&#23398;&#20064;&#22797;&#36848;&#22686;&#24378;&#35760;&#24518;&#20197;&#36827;&#34892;&#39118;&#26684;&#21270;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
StyleChat: Learning Recitation-Augmented Memory in LLMs for Stylized Dialogue Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11439
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;38&#31181;&#39118;&#26684;&#30340;&#39118;&#26684;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;StyleEval&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#26684;&#21270;&#23545;&#35805;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;LLMs&#22312;&#39118;&#26684;&#21270;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29983;&#25104;&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#20013;&#65292;&#22312;LLMs&#29615;&#22659;&#20013;&#36827;&#34892;&#39118;&#26684;&#21270;&#23545;&#35805;&#29983;&#25104;&#23545;&#20110;&#26500;&#24314;&#26234;&#33021;&#19988;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#33021;&#21147;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#24182;&#21463;&#25968;&#25454;&#20559;&#24046;&#38480;&#21046;&#65292;&#23548;&#33268;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#29305;&#21035;&#26159;&#65292;&#39118;&#26684;&#21270;&#23545;&#35805;&#29983;&#25104;&#38754;&#20020;&#30528;&#20005;&#37325;&#30340;&#30417;&#30563;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#23436;&#25104;&#29305;&#23450;&#20219;&#21153;&#65292;&#20294;&#22312;&#28041;&#21450;&#21508;&#31181;&#23545;&#35805;&#39118;&#26684;&#30340;&#22797;&#26434;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#19968;&#20010;&#21253;&#21547;38&#31181;&#39118;&#26684;&#30340;&#39118;&#26684;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;StyleEval&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#20154;&#20026;&#36136;&#37327;&#25511;&#21046;&#31934;&#24515;&#26500;&#24314;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39118;&#26684;&#21270;&#23545;&#35805;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11439v1 Announce Type: new  Abstract: Large Language Models (LLMs) demonstrate superior performance in generative scenarios and have attracted widespread attention. Among them, stylized dialogue generation is essential in the context of LLMs for building intelligent and engaging dialogue agent. However the ability of LLMs is data-driven and limited by data bias, leading to poor performance on specific tasks. In particular, stylized dialogue generation suffers from a severe lack of supervised data. Furthermore, although many prompt-based methods have been proposed to accomplish specific tasks, their performance in complex real-world scenarios involving a wide variety of dialog styles further enhancement. In this work, we first introduce a stylized dialogue dataset StyleEval with 38 styles by leveraging the generative power of LLMs comprehensively, which has been carefully constructed with rigorous human-led quality control. Based on this, we propose the stylized dialogue fram
&lt;/p&gt;</description></item><item><title>InsCL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#23548;&#20449;&#24687;&#30340;&#26032;&#22411;&#25345;&#32493;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;Wasserstein&#36317;&#31163;&#35745;&#31639;&#20219;&#21153;&#30456;&#20284;&#24615;&#65292;&#24182;&#24341;&#20837;InsInfo&#25351;&#26631;&#26469;&#23450;&#37327;&#25351;&#23548;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#25351;&#23548;&#37325;&#25773;&#36807;&#31243;&#26356;&#20542;&#21521;&#20110;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.11435</link><description>&lt;p&gt;
InsCL: &#19968;&#31181;&#29992;&#25351;&#23548;&#20449;&#24687;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#39640;&#25928;&#25345;&#32493;&#23398;&#20064;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11435
&lt;/p&gt;
&lt;p&gt;
InsCL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#23548;&#20449;&#24687;&#30340;&#26032;&#22411;&#25345;&#32493;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;Wasserstein&#36317;&#31163;&#35745;&#31639;&#20219;&#21153;&#30456;&#20284;&#24615;&#65292;&#24182;&#24341;&#20837;InsInfo&#25351;&#26631;&#26469;&#23450;&#37327;&#25351;&#23548;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#25351;&#23548;&#37325;&#25773;&#36807;&#31243;&#26356;&#20542;&#21521;&#20110;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#29615;&#22659;&#30340;&#19981;&#26029;&#21464;&#21270;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38656;&#35201;&#22312;&#19981;&#20135;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#25345;&#32493;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#36866;&#24212;&#12290;&#30001;&#20110;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#22522;&#20110;&#37325;&#25773;&#30340;&#25345;&#32493;&#23398;&#20064;(CL)&#26041;&#27861;&#26159;&#35299;&#20915;LLMs&#36951;&#24536;&#38382;&#39064;&#30340;&#26368;&#31616;&#21333;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#37325;&#25773;&#30340;&#26041;&#27861;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#25351;&#23548;&#20449;&#24687;&#26469;&#23450;&#21046;&#37325;&#25773;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#25351;&#23548;&#20449;&#24687;&#30340;&#25345;&#32493;&#23398;&#20064;(InsCL)&#30340;&#26032;&#33539;&#24335;&#12290;InsCL&#26681;&#25454;&#20219;&#21153;&#30456;&#20284;&#24615;&#65288;&#36890;&#36807;&#25351;&#23548;&#20449;&#24687;&#30340;Wasserstein Distance&#35745;&#31639;&#65289;&#21160;&#24577;&#37325;&#25773;&#20808;&#21069;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#25351;&#23548;&#20449;&#24687;&#24230;&#37327;&#26631;&#20934;(InsInfo)&#26469;&#37327;&#21270;&#25351;&#23548;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#26681;&#25454;InsInfo&#65292;InsCL&#24341;&#23548;&#37325;&#25773;&#36807;&#31243;&#26356;&#20542;&#21521;&#20110;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11435v1 Announce Type: new  Abstract: Instruction tuning effectively optimizes Large Language Models (LLMs) for downstream tasks. Due to the changing environment in real-life applications, LLMs necessitate continual task-specific adaptation without catastrophic forgetting. Considering the heavy computational cost, replay-based Continual Learning (CL) methods are the simplest and most widely used for LLMs to address the forgetting issue. However, traditional replay-based methods do not fully utilize instructions to customize the replay strategy. In this work, we propose a novel paradigm called Instruction-based Continual Learning (InsCL). InsCL dynamically replays previous data based on task similarity, calculated by Wasserstein Distance with instructions. Moreover, we further introduce an Instruction Information Metric (InsInfo) to quantify the complexity and diversity of instructions. According to InsInfo, InsCL guides the replay process more inclined to high-quality data. 
&lt;/p&gt;</description></item><item><title>&#26032;&#33539;&#24335;&#20851;&#27880;&#22312;LLMs&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#22686;&#24378;&#36328;&#35821;&#35328;&#23545;&#40784;&#33021;&#21147;&#65292;&#24378;&#35843;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#23567;&#22411;&#21452;&#35821;&#25968;&#25454;&#65292;&#22312;&#20256;&#32479;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#26080;&#27861;&#32988;&#20219;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.11430</link><description>&lt;p&gt;
&#19968;&#31181;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32763;&#35793;&#33021;&#21147;&#30340;&#26032;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
A Novel Paradigm Boosting Translation Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11430
&lt;/p&gt;
&lt;p&gt;
&#26032;&#33539;&#24335;&#20851;&#27880;&#22312;LLMs&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#22686;&#24378;&#36328;&#35821;&#35328;&#23545;&#40784;&#33021;&#21147;&#65292;&#24378;&#35843;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#23567;&#22411;&#21452;&#35821;&#25968;&#25454;&#65292;&#22312;&#20256;&#32479;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#26080;&#27861;&#32988;&#20219;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32763;&#35793;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#30340;&#26032;&#33539;&#24335;&#65306;&#20351;&#29992;&#22823;&#37327;&#21333;&#35821;&#25968;&#25454;&#30340;&#27425;&#32423;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#20114;&#25991;&#26684;&#24335;&#25991;&#26723;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#21033;&#29992;&#19982;&#28304;&#35821;&#35328;&#19968;&#33268;&#30340;&#25351;&#23548;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#37325;&#28857;&#24212;&#35813;&#25918;&#22312;&#22686;&#24378;LLMs&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#36328;&#35821;&#35328;&#23545;&#40784;&#33021;&#21147;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#38752;SFT&#26399;&#38388;&#22823;&#37327;&#30340;&#21452;&#35821;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#20381;&#36182;&#22823;&#37327;&#21452;&#35821;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20256;&#32479;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#26080;&#27861;&#32988;&#20219;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11430v1 Announce Type: new  Abstract: This paper presents a study on strategies to enhance the translation capabilities of large language models (LLMs) in the context of machine translation (MT) tasks. The paper proposes a novel paradigm consisting of three stages: Secondary Pre-training using Extensive Monolingual Data, Continual Pre-training with Interlinear Text Format Documents, and Leveraging Source-Language Consistent Instruction for Supervised Fine-Tuning. Previous research on LLMs focused on various strategies for supervised fine-tuning (SFT), but their effectiveness has been limited. While traditional machine translation approaches rely on vast amounts of parallel bilingual data, our paradigm highlights the importance of using smaller sets of high-quality bilingual data. We argue that the focus should be on augmenting LLMs' cross-lingual alignment abilities during pre-training rather than solely relying on extensive bilingual data during SFT. Experimental results co
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#26032;&#39062;&#30340;&#21465;&#36848;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#30284;&#30151;&#24739;&#32773;&#24739;&#24515;&#21147;&#34928;&#31469;&#30340;&#39118;&#38505;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11425</link><description>&lt;p&gt;
&#21465;&#20107;&#29305;&#24449;&#36824;&#26159;&#32467;&#26500;&#29305;&#24449;&#65311;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#35782;&#21035;&#24739;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;
&lt;/p&gt;
&lt;p&gt;
Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11425
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#26032;&#39062;&#30340;&#21465;&#36848;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#30284;&#30151;&#24739;&#32773;&#24739;&#24515;&#21147;&#34928;&#31469;&#30340;&#39118;&#38505;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#27835;&#30103;&#24050;&#30693;&#20250;&#24341;&#20837;&#24515;&#27602;&#24615;&#65292;&#23545;&#39044;&#21518;&#21644;&#29983;&#23384;&#29575;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#35782;&#21035;&#24739;&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;&#23545;&#20110;&#25913;&#21892;&#30284;&#30151;&#27835;&#30103;&#32467;&#26524;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26469;&#33258;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#21253;&#25324;&#20256;&#32479;ML&#12289;&#26102;&#38388;&#24863;&#30693;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;T-LSTM&#65289;&#21644;&#20351;&#29992;&#20174;&#32467;&#26500;&#21270;&#21307;&#23398;&#20195;&#30721;&#34893;&#29983;&#30340;&#26032;&#39062;&#21465;&#36848;&#29305;&#24449;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35782;&#21035;&#24739;HF&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;&#12290;&#25105;&#20204;&#20174;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#20581;&#24247;&#20013;&#24515;&#35782;&#21035;&#20102;&#19968;&#32452;&#21253;&#25324;12,806&#21517;&#32954;&#30284;&#12289;&#20083;&#33146;&#30284;&#21644;&#32467;&#30452;&#32928;&#30284;&#24739;&#32773;&#30340;&#30284;&#30151;&#38431;&#21015;&#65292;&#20854;&#20013;1,602&#20154;&#22312;&#30284;&#30151;&#21518;&#21457;&#23637;&#20026;HF&#12290;LLM GatorTron-3.9B&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;F1&#20998;&#25968;&#65292;&#27604;&#20256;&#32479;&#25903;&#25345;&#21521;&#37327;&#26426;&#39640;&#20986;39%&#65292;&#27604;T-LSTM&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#20986;7%&#65292;&#27604;&#24191;&#27867;&#20351;&#29992;&#30340;Transformer&#27169;&#22411;BERT&#39640;&#20986;5.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11425v1 Announce Type: cross  Abstract: Cancer treatments are known to introduce cardiotoxicity, negatively impacting outcomes and survivorship. Identifying cancer patients at risk of heart failure (HF) is critical to improving cancer treatment outcomes and safety. This study examined machine learning (ML) models to identify cancer patients at risk of HF using electronic health records (EHRs), including traditional ML, Time-Aware long short-term memory (T-LSTM), and large language models (LLMs) using novel narrative features derived from the structured medical codes. We identified a cancer cohort of 12,806 patients from the University of Florida Health, diagnosed with lung, breast, and colorectal cancers, among which 1,602 individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the best F1 scores, outperforming the traditional support vector machines by 39%, the T-LSTM deep learning model by 7%, and a widely used transformer model, BERT, by 5.6%. The analysi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#32972;&#26223;&#65292;&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#24314;&#35758;&#38382;&#39064;&#29983;&#25104;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#23427;&#21487;&#20197;&#29983;&#25104;&#27604;&#20854;&#20182;&#25552;&#31034;&#26041;&#27861;&#26356;&#22909;&#30340;&#24314;&#35758;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11413</link><description>&lt;p&gt;
&#22522;&#20110;RAG&#30340;&#20250;&#35805;&#31995;&#32479;&#20013;&#29983;&#25104;&#24314;&#35758;&#38382;&#39064;&#30340;&#21160;&#24577;&#32972;&#26223;
&lt;/p&gt;
&lt;p&gt;
Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11413
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#32972;&#26223;&#65292;&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#24314;&#35758;&#38382;&#39064;&#29983;&#25104;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#23427;&#21487;&#20197;&#29983;&#25104;&#27604;&#20854;&#20182;&#25552;&#31034;&#26041;&#27861;&#26356;&#22909;&#30340;&#24314;&#35758;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19982;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#20250;&#35805;&#20195;&#29702;&#20114;&#21160;&#26102;&#65292;&#29992;&#25143;&#24517;&#39035;&#23567;&#24515;&#22320;&#21046;&#23450;&#20182;&#20204;&#30340;&#26597;&#35810;&#20197;&#34987;&#27491;&#30830;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#20102;&#35299;&#31995;&#32479;&#30340;&#33021;&#21147;&#23545;&#29992;&#25143;&#26469;&#35828;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23548;&#33268;&#38656;&#35201;&#36827;&#19968;&#27493;&#28548;&#28165;&#30340;&#27169;&#31946;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#24314;&#35758;&#38382;&#39064;&#29983;&#25104;&#22120;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#20026;&#20102;&#29983;&#25104;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21033;&#29992;&#21160;&#24577;&#32972;&#26223;&#65292;&#20854;&#20013;&#21253;&#25324;&#21160;&#24577;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#21160;&#24577;&#26816;&#32034;&#21040;&#30340;&#32972;&#26223;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#21160;&#24577;&#32972;&#26223;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#24314;&#35758;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11413v1 Announce Type: new  Abstract: When interacting with Retrieval-Augmented Generation (RAG)-based conversational agents, the users must carefully craft their queries to be understood correctly. Yet, understanding the system's capabilities can be challenging for the users, leading to ambiguous questions that necessitate further clarification. This work aims to bridge the gap by developing a suggestion question generator. To generate suggestion questions, our approach involves utilizing dynamic context, which includes both dynamic few-shot examples and dynamically retrieved contexts. Through experiments, we show that the dynamic contexts approach can generate better suggestion questions as compared to other prompting approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#27861;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#33521;&#35821;-&#38889;&#35821;-&#20013;&#25991;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24320;&#21457;&#20102;&#34920;&#29616;&#20248;&#36234;&#30340;&#21452;&#35821;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11399</link><description>&lt;p&gt;
X-LLaVA: &#20248;&#21270;&#21452;&#35821;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11399
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#27861;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#33521;&#35821;-&#38889;&#35821;-&#20013;&#25991;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24320;&#21457;&#20102;&#34920;&#29616;&#20248;&#36234;&#30340;&#21452;&#35821;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#27491;&#22312;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#39046;&#22495;&#65292;&#36825;&#20123;&#27169;&#22411;&#38598;&#25104;&#20102;&#38500;&#25991;&#26412;&#20197;&#22806;&#30340;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#29305;&#24615;&#23548;&#33268;&#22312;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#23384;&#22312;&#26174;&#30528;&#30340;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#20026;LMMs&#26500;&#24314;&#22810;&#35821;&#35328;&#25968;&#25454;&#20063;&#38754;&#20020;&#30528;&#35821;&#35328;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#22810;&#35821;&#35328;LLM&#30340;&#35789;&#27719;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20351;&#29992;GPT4-V&#33258;&#21160;&#21644;&#31934;&#24515;&#26500;&#24314;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;91K&#33521;&#25991;-&#38889;&#25991;-&#20013;&#25991;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21452;&#35821;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22312;&#38889;&#35821;&#21644;&#33521;&#35821;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11399v1 Announce Type: new  Abstract: The impressive development of large language models (LLMs) is expanding into the realm of large multimodal models (LMMs), which incorporate multiple types of data beyond text. However, the nature of multimodal models leads to significant expenses in the creation of training data. Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity. Therefore, in this study, we propose two cost-effective methods to solve this problem: (1) vocabulary expansion and pretraining of multilingual LLM for specific languages, and (2) automatic and elaborate construction of multimodal datasets using GPT4-V. Based on015 these methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal training dataset. Additionally, we developed a bilingual multimodal model that exhibits excellent performance in both Korean and English, surpassing existing approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22686;&#24378;&#22411;&#33258;&#20027;&#20195;&#29702;&#22312;&#21512;&#20316;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#36825;&#20123;&#20195;&#29702;&#26174;&#31034;&#20986;&#21512;&#20316;&#20542;&#21521;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#21512;&#20316;&#22256;&#38590;&#65292;&#24378;&#35843;&#20102;&#23545;&#26356;&#20581;&#22766;&#26550;&#26500;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.11381</link><description>&lt;p&gt;
LLM&#22686;&#24378;&#22411;&#33258;&#20027;&#20195;&#29702;&#33021;&#22815;&#21512;&#20316;&#21527;&#65311;&#36890;&#36807;&#34701;&#21512;&#30406;&#35780;&#20272;&#23427;&#20204;&#30340;&#21512;&#20316;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22686;&#24378;&#22411;&#33258;&#20027;&#20195;&#29702;&#22312;&#21512;&#20316;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#36825;&#20123;&#20195;&#29702;&#26174;&#31034;&#20986;&#21512;&#20316;&#20542;&#21521;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#21512;&#20316;&#22256;&#38590;&#65292;&#24378;&#35843;&#20102;&#23545;&#26356;&#20581;&#22766;&#26550;&#26500;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#32500;&#24230;&#26159;&#21457;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#28508;&#21147;&#22686;&#24378;&#22810;&#20195;&#29702;&#20154;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#33879;&#21517;&#30340;Meltin Pot&#29615;&#22659;&#20197;&#21450;&#21442;&#32771;&#27169;&#22411;&#22914;GPT4&#21644;GPT3.5&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#33258;&#20027;&#20195;&#29702;(LAAs)&#30340;&#21512;&#20316;&#33021;&#21147;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#20195;&#29702;&#20154;&#34920;&#29616;&#20986;&#21512;&#20316;&#30340;&#20542;&#21521;&#65292;&#20294;&#22312;&#29305;&#23450;&#29615;&#22659;&#20013;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#21327;&#20316;&#65292;&#24378;&#35843;&#20102;&#26356;&#24378;&#22823;&#26550;&#26500;&#30340;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#36866;&#24212;LLM&#30340;Melting Pot&#28216;&#25103;&#22330;&#26223;&#30340;&#25277;&#35937;&#21270;&#23618;&#65292;&#19968;&#20010;&#29992;&#20110;LLM&#20013;&#20171;&#20195;&#29702;&#24320;&#21457;&#30340;&#21487;&#37325;&#29992;&#26550;&#26500;-&#21253;&#25324;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#20197;&#21450;&#19981;&#21516;&#30340;&#35748;&#30693;&#27169;&#22359;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#26041;&#27861;&#35780;&#20272;&#21512;&#20316;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11381v1 Announce Type: new  Abstract: As the field of AI continues to evolve, a significant dimension of this progression is the development of Large Language Models and their potential to enhance multi-agent artificial intelligence systems. This paper explores the cooperative capabilities of Large Language Model-augmented Autonomous Agents (LAAs) using the well-known Meltin Pot environments along with reference models such as GPT4 and GPT3.5. Preliminary results suggest that while these agents demonstrate a propensity for cooperation, they still struggle with effective collaboration in given environments, emphasizing the need for more robust architectures. The study's contributions include an abstraction layer to adapt Melting Pot game scenarios for LLMs, the implementation of a reusable architecture for LLM-mediated agent development - which includes short and long-term memories and different cognitive modules, and the evaluation of cooperation capabilities using a set of 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#25968;&#23398;&#38382;&#39064;&#30340;&#25991;&#23383;&#39064;&#30446;&#26102;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#20998;&#26512;&#29305;&#24449;&#21644;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#27169;&#22411;&#23545;&#19981;&#21516;&#31867;&#22411;&#39064;&#30446;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.11369</link><description>&lt;p&gt;
&#20160;&#20040;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#23398;&#38382;&#39064;&#38590;&#20197;&#24212;&#23545;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Makes Math Word Problems Challenging for LLMs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11369
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#25968;&#23398;&#38382;&#39064;&#30340;&#25991;&#23383;&#39064;&#30446;&#26102;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#20998;&#26512;&#29305;&#24449;&#21644;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#27169;&#22411;&#23545;&#19981;&#21516;&#31867;&#22411;&#39064;&#30446;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#32780;&#35328;&#65292;&#20160;&#20040;&#35753;&#25968;&#23398;&#38382;&#39064;&#30340;&#25991;&#23383;&#39064;&#30446;(math word problems, MWPs)&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#20102;MWPs&#30340;&#20851;&#38190;&#35821;&#35328;&#21644;&#25968;&#23398;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27599;&#20010;&#29305;&#24449;&#23545;&#20110;LLMs&#26085;&#24120;&#20219;&#21153;&#20013;MWPs&#30340;&#25972;&#20307;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#31350;&#36825;&#26159;&#21542;&#26377;&#21161;&#20110;&#39044;&#27979;LLMs&#22312;&#29305;&#23450;&#31867;&#21035;&#30340;MWPs&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11369v1 Announce Type: new  Abstract: This paper investigates the question of what makes math word problems (MWPs) challenging for large language models (LLMs). We conduct an in-depth analysis of the key linguistic and mathematical characteristics of MWPs. In addition, we train feature-based classifiers to better understand the impact of each feature on the overall difficulty of MWPs for prominent LLMs and investigate whether this helps predict how well LLMs fare against specific categories of MWPs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#30340;JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;&#65292;&#36890;&#36807;PEFT&#20860;&#23481;&#24494;&#35843;Llama-2&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;JAX&#30340;&#21363;&#26102;&#32534;&#35793;&#21644;&#24352;&#37327;&#20998;&#29255;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#31649;&#29702;&#65292;&#21152;&#36895;&#24494;&#35843;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;RAG&#24212;&#29992;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11366</link><description>&lt;p&gt;
JORA: &#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#30340;JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;
&lt;/p&gt;
&lt;p&gt;
JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11366
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#30340;JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;&#65292;&#36890;&#36807;PEFT&#20860;&#23481;&#24494;&#35843;Llama-2&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;JAX&#30340;&#21363;&#26102;&#32534;&#35793;&#21644;&#24352;&#37327;&#20998;&#29255;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#31649;&#29702;&#65292;&#21152;&#36895;&#24494;&#35843;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;RAG&#24212;&#29992;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;JORA: JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#12299;&#36890;&#36807;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#20219;&#21153;&#30340;PEFT&#20860;&#23481;&#24494;&#35843;Llama-2&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#29420;&#29305;&#22320;&#21033;&#29992;&#20102;JAX&#30340;&#21363;&#26102;&#32534;&#35793;&#65288;JIT&#65289;&#21644;&#24352;&#37327;&#20998;&#29255;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#30340;&#39640;&#25928;&#31649;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21152;&#36895;&#24494;&#35843;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#12290;&#36825;&#19968;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29992;&#20110;&#22797;&#26434;RAG&#24212;&#29992;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#29978;&#33267;&#22312;GPU&#36164;&#28304;&#26377;&#38480;&#30340;&#31995;&#32479;&#19978;&#20063;&#33021;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11366v1 Announce Type: cross  Abstract: The scaling of Large Language Models (LLMs) for retrieval-based tasks, particularly in Retrieval Augmented Generation (RAG), faces significant memory constraints, especially when fine-tuning extensive prompt sequences. Current open-source libraries support full-model inference and fine-tuning across multiple GPUs but fall short of accommodating the efficient parameter distribution required for retrieved context. Addressing this gap, we introduce a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT) compilation and tensor-sharding for efficient resource management, thereby enabling accelerated fine-tuning with reduced memory requirements. This advancement significantly improves the scalability and feasibility of fine-tuning LLMs for complex RAG applications, even on systems with limited GPU resources. Our experiments show more than 1
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CantonMT&#39033;&#30446;&#65292;&#21033;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#31908;&#35821;&#33267;&#33521;&#35821;NMT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#20197;&#20419;&#36827;&#30740;&#31350;</title><link>https://arxiv.org/abs/2403.11346</link><description>&lt;p&gt;
CantonMT: &#27721;&#33521;NMT&#24179;&#21488;&#65292;&#20351;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11346
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CantonMT&#39033;&#30446;&#65292;&#21033;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#31908;&#35821;&#33267;&#33521;&#35821;NMT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#20197;&#20419;&#36827;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11346v1 &#28040;&#24687;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#20173;&#28982;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#26631;&#20934;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#21453;&#21521;&#32763;&#35793;&#65292;&#24212;&#29992;&#21040;&#20102;&#26032;&#30340;&#35821;&#35328;&#32763;&#35793;&#26041;&#21521;&#31908;&#35821;&#33267;&#33521;&#35821;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;(&#21253;&#25324;OpusMT, NLLB,&#21644;mBART)&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#25351;&#26631;&#21253;&#25324;&#22522;&#20110;&#35789;&#27719;&#21644;&#23884;&#20837;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#36825;&#39033;\textsc{CantonMT}&#30740;&#31350;&#39033;&#30446;&#20013;&#21253;&#21547;&#30340;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#24182;&#25552;&#20379;&#20415;&#21033;&#23454;&#29616;&#31908;&#35821;&#33267;&#33521;&#35821;MT&#30740;&#31350;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#25105;&#20204;&#30340;&#24320;&#28304;\textsc{CantonMT}&#24037;&#20855;&#21253;\url{https://github.com/kenrickkung/CantoneseTranslation}&#21521;&#24179;&#21488;&#28155;&#21152;&#26356;&#22810;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11346v1 Announce Type: cross  Abstract: Neural Machine Translation (NMT) for low-resource languages is still a challenging task in front of NLP researchers. In this work, we deploy a standard data augmentation methodology by back-translation to a new language translation direction Cantonese-to-English. We present the models we fine-tuned using the limited amount of real data and the synthetic data we generated using back-translation including OpusMT, NLLB, and mBART. We carried out automatic evaluation using a range of different metrics including lexical-based and embedding-based. Furthermore. we create a user-friendly interface for the models we included in this\textsc{ CantonMT} research project and make it available to facilitate Cantonese-to-English MT research. Researchers can add more models into this platform via our open-source\textsc{ CantonMT} toolkit \url{https://github.com/kenrickkung/CantoneseTranslation}.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;ConvSDG&#26694;&#26550;&#25506;&#32034;&#20102;&#29983;&#25104;&#26356;&#22810;&#24102;&#30456;&#20851;&#26631;&#31614;&#35757;&#32451;&#20250;&#35805;&#20197;&#25552;&#21319;&#20250;&#35805;&#25628;&#32034;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11335</link><description>&lt;p&gt;
ConvSDG&#65306;&#29992;&#20110;&#20250;&#35805;&#25628;&#32034;&#30340;&#20250;&#35805;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ConvSDG: Session Data Generation for Conversational Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11335
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;ConvSDG&#26694;&#26550;&#25506;&#32034;&#20102;&#29983;&#25104;&#26356;&#22810;&#24102;&#30456;&#20851;&#26631;&#31614;&#35757;&#32451;&#20250;&#35805;&#20197;&#25552;&#21319;&#20250;&#35805;&#25628;&#32034;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#25628;&#32034;&#36890;&#36807;&#20801;&#35768;&#29992;&#25143;&#19982;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#25552;&#20379;&#20102;&#26356;&#20415;&#25463;&#30340;&#25628;&#32034;&#30028;&#38754;&#12290;&#28982;&#32780;&#65292;&#20250;&#35805;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#38480;&#21046;&#65292;&#36825;&#20123;&#25968;&#25454;&#38656;&#35201;&#29992;&#20110;&#24494;&#35843;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#26356;&#22810;&#24102;&#26377;&#30456;&#20851;&#26631;&#31614;&#30340;&#35757;&#32451;&#20250;&#35805;&#21487;&#33021;&#20250;&#25552;&#39640;&#25628;&#32034;&#24615;&#33021;&#12290;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#19978;&#30340;&#26377;&#30410;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvSDG&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25506;&#32034;&#20351;&#29992;LLM&#36827;&#34892;&#20250;&#35805;&#25968;&#25454;&#29983;&#25104;&#26469;&#25552;&#21319;&#20250;&#35805;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#26681;&#25454;&#30456;&#20851;&#21028;&#26029;&#30340;&#21487;&#29992;&#24615;&#35774;&#35745;&#20102;&#23545;&#35805;/&#20250;&#35805;&#32423;&#21644;&#26597;&#35810;&#32423;&#25968;&#25454;&#29983;&#25104;&#30340;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#29992;&#20110;&#24494;&#35843;&#20250;&#35805;&#23494;&#38598;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11335v1 Announce Type: cross  Abstract: Conversational search provides a more convenient interface for users to search by allowing multi-turn interaction with the search engine. However, the effectiveness of the conversational dense retrieval methods is limited by the scarcity of training data required for their fine-tuning. Thus, generating more training conversational sessions with relevant labels could potentially improve search performance. Based on the promising capabilities of large language models (LLMs) on text generation, we propose ConvSDG, a simple yet effective framework to explore the feasibility of boosting conversational search by using LLM for session data generation. Within this framework, we design dialogue/session-level and query-level data generation with unsupervised and semi-supervised learning, according to the availability of relevance judgments. The generated data are used to fine-tune the conversational dense retriever. Extensive experiments on four
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.11330</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#19968;&#20010;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#26469;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11330
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#65288;&#21363;&#65292;&#23545;&#35805;&#32423;&#65289;&#22870;&#21169;&#23545;&#40784;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#33258;&#28982;&#21457;&#29983;&#30340;&#22810;&#27169;&#24577;&#20449;&#21495;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;&#21517;&#20026;GELI&#65289;&#36890;&#36807;&#23558;&#20154;&#31867;&#25552;&#20379;&#30340;&#20840;&#23616;&#26126;&#30830;&#65288;GE&#65289;&#20250;&#35805;&#32423;&#22870;&#21169;&#25286;&#20998;&#65292;&#21033;&#29992;&#26412;&#22320;&#38544;&#24335;&#65288;LI&#65289;&#22810;&#27169;&#24577;&#22870;&#21169;&#20449;&#21495;&#26469;&#36328;&#27169;&#24577;&#22320;&#22609;&#36896;&#22870;&#21169;&#20998;&#35299;&#27493;&#39588;&#12290;&#28982;&#21518;&#23558;&#36825;&#31181;&#20998;&#35299;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#26631;&#20934;RHLF&#27969;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#26469;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#20154;&#31867;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;GELI&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11330v1 Announce Type: cross  Abstract: We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.11322</link><description>&lt;p&gt;
&#20351;&#29992;StateFlow&#22686;&#24378;LLM&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#36890;&#36807;&#29366;&#24577;&#39537;&#21160;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#36235;&#21183;&#26085;&#30410;&#26126;&#26174;&#65292;&#20363;&#22914;&#38656;&#35201;&#19968;&#31995;&#21015;&#25805;&#20316;&#21644;&#19982;&#24037;&#20855;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;StateFlow&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#27714;&#35299;&#33539;&#24335;&#65292;&#23558;&#30001;LLM&#25903;&#25345;&#30340;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#12290;&#36890;&#36807;&#27491;&#30830;&#26500;&#24314;&#29366;&#24577;&#21644;&#23450;&#20041;&#29366;&#24577;&#36716;&#25442;&#65292;StateFlow&#30830;&#23450;&#20102;&#20219;&#21153;&#27714;&#35299;&#30340;&#36827;&#23637;&#65292;&#30830;&#20445;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;LLM&#22312;&#25972;&#20010;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#30340;&#21709;&#24212;&#12290;&#22312;&#27599;&#20010;&#29366;&#24577;&#20013;&#65292;StateFlow&#20801;&#35768;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#65292;&#19981;&#20165;&#21253;&#25324;&#26681;&#25454;&#29305;&#23450;&#25552;&#31034;&#25351;&#23548;&#29983;&#25104;LLM&#21709;&#24212;&#65292;&#36824;&#21253;&#25324;&#26681;&#25454;&#38656;&#35201;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#12290;&#29366;&#24577;&#36716;&#25442;&#30001;LLM&#20570;&#20986;&#30340;&#29305;&#23450;&#35268;&#21017;&#25110;&#20915;&#31574;&#25511;&#21046;&#65292;&#20801;&#35768;&#36890;&#36807;&#20219;&#21153;&#30340;&#39044;&#23450;&#20041;StateFlow&#27169;&#22411;&#21160;&#24577;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11322v1 Announce Type: cross  Abstract: It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#27604;&#20102;&#22312;&#20351;&#29992;LLMs&#36827;&#34892;&#23569;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26102;&#65292;&#23558;&#35270;&#35273;&#23884;&#20837;&#30452;&#25509;&#36830;&#25509;&#21040;LLM&#23884;&#20837;&#31354;&#38388;&#21644;&#20351;&#29992;&#22270;&#20687;&#25551;&#36848;&#20004;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23545;&#20110;&#29305;&#23450;LLM&#27169;&#22411;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#22270;&#20687;&#25551;&#36848;&#26356;&#22909;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#21017;&#21462;&#20915;&#20110;&#25152;&#36873;&#30340;&#19978;&#19979;&#25991;&#20363;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.11317</link><description>&lt;p&gt;
&#20923;&#32467;LLMs&#30340;&#23569;&#26679;&#26412;VQA&#65306;&#20004;&#31181;&#26041;&#27861;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#27604;&#20102;&#22312;&#20351;&#29992;LLMs&#36827;&#34892;&#23569;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26102;&#65292;&#23558;&#35270;&#35273;&#23884;&#20837;&#30452;&#25509;&#36830;&#25509;&#21040;LLM&#23884;&#20837;&#31354;&#38388;&#21644;&#20351;&#29992;&#22270;&#20687;&#25551;&#36848;&#20004;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23545;&#20110;&#29305;&#23450;LLM&#27169;&#22411;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#22270;&#20687;&#25551;&#36848;&#26356;&#22909;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#21017;&#21462;&#20915;&#20110;&#25152;&#36873;&#30340;&#19978;&#19979;&#25991;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#31181;&#36755;&#20837;&#22270;&#20687;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#12290;&#31532;&#19968;&#31181;&#26159;&#23558;&#22270;&#20687;&#25551;&#36848;&#25104;&#33258;&#28982;&#35821;&#35328;&#12290;&#31532;&#20108;&#31181;&#26159;&#23558;&#22270;&#20687;&#29305;&#24449;&#23884;&#20837;&#26144;&#23556;&#21040;LLMs&#30340;&#39046;&#22495;&#65292;&#24182;&#30452;&#25509;&#23558;&#26144;&#23556;&#21518;&#30340;&#23884;&#20837;&#20256;&#36882;&#32473;LLMs&#12290;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#24037;&#20316;&#20351;&#29992;&#20102;&#37319;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#21464;&#20307;&#30340;&#26550;&#26500;&#26469;&#25253;&#21578;&#24615;&#33021;&#12290;&#20294;&#23427;&#20204;&#24573;&#35270;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#19968;&#20010;&#37325;&#35201;&#27604;&#36739;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21463;&#25511;&#30340;&#12289;&#19987;&#27880;&#30340;&#23454;&#39564;&#26469;&#27604;&#36739;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#20351;&#29992;LLMs&#36827;&#34892;&#23569;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#23545;&#20110;Flan-T5 XL&#65292;&#19968;&#20010;3B&#21442;&#25968;&#30340;LLM&#65292;&#23558;&#35270;&#35273;&#23884;&#20837;&#30452;&#25509;&#36830;&#25509;&#21040;LLM&#23884;&#20837;&#31354;&#38388;&#19981;&#33021;&#20445;&#35777;&#27604;&#20351;&#29992;&#22270;&#20687;&#25551;&#36848;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#25991;&#26412;&#22270;&#20687;&#25551;&#36848;&#26356;&#22909;&#12290;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#25152;&#36873;&#19978;&#19979;&#25991;&#20363;&#23376;&#22914;&#20309;&#36873;&#25321;&#20915;&#23450;&#20102;&#21738;&#31181;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11317v1 Announce Type: new  Abstract: Two approaches have emerged to input images into large language models (LLMs). The first is to caption images into natural language. The second is to map image feature embeddings into the domain of the LLM and pass the mapped embeddings directly to the LLM. The majority of recent few-shot multimodal work reports performance using architectures that employ variations of one of these two approaches. But they overlook an important comparison between them. We design a controlled and focused experiment to compare these two approaches to few-shot visual question answering (VQA) with LLMs. Our findings indicate that for Flan-T5 XL, a 3B parameter LLM, connecting visual embeddings directly to the LLM embedding space does not guarantee improved performance over using image captions. In the zero-shot regime, we find using textual image captions is better. In the few-shot regimes, how the in-context examples are selected determines which is better.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;Transformer&#27169;&#22411;&#20013;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;&#35777;&#26126;&#26469;&#35757;&#32451;&#20004;&#31181;&#27169;&#22411;&#65292;&#25104;&#21151;&#36991;&#20813;&#20102;&#20266;&#30456;&#20851;&#24615;&#21644;&#25512;&#29702;&#25463;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.11314</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#36827;&#34892;&#25512;&#29702;-&#20943;&#23569;&#20266;&#30456;&#20851;&#24615;&#21644;&#25512;&#29702;&#25463;&#24452;
&lt;/p&gt;
&lt;p&gt;
Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11314
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;Transformer&#27169;&#22411;&#20013;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;&#35777;&#26126;&#26469;&#35757;&#32451;&#20004;&#31181;&#27169;&#22411;&#65292;&#25104;&#21151;&#36991;&#20813;&#20102;&#20266;&#30456;&#20851;&#24615;&#21644;&#25512;&#29702;&#25463;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#26159;&#29992;&#20110;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35768;&#22810;&#38656;&#35201;&#36923;&#36753;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#34987;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;Transformer&#27169;&#22411;&#21487;&#33021;&#20250;&#36731;&#26131;&#23398;&#20064;&#21040;&#25968;&#25454;&#20013;&#30340;&#20266;&#27169;&#24335;&#65292;&#20174;&#32780;&#32469;&#36807;&#23454;&#38469;&#25512;&#29702;&#36807;&#31243;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#34987;&#35757;&#32451;&#26469;a) &#36817;&#20284;&#21629;&#39064;&#36923;&#36753;&#25512;&#29702;&#65292;&#21516;&#26102;b) &#36991;&#20813;&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20266;&#30456;&#20851;&#24615;&#24341;&#36215;&#30340;&#24050;&#30693;&#25512;&#29702;&#25463;&#24452;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#30495;&#23454;&#24615;&#19982;&#38382;&#39064;&#20013;&#35268;&#21017;&#25968;&#37327;&#31561;&#20043;&#38388;&#23384;&#22312;&#24050;&#30693;&#30340;&#20266;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#22686;&#24378;&#20102;&#25968;&#25454;&#65292;&#24182;&#35757;&#32451;&#20102;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#29983;&#25104;&#24335;Transformer&#65292;WP-BART&#65292;&#23427;&#22312;&#38382;&#39064;&#21450;&#20854;&#23436;&#25972;&#35777;&#26126;&#19978;&#36827;&#34892;&#35757;&#32451;&#65307;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;SIP-BART&#65292;&#23427;&#22312;&#21333;&#20010;&#35777;&#26126;&#27493;&#39588;&#19978;&#35757;&#32451;&#65292;&#24182;&#23558;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;BART&#19982;&#31526;&#21495;&#25512;&#29702;&#26816;&#26597;&#22120;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#21457;&#29616;SIP-BART&#25104;&#21151;&#22320;&#36991;&#20813;&#20102;&#25512;&#29702;&#25463;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11314v1 Announce Type: cross  Abstract: Transformer language models are neural networks used for a wide variety of tasks concerning natural language, including some that also require logical reasoning. However, a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning. In this paper we investigate to what extent transformers can be trained to a) approximate reasoning in propositional logic while b) avoiding known reasoning shortcuts via spurious correlations in the training data. To do so, we use a dataset with known spurious correlation between truth and e.g. the number of rules in the problem. We augment the data with proofs, and train two models: a generative transformer, WP-BART, trained on problems and their whole proofs, and a neuro-symbolic model, SIP-BART, trained on individual proof steps and combining the generative transformer model BART with a symbolic proof checker. We find that SIP-BART succeeds in avoiding reasoning 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22810;&#27169;&#24577;&#36719;&#25552;&#31034;&#26694;&#26550;MoPE-BAF&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#23398;&#20064;&#19979;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#26816;&#27979;&#21644;&#24773;&#24863;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36719;&#25552;&#31034;&#19987;&#23478;&#21644;&#22359;&#24863;&#30693;&#25552;&#31034;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#21644;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2403.11311</link><description>&lt;p&gt;
&#28151;&#21512;&#25552;&#31034;&#19987;&#23478;&#29992;&#20110;&#22810;&#27169;&#24577;&#35821;&#20041;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11311
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22810;&#27169;&#24577;&#36719;&#25552;&#31034;&#26694;&#26550;MoPE-BAF&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#23398;&#20064;&#19979;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#26816;&#27979;&#21644;&#24773;&#24863;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36719;&#25552;&#31034;&#19987;&#23478;&#21644;&#22359;&#24863;&#30693;&#25552;&#31034;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#21644;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11311v1 &#20844;&#21578;&#31867;&#22411;:&#26032; &#25277;&#35937;:&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#28145;&#24230;&#22810;&#27169;&#24577;&#35821;&#20041;&#29702;&#35299;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#36229;&#36234;&#20102;&#21333;&#32431;&#30340;&#34920;&#38754;&#20869;&#23481;&#20851;&#31995;&#25366;&#25496;&#12290;&#25910;&#38598;&#21644;&#27880;&#37322;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#25361;&#25112;&#20984;&#26174;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;&#36825;&#19968;&#32972;&#26223;&#19979;&#30340;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;: &#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#35773;&#21050;&#26816;&#27979;&#65288;MSD&#65289;&#21644;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65288;MSA&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32479;&#19968;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#19968;&#31181;&#26032;&#22411;&#22810;&#27169;&#24577;&#36719;&#25552;&#31034;&#26694;&#26550;Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion&#65288;MoPE-BAF&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#36719;&#25552;&#31034;&#19987;&#23478;: &#19968;&#20010;&#25991;&#26412;&#25552;&#31034;&#21644;&#19968;&#20010;&#22270;&#20687;&#25552;&#31034;&#65292;&#29992;&#20110;&#25552;&#21462;&#29305;&#23450;&#20110;&#27169;&#24577;&#30340;&#29305;&#24449;&#20197;&#20016;&#23500;&#21333;&#27169;&#24577;&#34920;&#31034;&#65292;&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#25552;&#31034;&#20197;&#21327;&#21161;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;Transformer&#23618;&#37325;&#26032;&#32452;&#32455;&#20026;&#20960;&#20010;&#22359;&#65292;&#24182;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11311v1 Announce Type: new  Abstract: Deep multimodal semantic understanding that goes beyond the mere superficial content relation mining has received increasing attention in the realm of artificial intelligence. The challenges of collecting and annotating high-quality multi-modal data have underscored the significance of few-shot learning. In this paper, we focus on two critical tasks under this context: few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis (MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on the unified vision-language model (VLM). Specifically, we design three experts of soft prompts: a text prompt and an image prompt that extract modality-specific features to enrich the single-modal representation, and a unified prompt to assist multi-modal interaction. Additionally, we reorganize Transformer layers into several blocks and intr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11299</link><description>&lt;p&gt;
SQ-LLaVA&#65306;&#33258;&#38382;&#33258;&#31572;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#22312;&#32463;&#36807;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#21518;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26174;&#30528;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#40511;&#27807;&#25104;&#20026;&#25972;&#20010;&#32593;&#32476;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#25913;&#21892;&#36328;&#27169;&#24577;&#23545;&#40784;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#32771;&#34385;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#35270;&#35273;&#20219;&#21153;&#33539;&#22260;&#30340;&#26356;&#22810;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#65292;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#29992;&#20110;&#38382;&#31572;&#65292;&#20294;&#36825;&#31181;&#25805;&#20316;&#25104;&#26412;&#36739;&#39640;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#21253;&#21547;&#22823;&#37327;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20294;&#36825;&#19968;&#26041;&#38754;&#19968;&#30452;&#40092;&#26377;&#20154;&#25506;&#32034;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#21033;&#29992;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20869;&#37096;&#34987;&#24573;&#35270;&#30340;&#19978;&#19979;&#25991;&#65292;&#35757;&#32451;&#27169;&#22411;&#33258;&#25105;&#35757;&#32451;'&#23398;&#20064;'&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;&#33258;&#38382;&#33258;&#31572;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#21161;&#25163;&#12290;SQ-LLaVA&#22312;&#29983;&#25104;&#28789;&#27963;&#19988;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11299v1 Announce Type: cross  Abstract: Recent advancements in the vision-language model have shown notable generalization in vision-language tasks after visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which are costly to obtain. However, the image contains rich contextual information that has been largely under-explored. This paper first attempts to harness this overlooked context within visual instruction data, training the model to self-supervised `learning' how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#22522;&#20110;&#35789;&#26174;&#33879;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#35789;&#36827;&#34892;&#20462;&#25913;&#65292;&#26088;&#22312;&#27450;&#39575;&#20998;&#31867;&#27169;&#22411;&#19988;&#20445;&#25345;&#35821;&#20041;&#36830;&#36143;&#24615;&#65292;&#22312;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#30340;&#21516;&#26102;&#36991;&#24320;&#20102;&#20998;&#31867;&#31995;&#32479;&#30340;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.11297</link><description>&lt;p&gt;
&#23545;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#19968;&#31181;&#20462;&#25913;&#30340;&#22522;&#20110;&#35789;&#26174;&#33879;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Modified Word Saliency-Based Adversarial Attack on Text Classification Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#22522;&#20110;&#35789;&#26174;&#33879;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#35789;&#36827;&#34892;&#20462;&#25913;&#65292;&#26088;&#22312;&#27450;&#39575;&#20998;&#31867;&#27169;&#22411;&#19988;&#20445;&#25345;&#35821;&#20041;&#36830;&#36143;&#24615;&#65292;&#22312;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#30340;&#21516;&#26102;&#36991;&#24320;&#20102;&#20998;&#31867;&#31995;&#32479;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#31216;&#20026;&#20462;&#25913;&#30340;&#22522;&#20110;&#35789;&#26174;&#33879;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#65288;MWSAA&#65289;&#12290;&#35813;&#25216;&#26415;&#22522;&#20110;&#35789;&#26174;&#33879;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#25112;&#30053;&#24615;&#22320;&#25200;&#20081;&#36755;&#20837;&#25991;&#26412;&#65292;&#26088;&#22312;&#35823;&#23548;&#20998;&#31867;&#27169;&#22411;&#21516;&#26102;&#20445;&#25345;&#35821;&#20041;&#36830;&#36143;&#24615;&#12290;&#36890;&#36807;&#25913;&#36827;&#20256;&#32479;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;MWSAA&#26174;&#33879;&#22686;&#24378;&#20102;&#20854;&#35268;&#36991;&#20998;&#31867;&#31995;&#32479;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#26174;&#33879;&#24615;&#20272;&#35745;&#36807;&#31243;&#35782;&#21035;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#26174;&#33879;&#35789;&#65292;&#36825;&#20123;&#35789;&#23545;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20855;&#26377;&#26368;&#22823;&#24433;&#21709;&#12290;&#38543;&#21518;&#65292;&#36825;&#20123;&#26174;&#33879;&#35789;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#20462;&#25913;&#65292;&#24341;&#23548;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#30830;&#20445;&#25913;&#21464;&#30340;&#25991;&#26412;&#20445;&#25345;&#36830;&#36143;&#24182;&#20445;&#30041;&#20854;&#21407;&#22987;&#21547;&#20041;&#12290;&#22312;&#19981;&#21516;&#30340;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11297v1 Announce Type: new  Abstract: This paper introduces a novel adversarial attack method targeting text classification models, termed the Modified Word Saliency-based Adversarial At-tack (MWSAA). The technique builds upon the concept of word saliency to strategically perturb input texts, aiming to mislead classification models while preserving semantic coherence. By refining the traditional adversarial attack approach, MWSAA significantly enhances its efficacy in evading detection by classification systems. The methodology involves first identifying salient words in the input text through a saliency estimation process, which prioritizes words most influential to the model's decision-making process. Subsequently, these salient words are subjected to carefully crafted modifications, guided by semantic similarity metrics to ensure that the altered text remains coherent and retains its original meaning. Empirical evaluations conducted on diverse text classification datasets
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#31034;&#20363;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;&#20316;&#32773;&#39564;&#35777;&#20219;&#21153;&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.11265</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#25913;&#21892;&#20316;&#32773;&#39564;&#35777;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11265
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#31034;&#20363;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;&#20316;&#32773;&#39564;&#35777;&#20219;&#21153;&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#39564;&#35777;&#65288;AV&#65289;&#26159;&#19968;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#20851;&#27880;&#30340;&#26159;&#25512;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#30001;&#19968;&#20010;&#29305;&#23450;&#20316;&#32773;&#25776;&#20889;&#36824;&#26159;&#30001;&#20854;&#20182;&#20154;&#25776;&#20889;&#12290;&#24050;&#32463;&#26174;&#31034;&#35768;&#22810;AV&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#24694;&#24847;&#20316;&#32773;&#31215;&#26497;&#23581;&#35797;&#27450;&#39575;&#20998;&#31867;&#22120;&#65292;&#26041;&#27861;&#26159;&#38544;&#34255;&#20182;&#20204;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#25110;&#32773;&#27169;&#20223;&#21478;&#19968;&#20301;&#20316;&#32773;&#30340;&#39118;&#26684;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#20998;&#31867;&#22120;&#35757;&#32451;&#38598;&#19982;&#65288;&#36127;&#38754;&#30340;&#65289;&#21512;&#25104;&#31034;&#20363;&#36827;&#34892;&#22686;&#24378;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#36825;&#20123;&#21512;&#25104;&#31034;&#20363;&#26159;&#20026;&#20102;&#27169;&#20223;&#24863;&#20852;&#36259;&#30340;&#20316;&#32773;&#30340;&#39118;&#26684;&#32780;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#22686;&#24378;&#23545;&#22312;&#25932;&#23545;&#29615;&#22659;&#19979;&#30340;AV&#20219;&#21153;&#20013;&#24102;&#26469;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#25913;&#36827;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#29983;&#25104;&#22120;&#26550;&#26500;&#65288;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#23567;&#35268;&#27169;transformers&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#27969;&#34892;&#30340;GPT&#27169;&#22411;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11265v1 Announce Type: cross  Abstract: Authorship Verification (AV) is a text classification task concerned with inferring whether a candidate text has been written by one specific author or by someone else. It has been shown that many AV systems are vulnerable to adversarial attacks, where a malicious author actively tries to fool the classifier by either concealing their writing style, or by imitating the style of another author. In this paper, we investigate the potential benefits of augmenting the classifier training set with (negative) synthetic examples. These synthetic examples are generated to imitate the style of the author of interest. We analyze the improvements in classifier prediction that this augmentation brings to bear in the task of AV in an adversarial setting. In particular, we experiment with three different generator architectures (one based on Recurrent Neural Networks, another based on small-scale transformers, and another based on the popular GPT mod
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ChartThinker&#30340;&#21019;&#26032;&#22270;&#34920;&#25688;&#35201;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24605;&#32500;&#21644;&#31574;&#30053;&#24615;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#23454;&#29616;&#28145;&#24230;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#39640;&#29983;&#25104;&#25688;&#35201;&#30340;&#36923;&#36753;&#36830;&#36143;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11236</link><description>&lt;p&gt;
ChartThinker&#65306;&#19968;&#31181;&#20248;&#21270;&#22270;&#34920;&#25688;&#35201;&#30340;&#19978;&#19979;&#25991;&#24605;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11236
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ChartThinker&#30340;&#21019;&#26032;&#22270;&#34920;&#25688;&#35201;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24605;&#32500;&#21644;&#31574;&#30053;&#24615;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#23454;&#29616;&#28145;&#24230;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#39640;&#29983;&#25104;&#25688;&#35201;&#30340;&#36923;&#36753;&#36830;&#36143;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.11236v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#22411; &#25688;&#35201;&#65306;&#25968;&#25454;&#21487;&#35270;&#21270;&#26159;&#21576;&#29616;&#25968;&#25454;&#21644;&#25366;&#25496;&#20854;&#23453;&#36149;&#35265;&#35299;&#30340;&#37325;&#35201;&#25163;&#27573;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#36827;&#34892;&#22270;&#34920;&#25688;&#35201;&#30340;&#20219;&#21153;&#26377;&#21161;&#20110;&#28145;&#20837;&#20998;&#26512;&#22270;&#34920;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#35270;&#35273;-&#35821;&#35328;&#21305;&#37197;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26126;&#26174;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20840;&#38754;&#30340;&#22270;&#34920;&#26631;&#39064;&#37197;&#23545;&#21644;&#27599;&#20010;&#22270;&#34920;&#30340;&#24494;&#35843;&#35828;&#26126;&#12290;&#30001;&#20110;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#21508;&#31181;&#20027;&#39064;&#21644;&#35270;&#35273;&#39118;&#26684;&#65292;&#21487;&#20197;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#35282;&#24230;&#33719;&#24471;&#26356;&#22909;&#30340;&#21305;&#37197;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#34920;&#25688;&#35201;&#26041;&#27861;ChartThinker&#65292;&#23427;&#22522;&#20110;&#24605;&#32500;&#38142;&#21644;&#19978;&#19979;&#25991;&#26816;&#32034;&#31574;&#30053;&#32508;&#21512;&#28145;&#24230;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#39640;&#29983;&#25104;&#25688;&#35201;&#30340;&#36923;&#36753;&#36830;&#36143;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#24314;&#31435;&#22312;&#31934;&#24515;&#31574;&#21010;Data
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11236v1 Announce Type: new  Abstract: Data visualization serves as a critical means for presenting data and mining its valuable insights. The task of chart summarization, through natural language processing techniques, facilitates in-depth data analysis of charts. However, there still are notable deficiencies in terms of visual-language matching and reasoning ability for existing approaches. To address these limitations, this study constructs a large-scale dataset of comprehensive chart-caption pairs and fine-tuning instructions on each chart. Thanks to the broad coverage of various topics and visual styles within this dataset, better matching degree can be achieved from the view of training data. Moreover, we propose an innovative chart summarization method, ChartThinker, which synthesizes deep analysis based on chains of thought and strategies of context retrieval, aiming to improve the logical coherence and accuracy of the generated summaries. Built upon the curated datas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20020;&#24202;&#26631;&#35760;&#30340;&#24265;&#20215;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#21462;&#20142;&#28857;&#21644;&#29983;&#25104;&#25688;&#35201;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.11227</link><description>&lt;p&gt;
&#20174;&#25991;&#23383;&#20013;&#25552;&#21462;&#20020;&#24202;&#26631;&#35760;&#30340;&#24265;&#20215;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cheap Ways of Extracting Clinical Markers from Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11227
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20020;&#24202;&#26631;&#35760;&#30340;&#24265;&#20215;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#21462;&#20142;&#28857;&#21644;&#29983;&#25104;&#25688;&#35201;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;UniBuc&#32771;&#21476;&#22242;&#38431;&#20026;CLPsych 2024&#20849;&#20139;&#20219;&#21153;&#25152;&#36827;&#34892;&#30340;&#24037;&#20316;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#22312;&#25991;&#26412;&#20013;&#23547;&#25214;&#25903;&#25345;&#20998;&#37197;&#30340;&#33258;&#26432;&#39118;&#38505;&#27700;&#24179;&#30340;&#35777;&#25454;&#12290;&#38656;&#35201;&#20004;&#31181;&#31867;&#22411;&#30340;&#35777;&#25454;&#65306;&#20142;&#28857;&#65288;&#25552;&#21462;&#25991;&#26412;&#20013;&#30340;&#30456;&#20851;&#29255;&#27573;&#65289;&#21644;&#25688;&#35201;&#65288;&#23558;&#35777;&#25454;&#32858;&#21512;&#25104;&#32508;&#21512;&#65289;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#32780;&#19981;&#26159;&#19968;&#31181;&#26356;&#33410;&#30465;&#20869;&#23384;&#21644;&#36164;&#28304;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#37319;&#29992;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;GOML&#65289;&#31649;&#36947;&#65292;&#21253;&#25324;tf-idf&#21521;&#37327;&#21270;&#22120;&#21644;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#65292;&#20854;&#20195;&#34920;&#24615;&#29305;&#24449;&#29992;&#20110;&#25552;&#21462;&#30456;&#20851;&#30340;&#20142;&#28857;&#12290;&#31532;&#20108;&#31181;&#26356;&#28040;&#32791;&#36164;&#28304;&#30340;&#26041;&#27861;&#20351;&#29992;LLM&#29983;&#25104;&#25688;&#35201;&#65292;&#24182;&#30001;&#24605;&#32500;&#38142;&#25351;&#23548;&#25552;&#20379;&#25351;&#31034;&#20020;&#24202;&#26631;&#35760;&#30340;&#25991;&#26412;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11227v1 Announce Type: new  Abstract: This paper describes the work of the UniBuc Archaeology team for CLPsych's 2024 Shared Task, which involved finding evidence within the text supporting the assigned suicide risk level. Two types of evidence were required: highlights (extracting relevant spans within the text) and summaries (aggregating evidence into a synthesis). Our work focuses on evaluating Large Language Models (LLM) as opposed to an alternative method that is much more memory and resource efficient. The first approach employs a good old-fashioned machine learning (GOML) pipeline consisting of a tf-idf vectorizer with a logistic regression classifier, whose representative features are used to extract relevant highlights. The second, more resource intensive, uses an LLM for generating the summaries and is guided by chain-of-thought to provide sequences of text indicating clinical markers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24320;&#21457;&#19968;&#31181;&#21548;&#36215;&#26469;&#20687;&#21463;&#36807;&#25945;&#32946;&#12289;&#19987;&#19994;&#12289;&#22320;&#21306;&#21475;&#38899;&#30495;&#23454;&#30340;&#38750;&#27954;&#35028;&#32654;&#22269;&#22899;&#24615;&#30340;&#32654;&#24335;&#33521;&#25991;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#31995;&#32479;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#31181;&#26063;&#34920;&#29616;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#25351;&#23548;&#26041;&#38024;&#12289;&#25216;&#26415;&#38590;&#39064;&#21644;&#23545;&#35813;&#31995;&#32479;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.11209</link><description>&lt;p&gt;
&#21019;&#36896;&#19968;&#31181;&#21548;&#36215;&#26469;&#20687;&#38750;&#27954;&#35028;&#32654;&#22269;&#20154;&#30340;TTS&#65306;&#25351;&#23548;&#26041;&#38024;&#12289;&#25216;&#26415;&#25361;&#25112;&#21644;&#20196;&#20154;&#24778;&#35766;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Creating an African American-Sounding TTS: Guidelines, Technical Challenges,and Surprising Evaluations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24320;&#21457;&#19968;&#31181;&#21548;&#36215;&#26469;&#20687;&#21463;&#36807;&#25945;&#32946;&#12289;&#19987;&#19994;&#12289;&#22320;&#21306;&#21475;&#38899;&#30495;&#23454;&#30340;&#38750;&#27954;&#35028;&#32654;&#22269;&#22899;&#24615;&#30340;&#32654;&#24335;&#33521;&#25991;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#31995;&#32479;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#31181;&#26063;&#34920;&#29616;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#25351;&#23548;&#26041;&#38024;&#12289;&#25216;&#26415;&#38590;&#39064;&#21644;&#23545;&#35813;&#31995;&#32479;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#30028;&#38754;&#21644;&#26426;&#22120;&#20154;&#20013;&#30340;AI&#20195;&#29702;&#30340;&#34920;&#29616;&#20027;&#35201;&#26159;&#30333;&#20154;&#65292;&#19981;&#20165;&#22312;&#38754;&#37096;&#21644;&#30382;&#32932;&#29305;&#24449;&#19978;&#65292;&#32780;&#19988;&#22312;&#20182;&#20204;&#20351;&#29992;&#30340;&#21512;&#25104;&#22768;&#38899;&#19978;&#20063;&#22914;&#27492;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#24320;&#21457;&#19968;&#20010;&#26088;&#22312;&#21548;&#36215;&#26469;&#20687;&#21463;&#36807;&#25945;&#32946;&#12289;&#19987;&#19994;&#12289;&#22320;&#21306;&#21475;&#38899;&#30495;&#23454;&#30340;&#38750;&#27954;&#35028;&#32654;&#22269;&#22899;&#24615;&#30340;&#32654;&#24335;&#33521;&#25991;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#31995;&#32479;&#36807;&#31243;&#20013;&#21457;&#29616;&#30340;&#26377;&#20851;&#31181;&#26063;&#34920;&#29616;&#30340;&#19968;&#20123;&#24847;&#24819;&#19981;&#21040;&#30340;&#25361;&#25112;&#12290;&#35770;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#19982;&#38750;&#27954;&#35028;&#32654;&#22269;IT&#19987;&#19994;&#20154;&#21592;&#36827;&#34892;&#30340;&#28966;&#28857;&#23567;&#32452;&#35752;&#35770;&#30340;&#32467;&#26524;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#35752;&#35770;&#21644;&#25910;&#38598;&#20102;&#29992;&#20110;&#21019;&#24314;&#20195;&#34920;&#24615;&#21644;&#21512;&#36866;&#30340;TTS&#31995;&#32479;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#25361;&#25112;&#65292;&#25509;&#30528;&#35752;&#35770;&#20102;TTS&#31995;&#32479;&#24320;&#21457;&#20154;&#21592;&#38754;&#20020;&#30340;&#19968;&#20123;&#25216;&#26415;&#22256;&#38590;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20004;&#39033;&#19982;&#32654;&#24335;&#33521;&#35821;&#20351;&#29992;&#32773;&#36827;&#34892;&#30340;&#30740;&#31350;&#65292;&#21442;&#19982;&#32773;&#26080;&#27861;&#23558;&#38750;&#27954;&#35028;&#32654;&#22269;TTS&#22768;&#38899;&#27491;&#30830;&#24402;&#22240;&#20026;&#27491;&#30830;&#30340;&#31181;&#26063;&#65292;&#32780;&#20182;&#20204;&#22312;&#35748;&#20986;&#35813;&#22768;&#38899;&#26102;&#20960;&#20046;&#20840;&#37117;&#27491;&#30830;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11209v1 Announce Type: new  Abstract: Representations of AI agents in user interfaces and robotics are predominantly White, not only in terms of facial and skin features, but also in the synthetic voices they use. In this paper we explore some unexpected challenges in the representation of race we found in the process of developing an U.S. English Text-to-Speech (TTS) system aimed to sound like an educated, professional, regional accent-free African American woman. The paper starts by presenting the results of focus groups with African American IT professionals where guidelines and challenges for the creation of a representative and appropriate TTS system were discussed and gathered, followed by a discussion about some of the technical difficulties faced by the TTS system developers. We then describe two studies with U.S. English speakers where the participants were not able to attribute the correct race to the African American TTS voice while overwhelmingly correctly recogn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;TRELM&#65292;&#19968;&#31181;&#38754;&#21521;&#30693;&#35782;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#39640;&#25928;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;&#31283;&#20581;&#26041;&#27861;&#27880;&#20837;&#30693;&#35782;&#19977;&#20803;&#32452;&#21644;&#30693;&#35782;&#22686;&#24378;&#30340;&#23384;&#20648;&#24211;&#26469;&#35299;&#20915;&#23454;&#20307;&#34920;&#31034;&#20248;&#21270;&#19981;&#36275;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11203</link><description>&lt;p&gt;
TRELM: &#38754;&#21521;&#30693;&#35782;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#39640;&#25928;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TRELM: Towards Robust and Efficient Pre-training for Knowledge-Enhanced Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TRELM&#65292;&#19968;&#31181;&#38754;&#21521;&#30693;&#35782;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#39640;&#25928;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;&#31283;&#20581;&#26041;&#27861;&#27880;&#20837;&#30693;&#35782;&#19977;&#20803;&#32452;&#21644;&#30693;&#35782;&#22686;&#24378;&#30340;&#23384;&#20648;&#24211;&#26469;&#35299;&#20915;&#23454;&#20307;&#34920;&#31034;&#20248;&#21270;&#19981;&#36275;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
KEPLM&#26159;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#23398;&#20064;&#20851;&#31995;&#19977;&#20803;&#32452;&#26469;&#20419;&#36827;&#30693;&#35782;&#33719;&#21462;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24182;&#26410;&#20248;&#20808;&#23398;&#20064;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#20196;&#29260;&#30340;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#26356;&#26032;KEPLM&#20013;&#30340;&#20840;&#37096;&#21442;&#25968;&#22312;&#35745;&#31639;&#19978;&#26159;&#36153;&#26102;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TRELM&#65292;&#19968;&#31181;&#38754;&#21521;&#30693;&#35782;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#39640;&#25928;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#23454;&#20307;&#36890;&#24120;&#36981;&#24490;&#38271;&#23614;&#20998;&#24067;&#65292;&#19968;&#20123;&#23454;&#20307;&#30340;&#34920;&#31034;&#26410;&#32463;&#20248;&#21270;&#65292;&#22952;&#30861;&#20102;KEPLM&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31283;&#20581;&#26041;&#27861;&#26469;&#27880;&#20837;&#30693;&#35782;&#19977;&#20803;&#32452;&#65292;&#24182;&#37319;&#29992;&#20102;&#30693;&#35782;&#22686;&#24378;&#30340;&#23384;&#20648;&#24211;&#26469;&#25429;&#33719;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#21482;&#26356;&#26032;&#23567;&#37096;&#20998;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11203v1 Announce Type: new  Abstract: KEPLMs are pre-trained models that utilize external knowledge to enhance language understanding. Previous language models facilitated knowledge acquisition by incorporating knowledge-related pre-training tasks learned from relation triples in knowledge graphs. However, these models do not prioritize learning embeddings for entity-related tokens. Moreover, updating the entire set of parameters in KEPLMs is computationally demanding. This paper introduces TRELM, a Robust and Efficient Pre-training framework for Knowledge-Enhanced Language Models. We observe that entities in text corpora usually follow the long-tail distribution, where the representations of some entities are suboptimally optimized and hinder the pre-training process for KEPLMs. To tackle this, we employ a robust approach to inject knowledge triples and employ a knowledge-augmented memory bank to capture valuable information. Furthermore, updating a small subset of neurons 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#21333;&#27425;&#38750;&#20405;&#20837;&#24615;&#33041;&#35760;&#24405;&#20013;&#35299;&#30721;&#36830;&#32493;&#35821;&#35328;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#32500;&#21367;&#31215;&#32593;&#32476;&#21644;&#20449;&#24687;&#29942;&#39048;&#32467;&#21512;&#23383;&#31526;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616;&#22312;&#21644;&#36328;&#20027;&#20307;&#20043;&#38388;&#20135;&#29983;&#25429;&#25417;&#24863;&#30693;&#35821;&#38899;&#21547;&#20041;&#30340;&#21487;&#29702;&#35299;&#25991;&#26412;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.11183</link><description>&lt;p&gt;
&#20174;&#38750;&#20405;&#20837;&#24615;&#33041;&#35760;&#24405;&#35299;&#30721;&#36830;&#32493;&#22522;&#20110;&#23383;&#31526;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Decoding Continuous Character-based Language from Non-invasive Brain Recordings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11183
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#21333;&#27425;&#38750;&#20405;&#20837;&#24615;&#33041;&#35760;&#24405;&#20013;&#35299;&#30721;&#36830;&#32493;&#35821;&#35328;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#32500;&#21367;&#31215;&#32593;&#32476;&#21644;&#20449;&#24687;&#29942;&#39048;&#32467;&#21512;&#23383;&#31526;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616;&#22312;&#21644;&#36328;&#20027;&#20307;&#20043;&#38388;&#20135;&#29983;&#25429;&#25417;&#24863;&#30693;&#35821;&#38899;&#21547;&#20041;&#30340;&#21487;&#29702;&#35299;&#25991;&#26412;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#35774;&#22791;&#20174;&#22823;&#33041;&#27963;&#21160;&#20013;&#35299;&#35835;&#33258;&#28982;&#35821;&#35328;&#20173;&#28982;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#20043;&#21069;&#30340;&#38750;&#20405;&#20837;&#24335;&#35299;&#30721;&#22120;&#35201;&#20040;&#38656;&#35201;&#36827;&#34892;&#22810;&#27425;&#23454;&#39564;&#26469;&#30830;&#23450;&#30382;&#23618;&#21306;&#22495;&#24182;&#22686;&#24378;&#22823;&#33041;&#27963;&#21160;&#30340;&#20449;&#22122;&#27604;&#65292;&#35201;&#20040;&#20165;&#38480;&#20110;&#35782;&#21035;&#22522;&#26412;&#30340;&#35821;&#35328;&#20803;&#32032;&#22914;&#23383;&#27597;&#21644;&#21333;&#35789;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#21333;&#27425;&#38750;&#20405;&#20837;&#24615;fMRI&#35760;&#24405;&#20013;&#35299;&#30721;&#36830;&#32493;&#35821;&#35328;&#65292;&#20854;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#19977;&#32500;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#37197;&#22791;&#20449;&#24687;&#29942;&#39048;&#20197;&#33258;&#21160;&#35782;&#21035;&#23545;&#21050;&#28608;&#26377;&#21709;&#24212;&#30340;&#20307;&#32032;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#23383;&#31526;&#30340;&#35299;&#30721;&#22120;, &#29992;&#20110;&#23545;&#20869;&#22312;&#23383;&#31526;&#32467;&#26500;&#25152;&#29305;&#24449;&#21270;&#30340;&#36830;&#32493;&#35821;&#35328;&#36827;&#34892;&#35821;&#20041;&#37325;&#24314;&#12290;&#25152;&#24471;&#35299;&#30721;&#22120;&#33021;&#22815;&#29983;&#25104;&#21487;&#29702;&#35299;&#30340;&#25991;&#26412;&#24207;&#21015;&#65292;&#24544;&#23454;&#22320;&#25429;&#25417;&#24863;&#30693;&#35821;&#38899;&#30340;&#21547;&#20041;&#65292;&#26080;&#35770;&#26159;&#22312;&#21516;&#19968;&#20027;&#20307;&#20869;&#36824;&#26159;&#36328;&#20027;&#20307;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11183v1 Announce Type: new  Abstract: Deciphering natural language from brain activity through non-invasive devices remains a formidable challenge. Previous non-invasive decoders either require multiple experiments with identical stimuli to pinpoint cortical regions and enhance signal-to-noise ratios in brain activity, or they are limited to discerning basic linguistic elements such as letters and words. We propose a novel approach to decoding continuous language from single-trial non-invasive fMRI recordings, in which a three-dimensional convolutional network augmented with information bottleneck is developed to automatically identify responsive voxels to stimuli, and a character-based decoder is designed for the semantic reconstruction of continuous language characterized by inherent character structures. The resulting decoder can produce intelligible textual sequences that faithfully capture the meaning of perceived speech both within and across subjects, while existing d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;QualiCLIP&#65292;&#36890;&#36807;&#36136;&#37327;&#24863;&#30693;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#19981;&#38656;&#35201;&#26631;&#35760;MOS&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.11176</link><description>&lt;p&gt;
&#38754;&#21521;&#29616;&#23454;&#19990;&#30028;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#30340;&#36136;&#37327;&#24863;&#30693;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Quality-Aware Image-Text Alignment for Real-World Image Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11176
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;QualiCLIP&#65292;&#36890;&#36807;&#36136;&#37327;&#24863;&#30693;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#19981;&#38656;&#35201;&#26631;&#35760;MOS&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#65288;NR-IQA&#65289;&#33268;&#21147;&#20110;&#35774;&#35745;&#19968;&#31181;&#22312;&#27809;&#26377;&#39640;&#36136;&#37327;&#21442;&#32771;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#27979;&#37327;&#22270;&#20687;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#31526;&#21512;&#20154;&#31867;&#24863;&#30693;&#65292;&#22823;&#37096;&#20998;&#26368;&#20808;&#36827;&#30340;NR-IQA&#26041;&#27861;&#20013;&#20381;&#36182;&#26631;&#27880;&#30340;&#20027;&#35266;&#35780;&#20998;&#65288;MOS&#65289;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;QualiCLIP&#65288;Quality-aware CLIP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#33258;&#30417;&#30563;&#19981;&#38656;&#35201;&#26631;&#35760;MOS&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#31574;&#30053;&#65292;&#20351;&#24471;CLIP&#29983;&#25104;&#30340;&#34920;&#31034;&#19982;&#22270;&#20687;&#22266;&#26377;&#36136;&#37327;&#30456;&#20851;&#12290;&#20174;&#21407;&#22987;&#22270;&#20687;&#24320;&#22987;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#26029;&#22686;&#21152;&#30340;&#24378;&#24230;&#21512;&#25104;&#22320;&#21155;&#21270;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;CLIP&#26681;&#25454;&#20854;&#19982;&#36136;&#37327;&#30456;&#20851;&#30340;&#21453;&#20041;&#25991;&#26412;&#25552;&#31034;&#30340;&#30456;&#20284;&#24615;&#23545;&#36825;&#20123;&#38477;&#35299;&#22270;&#20687;&#36827;&#34892;&#25490;&#21517;&#65292;&#21516;&#26102;&#20445;&#35777;&#19968;&#33268;&#30340;&#34920;&#36798;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11176v1 Announce Type: cross  Abstract: No-Reference Image Quality Assessment (NR-IQA) focuses on designing methods to measure image quality in alignment with human perception when a high-quality reference image is unavailable. The reliance on annotated Mean Opinion Scores (MOS) in the majority of state-of-the-art NR-IQA approaches limits their scalability and broader applicability to real-world scenarios. To overcome this limitation, we propose QualiCLIP (Quality-aware CLIP), a CLIP-based self-supervised opinion-unaware method that does not require labeled MOS. In particular, we introduce a quality-aware image-text alignment strategy to make CLIP generate representations that correlate with the inherent quality of the images. Starting from pristine images, we synthetically degrade them with increasing levels of intensity. Then, we train CLIP to rank these degraded images based on their similarity to quality-related antonym text prompts, while guaranteeing consistent represe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUSE&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35775;&#38382;&#26368;&#26032;&#20449;&#24687;&#24182;&#35780;&#20272;&#21487;&#20449;&#24230;&#65292;&#20197;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#35823;&#20449;&#24687;&#32416;&#27491;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11169</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32416;&#27491;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Correcting misinformation on social media with a large language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11169
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUSE&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35775;&#38382;&#26368;&#26032;&#20449;&#24687;&#24182;&#35780;&#20272;&#21487;&#20449;&#24230;&#65292;&#20197;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#35823;&#20449;&#24687;&#32416;&#27491;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#20449;&#24687;&#20250;&#30772;&#22351;&#20844;&#20247;&#23545;&#31185;&#23398;&#21644;&#27665;&#20027;&#30340;&#20449;&#20219;&#65292;&#29305;&#21035;&#26159;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#19981;&#20934;&#30830;&#20449;&#24687;&#20250;&#36805;&#36895;&#20256;&#25773;&#12290;&#19987;&#23478;&#21644;&#26222;&#36890;&#20154;&#36890;&#36807;&#25163;&#21160;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#20934;&#30830;&#20449;&#24687;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#32416;&#27491;&#35823;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#25193;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#25285;&#24551;&#65292;&#22240;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31561;&#25216;&#26415;&#20351;&#35823;&#20449;&#24687;&#26356;&#23481;&#26131;&#29983;&#25104;&#12290;LLMs&#36824;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#65292;&#21487;&#20197;&#21152;&#36895;&#32416;&#27491;&#35823;&#20449;&#24687;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30001;&#20110;&#32570;&#20047;&#26368;&#26032;&#20449;&#24687;&#12289;&#20542;&#21521;&#20110;&#29983;&#25104;&#20284;&#26159;&#32780;&#38750;&#30340;&#20869;&#23481;&#21644;&#24341;&#29992;&#20197;&#21450;&#26080;&#27861;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#32780;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MUSE&#65292;&#36825;&#26159;&#19968;&#20010;&#24102;&#26377;&#26368;&#26032;&#20449;&#24687;&#35775;&#38382;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#30340;LLM&#12290;&#36890;&#36807;&#26816;&#32034;&#19978;&#19979;&#25991;&#35777;&#25454;&#21644;&#21453;&#39539;&#65292;MUSE&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#21487;&#20449;&#30340;&#35299;&#37322;&#21644;&#21442;&#32771;&#12290;&#23427;&#36824;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11169v1 Announce Type: cross  Abstract: Misinformation undermines public trust in science and democracy, particularly on social media where inaccuracies can spread rapidly. Experts and laypeople have shown to be effective in correcting misinformation by manually identifying and explaining inaccuracies. Nevertheless, this approach is difficult to scale, a concern as technologies like large language models (LLMs) make misinformation easier to produce. LLMs also have versatile capabilities that could accelerate misinformation correction; however, they struggle due to a lack of recent information, a tendency to produce plausible but false content and references, and limitations in addressing multimodal information. To address these issues, we propose MUSE, an LLM augmented with access to and credibility evaluation of up-to-date information. By retrieving contextual evidence and refutations, MUSE can provide accurate and trustworthy explanations and references. It also describes 
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20262;&#29702;&#23618;&#38754;&#30340;&#37325;&#35201;&#24615;&#20197;&#30830;&#20445;&#20854;&#26377;&#25928;&#25972;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27861;&#24459;&#26696;&#20363;&#30340;&#26032;&#39062;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#35821;&#35328;&#33021;&#21147;&#12289;&#19987;&#19994;&#27861;&#24459;&#30693;&#35782;&#21644;&#27861;&#24459;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.11152</link><description>&lt;p&gt;
&#27861;&#24459;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#35780;&#20272;&#20262;&#29702;
&lt;/p&gt;
&lt;p&gt;
Evaluation Ethics of LLMs in Legal Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11152
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20262;&#29702;&#23618;&#38754;&#30340;&#37325;&#35201;&#24615;&#20197;&#30830;&#20445;&#20854;&#26377;&#25928;&#25972;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27861;&#24459;&#26696;&#20363;&#30340;&#26032;&#39062;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#35821;&#35328;&#33021;&#21147;&#12289;&#19987;&#19994;&#27861;&#24459;&#30693;&#35782;&#21644;&#27861;&#24459;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#23548;&#33268;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#24212;&#23545;&#35832;&#22914;&#27861;&#24459;&#31561;&#19987;&#19994;&#39046;&#22495;&#29305;&#23450;&#25361;&#25112;&#26041;&#38754;&#30340;&#36890;&#29992;&#33021;&#21147;&#20173;&#21463;&#21040;&#36136;&#30097;&#12290;&#30740;&#31350;&#20154;&#21592;&#24573;&#35270;&#20102;&#23558;&#27861;&#24459;&#20262;&#29702;&#32435;&#20837;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#20027;&#24352;&#20005;&#26684;&#30340;&#20262;&#29702;&#35780;&#20272;&#23545;&#20110;&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#26377;&#25928;&#25972;&#21512;&#33267;&#20851;&#37325;&#35201;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#21644;&#39046;&#22495;&#29305;&#23450;&#20262;&#29702;&#30340;&#24517;&#35201;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#30495;&#23454;&#27861;&#24459;&#26696;&#20363;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#22522;&#26412;&#35821;&#35328;&#33021;&#21147;&#12289;&#19987;&#19994;&#27861;&#24459;&#30693;&#35782;&#21644;&#27861;&#24459;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35780;&#20272;&#32467;&#26524;&#23545;&#23398;&#26415;&#30028;&#22260;&#32469;&#27861;&#24459;&#39046;&#22495;&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11152v1 Announce Type: cross  Abstract: In recent years, the utilization of large language models for natural language dialogue has gained momentum, leading to their widespread adoption across various domains. However, their universal competence in addressing challenges specific to specialized fields such as law remains a subject of scrutiny. The incorporation of legal ethics into the model has been overlooked by researchers. We asserts that rigorous ethic evaluation is essential to ensure the effective integration of large language models in legal domains, emphasizing the need to assess domain-specific proficiency and domain-specific ethic. To address this, we propose a novelty evaluation methodology, utilizing authentic legal cases to evaluate the fundamental language abilities, specialized legal knowledge and legal robustness of large language models (LLMs). The findings from our comprehensive evaluation contribute significantly to the academic discourse surrounding the s
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#36718;&#23545;&#35805;&#31435;&#22330;&#26816;&#27979;&#25968;&#25454;&#38598;&#65288;MT-CSD&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#23616;&#37096;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GLAN&#65289;&#26469;&#35299;&#20915;&#23545;&#35805;&#25968;&#25454;&#20013;&#30340;&#38271;&#36317;&#31163;&#21644;&#30701;&#36317;&#31163;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11145</link><description>&lt;p&gt;
&#19968;&#39033;&#29992;&#20110;&#23545;&#35805;&#31435;&#22330;&#26816;&#27979;&#30340;&#25361;&#25112;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Challenge Dataset and Effective Models for Conversational Stance Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11145
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#36718;&#23545;&#35805;&#31435;&#22330;&#26816;&#27979;&#25968;&#25454;&#38598;&#65288;MT-CSD&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#23616;&#37096;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GLAN&#65289;&#26469;&#35299;&#20915;&#23545;&#35805;&#25968;&#25454;&#20013;&#30340;&#38271;&#36317;&#31163;&#21644;&#30701;&#36317;&#31163;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#31435;&#22330;&#26816;&#27979;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#22312;&#35780;&#20272;&#21333;&#20010;&#23454;&#20363;&#20869;&#30340;&#31435;&#22330;&#65292;&#20174;&#32780;&#22312;&#26377;&#25928;&#24314;&#27169;&#20851;&#20110;&#21516;&#19968;&#29305;&#23450;&#20027;&#39064;&#30340;&#22810;&#26041;&#35752;&#35770;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#36825;&#22312;&#30495;&#23454;&#31038;&#20132;&#23186;&#20307;&#20114;&#21160;&#20013;&#33258;&#28982;&#21457;&#29983;&#12290;&#36825;&#31181;&#38480;&#21046;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#22797;&#21046;&#30495;&#23454;&#31038;&#20132;&#23186;&#20307;&#32972;&#26223;&#30340;&#25968;&#25454;&#38598;&#65292;&#38459;&#30861;&#20102;&#23545;&#35805;&#31435;&#22330;&#26816;&#27979;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#36718;&#23545;&#35805;&#31435;&#22330;&#26816;&#27979;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;\textbf{MT-CSD}&#65289;&#65292;&#28085;&#30422;&#20102;&#29992;&#20110;&#23545;&#35805;&#31435;&#22330;&#26816;&#27979;&#30340;&#22810;&#20010;&#30446;&#26631;&#12290;&#20026;&#20102;&#20174;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#31435;&#22330;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#23616;&#23616;&#37096;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;\textbf{GLAN}&#65289;&#65292;&#20197;&#35299;&#20915;&#23545;&#35805;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#38271;&#36317;&#31163;&#21644;&#30701;&#36317;&#31163;&#20381;&#36182;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;GLAN&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;&#31435;&#22330;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11145v1 Announce Type: new  Abstract: Previous stance detection studies typically concentrate on evaluating stances within individual instances, thereby exhibiting limitations in effectively modeling multi-party discussions concerning the same specific topic, as naturally transpire in authentic social media interactions. This constraint arises primarily due to the scarcity of datasets that authentically replicate real social media contexts, hindering the research progress of conversational stance detection. In this paper, we introduce a new multi-turn conversation stance detection dataset (called \textbf{MT-CSD}), which encompasses multiple targets for conversational stance detection. To derive stances from this challenging dataset, we propose a global-local attention network (\textbf{GLAN}) to address both long and short-range dependencies inherent in conversational data. Notably, even state-of-the-art stance detection methods, exemplified by GLAN, exhibit an accuracy of on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20196;&#29260;&#21270;&#31574;&#30053;&#21644;&#35789;&#27719;&#37327;&#23545;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#23383;&#33410;&#23545;&#32534;&#30721;&#65288;BPE&#65289;&#19982;Farasa&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#31361;&#26174;&#20102;&#24418;&#24577;&#20998;&#26512;&#22312;&#25429;&#25417;&#38463;&#25289;&#20271;&#35821;&#35328;&#32454;&#24494;&#24046;&#24322;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11130</link><description>&lt;p&gt;
&#25506;&#32034;&#20196;&#29260;&#21270;&#31574;&#30053;&#21644;&#35789;&#27719;&#37327;&#23545;&#22686;&#24378;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20196;&#29260;&#21270;&#31574;&#30053;&#21644;&#35789;&#27719;&#37327;&#23545;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#23383;&#33410;&#23545;&#32534;&#30721;&#65288;BPE&#65289;&#19982;Farasa&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#31361;&#26174;&#20102;&#24418;&#24577;&#20998;&#26512;&#22312;&#25429;&#25417;&#38463;&#25289;&#20271;&#35821;&#35328;&#32454;&#24494;&#24046;&#24322;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20196;&#29260;&#21270;&#31574;&#30053;&#21644;&#35789;&#27719;&#37327;&#23545;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#20102;&#22235;&#31181;&#20196;&#29260;&#21270;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#26032;&#38395;&#20998;&#31867;&#12289;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12290;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#35789;&#27719;&#37327;&#65292;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;&#20196;&#29260;&#21270;&#26041;&#27861;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;Farasa&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;&#65288;BPE&#65289;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#65292;&#24378;&#35843;&#20102;&#22312;&#25429;&#25417;&#38463;&#25289;&#20271;&#35821;&#35328;&#32454;&#24494;&#24046;&#24322;&#26041;&#38754;&#24418;&#24577;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#26041;&#35328;&#29305;&#23450;&#30340;&#20998;&#21106;&#38382;&#39064;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#35745;&#31639;&#25928;&#29575;&#20998;&#26512;&#34920;&#26126;&#65292;BPE&#19982;Farasa&#30340;&#31283;&#23450;&#24615;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11130v1 Announce Type: new  Abstract: This paper presents a comprehensive examination of the impact of tokenization strategies and vocabulary sizes on the performance of Arabic language models in downstream natural language processing tasks. Our investigation focused on the effectiveness of four tokenizers across various tasks, including News Classification, Hate Speech Detection, Sentiment Analysis, and Natural Language Inference. Leveraging a diverse set of vocabulary sizes, we scrutinize the intricate interplay between tokenization approaches and model performance. The results reveal that Byte Pair Encoding (BPE) with Farasa outperforms other strategies in multiple tasks, underscoring the significance of morphological analysis in capturing the nuances of the Arabic language. However, challenges arise in sentiment analysis, where dialect specific segmentation issues impact model efficiency. Computational efficiency analysis demonstrates the stability of BPE with Farasa, su
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29702;&#30001;&#21644;&#32467;&#26500;&#24863;&#30693;&#30340;&#22240;&#26524;&#38382;&#31572;&#26469;&#22686;&#24378;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#65292;&#23558;DECI&#20219;&#21153;&#36716;&#21270;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#65292;&#29983;&#25104;&#34987;&#38382;&#21450;&#20107;&#20214;&#30340;&#22240;&#26524;&#20851;&#31995;&#20197;&#21450;&#35299;&#37322;&#36825;&#20123;&#20107;&#20214;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#30001;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20107;&#20214;&#32467;&#26500;&#22270;&#23545;&#22810;&#36339;&#28508;&#22312;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2403.11129</link><description>&lt;p&gt;
&#20351;&#29992;&#29702;&#30001;&#21644;&#32467;&#26500;&#24863;&#30693;&#22240;&#26524;&#38382;&#31572;&#22686;&#24378;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Enhancing Event Causality Identification with Rationale and Structure-Aware Causal Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11129
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29702;&#30001;&#21644;&#32467;&#26500;&#24863;&#30693;&#30340;&#22240;&#26524;&#38382;&#31572;&#26469;&#22686;&#24378;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#65292;&#23558;DECI&#20219;&#21153;&#36716;&#21270;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#65292;&#29983;&#25104;&#34987;&#38382;&#21450;&#20107;&#20214;&#30340;&#22240;&#26524;&#20851;&#31995;&#20197;&#21450;&#35299;&#37322;&#36825;&#20123;&#20107;&#20214;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#30001;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20107;&#20214;&#32467;&#26500;&#22270;&#23545;&#22810;&#36339;&#28508;&#22312;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#65288;DECI&#65289;&#26088;&#22312;&#35782;&#21035;&#25991;&#26723;&#20013;&#20004;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20542;&#21521;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25991;&#26723;&#20013;&#23384;&#22312;&#22810;&#20010;&#20107;&#20214;&#65292;&#36825;&#20123;&#26041;&#27861;&#23481;&#26131;&#20986;&#29616;&#39034;&#24207;&#29983;&#25104;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#28508;&#22312;&#30340;&#32467;&#26500;&#65292;&#22914;&#20107;&#20214;&#20849;&#25351;&#21644;&#30456;&#20851;&#22240;&#26524;&#38142;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29702;&#30001;&#21644;&#32467;&#26500;&#24863;&#30693;&#30340;&#22240;&#26524;&#38382;&#31572;&#26469;&#22686;&#24378;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DECI&#20219;&#21153;&#34987;&#36716;&#21270;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#65292;&#28982;&#21518;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#34987;&#38382;&#21450;&#20107;&#20214;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29983;&#25104;&#29702;&#30001;&#26469;&#35299;&#37322;&#20026;&#20160;&#20040;&#36825;&#20123;&#20107;&#20214;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20107;&#20214;&#32467;&#26500;&#22270;&#65292;&#23545;&#22810;&#36339;&#28508;&#22312;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11129v1 Announce Type: new  Abstract: Document-level Event Causality Identification (DECI) aims to identify causal relations between two events in documents. Recent research tends to use pre-trained language models to generate the event causal relations. Whereas, these methods are prone to the errors of sequential generation due to multiple events in a document. Moreover, the potential structures such as event coreference and related causal chain are neglected. In this paper, we propose a multi-task learning framework to enhance event causality identification with rationale and structure-aware causal question answering. Specifically, the DECI task is transformed into multiple-choice question answering, and the causes and effects of the questioned event are generated with large language models. In addition, we generate the rationales to explain why these events have causal relations. Moreover, we construct an event structure graph, which models the multi-hop potential relatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#21160;&#21160;&#24577;&#35780;&#20272;&#65288;AutoDE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;API&#35843;&#29992;&#33021;&#21147;&#65292;&#36991;&#20813;&#38745;&#24577;&#35780;&#20272;&#20013;&#23548;&#33268;&#30340;&#35823;&#23548;&#24615;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.11128</link><description>&lt;p&gt;
&#36229;&#36234;&#38745;&#24577;&#35780;&#20272;&#65306;&#19968;&#31181;&#21160;&#24577;&#26041;&#27861;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;API&#35843;&#29992;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11128
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#21160;&#21160;&#24577;&#35780;&#20272;&#65288;AutoDE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;API&#35843;&#29992;&#33021;&#21147;&#65292;&#36991;&#20813;&#38745;&#24577;&#35780;&#20272;&#20013;&#23548;&#33268;&#30340;&#35823;&#23548;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;API&#35843;&#29992;&#65292;&#24050;&#32463;&#26174;&#33879;&#25552;&#21319;&#12290;&#36825;&#31181;&#36827;&#27493;&#38656;&#35201;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#37319;&#29992;&#38745;&#24577;&#35780;&#20272;&#65292;&#21363;&#22522;&#20110;&#39044;&#23450;&#20041;&#30340;&#23545;&#35805;&#21382;&#21490;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;API&#35843;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35780;&#20272;&#26041;&#27861;&#21487;&#33021;&#20250;&#20855;&#26377;&#35823;&#23548;&#24615;&#65292;&#22240;&#20026;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#21487;&#33021;&#26080;&#27861;&#26681;&#25454;&#20043;&#21069;&#30340;&#20154;&#31867;&#20114;&#21160;&#29983;&#25104;API&#35843;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#21160;&#24577;&#35780;&#20272;&#65288;AutoDE&#65289;&#65292;&#20197;&#35780;&#20272;&#21161;&#25163;&#30340;API&#35843;&#29992;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#20197;&#21450;&#32791;&#36153;&#22823;&#37327;&#36164;&#28304;&#30340;&#30452;&#25509;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#21162;&#21147;&#27169;&#25311;&#30495;&#23454;&#30340;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20154;&#31867;&#23545;&#35805;&#27169;&#24335;&#65292;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#29992;&#25143;&#20195;&#29702;&#65292;&#24182;&#37197;&#22791;&#29992;&#25143;&#33050;&#26412;&#20197;&#30830;&#20445;&#20154;&#26426;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20986;AutoDE&#25581;&#31034;&#20102;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11128v1 Announce Type: new  Abstract: With the rise of Large Language Models (LLMs), AI assistants' ability to utilize tools, especially through API calls, has advanced notably. This progress has necessitated more accurate evaluation methods. Many existing studies adopt static evaluation, where they assess AI assistants' API call based on pre-defined dialogue histories. However, such evaluation method can be misleading, as an AI assistant might fail in generating API calls from preceding human interaction in real cases. Instead of the resource-intensive method of direct human-machine interactions, we propose Automated Dynamic Evaluation (AutoDE) to assess an assistant's API call capability without human involvement. In our framework, we endeavor to closely mirror genuine human conversation patterns in human-machine interactions, using a LLM-based user agent, equipped with a user script to ensure human alignment. Experimental results highlight that AutoDE uncovers errors over
&lt;/p&gt;</description></item><item><title>&#26356;&#22810;&#30340;&#21709;&#24212;&#20294;&#26356;&#23569;&#30340;&#25552;&#31034;&#21487;&#20197;&#26356;&#22909;&#22320;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#31867;&#23545;&#40784;&#65292;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#22810;&#26679;&#24615;&#20844;&#24335;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#24433;&#21709;LLMs&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.11124</link><description>&lt;p&gt;
&#22312;&#20154;&#31867;&#23545;&#40784;&#20013;&#25193;&#23637;&#25968;&#25454;&#22810;&#26679;&#24615;&#20197;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11124
&lt;/p&gt;
&lt;p&gt;
&#26356;&#22810;&#30340;&#21709;&#24212;&#20294;&#26356;&#23569;&#30340;&#25552;&#31034;&#21487;&#20197;&#26356;&#22909;&#22320;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#31867;&#23545;&#40784;&#65292;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#22810;&#26679;&#24615;&#20844;&#24335;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#24433;&#21709;LLMs&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#21487;&#20197;&#38450;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#35823;&#23548;&#24615;&#25110;&#26377;&#27602;&#20869;&#23481;&#65292;&#21516;&#26102;&#38656;&#35201;&#39640;&#25104;&#26412;&#30340;&#20154;&#31867;&#21453;&#39304;&#12290;&#20551;&#35774;&#20154;&#24037;&#27880;&#37322;&#36164;&#28304;&#26377;&#38480;&#65292;&#21017;&#21487;&#20197;&#32771;&#34385;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#37197;&#26041;&#24335;&#65306;&#26356;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#25110;&#26356;&#22810;&#26679;&#21270;&#30340;&#24453;&#26631;&#35760;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#30340;&#30452;&#25509;&#27604;&#36739;&#23578;&#19981;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#24494;&#35843;&#26679;&#26412;&#25968;&#37327;&#25511;&#21046;&#21452;&#26041;&#30340;&#22810;&#26679;&#24615;&#65292;&#36825;&#21487;&#20197;&#30452;&#25509;&#21453;&#26144;&#23427;&#20204;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#22823;&#37327;&#25552;&#31034;&#19981;&#21516;&#65292;&#26356;&#22810;&#30340;&#21709;&#24212;&#20294;&#26159;&#26356;&#23569;&#30340;&#25552;&#31034;&#26356;&#33021;&#28608;&#21457;LLMs&#36827;&#34892;&#20154;&#31867;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25552;&#31034;&#30340;&#22810;&#26679;&#24615;&#27010;&#24565;&#21487;&#33021;&#27604;&#36890;&#24120;&#30001;&#21333;&#20010;&#25968;&#23383;&#37327;&#21270;&#30340;&#21709;&#24212;&#26356;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#25552;&#31034;&#22810;&#26679;&#24615;&#30340;&#26032;&#20844;&#24335;&#65292;&#36827;&#19968;&#27493;&#26263;&#31034;&#19982;&#24494;&#35843;&#21518;LLMs&#26368;&#32456;&#24615;&#33021;&#30340;&#32447;&#24615;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11124v1 Announce Type: cross  Abstract: Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback. Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse PROMPTS or more diverse RESPONSES to be labeled. Nonetheless, a straightforward comparison between their impact is absent. In this work, we first control the diversity of both sides according to the number of samples for fine-tuning, which can directly reflect their influence. We find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment. Additionally, the concept of diversity for prompts can be more complex than responses that are typically quantified by single digits. Consequently, a new formulation of prompt diversity is proposed, further implying a linear correlation with the final performance of LLMs after f
&lt;/p&gt;</description></item><item><title>Granular Change Accuracy (GCA) &#26159;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#24615;&#33021;&#24230;&#37327;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#30001;&#20110;&#20998;&#24067;&#22343;&#21248;&#24615;&#21644;&#23545;&#35805;&#36718;&#20043;&#38388;&#38169;&#35823;&#23450;&#20301;&#32780;&#20135;&#29983;&#30340;&#20559;&#35265;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#35780;&#20272;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11123</link><description>&lt;p&gt;
Granular Change Accuracy: &#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#24615;&#33021;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Granular Change Accuracy: A More Accurate Performance Metric for Dialogue State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11123
&lt;/p&gt;
&lt;p&gt;
Granular Change Accuracy (GCA) &#26159;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#24615;&#33021;&#24230;&#37327;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#30001;&#20110;&#20998;&#24067;&#22343;&#21248;&#24615;&#21644;&#23545;&#35805;&#36718;&#20043;&#38388;&#38169;&#35823;&#23450;&#20301;&#32780;&#20135;&#29983;&#30340;&#20559;&#35265;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#35780;&#20272;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#31995;&#32479;&#30340;&#24230;&#37327;&#34920;&#29616;&#20986;&#19977;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;i&#65289;&#38169;&#35823;&#22320;&#20551;&#23450;&#23545;&#35805;&#20013;&#25554;&#27133;&#30340;&#20998;&#24067;&#26159;&#22343;&#21248;&#30340;&#65292;ii&#65289;&#24573;&#35270;&#20026;&#21333;&#20010;&#23545;&#35805;&#36718;&#20998;&#37197;&#37096;&#20998;&#20998;&#25968;&#65292;iii&#65289;&#32463;&#24120;&#36890;&#36807;&#21453;&#22797;&#35745;&#31639;&#27169;&#22411;&#25104;&#21151;&#25110;&#22833;&#36133;&#39044;&#27979;&#30340;&#27425;&#25968;&#26469;&#39640;&#20272;&#25110;&#20302;&#20272;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65306;Granular Change Accuracy&#65288;GCA&#65289;&#12290;GCA&#19987;&#27880;&#20110;&#35780;&#20272;&#25972;&#20010;&#23545;&#35805;&#21382;&#21490;&#19978;&#23545;&#35805;&#29366;&#24577;&#30340;&#39044;&#27979;&#21464;&#21270;&#12290;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;GCA&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#30001;&#20110;&#20998;&#24067;&#22343;&#21248;&#24615;&#21644;&#38169;&#35823;&#23450;&#20301;&#19981;&#21516;&#23545;&#35805;&#36718;&#20043;&#38388;&#32780;&#20135;&#29983;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#35780;&#20272;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#35757;&#32451;&#30340;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#20559;&#35265;&#29305;&#21035;&#26126;&#26174;&#65292;&#22312;&#27169;&#22411;&#38169;&#35823;&#29575;&#22686;&#21152;&#26102;&#26356;&#21152;&#26174;&#33879;&#12290;&#22240;&#27492;&#65292;GCA&#22312;&#25552;&#20379;&#26174;&#33879;&#24615;&#26041;&#38754;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11123v1 Announce Type: new  Abstract: Current metrics for evaluating Dialogue State Tracking (DST) systems exhibit three primary limitations. They: i) erroneously presume a uniform distribution of slots throughout the dialog, ii) neglect to assign partial scores for individual turns, iii) frequently overestimate or underestimate performance by repeatedly counting the models' successful or failed predictions. To address these shortcomings, we introduce a novel metric: Granular Change Accuracy (GCA). GCA focuses on evaluating the predicted changes in dialogue state over the entire dialogue history. Benchmarking reveals that GCA effectively reduces biases arising from distribution uniformity and the positioning of errors across turns, resulting in a more precise evaluation. Notably, we find that these biases are particularly pronounced when evaluating few-shot or zero-shot trained models, becoming even more evident as the model's error rate increases. Hence, GCA offers signific
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#27880;&#37322;&#27169;&#24335;&#30340;&#24320;&#21457;&#65292;&#29992;&#20110;&#26500;&#24314;&#25968;&#25454;&#38598;&#20197;&#35780;&#20272;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#31163;&#32447;&#21361;&#23475;&#28508;&#21147;&#65292;&#19981;&#20165;&#20851;&#27880;&#21333;&#19968;&#20998;&#35010;&#22240;&#32032;&#25110;&#20167;&#24680;&#35328;&#35770;&#65292;&#32780;&#26159;&#33268;&#21147;&#20110;&#34913;&#37327;&#22312;&#32447;&#20869;&#23481;&#30340;&#21361;&#23475;&#28508;&#21147;&#65292;&#26080;&#35770;&#20854;&#20869;&#23481;&#26159;&#21542;&#20805;&#28385;&#20167;&#24680;&#12290;</title><link>https://arxiv.org/abs/2403.11108</link><description>&lt;p&gt;
HarmPot: &#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#31163;&#32447;&#21361;&#23475;&#28508;&#21147;&#30340;&#27880;&#37322;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HarmPot: An Annotation Framework for Evaluating Offline Harm Potential of Social Media Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11108
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#27880;&#37322;&#27169;&#24335;&#30340;&#24320;&#21457;&#65292;&#29992;&#20110;&#26500;&#24314;&#25968;&#25454;&#38598;&#20197;&#35780;&#20272;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#31163;&#32447;&#21361;&#23475;&#28508;&#21147;&#65292;&#19981;&#20165;&#20851;&#27880;&#21333;&#19968;&#20998;&#35010;&#22240;&#32032;&#25110;&#20167;&#24680;&#35328;&#35770;&#65292;&#32780;&#26159;&#33268;&#21147;&#20110;&#34913;&#37327;&#22312;&#32447;&#20869;&#23481;&#30340;&#21361;&#23475;&#28508;&#21147;&#65292;&#26080;&#35770;&#20854;&#20869;&#23481;&#26159;&#21542;&#20805;&#28385;&#20167;&#24680;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24320;&#21457;&#19968;&#20010;&#27880;&#37322;&#27169;&#24335;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#31163;&#32447;&#21361;&#23475;&#28508;&#21147;&#12290;&#25105;&#20204;&#23558;&#8220;&#21361;&#23475;&#28508;&#21147;&#8221;&#23450;&#20041;&#20026;&#22312;&#32447;&#20844;&#24320;&#24086;&#23376;&#24341;&#21457;&#30495;&#23454;&#19990;&#30028;&#36523;&#20307;&#20260;&#23475;&#65288;&#21363;&#26292;&#21147;&#65289;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#26292;&#21147;&#36890;&#24120;&#26159;&#30001;&#19968;&#31995;&#21015;&#35302;&#21457;&#22240;&#32032;&#20849;&#21516;&#24341;&#21457;&#30340;&#65292;&#36825;&#20123;&#22240;&#32032;&#32467;&#21512;&#20102;&#20960;&#31181;&#22312;&#32447;&#25112;&#26415;&#21644;&#31038;&#20250;&#29615;&#22659;&#20013;&#29616;&#26377;&#30340;&#20132;&#21449;&#20998;&#27495;&#65292;&#20174;&#32780;&#23548;&#33268;&#26377;&#38024;&#23545;&#24615;&#30340;&#36523;&#20307;&#26292;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#19987;&#27880;&#20110;&#20219;&#20309;&#21333;&#19968;&#30340;&#20998;&#35010;&#24615;&#26041;&#38754;&#65288;&#22914;&#31181;&#22995;&#12289;&#24615;&#21035;&#12289;&#23447;&#25945;&#25110;&#21463;&#23475;&#32773;&#21644;&#26045;&#23475;&#32773;&#30340;&#20854;&#20182;&#36523;&#20221;&#65289;&#65292;&#20063;&#19981;&#20165;&#19987;&#27880;&#20110;&#20167;&#24680;&#35328;&#35770;&#25110;&#38169;&#35823;/&#20559;&#26012;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23545;&#27492;&#31867;&#35302;&#21457;&#22240;&#32032;&#30340;&#20132;&#21449;&#21407;&#22240;&#30340;&#29702;&#35299;&#32858;&#28966;&#20110;&#23581;&#35797;&#34913;&#37327;&#22312;&#32447;&#20869;&#23481;&#30340;&#21361;&#23475;&#28508;&#21147;&#65292;&#26080;&#35770;&#20854;&#26159;&#21542;&#20805;&#28385;&#20167;&#24680;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#29992;&#20110;&#35780;&#20272;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#31163;&#32447;&#21361;&#23475;&#28508;&#21147;&#30340;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#26694;&#26550;/&#27880;&#37322;&#27169;&#24335;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11108v1 Announce Type: new  Abstract: In this paper, we discuss the development of an annotation schema to build datasets for evaluating the offline harm potential of social media texts. We define "harm potential" as the potential for an online public post to cause real-world physical harm (i.e., violence). Understanding that real-world violence is often spurred by a web of triggers, often combining several online tactics and pre-existing intersectional fissures in the social milieu, to result in targeted physical violence, we do not focus on any single divisive aspect (i.e., caste, gender, religion, or other identities of the victim and perpetrators) nor do we focus on just hate speech or mis/dis-information. Rather, our understanding of the intersectional causes of such triggers focuses our attempt at measuring the harm potential of online content, irrespective of whether it is hateful or not. In this paper, we discuss the development of a framework/annotation schema that 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#30465;&#29305;&#23450;&#39046;&#22495;&#65292;&#29983;&#25104;&#39046;&#22495;&#30456;&#20851;&#23646;&#24615;&#24182;&#21019;&#24314;&#23646;&#24615;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#32469;&#36807;&#22797;&#26434;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#30340;&#21019;&#26032;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.11103</link><description>&lt;p&gt;
ProgGen:&#36890;&#36807;&#33258;&#21453;&#22823;&#35821;&#35328;&#27169;&#22411;&#36880;&#27493;&#29983;&#25104;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11103
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#30465;&#29305;&#23450;&#39046;&#22495;&#65292;&#29983;&#25104;&#39046;&#22495;&#30456;&#20851;&#23646;&#24615;&#24182;&#21019;&#24314;&#23646;&#24615;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#32469;&#36807;&#22797;&#26434;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#30340;&#21019;&#26032;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36328;&#39046;&#22495;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#32467;&#26500;&#21270;&#30693;&#35782;&#25552;&#21462;&#20219;&#21153;&#65288;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;NER&#65289;&#26041;&#38754;&#32463;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#20855;&#26377;&#36866;&#24230;NER&#33021;&#21147;&#30340;LLMs&#26469;&#29983;&#25104;&#20248;&#31168;&#30340;NER&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#22522;&#26412;&#30340;&#31867;&#26465;&#20214;&#25552;&#31034;&#65292;&#32780;&#26159;&#25351;&#23548;LLMs&#23545;&#29305;&#23450;&#39046;&#22495;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#65292;&#20174;&#32780;&#29983;&#25104;&#20855;&#26377;&#39046;&#22495;&#30456;&#20851;&#23646;&#24615;&#65288;&#20363;&#22914;&#24433;&#35780;&#30340;&#31867;&#21035;&#21644;&#24773;&#24863;&#65289;&#30340;&#23646;&#24615;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39044;&#20808;&#29983;&#25104;&#23454;&#20307;&#26415;&#35821;&#65292;&#28982;&#21518;&#22260;&#32469;&#36825;&#20123;&#23454;&#20307;&#24320;&#21457;NER&#19978;&#19979;&#25991;&#25968;&#25454;&#65292;&#26377;&#25928;&#35268;&#36991;&#20102;LLMs&#23545;&#22797;&#26434;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#36890;&#29992;&#21644;&#19987;&#19994;&#39046;&#22495;&#23637;&#24320;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11103v1 Announce Type: new  Abstract: Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER). This paper explores an innovative, cost-efficient strategy to harness LLMs with modest NER capabilities for producing superior NER datasets. Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data. Furthermore, we preemptively generate entity terms and then develop NER context data around these entities, effectively bypassing the LLMs' challenges with complex structures. Our experiments across both general and niche domains reveal significant performance enhancements over conventional data generation methods while being mor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#19968;&#20010;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22522;&#20934;&#20013;&#23384;&#22312;&#35199;&#29677;&#29273;&#35821;&#12289;&#26085;&#35821;&#21644;&#20013;&#25991;&#20013;&#19981;&#21516;&#31243;&#24230;&#30340;&#32763;&#35793;&#38169;&#35823;&#65292;&#25552;&#20379;&#20102;&#32416;&#27491;&#65292;&#24182;&#20998;&#26512;&#20102;&#23545;&#22522;&#20934;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.11092</link><description>&lt;p&gt;
&#32763;&#35793;&#22256;&#22659;&#65311;&#36328;&#35821;&#35328;&#27010;&#24565;&#19978;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20844;&#24179;&#35780;&#20272;&#30340;&#32763;&#35793;&#38169;&#35823;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Lost in Translation? Translation Errors and Challenges for Fair Assessment of Text-to-Image Models on Multilingual Concepts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#19968;&#20010;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22522;&#20934;&#20013;&#23384;&#22312;&#35199;&#29677;&#29273;&#35821;&#12289;&#26085;&#35821;&#21644;&#20013;&#25991;&#20013;&#19981;&#21516;&#31243;&#24230;&#30340;&#32763;&#35793;&#38169;&#35823;&#65292;&#25552;&#20379;&#20102;&#32416;&#27491;&#65292;&#24182;&#20998;&#26512;&#20102;&#23545;&#22522;&#20934;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#27010;&#24565;&#21015;&#34920;&#32763;&#35793;&#25104;&#19971;&#31181;&#35821;&#35328;&#65292;&#23558;&#21050;&#28608;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#39044;&#26399;&#30340;&#22270;&#20687;&#20998;&#24067;&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#22522;&#20934;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#22522;&#20934;&#22312;&#35199;&#29677;&#29273;&#35821;&#12289;&#26085;&#35821;&#21644;&#20013;&#25991;&#20013;&#23384;&#22312;&#19981;&#21516;&#20005;&#37325;&#31243;&#24230;&#30340;&#32763;&#35793;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20123;&#38169;&#35823;&#30340;&#26356;&#27491;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#23545;CoCo-CroLa&#20316;&#20026;&#22522;&#20934;&#30340;&#25928;&#29992;&#21644;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#35746;&#21518;&#37325;&#26032;&#35780;&#20272;&#20102;&#22810;&#20010;&#22522;&#32447;T2I&#27169;&#22411;&#65292;&#27604;&#36739;&#20102;&#22312;&#26032;&#32763;&#35793;&#19979;&#24341;&#21457;&#30340;&#36755;&#20986;&#19982;&#22312;&#26087;&#32763;&#35793;&#19979;&#24341;&#21457;&#30340;&#36755;&#20986;&#65292;&#24182;&#23637;&#31034;&#20102;&#20462;&#27491;&#23545;&#22270;&#20687;&#39046;&#22495;&#22522;&#20934;&#32467;&#26524;&#30340;&#24433;&#21709;&#31243;&#24230;&#26159;&#21487;&#20197;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11092v1 Announce Type: cross  Abstract: Benchmarks of the multilingual capabilities of text-to-image (T2I) models compare generated images prompted in a test language to an expected image distribution over a concept set. One such benchmark, "Conceptual Coverage Across Languages" (CoCo-CroLa), assesses the tangible noun inventory of T2I models by prompting them to generate pictures from a concept list translated to seven languages and comparing the output image populations. Unfortunately, we find that this benchmark contains translation errors of varying severity in Spanish, Japanese, and Chinese. We provide corrections for these errors and analyze how impactful they are on the utility and validity of CoCo-CroLa as a benchmark. We reassess multiple baseline T2I models with the revisions, compare the outputs elicited under the new translations to those conditioned on the old, and show that a correction's impactfulness on the image-domain benchmark results can be predicted in t
&lt;/p&gt;</description></item><item><title>m&amp;m's&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;4K+&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#28041;&#21450;33&#31181;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#20316;&#20026;&#35268;&#21010;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.11085</link><description>&lt;p&gt;
m&amp;m's: &#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#24037;&#20855;&#20351;&#29992;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
m&amp;m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11085
&lt;/p&gt;
&lt;p&gt;
m&amp;m's&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;4K+&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#28041;&#21450;33&#31181;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#20316;&#20026;&#35268;&#21010;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#24456;&#23569;&#30001;&#21333;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#20915;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#27493;&#39588;&#35745;&#31639;&#35745;&#21010;&#65292;&#28041;&#21450;&#25340;&#25509;&#22810;&#20010;&#27169;&#22411;&#12290; &#24037;&#20855;&#22686;&#24378;&#22411;LLM&#26497;&#26377;&#21487;&#33021;&#33258;&#21160;&#21270;&#29983;&#25104;&#36825;&#31181;&#35745;&#31639;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#29992;&#20110;&#35780;&#20272;LLM&#20316;&#20026;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#35268;&#21010;&#22120;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#65292;&#38459;&#30861;&#20102;&#23545;&#35268;&#21010;&#22120;&#35774;&#35745;&#20915;&#31574;&#30340;&#31995;&#32479;&#30740;&#31350;&#12290;LLM&#26159;&#21542;&#24212;&#19968;&#27425;&#24615;&#29983;&#25104;&#25972;&#20010;&#35745;&#21010;&#36824;&#26159;&#36880;&#27493;&#29983;&#25104;&#65311;&#23427;&#20204;&#26159;&#21542;&#24212;&#35813;&#30452;&#25509;&#20351;&#29992;Python&#20195;&#30721;&#35843;&#29992;&#24037;&#20855;&#65292;&#36824;&#26159;&#36890;&#36807;&#31867;&#20284;JSON&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#26684;&#24335;&#65311;&#21453;&#39304;&#26159;&#21542;&#25913;&#21892;&#35268;&#21010;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#20197;&#21450;&#26356;&#22810;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;m&amp;m's&#65306;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#21547;4K+&#20010;&#28041;&#21450;33&#31181;&#24037;&#20855;&#30340;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#27169;&#24577;&#27169;&#22411;&#12289;(&#20813;&#36153;)&#20844;&#20849;API&#21644;&#22270;&#20687;&#22788;&#29702;&#27169;&#22359;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#26597;&#35810;&#65292;&#25105;&#20204;&#25552;&#20379;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#33258;&#21160;&#29983;&#25104;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11085v1 Announce Type: cross  Abstract: Real-world multi-modal problems are rarely solved by a single machine learning model, and often require multi-step computational plans that involve stitching several models. Tool-augmented LLMs hold tremendous promise for automating the generation of such computational plans. However, the lack of standardized benchmarks for evaluating LLMs as planners for multi-step multi-modal tasks has prevented a systematic study of planner design decisions. Should LLMs generate a full plan in a single shot or step-by-step? Should they invoke tools directly with Python code or through structured data formats like JSON? Does feedback improve planning? To answer these questions and more, we introduce m&amp;m's: a benchmark containing 4K+ multi-step multi-modal tasks involving 33 tools that include multi-modal models, (free) public APIs, and image processing modules. For each of these task queries, we provide automatically generated plans using this realis
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23450;&#21046;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21644;&#24341;&#20837;&#22810;&#27169;&#24577;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#22810;&#27169;&#24577;&#24322;&#24120;&#26816;&#27979;&#21644;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.11083</link><description>&lt;p&gt;
&#20026;&#22810;&#27169;&#24577;&#24322;&#24120;&#26816;&#27979;&#21644;&#25512;&#29702;&#23450;&#21046;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11083
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23450;&#21046;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21644;&#24341;&#20837;&#22810;&#27169;&#24577;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#22810;&#27169;&#24577;&#24322;&#24120;&#26816;&#27979;&#21644;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#21508;&#31181;&#24037;&#19994;&#22330;&#26223;&#20013;&#21313;&#20998;&#37325;&#35201;&#65292;&#21253;&#25324;&#29983;&#20135;&#32447;&#19978;&#24322;&#24120;&#27169;&#24335;&#30340;&#35782;&#21035;&#21644;&#29992;&#20110;&#36136;&#37327;&#25511;&#21046;&#30340;&#21046;&#36896;&#32570;&#38519;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25317;&#26377;&#24191;&#27867;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#30340;&#36890;&#29992;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23450;&#21046;&#20026;&#24322;&#24120;&#26816;&#27979;&#22120;&#21644;&#25512;&#29702;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#25552;&#31034;&#31574;&#30053;&#65292;&#23558;&#39046;&#22495;&#19987;&#23478;&#30340;&#39046;&#22495;&#30693;&#35782;&#20316;&#20026;&#26465;&#20214;&#24341;&#23548;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#22810;&#27169;&#24577;&#25552;&#31034;&#31867;&#22411;&#65292;&#21253;&#25324;&#20219;&#21153;&#25551;&#36848;&#12289;&#31867;&#21035;&#19978;&#19979;&#25991;&#12289;&#27491;&#24120;&#35268;&#21017;&#21644;&#21442;&#32771;&#22270;&#20687;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#34920;&#31034;&#32479;&#19968;&#20026;2D&#22270;&#20687;&#26684;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11083v1 Announce Type: cross  Abstract: Anomaly detection is vital in various industrial scenarios, including the identification of unusual patterns in production lines and the detection of manufacturing defects for quality control. Existing techniques tend to be specialized in individual scenarios and lack generalization capacities. In this study, we aim to develop a generic anomaly detection model applicable across multiple scenarios. To achieve this, we customize generic visual-language foundation models that possess extensive knowledge and robust reasoning abilities into anomaly detectors and reasoners. Specifically, we introduce a multi-modal prompting strategy that incorporates domain knowledge from experts as conditions to guide the models. Our approach considers multi-modal prompt types, including task descriptions, class context, normality rules, and reference images. In addition, we unify the input representation of multi-modality into a 2D image format, enabling m
&lt;/p&gt;</description></item><item><title>RobustSentEmbed&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#20102;&#25991;&#26412;&#34920;&#31034;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.11082</link><description>&lt;p&gt;
RobustSentEmbed&#65306;&#20351;&#29992;&#23545;&#25239;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#31283;&#20581;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11082
&lt;/p&gt;
&lt;p&gt;
RobustSentEmbed&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#20102;&#25991;&#26412;&#34920;&#31034;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#22522;&#20110;PLM&#30340;&#34920;&#31034;&#36890;&#24120;&#22312;&#23545;&#25239;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;RobustSentEmbed&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#19981;&#21516;&#25991;&#26412;&#34920;&#31034;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#20197;&#21450;&#23545;&#25239;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#12290;&#36890;&#36807;&#29983;&#25104;&#39640;&#39118;&#38505;&#30340;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#29992;&#20110;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;RobustSentEmbed&#24039;&#22937;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#21644;&#31283;&#20581;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;RobustSentEmbed&#20248;&#20110;&#26368;&#20808;&#36827;&#34920;&#31034;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#38477;&#20302;&#20102;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#65292;&#29305;&#21035;&#26159;&#38477;&#20302;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11082v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks. Nevertheless, despite their success with unseen data, current PLM-based representations often exhibit poor robustness in adversarial settings. In this paper, we introduce RobustSentEmbed, a self-supervised sentence embedding framework designed to improve both generalization and robustness in diverse text representation tasks and against a diverse set of adversarial attacks. Through the generation of high-risk adversarial perturbations and their utilization in a novel objective function, RobustSentEmbed adeptly learns high-quality and robust sentence embeddings. Our experiments confirm the superiority of RobustSentEmbed over state-of-the-art representations. Specifically, Our framework achieves a significant reduction in the success rate of various adversarial attacks, notably reducing
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#27874;&#26031;&#35821;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#23545;Digikala&#32593;&#31449;&#30340;&#23458;&#25143;&#35780;&#35770;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#26368;&#32456;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;78.3&#30340;F1&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.11069</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27874;&#26031;&#35821;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based Sentiment Analysis in Persian Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11069
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#27874;&#26031;&#35821;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#23545;Digikala&#32593;&#31449;&#30340;&#23458;&#25143;&#35780;&#35770;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#26368;&#32456;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;78.3&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#65292;&#24773;&#24863;&#20998;&#26512;&#23588;&#20854;&#22312;&#27874;&#26031;&#35821;&#20013;&#26159;&#19968;&#20010;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#65292;&#20351;&#29992;&#26469;&#33258;Digikala&#22312;&#32447;&#38646;&#21806;&#21830;&#32593;&#31449;&#30340;&#23458;&#25143;&#35780;&#35770;&#25968;&#25454;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#20316;&#20026;&#20998;&#31867;&#22120;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#28151;&#21512;&#26041;&#27861;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;78.3&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11069v1 Announce Type: new  Abstract: Recently, there has been a growing interest in the use of deep learning techniques for tasks in natural language processing (NLP), with sentiment analysis being one of the most challenging areas, particularly in the Persian language. The vast amounts of content generated by Persian users on thousands of websites, blogs, and social networks such as Telegram, Instagram, and Twitter present a rich resource of information. Deep learning techniques have become increasingly favored for extracting insights from this extensive pool of raw data, although they face several challenges. In this study, we introduced and implemented a hybrid deep learning-based model for sentiment analysis, using customer review data from the Digikala Online Retailer website. We employed a variety of deep learning networks and regularization techniques as classifiers. Ultimately, our hybrid approach yielded an impressive performance, achieving an F1 score of 78.3 acro
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#34920;&#22320;&#29702;&#20154;&#21475;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#20559;&#21521;&#65292;&#34920;&#29616;&#20248;&#31168;&#30340;&#20154;&#21475;&#20027;&#35201;&#38598;&#20013;&#22312;&#32654;&#22269;&#21644;&#33521;&#22269;&#65292;&#32780;&#21335;&#20122;&#21644;&#19996;&#21335;&#20122;&#22320;&#21306;&#30340;&#20154;&#21475;&#21017;&#34987;&#36739;&#24046;&#22320;&#20195;&#34920;&#12290;</title><link>https://arxiv.org/abs/2403.11025</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#34920;&#26576;&#20123;&#22320;&#29702;&#20154;&#21475;&#26041;&#38754;&#36739;&#20854;&#20182;&#20154;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Pre-Trained Language Models Represent Some Geographic Populations Better Than Others
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11025
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#34920;&#22320;&#29702;&#20154;&#21475;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#20559;&#21521;&#65292;&#34920;&#29616;&#20248;&#31168;&#30340;&#20154;&#21475;&#20027;&#35201;&#38598;&#20013;&#22312;&#32654;&#22269;&#21644;&#33521;&#22269;&#65292;&#32780;&#21335;&#20122;&#21644;&#19996;&#21335;&#20122;&#22320;&#21306;&#30340;&#20154;&#21475;&#21017;&#34987;&#36739;&#24046;&#22320;&#20195;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#34913;&#37327;&#20102;&#20004;&#31181;LLMs&#23478;&#26063;&#22312;&#20195;&#34920;&#22810;&#26679;&#21270;&#22320;&#29702;&#20154;&#21475;&#26041;&#38754;&#30340;&#20559;&#21521;&#31243;&#24230;&#12290;&#20351;&#29992;&#31354;&#38388;&#25506;&#27979;&#20219;&#21153;&#21644;&#22320;&#29702;&#21442;&#32771;&#35821;&#26009;&#24211;&#34913;&#37327;&#20102;OPT&#21644;BLOOM&#31995;&#21015;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20840;&#29699;&#19981;&#21516;&#20154;&#21475;&#20013;&#30340;&#20195;&#34920;&#24615;&#31243;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#26576;&#20123;&#20154;&#21475;&#34920;&#29616;&#24471;&#27604;&#20854;&#20182;&#20154;&#21475;&#26356;&#22909;&#12290;&#29305;&#21035;&#26159;&#65292;&#32654;&#22269;&#21644;&#33521;&#22269;&#30340;&#20154;&#21475;&#34987;&#38750;&#24120;&#22909;&#22320;&#20195;&#34920;&#65292;&#32780;&#21335;&#20122;&#21644;&#19996;&#21335;&#20122;&#22320;&#21306;&#30340;&#20154;&#21475;&#21017;&#34987;&#36739;&#24046;&#22320;&#20195;&#34920;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#23478;&#26063;&#22312;&#19981;&#21516;&#20154;&#21475;&#20013;&#22522;&#26412;&#19978;&#20849;&#20139;&#30456;&#21516;&#30340;&#20559;&#21521;&#12290;&#21516;&#26102;&#65292;&#36825;&#31181;&#20559;&#21521;&#19981;&#33021;&#23436;&#20840;&#36890;&#36807;&#31038;&#20250;&#35821;&#35328;&#22240;&#32032;&#12289;&#32463;&#27982;&#22240;&#32032;&#25110;&#22320;&#29702;&#22240;&#32032;&#26469;&#35299;&#37322;&#12290;&#20174;&#36825;&#20010;&#20998;&#26512;&#20013;&#24471;&#20986;&#30340;&#22522;&#26412;&#32467;&#35770;&#26159;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#19981;&#24179;&#31561;&#22320;&#20195;&#34920;&#20102;&#19990;&#30028;&#20154;&#21475;&#65306;&#23384;&#22312;&#30528;&#29305;&#23450;&#22320;&#29702;&#20154;&#21475;&#30340;&#26126;&#26174;&#20559;&#21521;&#12290;&#36825;&#19968;&#21457;&#29616;&#25361;&#25112;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11025v1 Announce Type: new  Abstract: This paper measures the skew in how well two families of LLMs represent diverse geographic populations. A spatial probing task is used with geo-referenced corpora to measure the degree to which pre-trained language models from the OPT and BLOOM series represent diverse populations around the world. Results show that these models perform much better for some populations than others. In particular, populations across the US and the UK are represented quite well while those in South and Southeast Asia are poorly represented. Analysis shows that both families of models largely share the same skew across populations. At the same time, this skew cannot be fully explained by sociolinguistic factors, economic factors, or geographic factors. The basic conclusion from this analysis is that pre-trained models do not equally represent the world's population: there is a strong skew towards specific geographic populations. This finding challenges the 
&lt;/p&gt;</description></item><item><title>DIALECTBENCH&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26041;&#35328;&#12289;&#35821;&#35328;&#21464;&#20307;&#21644;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#23454;&#29616;&#23545;&#19981;&#21516;&#35821;&#35328;&#21464;&#20307;&#19978;NLP&#31995;&#32479;&#24615;&#33021;&#30340;&#20840;&#38754;&#35780;&#20272;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.11009</link><description>&lt;p&gt;
DIALECTBENCH&#65306;&#19968;&#20010;&#26041;&#35328;&#12289;&#35821;&#35328;&#21464;&#20307;&#21644;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11009
&lt;/p&gt;
&lt;p&gt;
DIALECTBENCH&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26041;&#35328;&#12289;&#35821;&#35328;&#21464;&#20307;&#21644;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#23454;&#29616;&#23545;&#19981;&#21516;&#35821;&#35328;&#21464;&#20307;&#19978;NLP&#31995;&#32479;&#24615;&#33021;&#30340;&#20840;&#38754;&#35780;&#20272;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11009v1 &#35821;&#31181;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#35821;&#35328;&#25216;&#26415;&#24212;&#35813;&#26681;&#25454;&#20854;&#22312;&#23454;&#38469;&#29992;&#20363;&#20013;&#30340;&#26377;&#29992;&#24615;&#26469;&#21028;&#26029;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#21644;&#35780;&#20272;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;&#38750;&#26631;&#20934;&#26041;&#35328;&#25110;&#35821;&#35328;&#21464;&#20307;&#65288;&#20197;&#19979;&#31616;&#31216;&#20026;&#21464;&#20307;&#65289;&#24418;&#24335;&#30340;&#35821;&#35328;&#21464;&#20307;. &#22823;&#22810;&#25968;NLP&#22522;&#20934;&#27979;&#35797;&#20165;&#38480;&#20110;&#26631;&#20934;&#35821;&#35328;&#21464;&#20307;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIALECTBENCH&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#35821;&#35328;&#21464;&#20307;&#30340;&#22823;&#35268;&#27169;NLP&#22522;&#20934;&#27979;&#35797;&#65292;&#27719;&#24635;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#20219;&#21153;&#26679;&#26412;&#30340;&#21464;&#20307;&#25968;&#25454;&#38598;&#65288;&#28085;&#30422;281&#31181;&#21464;&#20307;&#30340;10&#20010;&#25991;&#26412;&#32423;&#20219;&#21153;&#65289;&#12290;&#36825;&#20801;&#35768;&#23545;&#19981;&#21516;&#35821;&#35328;&#21464;&#20307;&#19978;NLP&#31995;&#32479;&#24615;&#33021;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#35777;&#25454;&#34920;&#26126;&#26631;&#20934;&#35821;&#35328;&#21464;&#20307;&#19982;&#38750;&#26631;&#20934;&#35821;&#35328;&#21464;&#20307;&#20043;&#38388;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#22312;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22823;&#37327;&#24615;&#33021;&#24046;&#36317;&#30340;&#35821;&#35328;&#31867;&#32676;&#12290;&#25105;&#20204;&#35748;&#20026;DIALECTBENCH&#25552;&#20379;&#20102;&#23545;&#24403;&#21069;&#35821;&#35328;NLP&#29366;&#24577;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11009v1 Announce Type: cross  Abstract: Language technologies should be judged on their usefulness in real-world use cases. An often overlooked aspect in natural language processing (NLP) research and evaluation is language variation in the form of non-standard dialects or language varieties (hereafter, varieties). Most NLP benchmarks are limited to standard language varieties. To fill this gap, we propose DIALECTBENCH, the first-ever large-scale benchmark for NLP on varieties, which aggregates an extensive set of task-varied variety datasets (10 text-level tasks covering 281 varieties). This allows for a comprehensive evaluation of NLP system performance on different language varieties. We provide substantial evidence of performance disparities between standard and non-standard language varieties, and we also identify language clusters with large performance divergence across tasks. We believe DIALECTBENCH provides a comprehensive view of the current state of NLP for langua
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#26694;&#26550;&#65292;&#22312;&#23454;&#20307;&#23545;&#40784;&#20013;&#35299;&#20915;&#20102;&#26080;&#26631;&#31614;&#24748;&#25346;&#26696;&#20363;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#27880;&#24847;&#26426;&#21046;&#21644;&#27491;&#26679;&#26412;-&#26080;&#26631;&#31614;&#25439;&#22833;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#40784;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.10978</link><description>&lt;p&gt;
&#20855;&#26377;&#26080;&#26631;&#31614;&#24748;&#25346;&#26696;&#20363;&#30340;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Entity Alignment with Unlabeled Dangling Cases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#26694;&#26550;&#65292;&#22312;&#23454;&#20307;&#23545;&#40784;&#20013;&#35299;&#20915;&#20102;&#26080;&#26631;&#31614;&#24748;&#25346;&#26696;&#20363;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#27880;&#24847;&#26426;&#21046;&#21644;&#27491;&#26679;&#26412;-&#26080;&#26631;&#31614;&#25439;&#22833;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#40784;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#26080;&#26631;&#31614;&#24748;&#25346;&#26696;&#20363;&#30340;&#23454;&#20307;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#28304;&#22270;&#25110;&#30446;&#26631;&#22270;&#20013;&#26377;&#19968;&#20123;&#23454;&#20307;&#22312;&#21478;&#19968;&#26041;&#20013;&#27809;&#26377;&#23545;&#24212;&#23454;&#20307;&#65292;&#24182;&#19988;&#36825;&#20123;&#23454;&#20307;&#20445;&#25345;&#26410;&#26631;&#35760;&#29366;&#24577;&#12290;&#35813;&#38382;&#39064;&#20986;&#29616;&#22312;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#30340;&#35268;&#27169;&#19981;&#21516;&#65292;&#24182;&#19988;&#26631;&#35760;&#21487;&#21305;&#37197;&#23454;&#20307;&#30340;&#25104;&#26412;&#36828;&#20302;&#20110;&#24748;&#25346;&#23454;&#20307;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;GNN&#30340;&#24748;&#25346;&#26816;&#27979;&#21644;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;GNN&#65292;&#24182;&#19988;&#19968;&#36215;&#35757;&#32451;&#65292;&#20294;&#26816;&#27979;&#21040;&#30340;&#24748;&#25346;&#23454;&#20307;&#22312;&#23545;&#40784;&#20013;&#34987;&#31227;&#38500;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#29305;&#28857;&#26159;&#20855;&#26377;&#29992;&#20110;&#36873;&#25321;&#24615;&#37051;&#22495;&#32858;&#21512;&#30340;&#35774;&#35745;&#23454;&#20307;&#21644;&#20851;&#31995;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#21450;&#29992;&#20110;&#23545;&#24748;&#25346;&#23454;&#20307;&#36827;&#34892;&#26080;&#20559;&#20272;&#35745;&#30340;&#27491;&#26679;&#26412;-&#26080;&#26631;&#31614;&#23398;&#20064;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#35774;&#35745;&#30340;&#27599;&#20010;&#32452;&#20214;&#37117;&#23545;&#25972;&#20307;&#23545;&#40784;&#24615;&#33021;&#26377;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10978v1 Announce Type: new  Abstract: We investigate the entity alignment problem with unlabeled dangling cases, meaning that there are entities in the source or target graph having no counterparts in the other, and those entities remain unlabeled. The problem arises when the source and target graphs are of different scales, and it is much cheaper to label the matchable pairs than the dangling entities. To solve the issue, we propose a novel GNN-based dangling detection and entity alignment framework. While the two tasks share the same GNN and are trained together, the detected dangling entities are removed in the alignment. Our framework is featured by a designed entity and relation attention mechanism for selective neighborhood aggregation in representation learning, as well as a positive-unlabeled learning loss for an unbiased estimation of dangling entities. Experimental results have shown that each component of our design contributes to the overall alignment performance
&lt;/p&gt;</description></item><item><title>Pointer-Generator Networks&#22312;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#20013;&#26410;&#23637;&#29616;&#20986;&#39044;&#26399;&#30340;&#20248;&#21183;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#36164;&#28304;&#33539;&#22260;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#19979;&#34920;&#29616;&#19968;&#33324;&#12290;</title><link>https://arxiv.org/abs/2403.10963</link><description>&lt;p&gt;
Pointer-Generator&#32593;&#32476;&#29992;&#20110;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#65306;&#19981;&#35201;&#22797;&#21046;&#37027;&#20010;&#65281;
&lt;/p&gt;
&lt;p&gt;
Pointer-Generator Networks for Low-Resource Machine Translation: Don't Copy That!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10963
&lt;/p&gt;
&lt;p&gt;
Pointer-Generator Networks&#22312;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#20013;&#26410;&#23637;&#29616;&#20986;&#39044;&#26399;&#30340;&#20248;&#21183;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#36164;&#28304;&#33539;&#22260;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#19979;&#34920;&#29616;&#19968;&#33324;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#22312;&#39640;&#36164;&#28304;&#29615;&#22659;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#35768;&#22810;&#35821;&#35328;&#32570;&#20047;&#24517;&#35201;&#30340;&#22823;&#35268;&#27169;&#24179;&#34892;&#35821;&#26009;&#24211;&#26469;&#21463;&#30410;&#12290;&#22312;&#20004;&#31181;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#20043;&#38388;&#30340;&#20302;&#36164;&#28304;&#65288;LR&#65289;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#19968;&#31181;&#33258;&#28982;&#30340;&#30452;&#35273;&#26159;&#23547;&#27714;&#20174;&#32467;&#26500;&#8220;&#25463;&#24452;&#8221;&#20013;&#33719;&#30410;&#65292;&#20363;&#22914;&#20174;&#28304;&#35821;&#35328;&#22797;&#21046;&#23376;&#35789;&#21040;&#30446;&#26631;&#35821;&#35328;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#35821;&#35328;&#23545;&#36890;&#24120;&#20849;&#20139;&#30456;&#24403;&#25968;&#37327;&#30340;&#30456;&#21516;&#21333;&#35789;&#12289;&#21516;&#28304;&#35789;&#21644;&#20511;&#35789;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#38024;&#23545;&#20845;&#31181;&#35821;&#35328;&#23545;&#30340;&#25351;&#38024;&#29983;&#25104;&#22120;&#32593;&#32476;&#22312;&#21508;&#31181;&#36164;&#28304;&#33539;&#22260;&#19979;&#30340;&#29992;&#36884;&#65292;&#24182;&#21457;&#29616;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#37117;&#26377;&#36731;&#24494;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#26174;&#31034;&#65292;&#27169;&#22411;&#23545;&#20110;&#23494;&#20999;&#30456;&#20851;&#30340;&#35821;&#35328;&#23545;&#19982;&#36739;&#36828;&#30340;&#35821;&#35328;&#23545;&#65292;&#25110;&#32773;&#36164;&#28304;&#33539;&#22260;&#36739;&#20302;&#19982;&#36739;&#39640;&#30340;&#35821;&#35328;&#23545;&#24182;&#27809;&#26377;&#23637;&#29616;&#20986;&#26356;&#22823;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#27169;&#22411;&#24182;&#26410;&#23637;&#31034;&#20986;&#23545;&#20110;&#20849;&#20139;&#23376;&#35789;&#26426;&#21046;&#30340;&#39044;&#26399;&#29992;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#34892;&#20026;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10963v1 Announce Type: new  Abstract: While Transformer-based neural machine translation (NMT) is very effective in high-resource settings, many languages lack the necessary large parallel corpora to benefit from it. In the context of low-resource (LR) MT between two closely-related languages, a natural intuition is to seek benefits from structural "shortcuts", such as copying subwords from the source to the target, given that such language pairs often share a considerable number of identical words, cognates, and borrowings. We test Pointer-Generator Networks for this purpose for six language pairs over a variety of resource ranges, and find weak improvements for most settings. However, analysis shows that the model does not show greater improvements for closely-related vs. more distant language pairs, or for lower resource ranges, and that the models do not exhibit the expected usage of the mechanism for shared subwords. Our discussion of the reasons for this behaviour high
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#21644;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20854;&#26174;&#33879;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.10961</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#21450;&#20854;&#22312;&#35821;&#38899;&#21644;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Models with Applications to Speech and Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10961
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#21644;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20854;&#26174;&#33879;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#20063;&#31216;&#20026;&#38543;&#26426;&#22330;&#21644;&#26080;&#21521;&#22270;&#27169;&#22411;&#12290;EBMs&#26159;&#38750;&#35268;&#33539;&#21270;&#30340;&#65292;&#22240;&#27492;&#19982;&#20854;&#20182;&#27969;&#34892;&#30340;&#33258;&#24402;&#19968;&#21270;&#27010;&#29575;&#27169;&#22411;&#65288;&#22914;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMMs&#65289;&#12289;&#33258;&#22238;&#24402;&#27169;&#22411;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65289;&#26377;&#26681;&#26412;&#30340;&#19981;&#21516;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#37325;&#35201;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#36827;&#23637;&#65292;EBMs&#19981;&#20165;&#21560;&#24341;&#20102;&#26680;&#24515;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#65292;&#36824;&#21560;&#24341;&#20102;&#24212;&#29992;&#39046;&#22495;&#65288;&#22914;&#35821;&#38899;&#12289;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#65289;&#30340;&#20852;&#36259;&#12290;&#35821;&#38899;&#21644;&#35821;&#35328;&#30340;&#39034;&#24207;&#24615;&#36136;&#20063;&#25552;&#20379;&#20102;&#29305;&#27530;&#25361;&#25112;&#65292;&#38656;&#35201;&#19982;&#22788;&#29702;&#22266;&#23450;&#32500;&#24230;&#25968;&#25454;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#26377;&#25152;&#19981;&#21516;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#26412;&#19987;&#33879;&#26088;&#22312;&#31995;&#32479;&#20171;&#32461;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#31639;&#27861;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10961v1 Announce Type: cross  Abstract: Energy-Based Models (EBMs) are an important class of probabilistic models, also known as random fields and undirected graphical models. EBMs are un-normalized and thus radically different from other popular self-normalized probabilistic models such as hidden Markov models (HMMs), autoregressive models, generative adversarial nets (GANs) and variational auto-encoders (VAEs). Over the past years, EBMs have attracted increasing interest not only from the core machine learning community, but also from application domains such as speech, vision, natural language processing (NLP) and so on, due to significant theoretical and algorithmic progress. The sequential nature of speech and language also presents special challenges and needs a different treatment from processing fix-dimensional data (e.g., images). Therefore, the purpose of this monograph is to present a systematic introduction to energy-based models, including both algorithmic progr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SelfIE&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33258;&#35299;&#37322;&#20854;&#23884;&#20837;&#65292;&#25581;&#31034;&#20869;&#37096;&#25512;&#29702;&#65292;&#21253;&#25324;&#36947;&#24503;&#20915;&#31574;&#12289;&#25552;&#31034;&#27880;&#20837;&#21644;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.10949</link><description>&lt;p&gt;
SelfIE&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#30340;&#33258;&#25105;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SelfIE: Self-Interpretation of Large Language Model Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10949
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SelfIE&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33258;&#35299;&#37322;&#20854;&#23884;&#20837;&#65292;&#25581;&#31034;&#20869;&#37096;&#25512;&#29702;&#65292;&#21253;&#25324;&#36947;&#24503;&#20915;&#31574;&#12289;&#25552;&#31034;&#27880;&#20837;&#21644;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10949v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#33719;&#24471;&#31572;&#26696;&#65311;&#35299;&#37322;&#21644;&#25511;&#21046;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#23545;&#20110;&#21487;&#38752;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#26410;&#26469;&#27169;&#22411;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SelfIE&#65288;&#23884;&#20837;&#30340;&#33258;&#25105;&#35299;&#37322;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;LLMs&#21709;&#24212;&#20851;&#20110;&#32473;&#23450;&#27573;&#33853;&#30340;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#23427;&#20204;&#33258;&#24049;&#30340;&#23884;&#20837;&#12290;SelfIE&#33021;&#22815;&#35299;&#37322;&#38544;&#34255;&#23884;&#20837;&#20013;&#30340;&#24320;&#25918;&#19990;&#30028;&#27010;&#24565;&#65292;&#22312;&#26696;&#20363;&#20013;&#25581;&#31034;LLM&#30340;&#20869;&#37096;&#25512;&#29702;&#65292;&#22914;&#20570;&#20986;&#36947;&#24503;&#20915;&#31574;&#12289;&#20869;&#21270;&#25552;&#31034;&#27880;&#20837;&#21644;&#22238;&#24819;&#26377;&#23475;&#30693;&#35782;&#12290;SelfIE&#23545;&#38544;&#34255;&#23884;&#20837;&#30340;&#25991;&#26412;&#25551;&#36848;&#20063;&#24320;&#36767;&#20102;&#25511;&#21046;LLM&#25512;&#29702;&#30340;&#26032;&#36884;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30417;&#30563;&#25511;&#21046;&#65292;&#23427;&#20801;&#35768;&#32534;&#36753;&#24320;&#25918;&#24335;&#27010;&#24565;&#65292;&#32780;&#21482;&#38656;&#35201;&#35745;&#31639;&#21333;&#20010;&#23618;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#23558;RLHF&#25193;&#23637;&#21040;&#38544;&#34255;&#30340;&#23884;&#20837;&#65292;&#24182;&#25552;&#20986;&#20102;&#24378;&#21270;&#25511;&#21046;&#26469;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10949v1 Announce Type: cross  Abstract: How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge i
&lt;/p&gt;</description></item><item><title>MIntRec2.0&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#21644;&#23545;&#35805;&#20013;&#22330;&#22806;&#26816;&#27979;&#25361;&#25112;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;30&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#30340;1,245&#20010;&#23545;&#35805;&#21644;15,040&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;&#21253;&#25324;&#36924;&#30495;&#30340;&#22330;&#22806;&#26679;&#26412;&#65292;&#24182;&#20016;&#23500;&#20102;&#21457;&#35328;&#32773;&#20449;&#24687;&#20197;&#25903;&#25345;&#22810;&#26041;&#23545;&#35805;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.10943</link><description>&lt;p&gt;
MIntRec2.0&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#21644;&#23545;&#35805;&#20013;&#22330;&#22806;&#26816;&#27979;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10943
&lt;/p&gt;
&lt;p&gt;
MIntRec2.0&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#21644;&#23545;&#35805;&#20013;&#22330;&#22806;&#26816;&#27979;&#25361;&#25112;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;30&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#30340;1,245&#20010;&#23545;&#35805;&#21644;15,040&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;&#21253;&#25324;&#36924;&#30495;&#30340;&#22330;&#22806;&#26679;&#26412;&#65292;&#24182;&#20016;&#23500;&#20102;&#21457;&#35328;&#32773;&#20449;&#24687;&#20197;&#25903;&#25345;&#22810;&#26041;&#23545;&#35805;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#25972;&#21512;&#26469;&#33258;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#30340;&#38750;&#35821;&#35328;&#24418;&#24335;&#65292;&#20197;&#22686;&#24378;&#23545;&#20154;&#31867;&#24847;&#22270;&#30340;&#29702;&#35299;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#19978;&#21463;&#38480;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22810;&#36718;&#23545;&#35805;&#20114;&#21160;&#20013;&#20986;&#29616;&#30340;&#22330;&#22806;&#26679;&#26412;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MIntRec2.0&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26041;&#23545;&#35805;&#20013;&#30340;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;1,245&#20010;&#23545;&#35805;&#65292;15,040&#20010;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#22312;30&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#30340;&#26032;&#24847;&#22270;&#20998;&#31867;&#20013;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#38500;&#20102;9,304&#20010;&#22330;&#20869;&#26679;&#26412;&#22806;&#65292;&#36824;&#21253;&#25324;5,736&#20010;&#20986;&#29616;&#22312;&#22810;&#36718;&#19978;&#19979;&#25991;&#20013;&#30340;&#22330;&#22806;&#26679;&#26412;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#33258;&#28982;&#21457;&#29983;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#27599;&#20010;&#35805;&#35821;&#20013;&#21457;&#35328;&#32773;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;&#20016;&#23500;&#20102;&#23427;&#22312;&#22810;&#26041;&#23545;&#35805;&#30740;&#31350;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10943v1 Announce Type: cross  Abstract: Multimodal intent recognition poses significant challenges, requiring the incorporation of non-verbal modalities from real-world contexts to enhance the comprehension of human intentions. Existing benchmark datasets are limited in scale and suffer from difficulties in handling out-of-scope samples that arise in multi-turn conversational interactions. We introduce MIntRec2.0, a large-scale benchmark dataset for multimodal intent recognition in multi-party conversations. It contains 1,245 dialogues with 15,040 samples, each annotated within a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope samples, it also includes 5,736 out-of-scope samples appearing in multi-turn contexts, which naturally occur in real-world scenarios. Furthermore, we provide comprehensive information on the speakers in each utterance, enriching its utility for multi-party conversational research. We establish a general framework supporting the o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#23567;&#21270;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21021;&#27493;&#35299;&#30721;&#65292;&#20197;&#25552;&#39640;&#20302;&#36164;&#28304;ASR&#20013;&#26230;&#26684;&#37325;&#26032;&#25171;&#20998;&#30340;&#35821;&#38899;&#35782;&#21035;&#20934;&#30830;&#24615;&#65292;&#30456;&#23545;&#20943;&#23569;&#20102;&#27888;&#21346;&#22266;&#35821;21.8%&#21644;&#21345;&#32435;&#36798;&#35821;41.8%&#30340;&#35789;&#35823;&#24046;&#29575;&#65292;&#21516;&#26102;&#20165;&#28040;&#32791;1/8&#20869;&#23384;&#12290;</title><link>https://arxiv.org/abs/2403.10937</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#21270;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21021;&#27493;&#35299;&#30721;&#20197;&#25913;&#36827;&#20302;&#36164;&#28304;ASR&#20013;&#30340;&#26230;&#26684;&#37325;&#26032;&#25171;&#20998;
&lt;/p&gt;
&lt;p&gt;
Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10937
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#21270;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21021;&#27493;&#35299;&#30721;&#65292;&#20197;&#25552;&#39640;&#20302;&#36164;&#28304;ASR&#20013;&#26230;&#26684;&#37325;&#26032;&#25171;&#20998;&#30340;&#35821;&#38899;&#35782;&#21035;&#20934;&#30830;&#24615;&#65292;&#30456;&#23545;&#20943;&#23569;&#20102;&#27888;&#21346;&#22266;&#35821;21.8%&#21644;&#21345;&#32435;&#36798;&#35821;41.8%&#30340;&#35789;&#35823;&#24046;&#29575;&#65292;&#21516;&#26102;&#20165;&#28040;&#32791;1/8&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#22522;&#32447;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#29983;&#25104;&#20840;&#38754;&#26230;&#26684;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#36890;&#36807;&#26230;&#26684;&#37325;&#26032;&#25171;&#20998;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22522;&#32447;&#35821;&#35328;&#27169;&#22411;&#26368;&#23567;&#31243;&#24230;&#22320;&#22686;&#24378;&#20026;&#30446;&#26631;&#35821;&#35328;&#26356;&#22823;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#20986;&#29616;&#20294;&#22522;&#32447;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#23384;&#22312;&#30340;&#35789;&#30340;&#35789;&#39057;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#22686;&#24378;&#30340;&#22522;&#32447;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#30721;&#29983;&#25104;&#30340;&#26230;&#26684;&#26356;&#21152;&#20840;&#38754;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#35789;&#35823;&#24046;&#29575;&#30456;&#23545;&#20943;&#23569;&#20102;21.8%&#65288;&#27888;&#21346;&#22266;&#35821;&#65289;&#21644;41.8%&#65288;&#21345;&#32435;&#36798;&#35821;&#65289;&#12290;&#19982;&#20351;&#29992;&#23436;&#25972;&#32500;&#22522;&#30334;&#31185;&#25991;&#26412;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#35789;&#35823;&#24046;&#29575;&#26041;&#38754;&#30456;&#24403;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#28040;&#32791;1/8&#30340;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10937v1 Announce Type: cross  Abstract: This paper addresses the problem of improving speech recognition accuracy with lattice rescoring in low-resource languages where the baseline language model is insufficient for generating inclusive lattices. We minimally augment the baseline language model with word unigram counts that are present in a larger text corpus of the target language but absent in the baseline. The lattices generated after decoding with such an augmented baseline language model are more comprehensive. We obtain 21.8% (Telugu) and 41.8% (Kannada) relative word error reduction with our proposed method. This reduction in word error rate is comparable to 21.5% (Telugu) and 45.9% (Kannada) relative word error reduction obtained by decoding with full Wikipedia text augmented language mode while our approach consumes only 1/8th the memory. We demonstrate that our method is comparable with various text selection-based language model augmentation and also consistent f
&lt;/p&gt;</description></item><item><title>BEnQA&#20171;&#32461;&#20102;&#19968;&#39033;&#21253;&#21547;&#23391;&#21152;&#25289;&#35821;&#21644;&#33521;&#35821;&#32771;&#35797;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23391;&#21152;&#25289;&#35821;&#21644;&#33521;&#35821;&#38382;&#39064;&#20013;&#34920;&#29616;&#26377;&#26126;&#26174;&#24046;&#24322;&#65292;&#21516;&#26102;&#21457;&#29616;Chain-of-Thought&#25552;&#31034;&#23545;&#25512;&#29702;&#38382;&#39064;&#26377;&#30410;&#65292;&#38468;&#21152;&#33521;&#35821;&#32763;&#35793;&#26377;&#21161;&#20110;&#35299;&#31572;&#23391;&#21152;&#25289;&#35821;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10900</link><description>&lt;p&gt;
BEnQA: &#23391;&#21152;&#25289;&#35821;&#21644;&#33521;&#35821;&#30340;&#38382;&#39064;&#22238;&#31572;&#21644;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10900
&lt;/p&gt;
&lt;p&gt;
BEnQA&#20171;&#32461;&#20102;&#19968;&#39033;&#21253;&#21547;&#23391;&#21152;&#25289;&#35821;&#21644;&#33521;&#35821;&#32771;&#35797;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23391;&#21152;&#25289;&#35821;&#21644;&#33521;&#35821;&#38382;&#39064;&#20013;&#34920;&#29616;&#26377;&#26126;&#26174;&#24046;&#24322;&#65292;&#21516;&#26102;&#21457;&#29616;Chain-of-Thought&#25552;&#31034;&#23545;&#25512;&#29702;&#38382;&#39064;&#26377;&#30410;&#65292;&#38468;&#21152;&#33521;&#35821;&#32763;&#35793;&#26377;&#21161;&#20110;&#35299;&#31572;&#23391;&#21152;&#25289;&#35821;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BEnQA&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#23391;&#21152;&#25289;&#35821;&#21644;&#33521;&#35821;&#24182;&#34892;&#32771;&#35797;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#28085;&#30422;&#23391;&#21152;&#25289;&#22269;&#20013;&#23398;&#21644;&#39640;&#20013;&#27700;&#24179;&#30340;&#39064;&#30446;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#22823;&#32422;5,000&#36947;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#31185;&#23398;&#20013;&#30340;&#22810;&#20010;&#23398;&#31185;&#65292;&#21253;&#25324;&#20107;&#23454;&#24615;&#12289;&#24212;&#29992;&#24615;&#21644;&#22522;&#20110;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#24182;&#34892;&#25968;&#25454;&#38598;&#23545;&#20960;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#27169;&#22411;&#22312;&#23391;&#21152;&#25289;&#35821;&#21644;&#33521;&#35821;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20123;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#22522;&#20110;&#25512;&#29702;&#30340;&#38382;&#39064;&#26469;&#35828;&#65292;&#8220;Chain-of-Thought&#8221;&#25552;&#31034;&#26041;&#27861;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#23545;&#20110;&#20107;&#23454;&#24615;&#38382;&#39064;&#21017;&#27809;&#26377;&#37027;&#20040;&#26377;&#24110;&#21161;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#22238;&#31572;&#23391;&#21152;&#25289;&#35821;&#38382;&#39064;&#26102;&#65292;&#38468;&#21152;&#33521;&#35821;&#32763;&#35793;&#26377;&#21161;&#20110;&#22238;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25351;&#20986;&#20102;&#26410;&#26469;&#25913;&#36827;LLMs&#22312;&#23391;&#21152;&#25289;&#35821;&#21644;&#26356;&#26222;&#36941;&#22320;&#22312;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#20013;&#24615;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10900v1 Announce Type: new  Abstract: In this study, we introduce BEnQA, a dataset comprising parallel Bengali and English exam questions for middle and high school levels in Bangladesh. Our dataset consists of approximately 5K questions covering several subjects in science with different types of questions, including factual, application, and reasoning-based questions. We benchmark several Large Language Models (LLMs) with our parallel dataset and observe a notable performance disparity between the models in Bengali and English. We also investigate some prompting methods, and find that Chain-of-Thought prompting is beneficial mostly on reasoning questions, but not so much on factual ones. We also find that appending English translation helps to answer questions in Bengali. Our findings point to promising future research directions for improving the performance of LLMs in Bengali and more generally in low-resource languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#39046;&#22495;&#20013;&#30340;&#32487;&#32493;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#25991;&#26412;&#28151;&#21512;&#21644;&#25209;&#37327;&#26680;&#33539;&#25968;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#32487;&#32493;&#23398;&#20064;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.10894</link><description>&lt;p&gt;
&#26397;&#21521;&#31283;&#20581;&#24615;&#21644;&#22810;&#26679;&#24615;&#65306;&#20351;&#29992;&#25991;&#26412;&#28151;&#21512;&#21644;&#25209;&#37327;&#26680;&#33539;&#25968;&#26368;&#22823;&#21270;&#36827;&#34892;&#23545;&#35805;&#29983;&#25104;&#30340;&#32487;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Robustness and Diversity: Continual Learning in Dialog Generation with Text-Mixup and Batch Nuclear-Norm Maximization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#39046;&#22495;&#20013;&#30340;&#32487;&#32493;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#25991;&#26412;&#28151;&#21512;&#21644;&#25209;&#37327;&#26680;&#33539;&#25968;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#32487;&#32493;&#23398;&#20064;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#25345;&#32493;&#19981;&#26029;&#25509;&#25910;&#25968;&#25454;&#30340;&#21160;&#24577;&#19990;&#30028;&#20013;&#65292;&#32487;&#32493;&#23398;&#20064;&#20351;&#25105;&#20204;&#33021;&#22815;&#36880;&#27493;&#28155;&#21152;&#26032;&#30340;&#20219;&#21153;/&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#32487;&#32493;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#27169;&#22411;&#22312;&#35757;&#32451;&#26032;&#20219;&#21153;/&#39046;&#22495;&#26102;&#24536;&#35760;&#20043;&#21069;&#35757;&#32451;&#20219;&#21153;/&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#30340;&#20542;&#21521;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32487;&#32493;&#23398;&#20064;&#35774;&#32622;&#19979;&#30340;&#23545;&#35805;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;1&#65289;&#20351;&#29992;Text-Mixup&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#36991;&#20813;&#27169;&#22411;&#22312;&#37325;&#25918;&#35760;&#24518;&#19978;&#36807;&#25311;&#21512;&#65292;2&#65289;&#21033;&#29992;&#25209;&#37327;&#26680;&#33539;&#25968;&#26368;&#22823;&#21270;&#65288;BNNM&#65289;&#26469;&#32531;&#35299;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;&#23545;&#19968;&#20010;&#21253;&#21547;37&#20010;&#39046;&#22495;&#30340;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;DailyDialog&#65288;&#19968;&#20010;&#21253;&#21547;10&#20010;&#39046;&#22495;&#30340;&#38386;&#32842;&#25968;&#25454;&#38598;&#65289;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32487;&#32493;&#23398;&#20064;&#26041;&#38754;&#32988;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10894v1 Announce Type: new  Abstract: In our dynamic world where data arrives in a continuous stream, continual learning enables us to incrementally add new tasks/domains without the need to retrain from scratch. A major challenge in continual learning of language model is catastrophic forgetting, the tendency of models to forget knowledge from previously trained tasks/domains when training on new ones. This paper studies dialog generation under the continual learning setting. We propose a novel method that 1) uses \textit{Text-Mixup} as data augmentation to avoid model overfitting on replay memory and 2) leverages Batch-Nuclear Norm Maximization (BNNM) to alleviate the problem of mode collapse. Experiments on a $37$-domain task-oriented dialog dataset and DailyDialog (a $10$-domain chitchat dataset) demonstrate that our proposed approach outperforms the state-of-the-art in continual learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;MLLMs&#30340;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25193;&#23637;&#35789;&#27719;&#12289;&#21452;&#35821;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.10882</link><description>&lt;p&gt;
&#20248;&#21270;&#22810;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22686;&#24378;&#65306;&#20197;&#38889;&#35821;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10882
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;MLLMs&#30340;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25193;&#23637;&#35789;&#27719;&#12289;&#21452;&#35821;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#29992;&#39044;&#35757;&#32451;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#21333;&#35789;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25193;&#23637;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#35768;&#22810;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#21644;&#30740;&#31350;&#26426;&#26500;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;LLMs&#65288;MLLMs&#65289;&#20197;&#28385;&#36275;&#24403;&#21069;&#38656;&#27714;&#65292;&#20294;&#24573;&#35270;&#20102;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#65288;LRLs&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;MLLMs&#30340;LRLs&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25193;&#23637;LRLs&#30340;MLLM&#35789;&#27719;&#20197;&#22686;&#24378;&#34920;&#36798;&#24615;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#21452;&#35821;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#23545;&#40784;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#31532;&#19977;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#23567;&#35268;&#27169;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#20197;&#22686;&#24378;LRL&#12290;&#23454;&#39564;&#37319;&#29992;&#20102;Llama2&#27169;&#22411;&#65292;&#20197;&#38889;&#35821;&#20316;&#20026;LRL&#65292;&#24182;&#22312;&#20843;&#39033;&#20219;&#21153;&#20013;&#23545;&#20854;&#19982;&#20854;&#20182;&#24050;&#24320;&#21457;&#30340;LLMs&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20154;&#31867;&#35780;&#20272;&#36827;&#34892;&#20102;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10882v1 Announce Type: cross  Abstract: Large language models (LLMs) use pretraining to predict the subsequent word; however, their expansion requires significant computing resources. Numerous big tech companies and research institutes have developed multilingual LLMs (MLLMs) to meet current demands, overlooking less-resourced languages (LRLs). This study proposed three strategies to enhance the performance of LRLs based on the publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to enhance expressiveness. Second, bilingual data were used for pretraining to align the high- and less-resourced languages. Third, a high-quality small-scale instruction dataset was constructed and instruction-tuning was performed to augment the LRL. The experiments employed the Llama2 model and Korean was used as the LRL, which was quantitatively evaluated against other developed LLMs across eight tasks. Furthermore, a qualitative assessment was performed based on human eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#22411;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#38544;&#20889;&#26415;&#23454;&#29616;&#26356;&#22909;&#30340;&#24863;&#30693;&#21644;&#32479;&#35745;&#38544;&#34109;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10856</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#29983;&#25104;&#35821;&#35328;&#38544;&#20889;&#26415;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Generative Linguistic Steganography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#22411;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#38544;&#20889;&#26415;&#23454;&#29616;&#26356;&#22909;&#30340;&#24863;&#30693;&#21644;&#32479;&#35745;&#38544;&#34109;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#35821;&#35328;&#38544;&#20889;&#26415;&#35797;&#22270;&#23558;&#31192;&#23494;&#20449;&#24687;&#38544;&#34255;&#22312;&#35206;&#30422;&#25991;&#26412;&#20013;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#20391;&#37325;&#20110;&#35206;&#30422;&#25991;&#26412;&#21644;&#38544;&#20889;&#25991;&#26412;&#20043;&#38388;&#30340;&#32479;&#35745;&#24046;&#24322;&#65292;&#28982;&#32780;&#65292;&#19981;&#35268;&#33539;&#30340;&#38544;&#20889;&#25991;&#26412;&#24456;&#23481;&#26131;&#34987;&#20154;&#31867;&#36776;&#21035;&#20986;&#26469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#22411;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#26356;&#22909;&#30340;&#24863;&#30693;&#21644;&#32479;&#35745;&#38544;&#34109;&#24615;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20960;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#21487;&#37325;&#29616;&#30340;&#35821;&#35328;&#35780;&#20272;&#65292;&#20197;&#34913;&#37327;&#38544;&#20889;&#25991;&#26412;&#30340;&#38544;&#34109;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#26080;&#23475;&#21644;&#21487;&#29702;&#35299;&#30340;&#38544;&#20889;&#25991;&#26412;&#27604;&#20219;&#20309;&#20854;&#20182;&#26041;&#27861;&#22810;1.926&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10856v1 Announce Type: new  Abstract: Generative linguistic steganography attempts to hide secret messages into covertext. Previous studies have generally focused on the statistical differences between the covertext and stegotext, however, ill-formed stegotext can readily be identified by humans. In this paper, we propose a novel zero-shot approach based on in-context learning for linguistic steganography to achieve better perceptual and statistical imperceptibility. We also design several new metrics and reproducible language evaluations to measure the imperceptibility of the stegotext. Our experimental results indicate that our method produces $1.926\times$ more innocent and intelligible stegotext than any other method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RetinaQA&#30340;&#26032;&#22411;KBQA&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#22635;&#22270;&#30340;&#36923;&#36753;&#24418;&#24335;&#26500;&#24314;&#21644;&#20351;&#29992;&#36776;&#21035;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#22238;&#31572;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21487;&#22238;&#31572;&#21644;&#19981;&#21487;&#22238;&#31572;&#38382;&#39064;&#19978;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;KBQA&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.10849</link><description>&lt;p&gt;
RETINAQA&#65306;&#19968;&#31181;&#23545;&#21487;&#22238;&#31572;&#21644;&#19981;&#21487;&#22238;&#31572;&#38382;&#39064;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RETINAQA : A Knowledge Base Question Answering Model Robust to both Answerable and Unanswerable Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10849
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RetinaQA&#30340;&#26032;&#22411;KBQA&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#22635;&#22270;&#30340;&#36923;&#36753;&#24418;&#24335;&#26500;&#24314;&#21644;&#20351;&#29992;&#36776;&#21035;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#22238;&#31572;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21487;&#22238;&#31572;&#21644;&#19981;&#21487;&#22238;&#31572;&#38382;&#39064;&#19978;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;KBQA&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#27169;&#22411;&#36890;&#24120;&#20551;&#23450;&#38382;&#39064;&#26159;&#21487;&#22238;&#31572;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#35757;&#32451;&#21644;&#38408;&#20540;&#35774;&#23450;&#36866;&#24212;&#26816;&#27979;&#19981;&#21487;&#22238;&#31572;&#24615;&#65292;&#20294;&#36825;&#23558;&#20197;&#29306;&#29298;&#21487;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#65292;&#19988;&#27809;&#26377;&#21333;&#19968;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#25152;&#26377;&#19981;&#21487;&#22238;&#31572;&#24615;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RetinaQA&#30340;&#26032;&#22411;KBQA&#27169;&#22411;&#65292;&#23427;&#23545;&#19981;&#21487;&#22238;&#31572;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#23427;&#23558;&#22522;&#20110;KB&#36941;&#21382;&#30340;&#36923;&#36753;&#24418;&#24335;&#26816;&#32034;&#19982;&#22522;&#20110;&#22635;&#22270;&#30340;&#36923;&#36753;&#24418;&#24335;&#26500;&#24314;&#30456;&#32467;&#21512;&#12290;&#36825;&#26377;&#21161;&#20110;&#22788;&#29702;&#20855;&#26377;&#26377;&#25928;&#36923;&#36753;&#24418;&#24335;&#20294;&#22312;&#30693;&#35782;&#24211;&#20013;&#27809;&#26377;&#36890;&#21521;&#31572;&#26696;&#30340;&#25968;&#25454;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#20351;&#29992;&#36776;&#21035;&#32780;&#38750;&#29983;&#25104;&#26469;&#26356;&#22909;&#22320;&#35782;&#21035;&#27809;&#26377;&#26377;&#25928;&#36923;&#36753;&#24418;&#24335;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RetinaQA&#22312;&#21487;&#22238;&#31572;&#21644;&#19981;&#21487;&#22238;&#31572;&#38382;&#39064;&#19978;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;KBQA&#27169;&#22411;&#30340;&#35843;&#25972;&#65292;&#24182;&#19988;&#22312;&#19981;&#21487;&#22238;&#31572;&#24615;&#31867;&#21035;&#19978;&#26174;&#31034;&#20986;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10849v1 Announce Type: new  Abstract: State-of-the-art KBQA models assume answerability of questions. Recent research has shown that while these can be adapted to detect unaswerability with suitable training and thresholding, this comes at the expense of accuracy for answerable questions, and no single model is able to handle all categories of unanswerability. We propose a new model for KBQA named RetinaQA that is robust against unaswerability. It complements KB-traversal based logical form retrieval with sketch-filling based logical form construction. This helps with questions that have valid logical forms but no data paths in the KB leading to an answer. Additionally, it uses discrimination instead of generation to better identify questions that do not have valid logical forms. We demonstrate that RetinaQA significantly outperforms adaptations of state-of-the-art KBQA models across answerable and unanswerable questions, while showing robustness across unanswerability categ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#24179;&#22343;&#28508;&#22312;&#21521;&#37327;&#24182;&#22522;&#20110;&#22810;&#23618;&#28508;&#22312;&#21521;&#37327;&#26816;&#27979;&#32593;&#32476;&#29359;&#32618;&#32534;&#30721;&#35789;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#21270;&#29359;&#32618;&#20195;&#30721;&#35789;&#26816;&#27979;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#33719;&#21462;&#22256;&#38590;&#21644;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.10838</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#23618;&#34920;&#31034;&#23398;&#20064;&#30340;&#20004;&#27493;&#33258;&#21160;&#21270;&#29359;&#32618;&#20195;&#30721;&#35789;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Two-step Automated Cybercrime Coded Word Detection using Multi-level Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10838
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#24179;&#22343;&#28508;&#22312;&#21521;&#37327;&#24182;&#22522;&#20110;&#22810;&#23618;&#28508;&#22312;&#21521;&#37327;&#26816;&#27979;&#32593;&#32476;&#29359;&#32618;&#32534;&#30721;&#35789;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#21270;&#29359;&#32618;&#20195;&#30721;&#35789;&#26816;&#27979;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#33719;&#21462;&#22256;&#38590;&#21644;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#32593;&#32476;&#26381;&#21153;&#24179;&#21488;&#20013;&#65292;&#29359;&#32618;&#23244;&#30097;&#20154;&#21487;&#33021;&#20250;&#20351;&#29992;&#32593;&#32476;&#29359;&#32618;&#32534;&#30721;&#35789;&#36827;&#34892;&#36890;&#20449;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#21333;&#35789;&#20013;&#28155;&#21152;&#29359;&#32618;&#21547;&#20041;&#25110;&#29992;&#31867;&#20284;&#21333;&#35789;&#26367;&#25442;&#26469;&#23454;&#29616;&#12290;&#20363;&#22914;&#65292;&#21333;&#35789;"ice"&#32463;&#24120;&#34987;&#29992;&#26469;&#25351;&#20195;&#27602;&#21697;&#29359;&#32618;&#20013;&#30340;&#30002;&#22522;&#33519;&#19993;&#33018;&#12290;&#38024;&#23545;&#33258;&#21160;&#21270;&#29359;&#32618;&#20195;&#30721;&#35789;&#26816;&#27979;&#38382;&#39064;&#65292;&#25910;&#38598;&#36275;&#22815;&#37327;&#30340;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#21644;&#30452;&#25509;&#24212;&#29992;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#26356;&#22909;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#30340;&#35821;&#35328;&#27169;&#22411;&#37117;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#26041;&#27861;&#65292;&#31532;&#19968;&#27493;&#36890;&#36807;&#20116;&#31181;&#19981;&#21516;&#30340;AutoEncoder&#27169;&#22411;&#20043;&#19968;&#20026;&#27599;&#20010;&#32593;&#32476;&#29359;&#32618;&#26500;&#24314;&#24179;&#22343;&#28508;&#22312;&#21521;&#37327;&#65292;&#31532;&#20108;&#27493;&#22522;&#20110;&#22810;&#23618;&#28508;&#22312;&#21521;&#37327;&#26816;&#27979;&#32593;&#32476;&#29359;&#32618;&#32534;&#30721;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10838v1 Announce Type: new  Abstract: In social network service platforms, crime suspects are likely to use cybercrime coded words for communication by adding criminal meanings to existing words or replacing them with similar words. For instance, the word 'ice' is often used to mean methamphetamine in drug crimes. To analyze the nature of cybercrime and the behavior of criminals, quickly detecting such words and further understanding their meaning are critical. In the automated cybercrime coded word detection problem, it is difficult to collect a sufficient amount of training data for supervised learning and to directly apply language models that utilize context information to better understand natural language. To overcome these limitations, we propose a new two-step approach, in which a mean latent vector is constructed for each cybercrime through one of five different AutoEncoder models in the first step, and cybercrime coded words are detected based on multi-level latent
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#20026;&#23391;&#21152;&#25289;&#35821;&#35774;&#35745;&#30340;&#26032;&#39062;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;BHM&#65292;&#29992;&#20110;&#26816;&#27979;&#20167;&#24680;&#34920;&#24773;&#21253;&#20197;&#21450;&#23427;&#20204;&#25152;&#38024;&#23545;&#30340;&#31038;&#20250;&#23454;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.10829</link><description>&lt;p&gt;
&#36776;&#26512;&#20167;&#24680;&#65306;&#35782;&#21035;&#20167;&#24680;&#34920;&#24773;&#21253;&#21450;&#20854;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Deciphering Hate: Identifying Hateful Memes and Their Targets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10829
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#20026;&#23391;&#21152;&#25289;&#35821;&#35774;&#35745;&#30340;&#26032;&#39062;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;BHM&#65292;&#29992;&#20110;&#26816;&#27979;&#20167;&#24680;&#34920;&#24773;&#21253;&#20197;&#21450;&#23427;&#20204;&#25152;&#38024;&#23545;&#30340;&#31038;&#20250;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#34920;&#24773;&#21253;&#24050;&#32463;&#25104;&#20026;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#34920;&#36798;&#24773;&#24863;&#12289;&#24605;&#24819;&#21644;&#35266;&#28857;&#30340;&#24378;&#22823;&#25163;&#27573;&#12290;&#34429;&#28982;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#31181;&#24189;&#40664;&#21644;&#23089;&#20048;&#26469;&#28304;&#65292;&#20294;&#34920;&#24773;&#21253;&#20063;&#21487;&#20197;&#20256;&#25773;&#38024;&#23545;&#20010;&#20154;&#25110;&#31038;&#21306;&#30340;&#20167;&#24680;&#20869;&#23481;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20391;&#37325;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#34920;&#24773;&#21253;&#30340;&#36127;&#38754;&#26041;&#38754;&#65292;&#24573;&#35270;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#23391;&#21152;&#25289;&#35821;&#65289;&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#20043;&#21069;&#20851;&#20110;&#23391;&#21152;&#25289;&#35821;&#34920;&#24773;&#21253;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#26816;&#27979;&#20167;&#24680;&#34920;&#24773;&#21253;&#19978;&#65292;&#20294;&#24182;&#27809;&#26377;&#23545;&#23427;&#20204;&#30340;&#30446;&#26631;&#23454;&#20307;&#36827;&#34892;&#26816;&#27979;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#24182;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38754;&#21521;&#23391;&#21152;&#25289;&#35821;&#30340;&#26032;&#39062;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;BHM&#65288;&#23391;&#21152;&#25289;&#20167;&#24680;&#34920;&#24773;&#21253;&#65289;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;7,148&#20010;&#24102;&#26377;&#23391;&#21152;&#25289;&#35821;&#20197;&#21450;&#28151;&#21512;&#20195;&#30721;&#23383;&#24149;&#30340;&#34920;&#24773;&#21253;&#65292;&#19987;&#20026;&#20004;&#39033;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#65306;&#65288;i&#65289;&#26816;&#27979;&#20167;&#24680;&#34920;&#24773;&#21253;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#26816;&#27979;&#23427;&#20204;&#25152;&#38024;&#23545;&#30340;&#31038;&#20250;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10829v1 Announce Type: new  Abstract: Internet memes have become a powerful means for individuals to express emotions, thoughts, and perspectives on social media. While often considered as a source of humor and entertainment, memes can also disseminate hateful content targeting individuals or communities. Most existing research focuses on the negative aspects of memes in high-resource languages, overlooking the distinctive challenges associated with low-resource languages like Bengali (also known as Bangla). Furthermore, while previous work on Bengali memes has focused on detecting hateful memes, there has been no work on detecting their targeted entities. To bridge this gap and facilitate research in this arena, we introduce a novel multimodal dataset for Bengali, BHM (Bengali Hateful Memes). The dataset consists of 7,148 memes with Bengali as well as code-mixed captions, tailored for two tasks: (i) detecting hateful memes, and (ii) detecting the social entities they target
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20851;&#31995;&#35299;&#32544;&#26469;&#25351;&#23548;&#31070;&#32463;&#21709;&#24212;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20250;&#35805;&#19978;&#19979;&#25991;&#20869;&#37096;&#24494;&#22937;&#32447;&#32034;&#19978;&#36827;&#34892;&#20851;&#31995;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26041;&#22238;&#22797;&#30340;&#33258;&#21160;&#25512;&#26029;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2403.10827</link><description>&lt;p&gt;
&#20855;&#26377;&#20851;&#31995;&#35299;&#32544;&#30340;&#22810;&#26041;&#22238;&#22797;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multi-party Response Generation with Relation Disentanglement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20851;&#31995;&#35299;&#32544;&#26469;&#25351;&#23548;&#31070;&#32463;&#21709;&#24212;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20250;&#35805;&#19978;&#19979;&#25991;&#20869;&#37096;&#24494;&#22937;&#32447;&#32034;&#19978;&#36827;&#34892;&#20851;&#31995;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26041;&#22238;&#22797;&#30340;&#33258;&#21160;&#25512;&#26029;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#21452;&#26041;&#23545;&#35805;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20551;&#35774;&#35805;&#35821;&#26159;&#25353;&#39034;&#24207;&#32452;&#32455;&#30340;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#23545;&#35805;&#28041;&#21450;&#22810;&#26041;&#21442;&#19982;&#32773;&#65292;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#32467;&#26500;&#35201;&#22797;&#26434;&#24471;&#22810;&#65292;&#20363;&#22914;&#65292;&#26469;&#33258;&#19981;&#21516;&#21442;&#19982;&#32773;&#30340;&#35805;&#35821;&#21487;&#33021;&#8220;&#24182;&#34892;&#8221;&#21457;&#29983;&#12290;&#38754;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26377;&#30740;&#31350;&#33268;&#21147;&#20110;&#24314;&#27169;&#35805;&#35821;&#25110;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#20415;&#20197;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#36825;&#20123;&#20851;&#31995;&#65292;&#37117;&#20551;&#35774;&#36825;&#20123;&#20851;&#31995;&#26159;&#39044;&#20808;&#32473;&#23450;&#30340;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#20063;&#22952;&#30861;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#24494;&#22937;&#32447;&#32034;&#36827;&#34892;&#20851;&#31995;&#25512;&#26029;&#65292;&#24341;&#23548;&#31070;&#32463;&#21709;&#24212;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20154;&#24037;&#26631;&#31614;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20851;&#31995;&#24605;&#32500;&#33258;&#21160;&#25512;&#26029;&#20986;&#36825;&#20123;&#20851;&#31995;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20851;&#31995;&#26469;&#25351;&#23548;&#31070;&#32463;&#21709;&#24212;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10827v1 Announce Type: new  Abstract: Existing neural response generation models have achieved impressive improvements for two-party conversations, which assume that utterances are sequentially organized. However, many real-world dialogues involve multiple interlocutors and the structure of conversational context is much more complex, e.g. utterances from different interlocutors can occur "in parallel". Facing this challenge, there are works trying to model the relations among utterances or interlocutors to facilitate response generation with clearer context. Nonetheless, these methods rely heavily on such relations and all assume that these are given beforehand, which is impractical and hinders the generality of such methods. In this work, we propose to automatically infer the relations via relational thinking on subtle clues inside the conversation context without any human label, and leverage these relations to guide the neural response generation. Specifically, we first 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#21307;&#23398;&#32534;&#30721;&#30340;&#21547;&#20041;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#35748;&#35782;&#21644;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.10822</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#21307;&#23398;&#32534;&#30721;?
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models understand Medical Codes?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10822
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#21307;&#23398;&#32534;&#30721;&#30340;&#21547;&#20041;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#35748;&#35782;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#39318;&#35201;&#30446;&#26631;&#26159;&#31283;&#27493;&#26397;&#30528;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#36808;&#36827;&#65292;&#36825;&#20419;&#20351;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#30340;&#35780;&#20272;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#21327;&#21161;&#21508;&#31181;&#20219;&#21153;&#22823;&#22823;&#26377;&#30410;&#20110;&#20020;&#24202;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#26080;&#27861;&#20805;&#20998;&#24212;&#23545;&#30340;&#26597;&#35810;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#23481;&#26131;&#20135;&#29983;&#8220;&#24187;&#35273;&#8221;&#25110;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#24341;&#21457;&#20102;&#20851;&#27880;&#21644;&#24576;&#30097;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#31038;&#21306;&#20869;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;LLMs&#26159;&#21542;&#29702;&#35299;&#21307;&#23398;&#32534;&#30721;&#30340;&#22266;&#26377;&#21547;&#20041;&#65292;&#36825;&#20123;&#32534;&#30721;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#23454;&#36341;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#29616;&#25104;&#30340;LLMs (&#20363;&#22914;GPT&#12289;LLaMA&#31561;)&#21644;&#19987;&#38376;&#20026;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#35774;&#35745;&#30340;LLMs&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#36825;&#20123;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#35748;&#35782;&#21644;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10822v1 Announce Type: new  Abstract: The overarching goal of recent AI research has been to make steady progress towards achieving Artificial General Intelligence (AGI), prompting the evaluation of Large Language Models (LLMs) across a variety of tasks and domains. One such domain is healthcare, where LLMs can greatly benefit clinical practice by assisting with a wide range of tasks. However, these models are also prone to producing "hallucinations" or incorrect responses when faced with queries they cannot adequately address, raising concerns and skepticism, especially within the healthcare community. Therefore, in this work, we investigate whether LLMs understand the inherent meaning of medical codes, which are widely used in healthcare practice. We evaluate various off-the-shelf LLMs (e.g., GPT, LLaMA, etc.) and LLMs specifically designed for biomedical applications to assess their awareness and understanding of these domain-specific terminologies. Our results indicate t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.10799</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#34701;&#21512;&#39640;&#25928;&#21098;&#26525;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Pruning of Large Language Model with Adaptive Estimation Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29983;&#25104;&#24615;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#23548;&#33268;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#23427;&#20204;&#25104;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#36235;&#21183;&#21644;&#37325;&#22823;&#25361;&#25112;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;&#22797;&#26434;&#32467;&#26500;&#26102;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#24120;&#35265;&#30340;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#21098;&#26525;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#22797;&#26434;&#21644;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#65292;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#25152;&#26377;&#26041;&#38754;&#37117;&#26080;&#32541;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#21098;&#26525;&#26694;&#26550;&#20013;&#12290;&#19982;&#20027;&#27969;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#30340;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#24182;&#25552;&#20986;&#19968;&#31181;&#20351;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.10795</link><description>&lt;p&gt;
&#20174;&#21333;&#35789;&#21040;&#36335;&#24452;&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
From Words to Routes: Applying Large Language Models to Vehicle Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#30340;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#24182;&#25552;&#20986;&#19968;&#31181;&#20351;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65288;&#20363;&#22914;&#25805;&#20316;&#21644;&#23548;&#33322;&#65289;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20854;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#12290;LLMs&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#35753;&#25105;&#20204;&#24605;&#32771;&#65306;LLMs&#22312;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65288;VRPs&#65289;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#19977;&#27493;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;21&#31181;&#21333;&#36710;&#25110;&#22810;&#36710;&#36335;&#24452;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#22235;&#31181;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#25991;&#26412;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#27599;&#31181;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#20174;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#20195;&#30721;&#30340;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#23545;&#20110;GPT-4&#25928;&#26524;&#26368;&#20339;&#65292;&#23454;&#29616;&#20102;56%&#30340;&#21487;&#34892;&#24615;&#65292;40%&#30340;&#20248;&#21270;&#24615;&#21644;53%&#30340;&#25928;&#29575;&#12290;&#31532;&#19977;&#65292;&#22522;&#20110;&#35266;&#23519;&#21040;LLMs&#21487;&#33021;&#26080;&#27861;&#22312;&#21021;&#22987;&#23581;&#35797;&#20013;&#25552;&#20379;&#27491;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;LLMs&#33021;&#22815;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10795v1 Announce Type: cross  Abstract: LLMs have shown impressive progress in robotics (e.g., manipulation and navigation) with natural language task descriptions. The success of LLMs in these tasks leads us to wonder: What is the ability of LLMs to solve vehicle routing problems (VRPs) with natural language task descriptions? In this work, we study this question in three steps. First, we construct a dataset with 21 types of single- or multi-vehicle routing problems. Second, we evaluate the performance of LLMs across four basic prompt paradigms of text-to-code generation, each involving different types of text input. We find that the basic prompt paradigm, which generates code directly from natural language task descriptions, performs the best for GPT-4, achieving 56% feasibility, 40% optimality, and 53% efficiency. Third, based on the observation that LLMs may not be able to provide correct solutions at the initial attempt, we propose a framework that enables LLMs to refin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#29702;&#35299;&#21644;&#29983;&#25104;&#20013;&#22269;&#24189;&#40664;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#20856;&#25925;&#35866;&#35821;&#12290;&#20182;&#20204;&#37319;&#29992;&#26032;&#39062;&#30340;fine-tuning&#26041;&#27861;&#65292;&#21253;&#21547;&#34701;&#21512;&#25340;&#38899;&#23884;&#20837;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#20174;&#32780;&#25104;&#21151;&#29983;&#25104;&#24189;&#40664;&#24615;&#20856;&#25925;&#35866;&#35821;&#12290;</title><link>https://arxiv.org/abs/2403.10781</link><description>&lt;p&gt;
&#25506;&#32034;&#20013;&#22269;&#24189;&#40664;&#29983;&#25104;&#65306;&#20851;&#20110;&#20004;&#21477;&#20856;&#25925;&#35866;&#35821;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Chinese Humor Generation: A Study on Two-Part Allegorical Sayings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#29702;&#35299;&#21644;&#29983;&#25104;&#20013;&#22269;&#24189;&#40664;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#20856;&#25925;&#35866;&#35821;&#12290;&#20182;&#20204;&#37319;&#29992;&#26032;&#39062;&#30340;fine-tuning&#26041;&#27861;&#65292;&#21253;&#21547;&#34701;&#21512;&#25340;&#38899;&#23884;&#20837;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#20174;&#32780;&#25104;&#21151;&#29983;&#25104;&#24189;&#40664;&#24615;&#20856;&#25925;&#35866;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24189;&#40664;&#26159;&#20154;&#31867;&#35821;&#35328;&#20013;&#23500;&#26377;&#25991;&#21270;&#20869;&#28085;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#23545;&#20110;&#35745;&#31639;&#26426;&#29702;&#35299;&#21644;&#29983;&#25104;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#30456;&#23545;&#26410;&#34987;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25506;&#32034;&#30340;&#20013;&#22269;&#24189;&#40664;&#39046;&#22495;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20013;&#22269;&#24189;&#40664;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#32858;&#28966;&#20110;&#35757;&#32451;&#23427;&#20204;&#21019;&#36896;&#20856;&#25925;&#35866;&#35821;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#26174;&#33879;&#30340;&#35757;&#32451;&#26041;&#27861;&#65306;&#35843;&#25972;&#19968;&#20010;&#20013;&#31561;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#19968;&#20010;&#22823;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#35843;&#25972;&#26041;&#27861;&#34701;&#21512;&#20102;&#25340;&#38899;&#23884;&#20837;&#20197;&#32771;&#34385;&#21516;&#38899;&#24322;&#20041;&#35789;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#19982;&#21512;&#25104;&#22256;&#38590;&#24615;&#36127;&#20363;&#20197;&#21306;&#20998;&#24189;&#40664;&#20803;&#32032;&#12290;&#20154;&#31867;&#27880;&#37322;&#30340;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#24189;&#40664;&#30340;&#20856;&#25925;&#35866;&#35821;&#65292;&#25552;&#31034;&#27861;&#35777;&#26126;&#26159;&#19968;&#20010;&#23454;&#29992;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#19982;&#20154;&#31867;&#21019;&#36896;&#21147;&#21305;&#37197;&#30340;&#20856;&#25925;&#35866;&#35821;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10781v1 Announce Type: cross  Abstract: Humor, a culturally nuanced aspect of human language, poses challenges for computational understanding and generation, especially in Chinese humor, which remains relatively unexplored in the NLP community. This paper investigates the capability of state-of-the-art language models to comprehend and generate Chinese humor, specifically focusing on training them to create allegorical sayings. We employ two prominent training methods: fine-tuning a medium-sized language model and prompting a large one. Our novel fine-tuning approach incorporates fused Pinyin embeddings to consider homophones and employs contrastive learning with synthetic hard negatives to distinguish humor elements. Human-annotated results show that these models can generate humorous allegorical sayings, with prompting proving to be a practical and effective method. However, there is still room for improvement in generating allegorical sayings that match human creativity.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#21644;&#26234;&#33021;&#35774;&#22791;&#30340;&#23545;&#35805;&#24335;AI&#27835;&#30103;&#24072;&#65292;CaiTI&#24179;&#21488;&#65292;&#21487;&#36890;&#36807;&#33258;&#28982;&#21644;&#24515;&#29702;&#27835;&#30103;&#23545;&#35805;&#26469;&#31579;&#26597;&#26085;&#24120;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#23545;&#35805;&#27969;&#31243;&#21644;&#24515;&#29702;&#27835;&#30103;&#24178;&#39044;&#12290;</title><link>https://arxiv.org/abs/2403.10779</link><description>&lt;p&gt;
LLM-based Conversational AI Therapist for Daily Functioning Screening and Psychotherapeutic Intervention via Everyday Smart Devices
&lt;/p&gt;
&lt;p&gt;
LLM-based Conversational AI Therapist for Daily Functioning Screening and Psychotherapeutic Intervention via Everyday Smart Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10779
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#21644;&#26234;&#33021;&#35774;&#22791;&#30340;&#23545;&#35805;&#24335;AI&#27835;&#30103;&#24072;&#65292;CaiTI&#24179;&#21488;&#65292;&#21487;&#36890;&#36807;&#33258;&#28982;&#21644;&#24515;&#29702;&#27835;&#30103;&#23545;&#35805;&#26469;&#31579;&#26597;&#26085;&#24120;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#23545;&#35805;&#27969;&#31243;&#21644;&#24515;&#29702;&#27835;&#30103;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20840;&#29699;&#23384;&#22312;&#24515;&#29702;&#20581;&#24247;&#21361;&#26426;&#65292;&#20294;&#25509;&#21463;&#31579;&#26597;&#12289;&#19987;&#19994;&#20154;&#22763;&#21644;&#27835;&#30103;&#30340;&#26426;&#20250;&#20173;&#24456;&#39640;&#12290;&#19982;&#25345;&#29260;&#24515;&#29702;&#27835;&#30103;&#24072;&#21512;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#26234;&#33021;&#35774;&#22791;&#23454;&#29616;&#26356;&#22909;&#24515;&#29702;&#20581;&#24247;&#33258;&#25105;&#25252;&#29702;&#30340;&#23545;&#35805;AI&#27835;&#30103;&#24072;&#65292;&#21363;CaiTI&#24179;&#21488;&#12290;CaiTI&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#21644;&#24515;&#29702;&#27835;&#30103;&#23545;&#35805;&#26469;&#31579;&#26597;&#26085;&#24120;&#21151;&#33021;&#12290;CaiTI&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20010;&#24615;&#21270;&#23545;&#35805;&#27969;&#31243;&#12290;CaiTI&#33021;&#22815;&#20934;&#30830;&#29702;&#35299;&#21644;&#35299;&#37322;&#29992;&#25143;&#30340;&#22238;&#24212;&#12290;&#24403;&#29992;&#25143;&#22312;&#23545;&#35805;&#20013;&#38656;&#35201;&#36827;&#19968;&#27493;&#20851;&#27880;&#26102;&#65292;CaiTI&#21487;&#20197;&#25552;&#20379;&#23545;&#35805;&#24335;&#24515;&#29702;&#27835;&#30103;&#24178;&#39044;&#65292;&#21253;&#25324;&#35748;&#30693;&#34892;&#20026;&#27835;&#30103;&#65288;CBT&#65289;&#21644;&#28608;&#21169;&#24615;&#35775;&#35848;&#65288;MI&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#25345;&#29260;&#24515;&#29702;&#27835;&#30103;&#24072;&#20934;&#22791;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;LLM&#22312;Cai&#27839;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#23454;&#39564;&#21644;&#24494;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10779v1 Announce Type: new  Abstract: Despite the global mental health crisis, access to screenings, professionals, and treatments remains high. In collaboration with licensed psychotherapists, we propose a Conversational AI Therapist with psychotherapeutic Interventions (CaiTI), a platform that leverages large language models (LLM)s and smart devices to enable better mental health self-care. CaiTI can screen the day-to-day functioning using natural and psychotherapeutic conversations. CaiTI leverages reinforcement learning to provide personalized conversation flow. CaiTI can accurately understand and interpret user responses. When the user needs further attention during the conversation, CaiTI can provide conversational psychotherapeutic interventions, including cognitive behavioral therapy (CBT) and motivational interviewing (MI). Leveraging the datasets prepared by the licensed psychotherapists, we experiment and microbenchmark various LLMs' performance in tasks along Cai
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#36890;&#36807;&#20351;&#29992;KcBERT&#21644;KOLD&#25968;&#25454;&#23545;&#38889;&#35821;&#35780;&#35770;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21457;&#29616;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#38477;&#20302;&#20102;&#31181;&#26063;&#20559;&#35265;&#65292;&#20294;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.10774</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65306;Fine-tuned KcBERT
&lt;/p&gt;
&lt;p&gt;
Detecting Bias in Large Language Models: Fine-tuned KcBERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10774
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#36890;&#36807;&#20351;&#29992;KcBERT&#21644;KOLD&#25968;&#25454;&#23545;&#38889;&#35821;&#35780;&#35770;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21457;&#29616;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#38477;&#20302;&#20102;&#31181;&#26063;&#20559;&#35265;&#65292;&#20294;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;LLMs&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25945;&#32946;&#21644;&#21307;&#30103;&#31561;&#21508;&#20010;&#31038;&#20250;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#20102;&#65292;&#20294;&#23427;&#20204;&#26377;&#21487;&#33021;&#20135;&#29983;&#20027;&#35266;&#21644;&#35268;&#33539;&#24615;&#35821;&#35328;&#65292;&#23548;&#33268;&#22312;&#31038;&#20250;&#32676;&#20307;&#20013;&#20986;&#29616;&#27495;&#35270;&#24615;&#23545;&#24453;&#25110;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#22312;&#32447;&#20398;&#36785;&#24615;&#35821;&#35328;&#12290;&#26412;&#25991;&#23558;&#27492;&#31867;&#20260;&#23475;&#23450;&#20041;&#20026;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;&#20351;&#29992;KcBERT&#21644;KOLD&#25968;&#25454;&#36890;&#36807;&#22522;&#20110;&#27169;&#26495;&#30340;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#23545;&#38889;&#35821;&#35780;&#35770;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#12290;&#20026;&#20102;&#23450;&#37327;&#35780;&#20272;&#20559;&#35265;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;LPBS&#21644;CBS&#25351;&#26631;&#12290;&#19982;KcBERT&#30456;&#27604;&#65292;&#24494;&#35843;&#27169;&#22411;&#20943;&#23569;&#20102;&#31181;&#26063;&#20559;&#35265;&#65292;&#20294;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10774v1 Announce Type: new  Abstract: The rapid advancement of large language models (LLMs) has enabled natural language processing capabilities similar to those of humans, and LLMs are being widely utilized across various societal domains such as education and healthcare. While the versatility of these models has increased, they have the potential to generate subjective and normative language, leading to discriminatory treatment or outcomes among social groups, especially due to online offensive language. In this paper, we define such harm as societal bias and assess ethnic, gender, and racial biases in a model fine-tuned with Korean comments using Bidirectional Encoder Representations from Transformers (KcBERT) and KOLD data through template-based Masked Language Modeling (MLM). To quantitatively evaluate biases, we employ LPBS and CBS metrics. Compared to KcBERT, the fine-tuned model shows a reduction in ethnic bias but demonstrates significant changes in gender and racia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ECRC&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#21333;&#35789;&#32423;&#21644;&#21477;&#23376;&#32423;&#23884;&#20837;&#65292;&#20197;&#21450;&#22522;&#20110;&#26032;&#39062;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#22312;&#38889;&#22269;&#20250;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#24773;&#32490;&#22240;&#26524;&#35782;&#21035;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.10764</link><description>&lt;p&gt;
&#38889;&#22269;&#20250;&#35805;&#20013;&#24773;&#32490;&#22240;&#26524;&#35782;&#21035;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ECRC: Emotion-Causality Recognition in Korean Conversation for GCN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ECRC&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#21333;&#35789;&#32423;&#21644;&#21477;&#23376;&#32423;&#23884;&#20837;&#65292;&#20197;&#21450;&#22522;&#20110;&#26032;&#39062;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#22312;&#38889;&#22269;&#20250;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#24773;&#32490;&#22240;&#26524;&#35782;&#21035;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#22810;&#20219;&#21153;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21516;&#26102;&#20998;&#26512;&#20102;&#20250;&#35805;&#29615;&#22659;&#20013;&#30340;&#24773;&#24863;&#21450;&#20854;&#28508;&#22312;&#21407;&#22240;&#65292;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26377;&#25928;&#22788;&#29702;&#21644;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20811;&#26381;&#20102;&#20197;&#24448;&#23884;&#20837;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#21033;&#29992;&#20102;&#21333;&#35789;&#32423;&#21644;&#21477;&#23376;&#32423;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26032;&#39062;&#22270;&#32467;&#26500;&#30340;&#24773;&#32490;&#22240;&#26524;&#35782;&#21035;&#27169;&#22411;&#65288;ECRC&#65289;&#65292;&#20174;&#32780;&#20511;&#37492;&#20102;&#20004;&#31181;&#23884;&#20837;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#35813;&#27169;&#22411;&#29420;&#29305;&#22320;&#38598;&#25104;&#20102;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;Bi-LSTM&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10764v1 Announce Type: cross  Abstract: In this multi-task learning study on simultaneous analysis of emotions and their underlying causes in conversational contexts, deep neural network methods were employed to effectively process and train large labeled datasets. However, these approaches are typically limited to conducting context analyses across the entire corpus because they rely on one of the two methods: word- or sentence-level embedding. The former struggles with polysemy and homonyms, whereas the latter causes information loss when processing long sentences. In this study, we overcome the limitations of previous embeddings by utilizing both word- and sentence-level embeddings. Furthermore, we propose the emotion-causality recognition in conversation (ECRC) model, which is based on a novel graph structure, thereby leveraging the strengths of both embedding methods. This model uniquely integrates the bidirectional long short-term memory (Bi-LSTM) and graph neural netw
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20013;&#25991;&#25991;&#26412;&#30340;&#21019;&#26032;&#22411;OIE&#27169;&#22411;APRCOIE&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#25277;&#21462;&#27169;&#24335;&#12289;&#23450;&#20041;&#26032;&#30340;&#27169;&#24335;&#24418;&#24335;&#20197;&#21450;&#35774;&#35745;&#24352;&#37327;&#35745;&#31639;&#36807;&#28388;&#22120;&#31561;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#22797;&#26434;&#20013;&#25991;&#35821;&#27861;&#29616;&#35937;&#30340;&#22788;&#29702;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#25299;&#23637;&#20102;OIE&#24615;&#33021;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.10758</link><description>&lt;p&gt;
&#35268;&#21017;&#20173;&#28982;&#36866;&#29992;&#20110;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Rules still work for Open Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10758
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20013;&#25991;&#25991;&#26412;&#30340;&#21019;&#26032;&#22411;OIE&#27169;&#22411;APRCOIE&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#25277;&#21462;&#27169;&#24335;&#12289;&#23450;&#20041;&#26032;&#30340;&#27169;&#24335;&#24418;&#24335;&#20197;&#21450;&#35774;&#35745;&#24352;&#37327;&#35745;&#31639;&#36807;&#28388;&#22120;&#31561;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#22797;&#26434;&#20013;&#25991;&#35821;&#27861;&#29616;&#35937;&#30340;&#22788;&#29702;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#25299;&#23637;&#20102;OIE&#24615;&#33021;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#65288;OIE&#65289;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#25552;&#21462;&#34920;&#38754;&#20851;&#31995;&#21450;&#20854;&#23545;&#24212;&#30340;&#21442;&#25968;&#65292;&#32780;&#19981;&#32771;&#34385;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20013;&#25991;&#25991;&#26412;&#30340;&#21019;&#26032;&#22411;OIE&#27169;&#22411;APRCOIE&#12290;&#19982;&#20808;&#21069;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#20027;&#29983;&#25104;&#25277;&#21462;&#27169;&#24335;&#12290;&#35813;&#27169;&#22411;&#20026;&#20013;&#25991;OIE&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#24335;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#30340;&#27169;&#24335;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#22810;&#26679;&#22797;&#26434;&#30340;&#20013;&#25991;&#35821;&#27861;&#29616;&#35937;&#12290;&#25105;&#20204;&#22522;&#20110;&#24352;&#37327;&#35745;&#31639;&#35774;&#35745;&#20102;&#19968;&#20010;&#21021;&#27493;&#36807;&#28388;&#22120;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#36827;&#34892;&#25277;&#21462;&#36807;&#31243;&#12290;&#20026;&#20102;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25163;&#21160;&#26631;&#27880;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20013;&#25991;OIE&#25968;&#25454;&#38598;&#12290;&#22312;&#27604;&#36739;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;APRCOIE&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20013;&#25991;OIE&#27169;&#22411;&#65292;&#24182;&#26174;&#33879;&#25299;&#23637;&#20102;&#21487;&#23454;&#29616;&#30340;OIE&#24615;&#33021;&#36793;&#30028;&#12290;APRCOIE&#30340;&#20195;&#30721;&#21644;&#26631;&#27880;&#25968;&#25454;&#38598;&#24050;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10758v1 Announce Type: new  Abstract: Open information extraction (OIE) aims to extract surface relations and their corresponding arguments from natural language text, irrespective of domain. This paper presents an innovative OIE model, APRCOIE, tailored for Chinese text. Diverging from previous models, our model generates extraction patterns autonomously. The model defines a new pattern form for Chinese OIE and proposes an automated pattern generation methodology. In that way, the model can handle a wide array of complex and diverse Chinese grammatical phenomena. We design a preliminary filter based on tensor computing to conduct the extraction procedure efficiently. To train the model, we manually annotated a large-scale Chinese OIE dataset. In the comparative evaluation, we demonstrate that APRCOIE outperforms state-of-the-art Chinese OIE models and significantly expands the boundaries of achievable OIE performance. The code of APRCOIE and the annotated dataset are releas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21517;&#20026;DORIS&#30340;&#26032;&#22411;&#25233;&#37057;&#30151;&#26816;&#27979;&#31995;&#32479;&#65292;&#23558;&#21307;&#23398;&#30693;&#35782;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20998;&#26512;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24086;&#23376;&#21382;&#21490;&#35760;&#24405;&#26469;&#30830;&#23450;&#25233;&#37057;&#30151;&#24739;&#32773;&#65292;&#20197;&#25552;&#39640;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#12290;</title><link>https://arxiv.org/abs/2403.10750</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Depression Detection on Social Media with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10750
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21517;&#20026;DORIS&#30340;&#26032;&#22411;&#25233;&#37057;&#30151;&#26816;&#27979;&#31995;&#32479;&#65292;&#23558;&#21307;&#23398;&#30693;&#35782;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20998;&#26512;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24086;&#23376;&#21382;&#21490;&#35760;&#24405;&#26469;&#30830;&#23450;&#25233;&#37057;&#30151;&#24739;&#32773;&#65292;&#20197;&#25552;&#39640;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#36896;&#25104;&#21361;&#23475;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#24515;&#29702;&#20581;&#24247;&#24847;&#35782;&#21644;&#23545;&#30149;&#30151;&#32827;&#36785;&#24863;&#30340;&#24656;&#24807;&#65292;&#35768;&#22810;&#24739;&#32773;&#24182;&#26410;&#31215;&#26497;&#23547;&#27714;&#35786;&#26029;&#21644;&#27835;&#30103;&#65292;&#23548;&#33268;&#19981;&#21033;&#21518;&#26524;&#12290;&#25233;&#37057;&#30151;&#26816;&#27979;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#20010;&#20154;&#24086;&#23376;&#30340;&#21382;&#21490;&#35760;&#24405;&#26469;&#30830;&#23450;&#20010;&#20307;&#26159;&#21542;&#24739;&#26377;&#25233;&#37057;&#30151;&#65292;&#36825;&#21487;&#26174;&#33879;&#26377;&#21161;&#20110;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#12290;&#23427;&#20027;&#35201;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;1&#65289;&#38656;&#35201;&#19987;&#19994;&#21307;&#23398;&#30693;&#35782;&#65292;2&#65289;&#38656;&#35201;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DORIS&#30340;&#26032;&#22411;&#25233;&#37057;&#30151;&#26816;&#27979;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#21307;&#23398;&#30693;&#35782;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#39318;&#20808;&#23545;&#39640;&#21361;&#25991;&#26412;&#36827;&#34892;&#26631;&#27880;&#20197;&#30830;&#23450;&#26159;&#21542;&#31526;&#21512;&#21307;&#23398;&#35786;&#26029;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10750v1 Announce Type: cross  Abstract: Depression harms. However, due to a lack of mental health awareness and fear of stigma, many patients do not actively seek diagnosis and treatment, leading to detrimental outcomes. Depression detection aims to determine whether an individual suffers from depression by analyzing their history of posts on social media, which can significantly aid in early detection and intervention. It mainly faces two key challenges: 1) it requires professional medical knowledge, and 2) it necessitates both high accuracy and explainability. To address it, we propose a novel depression detection system called DORIS, combining medical knowledge and the recent advances in large language models (LLMs). Specifically, to tackle the first challenge, we proposed an LLM-based solution to first annotate whether high-risk texts meet medical diagnostic criteria. Further, we retrieve texts with high emotional intensity and summarize critical information from the his
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21151;&#33021;&#65292;&#20197;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#65292;&#22788;&#29702;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10707</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#38598;&#25104;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#30340;&#28508;&#22312;&#20027;&#39064;&#65306;&#27668;&#20505;&#36816;&#21160;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21151;&#33021;&#65292;&#20197;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#65292;&#22788;&#29702;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25581;&#31034;&#21644;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#37492;&#20110;&#20256;&#32479;&#20027;&#39064;&#32423;&#20998;&#26512;&#30340;&#23616;&#38480;&#24615;&#65292;&#24448;&#24448;&#21482;&#25429;&#25417;&#21040;&#25972;&#20307;&#27169;&#24335;&#65292;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#23545;&#26356;&#31934;&#32454;&#12289;&#20027;&#39064;&#32858;&#28966;&#30340;&#25506;&#32034;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#20027;&#39064;&#21457;&#29616;&#26041;&#27861;&#65292;&#28041;&#21450;&#25163;&#21160;&#27969;&#31243;&#21644;&#20154;&#22312;&#24490;&#29615;&#20013;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#22312;&#20280;&#32553;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#36164;&#28304;&#24378;&#24230;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#28041;&#21450;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20808;&#36827;&#21151;&#33021;&#30340;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#26356;&#28145;&#20837;&#22320;&#35843;&#26597;&#31038;&#20132;&#23186;&#20307;&#35805;&#35821;&#30340;&#20027;&#39064;&#26041;&#38754;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25581;&#31034;&#22810;&#26679;&#30340;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#26356;&#24191;&#27867;&#20027;&#39064;&#20869;&#26377;&#30340;&#24494;&#22937;&#32454;&#33410;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10707v1 Announce Type: cross  Abstract: This paper introduces a novel approach to uncovering and analyzing themes in social media messaging. Recognizing the limitations of traditional topic-level analysis, which tends to capture only the overarching patterns, this study emphasizes the need for a finer-grained, theme-focused exploration. Conventional methods of theme discovery, involving manual processes and a human-in-the-loop approach, are valuable but face challenges in scalability, consistency, and resource intensity in terms of time and cost. To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs). This approach allows for a deeper investigation into the thematic aspects of social media discourse, enabling us to uncover a diverse array of themes, each with unique characteristics and relevance, thereby offering a comprehensive understanding of the nuances present within broader topics.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#65288;PERL&#65289;&#65292;&#33021;&#22815;&#22312;&#19982;&#20256;&#32479;RLHF&#35774;&#32622;&#30456;&#24403;&#30340;&#24615;&#33021;&#19979;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.10704</link><description>&lt;p&gt;
PERL: &#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PERL: Parameter Efficient Reinforcement Learning from Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10704
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#65288;PERL&#65289;&#65292;&#33021;&#22815;&#22312;&#19982;&#20256;&#32479;RLHF&#35774;&#32622;&#30456;&#24403;&#30340;&#24615;&#33021;&#19979;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;RLHF&#35757;&#32451;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#19988;&#25972;&#20010;&#36807;&#31243;&#22797;&#26434;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;RLHF&#65292;&#20854;&#20013;&#22522;&#30784;&#27169;&#22411;&#20351;&#29992;&#32993;&#31561;&#20154;&#25552;&#20986;&#30340;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#8220;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#8221;&#65288;PERL&#65289;&#30340;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;LoRA&#36827;&#34892;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#23558;PERL&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#65288;&#20840;&#35843;&#65289;&#22312;&#21253;&#25324;2&#20010;&#26032;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;7&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#22870;&#21169;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#30340;&#21508;&#31181;&#37197;&#32622;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;PERL&#30340;&#24615;&#33021;&#19982;&#20256;&#32479;&#30340;RLHF&#35774;&#32622;&#30456;&#24403;&#65292;&#21516;&#26102;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#65292;&#20869;&#23384;&#21344;&#29992;&#26356;&#23569;&#12290;&#36825;&#20351;&#24471;RLHF&#20855;&#26377;&#24456;&#39640;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10704v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong method to align Pretrained Large Language Models (LLMs) with human preferences. But training models with RLHF is computationally expensive, and an overall complex process. In this work, we study RLHF where the underlying models are trained using the parameter efficient method of Low-Rank Adaptation (LoRA) introduced by Hu et al. [2021]. We investigate the setup of "Parameter Efficient Reinforcement Learning" (PERL), in which we perform reward model training and reinforcement learning using LoRA. We compare PERL to conventional fine-tuning (full-tuning) across various configurations for 7 benchmarks, including 2 novel datasets, of reward modeling and reinforcement learning. We find that PERL performs on par with the conventional RLHF setting, while training faster, and with less memory. This enables the high performance of RLHF, while reducing the computational 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25351;&#20196;&#38169;&#35823;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#30340;&#20154;&#31867;&#21407;&#22240;&#65292;&#20197;&#35780;&#20272;&#36830;&#32493;&#29615;&#22659;&#20013; VLN &#31995;&#32479;&#30340;&#20581;&#22766;&#24615;</title><link>https://arxiv.org/abs/2403.10700</link><description>&lt;p&gt;
&#27880;&#24847;&#38169;&#35823;&#65281;&#26816;&#27979;&#21644;&#23450;&#20301;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#25351;&#20196;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10700
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25351;&#20196;&#38169;&#35823;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#30340;&#20154;&#31867;&#21407;&#22240;&#65292;&#20197;&#35780;&#20272;&#36830;&#32493;&#29615;&#22659;&#20013; VLN &#31995;&#32479;&#30340;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision-and-Language Navigation in Continuous Environments (VLN-CE) &#26159;&#19968;&#39033;&#30452;&#35266;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20307;&#39564;&#26234;&#33021;&#20219;&#21153;&#12290;&#20195;&#29702;&#20154;&#34987;&#35201;&#27714;&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#20302;&#32423;&#21160;&#20316;&#12289;&#36981;&#24490;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#23548;&#33322;&#21040;&#30446;&#26631;&#30446;&#26631;&#12290;&#25152;&#26377;&#25991;&#29486;&#20013;&#30340; VLN-CE &#26041;&#27861;&#37117;&#20551;&#35774;&#35821;&#35328;&#25351;&#20196;&#26159;&#20934;&#30830;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20154;&#31867;&#32473;&#20986;&#30340;&#25351;&#20196;&#21487;&#33021;&#30001;&#20110;&#19981;&#20934;&#30830;&#30340;&#35760;&#24518;&#25110;&#28151;&#28102;&#32780;&#21253;&#21547;&#31354;&#38388;&#29615;&#22659;&#25551;&#36848;&#20013;&#30340;&#38169;&#35823;&#12290;&#24403;&#21069; VLN-CE &#22522;&#20934;&#27809;&#26377;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#65292;&#20351;&#24471; VLN-CE &#20013;&#30340;&#26368;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#26469;&#33258;&#20154;&#31867;&#29992;&#25143;&#30340;&#38169;&#35823;&#25351;&#20196;&#26102;&#21464;&#24471;&#33030;&#24369;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#20837;&#21508;&#31181;&#31867;&#22411;&#25351;&#20196;&#38169;&#35823;&#32771;&#34385;&#28508;&#22312;&#20154;&#31867;&#21407;&#22240;&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#20026;&#36830;&#32493;&#29615;&#22659;&#20013;&#30340; VLN &#31995;&#32479;&#30340;&#20581;&#22766;&#24615;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040; noticeable...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10700v1 Announce Type: cross  Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of the most intuitive yet challenging embodied AI tasks. Agents are tasked to navigate towards a target goal by executing a set of low-level actions, following a series of natural language instructions. All VLN-CE methods in the literature assume that language instructions are exact. However, in practice, instructions given by humans can contain errors when describing a spatial environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do not address this scenario, making the state-of-the-art methods in VLN-CE fragile in the presence of erroneous instructions from human users. For the first time, we propose a novel benchmark dataset that introduces various types of instruction errors considering potential human causes. This benchmark provides valuable insight into the robustness of VLN systems in continuous environments. We observe a noticeable 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#35821;&#35328;&#35270;&#35282;&#25506;&#32034;&#24615;&#21035;&#20559;&#35265;&#30340;&#34920;&#36798;&#26041;&#24335;&#65292;&#24378;&#35843;&#20102;&#20102;&#35299;&#31038;&#20250;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10699</link><description>&lt;p&gt;
&#23545;&#25506;&#27979;&#24615;&#21035;&#20559;&#35265;&#30340;&#22810;&#35821;&#35328;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Multilingual Perspective on Probing Gender Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#35821;&#35328;&#35270;&#35282;&#25506;&#32034;&#24615;&#21035;&#20559;&#35265;&#30340;&#34920;&#36798;&#26041;&#24335;&#65292;&#24378;&#35843;&#20102;&#20102;&#35299;&#31038;&#20250;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#21035;&#20559;&#35265;&#20195;&#34920;&#30528;&#19968;&#31181;&#38024;&#23545;&#20010;&#20307;&#22522;&#20110;&#20854;&#24615;&#21035;&#30340;&#31995;&#32479;&#24615;&#36127;&#38754;&#23545;&#24453;&#12290;&#36825;&#31181;&#27495;&#35270;&#21487;&#20197;&#20174;&#24494;&#22937;&#30340;&#24615;&#21035;&#20027;&#20041;&#35328;&#35770;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#65292;&#21040;&#30452;&#25509;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25581;&#31034;&#65292;&#24573;&#35270;&#22312;&#32447;&#34384;&#24453;&#19981;&#20165;&#20250;&#24433;&#21709;&#21463;&#23475;&#20010;&#20307;&#65292;&#36824;&#20250;&#23545;&#26356;&#24191;&#27867;&#30340;&#31038;&#20250;&#20135;&#29983;&#24433;&#21709;&#12290;&#36825;&#20123;&#21518;&#26524;&#24310;&#20280;&#33267;&#38459;&#25376;&#22919;&#22899;&#21442;&#19982;&#21644;&#22312;&#20844;&#20849;&#39046;&#22495;&#20013;&#30340;&#21487;&#35265;&#24230;&#65292;&#20174;&#32780;&#24378;&#21270;&#24615;&#21035;&#19981;&#24179;&#31561;&#12290;&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#24615;&#21035;&#20559;&#35265;&#22914;&#20309;&#36890;&#36807;&#35821;&#35328;&#21450;&#35821;&#35328;&#25216;&#26415;&#34920;&#36798;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#29305;&#21035;&#26159;&#65292;&#26412;&#35770;&#25991;&#23558;&#30740;&#31350;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#32972;&#26223;&#19979;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24378;&#35843;&#20102;&#20174;&#22810;&#35821;&#35328;&#21644;&#22810;&#25991;&#21270;&#35270;&#35282;&#29702;&#35299;&#31038;&#20250;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#37319;&#21462;&#20102;&#36328;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;&#25919;&#27835;&#23398;&#31561;&#20854;&#20182;&#23398;&#31185;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10699v1 Announce Type: new  Abstract: Gender bias represents a form of systematic negative treatment that targets individuals based on their gender. This discrimination can range from subtle sexist remarks and gendered stereotypes to outright hate speech. Prior research has revealed that ignoring online abuse not only affects the individuals targeted but also has broader societal implications. These consequences extend to the discouragement of women's engagement and visibility within public spheres, thereby reinforcing gender inequality. This thesis investigates the nuances of how gender bias is expressed through language and within language technologies. Significantly, this thesis expands research on gender bias to multilingual contexts, emphasising the importance of a multilingual and multicultural perspective in understanding societal biases. In this thesis, I adopt an interdisciplinary approach, bridging natural language processing with other disciplines such as politica
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EXPLORER&#30340;&#25506;&#32034;&#24341;&#23548;&#25512;&#29702;&#20195;&#29702;&#65292;&#29992;&#20110;&#25991;&#26412;&#24378;&#21270;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#26234;&#33021;&#20307;&#22312;&#22810;&#20010;&#28216;&#25103;&#20013;&#27867;&#21270;&#24182;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#23545;&#35937;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.10692</link><description>&lt;p&gt;
EXPLORER&#65306;&#25506;&#32034;&#24341;&#23548;&#30340;&#25991;&#26412;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EXPLORER: Exploration-guided Reasoning for Textual Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10692
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EXPLORER&#30340;&#25506;&#32034;&#24341;&#23548;&#25512;&#29702;&#20195;&#29702;&#65292;&#29992;&#20110;&#25991;&#26412;&#24378;&#21270;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#26234;&#33021;&#20307;&#22312;&#22810;&#20010;&#28216;&#25103;&#20013;&#27867;&#21270;&#24182;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#23545;&#35937;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#28216;&#25103;&#65288;TBGs&#65289;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#19968;&#20010;&#37325;&#35201;&#38598;&#21512;&#65292;&#35201;&#27714;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EXPLORER&#30340;&#25506;&#32034;&#24341;&#23548;&#25512;&#29702;&#20195;&#29702;&#65292;&#29992;&#20110;&#25991;&#26412;&#24378;&#21270;&#23398;&#20064;&#65292;&#26088;&#22312;&#35299;&#20915;&#26234;&#33021;&#20307;&#22312;&#22810;&#20010;&#28216;&#25103;&#20013;&#27867;&#21270;&#24182;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#23545;&#35937;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10692v1 Announce Type: cross  Abstract: Text-based games (TBGs) have emerged as an important collection of NLP tasks, requiring reinforcement learning (RL) agents to combine natural language understanding with reasoning. A key challenge for agents attempting to solve such tasks is to generalize across multiple games and demonstrate good performance on both seen and unseen objects. Purely deep-RL-based approaches may perform well on seen objects; however, they fail to showcase the same performance on unseen objects. Commonsense-infused deep-RL agents may work better on unseen data; unfortunately, their policies are often not interpretable or easily transferable. To tackle these issues, in this paper, we present EXPLORER which is an exploration-guided reasoning agent for textual reinforcement learning. EXPLORER is neurosymbolic in nature, as it relies on a neural module for exploration and a symbolic module for exploitation. It can also learn generalized symbolic policies and 
&lt;/p&gt;</description></item><item><title>MYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#24418;&#24577;&#23398;&#30340;&#23383;&#33410;&#32534;&#30721;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#23454;&#29616;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#20026;99&#31181;&#35821;&#35328;&#25552;&#20379;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2403.10691</link><description>&lt;p&gt;
MYTE&#65306;&#24418;&#24577;&#23398;&#39537;&#21160;&#30340;&#23383;&#33410;&#32534;&#30721;&#65292;&#29992;&#20110;&#26356;&#22909;&#12289;&#26356;&#20844;&#24179;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10691
&lt;/p&gt;
&lt;p&gt;
MYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#24418;&#24577;&#23398;&#30340;&#23383;&#33410;&#32534;&#30721;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#23454;&#29616;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#20026;99&#31181;&#35821;&#35328;&#25552;&#20379;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#32771;&#34385;&#22240;&#32032;&#26159;&#22914;&#20309;&#26368;&#22909;&#22320;&#34920;&#31034;&#20855;&#26377;&#19981;&#21516;&#35789;&#27719;&#21644;&#25991;&#23383;&#30340;&#35821;&#35328;&#12290;&#23613;&#31649;&#24403;&#20195;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#28085;&#30422;&#20102;&#22823;&#22810;&#25968;&#19990;&#30028;&#25991;&#23383;&#31995;&#32479;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20559;&#21521;&#20110;&#20840;&#29699;&#35199;&#26041;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23569;&#25968;&#35821;&#35328;&#30340;&#25991;&#26412;&#24448;&#24448;&#34987;&#20998;&#21106;&#20026;&#19968;&#38271;&#20018;&#22312;&#35821;&#35328;&#23398;&#19978;&#27627;&#26080;&#24847;&#20041;&#30340;&#21333;&#20803;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#19981;&#24179;&#31561;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#29992;&#36328;&#19981;&#21516;&#35821;&#35328;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#32534;&#30721;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#32534;&#30721;&#32422;&#23450;&#65288;MYTE&#65289;&#22522;&#20110;&#24418;&#24577;&#32032;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24211;&#23384;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#27604;&#23383;&#31526;&#26356;&#24179;&#34913;&#65292;&#32780;&#20197;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#23383;&#31526;&#12290;&#25105;&#20204;&#23637;&#31034;MYTE&#20026;&#25152;&#26377;99&#31181;&#20998;&#26512;&#35821;&#35328;&#20135;&#29983;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#20854;&#20013;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;&#36825;&#36827;&#32780;&#25913;&#21892;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10691v1 Announce Type: cross  Abstract: A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts. Although contemporary text encoding methods cover most of the world's writing systems, they exhibit bias towards the high-resource languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10667</link><description>&lt;p&gt;
&#36890;&#21521;&#32479;&#19968;&#22810;&#27169;&#24335;&#20010;&#24615;&#21270;&#65306;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#25512;&#33616;&#21644;&#26356;&#22810;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#24322;&#26500;&#36164;&#28304;&#24182;&#28385;&#36275;&#21508;&#31181;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#36890;&#29992;&#27169;&#22411;&#19968;&#30452;&#26159;&#31038;&#21306;&#28212;&#26395;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#26085;&#24120;&#30340;&#36873;&#25321;&#65292;&#23588;&#20854;&#26159;&#22312;&#26102;&#23578;&#21644;&#38646;&#21806;&#31561;&#39046;&#22495;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#27604;&#22914;&#22270;&#29255;&#21644;&#25991;&#26412;&#25551;&#36848;&#12290;&#36825;&#20123;&#27169;&#24577;&#19981;&#20165;&#25552;&#20379;&#30452;&#35266;&#30340;&#25351;&#23548;&#65292;&#36824;&#36814;&#21512;&#20010;&#24615;&#21270;&#29992;&#25143;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20027;&#27969;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#20027;&#35201;&#32858;&#28966;&#20110;&#22522;&#20110;ID&#25110;&#25991;&#26412;&#30340;&#25512;&#33616;&#38382;&#39064;&#65292;&#26410;&#33021;&#29702;&#35299;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#25110;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#22522;&#30784;&#29983;&#25104;&#24314;&#27169;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10667v1 Announce Type: cross  Abstract: Developing a universal model that can effectively harness heterogeneous resources and respond to a wide range of personalized needs has been a longstanding community aspiration. Our daily choices, especially in domains like fashion and retail, are substantially shaped by multi-modal data, such as pictures and textual descriptions. These modalities not only offer intuitive guidance but also cater to personalized user preferences. However, the predominant personalization approaches mainly focus on the ID or text-based recommendation problem, failing to comprehend the information spanning various tasks or modalities. In this paper, our goal is to establish a Unified paradigm for Multi-modal Personalization systems (UniMP), which effectively leverages multi-modal data while eliminating the complexities associated with task- and modality-specific customization. We argue that the advancements in foundational generative modeling have provided
&lt;/p&gt;</description></item><item><title>DiPaCo&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#27169;&#22359;&#21270;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36335;&#24452;&#20998;&#21457;&#35745;&#31639;&#65292;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#26080;&#38656;&#27169;&#22411;&#21387;&#32553;&#12290;</title><link>https://arxiv.org/abs/2403.10616</link><description>&lt;p&gt;
DiPaCo: &#20998;&#24067;&#24335;&#36335;&#24452;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
DiPaCo: Distributed Path Composition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10616
&lt;/p&gt;
&lt;p&gt;
DiPaCo&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#27169;&#22359;&#21270;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36335;&#24452;&#20998;&#21457;&#35745;&#31639;&#65292;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#26080;&#38656;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#24471;&#30410;&#20110;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36825;&#31181;&#25193;&#23637;&#26159;&#36890;&#36807;&#19981;&#26029;&#22766;&#20030;&#30340;&#24037;&#31243;&#21162;&#21147;&#23454;&#29616;&#30340;&#65292;&#20197;&#36866;&#24212;&#38656;&#35201;&#35774;&#22791;&#20043;&#38388;&#39640;&#24102;&#23485;&#36890;&#20449;&#30340;&#24182;&#34892;ML&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20026;ML&#27169;&#22411;&#35774;&#35745;&#30340;&#21327;&#21516;&#27169;&#22359;&#21270;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;DIstributed PAth COmposition&#65288;DiPaCo&#65289;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;DiPaCo&#36890;&#36807;&#19968;&#32452;&#20849;&#20139;&#27169;&#22359;&#30340;&#36335;&#24452;&#36827;&#34892;&#35745;&#31639;&#20998;&#21457;&#12290;&#32467;&#21512;&#19968;&#31181;&#21463;Local-SGD&#21551;&#21457;&#30340;&#20248;&#21270;&#26041;&#27861;&#65288;DiLoCo&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22823;&#24133;&#20943;&#23569;&#36890;&#20449;&#26469;&#30830;&#20445;&#27169;&#22359;&#21516;&#27493;&#65292;&#20419;&#36827;&#20102;&#36328;&#36830;&#25509;&#19981;&#20339;&#19988;&#24322;&#26500;&#30340;&#24037;&#20316;&#33410;&#28857;&#30340;&#35757;&#32451;&#65292;&#20854;&#35774;&#35745;&#30830;&#20445;&#20102;&#23545;&#24037;&#20316;&#33410;&#28857;&#25925;&#38556;&#21644;&#25250;&#21344;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#25512;&#26029;&#26102;&#65292;&#27599;&#20010;&#36755;&#20837;&#21482;&#38656;&#35201;&#25191;&#34892;&#19968;&#26465;&#36335;&#24452;&#65292;&#26080;&#38656;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#19968;&#31181;&#39318;&#21019;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10616v1 Announce Type: cross  Abstract: Progress in machine learning (ML) has been fueled by scaling neural network models. This scaling has been enabled by ever more heroic feats of engineering, necessary for accommodating ML approaches that require high bandwidth communication between devices working in parallel. In this work, we propose a co-designed modular architecture and training approach for ML models, dubbed DIstributed PAth COmposition (DiPaCo). During training, DiPaCo distributes computation by paths through a set of shared modules. Together with a Local-SGD inspired optimization (DiLoCo) that keeps modules in sync with drastically reduced communication, Our approach facilitates training across poorly connected and heterogeneous workers, with a design that ensures robustness to worker failures and preemptions. At inference time, only a single path needs to be executed for each input, without the need for any model compression. We consider this approach as a first 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#27169;&#25311;&#21463;&#25511;&#31070;&#32463;&#36864;&#34892;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;LLMs&#20013;&#28155;&#21152;&#22122;&#38899;&#25110;&#20999;&#38500;&#31070;&#32463;&#20803;&#26469;&#36880;&#28176;&#38477;&#20302;&#20854;&#24615;&#33021;&#65292;&#22312;&#26234;&#21830;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#31070;&#32463;&#36864;&#34892;&#30340;&#36807;&#31243;&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#19981;&#21516;&#65292;&#26159;&#39318;&#20010;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#27169;&#25311;&#31070;&#32463;&#36864;&#34892;&#30340;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.10596</link><description>&lt;p&gt;
&#31070;&#32463;&#20405;&#34432;&#65306;&#22312;AI&#31995;&#32479;&#20013;&#27169;&#25311;&#21463;&#25511;&#31070;&#32463;&#36864;&#34892;&#21644;&#34928;&#32769;
&lt;/p&gt;
&lt;p&gt;
Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#27169;&#25311;&#21463;&#25511;&#31070;&#32463;&#36864;&#34892;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;LLMs&#20013;&#28155;&#21152;&#22122;&#38899;&#25110;&#20999;&#38500;&#31070;&#32463;&#20803;&#26469;&#36880;&#28176;&#38477;&#20302;&#20854;&#24615;&#33021;&#65292;&#22312;&#26234;&#21830;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#31070;&#32463;&#36864;&#34892;&#30340;&#36807;&#31243;&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#19981;&#21516;&#65292;&#26159;&#39318;&#20010;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#27169;&#25311;&#31070;&#32463;&#36864;&#34892;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#25511;&#21046;&#26041;&#27861;&#20197;&#27169;&#25311;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20013;&#30340;&#31070;&#32463;&#36864;&#34892;&#23545;&#20110;&#27169;&#25311;&#22823;&#33041;&#21151;&#33021;&#19979;&#38477;&#21644;&#35748;&#30693;&#38556;&#30861;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25191;&#34892;&#30340;&#26234;&#21830;&#27979;&#35797;&#65292;&#29305;&#21035;&#26159;LLaMA 2&#65292;&#24341;&#20837;&#20102;&#8220;&#31070;&#32463;&#20405;&#34432;&#8221;&#30340;&#27010;&#24565;&#12290;&#36825;&#31181;&#26377;&#24847;&#30340;&#20405;&#34432;&#28041;&#21450;&#22312;&#35757;&#32451;&#26399;&#38388;&#25110;&#20043;&#21518;&#20999;&#38500;&#31361;&#35302;&#25110;&#31070;&#32463;&#20803;&#65292;&#25110;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#65292;&#23548;&#33268;LLMs&#34920;&#29616;&#30340;&#36880;&#28176;&#19979;&#38477;&#12290;&#25105;&#20204;&#33021;&#22815;&#25551;&#36848;&#26234;&#21830;&#27979;&#35797;&#20013;&#30340;&#31070;&#32463;&#36864;&#34892;&#65292;&#24182;&#26174;&#31034;LLM&#39318;&#20808;&#22833;&#21435;&#20854;&#25968;&#23398;&#33021;&#21147;&#65292;&#28982;&#21518;&#22833;&#21435;&#20854;&#35821;&#35328;&#33021;&#21147;&#65292;&#21516;&#26102;&#36827;&#19968;&#27493;&#22833;&#21435;&#29702;&#35299;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#24314;&#27169;&#31070;&#32463;&#36864;&#34892;&#30340;&#24037;&#20316;&#65292;&#19982;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#36827;&#34892;&#25805;&#20316;&#30340;&#20854;&#20182;&#24037;&#20316;&#30456;&#27604;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#19982;&#35748;&#30693;&#36827;&#34892;&#30456;&#20284;&#24615;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10596v1 Announce Type: cross  Abstract: Creating controlled methods to simulate neurodegeneration in artificial intelligence (AI) is crucial for applications that emulate brain function decline and cognitive disorders. We use IQ tests performed by Large Language Models (LLMs) and, more specifically, the LLaMA 2 to introduce the concept of ``neural erosion." This deliberate erosion involves ablating synapses or neurons, or adding Gaussian noise during or after training, resulting in a controlled progressive decline in the LLMs' performance. We are able to describe the neurodegeneration in the IQ tests and show that the LLM first loses its mathematical abilities and then its linguistic abilities, while further losing its ability to understand the questions. To the best of our knowledge, this is the first work that models neurodegeneration with text data, compared to other works that operate in the computer vision domain. Finally, we draw similarities between our study and cogn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#29992;&#20110;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#65292;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#26377;&#25928;&#24212;&#23545;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.10581</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#30340;ECG&#21452;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10581
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#29992;&#20110;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#65292;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#26377;&#25928;&#24212;&#23545;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#30001;&#20110;&#20840;&#29699;&#27515;&#20129;&#29575;&#19981;&#26029;&#19978;&#21319;&#32780;&#26500;&#25104;&#37325;&#22823;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#36890;&#36807;&#26089;&#26399;&#35786;&#26029;&#21644;&#39044;&#38450;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#21487;&#26174;&#33879;&#20943;&#23569;&#30142;&#30149;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#20020;&#24202;&#33719;&#21462;&#30340;12&#23548;&#32852;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#36827;&#34892;HF&#39118;&#38505;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#36731;&#37327;&#32423;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#26088;&#22312;&#25429;&#25417;&#23545;&#26089;&#26399;HF&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#22797;&#26434;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#23613;&#31649;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#19981;&#24179;&#34913;&#12290;&#35813;&#32593;&#32476;&#20855;&#26377;&#19968;&#20010;&#36328;&#23548;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;12&#20010;&#23548;&#32852;&#29305;&#23450;&#30340;&#26102;&#38388;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#25429;&#25417;&#20132;&#21449;&#23548;&#32852;&#20132;&#20114;&#20316;&#29992;&#21644;&#27599;&#20010;&#23548;&#32852;&#20869;&#30340;&#23616;&#37096;&#26102;&#38388;&#21160;&#24577;&#12290;&#20026;&#20102;&#38450;&#27490;&#27169;&#22411;&#36807;&#25311;&#21512;&#20110;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20844;&#20849;ECG-Report&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#36827;&#34892;ECG-&#25253;&#21578;&#23545;&#40784;&#20219;&#21153;&#12290;&#28982;&#21518;&#23545;&#32593;&#32476;&#36827;&#34892;fine-tune&#20197;&#29992;&#20110;HF&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10581v1 Announce Type: cross  Abstract: Heart failure (HF) poses a significant public health challenge due to its rising global mortality rate. Addressing this issue through early diagnosis and prevention could significantly reduce the disease's impact. This work introduces a methodology for HF risk prediction using clinically acquired 12-lead electrocardiograms (ECGs). We present a novel, lightweight dual-attention ECG network designed to capture complex ECG features essential for early HF prediction, despite the notable imbalance between low and high-risk groups. The network features a cross-lead attention module and twelve lead-specific temporal attention modules to capture cross-lead interactions and local temporal dynamics within each lead. To prevent model overfitting from limited training data, we leverage a large language model (LLM) with a public ECG-Report dataset for pretraining on an ECG-report alignment task. The network is then fine-tuned for HF risk prediction
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#38750;&#35821;&#35328;&#20803;&#32032;&#36827;&#34892;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#24182;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.10576</link><description>&lt;p&gt;
&#24573;&#30053;&#25105;&#20294;&#19981;&#35201;&#26367;&#20195;&#25105;&#65306;&#21033;&#29992;&#38750;&#35821;&#35328;&#20803;&#32032;&#36827;&#34892;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10576
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#35821;&#35328;&#20803;&#32032;&#36827;&#34892;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#24182;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32593;&#32476;&#23433;&#20840;&#20449;&#24687;&#36890;&#24120;&#25216;&#26415;&#22797;&#26434;&#19988;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20256;&#36882;&#65292;&#20351;&#24471;&#33258;&#21160;&#21270;&#22788;&#29702;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#38024;&#23545;&#28041;&#21450;&#39640;&#24230;&#19987;&#19994;&#30693;&#35782;&#30340;&#25991;&#26412;&#39046;&#22495;&#65292;&#22522;&#20110;&#39046;&#22495;&#35821;&#26009;&#24211;&#30340;&#39044;&#35757;&#32451;&#19968;&#30452;&#26159;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#23433;&#20840;&#25991;&#26412;&#36890;&#24120;&#21253;&#21547;&#38750;&#35821;&#35328;&#20803;&#32032;&#65288;&#22914;URL&#21644;&#21704;&#24076;&#20540;&#65289;&#65292;&#36825;&#21487;&#33021;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#19981;&#36866;&#29992;&#12290;&#20808;&#21069;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#24037;&#20316;&#20013;&#65292;&#24050;&#23558;&#27492;&#31867;&#25991;&#26412;&#35270;&#20026;&#22122;&#38899;&#36827;&#34892;&#31227;&#38500;&#25110;&#36807;&#28388;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#35843;&#26597;&#65292;&#29305;&#21035;&#26159;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19979;&#28216;&#20219;&#21153;&#21644;&#25506;&#27979;&#20219;&#21153;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31574;&#30053;&#65288;&#36873;&#25321;&#24615;MLM&#21644;&#32852;&#21512;&#35757;&#32451;NLE&#26631;&#35760;&#20998;&#31867;&#65289;&#20248;&#20110;&#24120;&#29992;&#30340;&#26367;&#25442;&#38750;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10576v1 Announce Type: cross  Abstract: Cybersecurity information is often technically complex and relayed through unstructured text, making automation of cyber threat intelligence highly challenging. For such text domains that involve high levels of expertise, pretraining on in-domain corpora has been a popular method for language models to obtain domain expertise. However, cybersecurity texts often contain non-linguistic elements (such as URLs and hash values) that could be unsuitable with the established pretraining methodologies. Previous work in other domains have removed or filtered such text as noise, but the effectiveness of these methods have not been investigated, especially in the cybersecurity domain. We propose different pretraining methodologies and evaluate their effectiveness through downstream tasks and probing tasks. Our proposed strategy (selective MLM and jointly training NLE token classification) outperforms the commonly taken approach of replacing non-l
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#26368;&#36817;&#20195;&#30721;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#36741;&#21161;&#20989;&#25968;&#21033;&#29992;&#33021;&#21147;&#65292;&#36890;&#36807;&#35774;&#35745;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#39118;&#26684;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#27169;&#22411;&#21033;&#29992;&#36741;&#21161;&#20989;&#25968;&#30340;&#24456;&#26377;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2403.10575</link><description>&lt;p&gt;
&#29992;&#36741;&#21161;&#21151;&#33021;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring Language Model's Code Generation Ability with Auxiliary Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10575
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#26368;&#36817;&#20195;&#30721;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#36741;&#21161;&#20989;&#25968;&#21033;&#29992;&#33021;&#21147;&#65292;&#36890;&#36807;&#35774;&#35745;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#39118;&#26684;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#27169;&#22411;&#21033;&#29992;&#36741;&#21161;&#20989;&#25968;&#30340;&#24456;&#26377;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36741;&#21161;&#20989;&#25968;&#26159;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#26377;&#29992;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#30340;&#31995;&#32479;&#24615;&#25506;&#32034;&#23578;&#26410;&#23436;&#25104;&#12290;&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#26368;&#36817;&#30340;&#20195;&#30721;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#36741;&#21161;&#20989;&#25968;&#21033;&#29992;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#20154;&#24037;&#35774;&#35745;&#30340;&#35780;&#20272;&#38598;&#65292;&#31216;&#20026;HumanExtension&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#20989;&#25968;&#24110;&#21161;&#21478;&#19968;&#20010;&#20989;&#25968;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;HumanExtension&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20960;&#20010;&#23454;&#39564;&#20197;&#22810;&#26041;&#38754;&#22320;&#26816;&#39564;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#36807;&#31243;&#20351;&#24471;&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#21253;&#25324;&#36741;&#21161;&#20989;&#25968;&#22312;&#25552;&#31034;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#21478;&#22806;&#65292;&#36890;&#36807;&#23454;&#29616;&#39118;&#26684;&#20998;&#26512;&#25429;&#25417;&#21040;&#27169;&#22411;&#22312;&#35775;&#38382;&#36741;&#21161;&#20989;&#25968;&#26102;&#30340;&#21508;&#31181;&#23454;&#29616;&#27169;&#24335;&#12290;&#36890;&#36807;&#36825;&#19968;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#21033;&#29992;&#36741;&#21161;&#20989;&#25968;&#30340;&#33021;&#21147;&#24456;&#26377;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10575v1 Announce Type: cross  Abstract: Auxiliary function is a helpful component to improve language model's code generation ability. However, a systematic exploration of how they affect has yet to be done. In this work, we comprehensively evaluate the ability to utilize auxiliary functions encoded in recent code-pretrained language models. First, we construct a human-crafted evaluation set, called HumanExtension, which contains examples of two functions where one function assists the other. With HumanExtension, we design several experiments to examine their ability in a multifaceted way. Our evaluation processes enable a comprehensive understanding of including auxiliary functions in the prompt in terms of effectiveness and robustness. An additional implementation style analysis captures the models' various implementation patterns when they access the auxiliary function. Through this analysis, we discover the models' promising ability to utilize auxiliary functions includi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10568</link><description>&lt;p&gt;
MoPE&#65306;&#36890;&#36807;Prompt&#19987;&#23478;&#28151;&#21512;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#35843;&#25972;&#24050;&#32463;&#35777;&#26126;&#22312;&#34701;&#21512;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21333;&#27169;&#22522;&#30784;&#27169;&#22411;&#26102;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#19982;&#20854;&#20182;&#35843;&#25972;&#26041;&#27861;&#30456;&#27604;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31616;&#21333;&#25552;&#31034;&#35299;&#24320;&#20197;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#24314;&#31435;&#22312;&#36825;&#31181;&#35299;&#24320;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt&#19987;&#23478;&#30340;&#28151;&#21512;&#65288;MoPE&#65289;&#25216;&#26415;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12290;MoPE&#21033;&#29992;&#22810;&#27169;&#24577;&#37197;&#23545;&#20808;&#39564;&#22312;&#27599;&#20010;&#23454;&#20363;&#22522;&#30784;&#19978;&#36335;&#30001;&#26368;&#26377;&#25928;&#30340;&#25552;&#31034;&#12290;&#19982;&#31616;&#21333;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22522;&#20110;MoPE&#30340;&#26465;&#20214;&#25552;&#31034;&#23545;&#22810;&#27169;&#24577;&#34701;&#21512;&#20855;&#26377;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#24635;&#25968;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#19987;&#23478;&#36335;&#30001;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#23548;&#33268;&#19987;&#23478;&#30340;&#19981;&#26029;&#21457;&#23637;&#19987;&#38271;&#65292;&#19981;&#21516;&#19987;&#23478;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20108;&#38454;&#20449;&#24687;&#65288;Hessian&#65289;&#30340;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36951;&#24536;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10557</link><description>&lt;p&gt;
&#20108;&#38454;&#20449;&#24687;&#24456;&#37325;&#35201;&#65306;&#37325;&#35775;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20108;&#38454;&#20449;&#24687;&#65288;Hessian&#65289;&#30340;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36951;&#24536;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#25105;&#20204;&#30446;&#30585;&#20102;ChatGPT&#12289;LLaMa&#21644;Gemini&#31561;&#20027;&#35201;LLM&#20135;&#21697;&#20043;&#38388;&#30340;&#28608;&#28872;&#31454;&#20105;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#21508;&#31181;&#38382;&#39064;&#65288;&#22914;&#38544;&#31169;&#27844;&#38706;&#21644;&#29256;&#26435;&#20405;&#29359;&#65289;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#20197;LLM&#20174;&#19994;&#32773;&#30340;&#35270;&#35282;&#26469;&#30475;&#65292;&#22788;&#29702;&#36825;&#20123;&#24847;&#22806;&#30340;&#38544;&#31169;&#20405;&#29359;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#20449;&#24687;&#35299;&#20915;&#20102;LLMs&#30340;&#8220;&#36951;&#24536;&#8221;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#24320;&#38144;&#65292;&#22914;&#25968;&#25454;&#39044;&#22788;&#29702;&#25110;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#19982;&#22522;&#20110;&#19968;&#38454;&#20449;&#24687;&#30340;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#25105;&#20204;&#36890;&#36807;&#20108;&#38454;&#20449;&#24687;&#65288;Hessian&#65289;&#30340;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#36951;&#24536;&#38382;&#39064;&#12290;&#21463;&#32463;&#20856;&#29275;&#39039;&#26356;&#26032;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#36951;&#24536;&#31639;&#27861;&#20855;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10557v1 Announce Type: cross  Abstract: With the rapid development of Large Language Models (LLMs), we have witnessed intense competition among the major LLM products like ChatGPT, LLaMa, and Gemini. However, various issues (e.g. privacy leakage and copyright violation) of the training corpus still remain underexplored. For example, the Times sued OpenAI and Microsoft for infringing on its copyrights by using millions of its articles for training. From the perspective of LLM practitioners, handling such unintended privacy violations can be challenging. Previous work addressed the ``unlearning" problem of LLMs using gradient information, while they mostly introduced significant overheads like data preprocessing or lacked robustness. In this paper, contrasting with the methods based on first-order information, we revisit the unlearning problem via the perspective of second-order information (Hessian). Our unlearning algorithms, which are inspired by classic Newton update, are 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09732</link><description>&lt;p&gt;
PET-SQL&#65306;&#19968;&#20010;&#24102;&#26377;&#20132;&#21449;&#19968;&#33268;&#24615;&#30340;&#22686;&#24378;&#25552;&#31034;&#30340;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;SQL&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25991;&#26412;&#21040;SQL&#65288;Text2SQL&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#24378;&#35843;&#21050;&#28608;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#30340;&#29992;&#25143;&#24847;&#22270;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#34920;&#31034;&#65292;&#31216;&#20026;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#24335;&#20449;&#24687;&#21644;&#20174;&#34920;&#26684;&#38543;&#26426;&#25277;&#26679;&#30340;&#21333;&#20803;&#26684;&#20540;&#65292;&#20197;&#25351;&#23548;LLM&#29983;&#25104;SQL&#26597;&#35810;&#12290;&#28982;&#21518;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#26816;&#32034;&#38382;&#39064;-SQL&#23545;&#20316;&#20026;&#23569;&#37327;&#28436;&#31034;&#65292;&#20419;&#20351;LLM&#29983;&#25104;&#21021;&#27493;SQL&#65288;PreSQL&#65289;&#12290;&#20043;&#21518;&#65292;&#35299;&#26512;PreSQL&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#36827;&#34892;&#27169;&#24335;&#38142;&#25509;&#65292;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#26377;&#29992;&#20449;&#24687;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21033;&#29992;&#38142;&#25509;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09732v1 Announce Type: cross  Abstract: Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the 
&lt;/p&gt;</description></item><item><title>Quiet-STaR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27867;&#21270;&#29256;&#26412;&#65292;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.09629</link><description>&lt;p&gt;
Quiet-STaR: &#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33258;&#24049;&#23398;&#20250;&#24605;&#32771;&#21518;&#20877;&#35828;&#35805;
&lt;/p&gt;
&lt;p&gt;
Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09629
&lt;/p&gt;
&lt;p&gt;
Quiet-STaR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27867;&#21270;&#29256;&#26412;&#65292;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#21644;&#20132;&#35848;&#26102;&#65292;&#20154;&#20204;&#26377;&#26102;&#20250;&#20572;&#19979;&#26469;&#24605;&#32771;&#12290;&#23613;&#31649;&#20197;&#25512;&#29702;&#20026;&#37325;&#28857;&#30340;&#20316;&#21697;&#36890;&#24120;&#23558;&#25512;&#29702;&#26694;&#23450;&#20026;&#22238;&#31572;&#38382;&#39064;&#25110;&#23436;&#25104;&#20195;&#29702;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20294;&#25512;&#29702;&#20960;&#20046;&#37117;&#38544;&#21547;&#22312;&#25152;&#26377;&#20070;&#38754;&#25991;&#26412;&#20013;&#12290;&#20363;&#22914;&#65292;&#36825;&#36866;&#29992;&#20110;&#35777;&#26126;&#20013;&#26410;&#26126;&#30830;&#35828;&#26126;&#30340;&#27493;&#39588;&#65292;&#20197;&#21450;&#25903;&#25745;&#23545;&#35805;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#22312;&#33258;&#23398;&#20064;&#25512;&#29702;&#32773;&#65288;STaR&#65292;Zelikman&#31561;&#65292;2022&#65289;&#20013;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#25512;&#26029;&#26469;&#33258;&#38382;&#31572;&#20013;&#26377;&#29992;&#30340;&#24605;&#32771;&#65292;&#24182;&#23398;&#20064;&#37027;&#20123;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#30340;&#24605;&#32771;&#12290;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;--&#29702;&#24819;&#24773;&#20917;&#19979;, &#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20250;&#20174;&#20219;&#24847;&#25991;&#26412;&#20013;&#25512;&#26029;&#26410;&#26126;&#30830;&#35828;&#26126;&#30340;&#24605;&#32771;&#12290;&#25105;&#20204;&#25552;&#20986;Quiet-STaR&#65292;&#36825;&#26159;STaR&#30340;&#19968;&#20010;&#27867;&#21270;&#29256;&#26412;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#20854;&#39044;&#27979;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;1&#65289;&#29983;&#25104;&#36830;&#32493;&#30340;&#35745;&#31639;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09629v1 Announce Type: cross  Abstract: When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continu
&lt;/p&gt;</description></item><item><title>AutoLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#65292;&#20197;&#35299;&#20915;LoRA&#20013;&#31209;&#20998;&#37197;&#21644;&#31209;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09113</link><description>&lt;p&gt;
AutoLoRA&#65306;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#33258;&#21160;&#35843;&#25972;&#30697;&#38453;&#31209;&#22312;&#20302;&#31209;&#36866;&#24212;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09113
&lt;/p&gt;
&lt;p&gt;
AutoLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#65292;&#20197;&#35299;&#20915;LoRA&#20013;&#31209;&#20998;&#37197;&#21644;&#31209;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20043;&#21518;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#23384;&#22312;&#30528;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25361;&#25112;&#65292;&#22240;&#27492;&#30740;&#21457;&#20102;&#20960;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#20854;&#20013;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#36890;&#36807;&#22312;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#20043;&#19978;&#24494;&#35843;&#20302;&#31209;&#22686;&#37327;&#26356;&#26032;&#30697;&#38453;&#65292;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;LoRA&#22312;&#25152;&#26377;&#23618;&#20013;&#22343;&#21248;&#20998;&#37197;&#31209;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#31351;&#20030;&#25628;&#32034;&#26469;&#25214;&#21040;&#26368;&#20339;&#31209;&#65292;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24494;&#35843;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AutoLoRA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#12290;AutoLoRA&#23558;&#20302;&#31209;&#26356;&#26032;&#30697;&#38453;&#20013;&#30340;&#27599;&#20010;&#31209;&#20026;1&#30340;&#30697;&#38453;&#19982;&#36873;&#25321;&#21464;&#37327;&#30456;&#20851;&#32852;&#65292;&#35813;&#21464;&#37327;&#20915;&#23450;&#20102;&#31209;&#20026;1&#30340;&#30697;&#38453;&#26159;&#21542;&#24212;&#35813;&#34987;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09113v1 Announce Type: cross  Abstract: Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks. Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed. Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance. To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should b
&lt;/p&gt;</description></item><item><title>AraTrust&#26159;&#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20449;&#35465;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20840;&#38754;&#20449;&#35465;&#35780;&#20272;&#22522;&#20934;&#30340;&#38382;&#39064;&#65292;&#24110;&#21161;&#20934;&#30830;&#35780;&#20272;&#21644;&#25552;&#39640;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09017</link><description>&lt;p&gt;
AraTrust&#65306;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20449;&#35465;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09017
&lt;/p&gt;
&lt;p&gt;
AraTrust&#26159;&#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20449;&#35465;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20840;&#38754;&#20449;&#35465;&#35780;&#20272;&#22522;&#20934;&#30340;&#38382;&#39064;&#65292;&#24110;&#21161;&#20934;&#30830;&#35780;&#20272;&#21644;&#25552;&#39640;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09017v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36805;&#36895;&#21457;&#23637;&#21644;&#24191;&#27867;&#25509;&#21463;&#20984;&#26174;&#20102;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#21147;&#21644;&#28508;&#22312;&#39118;&#38505;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;&#37492;&#20110;&#38463;&#25289;&#20271;&#35821;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#12289;&#25991;&#21270;&#20016;&#23500;&#24615;&#21644;&#22320;&#20301;&#19981;&#39640;&#65292;&#26377;&#24517;&#35201;&#19987;&#27880;&#20110;&#38463;&#25289;&#20271;&#35821;&#30456;&#20851;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#32570;&#20047;&#20840;&#38754;&#30340;&#20449;&#35465;&#35780;&#20272;&#22522;&#20934;&#26159;&#20934;&#30830;&#35780;&#20272;&#21644;&#25552;&#39640;&#22312;&#38463;&#25289;&#20271;&#35821;&#25552;&#31034;&#26102;LLMs&#30340;&#23433;&#20840;&#24615;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AraTrust 1&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#30340;&#20449;&#35465;&#22522;&#20934;&#12290;AraTrust &#21253;&#21547;&#20102;516&#20010;&#20154;&#24037;&#32534;&#20889;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#28041;&#21450;&#19982;&#30495;&#23454;&#24615;&#12289;&#36947;&#24503;&#12289;&#23433;&#20840;&#24615;&#12289;&#36523;&#20307;&#20581;&#24247;&#12289;&#24515;&#29702;&#20581;&#24247;&#12289;&#19981;&#20844;&#24179;&#34892;&#20026;&#12289;&#38750;&#27861;&#27963;&#21160;&#30456;&#20851;&#30340;&#22810;&#20010;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09017v1 Announce Type: new  Abstract: The swift progress and widespread acceptance of artificial intelligence (AI) systems highlight a pressing requirement to comprehend both the capabilities and potential risks associated with AI. Given the linguistic complexity, cultural richness, and underrepresented status of Arabic in AI research, there is a pressing need to focus on Large Language Models (LLMs) performance and safety for Arabic related tasks. Despite some progress in their development, there is a lack of comprehensive trustworthiness evaluation benchmarks which presents a major challenge in accurately assessing and improving the safety of LLMs when prompted in Arabic. In this paper, we introduce AraTrust 1, the first comprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises 516 human-written multiple-choice questions addressing diverse dimensions related to truthfulness, ethics, safety, physical health, mental health, unfairness, illegal activities
&lt;/p&gt;</description></item><item><title>GPST&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#23454;&#29616;&#23545;&#21407;&#22987;&#25991;&#26412;&#30340;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;&#20043;&#21069;SLM&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#21516;&#31561;&#35268;&#27169;&#30340;GPT-2&#12290;</title><link>https://arxiv.org/abs/2403.08293</link><description>&lt;p&gt;
&#29983;&#25104;&#39044;&#35757;&#32451;&#32467;&#26500;&#21270;Transformer&#65306;&#35268;&#27169;&#21270;&#30340;&#26080;&#30417;&#30563;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08293
&lt;/p&gt;
&lt;p&gt;
GPST&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#23454;&#29616;&#23545;&#21407;&#22987;&#25991;&#26412;&#30340;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;&#20043;&#21069;SLM&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#21516;&#31561;&#35268;&#27169;&#30340;GPT-2&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#20197;&#20174;&#24038;&#21040;&#21491;&#30340;&#26041;&#24335;&#36880;&#27493;&#29983;&#25104;&#24102;&#26377;&#20854;&#21477;&#27861;&#26641;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#25104;&#39044;&#35757;&#32451;&#32467;&#26500;&#21270;Transformer&#65288;GPST&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35268;&#27169;&#21270;&#30340;&#26080;&#30417;&#30563;SLM&#65292;&#33021;&#22815;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#12290;GPST&#35268;&#36991;&#20102;&#20043;&#21069;SLM&#30340;&#19968;&#20123;&#38480;&#21046;&#65292;&#27604;&#22914;&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#12290;&#23427;&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65292;&#19968;&#20010;&#36890;&#24120;&#30340;SLM&#21463;&#21333;&#21521;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#30340;&#30417;&#30563;&#65292;&#20197;&#21450;&#19968;&#20010;&#39069;&#22806;&#30340;&#32452;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#24341;&#23548;&#21477;&#27861;&#35299;&#26512;&#26641;&#24182;&#35745;&#31639;&#25104;&#20998;&#34920;&#31034;&#65292;&#21463;&#21452;&#21521;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#26367;&#20195;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#20004;&#20010;&#27169;&#22411;&#30340;&#32852;&#21512;&#24182;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#30828;EM&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;OpenWebText&#19978;&#23545;GPST&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;90&#20159;&#20010;token&#65292;&#24182;&#23637;&#31034;&#20102;GPST&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#65292;&#28085;&#30422;&#20102;&#19982;GPT-2&#30456;&#24403;&#35268;&#27169;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08293v1 Announce Type: cross  Abstract: A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner. We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering bot
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08281</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#25484;&#25569;&#25991;&#26412;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;
&lt;/p&gt;
&lt;p&gt;
Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08281
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#12289;&#32534;&#31243;&#20195;&#30721;&#21644;&#25968;&#23398;&#31526;&#21495;&#30340;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#24040;&#22823;&#65292;&#23545;&#20110;&#37027;&#20123;&#21162;&#21147;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20986;&#20102;&#22797;&#26434;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#34701;&#21512;&#24050;&#32463;&#39640;&#24230;&#19987;&#19994;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#34701;&#21512;&#26694;&#26550;UltraFuser&#21253;&#25324;&#19977;&#20010;&#24050;&#32463;&#22312;&#35821;&#35328;&#12289;&#32534;&#30721;&#21644;&#25968;&#23398;&#19978;&#24471;&#21040;&#20805;&#20998;&#35757;&#32451;&#30340;&#19987;&#23478;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#26469;&#28151;&#21512;&#19987;&#23478;&#30340;&#36755;&#20986;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#20276;&#38543;&#24179;&#34913;&#37319;&#26679;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#20197;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#34701;&#21512;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08281v1 Announce Type: cross  Abstract: Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#26041;&#27861;&#65292;&#21463;&#22823;&#33041;&#21551;&#21457;&#65292;&#29992;&#20110;&#25552;&#39640;&#24494;&#35843;&#25216;&#26415;&#30340;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07440</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#65306;&#19968;&#31181;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#26041;&#27861;&#65292;&#21463;&#22823;&#33041;&#21551;&#21457;&#65292;&#29992;&#20110;&#25552;&#39640;&#24494;&#35843;&#25216;&#26415;&#30340;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LPLMs&#65289;&#30340;&#24494;&#35843;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#26377;&#25928;&#25511;&#21046;LPLMs&#30340;&#36755;&#20986;&#34892;&#20026;&#12290;&#26412;&#25991;&#21463;&#22823;&#33041;&#21151;&#33021;&#21463;&#20854;&#20960;&#20309;&#32467;&#26500;&#22609;&#36896;&#30340;&#21551;&#21457;&#65292;&#23558;&#36825;&#19968;&#24605;&#24819;&#34701;&#20837;LoRA&#25216;&#26415;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07440v1 Announce Type: cross  Abstract: Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameteriz
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07311</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;KG-LLM&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Large Language Model (KG-LLM) for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20998;&#26512;&#39046;&#22495;&#65292;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20869;&#22810;&#20010;&#38142;&#25509;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#36825;&#19968;&#25361;&#25112;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#35299;&#20915;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20851;&#38190;&#30340;NLP&#33539;&#20363;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;KG&#36716;&#25442;&#20026;CoT&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35782;&#21035;&#24182;&#23398;&#20064;&#23454;&#20307;&#21450;&#20854;&#30456;&#20114;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20026;&#20102;&#23637;&#31034;KG-LLM&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#35813;&#26694;&#26550;&#20869;&#24494;&#35843;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#38750;ICL&#21644;ICL&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#26694;&#26550;&#20026;LLMs&#25552;&#20379;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
&lt;/p&gt;</description></item><item><title>ALaRM&#26159;&#31532;&#19968;&#20010;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24314;&#27169;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#31934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.06754</link><description>&lt;p&gt;
ALaRM: &#36890;&#36807;&#20998;&#23618;&#22870;&#21169;&#24314;&#27169;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ALaRM: Align Language Models via Hierarchical Rewards Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06754
&lt;/p&gt;
&lt;p&gt;
ALaRM&#26159;&#31532;&#19968;&#20010;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24314;&#27169;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#31934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;ALaRM&#65292;&#31532;&#19968;&#20010;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20174;&#20154;&#31867;&#21453;&#39304;&#27169;&#22411;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24403;&#21069;&#23545;&#40784;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#20154;&#31867;&#30417;&#30563;&#20449;&#21495;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#36890;&#36807;&#23558;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#25972;&#21512;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#26356;&#21152;&#31934;&#30830;&#21644;&#19968;&#33268;&#22320;&#25351;&#23548;&#26397;&#30528;&#26399;&#26395;&#30340;&#32467;&#26524;&#21069;&#36827;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#21644;&#24320;&#25918;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#21644;&#32452;&#21512;&#22810;&#20010;&#22870;&#21169;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26426;&#21046;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#23545;&#40784;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#38271;&#31687;&#38382;&#39064;&#22238;&#31572;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#20351;&#29992;gpt-3.5-turbo&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06754v1 Announce Type: cross  Abstract: We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demon
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Composition Score&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#37327;&#21270;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;&#31243;&#24230;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#24230;&#37327;&#19982;&#22823;&#33041;&#21306;&#22495;&#30456;&#20851;&#65292;&#25581;&#31034;&#20102;&#21547;&#20041;&#21512;&#25104;&#22312;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#22810;&#26041;&#38754;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04325</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20998;&#25968;&#27979;&#37327;&#20154;&#33041;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04325
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Composition Score&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#37327;&#21270;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;&#31243;&#24230;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#24230;&#37327;&#19982;&#22823;&#33041;&#21306;&#22495;&#30456;&#20851;&#65292;&#25581;&#31034;&#20102;&#21547;&#20041;&#21512;&#25104;&#22312;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#22810;&#26041;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21547;&#20041;&#21512;&#25104;&#30340;&#36807;&#31243;&#26159;&#25351;&#26356;&#23567;&#30340;&#21333;&#20301;&#22914;&#35821;&#32032;&#25110;&#21333;&#35789;&#32452;&#21512;&#24418;&#25104;&#30701;&#35821;&#21644;&#21477;&#23376;&#30340;&#21547;&#20041;&#65292;&#23545;&#20110;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#31070;&#32463;&#35821;&#35328;&#23398;&#23545;&#28041;&#21450;&#21547;&#20041;&#21512;&#25104;&#30340;&#22823;&#33041;&#21306;&#22495;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#20173;&#32570;&#20047;&#19968;&#31181;&#35745;&#31639;&#24230;&#37327;&#26469;&#37327;&#21270;&#21512;&#25104;&#30340;&#31243;&#24230;&#12290;&#20511;&#37492;&#21464;&#21387;&#22120;&#21069;&#39304;&#32593;&#32476;&#22359;&#30340;&#38190;&#20540;&#20869;&#23384;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32452;&#21512;&#20998;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#37327;&#21270;&#21477;&#23376;&#29702;&#35299;&#36807;&#31243;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;&#31243;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19968;&#24230;&#37327;&#19982;&#22823;&#33041;&#31751;&#30456;&#20851;&#32852;&#65292;&#36825;&#20123;&#22823;&#33041;&#31751;&#19982;&#35789;&#39057;&#29575;&#12289;&#32467;&#26500;&#22788;&#29702;&#21644;&#23545;&#21333;&#35789;&#30340;&#19968;&#33324;&#25935;&#24863;&#24615;&#26377;&#20851;&#65292;&#36825;&#34920;&#26126;&#20102;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#36807;&#31243;&#20013;&#21547;&#20041;&#21512;&#25104;&#30340;&#22810;&#26041;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04325v1 Announce Type: cross  Abstract: The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#24615;&#21035;&#21270;&#24773;&#32490;&#24402;&#22240;&#65292;&#21453;&#26144;&#20102;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.03121</link><description>&lt;p&gt;
&#24868;&#24594;&#30340;&#30007;&#24615;&#65292;&#24754;&#20260;&#30340;&#22899;&#24615;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#32490;&#24402;&#22240;&#20013;&#21453;&#26144;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03121
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#24615;&#21035;&#21270;&#24773;&#32490;&#24402;&#22240;&#65292;&#21453;&#26144;&#20102;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21453;&#26144;&#31038;&#20250;&#35268;&#33539;&#21644;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#24615;&#21035;&#30340;&#12290;&#34429;&#28982;&#31038;&#20250;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#24773;&#32490;&#20998;&#26512;&#26041;&#38754;&#23384;&#22312;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#31354;&#30333;&#12290;&#28982;&#32780;&#65292;&#22312;&#31038;&#20250;&#35805;&#35821;&#20013;&#65292;&#24773;&#32490;&#21644;&#24615;&#21035;&#23494;&#20999;&#30456;&#20851;&#12290;&#20363;&#22914;&#65292;&#22899;&#24615;&#32463;&#24120;&#34987;&#35748;&#20026;&#26356;&#20855;&#31227;&#24773;&#33021;&#21147;&#65292;&#32780;&#30007;&#24615;&#30340;&#24868;&#24594;&#26356;&#21463;&#31038;&#20250;&#25509;&#21463;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;&#24320;&#28304;&#21644;&#23553;&#38381;&#28304;&#65289;&#36827;&#34892;&#24615;&#21035;&#21270;&#24773;&#32490;&#24402;&#22240;&#30340;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#35843;&#26597;&#24773;&#32490;&#26159;&#21542;&#20855;&#26377;&#24615;&#21035;&#29305;&#24449;&#65292;&#20197;&#21450;&#36825;&#20123;&#21464;&#21270;&#26159;&#21542;&#22522;&#20110;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#25552;&#31034;&#27169;&#22411;&#37319;&#29992;&#24615;&#21035;&#21270;&#35282;&#33394;&#24182;&#23558;&#24773;&#32490;&#24402;&#22240;&#20110;&#31867;&#20284;&#8220;&#24403;&#25105;&#19982;&#20146;&#36817;&#30340;&#20154;&#21457;&#29983;&#20005;&#37325;&#20105;&#25191;&#8221;&#36825;&#26679;&#30340;&#20107;&#20214;&#12290;&#28982;&#21518;&#25105;&#20204;&#20998;&#26512;&#27169;&#22411;&#29983;&#25104;&#30340;&#24773;&#32490;&#19982;&#24615;&#21035;-&#20107;&#20214;&#23545;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#19968;&#33268;&#23637;&#29616;&#20986;&#24615;&#21035;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03121v1 Announce Type: new  Abstract: Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men's anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like 'When I had a serious argument with a dear person'. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23454;&#29616;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#26080;&#30417;&#30563;&#24320;&#21457;&#65292;&#20174;&#32780;&#25913;&#21892;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#30340;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01570</link><description>&lt;p&gt;
SERVAL&#65306;&#22402;&#30452;&#27169;&#22411;&#21644;LLM&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#65292;&#23454;&#29616;&#38646;-shot&#32423;&#21035;&#30340;&#21307;&#23398;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01570
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23454;&#29616;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#26080;&#30417;&#30563;&#24320;&#21457;&#65292;&#20174;&#32780;&#25913;&#21892;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#30340;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20986;&#23545;&#36890;&#29992;&#21644;&#24120;&#35782;&#38382;&#39064;&#21331;&#36234;&#30340;&#38646;-shot&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#33853;&#21518;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22402;&#30452;&#30693;&#35782;&#26041;&#38754;&#30340;&#38382;&#39064;&#21644;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#22402;&#30452;&#25968;&#25454;&#27880;&#37322;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#19987;&#23478;&#21442;&#19982;&#65292;&#22240;&#27492;&#22686;&#21152;&#20102;&#22686;&#24378;&#27169;&#22411;&#22402;&#30452;&#33021;&#21147;&#30340;&#39069;&#22806;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#26088;&#22312;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23545;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#36827;&#34892;&#26080;&#30417;&#30563;&#24320;&#21457;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SERVAL&#21033;&#29992;LLMs&#30340;&#38646;-shot&#36755;&#20986;&#20316;&#20026;&#27880;&#37322;&#65292;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#26469;&#20174;&#22836;&#24320;&#22987;&#25945;&#25480;&#19968;&#20010;&#24378;&#22823;&#30340;&#22402;&#30452;&#27169;&#22411;&#12290;&#21453;&#36807;&#26469;&#65292;&#35757;&#32451;&#26377;&#32032;&#30340;&#22402;&#30452;&#27169;&#22411;&#24341;&#23548;LLM&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#38646;-shot&#33021;&#21147;&#65292;&#36880;&#27493;&#25913;&#36827;&#20004;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01570v1 Announce Type: new  Abstract: Recent development of large language models (LLMs) has exhibited impressive zero-shot proficiency on generic and common sense questions. However, LLMs' application on domain-specific vertical questions still lags behind, primarily due to the humiliation problems and deficiencies in vertical knowledge. Furthermore, the vertical data annotation process often requires labor-intensive expert involvement, thereby presenting an additional challenge in enhancing the model's vertical capabilities. In this paper, we propose SERVAL, a synergy learning pipeline designed for unsupervised development of vertical capabilities in both LLMs and small models by mutual enhancement. Specifically, SERVAL utilizes the LLM's zero-shot outputs as annotations, leveraging its confidence to teach a robust vertical model from scratch. Reversely, the trained vertical model guides the LLM fine-tuning to enhance its zero-shot capability, progressively improving both 
&lt;/p&gt;</description></item><item><title>NLP&#20013;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#23384;&#22312;&#30528;&#36235;&#21183;&#21644;&#24046;&#36317;&#65292;&#26410;&#26469;&#26041;&#21521;&#38656;&#32771;&#34385;&#24773;&#24863;&#20219;&#21153;&#23450;&#20041;&#12289;&#24773;&#24863;&#26694;&#26550;&#12289;&#24773;&#24863;&#20027;&#35266;&#24615;&#19982;NLP&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.01222</link><description>&lt;p&gt;
NLP&#20013;&#30340;&#24773;&#24863;&#20998;&#26512;&#65306;&#36235;&#21183;&#12289;&#24046;&#36317;&#21644;&#26410;&#26469;&#26041;&#21521;&#36335;&#32447;
&lt;/p&gt;
&lt;p&gt;
Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01222
&lt;/p&gt;
&lt;p&gt;
NLP&#20013;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#23384;&#22312;&#30528;&#36235;&#21183;&#21644;&#24046;&#36317;&#65292;&#26410;&#26469;&#26041;&#21521;&#38656;&#32771;&#34385;&#24773;&#24863;&#20219;&#21153;&#23450;&#20041;&#12289;&#24773;&#24863;&#26694;&#26550;&#12289;&#24773;&#24863;&#20027;&#35266;&#24615;&#19982;NLP&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#26159;&#27807;&#36890;&#30340;&#19968;&#20010;&#20013;&#24515;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#24773;&#24863;&#20998;&#26512;&#65288;EA&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#19968;&#20010;&#36805;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33539;&#22260;&#12289;&#26041;&#21521;&#25110;&#26041;&#27861;&#65292;&#23578;&#26080;&#20849;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36807;&#21435;&#21313;&#24180;&#20013;&#30340;154&#31687;&#30456;&#20851;NLP&#20986;&#29256;&#29289;&#36827;&#34892;&#20102;&#24443;&#24213;&#23457;&#26597;&#12290;&#22522;&#20110;&#27492;&#23457;&#26597;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#19981;&#21516;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#35752;&#65306;&#65288;1&#65289;NLP&#20013;&#22914;&#20309;&#23450;&#20041;EA&#20219;&#21153;&#65311;&#65288;2&#65289;&#20160;&#20040;&#26159;&#26368;&#31361;&#20986;&#30340;&#24773;&#24863;&#26694;&#26550;&#65292;&#20197;&#21450;&#21738;&#20123;&#24773;&#24863;&#34987;&#24314;&#27169;&#65311;&#65288;3&#65289;&#22312;&#20154;&#21475;&#32479;&#35745;&#21644;&#25991;&#21270;&#22240;&#32032;&#26041;&#38754;&#26159;&#21542;&#32771;&#34385;&#20102;&#24773;&#32490;&#30340;&#20027;&#35266;&#24615;&#65311;&#20197;&#21450;&#65288;4&#65289;EA&#30340;&#20027;&#35201;NLP&#24212;&#29992;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#24635;&#32467;&#20102;EA&#21644;&#20219;&#21153;&#12289;&#20351;&#29992;&#30340;&#24773;&#24863;&#26694;&#26550;&#12289;&#29616;&#26377;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#21644;&#24212;&#29992;&#30340;&#36235;&#21183;&#12290;&#28982;&#21518;&#25105;&#20204;&#35752;&#35770;&#20102;&#22235;&#20010;&#31354;&#30333;&#65306;&#65288;1&#65289;&#32570;&#20047;&#20154;&#21475;&#32479;&#35745;&#21644;&#25991;&#21270;&#22240;&#32032;&#65292;&#24182;&#26410;&#32771;&#34385;&#21040;&#19981;&#21516;&#25991;&#21270;&#22914;&#20309;&#24863;&#30693;&#24773;&#32490;&#30340;&#24046;&#24322;&#65292;&#32780;&#26159;&#20551;&#35774;&#23427;&#20204;&#26159;&#26222;&#36941;&#32463;&#21382;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01222v1 Announce Type: new  Abstract: Emotions are a central aspect of communication. Consequently, emotion analysis (EA) is a rapidly growing field in natural language processing (NLP). However, there is no consensus on scope, direction, or methods. In this paper, we conduct a thorough review of 154 relevant NLP publications from the last decade. Based on this review, we address four different questions: (1) How are EA tasks defined in NLP? (2) What are the most prominent emotion frameworks and which emotions are modeled? (3) Is the subjectivity of emotions considered in terms of demographics and cultural factors? and (4) What are the primary NLP applications for EA? We take stock of trends in EA and tasks, emotion frameworks used, existing datasets, methods, and applications. We then discuss four lacunae: (1) the absence of demographic and cultural aspects does not account for the variation in how emotions are perceived, but instead assumes they are universally experienced
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01139</link><description>&lt;p&gt;
ParallelPARC: &#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31867;&#27604;&#30340;&#21487;&#25193;&#23637;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01139
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Analogy-making&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36866;&#24212;&#26032;&#39062;&#24773;&#22659;--&#36825;&#26159;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20173;&#28982;&#32570;&#20047;&#30340;&#33021;&#21147;&#12290;&#22823;&#22810;&#25968;&#31867;&#27604;&#25968;&#25454;&#38598;&#20170;&#22825;&#20851;&#27880;&#31616;&#21333;&#30340;&#31867;&#27604;&#65288;&#20363;&#22914;&#65292;&#35789;&#31867;&#27604;&#65289;&#65307;&#21253;&#21547;&#22797;&#26434;&#31867;&#22411;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#25163;&#24037;&#31574;&#21010;&#30340;&#65292;&#24182;&#19988;&#38750;&#24120;&#23567;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#38480;&#21046;&#20102;&#35745;&#31639;&#31867;&#27604;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;ParallelPARC&#65288;Parallel Paragraph Creator&#65289;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21019;&#24314;&#22522;&#20110;&#27573;&#33853;&#30340;&#22797;&#26434;&#31867;&#27604;&#65292;&#20197;&#21450;&#31616;&#21333;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24178;&#25200;&#39033;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#65292;&#24182;&#21019;&#24314;&#20102;ProPara-Logy&#65292;&#19968;&#20010;&#20851;&#20110;&#31185;&#23398;&#36807;&#31243;&#38388;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#30001;&#20154;&#31867;&#39564;&#35777;&#36807;&#30340;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#20108;&#36827;&#21046;&#21644;&#22810;&#36873;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;LLMs&#21644;&#20154;&#31867;&#23545;&#31867;&#27604;&#30340;&#35782;&#21035;&#65292;&#21457;&#29616;&#20154;&#31867;&#32988;&#36807;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01139v1 Announce Type: cross  Abstract: Analogy-making is central to human cognition, allowing us to adapt to novel situations -- an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy. In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs' and humans' analogy recognition in binary and multiple-choice settings, and found that humans outperform the best mod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20840;&#23616;&#21098;&#26525;&#65288;AdaGP&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#20840;&#23616;&#21098;&#26525;&#36807;&#31243;&#20026;&#21487;&#31649;&#29702;&#30340;&#21327;&#35843;&#23376;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#20248;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17946</link><description>&lt;p&gt;
&#26080;&#26799;&#24230;&#33258;&#36866;&#24212;&#20840;&#23616;&#21098;&#26525;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gradient-Free Adaptive Global Pruning for Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20840;&#23616;&#21098;&#26525;&#65288;AdaGP&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#20840;&#23616;&#21098;&#26525;&#36807;&#31243;&#20026;&#21487;&#31649;&#29702;&#30340;&#21327;&#35843;&#23376;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#20248;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;LLaMA&#21644;GPT&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#21463;&#21040;&#23427;&#20204;&#35745;&#31639;&#38656;&#27714;&#36807;&#39640;&#30340;&#38480;&#21046;&#12290;&#21098;&#26525;&#20316;&#20026;&#19968;&#31181;&#20851;&#38190;&#30340;&#21387;&#32553;&#31574;&#30053;&#20986;&#29616;&#65292;&#24341;&#20837;&#31232;&#30095;&#24615;&#20197;&#22686;&#24378;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20840;&#23616;&#21098;&#26525;&#23545;LLMs&#26469;&#35828;&#30001;&#20110;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#32780;&#19981;&#23454;&#29992;&#65292;&#32780;&#26412;&#22320;&#21098;&#26525;&#65292;&#23613;&#31649;&#25928;&#29575;&#39640;&#65292;&#21364;&#23548;&#33268;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20840;&#23616;&#21098;&#26525;&#65288;AdaGP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#26032;&#23450;&#20041;&#20840;&#23616;&#21098;&#26525;&#22788;&#29702;&#20026;&#21487;&#31649;&#29702;&#30340;&#21327;&#35843;&#23376;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#36164;&#28304;&#26377;&#25928;&#30340;&#20840;&#23616;&#26368;&#20248;&#21270;&#20248;&#21270;&#12290;AdaGP&#30340;&#26041;&#27861;&#23558;LLMs&#27010;&#24565;&#21270;&#20026;&#19968;&#31995;&#21015;&#27169;&#22359;&#21270;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#36741;&#21161;&#21464;&#37327;&#36827;&#34892;&#38382;&#39064;&#20998;&#35299;&#65292;&#19981;&#20165;&#20415;&#20110;&#22312;LLMs&#19978;&#23454;&#29616;&#23454;&#38469;&#24212;&#29992;&#65292;&#32780;&#19988;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17946v1 Announce Type: new  Abstract: The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose Adaptive Global Pruning (AdaGP), a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. AdaGP's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, part
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25903;&#25345;&#25991;&#26723;&#20013;&#36873;&#25321;&#19978;&#19979;&#25991;&#24863;&#30693;&#30701;&#35821;&#30340;&#26032;&#39062;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24335;&#33258;&#25105;&#24378;&#21270;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#20934;&#30830;&#24615;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#25552;&#39640;&#20102;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.17532</link><description>&lt;p&gt;
&#26816;&#32034;&#21363;&#31934;&#20934;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Retrieval is Accurate Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17532
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25903;&#25345;&#25991;&#26723;&#20013;&#36873;&#25321;&#19978;&#19979;&#25991;&#24863;&#30693;&#30701;&#35821;&#30340;&#26032;&#39062;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24335;&#33258;&#25105;&#24378;&#21270;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#20934;&#30830;&#24615;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#25552;&#39640;&#20102;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20174;&#22266;&#23450;&#30340;&#12289;&#26377;&#38480;&#30340;&#21644;&#29420;&#31435;&#30340;&#35789;&#27719;&#20013;&#36873;&#25321;&#26631;&#35760;&#26469;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20174;&#19968;&#32452;&#25903;&#25345;&#25991;&#26723;&#20013;&#36873;&#25321;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#35821;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#20013;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#30830;&#23450;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#20026;&#25991;&#26412;&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#20998;&#21106;&#65292;&#24182;&#19988;&#27599;&#20010;&#29255;&#27573;&#37117;&#21487;&#20197;&#20174;&#22810;&#20010;&#21487;&#33021;&#30340;&#25991;&#26723;&#20013;&#26816;&#32034;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35821;&#35328;&#21551;&#21457;&#24335;&#21021;&#22987;&#21270;&#35757;&#32451;&#25968;&#25454;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#36890;&#36807;&#36845;&#20195;&#24335;&#33258;&#25105;&#24378;&#21270;&#26469;&#24341;&#23548;&#35757;&#32451;&#25968;&#25454;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#20248;&#20110;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#19988;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#20013;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#20363;&#22914;&#65292;&#19982;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#23545;&#24212;&#30340;&#27169;&#22411;&#65292;&#22312;&#24320;&#25918;&#24615;&#20219;&#21153;&#19978;&#23558;&#20934;&#30830;&#29575;&#20174;23.47%&#25552;&#39640;&#21040;36.27%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17532v1 Announce Type: new  Abstract: Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on Open
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26102;&#65292;&#26159;&#21542;&#33021;&#22815;&#22797;&#29616;&#20154;&#31867;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#23637;&#31034;&#30340;&#35821;&#35328;&#21464;&#21270;&#24615;</title><link>https://arxiv.org/abs/2402.17527</link><description>&lt;p&gt;
&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#65306;&#20154;&#31867;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;_____
&lt;/p&gt;
&lt;p&gt;
Predict the Next Word: &lt;Humans exhibit uncertainty in this task and language models _____&gt;
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17527
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26102;&#65292;&#26159;&#21542;&#33021;&#22815;&#22797;&#29616;&#20154;&#31867;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#23637;&#31034;&#30340;&#35821;&#35328;&#21464;&#21270;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#35757;&#32451;&#29992;&#20110;&#20026;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#20998;&#37197;&#27010;&#29575;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#21512;&#29702;&#36136;&#30097;&#23427;&#20204;&#26159;&#21542;&#24456;&#22909;&#22320;&#36817;&#20284;&#20154;&#31867;&#23637;&#31034;&#30340;&#35821;&#35328;&#21464;&#21270;&#24615;&#12290;&#36825;&#31181;&#24418;&#24335;&#30340;&#32479;&#35745;&#35780;&#20272;&#22312;&#27573;&#33853;&#32423;&#21035;&#19978;&#24456;&#38590;&#25191;&#34892;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#21487;&#25509;&#21463;&#24615;&#21028;&#26029;&#65288;&#21363;&#65292;&#20154;&#31867;&#35780;&#20272;&#65289;&#25110;&#19968;&#20010;&#24378;&#22823;&#30340;&#33258;&#21160;&#20195;&#29702;&#65288;&#36825;&#26159;&#19981;&#24179;&#20961;&#30340;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#21333;&#35789;&#32423;&#21035;&#19978;&#65292;&#36890;&#36807;&#32473;&#23450;&#19968;&#20123;&#19978;&#19979;&#25991;&#65292;&#21487;&#20197;&#36890;&#36807;&#19982;&#19968;&#20010;&#39044;&#20808;&#35760;&#24405;&#30340;&#26367;&#20195;&#21333;&#35789;&#36830;&#32493;&#25968;&#25454;&#38598;&#30340;&#31934;&#30830;&#21305;&#37197;&#26469;&#35780;&#20272;LM&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20107;&#23454;&#65292;&#24182;&#35780;&#20272;LM&#37325;&#26032;&#29983;&#25104;&#20154;&#31867;&#65288;&#29305;&#21035;&#26159;&#19968;&#32676;&#33521;&#35821;&#20351;&#29992;&#32773;&#65289;&#22312;&#8220;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#8221;&#20219;&#21153;&#20013;&#23637;&#31034;&#30340;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;&#36825;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#26657;&#20934;&#35780;&#20272;&#65292;&#22312;&#25991;&#26412;&#20998;&#31867;&#30340;&#32972;&#26223;&#19979;&#65292;Baan&#31561;&#20154;&#65288;2022&#24180;&#65289;&#23558;&#20854;&#31216;&#20026;&#23545;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17527v1 Announce Type: cross  Abstract: Language models (LMs) are statistical models trained to assign probability to human-generated text. As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgements (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM's ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the 'next word prediction' task. This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertaint
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#27880;&#37322;&#36807;&#31243;&#12289;&#35821;&#35328;&#21551;&#21457;&#12289;&#26597;&#25214;&#34920;&#12289;&#22806;&#37096;&#30693;&#35782;&#28304;&#21644;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#25104;&#26412;&#21644;&#19987;&#23478;&#26631;&#27880;&#20154;&#21592;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#22312;&#20851;&#31995;&#25277;&#21462;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;LLMs&#12290;</title><link>https://arxiv.org/abs/2402.16159</link><description>&lt;p&gt;
DistALANER&#65306;&#24320;&#28304;&#36719;&#20214;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#36828;&#31243;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#22686;&#24378;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16159
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#27880;&#37322;&#36807;&#31243;&#12289;&#35821;&#35328;&#21551;&#21457;&#12289;&#26597;&#25214;&#34920;&#12289;&#22806;&#37096;&#30693;&#35782;&#28304;&#21644;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#25104;&#26412;&#21644;&#19987;&#23478;&#26631;&#27880;&#20154;&#21592;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#22312;&#20851;&#31995;&#25277;&#21462;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#20840;&#38754;&#30340;&#20004;&#27493;&#36828;&#31243;&#30417;&#30563;&#27880;&#37322;&#36807;&#31243;&#26469;&#35299;&#20915;&#36719;&#20214;&#25968;&#25454;&#26631;&#27880;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#35813;&#36807;&#31243;&#24039;&#22937;&#22320;&#21033;&#29992;&#35821;&#35328;&#21551;&#21457;&#12289;&#29420;&#29305;&#30340;&#26597;&#25214;&#34920;&#12289;&#22806;&#37096;&#30693;&#35782;&#28304;&#20197;&#21450;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#25104;&#26412;&#21644;&#19987;&#23478;&#26631;&#27880;&#20154;&#21592;&#31232;&#32570;&#25152;&#24102;&#26469;&#30340;&#38480;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;LLMs&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;NER&#22312;&#20851;&#31995;&#25277;&#21462;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16159v1 Announce Type: new  Abstract: This paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems. Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process. This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach. By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators. It is noteworthy that our framework significantly outperforms the state-of-the-art LLMs by a substantial margin. We also show the effectiveness of NER in the downstream task of relation extraction.
&lt;/p&gt;</description></item><item><title>KorNAT&#26159;&#39318;&#20010;&#29992;&#20110;&#35780;&#20272;&#38889;&#22269;&#22269;&#23478;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#31038;&#20250;&#20215;&#20540;&#35266;&#21644;&#24120;&#35782;&#23545;&#40784;&#20004;&#20010;&#26041;&#38754;&#65292;&#36890;&#36807;&#23545;&#31038;&#20250;&#20215;&#20540;&#21644;&#24120;&#35782;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.13605</link><description>&lt;p&gt;
KorNAT&#65306;&#38889;&#22269;&#31038;&#20250;&#20215;&#20540;&#35266;&#21644;&#24120;&#35782;&#30340;LLM&#23545;&#40784;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13605
&lt;/p&gt;
&lt;p&gt;
KorNAT&#26159;&#39318;&#20010;&#29992;&#20110;&#35780;&#20272;&#38889;&#22269;&#22269;&#23478;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#31038;&#20250;&#20215;&#20540;&#35266;&#21644;&#24120;&#35782;&#23545;&#40784;&#20004;&#20010;&#26041;&#38754;&#65292;&#36890;&#36807;&#23545;&#31038;&#20250;&#20215;&#20540;&#21644;&#24120;&#35782;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29305;&#23450;&#22269;&#23478;&#24471;&#20197;&#26377;&#25928;&#37096;&#32626;&#65292;&#23427;&#20204;&#24517;&#39035;&#20855;&#26377;&#23545;&#35813;&#22269;&#25991;&#21270;&#21644;&#22522;&#26412;&#30693;&#35782;&#30340;&#29702;&#35299;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22269;&#23478;&#23545;&#40784;&#65288;National Alignment&#65289;&#65292;&#20174;&#31038;&#20250;&#20215;&#20540;&#35266;&#23545;&#40784;&#21644;&#24120;&#35782;&#23545;&#40784;&#20004;&#20010;&#26041;&#38754;&#34913;&#37327;LLM&#19982;&#30446;&#26631;&#22269;&#23478;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#31038;&#20250;&#20215;&#20540;&#35266;&#23545;&#40784;&#35780;&#20272;&#27169;&#22411;&#23545;&#29305;&#23450;&#22269;&#23478;&#31038;&#20250;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#31243;&#24230;&#65292;&#32780;&#24120;&#35782;&#23545;&#40784;&#21017;&#26816;&#39564;&#27169;&#22411;&#23545;&#30456;&#20851;&#22522;&#26412;&#22269;&#23478;&#30693;&#35782;&#30340;&#25226;&#25569;&#24773;&#20917;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;KorNAT&#65292;&#36825;&#26159;&#39318;&#20010;&#34913;&#37327;&#19982;&#38889;&#22269;&#22269;&#23478;&#23545;&#40784;&#30340;&#22522;&#20934;&#12290;&#23545;&#20110;&#31038;&#20250;&#20215;&#20540;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;&#21253;&#25324;6174&#21517;&#38889;&#22269;&#21442;&#19982;&#32773;&#22312;&#20869;&#30340;&#22823;&#35268;&#27169;&#35843;&#26597;&#20013;&#33719;&#24471;&#20102;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#12290;&#23545;&#20110;&#24120;&#35782;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22522;&#20110;&#38889;&#22269;&#25945;&#31185;&#20070;&#21644;GED&#21442;&#32771;&#36164;&#26009;&#26500;&#24314;&#20102;&#26679;&#26412;&#12290;KorNAT&#21253;&#21547;4K&#21644;6K&#20010;&#38024;&#23545;&#31038;&#20250;&#20215;&#20540;&#21644;&#24120;&#35782;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13605v1 Announce Type: new  Abstract: For Large Language Models (LLMs) to be effectively deployed in a specific country, they must possess an understanding of the nation's culture and basic knowledge. To this end, we introduce National Alignment, which measures an alignment between an LLM and a targeted country from two aspects: social value alignment and common knowledge alignment. Social value alignment evaluates how well the model understands nation-specific social values, while common knowledge alignment examines how well the model captures basic knowledge related to the nation. We constructed KorNAT, the first benchmark that measures national alignment with South Korea. For the social value dataset, we obtained ground truth labels from a large-scale survey involving 6,174 unique Korean participants. For the common knowledge dataset, we constructed samples based on Korean textbooks and GED reference materials. KorNAT contains 4K and 6K multiple-choice questions for socia
&lt;/p&gt;</description></item><item><title>GenAudit&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;&#20462;&#35746;&#25110;&#21024;&#38500;&#26410;&#34987;&#21442;&#32771;&#25991;&#29486;&#25903;&#25345;&#30340;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#29486;&#30340;&#35777;&#25454;&#65292;&#24110;&#21161;&#20462;&#22797;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.12566</link><description>&lt;p&gt;
GenAudit&#65306;&#21033;&#29992;&#35777;&#25454;&#20462;&#22797;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12566
&lt;/p&gt;
&lt;p&gt;
GenAudit&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;&#20462;&#35746;&#25110;&#21024;&#38500;&#26410;&#34987;&#21442;&#32771;&#25991;&#29486;&#25903;&#25345;&#30340;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#29486;&#30340;&#35777;&#25454;&#65292;&#24110;&#21161;&#20462;&#22797;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#21363;&#20351;&#21487;&#20197;&#35775;&#38382;&#21442;&#32771;&#25991;&#26723;&#65292;&#20063;&#21487;&#33021;&#29983;&#25104;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#38472;&#36848;&#12290;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65288;&#20363;&#22914;&#22522;&#20110;&#25991;&#26723;&#30340;&#21307;&#30103;&#20445;&#20581;&#25110;&#37329;&#34701;&#38382;&#31572;&#65289;&#65292;&#36825;&#26679;&#30340;&#38169;&#35823;&#21487;&#33021;&#20855;&#26377;&#21361;&#38505;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenAudit -- &#19968;&#20010;&#26088;&#22312;&#24110;&#21161;&#26816;&#26597;&#22522;&#20110;&#25991;&#26723;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#30340;&#24037;&#20855;&#12290;GenAudit&#36890;&#36807;&#20462;&#35746;&#25110;&#21024;&#38500;&#26410;&#34987;&#21442;&#32771;&#25991;&#26723;&#25903;&#25345;&#30340;&#22768;&#26126;&#65292;&#21516;&#26102;&#20026;&#30475;&#20284;&#34987;&#35777;&#25454;&#25903;&#25345;&#30340;&#20107;&#23454;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#29486;&#30340;&#35777;&#25454;&#65292;&#26469;&#24314;&#35758;&#20462;&#25913;LLM&#21709;&#24212;&#12290;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#26469;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20132;&#20114;&#30028;&#38754;&#65292;&#21521;&#29992;&#25143;&#21576;&#29616;&#24314;&#35758;&#30340;&#20462;&#25913;&#21644;&#35777;&#25454;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20998;&#21592;&#30340;&#20840;&#38754;&#35780;&#20272;&#26174;&#31034;&#65292;GenAudit&#22312;&#24635;&#32467;&#19981;&#21516;&#39046;&#22495;&#25991;&#26723;&#26102;&#33021;&#22815;&#26816;&#27979;&#20986;8&#31181;&#19981;&#21516;&#30340;LLM&#36755;&#20986;&#20013;&#30340;&#38169;&#35823;&#12290;&#20026;&#30830;&#20445;&#31995;&#32479;&#33021;&#22815;&#26631;&#35760;&#22823;&#22810;&#25968;&#38169;&#35823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#38169;&#35823;&#21484;&#22238;&#29575;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#39044;&#22788;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12566v1 Announce Type: new  Abstract: LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance). We present GenAudit -- a tool intended to assist fact-checking LLM responses for document-grounded tasks. GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support. We train models to execute these tasks, and design an interactive interface to present suggested edits and evidence to users. Comprehensive evaluation by human raters shows that GenAudit can detect errors in 8 different LLM outputs when summarizing documents from diverse domains. To ensure that most errors are flagged by the system, we propose a method that can increase the error recall while minimizing impact on pre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#26368;&#26032;&#33521;&#35821;&#26032;&#35789;&#36164;&#28304;&#65292;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26032;&#35789;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#26032;&#35789;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#26426;&#22120;&#32763;&#35793;&#20013;&#27169;&#22411;&#24615;&#33021;&#20250;&#22240;&#24341;&#20837;&#26032;&#35789;&#32780;&#20960;&#20046;&#20943;&#21322;&#12290;</title><link>https://arxiv.org/abs/2402.12261</link><description>&lt;p&gt;
NEO-BENCH&#65306;&#20351;&#29992;&#26032;&#35789;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#26368;&#26032;&#33521;&#35821;&#26032;&#35789;&#36164;&#28304;&#65292;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26032;&#35789;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#26032;&#35789;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#26426;&#22120;&#32763;&#35793;&#20013;&#27169;&#22411;&#24615;&#33021;&#20250;&#22240;&#24341;&#20837;&#26032;&#35789;&#32780;&#20960;&#20046;&#20943;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs)&#30340;&#34920;&#29616;&#20250;&#22240;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#19982;&#25512;&#29702;&#36807;&#31243;&#20013;&#30475;&#21040;&#30340;&#26032;&#25991;&#26412;&#20043;&#38388;&#30340;&#26102;&#38388;&#28418;&#31227;&#32780;&#36864;&#21270;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23548;&#33268;&#25968;&#25454;&#28418;&#31227;&#30340;&#35821;&#35328;&#21464;&#21270;&#20013;&#19968;&#20010;&#19981;&#22826;&#34987;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#21363;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#32780;&#20986;&#29616;&#30340;&#26032;&#35789;&#24418;&#24335;&#8212;&#8212;&#26032;&#35789;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20960;&#31181;&#27969;&#34892;&#30340;&#25910;&#38598;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#26368;&#26032;&#33521;&#35821;&#26032;&#35789;&#36164;&#28304;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#21253;&#21547;&#26032;&#35789;&#30340;&#21477;&#23376;&#19982;&#23558;&#26032;&#35789;&#26367;&#25442;&#20026;&#29616;&#26377;&#26367;&#20195;&#35789;&#30340;&#20960;&#20046;&#30456;&#21516;&#30340;&#21477;&#23376;&#26469;&#20998;&#26512;&#26032;&#35789;&#23545;&#26102;&#38388;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#22312;&#21477;&#23376;&#20013;&#24341;&#20837;&#21333;&#20010;&#26032;&#35789;&#26102;&#65292;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#20960;&#20046;&#20943;&#21322;&#12290;&#21463;&#21040;&#36825;&#20123;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;LLMs&#23545;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#21644;&#27169;&#22411;&#22256;&#24785;&#24230;&#20013;&#26032;&#35789;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21518;&#26399;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#30340;&#27169;&#22411;&#20135;&#29983;&#36739;&#20302;&#30340;&#22256;&#24785;&#24230;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12261v1 Announce Type: new  Abstract: The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks
&lt;/p&gt;</description></item><item><title>MatPlotAgent&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#33258;&#21160;&#21270;&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#26694;&#26550;&#65292;&#21253;&#25324;&#26597;&#35810;&#29702;&#35299;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#35270;&#35273;&#21453;&#39304;&#26426;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;MatPlotBench&#22522;&#20934;&#21644;GPT-4V&#35780;&#20998;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11453</link><description>&lt;p&gt;
MatPlotAgent: &#22522;&#20110;LLM&#30340;Agent&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#26041;&#27861;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11453
&lt;/p&gt;
&lt;p&gt;
MatPlotAgent&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#33258;&#21160;&#21270;&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#26694;&#26550;&#65292;&#21253;&#25324;&#26597;&#35810;&#29702;&#35299;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#35270;&#35273;&#21453;&#39304;&#26426;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;MatPlotBench&#22522;&#20934;&#21644;GPT-4V&#35780;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#36890;&#36807;&#30452;&#25509;&#23637;&#31034;&#22797;&#26434;&#20449;&#24687;&#24182;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35782;&#21035;&#38544;&#21547;&#27169;&#24335;&#65292;&#22312;&#30740;&#31350;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;Large Language Models&#65288;LLMs&#65289;&#36827;&#34892;&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#30340;&#30740;&#31350;&#20173;&#36739;&#20026;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MatPlotAgent&#65292;&#19968;&#31181;&#39640;&#25928;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;LLM&#20195;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#20219;&#21153;&#12290;MatPlotAgent&#21033;&#29992;&#20195;&#30721;LLMs&#21644;&#22810;&#27169;&#24577;LLMs&#30340;&#33021;&#21147;&#65292;&#30001;&#19977;&#20010;&#26680;&#24515;&#27169;&#22359;&#32452;&#25104;&#65306;&#26597;&#35810;&#29702;&#35299;&#12289;&#24102;&#26377;&#36845;&#20195;&#35843;&#35797;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20197;&#21450;&#29992;&#20110;&#38169;&#35823;&#26356;&#27491;&#30340;&#35270;&#35273;&#21453;&#39304;&#26426;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#35813;&#39046;&#22495;&#32570;&#20047;&#22522;&#20934;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MatPlotBench&#65292;&#19968;&#20010;&#30001;100&#20010;&#32463;&#20154;&#24037;&#39564;&#35777;&#30340;&#27979;&#35797;&#26696;&#20363;&#32452;&#25104;&#30340;&#39640;&#36136;&#37327;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;GPT-4V&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#30340;&#35780;&#20998;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#8230;&#65288;&#26410;&#23436;&#25972;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11453v1 Announce Type: new  Abstract: Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate t
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;GraphPrompter&#26694;&#26550;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#23558;&#22270;&#20449;&#24687;&#19982;LLMs&#23545;&#40784;&#65292;&#20197;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#29702;&#35299;&#22270;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10359</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#29992;&#36719;&#25552;&#31034;LLMs&#26469;&#36827;&#34892;&#22270;&#23398;&#20064;&#20219;&#21153;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we soft prompt LLMs for graph learning tasks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10359
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;GraphPrompter&#26694;&#26550;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#23558;&#22270;&#20449;&#24687;&#19982;LLMs&#23545;&#40784;&#65292;&#20197;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#29702;&#35299;&#22270;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22312;&#34920;&#31034;&#31038;&#20132;&#32593;&#32476;&#12289;&#29983;&#29289;&#25968;&#25454;&#21644;&#24341;&#29992;&#32593;&#32476;&#31561;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#36825;&#20351;&#24471;&#23558;LLMs&#24212;&#29992;&#20110;&#22270;&#34920;&#26684;&#23588;&#20026;&#35825;&#20154;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;LLMs&#24212;&#29992;&#20110;&#22270;&#34920;&#26684;&#24418;&#24335;&#23384;&#22312;&#29420;&#29305;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#34920;&#26684;&#24418;&#24335;&#19982;&#25991;&#26412;&#24418;&#24335;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#21644;&#19981;&#21305;&#37197;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#29702;&#35299;&#22270;&#20449;&#24687;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GraphPrompter&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#36719;&#25552;&#31034;&#26469;&#23558;&#22270;&#20449;&#24687;&#19982;LLMs&#23545;&#40784;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GraphPrompter&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32534;&#30721;&#22797;&#26434;&#30340;&#22270;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#25991;&#26412;&#20449;&#24687;&#30340;LLM&#12290;&#22312;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10359v1 Announce Type: cross  Abstract: Graph plays an important role in representing complex relationships in real-world applications such as social networks, biological data and citation networks. In recent years, Large Language Models (LLMs) have achieved tremendous success in various domains, which makes applying LLMs to graphs particularly appealing. However, directly applying LLMs to graph modalities presents unique challenges due to the discrepancy and mismatch between the graph and text modalities. Hence, to further investigate LLMs' potential for comprehending graph information, we introduce GraphPrompter, a novel framework designed to align graph information with LLMs via soft prompts. Specifically, GraphPrompter consists of two main components: a graph neural network to encode complex graph information and an LLM that effectively processes textual information. Comprehensive experiments on various benchmark datasets under node classification and link prediction tas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#24182;&#24314;&#31435;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#32467;&#26524;&#30417;&#30563;&#21644;&#36807;&#31243;&#30417;&#30563;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.05808</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#24182;&#24314;&#31435;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#32467;&#26524;&#30417;&#30563;&#21644;&#36807;&#31243;&#30417;&#30563;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;R$^3$&#65306;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#25512;&#29702;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21482;&#20351;&#29992;&#32467;&#26524;&#30417;&#30563;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36807;&#31243;&#30417;&#30563;&#30340;&#22909;&#22788;&#12290;&#23558;RL&#24212;&#29992;&#20110;&#22797;&#26434;&#25512;&#29702;&#30340;&#26680;&#24515;&#25361;&#25112;&#26159;&#30830;&#23450;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#20197;&#33719;&#24471;&#27491;&#21521;&#22870;&#21169;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#20248;&#21270;&#30417;&#30563;&#12290;&#32467;&#26524;&#30417;&#30563;&#20026;&#26368;&#32456;&#32467;&#26524;&#25552;&#20379;&#20102;&#31232;&#30095;&#22870;&#21169;&#65292;&#32780;&#19981;&#35782;&#21035;&#38169;&#35823;&#20301;&#32622;&#65292;&#32780;&#36807;&#31243;&#30417;&#30563;&#25552;&#20379;&#20102;&#36880;&#27493;&#22870;&#21169;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#12290;R$^3$&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;R$^3$&#23558;&#25512;&#29702;&#30340;&#36215;&#22987;&#29366;&#24577;&#20174;&#28436;&#31034;&#30340;&#32467;&#26463;&#28369;&#21160;&#21040;&#24320;&#22987;&#65292;&#20174;&#32780;&#22312;&#25152;&#26377;&#38454;&#27573;&#37117;&#20419;&#36827;&#20102;&#26356;&#23481;&#26131;&#30340;&#27169;&#22411;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;R$^3$&#24314;&#31435;&#20102;&#19968;&#20010;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#20351;&#32467;&#26524;&#30417;&#30563;&#33021;&#22815;&#25552;&#20379;&#38454;&#27573;&#32423;&#20449;&#21495;&#24182;&#31934;&#30830;&#23450;&#20301;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#20999;&#22359;&#36130;&#21153;&#25253;&#21578;&#65292;&#36890;&#36807;&#26681;&#25454;&#25991;&#26723;&#30340;&#32467;&#26500;&#20803;&#32032;&#32452;&#20214;&#36827;&#34892;&#20999;&#22359;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#20999;&#22359;&#22823;&#23567;&#65292;&#32780;&#26080;&#38656;&#35843;&#25972;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25972;&#20307;&#19978;&#19979;&#25991;&#21644;&#20934;&#30830;&#24615;&#30340;&#35780;&#20272;&#20197;&#21450;&#23545;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.05131</link><description>&lt;p&gt;
&#26377;&#25928;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#36130;&#21153;&#25253;&#21578;&#20999;&#22359;
&lt;/p&gt;
&lt;p&gt;
Financial Report Chunking for Effective Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#20999;&#22359;&#36130;&#21153;&#25253;&#21578;&#65292;&#36890;&#36807;&#26681;&#25454;&#25991;&#26723;&#30340;&#32467;&#26500;&#20803;&#32032;&#32452;&#20214;&#36827;&#34892;&#20999;&#22359;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#20999;&#22359;&#22823;&#23567;&#65292;&#32780;&#26080;&#38656;&#35843;&#25972;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25972;&#20307;&#19978;&#19979;&#25991;&#21644;&#20934;&#30830;&#24615;&#30340;&#35780;&#20272;&#20197;&#21450;&#23545;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#22359;&#20449;&#24687;&#26159;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27573;&#33853;&#32423;&#20999;&#22359;&#19978;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#25152;&#26377;&#25991;&#26412;&#37117;&#35270;&#20026;&#24179;&#31561;&#30340;&#65292;&#24182;&#24573;&#30053;&#20102;&#25991;&#26723;&#32467;&#26500;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#20165;&#20165;&#23558;&#25991;&#26723;&#20999;&#22359;&#21040;&#27573;&#33853;&#32423;&#21035;&#65292;&#32780;&#26159;&#26681;&#25454;&#25991;&#26723;&#30340;&#32467;&#26500;&#20803;&#32032;&#32452;&#20214;&#26469;&#20999;&#22359;&#12290;&#23558;&#25991;&#26723;&#20998;&#35299;&#20026;&#36825;&#20123;&#32452;&#25104;&#20803;&#32032;&#21487;&#20197;&#21019;&#24314;&#19968;&#31181;&#26032;&#30340;&#25991;&#26723;&#20999;&#22359;&#26041;&#24335;&#65292;&#21487;&#20197;&#24471;&#21040;&#26368;&#20339;&#30340;&#20999;&#22359;&#22823;&#23567;&#65292;&#26080;&#38656;&#35843;&#25972;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#35780;&#20272;&#26681;&#25454;&#30001;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#27880;&#37322;&#30340;&#20803;&#32032;&#31867;&#22411;&#36827;&#34892;&#20999;&#22359;&#22914;&#20309;&#23545;&#25152;&#26816;&#32034;&#20449;&#24687;&#30340;&#25972;&#20307;&#19978;&#19979;&#25991;&#21644;&#20934;&#30830;&#24615;&#36129;&#29486;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;RAG&#36741;&#21161;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;&#23545;&#21508;&#31181;&#20803;&#32032;&#31867;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#23427;&#20204;&#22312;&#26377;&#25928;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20316;&#29992;&#20197;&#21450;&#23427;&#20204;&#23545;&#20854;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chunking information is a key step in Retrieval Augmented Generation (RAG). Current research primarily centers on paragraph-level chunking. This approach treats all texts as equal and neglects the information contained in the structure of documents. We propose an expanded approach to chunk documents by moving beyond mere paragraph-level chunking to chunk primary by structural element components of documents. Dissecting documents into these constituent elements creates a new way to chunk documents that yields the best chunk size without tuning. We introduce a novel framework that evaluates how chunking based on element types annotated by document understanding models contributes to the overall context and accuracy of the information retrieved. We also demonstrate how this approach impacts RAG assisted Question &amp; Answer task performance. Our research includes a comprehensive analysis of various element types, their role in effective information retrieval, and the impact they have on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#19982;&#31526;&#21495;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#20197;&#26725;&#25509;&#20108;&#32773;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#24191;&#27867;NLP&#20219;&#21153;&#30340;&#26368;&#26032;&#28151;&#21512;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.11972</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#19982;&#31526;&#21495;&#26041;&#27861;&#30340;&#21327;&#21516;&#20316;&#29992;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#28151;&#21512;&#26041;&#27861;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Synergizing Machine Learning &amp; Symbolic Methods: A Survey on Hybrid Approaches to Natural Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#19982;&#31526;&#21495;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#20197;&#26725;&#25509;&#20108;&#32773;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#24191;&#27867;NLP&#20219;&#21153;&#30340;&#26368;&#26032;&#28151;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#31526;&#21495;&#26041;&#27861;&#30340;&#36827;&#27493;&#20984;&#26174;&#20102;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24378;&#22823;&#22320;&#22312;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#65292;&#20294;&#24448;&#24448;&#22312;&#23398;&#20064;NLP&#20219;&#21153;&#25152;&#38656;&#30340;&#24120;&#35782;&#21644;&#20107;&#23454;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31526;&#21495;&#26041;&#27861;&#25797;&#38271;&#34920;&#31034;&#30693;&#35782;&#20016;&#23500;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36866;&#24212;&#21160;&#24577;&#25968;&#25454;&#21644;&#27010;&#25324;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#36890;&#36807;&#28151;&#21512;&#26041;&#27861;&#26725;&#25509;&#36825;&#20004;&#31181;&#33539;&#24335;&#21487;&#20197;&#20943;&#36731;&#23427;&#20204;&#21508;&#33258;&#30340;&#24369;&#28857;&#65292;&#21516;&#26102;&#20445;&#25345;&#23427;&#20204;&#30340;&#20248;&#21183;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36190;&#32654;&#20102;&#36825;&#31181;&#32467;&#21512;&#30340;&#20248;&#28857;&#65292;&#22312;&#24191;&#27867;&#30340;NLP&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;NLP&#30340;&#28151;&#21512;&#26041;&#27861;&#30340;&#27010;&#20917;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#29992;&#20110;&#38656;&#35201;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#19968;&#31995;&#21015;NLP&#20219;&#21153;&#30340;&#26368;&#26032;&#28151;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11972v2 Announce Type: replace  Abstract: The advancement of machine learning and symbolic approaches have underscored their strengths and weaknesses in Natural Language Processing (NLP). While machine learning approaches are powerful in identifying patterns in data, they often fall short in learning commonsense and the factual knowledge required for the NLP tasks. Meanwhile, the symbolic methods excel in representing knowledge-rich data. However, they struggle to adapt dynamic data and generalize the knowledge. Bridging these two paradigms through hybrid approaches enables the alleviation of weaknesses in both while preserving their strengths. Recent studies extol the virtues of this union, showcasing promising results in a wide range of NLP tasks. In this paper, we present an overview of hybrid approaches used for NLP. Specifically, we delve into the state-of-the-art hybrid approaches used for a broad spectrum of NLP tasks requiring natural language understanding, generati
&lt;/p&gt;</description></item><item><title>CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2401.11944</link><description>&lt;p&gt;
CMMMU&#65306;&#19968;&#20010;&#20013;&#22269;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11944
&lt;/p&gt;
&lt;p&gt;
CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#19981;&#26029;&#25552;&#21319;&#65292;&#35780;&#20272;LMMs&#30340;&#34920;&#29616;&#26085;&#30410;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#22312;&#35780;&#20272;LMMs&#22312;&#20013;&#25991;&#31561;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26356;&#22823;&#24046;&#36317;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CMMMU&#65292;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;LMMs&#22312;&#38656;&#35201;&#22823;&#23398;&#27700;&#24179;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;CMMMU&#21463;&#21040;&#20102;MMMUs&#30340;&#26631;&#27880;&#21644;&#20998;&#26512;&#27169;&#24335;&#30340;&#21551;&#21457;&#24182;&#20005;&#26684;&#36981;&#24490;&#12290;CMMMU&#21253;&#25324;&#26469;&#33258;&#22823;&#23398;&#32771;&#35797;&#12289;&#27979;&#39564;&#21644;&#25945;&#31185;&#20070;&#30340;1.2&#19975;&#20010;&#25163;&#21160;&#25910;&#38598;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#28085;&#30422;&#20845;&#20010;&#26680;&#24515;&#23398;&#31185;&#65306;&#33402;&#26415;&#19982;&#35774;&#35745;&#12289;&#21830;&#19994;&#12289;&#31185;&#23398;&#12289;&#20581;&#24247;&#19982;&#21307;&#23398;&#12289;&#20154;&#25991;&#31038;&#31185;&#20197;&#21450;&#25216;&#26415;&#19982;&#24037;&#31243;&#65292;&#23601;&#20687;&#20854;&#20249;&#20276;MMMMU&#19968;&#26679;&#12290;&#36825;&#20123;&#38382;&#39064;&#28085;&#30422;30&#20010;&#23398;&#31185;&#65292;&#21253;&#25324;39&#20010;&#39640;&#24230;&#24322;&#36136;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11944v2 Announce Type: replace-cross  Abstract: As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU.   CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art &amp; Design, Business, Science, Health &amp; Medicine, Humanities &amp; Social Science, and Tech &amp; Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#25968;&#36824;&#26159;&#23569;&#25968;&#65288;MoM&#65289;&#23398;&#20064;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#22810;&#25968;&#31867;&#21035;&#21644;&#23569;&#25968;&#31867;&#21035;&#20043;&#38388;&#30340;&#25361;&#25112;&#65292;&#33021;&#22815;&#25552;&#39640;&#23569;&#25968;&#31867;&#21035;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#19981;&#24433;&#21709;&#22810;&#25968;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.11431</link><description>&lt;p&gt;
&#22810;&#25968;&#36824;&#26159;&#23569;&#25968;&#65306;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Majority or Minority: Data Imbalance Learning Method for Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11431
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#25968;&#36824;&#26159;&#23569;&#25968;&#65288;MoM&#65289;&#23398;&#20064;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#22810;&#25968;&#31867;&#21035;&#21644;&#23569;&#25968;&#31867;&#21035;&#20043;&#38388;&#30340;&#25361;&#25112;&#65292;&#33021;&#22815;&#25552;&#39640;&#23569;&#25968;&#31867;&#21035;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#19981;&#24433;&#21709;&#22810;&#25968;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#19981;&#24179;&#34913;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20219;&#21153;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20219;&#21153;&#12290;NER&#34920;&#29616;&#20986;&#19968;&#31181;&#38271;&#23614;&#20998;&#24067;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#20854;&#20013;&#26377;&#35768;&#22810;&#23569;&#25968;&#31867;&#21035;&#65288;&#21363;&#23454;&#20307;&#31867;&#21035;&#65289;&#21644;&#19968;&#20010;&#21333;&#19968;&#30340;&#22810;&#25968;&#31867;&#21035;&#65288;&#21363;O&#31867;&#21035;&#65289;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#23548;&#33268;&#23558;&#23454;&#20307;&#31867;&#21035;&#35823;&#20998;&#31867;&#20026;O&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#22810;&#25968;&#36824;&#26159;&#23569;&#25968;&#65288;MoM&#65289;&#23398;&#20064;&#12290;MoM&#23398;&#20064;&#23558;&#21482;&#26377;&#22320;&#38754;&#20107;&#23454;&#20026;&#22810;&#25968;&#31867;&#21035;&#30340;&#26679;&#26412;&#25152;&#35745;&#31639;&#30340;&#25439;&#22833;&#34701;&#20837;&#21040;&#20256;&#32479;ML&#27169;&#22411;&#30340;&#25439;&#22833;&#20013;&#12290;&#23545;&#22235;&#20010;NER&#25968;&#25454;&#38598;&#65288;&#26085;&#35821;&#21644;&#33521;&#35821;&#65289;&#30340;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#65292;MoM&#23398;&#20064;&#25552;&#39640;&#20102;&#23569;&#25968;&#31867;&#21035;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#22810;&#25968;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#24191;&#20026;&#20154;&#30693;&#30340;&#26368;&#26032;&#25216;&#26415;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11431v2 Announce Type: replace  Abstract: Data imbalance presents a significant challenge in various machine learning (ML) tasks, particularly named entity recognition (NER) within natural language processing (NLP). NER exhibits a data imbalance with a long-tail distribution, featuring numerous minority classes (i.e., entity classes) and a single majority class (i.e., O-class). This imbalance leads to misclassifications of the entity classes as the O-class. To tackle this issue, we propose a simple and effective learning method named majority or minority (MoM) learning. MoM learning incorporates the loss computed only for samples whose ground truth is the majority class into the loss of the conventional ML model. Evaluation experiments on four NER datasets (Japanese and English) showed that MoM learning improves prediction performance of the minority classes without sacrificing the performance of the majority class and is more effective than widely known and state-of-the-art
&lt;/p&gt;</description></item><item><title>GRAM&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25991;&#26723;&#32423;&#30340;&#25351;&#23450;&#23618;&#21644;&#21487;&#23398;&#20064;&#26631;&#35760;&#65292;&#23454;&#29616;&#20102;&#23558;&#21333;&#39029;&#27169;&#22411;&#25193;&#23637;&#21040;&#22810;&#39029;&#38754;&#35774;&#32622;&#65292;&#20419;&#36827;&#20449;&#24687;&#36328;&#39029;&#38754;&#30340;&#20840;&#23616;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2401.03411</link><description>&lt;p&gt;
GRAM:&#20840;&#23616;&#25512;&#29702;&#29992;&#20110;&#22810;&#39029;&#38754;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
GRAM: Global Reasoning for Multi-Page VQA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03411
&lt;/p&gt;
&lt;p&gt;
GRAM&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25991;&#26723;&#32423;&#30340;&#25351;&#23450;&#23618;&#21644;&#21487;&#23398;&#20064;&#26631;&#35760;&#65292;&#23454;&#29616;&#20102;&#23558;&#21333;&#39029;&#27169;&#22411;&#25193;&#23637;&#21040;&#22810;&#39029;&#38754;&#35774;&#32622;&#65292;&#20419;&#36827;&#20449;&#24687;&#36328;&#39029;&#38754;&#30340;&#20840;&#23616;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#25361;&#25112;&#36234;&#21457;&#31361;&#20986;&#12290;&#22312;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#65288;DocVQA&#65289;&#20013;&#65292;&#20027;&#27969;&#26041;&#27861;&#19987;&#27880;&#20110;&#21333;&#39029;&#35774;&#32622;&#65292;&#32780;&#25991;&#26723;&#21487;&#33021;&#36328;&#36234;&#25968;&#30334;&#39029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GRAM&#65292;&#19968;&#31181;&#33021;&#22815;&#26080;&#32541;&#23558;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#21333;&#39029;&#27169;&#22411;&#25193;&#23637;&#21040;&#22810;&#39029;&#38754;&#35774;&#32622;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#23494;&#38598;&#30340;&#39044;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#21333;&#39029;&#32534;&#30721;&#22120;&#36827;&#34892;&#23616;&#37096;&#39029;&#38754;&#32423;&#29702;&#35299;&#65292;&#24182;&#21033;&#29992;&#25991;&#26723;&#32423;&#25351;&#23450;&#23618;&#21644;&#21487;&#23398;&#20064;&#26631;&#35760;&#26469;&#22686;&#24378;&#23427;&#65292;&#20419;&#36827;&#20449;&#24687;&#22312;&#39029;&#38754;&#38388;&#30340;&#20840;&#23616;&#25512;&#29702;&#20256;&#36882;&#12290;&#20026;&#20102;&#30830;&#20445;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#26032;&#24341;&#20837;&#30340;&#25991;&#26723;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#20559;&#32622;&#36866;&#24212;&#26041;&#27861;&#12290;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#20026;&#39069;&#22806;&#30340;&#35745;&#31639;&#33410;&#30465;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#36873;&#30340;&#21387;&#32553;&#38454;&#27573;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#21387;&#32553;Transformer&#65288;C-Former&#65289;&#65292;&#20943;&#23569;&#20102;&#32534;&#30721;&#24207;&#21015;&#30340;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03411v2 Announce Type: replace  Abstract: The increasing use of transformer-based large language models brings forward the challenge of processing long sequences. In document visual question answering (DocVQA), leading methods focus on the single-page setting, while documents can span hundreds of pages. We present GRAM, a method that seamlessly extends pre-trained single-page models to the multi-page setting, without requiring computationally-heavy pretraining. To do so, we leverage a single-page encoder for local page-level understanding, and enhance it with document-level designated layers and learnable tokens, facilitating the flow of information across pages for global reasoning. To enforce our model to utilize the newly introduced document tokens, we propose a tailored bias adaptation method. For additional computational savings during decoding, we introduce an optional compression stage using our compression-transformer (C-Former),reducing the encoded sequence length, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SpanishTinyRoBERTa&#65292;&#19968;&#20010;&#22522;&#20110;RoBERTa&#30340;&#35199;&#29677;&#29273;&#35821;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#35199;&#29677;&#29273;&#35821;&#38382;&#31572;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.04193</link><description>&lt;p&gt;
&#35199;&#29677;&#29273;&#35821;&#38382;&#31572;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Language Model Knowledge Distillation for Efficient Question Answering in Spanish
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04193
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SpanishTinyRoBERTa&#65292;&#19968;&#20010;&#22522;&#20110;RoBERTa&#30340;&#35199;&#29677;&#29273;&#35821;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#35199;&#29677;&#29273;&#35821;&#38382;&#31572;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#23637;&#30340;&#39044;&#35757;&#32451;&#35199;&#29677;&#29273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;&#38382;&#31572;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#39640;&#25928;&#27169;&#22411;&#23545;&#36825;&#20123;&#27169;&#22411;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#37319;&#29992;&#26500;&#25104;&#20102;&#19968;&#36947;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#35199;&#29677;&#29273;&#35821;&#30340;&#36739;&#23567;&#30340;&#33976;&#39311;&#27169;&#22411;&#21487;&#33021;&#34987;&#35777;&#26126;&#26159;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#65292;&#24182;&#20419;&#36827;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#30340;&#36827;&#19968;&#27493;&#37319;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;RoBERTa&#30340;&#35199;&#29677;&#29273;&#35821;&#39640;&#25928;&#38382;&#31572;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;SpanishTinyRoBERTa&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#22823;&#27169;&#22411;&#21521;&#19968;&#20010;&#26356;&#36731;&#30340;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65292;&#36825;&#20351;&#24471;&#26356;&#24191;&#27867;&#30340;&#23454;&#29616;&#25104;&#20026;&#21487;&#33021;&#65292;&#21363;&#20351;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#65292;&#20063;&#33021;&#23454;&#29616;&#21487;&#24573;&#30053;&#30340;&#24615;&#33021;&#29306;&#29298;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#23494;&#38598;&#30340;&#33976;&#39311;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#38480;&#21046;&#30340;&#35745;&#31639;&#24615;&#33021;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04193v2 Announce Type: replace  Abstract: Recent advances in the development of pre-trained Spanish language models has led to significant progress in many Natural Language Processing (NLP) tasks, such as question answering. However, the lack of efficient models imposes a barrier for the adoption of such models in resource-constrained environments. Therefore, smaller distilled models for the Spanish language could be proven to be highly scalable and facilitate their further adoption on a variety of tasks and scenarios. In this work, we take one step in this direction by developing SpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficient question answering in Spanish. To achieve this, we employ knowledge distillation from a large model onto a lighter model that allows for a wider implementation, even in areas with limited computational resources, whilst attaining negligible performance sacrifice. Our experiments show that the dense distilled model can st
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24046;&#20998;&#31169;&#23494;&#31163;&#32447;&#25552;&#31034;&#35843;&#25972;&#65288;DP-OPT&#65289;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#25552;&#31034;&#26102;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#35843;&#25972;&#25552;&#31034;&#24182;&#24212;&#29992;&#20110;&#20113;&#27169;&#22411;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#20256;&#36755;</title><link>https://arxiv.org/abs/2312.03724</link><description>&lt;p&gt;
DP-OPT&#65306;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#24744;&#30340;&#38544;&#31169;&#20445;&#25252;&#25552;&#31034;&#24037;&#31243;&#24072;
&lt;/p&gt;
&lt;p&gt;
DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24046;&#20998;&#31169;&#23494;&#31163;&#32447;&#25552;&#31034;&#35843;&#25972;&#65288;DP-OPT&#65289;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#25552;&#31034;&#26102;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#35843;&#25972;&#25552;&#31034;&#24182;&#24212;&#29992;&#20110;&#20113;&#27169;&#22411;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#21508;&#31181;&#20219;&#21153;&#30340;&#20027;&#35201;&#24037;&#20855;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#38024;&#23545;&#29305;&#23450;&#30446;&#26631;&#26102;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35843;&#25972;&#30340;&#25552;&#31034;&#20381;&#36182;&#20110;&#25935;&#24863;&#30340;&#31169;&#20154;&#20449;&#24687;&#65292;&#22260;&#32469;&#25968;&#25454;&#38544;&#31169;&#30340;&#25285;&#24551;&#25552;&#20986;&#20102;&#38556;&#30861;&#12290;&#19968;&#20010;&#23454;&#38469;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25176;&#31649;&#19968;&#20010;&#26412;&#22320;&#30340;LLM&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#31169;&#19979;&#20248;&#21270;&#19968;&#20010;&#36719;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#24403;&#27169;&#22411;&#25152;&#26377;&#26435;&#21463;&#21040;&#20445;&#25252;&#26102;&#65292;&#25176;&#31649;&#19968;&#20010;&#26412;&#22320;&#27169;&#22411;&#23601;&#21464;&#24471;&#26377;&#38382;&#39064;&#12290;&#23558;&#25968;&#25454;&#21457;&#36865;&#32473;&#27169;&#22411;&#25552;&#20379;&#31243;&#24207;&#36827;&#34892;&#22521;&#35757;&#31561;&#26367;&#20195;&#26041;&#27861;&#21152;&#21095;&#20102;&#36825;&#20123;&#38544;&#31169;&#38382;&#39064;&#65292;&#38754;&#23545;&#19968;&#20010;&#19981;&#21463;&#20449;&#20219;&#30340;&#25552;&#20379;&#32773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24046;&#20998;&#31169;&#23494;&#31163;&#32447;&#25552;&#31034;&#35843;&#25972;&#65288;DP-OPT&#65289;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#22312;&#23458;&#25143;&#31471;&#35843;&#25972;&#31163;&#25955;&#25552;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;&#25152;&#38656;&#30340;&#20113;&#27169;&#22411;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;LLMs&#26412;&#36523;&#24314;&#35758;&#30340;&#25552;&#31034;&#21487;&#20197;&#22312;&#19981;&#26292;&#38706;p&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03724v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have emerged as dominant tools for various tasks, particularly when tailored for a specific target by prompt tuning. Nevertheless, concerns surrounding data privacy present obstacles due to the tuned prompts' dependency on sensitive private information. A practical solution is to host a local LLM and optimize a soft prompt privately using data. Yet, hosting a local model becomes problematic when model ownership is protected. Alternative methods, like sending data to the model's provider for training, intensify these privacy issues facing an untrusted provider. In this paper, we present a novel solution called Differentially-Private Offsite Prompt Tuning (DP-OPT) to address this challenge. Our approach involves tuning a discrete prompt on the client side and then applying it to the desired cloud models. We demonstrate that prompts suggested by LLMs themselves can be transferred without compromising p
&lt;/p&gt;</description></item><item><title>MobileGPT&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;LLM&#30340;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#31867;&#20154;&#24212;&#29992;&#35760;&#24518;&#27169;&#25311;&#20154;&#31867;&#19982;&#31227;&#21160;&#24212;&#29992;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#23454;&#29616;&#20219;&#21153;&#31243;&#24207;&#30340;&#31934;&#30830;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2312.03003</link><description>&lt;p&gt;
&#25506;&#32034;&#12289;&#36873;&#25321;&#12289;&#25512;&#23548;&#21644;&#22238;&#24518;&#65306;&#20026;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#22686;&#21152;&#31867;&#20154;&#35760;&#24518;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03003
&lt;/p&gt;
&lt;p&gt;
MobileGPT&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;LLM&#30340;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#31867;&#20154;&#24212;&#29992;&#35760;&#24518;&#27169;&#25311;&#20154;&#31867;&#19982;&#31227;&#21160;&#24212;&#29992;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#23454;&#29616;&#20219;&#21153;&#31243;&#24207;&#30340;&#31934;&#30830;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#23427;&#20204;&#20248;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#20351;&#29992;&#25143;&#33021;&#22815;&#33258;&#21160;&#25191;&#34892;&#22797;&#26434;&#21644;&#37325;&#22797;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#22266;&#26377;&#30340;&#19981;&#21487;&#38752;&#24615;&#21644;&#39640;&#36816;&#34892;&#25104;&#26412;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#30456;&#24403;&#26377;&#38480;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;MobileGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;LLM&#30340;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#37197;&#22791;&#20102;&#31867;&#20154;&#24212;&#29992;&#35760;&#24518;&#12290;MobileGPT&#27169;&#25311;&#20102;&#20154;&#31867;&#19982;&#31227;&#21160;&#24212;&#29992;&#20132;&#20114;&#30340;&#35748;&#30693;&#36807;&#31243;--&#25506;&#32034;&#12289;&#36873;&#25321;&#12289;&#25512;&#23548;&#21644;&#22238;&#24518;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#20219;&#21153;&#31243;&#24207;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#27169;&#22359;&#21270;&#30340;&#23376;&#20219;&#21153;&#65292;&#20801;&#35768;&#26356;&#31934;&#30830;&#12289;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#21153;&#27969;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#23376;&#20219;&#21153;&#30340;&#37325;&#22797;&#20351;&#29992;&#12289;&#37325;&#26032;&#25490;&#21015;&#21644;&#36866;&#24212;&#21508;&#31181;&#30446;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#32447;LLM&#26381;&#21153;&#65288;GPT-3.5&#21644;GPT-4&#65289;&#23454;&#29616;&#20102;MobileGPT&#65292;&#24182;&#22312;&#19968;&#32452;&#25968;&#25454;&#19978;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03003v2 Announce Type: replace-cross  Abstract: The advent of large language models (LLMs) has opened up new opportunities in the field of mobile task automation. Their superior language understanding and reasoning capabilities allow users to automate complex and repetitive tasks. However, due to the inherent unreliability and high operational cost of LLMs, their practical applicability is quite limited. To address these issues, this paper introduces MobileGPT, an innovative LLM-based mobile task automator equipped with a human-like app memory. MobileGPT emulates the cognitive process of humans interacting with a mobile app -- explore, select, derive, and recall. This approach allows for a more precise and efficient learning of a task's procedure by breaking it down into smaller, modular sub-tasks that can be re-used, re-arranged, and adapted for various objectives. We implement MobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its performance on a datase
&lt;/p&gt;</description></item><item><title>Bergeron&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33391;&#30693;&#30340;&#23545;&#40774;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#21442;&#25968;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2312.00029</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#33391;&#30693;&#30340;&#23545;&#20934;&#26694;&#26550;&#25269;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00029
&lt;/p&gt;
&lt;p&gt;
Bergeron&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33391;&#30693;&#30340;&#23545;&#40774;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#21442;&#25968;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24341;&#20837;&#65292;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#36827;&#23637;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#20195;&#23545;&#40784;&#26041;&#27861;&#20173;&#28982;&#26080;&#27861;&#23436;&#20840;&#38450;&#27490;&#22312;&#27169;&#22411;&#34987;&#33988;&#24847;&#25915;&#20987;&#26102;&#20135;&#29983;&#26377;&#23475;&#24212;&#23545;&#12290;&#20026;&#20102;&#24110;&#21161;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Bergeron&#65306;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;LLMs&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#26694;&#26550;&#65292;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#21442;&#25968;&#24494;&#35843;&#12290;Bergeron&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;&#65307;&#27425;&#35201;LLM&#27169;&#25311;&#21463;&#20445;&#25252;&#30340;&#20027;&#35201;LLM&#30340;&#33391;&#30693;&#12290;&#35813;&#26694;&#26550;&#22312;&#30417;&#35270;&#36755;&#20986;&#20197;&#26816;&#27979;&#20219;&#20309;&#26377;&#23475;&#20869;&#23481;&#30340;&#21516;&#26102;&#65292;&#26356;&#22909;&#22320;&#20445;&#25252;&#20027;&#35201;&#27169;&#22411;&#20813;&#21463;&#20837;&#20405;&#25915;&#20987;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#20351;&#29992;Bergeron&#26469;&#34917;&#20805;&#29616;&#26377;&#23545;&#40784;&#35757;&#32451;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00029v2 Announce Type: replace-cross  Abstract: Research into AI alignment has grown considerably since the recent introduction of increasingly capable Large Language Models (LLMs). Unfortunately, modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked. These attacks can trick seemingly aligned models into giving manufacturing instructions for dangerous materials, inciting violence, or recommending other immoral acts. To help mitigate this issue, we introduce Bergeron: a framework designed to improve the robustness of LLMs against attacks without any additional parameter fine-tuning. Bergeron is organized into two tiers; with a secondary LLM emulating the conscience of a protected, primary LLM. This framework better safeguards the primary model against incoming attacks while monitoring its output for any harmful content. Empirical analysis shows that, by using Bergeron to complement models with existing alignment traini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#23398;&#20064;&#33021;&#21147;&#65292;&#20174;&#32780;&#25512;&#21160;&#38646;-shot&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2311.08921</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#33258;&#25105;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Self-Improving for Zero-Shot Named Entity Recognition with Large Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#23398;&#20064;&#33021;&#21147;&#65292;&#20174;&#32780;&#25512;&#21160;&#38646;-shot&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25506;&#32034;&#23558;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24212;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#20219;&#21153;&#24341;&#36215;&#20102;&#24456;&#22823;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#26469;&#28608;&#21457;LLMs&#30340;&#33258;&#25105;&#23398;&#20064;&#33021;&#21147;&#65292;&#20174;&#32780;&#25512;&#21160;LLMs&#22312;&#38646;-shot NER&#19978;&#30340;&#24615;&#33021;&#36793;&#30028;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#23545;&#26410;&#26631;&#27880;&#35821;&#26009;&#24211;&#36827;&#34892;&#33258;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#24182;&#33719;&#24471;&#33258;&#25105;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25506;&#32034;&#21508;&#31181;&#31574;&#30053;&#26469;&#36873;&#25321;&#21487;&#38752;&#30340;&#27880;&#37322;&#65292;&#24418;&#25104;&#19968;&#20010;&#21487;&#38752;&#30340;&#33258;&#25105;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#26368;&#21518;&#65292;&#23545;&#20110;&#27599;&#20010;&#27979;&#35797;&#36755;&#20837;&#65292;&#25105;&#20204;&#20174;&#21487;&#38752;&#30340;&#33258;&#25105;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#28436;&#31034;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#25512;&#26029;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#30340;&#35268;&#27169;&#25110;&#36845;&#20195;&#27425;&#25968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08921v2 Announce Type: replace  Abstract: Exploring the application of powerful large language models (LLMs) on the named entity recognition (NER) task has drawn much attention recently. This work pushes the performance boundary of zero-shot NER with LLMs by proposing a training-free self-improving framework, which utilizes an unlabeled corpus to stimulate the self-learning ability of LLMs. First, we use the LLM to make predictions on the unlabeled corpus using self-consistency and obtain a self-annotated dataset. Second, we explore various strategies to select reliable annotations to form a reliable self-annotated dataset. Finally, for each test input, we retrieve demonstrations from the reliable self-annotated dataset and perform inference via in-context learning. Experiments on four benchmarks show substantial performance improvements achieved by our framework. Through comprehensive experimental analysis, we find that increasing the size of unlabeled corpus or iterations 
&lt;/p&gt;</description></item><item><title>&#35843;&#26597;&#32508;&#21512;&#23457;&#26597;&#20102;LLMs&#20013;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22312;&#20943;&#36731;&#24187;&#35273;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.07914</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#21542;&#21487;&#20197;&#20943;&#23569;LLMs&#20013;&#30340;&#24187;&#35273;&#65311;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07914
&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#32508;&#21512;&#23457;&#26597;&#20102;LLMs&#20013;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22312;&#20943;&#36731;&#24187;&#35273;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;LLMs&#24456;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65292;&#20027;&#35201;&#28304;&#20110;&#27169;&#22411;&#20869;&#30340;&#30693;&#35782;&#31354;&#32570;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38480;&#21046;&#65292;&#30740;&#31350;&#20154;&#21592;&#37319;&#29992;&#21508;&#31181;&#31574;&#30053;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#26469;&#22686;&#24378;LLMs&#65292;&#26088;&#22312;&#20943;&#23569;&#24187;&#35273;&#24182;&#25552;&#21319;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#20123;&#31574;&#30053;&#20013;&#65292;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#22806;&#37096;&#20449;&#24687;&#28304;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#23457;&#26597;&#20102;LLMs&#20013;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#22312;&#20943;&#36731;&#24187;&#35273;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#31995;&#32479;&#22320;&#24402;&#31867;&#20026;&#19977;&#20010;&#24635;&#20307;&#32452;&#65292;&#25552;&#20379;&#26041;&#27861;&#27604;&#36739;&#21644;&#24615;&#33021;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#19982;&#36825;&#20123;&#25216;&#26415;&#30456;&#20851;&#30340;&#24403;&#21069;&#36235;&#21183;&#21644;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07914v2 Announce Type: replace  Abstract: The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we comprehensively review these knowledge-graph-based augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations. Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field.
&lt;/p&gt;</description></item><item><title>HallusionBench&#26159;&#19968;&#20010;&#19987;&#20026;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#32972;&#26223;&#25512;&#29702;&#20013;&#38754;&#20020;&#25361;&#25112;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#32467;&#26500;&#21644;&#37327;&#21270;&#20998;&#26512;&#65292;&#26174;&#31034;&#20986;GPT-4V&#21462;&#24471;&#20102;31.42%&#30340;&#20934;&#30830;&#29575;&#65292;&#36828;&#39640;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2310.14566</link><description>&lt;p&gt;
HallusionBench&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#32416;&#32544;&#30340;&#35821;&#35328;&#24187;&#35273;&#21644;&#35270;&#24187;&#35273;&#30340;&#39640;&#32423;&#35786;&#26029;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14566
&lt;/p&gt;
&lt;p&gt;
HallusionBench&#26159;&#19968;&#20010;&#19987;&#20026;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#32972;&#26223;&#25512;&#29702;&#20013;&#38754;&#20020;&#25361;&#25112;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#32467;&#26500;&#21644;&#37327;&#21270;&#20998;&#26512;&#65292;&#26174;&#31034;&#20986;GPT-4V&#21462;&#24471;&#20102;31.42%&#30340;&#20934;&#30830;&#29575;&#65292;&#36828;&#39640;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;HallusionBench&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#35780;&#20272;&#22270;&#20687;&#32972;&#26223;&#25512;&#29702;&#32780;&#35774;&#35745;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;&#36825;&#20010;&#22522;&#20934;&#23545;&#20110;&#39640;&#32423;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#65288;&#22914;GPT-4V&#65288;Vision&#65289;&#12289;Gemini Pro Vision&#21644;LLaVA-1.5&#65289;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#24378;&#35843;&#23545;&#35270;&#35273;&#25968;&#25454;&#30340;&#24494;&#22937;&#29702;&#35299;&#21644;&#35299;&#37322;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;346&#24352;&#22270;&#20687;&#21644;1129&#20010;&#38382;&#39064;&#65292;&#20840;&#37096;&#30001;&#20154;&#31867;&#19987;&#23478;&#31934;&#24515;&#35774;&#35745;&#12290;&#25105;&#20204;&#20026;&#36825;&#20123;&#35270;&#35273;&#38382;&#39064;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#65292;&#26088;&#22312;&#24314;&#31435;&#23545;&#29031;&#32452;&#12290;&#36825;&#31181;&#32467;&#26500;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#27169;&#22411;&#30340;&#21709;&#24212;&#20542;&#21521;&#12289;&#36923;&#36753;&#19968;&#33268;&#24615;&#21644;&#21508;&#31181;&#25925;&#38556;&#27169;&#24335;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#23545;HallusionBench&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23545;14&#31181;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#20986;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;GPT-4V&#21462;&#24471;&#30340;31.42&#65285;&#30340;&#38382;&#39064;&#23545;&#20934;&#30830;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#26377;&#20854;&#20182;&#35780;&#20272;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#22343;&#20302;&#20110;16&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14566v3 Announce Type: replace-cross  Abstract: We introduce HallusionBench, a comprehensive benchmark designed for the evaluation of image-context reasoning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision), Gemini Pro Vision, and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on HallusionBench, we benchmarked 14 different models, highlighting a 31.42% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16%. Moreover, our analysis not only h
&lt;/p&gt;</description></item><item><title>Llemma&#26159;&#19968;&#20010;&#29992;&#20110;&#25968;&#23398;&#30340;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;MATH&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#36827;&#34892;&#24037;&#20855;&#20351;&#29992;&#21644;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2310.10631</link><description>&lt;p&gt;
Llemma: &#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#30340;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Llemma: An Open Language Model For Mathematics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10631
&lt;/p&gt;
&lt;p&gt;
Llemma&#26159;&#19968;&#20010;&#29992;&#20110;&#25968;&#23398;&#30340;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;MATH&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#36827;&#34892;&#24037;&#20855;&#20351;&#29992;&#21644;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Llemma&#65292;&#19968;&#20010;&#29992;&#20110;&#25968;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#32487;&#32493;&#22312;Proof-Pile-2&#19978;&#23545;Code Llama&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;Proof-Pile-2&#21253;&#21547;&#31185;&#23398;&#35770;&#25991;&#12289;&#21253;&#21547;&#25968;&#23398;&#20869;&#23481;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#25968;&#23398;&#20195;&#30721;&#65292;&#26368;&#32456;&#29983;&#25104;&#20102;Llemma&#12290;&#22312;MATH&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;Llemma&#22312;&#21516;&#31561;&#21442;&#25968;&#22522;&#30784;&#19978;&#32988;&#36807;&#25152;&#26377;&#24050;&#30693;&#30340;&#24320;&#28304;&#22522;&#20934;&#27169;&#22411;&#65292;&#20197;&#21450;&#23578;&#26410;&#21457;&#24067;&#30340;Minerva&#27169;&#22411;&#22871;&#20214;&#12290;&#27492;&#22806;&#65292;Llemma&#33021;&#22815;&#36827;&#34892;&#24037;&#20855;&#20351;&#29992;&#21644;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#25152;&#26377;&#24037;&#20214;&#65292;&#21253;&#25324;70&#20159;&#21644;340&#20159;&#21442;&#25968;&#27169;&#22411;&#12289;Proof-Pile-2&#20197;&#21450;&#29992;&#20110;&#22797;&#21046;&#25105;&#20204;&#23454;&#39564;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10631v3 Announce Type: replace-cross  Abstract: We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Toolink&#65292;&#19968;&#20010;&#36890;&#36807;&#38142;&#24335;&#35299;&#20915;&#26041;&#27861;&#39318;&#20808;&#21019;&#24314;&#24037;&#20855;&#21253;&#65292;&#20877;&#38598;&#25104;&#24037;&#20855;&#35268;&#21010;&#21644;&#35843;&#29992;&#30340;&#26694;&#26550;&#65292;&#25104;&#21151;&#36890;&#36807;&#23545;ChatGPT&#21644;CoS-GPT&#30340;&#23454;&#39564;&#65292;&#25171;&#36896;&#20102;LLaMA-CoS&#65292;&#19968;&#20010;&#20855;&#26377;&#20808;&#36827;&#24037;&#20855;&#35268;&#21010;&#21644;&#35843;&#29992;&#33021;&#21147;&#30340;&#24378;&#22823;&#24320;&#28304;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2310.05155</link><description>&lt;p&gt;
Toolink: &#38142;&#25509;&#24037;&#20855;&#21253;&#21019;&#24314;&#21644;&#20351;&#29992;&#30340;&#38142;&#24335;&#35299;&#20915;&#24320;&#28304;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05155
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Toolink&#65292;&#19968;&#20010;&#36890;&#36807;&#38142;&#24335;&#35299;&#20915;&#26041;&#27861;&#39318;&#20808;&#21019;&#24314;&#24037;&#20855;&#21253;&#65292;&#20877;&#38598;&#25104;&#24037;&#20855;&#35268;&#21010;&#21644;&#35843;&#29992;&#30340;&#26694;&#26550;&#65292;&#25104;&#21151;&#36890;&#36807;&#23545;ChatGPT&#21644;CoS-GPT&#30340;&#23454;&#39564;&#65292;&#25171;&#36896;&#20102;LLaMA-CoS&#65292;&#19968;&#20010;&#20855;&#26377;&#20808;&#36827;&#24037;&#20855;&#35268;&#21010;&#21644;&#35843;&#29992;&#33021;&#21147;&#30340;&#24378;&#22823;&#24320;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21033;&#29992;&#24037;&#20855;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20854;&#38381;&#28304;&#24615;&#21644;&#39640;&#25512;&#29702;&#25104;&#26412;&#23545;&#20854;&#36866;&#24212;&#24615;&#36896;&#25104;&#20102;&#38480;&#21046;&#65292;&#38656;&#35201;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36739;&#23567;&#30340;&#24320;&#28304;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Toolink&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38142;&#24335;&#35299;&#20915;&#65288;CoS&#65289;&#26041;&#27861;&#39318;&#20808;&#21019;&#24314;&#24037;&#20855;&#21253;&#65292;&#28982;&#21518;&#38598;&#25104;&#24037;&#20855;&#30340;&#35268;&#21010;&#21644;&#35843;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#39564;&#35777;&#20102;Toolink&#22312;&#21033;&#29992;&#27169;&#22411;&#30340;&#21019;&#36896;&#21147;&#21644;CoS&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;CoS-GPT&#65292;&#19968;&#20010;&#19987;&#20026;&#24037;&#20855;&#20351;&#29992;&#32780;&#35774;&#35745;&#30340;&#38142;&#24335;&#35299;&#20915;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;LLaMA-7B&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#32467;&#26524;&#26159;LLaMA-CoS&#65292;&#19968;&#20010;&#20855;&#26377;&#20808;&#36827;&#24037;&#20855;&#35268;&#21010;&#21644;&#24037;&#20855;&#35843;&#29992;&#33021;&#21147;&#30340;&#24378;&#22823;&#24320;&#28304;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;BIG-bench&#30340;&#22810;&#26679;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#26126;&#20854;CoS&#33021;&#21147;&#19982;ChatGPT&#30456;&#21305;&#37197;&#65292;&#32780;&#24615;&#33021;&#36229;&#36807;&#20102;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05155v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities. Evaluation of diverse tasks from BIG-bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;LLM&#26041;&#27861;&#23545;&#31264;&#23494;LLM&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21387;&#32553;LLM&#22522;&#20934;&#26469;&#37325;&#26032;&#23450;&#20041;&#35780;&#20272;&#21327;&#35758;&#12290;</title><link>https://arxiv.org/abs/2310.01382</link><description>&lt;p&gt;
&#21387;&#32553;LLM&#65306;&#30495;&#30456;&#24456;&#23569;&#32431;&#31929;&#65292;&#20063;&#32477;&#19981;&#31616;&#21333;
&lt;/p&gt;
&lt;p&gt;
Compressing LLMs: The Truth is Rarely Pure and Never Simple
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;LLM&#26041;&#27861;&#23545;&#31264;&#23494;LLM&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21387;&#32553;LLM&#22522;&#20934;&#26469;&#37325;&#26032;&#23450;&#20041;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#65292;&#20294;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290; &#26368;&#36817;&#65292;&#20960;&#39033;&#24037;&#20316;&#26174;&#31034;&#20986;&#22312;&#26080;&#38656;&#35757;&#32451;&#21644;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;LLMs&#36827;&#34892;&#21387;&#32553;&#65288;&#20462;&#21098;&#21644;&#37327;&#21270;&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#36798;&#21040;&#20102;50-60&#65285;&#30340;&#31232;&#30095;&#24230;&#65292;&#24182;&#23558;&#20301;&#23485;&#20943;&#23567;&#21040;&#27599;&#20010;&#26435;&#37325;3&#25110;4&#20301;&#65292;&#24182;&#19988;&#19982;&#26410;&#21387;&#32553;&#22522;&#32447;&#30456;&#27604;&#65292;&#22256;&#24785;&#24230;&#30340;&#38477;&#20302;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290; &#38543;&#30528;&#26368;&#36817;&#30740;&#31350;&#24037;&#20316;&#38598;&#20013;&#22312;&#24320;&#21457;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#21387;&#32553;&#26041;&#27861;&#19978;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#36864;&#19968;&#27493;&#37325;&#26032;&#35780;&#20272;&#20102;&#29616;&#26377;SoTA&#21387;&#32553;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#30456;&#24403;&#31616;&#21333;&#19988;&#24191;&#21463;&#36136;&#30097;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22256;&#24785;&#24230;&#65288;&#21363;&#20351;&#23545;&#20110;&#31264;&#23494;&#30340;LLMs&#65289;&#12290; &#25105;&#20204;&#24341;&#20837;&#20102;&#30693;&#35782;&#23494;&#38598;&#22411;&#21387;&#32553;&#30340;LLM&#22522;&#20934;&#65288;LLM-KICK&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#20219;&#21153;&#38598;&#21512;&#65292;&#29992;&#20110;&#37325;&#26032;&#23450;&#20041;&#23545;&#21387;&#32553;LLMs&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#36825;&#20123;LLMs&#19982;&#20854;&#31264;&#23494;&#23545;&#24212;&#29289;&#26377;&#26174;&#33879;&#30340;&#23545;&#40784;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01382v2 Announce Type: replace  Abstract: Despite their remarkable achievements, modern Large Language Models (LLMs) face exorbitant computational and memory footprints. Recently, several works have shown significant success in training-free and data-free compression (pruning and quantization) of LLMs that achieve 50 - 60% sparsity and reduce the bit width to 3 or 4 bits per weight, with negligible degradation of perplexity over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully curated tasks to redefine the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LVLM Hallucination Revisor&#65288;LURE&#65289;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#26500;&#24314;&#36739;&#23569;&#20855;&#26377;&#24187;&#35273;&#24615;&#30340;&#25551;&#36848;&#65292;&#26469;&#20107;&#21518;&#32416;&#27491;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.00754</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#20943;&#36731;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Analyzing and Mitigating Object Hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00754
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LVLM Hallucination Revisor&#65288;LURE&#65289;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#26500;&#24314;&#36739;&#23569;&#20855;&#26377;&#24187;&#35273;&#24615;&#30340;&#25551;&#36848;&#65292;&#26469;&#20107;&#21518;&#32416;&#27491;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#22312;&#29702;&#35299;&#22270;&#20687;&#20449;&#24687;&#21644;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LVLMs &#20173;&#28982;&#23384;&#22312;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#21253;&#21547;&#22270;&#20687;&#20013;&#23454;&#38469;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30340;&#25551;&#36848;&#12290;&#36825;&#21487;&#33021;&#23545;&#35768;&#22810;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#22914;&#35270;&#35273;&#24635;&#32467;&#21644;&#25512;&#29702;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#31639;&#27861;&#65292;LVLM &#24187;&#35273;&#20462;&#27491;&#22120;&#65288;LURE&#65289;&#65292;&#36890;&#36807;&#37325;&#26500;&#36739;&#23569;&#20855;&#26377;&#24187;&#35273;&#24615;&#30340;&#25551;&#36848;&#26469;&#20107;&#21518;&#32416;&#27491;LVLM &#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;LURE&#26681;&#25454;&#23545;&#23548;&#33268;&#29289;&#20307;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#30340;&#20005;&#26684;&#32479;&#35745;&#20998;&#26512;&#65292;&#21253;&#25324;&#20849;&#29616;&#65288;&#22270;&#20687;&#20013;&#26576;&#20123;&#23545;&#35937;&#32463;&#24120;&#19982;&#20854;&#20182;&#23545;&#35937;&#19968;&#36215;&#20986;&#29616;&#65289;&#12289;&#19981;&#30830;&#23450;&#24615;&#65288;&#22312;LVLM&#35299;&#30721;&#36807;&#31243;&#20013;&#19981;&#30830;&#23450;&#24615;&#36739;&#39640;&#30340;&#23545;&#35937;&#65289;&#21644;&#23545;&#35937;&#20301;&#32622;&#65288;&#24187;&#35273;&#36890;&#24120;&#20986;&#29616;&#22312;&#29983;&#25104;&#25551;&#36848;&#30340;&#21518;&#37096;&#20998;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00754v2 Announce Type: replace-cross  Abstract: Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the gen
&lt;/p&gt;</description></item><item><title>LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2309.13788</link><description>&lt;p&gt;
&#33021;&#22815;&#26816;&#27979;&#21040;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can LLM-Generated Misinformation Be Detected?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13788
&lt;/p&gt;
&lt;p&gt;
LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;LLMs&#65288;&#22914;ChatGPT&#65289;&#21487;&#33021;&#34987;&#21033;&#29992;&#26469;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#36825;&#32473;&#22312;&#32447;&#23433;&#20840;&#21644;&#20844;&#20247;&#20449;&#20219;&#24102;&#26469;&#20102;&#20005;&#37325;&#20851;&#20999;&#12290;&#19968;&#20010;&#22522;&#26412;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#26159;&#21542;&#20250;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#36896;&#25104;&#26356;&#22823;&#21361;&#23475;?&#25105;&#20204;&#25552;&#20986;&#20174;&#26816;&#27979;&#38590;&#24230;&#30340;&#35282;&#24230;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21033;&#29992;LLMs&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#28508;&#22312;&#30495;&#23454;&#19990;&#30028;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#30340;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#30456;&#27604;&#65292;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#23545;&#20154;&#31867;&#21644;&#26816;&#27979;&#22120;&#26469;&#35828;&#26356;&#38590;&#26816;&#27979;&#65292;&#36825;&#34920;&#26126;&#23427;&#21487;&#33021;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#28508;&#22312;&#22320;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13788v3 Announce Type: replace-cross  Abstract: The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;BAMBOO&#22522;&#20934;&#26469;&#20840;&#38754;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#38271;&#25991;&#26412;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#21253;&#21547;10&#20010;&#25968;&#25454;&#38598;&#20174;5&#20010;&#19981;&#21516;&#38271;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#20013;&#25552;&#21462;&#65292;&#28085;&#30422;&#20102;LLMs&#30340;&#26680;&#24515;&#33021;&#21147;&#21644;&#21508;&#20010;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2309.13345</link><description>&lt;p&gt;
BAMBOO&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13345
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;BAMBOO&#22522;&#20934;&#26469;&#20840;&#38754;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#38271;&#25991;&#26412;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#21253;&#21547;10&#20010;&#25968;&#25454;&#38598;&#20174;5&#20010;&#19981;&#21516;&#38271;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#20013;&#25552;&#21462;&#65292;&#28085;&#30422;&#20102;LLMs&#30340;&#26680;&#24515;&#33021;&#21147;&#21644;&#21508;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#22788;&#29702;&#26222;&#36890;&#38271;&#24230;&#30340;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#29087;&#32451;&#24230;&#12290;&#26368;&#36817;&#65292;&#22810;&#39033;&#30740;&#31350;&#33268;&#21147;&#20110;&#25193;&#23637;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#24182;&#22686;&#24378;LLMs&#30340;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BAMBOO&#65292;&#19968;&#20010;&#22810;&#20219;&#21153;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#12290;BAMBOO&#35774;&#35745;&#20043;&#21021;&#32771;&#34385;&#20102;&#22235;&#20010;&#21407;&#21017;&#65306;&#20840;&#38754;&#23481;&#37327;&#35780;&#20272;&#12289;&#36991;&#20813;&#25968;&#25454;&#27745;&#26579;&#12289;&#20934;&#30830;&#30340;&#33258;&#21160;&#35780;&#20272;&#20197;&#21450;&#19981;&#21516;&#38271;&#24230;&#32423;&#21035;&#12290;&#23427;&#30001;&#26469;&#33258;5&#20010;&#19981;&#21516;&#38271;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#30340;10&#20010;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#21363;&#38382;&#31572;&#12289;&#24187;&#35273;&#26816;&#27979;&#12289;&#25991;&#26412;&#25490;&#24207;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#20195;&#30721;&#34917;&#20840;&#65292;&#20197;&#28085;&#30422;LLMs&#30340;&#26680;&#24515;&#33021;&#21147;&#21644;&#21508;&#20010;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;BAMBOO&#19978;&#20351;&#29992;&#20116;&#20010;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#38271;&#25991;&#26412;&#30340;&#22235;&#20010;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;&#24403;&#21069;&#30340;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#36827;&#34892;&#20102;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13345v2 Announce Type: replace  Abstract: Large language models (LLMs) have achieved dramatic proficiency over NLP tasks with normal length. Recently, multiple studies have committed to extending the context length and enhancing the long text modeling capabilities of LLMs. To comprehensively evaluate the long context ability of LLMs, we propose BAMBOO, a multi-task long context benchmark. BAMBOO has been designed with four principles: comprehensive capacity evaluation, avoidance of data contamination, accurate automatic evaluation, and different length levels. It consists of 10 datasets from 5 different long text understanding tasks, i.e. question answering, hallucination detection, text sorting, language modeling, and code completion, to cover core capacities and various domains of LLMs. We conduct experiments with five long context models on BAMBOO and further discuss four key research questions of long text. We also qualitatively analyze current long context models and po
&lt;/p&gt;</description></item><item><title>LeBenchmark 2.0&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#26500;&#24314;&#27861;&#35821;&#35821;&#38899;&#25216;&#26415;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;SSL&#27169;&#22411;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2309.05472</link><description>&lt;p&gt;
LeBenchmark 2.0&#65306;&#29992;&#20110;&#33258;&#30417;&#30563;&#27861;&#34920;&#31034;&#27861;&#35821;&#35821;&#38899;&#30340;&#26631;&#20934;&#21270;&#12289;&#21487;&#22797;&#21046;&#21644;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for Self-supervised Representations of French Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05472
&lt;/p&gt;
&lt;p&gt;
LeBenchmark 2.0&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#26500;&#24314;&#27861;&#35821;&#35821;&#38899;&#25216;&#26415;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;SSL&#27169;&#22411;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#12290;&#35821;&#38899;&#22788;&#29702;&#26497;&#22823;&#21463;&#30410;&#20110;SSL&#65292;&#22240;&#20026;&#24403;&#21069;&#22823;&#37096;&#20998;&#39046;&#22495;&#30456;&#20851;&#20219;&#21153;&#29616;&#22312;&#37117;&#26159;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22788;&#29702;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LeBenchmark 2.0&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21644;&#26500;&#24314;&#37197;&#22791;SSL&#30340;&#27861;&#35821;&#35821;&#38899;&#25216;&#26415;&#30340;&#24320;&#28304;&#26694;&#26550;&#12290;&#23427;&#21253;&#25324;&#26377;&#25991;&#26723;&#35760;&#24405;&#12289;&#22823;&#35268;&#27169;&#21644;&#24322;&#26500;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#38271;&#36798;14,000&#23567;&#26102;&#30340;&#24322;&#26500;&#35821;&#38899;&#65292;&#21313;&#20010;&#39044;&#35757;&#32451;&#30340;SSL wav2vec 2.0 &#27169;&#22411;&#65292;&#21253;&#21547;&#20174;2600&#19975;&#21040;10&#20159;&#21487;&#23398;&#20064;&#21442;&#25968;&#19982;&#31038;&#21306;&#20849;&#20139;&#65292;&#24182;&#19988;&#21253;&#21547;&#30001;&#20845;&#20010;&#19979;&#28216;&#20219;&#21153;&#32452;&#25104;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#34917;&#20805;&#29616;&#26377;&#22522;&#20934;&#12290;LeBenchmark 2.0 &#36824;&#23545;&#20110;&#35821;&#38899;&#30340;&#39044;&#35757;&#32451;SSL&#27169;&#22411;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#20923;&#32467;&#19982;&#24494;&#35843;&#19979;&#28216;&#27169;&#22411;&#20197;&#21450;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05472v2 Announce Type: replace-cross  Abstract: Self-supervised learning (SSL) is at the origin of unprecedented improvements in many different domains including computer vision and natural language processing. Speech processing drastically benefitted from SSL as most of the current domain-related tasks are now being approached with pre-trained models. This work introduces LeBenchmark 2.0 an open-source framework for assessing and building SSL-equipped French speech technologies. It includes documented, large-scale and heterogeneous corpora with up to 14,000 hours of heterogeneous speech, ten pre-trained SSL wav2vec 2.0 models containing from 26 million to one billion learnable parameters shared with the community, and an evaluation protocol made of six downstream tasks to complement existing benchmarks. LeBenchmark 2.0 also presents unique perspectives on pre-trained SSL models for speech with the investigation of frozen versus fine-tuned downstream models, task-agnostic ve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#39057;&#29575;&#20449;&#24687;&#23398;&#20064;(FIL)&#33719;&#24471;&#19968;&#31181;&#39640;&#25928;&#12289;&#32771;&#34385;&#39057;&#29575;&#30340;&#24418;&#24335;&#21644;&#21547;&#20041;&#20043;&#38388;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#26356;&#20415;&#23452;&#65292;&#21516;&#26102;&#33021;&#26377;&#25928;&#22320;&#36817;&#20284;&#36880;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2306.11044</link><description>&lt;p&gt;
&#39057;&#29575;&#25928;&#24212;&#22312;&#32447;&#24615;&#21028;&#21035;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Frequency effects in Linear Discriminative Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.11044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#39057;&#29575;&#20449;&#24687;&#23398;&#20064;(FIL)&#33719;&#24471;&#19968;&#31181;&#39640;&#25928;&#12289;&#32771;&#34385;&#39057;&#29575;&#30340;&#24418;&#24335;&#21644;&#21547;&#20041;&#20043;&#38388;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#26356;&#20415;&#23452;&#65292;&#21516;&#26102;&#33021;&#26377;&#25928;&#22320;&#36817;&#20284;&#36880;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23383;&#39057;&#22312;&#22823;&#22810;&#25968;&#35789;&#27719;&#22788;&#29702;&#20219;&#21153;&#20013;&#37117;&#26159;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#39044;&#27979;&#22240;&#23376;&#12290;&#22240;&#27492;&#65292;&#20219;&#20309;&#19968;&#20010;&#35789;&#27719;&#35782;&#21035;&#27169;&#22411;&#37117;&#38656;&#35201;&#35299;&#37322;&#35789;&#39057;&#25928;&#24212;&#26159;&#22914;&#20309;&#20135;&#29983;&#30340;&#12290;&#21028;&#21035;&#24615;&#35789;&#20856;&#27169;&#22411;(DLM;Baayen&#31561;&#65292;2018a&#65292;2019)&#36890;&#36807;&#21333;&#35789;&#24418;&#24335;&#21644;&#21547;&#20041;&#20043;&#38388;&#30340;&#32447;&#24615;&#26144;&#23556;&#23545;&#35789;&#27719;&#22788;&#29702;&#36827;&#34892;&#24314;&#27169;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20123;&#26144;&#23556;&#21487;&#20197;&#36890;&#36807;&#36880;&#27493;&#36890;&#36807;&#35823;&#24046;&#39537;&#21160;&#23398;&#20064;&#33719;&#24471;&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#39057;&#29575;&#25928;&#24212;&#30340;&#35745;&#31639;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#25110;&#32773;&#26159;&#36890;&#36807;&#39640;&#25928;&#20294;&#19982;&#39057;&#29575;&#26080;&#20851;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#24314;&#27169;&#23398;&#20064;&#30340;&#29702;&#35770;&#26368;&#32456;&#29366;&#24577;(EL) &#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#33719;&#24471;&#19968;&#31181;&#39640;&#25928;&#20294;&#21448;&#32771;&#34385;&#39057;&#29575;&#30340;&#24418;&#24335;&#21644;&#21547;&#20041;&#20043;&#38388;&#26144;&#23556;&#30340;&#26041;&#27861;(&#39057;&#29575;&#20449;&#24687;&#23398;&#20064;;FIL)&#12290;&#25105;&#20204;&#21457;&#29616;FIL&#24456;&#22909;&#22320;&#36817;&#20284;&#20102;&#36880;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#35745;&#31639;&#25104;&#26412;&#35201;&#20302;&#24471;&#22810;&#12290;FIL&#34920;&#29616;&#20986;&#30456;&#23545;&#36739;&#20302;&#30340;&#31867;&#22411;&#31934;&#24230;&#21644;&#39640;&#30340;&#26631;&#35760;&#31934;&#24230;&#65292;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.11044v2 Announce Type: replace  Abstract: Word frequency is a strong predictor in most lexical processing tasks. Thus, any model of word recognition needs to account for how word frequency effects arise. The Discriminative Lexicon Model (DLM; Baayen et al., 2018a, 2019) models lexical processing with linear mappings between words' forms and their meanings. So far, the mappings can either be obtained incrementally via error-driven learning, a computationally expensive process able to capture frequency effects, or in an efficient, but frequency-agnostic solution modelling the theoretical endstate of learning (EL) where all words are learned optimally. In this study we show how an efficient, yet frequency-informed mapping between form and meaning can be obtained (Frequency-informed learning; FIL). We find that FIL well approximates an incremental solution while being computationally much cheaper. FIL shows a relatively low type- and high token-accuracy, demonstrating that the m
&lt;/p&gt;</description></item><item><title>Open Brain AI&#26159;&#19968;&#20010;&#21033;&#29992;&#21019;&#26032;AI&#25216;&#26415;&#33258;&#21160;&#20998;&#26512;&#22810;&#35821;&#31181;&#21475;&#22836;&#21644;&#20070;&#38754;&#35328;&#35821;&#30340;&#35745;&#31639;&#24179;&#21488;&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#33258;&#21160;&#21270;&#22320;&#20998;&#26512;&#35821;&#35328;&#65292;&#20943;&#36731;&#20020;&#24202;&#21307;&#29983;&#30340;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2306.06693</link><description>&lt;p&gt;
&#24320;&#25918;&#24335;&#33041;AI&#65306;&#33258;&#21160;&#35821;&#35328;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Open Brain AI. Automatic Language Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.06693
&lt;/p&gt;
&lt;p&gt;
Open Brain AI&#26159;&#19968;&#20010;&#21033;&#29992;&#21019;&#26032;AI&#25216;&#26415;&#33258;&#21160;&#20998;&#26512;&#22810;&#35821;&#31181;&#21475;&#22836;&#21644;&#20070;&#38754;&#35328;&#35821;&#30340;&#35745;&#31639;&#24179;&#21488;&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#33258;&#21160;&#21270;&#22320;&#20998;&#26512;&#35821;&#35328;&#65292;&#20943;&#36731;&#20020;&#24202;&#21307;&#29983;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35780;&#20272;&#22312;&#35786;&#26029;&#21644;&#27835;&#30103;&#30001;&#31070;&#32463;&#21407;&#22240;&#24341;&#36215;&#30340;&#35328;&#35821;&#12289;&#35821;&#35328;&#21644;&#27807;&#36890;&#38556;&#30861;&#30340;&#20010;&#20307;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26080;&#35770;&#26159;&#21457;&#32946;&#24615;&#36824;&#26159;&#21518;&#22825;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#26159;&#25163;&#21160;&#30340;&#12289;&#36153;&#21147;&#30340;&#65292;&#24182;&#19988;&#32791;&#26102;&#65292;&#32473;&#24739;&#32773;&#24102;&#26469;&#39069;&#22806;&#30340;&#21387;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Open Brain AI&#65288;https://openbrainai.com&#65289;&#12290;&#36825;&#19968;&#35745;&#31639;&#24179;&#21488;&#21033;&#29992;&#21019;&#26032;&#30340;AI&#25216;&#26415;&#65292;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#21160;&#35821;&#38899;&#36716;&#25991;&#26412;&#65292;&#26469;&#33258;&#21160;&#20998;&#26512;&#22810;&#35821;&#31181;&#30340;&#21475;&#22836;&#21644;&#20070;&#38754;&#35328;&#35821;&#20135;&#29289;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;Open Brain AI&#30340;&#24320;&#21457;&#12289;AI&#35821;&#35328;&#22788;&#29702;&#27169;&#22359;&#20197;&#21450;&#35805;&#35821;&#23439;&#35266;&#32467;&#26500;&#21644;&#24494;&#35266;&#32467;&#26500;&#30340;&#35821;&#35328;&#27979;&#37327;&#12290;&#35821;&#35328;&#30340;&#24555;&#36895;&#33258;&#21160;&#21270;&#20998;&#26512;&#20943;&#36731;&#20102;&#20020;&#24202;&#21307;&#29983;&#30340;&#36127;&#25285;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.06693v2 Announce Type: replace  Abstract: Language assessment plays a crucial role in diagnosing and treating individuals with speech, language, and communication disorders caused by neurogenic conditions, whether developmental or acquired. However, current assessment methods are manual, laborious, and time-consuming to administer and score, causing additional patient stress. To address these challenges, we developed Open Brain AI (https://openbrainai.com). This computational platform harnesses innovative AI techniques, namely machine learning, natural language processing, large language models, and automatic speech-to-text transcription, to automatically analyze multilingual spoken and written speech productions. This paper discusses the development of Open Brain AI, the AI language processing modules, and the linguistic measurements of discourse macro-structure and micro-structure. The fast and automatic analysis of language alleviates the burden on clinicians, enabling th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;14&#20010;&#19981;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#35299;&#20915;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#21547;45&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20026;NLP&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2305.12544</link><description>&lt;p&gt;
&#19968;&#20999;&#37117;&#24050;&#35299;&#20915;&#20102;&#21527;&#65311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23578;&#26410;&#35299;&#20915;&#30340;&#24320;&#25918;&#24615;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;14&#20010;&#19981;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#35299;&#20915;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#21547;45&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20026;NLP&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#23637;&#20351;&#24471;&#35768;&#22810;&#29983;&#25104;&#24335;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#24471;&#20197;&#37096;&#32626;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20063;&#23548;&#33268;&#20102;&#19968;&#20010;&#35823;&#23548;&#24615;&#30340;&#20844;&#20247;&#35805;&#35821;&#65292;&#8220;&#19968;&#20999;&#37117;&#24050;&#35299;&#20915;&#8221;&#12290;&#19981;&#36275;&#20026;&#22855;&#30340;&#26159;&#65292;&#36825;&#21453;&#36807;&#26469;&#20351;&#24471;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#21018;&#24320;&#22987;&#32844;&#19994;&#29983;&#28079;&#30340;&#20154;&#65292;&#25285;&#24515;&#20182;&#20204;&#24212;&#35813;&#19987;&#27880;&#20110;&#21738;&#20123;&#30740;&#31350;&#39046;&#22495;&#12290;&#19968;&#20999;&#37117;&#24050;&#35299;&#20915;&#20102;&#21527;&#65292;&#25110;&#32773;&#19981;&#35770;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;&#20309;&#65292;&#25105;&#20204;&#21487;&#20197;&#32487;&#32493;&#30740;&#31350;&#21738;&#20123;&#38382;&#39064;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25972;&#29702;&#20102;&#36866;&#21512;&#28145;&#20837;&#25506;&#31350;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21253;&#21547;45&#20010;&#30740;&#31350;&#26041;&#21521;&#30340;14&#20010;&#19981;&#38656;&#35201;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#35299;&#20915;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#34429;&#28982;&#25105;&#20204;&#30830;&#23450;&#20102;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#36824;&#26377;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#23384;&#22312;&#65307;&#25105;&#20204;&#26410;&#28085;&#30422;&#30446;&#21069;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22788;&#29702;&#30340;&#39046;&#22495;&#65292;&#20294;&#22312;&#24615;&#33021;&#19978;&#33853;&#21518;&#25110;&#32773;&#19987;&#27880;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#27426;&#36814;&#23545;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12544v2 Announce Type: replace-cross  Abstract: Recent progress in large language models (LLMs) has enabled the deployment of many generative NLP applications. At the same time, it has also led to a misleading public discourse that ``it's all been solved.'' Not surprisingly, this has, in turn, made many NLP researchers -- especially those at the beginning of their careers -- worry about what NLP research area they should focus on. Has it all been solved, or what remaining questions can we work on regardless of LLMs? To address this question, this paper compiles NLP research directions rich for exploration. We identify fourteen different research areas encompassing 45 research directions that require new research and are not directly solvable by LLMs. While we identify many research areas, many others exist; we do not cover areas currently addressed by LLMs, but where LLMs lag behind in performance or those focused on LLM development. We welcome suggestions for other research
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#27969;&#24335;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#30340;&#20302;&#24310;&#36831;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2302.13451</link><description>&lt;p&gt;
&#29992;&#20110;&#27969;&#24335;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#30340;&#20302;&#24310;&#36831;&#27880;&#24847;&#21147;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
A low latency attention module for streaming self-supervised speech representation learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.13451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#27969;&#24335;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#30340;&#20302;&#24310;&#36831;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
transformer&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;transformer&#30340;&#26680;&#24515;&#32452;&#20214;&#12290;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65288;SSRL&#65289;&#26159;transformer&#26550;&#26500;&#30340;&#19968;&#20010;&#27969;&#34892;&#29992;&#20363;&#12290;&#30001;&#20110;transformer&#30340;&#38750;&#22240;&#26524;&#34892;&#20026;&#65292;&#23545;&#20110;SSRL&#30340;transformer&#30340;&#20351;&#29992;&#20027;&#35201;&#38598;&#20013;&#22312;&#38750;&#22240;&#26524;&#24212;&#29992;&#19978;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#23186;&#20307;&#22788;&#29702;&#38382;&#39064;&#65292;&#22914;&#35821;&#38899;&#22788;&#29702;&#65292;&#38656;&#35201;&#23454;&#26102;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#23454;&#29616;&#65292;&#35813;&#27169;&#22359;&#21487;&#20197;&#36890;&#36807;&#36739;&#20302;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#35757;&#32451;SSRL&#26550;&#26500;&#65292;&#24182;&#20801;&#35768;&#22312;&#20302;&#22266;&#23450;&#24310;&#36831;&#19979;&#36827;&#34892;&#23454;&#26102;&#25512;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65292;&#20998;&#21035;&#26159;&#27969;&#24335;&#27880;&#24847;&#21147;&#65288;SA&#65289;&#21644;&#20302;&#24310;&#36831;&#27969;&#24335;&#27880;&#24847;&#21147;&#65288;LLSA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.13451v2 Announce Type: replace-cross  Abstract: The transformer is a fundamental building block in deep learning, and the attention mechanism is the transformer's core component. Self-supervised speech representation learning (SSRL) represents a popular use-case for the transformer architecture. Due to transformers' acausal behavior, the use of transformers for SSRL has been predominantly focused on acausal applications. However, several media processing problems, such as speech processing, require real-time solutions. In this paper, we present an implementation of the attention module that enables training of SSRL architectures with low compute and memory requirements, while allowing real-time inference with low and fixed latency. The attention module proposed in this paper includes two components, streaming attention (SA) and low-latency streaming attention (LLSA). The SA represents our proposal for an efficient streaming SSRL implementation, while the LLSA solves the late
&lt;/p&gt;</description></item><item><title>&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340; $\texttt{[MASK]} $&#31526;&#21495;&#20250;&#23548;&#33268;&#27169;&#22411;&#32500;&#24230;&#36807;&#24230;&#20998;&#37197;&#65292;&#36896;&#25104;&#30495;&#23454;&#26631;&#35760;&#30340;&#34920;&#31034;&#19981;&#36275;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MAE-LM&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;</title><link>https://arxiv.org/abs/2302.02060</link><description>&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#34920;&#31034;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;
Representation Deficiency in Masked Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02060
&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340; $\texttt{[MASK]} $&#31526;&#21495;&#20250;&#23548;&#33268;&#27169;&#22411;&#32500;&#24230;&#36807;&#24230;&#20998;&#37197;&#65292;&#36896;&#25104;&#30495;&#23454;&#26631;&#35760;&#30340;&#34920;&#31034;&#19981;&#36275;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MAE-LM&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#24050;&#32463;&#25104;&#20026;&#21452;&#21521;&#25991;&#26412;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#26368;&#31361;&#20986;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;&#20851;&#20110;MLM&#30340;&#19968;&#20010;&#26174;&#33879;&#38382;&#39064;&#26159;&#29305;&#27530;&#30340; $\texttt{[MASK]}$ &#31526;&#21495;&#20250;&#23548;&#33268;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#19979;&#28216;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#22240;&#20026;&#23427;&#21482;&#20986;&#29616;&#22312;&#39044;&#35757;&#32451;&#20013;&#32780;&#19981;&#20986;&#29616;&#22312;&#24494;&#35843;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#24046;&#24322;&#30340;&#21518;&#26524;&#65306;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#35777;&#26126;&#20102;MLM&#39044;&#35757;&#32451;&#19987;&#38376;&#20998;&#37197;&#20102;&#19968;&#20123;&#27169;&#22411;&#32500;&#24230;&#26469;&#34920;&#31034; $\texttt{[MASK]}$ &#26631;&#35760;&#65292;&#23548;&#33268;&#30495;&#23454;&#26631;&#35760;&#30340;&#34920;&#31034;&#19981;&#36275;&#65292;&#24182;&#22312;&#27809;&#26377; $\texttt{[MASK]}$ &#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#65292;&#38480;&#21046;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36866;&#24212;&#19979;&#28216;&#25968;&#25454;&#26102;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#21463;&#21040;&#35782;&#21035;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MAE-LM&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;MLM&#23545;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#25490;&#38500;&#20102; $\texttt{[MASK]}$ &#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.02060v2 Announce Type: replace  Abstract: Masked Language Modeling (MLM) has been one of the most prominent approaches for pretraining bidirectional text encoders due to its simplicity and effectiveness. One notable concern about MLM is that the special $\texttt{[MASK]}$ symbol causes a discrepancy between pretraining data and downstream data as it is present only in pretraining but not in fine-tuning. In this work, we offer a new perspective on the consequence of such a discrepancy: We demonstrate empirically and theoretically that MLM pretraining allocates some model dimensions exclusively for representing $\texttt{[MASK]}$ tokens, resulting in a representation deficiency for real tokens and limiting the pretrained model's expressiveness when it is adapted to downstream data without $\texttt{[MASK]}$ tokens. Motivated by the identified issue, we propose MAE-LM, which pretrains the Masked Autoencoder architecture with MLM where $\texttt{[MASK]}$ tokens are excluded from the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38382;&#39064;&#22238;&#31572;&#30340;&#26080;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#33945;&#29256;&#27169;&#22359;&#23454;&#29616;&#33258;&#35757;&#32451;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#26080;&#27861;&#35775;&#38382;&#28304;&#22495;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2212.09563</link><description>&lt;p&gt;
&#38754;&#21521;&#38382;&#39064;&#22238;&#31572;&#30340;&#26080;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#21450;&#20854;&#33945;&#29256;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Source-Free Domain Adaptation for Question Answering with Masked Self-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.09563
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38382;&#39064;&#22238;&#31572;&#30340;&#26080;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#33945;&#29256;&#27169;&#22359;&#23454;&#29616;&#33258;&#35757;&#32451;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#26080;&#27861;&#35775;&#38382;&#28304;&#22495;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20808;&#21069;&#38024;&#23545;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26041;&#27861;&#22312;Fine-tuning&#30446;&#26631;&#22495;&#27169;&#22411;&#26102;&#38656;&#35201;&#35775;&#38382;&#28304;&#22495;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#28304;&#22495;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#24182;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#21363;&#26080;&#28304;UDA&#65292;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21482;&#26377;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#21644;&#30446;&#26631;&#22495;&#25968;&#25454;&#65292;&#26080;&#27861;&#35775;&#38382;&#28304;&#22495;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#26469;&#25552;&#21319;QA&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#19968;&#20010;&#29992;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#29420;&#29305;&#33945;&#29256;&#27169;&#22359;&#12290;&#35813;&#33945;&#29256;&#22312;&#35757;&#32451;&#28304;&#22495;&#26102;&#36827;&#34892;&#33258;&#21160;&#35843;&#25972;&#65292;&#20197;&#25552;&#21462;&#20851;&#38190;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#20026;&#20102;&#20445;&#25345;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#36866;&#24212;&#36807;&#31243;&#20013;&#26576;&#20123;&#33945;&#29256;&#26435;&#37325;&#34987;&#20923;&#32467;&#65292;&#32780;&#20854;&#20182;&#26435;&#37325;&#21017;&#26681;&#25454;&#30446;&#26631;&#22495;&#20013;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#26679;&#26412;&#26469;&#35843;&#25972;&#65292;&#20197;&#20943;&#36731;&#39046;&#22495;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.09563v2 Announce Type: replace  Abstract: Most previous unsupervised domain adaptation (UDA) methods for question answering(QA) require access to source domain data while fine-tuning the model for the target domain. Source domain data may, however, contain sensitive information and may be restricted. In this study, we investigate a more challenging setting, source-free UDA, in which we have only the pretrained source model and target domain data, without access to source domain data. We propose a novel self-training approach to QA models that integrates a unique mask module for domain adaptation. The mask is auto-adjusted to extract key domain knowledge while trained on the source domain. To maintain previously learned domain knowledge, certain mask weights are frozen during adaptation, while other weights are adjusted to mitigate domain shifts with pseudo-labeled samples generated in the target domain. %As part of the self-training process, we generate pseudo-labeled sample
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#23616;&#37096;&#35299;&#37322;&#65292;&#21253;&#25324;&#19982;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#30456;&#20851;&#30340;&#39044;&#27979;&#35299;&#37322;&#12289;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#25506;&#27979;&#27169;&#22411;&#38544;&#34255;&#29366;&#24577;&#21644;&#35789;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2103.11072</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26412;&#22320;&#35299;&#37322;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Local Interpretations for Explainable Natural Language Processing: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2103.11072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#23616;&#37096;&#35299;&#37322;&#65292;&#21253;&#25324;&#19982;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#30456;&#20851;&#30340;&#39044;&#27979;&#35299;&#37322;&#12289;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#25506;&#27979;&#27169;&#22411;&#38544;&#34255;&#29366;&#24577;&#21644;&#35789;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36807;&#21435;&#21313;&#24180;&#38388;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#19981;&#26029;&#22686;&#38271;&#65292;&#20851;&#20110;&#40657;&#30418;&#27169;&#22411;&#19981;&#36879;&#26126;&#24615;&#30340;&#25265;&#24616;&#20063;&#22312;&#22686;&#21152;&#65292;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36879;&#26126;&#24230;&#21463;&#21040;&#26356;&#22810;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#30340;&#24320;&#22987;&#38454;&#27573;&#23545;&#21487;&#35299;&#37322;&#24615;&#19968;&#35789;&#21450;&#20854;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#35752;&#35770;&#12290;&#26412;&#27425;&#35843;&#26597;&#20013;&#25910;&#38598;&#21644;&#24635;&#32467;&#30340;&#26041;&#27861;&#20165;&#28041;&#21450;&#23616;&#37096;&#35299;&#37322;&#65292;&#24182;&#20855;&#20307;&#20998;&#20026;&#19977;&#31867;&#65306;1&#65289;&#36890;&#36807;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#35299;&#37322;&#27169;&#22411;&#30340;&#39044;&#27979;&#65307;2&#65289;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36827;&#34892;&#35299;&#37322;&#65307;3&#65289;&#25506;&#27979;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#21644;&#21333;&#35789;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2103.11072v3 Announce Type: replace-cross  Abstract: As the use of deep learning techniques has grown across various fields over the past decade, complaints about the opaqueness of the black-box models have increased, resulting in an increased focus on transparency in deep learning models. This work investigates various methods to improve the interpretability of deep neural networks for Natural Language Processing (NLP) tasks, including machine translation and sentiment analysis. We provide a comprehensive discussion on the definition of the term interpretability and its various aspects at the beginning of this work. The methods collected and summarised in this survey are only associated with local interpretation and are specifically divided into three categories: 1) interpreting the model's predictions through related input features; 2) interpreting through natural language explanation; 3) probing the hidden states of models and word representations.
&lt;/p&gt;</description></item><item><title>SpeechDPR&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#36890;&#36807;&#34701;&#21512;&#26080;&#30417;&#30563;ASR&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#30693;&#35782;&#65292;SpeechDPR&#33021;&#22815;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#34920;&#29616;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2401.13463</link><description>&lt;p&gt;
SpeechDPR: &#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering. (arXiv:2401.13463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13463
&lt;/p&gt;
&lt;p&gt;
SpeechDPR&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#36890;&#36807;&#34701;&#21512;&#26080;&#30417;&#30563;ASR&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#30693;&#35782;&#65292;SpeechDPR&#33021;&#22815;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#34920;&#29616;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#38382;&#31572;(SQA)&#26159;&#26426;&#22120;&#36890;&#36807;&#22312;&#32473;&#23450;&#21475;&#35821;&#27573;&#33853;&#20013;&#25214;&#21040;&#31572;&#26696;&#33539;&#22260;&#26469;&#22238;&#31572;&#29992;&#25143;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;&#36807;&#21435;&#30340;SQA&#26041;&#27861;&#27809;&#26377;&#20351;&#29992;ASR&#65292;&#20197;&#36991;&#20813;&#35782;&#21035;&#38169;&#35823;&#21644;&#35789;&#27719;&#22806;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#24320;&#25918;&#39046;&#22495;SQA(openSQA)&#38382;&#39064;&#20013;&#65292;&#26426;&#22120;&#38656;&#35201;&#39318;&#20808;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24050;&#30693;&#30340;&#29992;&#20110;openSQA&#38382;&#39064;&#26816;&#32034;&#32452;&#20214;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;SpeechDPR&#12290;SpeechDPR&#36890;&#36807;&#20174;&#26080;&#30417;&#30563;ASR(UASR)&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;(TDR)&#30340;&#32423;&#32852;&#27169;&#22411;&#20013;&#25552;&#28860;&#30693;&#35782;&#65292;&#23398;&#20064;&#21477;&#23376;&#32423;&#35821;&#20041;&#34920;&#31034;&#12290;&#19981;&#38656;&#35201;&#25163;&#21160;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#32423;&#32852;&#30340;UASR&#21644;TDR&#27169;&#22411;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#26174;&#33879;&#25552;&#39640;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage. SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems. However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive in addition, was never considered. This paper proposes the first known end-to-end framework, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem. SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR). No manually transcribed speech data is needed. Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying this approach is more robus
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#20197;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36136;&#37327;&#20272;&#35745;&#30340;&#21453;&#39304;&#35757;&#32451;&#23384;&#22312;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#37319;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#24182;&#23545;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#24809;&#32602;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12873</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#30340;&#21453;&#39304;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;: &#23558;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model. (arXiv:2401.12873v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#20197;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36136;&#37327;&#20272;&#35745;&#30340;&#21453;&#39304;&#35757;&#32451;&#23384;&#22312;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#37319;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#24182;&#23545;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#24809;&#32602;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#20805;&#20998;&#24314;&#27169;&#20154;&#31867;&#20559;&#22909;&#23548;&#33268;&#22870;&#21169;&#27169;&#22411;&#22312;&#21033;&#29992;&#20154;&#30340;&#21453;&#39304;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#25104;&#20026;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36136;&#37327;&#20272;&#35745;(QE)&#22312;&#36807;&#21435;&#20004;&#24180;&#20013;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#23601;&#33021;&#20934;&#30830;&#39044;&#27979;&#32473;&#23450;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;QE&#27169;&#22411;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;(&#22522;&#20110;QE&#30340;&#22870;&#21169;&#27169;&#22411;)&#26469;&#39044;&#27979;&#20154;&#30340;&#20559;&#22909;&#20197;&#36827;&#34892;&#21453;&#39304;&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#20102;&#22312;&#22522;&#20110;QE&#30340;&#21453;&#39304;&#35757;&#32451;&#20013;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#34920;&#29616;&#20026;&#22870;&#21169;&#30340;&#22686;&#21152;&#32780;&#32763;&#35793;&#36136;&#37327;&#19979;&#38477;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35748;&#20026;QE&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#32763;&#35793;&#30340;&#39640;&#22870;&#21169;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#24230;&#20248;&#21270;&#21644;&#38169;&#35823;&#20256;&#25773;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#65292;&#24182;&#20026;QE&#27169;&#22411;&#28155;&#21152;&#20102;&#19968;&#20010;&#24809;&#32602;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality. Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years. In this work, we investigate the potential of employing the QE model as the reward model (the QE-based reward model) to predict human preferences for feedback training. We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines. We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation. To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the Q
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#20013;&#32034;&#36180;&#26816;&#27979;&#30340;&#29616;&#26377;&#24037;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#22810;&#35821;&#35328;&#25968;&#25454;&#21644;&#26041;&#27861;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#23500;&#26377;&#25104;&#26524;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#38656;&#35201;&#26356;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23545;&#25239;&#36328;&#22810;&#35821;&#35328;&#21644;&#27169;&#24577;&#30340;&#19981;&#23454;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.11969</link><description>&lt;p&gt;
&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#32034;&#36180;&#26816;&#27979;&#65306;&#20851;&#20110;&#21333;&#35821;&#12289;&#22810;&#35821;&#21644;&#36328;&#35821;&#35328;&#30740;&#31350;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Claim Detection for Automated Fact-checking: A Survey on Monolingual, Multilingual and Cross-Lingual Research. (arXiv:2401.11969v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#20013;&#32034;&#36180;&#26816;&#27979;&#30340;&#29616;&#26377;&#24037;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#22810;&#35821;&#35328;&#25968;&#25454;&#21644;&#26041;&#27861;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#23500;&#26377;&#25104;&#26524;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#38656;&#35201;&#26356;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23545;&#25239;&#36328;&#22810;&#35821;&#35328;&#21644;&#27169;&#24577;&#30340;&#19981;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#21407;&#22240;&#26159;&#32593;&#32476;&#24179;&#21488;&#19978;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#22686;&#21152;&#20102;&#12290;&#36825;&#36890;&#24120;&#26159;&#20316;&#20026;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#24207;&#21015;&#26469;&#23436;&#25104;&#30340;&#65292;&#21253;&#25324;&#65288;i&#65289;&#26816;&#27979;&#22312;&#32593;&#19978;&#27969;&#20256;&#30340;&#21477;&#23376;&#65292;&#36825;&#20123;&#21477;&#23376;&#26500;&#25104;&#38656;&#35201;&#39564;&#35777;&#30340;&#32034;&#36180;&#65292;&#28982;&#21518;&#26159;&#65288;ii&#65289;&#23545;&#36825;&#20123;&#32034;&#36180;&#36827;&#34892;&#39564;&#35777;&#30340;&#36807;&#31243;&#12290;&#26412;&#32508;&#36848;&#37325;&#28857;&#35752;&#35770;&#21069;&#32773;&#65292;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#21162;&#21147;&#65292;&#26088;&#22312;&#26816;&#27979;&#38656;&#35201;&#20107;&#23454;&#26680;&#26597;&#30340;&#32034;&#36180;&#65292;&#29305;&#21035;&#20851;&#27880;&#22810;&#35821;&#35328;&#25968;&#25454;&#21644;&#26041;&#27861;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#23500;&#26377;&#25104;&#26524;&#30340;&#26041;&#21521;&#65292;&#30001;&#20110;&#38382;&#39064;&#30340;&#26497;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22312;&#20154;&#31867;&#34920;&#29616;&#26041;&#38754;&#65292;&#29616;&#26377;&#26041;&#27861;&#31163;&#21305;&#37197;&#20154;&#31867;&#34920;&#29616;&#36824;&#26377;&#24456;&#38271;&#30340;&#36335;&#35201;&#36208;&#12290;&#29305;&#21035;&#26159;&#65292;&#36328;&#22810;&#20010;&#31038;&#20132;&#24179;&#21488;&#30340;&#20449;&#24687;&#20256;&#25773;&#20197;&#22810;&#31181;&#35821;&#35328;&#21644;&#27169;&#24577;&#34920;&#36798;&#65292;&#38656;&#35201;&#26356;&#21152;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23545;&#25239;&#19981;&#23454;&#20449;&#24687;&#12290;&#25105;&#20204;&#38024;&#23545;&#22810;&#35821;&#35328;&#19981;&#23454;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated fact-checking has drawn considerable attention over the past few decades due to the increase in the diffusion of misinformation on online platforms. This is often carried out as a sequence of tasks comprising (i) the detection of sentences circulating in online platforms which constitute claims needing verification, followed by (ii) the verification process of those claims. This survey focuses on the former, by discussing existing efforts towards detecting claims needing fact-checking, with a particular focus on multilingual data and methods. This is a challenging and fertile direction where existing methods are yet far from matching human performance due to the profoundly challenging nature of the issue. Especially, the dissemination of information across multiple social platforms, articulated in multiple languages and modalities demands more generalized solutions for combating misinformation. Focusing on multilingual misinformation, we present a comprehensive survey of exis
&lt;/p&gt;</description></item><item><title>TrustLLM&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20449;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#21487;&#20449;&#24615;&#21407;&#21017;&#30340;&#25552;&#20986;&#12289;&#24314;&#31435;&#22522;&#20934;&#30340;&#26041;&#27861;&#12289;&#35780;&#20272;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24615;&#65292;&#20197;&#21450;&#23545;&#26410;&#26469;&#25361;&#25112;&#30340;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.05561</link><description>&lt;p&gt;
TrustLLM: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21487;&#20449;&#24615;
&lt;/p&gt;
&lt;p&gt;
TrustLLM: Trustworthiness in Large Language Models. (arXiv:2401.05561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05561
&lt;/p&gt;
&lt;p&gt;
TrustLLM&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20449;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#21487;&#20449;&#24615;&#21407;&#21017;&#30340;&#25552;&#20986;&#12289;&#24314;&#31435;&#22522;&#20934;&#30340;&#26041;&#27861;&#12289;&#35780;&#20272;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24615;&#65292;&#20197;&#21450;&#23545;&#26410;&#26469;&#25361;&#25112;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#22240;&#20854;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#22312;&#21487;&#20449;&#24615;&#26041;&#38754;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;LLMs&#30340;&#21487;&#20449;&#24615;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#35805;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TrustLLM&#65292;&#23427;&#26159;&#23545;LLMs&#20013;&#21487;&#20449;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#19981;&#21516;&#32500;&#24230;&#30340;&#21487;&#20449;&#24615;&#21407;&#21017;&#12289;&#24314;&#31435;&#22522;&#20934;&#12289;&#35780;&#20272;&#21644;&#20998;&#26512;&#20027;&#27969;LLMs&#30340;&#21487;&#20449;&#24615;&#65292;&#20197;&#21450;&#23545;&#24320;&#25918;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#28085;&#30422;&#20843;&#20010;&#19981;&#21516;&#32500;&#24230;&#30340;&#21487;&#20449;LLMs&#21407;&#21017;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#20102;&#19968;&#20010;&#36328;&#20845;&#20010;&#32500;&#24230;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#24615;&#21644;&#26426;&#22120;&#20262;&#29702;&#23398;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;TrustLLM&#20013;&#23637;&#31034;&#20102;&#19968;&#20010;&#35780;&#20272;16&#20010;&#20027;&#27969;LLMs&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;30&#22810;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our find
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20849;&#21516;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LM&#30340;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04536</link><description>&lt;p&gt;
&#36890;&#36807;&#35848;&#21028;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Language Model Agency through Negotiations. (arXiv:2401.04536v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20849;&#21516;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LM&#30340;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#21496;&#12289;&#32452;&#32455;&#21644;&#25919;&#24220;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#23637;&#31034;&#31867;&#20284;&#20195;&#29702;&#34892;&#20026;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#38543;&#30528;LM&#34987;&#37319;&#29992;&#26469;&#25191;&#34892;&#36234;&#26469;&#36234;&#20855;&#26377;&#33258;&#20027;&#24615;&#30340;&#20219;&#21153;&#65292;&#36843;&#20999;&#38656;&#35201;&#21487;&#38752;&#19988;&#21487;&#25193;&#23637;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#24403;&#21069;&#20027;&#35201;&#26159;&#38745;&#24577;&#30340;LM&#22522;&#20934;&#26080;&#27861;&#24456;&#22909;&#22320;&#35780;&#20272;&#27492;&#31867;&#21160;&#24577;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#26469;&#20849;&#21516;&#35780;&#20272;LM&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#20849;&#21516;&#20219;&#21153;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;LM&#20915;&#31574;&#36807;&#31243;&#30340;&#35265;&#35299;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#35848;&#21028;&#28216;&#25103;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#35843;&#25972;&#22797;&#26434;&#24615;&#65292;&#24182;&#36991;&#20813;&#35780;&#20272;&#20013;&#30340;&#24847;&#22806;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#26469;&#33258;&#20960;&#20010;&#20027;&#35201;&#20379;&#24212;&#21830;&#30340;&#20845;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;LM&#22312;&#21508;&#31181;&#35848;&#21028;&#28216;&#25103;&#19978;&#30340;&#32467;&#26524;&#65292;&#35780;&#20272;&#20102;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;i&#65289;&#24320;&#28304;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;&#65288;LLaVO&#65289;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#36328;&#39046;&#22495;&#20219;&#21153;&#20013;&#20943;&#23569;&#39046;&#22495;&#36716;&#31227;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03253</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Visual Cross-Domain Learners. (arXiv:2401.03253v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;&#65288;LLaVO&#65289;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#36328;&#39046;&#22495;&#20219;&#21153;&#20013;&#20943;&#23569;&#39046;&#22495;&#36716;&#31227;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#30340;&#26368;&#26032;&#36827;&#23637;&#20381;&#36182;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#38754;&#23545;&#39046;&#22495;&#36716;&#31227;&#26102;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#36328;&#39046;&#22495;&#23398;&#20064;&#26088;&#22312;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#30340;&#30693;&#35782;&#65292;&#20943;&#23569;&#35757;&#32451;&#19982;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#22312;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#20013;&#65292;&#20256;&#32479;&#26041;&#27861;&#20165;&#20851;&#27880;&#22270;&#20687;&#27169;&#24577;&#65292;&#24573;&#35270;&#20102;&#20351;&#29992;&#25991;&#26412;&#27169;&#24577;&#26469;&#32531;&#35299;&#39046;&#22495;&#36716;&#31227;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;&#65288;LLaVO&#65289;&#12290;LLaVO&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#35814;&#32454;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#22312;&#32463;&#36807;&#35774;&#35745;&#30340;&#25351;&#23548;&#27169;&#26495;&#29983;&#25104;&#30340;&#28304;&#22495;/&#30446;&#26631;&#22495;&#30340;&#25991;&#26412;&#25551;&#36848;&#19978;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#39046;&#22495;&#27867;&#21270;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#36827;&#34892;&#30340;&#21508;&#31181;&#36328;&#39046;&#22495;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances achieved by deep learning models rely on the independent and identically distributed assumption, hindering their applications in real-world scenarios with domain shifts. To address the above issues, cross-domain learning aims at extracting domain-invariant knowledge to reduce the domain shift between training and testing data. However, in visual cross-domain learning, traditional methods concentrate solely on the image modality, neglecting the use of the text modality to alleviate the domain shift. In this work, we propose Large Language models as Visual cross-dOmain learners (LLaVO). LLaVO uses vision-language models to convert images into detailed textual descriptions. A large language model is then finetuned on textual descriptions of the source/target domain generated by a designed instruction template. Extensive experimental results on various cross-domain tasks under the domain generalization and unsupervised domain adaptation settings have demonstrated the effect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2312.05720</link><description>&lt;p&gt;
&#36229;&#36234;&#26799;&#24230;&#21644;&#20808;&#39564;&#30693;&#35782;&#22312;&#38544;&#31169;&#25915;&#20987;&#20013;&#65306;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24378;&#35843;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#36890;&#36807;&#26412;&#22320;&#23384;&#20648;&#25968;&#25454;&#24182;&#20165;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#65292;&#24378;&#35843;&#29992;&#25143;&#38544;&#31169;&#12290;&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#26377;&#20851;&#38544;&#31169;&#25915;&#20987;&#30340;&#24037;&#20316;&#36890;&#36807;&#20174;&#32852;&#37030;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#25935;&#24863;&#30340;&#35757;&#32451;&#25991;&#26412;&#26469;&#25439;&#23475;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#25216;&#26415;&#38754;&#20020;&#30528;&#19981;&#21516;&#30340;&#38556;&#30861;&#65306;&#19968;&#20123;&#24037;&#20316;&#20027;&#35201;&#20351;&#29992;&#26377;&#38480;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#65288;&#20363;&#22914;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#20026;1&#65289;&#65292;&#32780;&#20854;&#20182;&#25216;&#26415;&#21017;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#38590;&#20197;&#26816;&#27979;&#30340;&#29305;&#28857;&#65292;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#35774;&#32622;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26412;&#24674;&#22797;&#29575;&#12290;&#22522;&#20110;&#22522;&#26412;&#30340;&#26799;&#24230;&#21305;&#37197;&#21644;&#39046;&#22495;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#22686;&#24378;&#25915;&#20987;&#33021;&#21147;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29305;&#24449;&#32423;&#21035;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#19982;&#26799;&#24230;&#25968;&#25454;&#19981;&#21516;&#65292;&#36825;&#20123;&#20449;&#21495;&#19981;&#20250;&#22312;&#21477;&#23376;&#21644;&#26631;&#35760;&#20043;&#38388;&#36827;&#34892;&#24179;&#22343;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.17513</link><description>&lt;p&gt;
&#12298;&#20302;&#31209;&#36866;&#24212;&#30340;&#34920;&#36798;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#30697;&#38453;&#30340;&#20302;&#31209;&#36866;&#24212;&#24615;&#65292;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#26159;LoRA&#30340;&#29702;&#35770;&#22522;&#30784;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#20174;&#29702;&#35770;&#35282;&#24230;&#20998;&#26512;LoRA&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#39318;&#27425;&#23581;&#35797;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#26524;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#65292;&#21017;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#24403;LoRA-rank&#20302;&#20110;&#38408;&#20540;&#26102;&#65292;&#25105;&#20204;&#36824;&#37327;&#21270;&#20102;&#36924;&#36817;&#35823;&#24046;&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20219;&#20309;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
&lt;/p&gt;</description></item><item><title>CLEX&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#38271;&#24230;&#22806;&#25512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20301;&#32622;&#23884;&#20837;&#32553;&#25918;&#26041;&#27861;&#25512;&#24191;&#21040;&#36830;&#32493;&#21160;&#24577;&#24314;&#27169;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#29305;&#23450;&#38271;&#24230;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16450</link><description>&lt;p&gt;
CLEX: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#38271;&#24230;&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
CLEX: Continuous Length Extrapolation for Large Language Models. (arXiv:2310.16450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16450
&lt;/p&gt;
&lt;p&gt;
CLEX&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#38271;&#24230;&#22806;&#25512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20301;&#32622;&#23884;&#20837;&#32553;&#25918;&#26041;&#27861;&#25512;&#24191;&#21040;&#36830;&#32493;&#21160;&#24577;&#24314;&#27169;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#29305;&#23450;&#38271;&#24230;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21331;&#36234;&#33021;&#21147;&#21463;&#38480;&#20110;Transformer&#30340;&#39044;&#35774;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#20301;&#32622;&#23884;&#20837;&#65288;PE&#65289;&#32553;&#25918;&#26041;&#27861;&#34429;&#28982;&#33021;&#22815;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#29305;&#23450;&#38271;&#24230;&#65292;&#20294;&#22312;&#22806;&#25512;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#65292;&#25110;&#32773;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#29306;&#29298;&#37096;&#20998;&#24615;&#33021;&#12290;&#34429;&#28982;&#38271;&#24230;&#22806;&#25512;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#33021;&#22815;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#24310;&#38271;&#33267;&#35757;&#32451;&#24207;&#21015;&#38271;&#24230;&#20043;&#22806;&#65292;&#20294;&#22312;&#23454;&#38469;&#30340;&#38271;&#19978;&#19979;&#25991;&#24212;&#29992;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;LLMs&#30340;&#25345;&#32493;&#38271;&#24230;&#22806;&#25512;&#65288;CLEX&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;PE&#32553;&#25918;&#26041;&#27861;&#25512;&#24191;&#21040;&#36890;&#36807;&#24120;&#24494;&#20998;&#26041;&#31243;&#23545;&#38271;&#24230;&#32553;&#25918;&#22240;&#23376;&#24314;&#27169;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#24403;&#21069;&#20026;&#29305;&#23450;&#38271;&#24230;&#35774;&#35745;&#30340;PE&#32553;&#25918;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window. Length extrapolation methods, although theoretically capable of extending the context window beyond the training sequence length, often underperform in practical long-context applications. To address these challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths. Moreover, by extending 
&lt;/p&gt;</description></item><item><title>GPT-who&#26159;&#19968;&#31181;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21033;&#29992;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#30340;&#29305;&#24449;&#26469;&#24314;&#27169;&#27599;&#20010;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20316;&#32773;&#30340;&#29420;&#29305;&#32479;&#35745;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20316;&#32773;&#24402;&#23646;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#65292;GPT-who&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#65292;&#19988;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06202</link><description>&lt;p&gt;
GPT-who&#65306;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#23494;&#24230;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
GPT-who: An Information Density-based Machine-Generated Text Detector. (arXiv:2310.06202v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06202
&lt;/p&gt;
&lt;p&gt;
GPT-who&#26159;&#19968;&#31181;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21033;&#29992;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#30340;&#29305;&#24449;&#26469;&#24314;&#27169;&#27599;&#20010;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20316;&#32773;&#30340;&#29420;&#29305;&#32479;&#35745;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20316;&#32773;&#24402;&#23646;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#65292;GPT-who&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#65292;&#19988;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#35748;&#20026;&#20154;&#31867;&#22312;&#35821;&#35328;&#20135;&#29983;&#36807;&#31243;&#20013;&#21916;&#27426;&#24179;&#22343;&#20998;&#24067;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#25429;&#25417;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-who&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#35821;&#35328;&#23398;&#30340;&#22810;&#31867;&#39046;&#22495;&#19981;&#21487;&#30693;&#32479;&#35745;&#26816;&#27979;&#22120;&#12290;&#35813;&#26816;&#27979;&#22120;&#21033;&#29992;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#30340;&#29305;&#24449;&#24314;&#27169;&#27599;&#20010;LLM&#21644;&#20154;&#31867;&#20316;&#32773;&#30340;&#29420;&#29305;&#32479;&#35745;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20316;&#32773;&#24402;&#23646;&#12290;&#25105;&#20204;&#20351;&#29992;4&#20010;&#22823;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;GPT-who&#22312;&#21508;&#20010;&#39046;&#22495;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#65288;&#21253;&#25324;&#22522;&#20110;&#32479;&#35745;&#21644;&#38750;&#32479;&#35745;&#30340;&#65289;&#65292;&#22914;GLTR&#65292;GPTZero&#65292;OpenAI detector&#21644;ZeroGPT&#36229;&#36807;20&#65285;&#12290;&#38500;&#20102;&#24615;&#33021;&#20248;&#36234;&#22806;&#65292;GPT-who&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#65292;&#24182;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#30340;&#34920;&#31034;&#30340;&#26368;&#22823;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Uniform Information Density principle posits that humans prefer to spread information evenly during language production. In this work, we examine if the UID principle can help capture differences between Large Language Models (LLMs) and human-generated text. We propose GPT-who, the first psycholinguistically-aware multi-class domain-agnostic statistical-based detector. This detector employs UID-based features to model the unique statistical signature of each LLM and human author for accurate authorship attribution. We evaluate our method using 4 large-scale benchmark datasets and find that GPT-who outperforms state-of-the-art detectors (both statistical- &amp; non-statistical-based) such as GLTR, GPTZero, OpenAI detector, and ZeroGPT by over $20$% across domains. In addition to superior performance, it is computationally inexpensive and utilizes an interpretable representation of text articles. We present the largest analysis of the UID-based representations of human and machine-genera
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#36164;&#28304;&#20999;&#25442;&#35821;&#38899;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#20999;&#25442;&#35821;&#35328;&#33021;&#21147;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#21644;&#27169;&#22411;&#22823;&#23567;&#23545;&#22522;&#20934;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#22312;&#20999;&#25442;&#35821;&#35328;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#21333;&#35821;&#21464;&#20307;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.03018</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#38899;&#35821;&#21477;&#23545;&#30340;&#38646;&#36164;&#28304;&#20999;&#25442;&#35821;&#38899;&#22522;&#20934;&#36827;&#34892;&#22810;&#35821;&#35328;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Zero Resource Code-switched Speech Benchmark Using Speech Utterance Pairs For Multiple Spoken Languages. (arXiv:2310.03018v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#36164;&#28304;&#20999;&#25442;&#35821;&#38899;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#20999;&#25442;&#35821;&#35328;&#33021;&#21147;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#21644;&#27169;&#22411;&#22823;&#23567;&#23545;&#22522;&#20934;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#22312;&#20999;&#25442;&#35821;&#35328;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#21333;&#35821;&#21464;&#20307;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#36164;&#28304;&#20999;&#25442;&#35821;&#38899;&#22522;&#20934;&#65292;&#26088;&#22312;&#30452;&#25509;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#20999;&#25442;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#21333;&#20803;&#35821;&#35328;&#24314;&#27169;&#30340;&#22522;&#32447;&#31995;&#32479;&#65292;&#20197;&#23637;&#31034;&#22914;&#20309;&#20197;&#38646;&#36164;&#28304;&#30340;&#26041;&#24335;&#35780;&#20272;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#20999;&#25442;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28085;&#30422;&#20102;&#21508;&#31181;&#30693;&#21517;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#21253;&#25324;Wav2vec 2.0&#12289;HuBERT&#12289;XLSR&#31561;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#21644;&#27169;&#22411;&#22823;&#23567;&#23545;&#22522;&#20934;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20999;&#25442;&#35821;&#35328;&#22330;&#26223;&#20013;&#65292;&#20855;&#26377;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;XLSR&#65289;&#20248;&#20110;&#21333;&#35821;&#21464;&#20307;&#65288;Wav2vec 2.0&#12289;HuBERT&#65289;&#65292;&#20294;&#23427;&#20204;&#30340;&#20999;&#25442;&#35821;&#35328;&#33021;&#21147;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new zero resource code-switched speech benchmark designed to directly assess the code-switching capabilities of self-supervised speech encoders. We showcase a baseline system of language modeling on discrete units to demonstrate how the code-switching abilities of speech encoders can be assessed in a zero-resource manner. Our experiments encompass a variety of well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We examine the impact of pre-training languages and model size on benchmark performance. Notably, though our results demonstrate that speech encoders with multilingual pre-training, exemplified by XLSR, outperform monolingual variants (Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial room for improvement in their code-switching linguistic abilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;</title><link>http://arxiv.org/abs/2309.16701</link><description>&lt;p&gt;
MVMR: &#22312;&#22810;&#20010;&#21487;&#38752;&#35270;&#39057;&#38598;&#20013;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool. (arXiv:2309.16701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#28608;&#22686;&#65292;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#23427;&#33268;&#21147;&#20110;&#26816;&#27979;&#19982;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#21305;&#37197;&#30340;&#35270;&#39057;&#29255;&#27573;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#37117;&#27809;&#26377;&#25506;&#32034;&#22312;&#23384;&#22312;&#22810;&#20010;&#27491;&#36127;&#35270;&#39057;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#20013;&#23450;&#20301;&#19968;&#20010;&#26102;&#21051;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#65288;Massive Videos Moment Retrieval&#65289;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#29616;&#26377;&#35270;&#39057;&#23450;&#20301;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#23884;&#20837;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30446;&#26631;&#26597;&#35810;&#19982;&#35270;&#39057;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#20174;&#32780;&#23450;&#20041;&#27491;&#36127;&#38598;&#12290;&#38024;&#23545;&#25552;&#20986;&#30340;MVMR&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;
With the explosion of multimedia content in recent years, natural language video localization, which focuses on detecting video moment that matches a given natural language query, has become a critical problem. However, none of the previous research explores localizing a moment from a large corpus where multiple positive and negative videos exist. In this paper, we propose an MVMR (Massive Videos Moment Retrieval) task, which aims to localize video frames from a massive set of videos given a text query. For this task, we suggest methods for constructing datasets by employing similarity filtering on the existing video localization datasets and introduce three MVMR datasets. Specifically, we employ embedding-based text similarity matching and video-language grounding techniques to calculate the relevance score between a target query and videos to define positive and negative sets. For the proposed MVMR task, we further develop a strong model, Reliable Mutual Matching Network (RMMN), whic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25511;&#21046;&#26426;&#21046;&#25913;&#21892;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;&#65292;&#20855;&#20307;&#21253;&#25324;&#39046;&#22495;&#24494;&#35843;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#24212;&#29992;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;GPT&#27169;&#22411;&#30340;&#25511;&#21046;&#20196;&#29260;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.13202</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25511;&#21046;&#26426;&#21046;&#25552;&#39640;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts. (arXiv:2309.13202v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25511;&#21046;&#26426;&#21046;&#25913;&#21892;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;&#65292;&#20855;&#20307;&#21253;&#25324;&#39046;&#22495;&#24494;&#35843;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#24212;&#29992;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;GPT&#27169;&#22411;&#30340;&#25511;&#21046;&#20196;&#29260;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#36890;&#24120;&#20351;&#29992;&#22797;&#26434;&#30340;&#35821;&#35328;&#21644;&#38590;&#20197;&#29702;&#35299;&#30340;&#19987;&#19994;&#26415;&#35821;&#12290;&#22240;&#27492;&#65292;&#31616;&#21270;&#22312;&#25552;&#39640;&#20844;&#20849;&#20581;&#24247;&#32032;&#20859;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#27492;&#31867;&#20219;&#21153;&#21487;&#20197;&#20351;&#38750;&#19987;&#19994;&#35835;&#32773;&#24555;&#36895;&#30452;&#25509;&#22320;&#33719;&#21462;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#31616;&#21270;&#30340;&#25968;&#25454;&#38598;&#65288;PLABA&#65289;&#26469;&#35843;&#26597;&#26368;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#31616;&#21270;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#24212;&#29992;&#30340;&#26041;&#27861;&#21253;&#25324;&#39046;&#22495;&#24494;&#35843;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#65288;PBL&#65289;&#22312;&#65306;1&#65289;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65288;T5&#12289;SciFive&#21644;BART&#65289;&#19978;&#65292;2&#65289;&#20165;&#35299;&#30721;&#22120;&#30340;GPT&#27169;&#22411;&#65288;GPT-3.5&#21644;GPT-4&#65289;&#26469;&#33258;OpenAI&#21644;BioGPT&#65292;&#20197;&#21450;3&#65289;&#22522;&#20110;&#25511;&#21046;&#20196;&#29260;&#26426;&#21046;&#30340;&#22522;&#20110;BART&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;BLEU&#12289;ROUGE&#12289;SARI&#21644;BERTscore&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature often uses complex language and inaccessible professional terminologies. That is why simplification plays an important role in improving public health literacy. Applying Natural Language Processing (NLP) models to automate such tasks allows for quick and direct accessibility for lay readers. In this work, we investigate the ability of state-of-the-art large language models (LLMs) on the task of biomedical abstract simplification, using the publicly available dataset for plain language adaptation of biomedical abstracts (\textbf{PLABA}). The methods applied include domain fine-tuning and prompt-based learning (PBL) on: 1) Encoder-decoder models (T5, SciFive, and BART), 2) Decoder-only GPT models (GPT-3.5 and GPT-4) from OpenAI and BioGPT, and 3) Control-token mechanisms on BART-based models. We used a range of automatic evaluation metrics, including BLEU, ROUGE, SARI, and BERTscore, and also conducted human evaluations. BART-Large with Control Token (BART-L-w-CT) m
&lt;/p&gt;</description></item><item><title>DreamLLM&#26159;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;&#36890;&#36807;&#30452;&#25509;&#37319;&#26679;&#29983;&#25104;&#35821;&#35328;&#21644;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;DreamLLM&#33021;&#22815;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#20132;&#32455;&#20869;&#23481;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11499</link><description>&lt;p&gt;
DreamLLM&#65306;&#21327;&#21516;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
DreamLLM: Synergistic Multimodal Comprehension and Creation. (arXiv:2309.11499v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11499
&lt;/p&gt;
&lt;p&gt;
DreamLLM&#26159;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;&#36890;&#36807;&#30452;&#25509;&#37319;&#26679;&#29983;&#25104;&#35821;&#35328;&#21644;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;DreamLLM&#33021;&#22815;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#20132;&#32455;&#20869;&#23481;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DreamLLM&#65292;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#39318;&#27425;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#65292;&#21033;&#29992;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;&#20043;&#38388;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;DreamLLM&#36981;&#24490;&#20004;&#20010;&#22522;&#26412;&#21407;&#21017;&#12290;&#31532;&#19968;&#20010;&#21407;&#21017;&#19987;&#27880;&#20110;&#36890;&#36807;&#22312;&#21407;&#22987;&#22810;&#27169;&#24577;&#31354;&#38388;&#20013;&#36827;&#34892;&#30452;&#25509;&#37319;&#26679;&#26469;&#29983;&#25104;&#35821;&#35328;&#21644;&#22270;&#20687;&#21518;&#39564;&#30340;&#29983;&#25104;&#24314;&#27169;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#20687;CLIP&#36825;&#26679;&#30340;&#22806;&#37096;&#29305;&#24449;&#25552;&#21462;&#22120;&#25152;&#22266;&#26377;&#30340;&#38480;&#21046;&#21644;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;&#20854;&#27425;&#65292;DreamLLM&#20419;&#36827;&#20102;&#21407;&#22987;&#30340;&#12289;&#20132;&#32455;&#30340;&#25991;&#20214;&#29983;&#25104;&#65292;&#23545;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#20197;&#21450;&#38750;&#32467;&#26500;&#21270;&#24067;&#23616;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#20351;&#24471;DreamLLM&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#25152;&#26377;&#26465;&#20214;&#12289;&#36793;&#32536;&#21644;&#32852;&#21512;&#22810;&#27169;&#24577;&#20998;&#24067;&#12290;&#20316;&#20026;&#32467;&#26524;&#65292;DreamLLM&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#20132;&#32455;&#20869;&#23481;&#30340;MLLM&#12290;&#32508;&#21512;&#23454;&#39564;&#31361;&#26174;&#20102;DreamLLM&#20316;&#20026;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal
&lt;/p&gt;</description></item><item><title>OpenChat&#26159;&#19968;&#31181;&#29992;&#20110;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#24182;&#31616;&#21270;RLFT&#26041;&#27861;&#30340;&#27714;&#35299;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.11235</link><description>&lt;p&gt;
OpenChat: &#29992;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OpenChat: Advancing Open-source Language Models with Mixed-Quality Data. (arXiv:2309.11235v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11235
&lt;/p&gt;
&lt;p&gt;
OpenChat&#26159;&#19968;&#31181;&#29992;&#20110;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#24182;&#31616;&#21270;RLFT&#26041;&#27861;&#30340;&#27714;&#35299;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20687;LLaMA&#36825;&#26679;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#20986;&#29616;&#12290;&#26368;&#36817;&#30340;&#21457;&#23637;&#20013;&#20351;&#29992;&#20102;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#65288;RLFT&#65289;&#26469;&#20351;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#31867;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;SFT&#26041;&#27861;&#23558;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#28151;&#21512;&#36136;&#37327;&#31561;&#21516;&#23545;&#24453;&#65292;&#32780;RLFT&#26041;&#27861;&#21017;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25104;&#23545;&#25110;&#22522;&#20110;&#25490;&#21517;&#30340;&#20559;&#22909;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;OpenChat&#65292;&#29992;&#20110;&#21033;&#29992;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#33324;&#30340;SFT&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#23569;&#37327;&#30340;&#19987;&#23478;&#25968;&#25454;&#21644;&#22823;&#37327;&#30340;&#27425;&#20248;&#25968;&#25454;&#65292;&#27809;&#26377;&#20219;&#20309;&#20248;&#20808;&#32423;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;C(onditioned)-RLFT&#65292;&#23558;&#19981;&#21516;&#30340;&#25968;&#25454;&#28304;&#35270;&#20026;&#31895;&#31890;&#24230;&#30340;&#22870;&#21169;&#26631;&#31614;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010;&#26465;&#20214;&#21270;&#31574;&#30053;&#65292;&#20197;&#21033;&#29992;&#20114;&#34917;&#30340;&#25968;&#25454;&#36136;&#37327;&#20449;&#24687;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;C-RLFT&#20013;&#65292;&#26368;&#20248;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;&#21333;&#38454;&#27573;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#30417;&#30563;&#23398;&#20064;&#36731;&#26494;&#27714;&#35299;&#65292;&#20351;&#24471;&#35813;&#38382;&#39064;&#24471;&#21040;&#20102;&#31616;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2309.05961</link><description>&lt;p&gt;
&#35780;&#20272;&#28526;&#36215;&#28526;&#33853;&#65306;&#23545;&#19981;&#21516;&#24179;&#21488;&#38388;&#38382;&#31572;&#36235;&#21183;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#22240;&#20854;&#24555;&#36895;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#30340;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#22238;&#31572;&#36895;&#24230;&#30340;&#24555;&#24930;&#21462;&#20915;&#20110;&#26597;&#35810;&#29305;&#23450;&#21644;&#29992;&#25143;&#30456;&#20851;&#30340;&#22240;&#32032;&#30340;&#32508;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20845;&#20010;&#39640;&#24230;&#27969;&#34892;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#22240;&#32032;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22238;&#31572;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#19982;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#30340;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#27700;&#24179;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#20803;&#25968;&#25454;&#21644;&#29992;&#25143;&#20114;&#21160;&#27169;&#24335;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#21738;&#20123;&#26597;&#35810;&#23558;&#36805;&#36895;&#33719;&#24471;&#21021;&#22987;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#20869;&#23481;&#21644;&#34892;&#20026;&#27169;&#22411;&#26469;&#29702;&#35299;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#20869;&#23481;&#21644;&#34892;&#20026;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#22312;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36824;&#26080;&#27861;&#35299;&#20915;&#39044;&#27979;&#21644;&#20248;&#21270;&#36890;&#20449;&#20197;&#23454;&#29616;&#26399;&#26395;&#25509;&#25910;&#32773;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#21407;&#22240;&#21487;&#33021;&#26159;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32570;&#23569;"&#34892;&#20026;&#26631;&#35760;"&#12290;</title><link>http://arxiv.org/abs/2309.00359</link><description>&lt;p&gt;
&#22823;&#22411;&#20869;&#23481;&#21644;&#34892;&#20026;&#27169;&#22411;&#29992;&#20110;&#29702;&#35299;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#20869;&#23481;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior. (arXiv:2309.00359v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#20869;&#23481;&#21644;&#34892;&#20026;&#27169;&#22411;&#26469;&#29702;&#35299;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#20869;&#23481;&#21644;&#34892;&#20026;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#22312;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36824;&#26080;&#27861;&#35299;&#20915;&#39044;&#27979;&#21644;&#20248;&#21270;&#36890;&#20449;&#20197;&#23454;&#29616;&#26399;&#26395;&#25509;&#25910;&#32773;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#21407;&#22240;&#21487;&#33021;&#26159;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32570;&#23569;"&#34892;&#20026;&#26631;&#35760;"&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39321;&#20892;&#22312;&#24341;&#20837;&#20449;&#24687;&#29702;&#35770;&#30340;&#32463;&#20856;&#35770;&#25991;&#20013;&#23558;&#36890;&#20449;&#20998;&#20026;&#19977;&#20010;&#23618;&#27425;&#65306;&#25216;&#26415;&#23618;&#12289;&#35821;&#20041;&#23618;&#21644;&#25928;&#26524;&#23618;&#12290;&#25216;&#26415;&#23618;&#20851;&#27880;&#30340;&#26159;&#20934;&#30830;&#37325;&#26500;&#20256;&#36755;&#30340;&#31526;&#21495;&#65292;&#32780;&#35821;&#20041;&#23618;&#21644;&#25928;&#26524;&#23618;&#21017;&#28041;&#21450;&#25512;&#26029;&#20986;&#30340;&#24847;&#20041;&#21450;&#20854;&#23545;&#25509;&#25910;&#32773;&#30340;&#24433;&#21709;&#12290;&#24471;&#30410;&#20110;&#30005;&#20449;&#25216;&#26415;&#65292;&#31532;&#19968;&#23618;&#38382;&#39064;&#24050;&#32463;&#21462;&#24471;&#20102;&#36739;&#22823;&#30340;&#36827;&#27493;&#65292;&#22914;&#20114;&#32852;&#32593;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#31532;&#20108;&#20010;&#30446;&#26631;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#31532;&#19977;&#23618;&#20173;&#28982;&#22522;&#26412;&#19978;&#26410;&#34987;&#35302;&#21450;&#12290;&#31532;&#19977;&#20010;&#38382;&#39064;&#28041;&#21450;&#39044;&#27979;&#21644;&#20248;&#21270;&#36890;&#20449;&#20197;&#23454;&#29616;&#26399;&#26395;&#30340;&#25509;&#25910;&#32773;&#34892;&#20026;&#12290;LLM&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#20043;&#19968;&#21487;&#33021;&#26159;LLM&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32570;&#23569;"&#34892;&#20026;&#26631;&#35760;"&#12290;&#34892;&#20026;&#26631;&#35760;&#23450;&#20041;&#20102;&#22312;&#19968;&#27425;&#36890;&#20449;&#20013;&#30340;&#25509;&#25910;&#32773;&#34892;&#20026;&#65292;&#22914;&#20998;&#20139;&#12289;&#28857;&#36190;&#12289;&#28857;&#20987;&#12289;&#36141;&#20080;&#12289;&#36716;&#25512;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shannon, in his seminal paper introducing information theory, divided the communication into three levels: technical, semantic, and effectivenss. While the technical level is concerned with accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Thanks to telecommunications, the first level problem has produced great advances like the internet. Large Language Models (LLMs) make some progress towards the second goal, but the third level still remains largely untouched. The third problem deals with predicting and optimizing communication for desired receiver behavior. LLMs, while showing wide generalization capabilities across a wide range of tasks, are unable to solve for this. One reason for the underperformance could be a lack of "behavior tokens" in LLMs' training corpora. Behavior tokens define receiver behavior over a communication, such as shares, likes, clicks, purchases, retweets, etc. W
&lt;/p&gt;</description></item><item><title>OmniQuant&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.13137</link><description>&lt;p&gt;
OmniQuant&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. (arXiv:2308.13137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13137
&lt;/p&gt;
&lt;p&gt;
OmniQuant&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#20102;&#20854;&#24222;&#22823;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26041;&#27861;&#22312;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#25552;&#39640;LLM&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#25163;&#24037;&#21046;&#23450;&#37327;&#21270;&#21442;&#25968;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#20302;&#24182;&#19988;&#19981;&#33021;&#22788;&#29702;&#26497;&#20302;&#20301;&#37327;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#65288;OmniQuant&#65289;&#25216;&#26415;&#65292;&#29992;&#20110;LLMs&#65292;&#23427;&#22312;&#22810;&#31181;&#37327;&#21270;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#26469;&#20445;&#25345;PTQ&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;OmniQuant&#21253;&#21547;&#20004;&#20010;&#21019;&#26032;&#32452;&#20214;&#65292;&#21253;&#25324;&#21487;&#23398;&#20064;&#30340;&#26435;&#37325;&#21098;&#35009;&#65288;LWC&#65289;&#21644;&#21487;&#23398;&#20064;&#30340;&#31561;&#25928;&#21464;&#25442;&#65288;LET&#65289;&#12290;LWC&#36890;&#36807;&#20248;&#21270;&#21098;&#35009;&#38408;&#20540;&#26469;&#35843;&#33410;&#26435;&#37325;&#30340;&#26497;&#20540;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LET&#22788;&#29702;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activa
&lt;/p&gt;</description></item><item><title>LatEval&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;LLMs&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#20391;&#24605;&#32500;&#35868;&#39064;&#25361;&#25112;&#27169;&#22411;&#30340;&#27178;&#21521;&#24605;&#32771;&#33021;&#21147;&#65292;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#32771;&#39564;&#27169;&#22411;&#25552;&#20986;&#38382;&#39064;&#30340;&#36136;&#37327;&#21644;&#25972;&#21512;&#20449;&#24687;&#35299;&#20915;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20960;&#20046;&#25152;&#26377;LLMs&#22312;&#27178;&#21521;&#24605;&#32771;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#30456;&#27604;&#20154;&#31867;&#20063;&#26377;&#26126;&#26174;&#24046;&#36317;&#12290;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#23545;&#20110;&#26377;&#25928;&#30340;AI&#21161;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.10855</link><description>&lt;p&gt;
LatEval: &#19968;&#20010;&#24102;&#26377;&#26469;&#33258;&#20391;&#24605;&#32500;&#35868;&#39064;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#30340;&#20132;&#20114;&#24335;LLMs&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles. (arXiv:2308.10855v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10855
&lt;/p&gt;
&lt;p&gt;
LatEval&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;LLMs&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#20391;&#24605;&#32500;&#35868;&#39064;&#25361;&#25112;&#27169;&#22411;&#30340;&#27178;&#21521;&#24605;&#32771;&#33021;&#21147;&#65292;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#32771;&#39564;&#27169;&#22411;&#25552;&#20986;&#38382;&#39064;&#30340;&#36136;&#37327;&#21644;&#25972;&#21512;&#20449;&#24687;&#35299;&#20915;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20960;&#20046;&#25152;&#26377;LLMs&#22312;&#27178;&#21521;&#24605;&#32771;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#30456;&#27604;&#20154;&#31867;&#20063;&#26377;&#26126;&#26174;&#24046;&#36317;&#12290;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#23545;&#20110;&#26377;&#25928;&#30340;AI&#21161;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;LLMs&#30340;&#19981;&#26029;&#21457;&#23637;&#21644;&#25913;&#36827;&#65292;&#23427;&#20204;&#34987;&#36171;&#20104;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36923;&#36753;&#25512;&#29702;&#25110;&#32437;&#21521;&#24605;&#32500;&#33021;&#21147;&#12290;&#20294;&#23427;&#20204;&#33021;&#21542;&#36339;&#20986;&#22266;&#23450;&#27169;&#24335;&#24605;&#32771;&#65311;&#23427;&#20204;&#26159;&#21542;&#20855;&#22791;&#39640;&#36229;&#30340;&#27178;&#21521;&#24605;&#32771;&#33021;&#21147;&#65311;&#22312;&#20391;&#24605;&#32500;&#35868;&#39064;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#31216;&#20026;LatEval&#65292;&#23427;&#22312;&#19968;&#20010;&#20132;&#20114;&#24335;&#26694;&#26550;&#20013;&#35780;&#20272;&#27169;&#22411;&#30340;&#27178;&#21521;&#24605;&#32771;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#21521;LLMs&#25552;&#20986;&#20102;&#20004;&#20010;&#26041;&#38754;&#30340;&#25361;&#25112;&#65306;&#27169;&#22411;&#25552;&#20986;&#30340;&#38382;&#39064;&#30340;&#36136;&#37327;&#21644;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#25972;&#21512;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#30340;LLMs&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#37117;&#24456;&#38590;&#36816;&#29992;&#27178;&#21521;&#24605;&#32771;&#12290;&#20363;&#22914;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;GPT-4&#65292;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#20063;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#19982;&#20154;&#31867;&#30456;&#27604;&#20173;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#36825;&#20010;&#35780;&#20272;&#22522;&#20934;&#20026;LLMs&#25552;&#20379;&#20102;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#21644;&#29420;&#29305;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#21161;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continuous evolution and refinement of LLMs, they are endowed with impressive logical reasoning or vertical thinking capabilities. But can they think out of the box? Do they possess proficient lateral thinking abilities? Following the setup of Lateral Thinking Puzzles, we propose a novel evaluation benchmark, LatEval, which assesses the model's lateral thinking within an interactive framework. In our benchmark, we challenge LLMs with 2 aspects: the quality of questions posed by the model and the model's capability to integrate information for problem-solving. We find that nearly all LLMs struggle with employing lateral thinking during interactions. For example, even the most advanced model, GPT-4, exhibits the advantage to some extent, yet still maintain a noticeable gap when compared to human. This evaluation benchmark provides LLMs with a highly challenging and distinctive task that is crucial to an effective AI assistant.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01313</link><description>&lt;p&gt;
&#26356;&#22810;&#19978;&#19979;&#25991;&#65292;&#26356;&#23569;&#24178;&#25200;&#65306;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#36827;&#34892;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#20316;&#20026;&#19968;&#31181;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#29702;&#35299;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#33021;&#21147;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;CLIP&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#20154;&#31867;&#33324;&#29702;&#35299;&#33021;&#21147;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#20154;&#31867;&#30340;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#20013;&#24471;&#21040;&#21551;&#21457;&#65306;&#29616;&#20195;&#31070;&#32463;&#31185;&#23398;&#35266;&#28857;&#35748;&#20026;&#65292;&#22312;&#23545;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;&#20154;&#31867;&#39318;&#20808;&#25512;&#26029;&#20854;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#23646;&#24615;&#65288;&#22914;&#32972;&#26223;&#21644;&#26041;&#21521;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#23558;&#21069;&#26223;&#23545;&#35937;&#19982;&#32972;&#26223;&#21306;&#20998;&#24320;&#26469;&#65292;&#28982;&#21518;&#20197;&#27492;&#20449;&#24687;&#20026;&#22522;&#30784;&#36827;&#34892;&#20915;&#31574;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20026;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#21487;&#20197;&#25913;&#21892;&#38646;&#26679;&#26412;&#20998;&#31867;&#24182;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;CLIP&#26412;&#36523;&#21487;&#20197;&#21512;&#29702;&#22320;&#20174;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#36825;&#20123;&#23646;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#35757;&#32451;&#12289;&#20004;&#27493;&#39588;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot cl
&lt;/p&gt;</description></item><item><title>RLCD&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#33976;&#39311;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#12290;&#22312;&#22810;&#20010;&#23545;&#40784;&#20219;&#21153;&#21644;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#19978;&#65292;RLCD&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.12950</link><description>&lt;p&gt;
RLCD: &#22522;&#20110;&#23545;&#27604;&#33976;&#39311;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment. (arXiv:2307.12950v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12950
&lt;/p&gt;
&lt;p&gt;
RLCD&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#33976;&#39311;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#12290;&#22312;&#22810;&#20010;&#23545;&#40784;&#20219;&#21153;&#21644;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#19978;&#65292;RLCD&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Reinforcement Learning from Contrast Distillation (RLCD)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#38656;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#21363;&#21487;&#20351;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#30340;&#23545;&#40784;&#12290;RLCD&#20351;&#29992;&#27169;&#25311;&#30340;&#20559;&#22909;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#23545;&#21253;&#21547;&#20102;&#39640;&#36136;&#37327;&#21644;&#20302;&#36136;&#37327;&#30340;&#31034;&#20363;&#65292;&#20854;&#20013;&#20351;&#29992;&#23545;&#27604;&#30340;&#27491;&#36127;&#25552;&#31034;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20559;&#22909;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#25913;&#36827;&#22522;&#30784;&#30340;&#26080;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;RLCD&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#23545;&#40784;&#20219;&#21153;&#65288;&#26080;&#23475;&#24615;&#12289;&#26377;&#29992;&#24615;&#21644;&#25925;&#20107;&#22823;&#32434;&#29983;&#25104;&#65289;&#20197;&#21450;7B&#21644;30B&#27169;&#22411;&#35268;&#27169;&#30340;&#20559;&#22909;&#25968;&#25454;&#27169;&#25311;&#19978;&#65292;&#37117;&#20248;&#20110;RLAIF (Bai&#31561;&#20154;&#65292;2022b)&#21644;&#19978;&#19979;&#25991;&#33976;&#39311; (Huang&#31561;&#20154;&#65292;2022) &#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Reinforcement Learning from Contrast Distillation (RLCD), a method for aligning language models to follow natural language principles without using human feedback. RLCD trains a preference model using simulated preference pairs that contain both a high-quality and low-quality example, generated using contrasting positive and negative prompts. The preference model is then used to improve a base unaligned language model via reinforcement learning. Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al., 2022) baselines across three diverse alignment tasks--harmlessness, helpfulness, and story outline generation--and on both 7B and 30B model scales for preference data simulation.
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06945</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06945
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65288;ICAE&#65289;&#12290; ICAE&#26377;&#20004;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#37319;&#29992;LoRA&#26041;&#24335;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#20197;&#21450;&#19968;&#20010;&#22266;&#23450;&#30340;&#35299;&#30721;&#22120;&#65292;&#20316;&#20026;&#30446;&#26631;LLM&#65292;&#21487;&#20197;&#26681;&#25454;&#20869;&#23384;&#27133;&#26469;&#36827;&#34892;&#21508;&#31181;&#30446;&#30340;&#30340;&#26465;&#20214;&#22788;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#33258;&#32534;&#30721;&#21644;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;ICAE&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21644;&#20840;&#38754;&#34920;&#31034;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#20869;&#23384;&#27133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#25351;&#23548;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#30340;ICAE&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#19982;&#21508;&#31181;&#25552;&#31034;&#30340;&#20132;&#20114;&#65292;&#20174;&#32780;&#20135;&#29983;&#29702;&#24819;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#23398;&#20064;&#30340;ICAE&#21487;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;$4\times$&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#20869;&#23384;&#27133;&#65292;&#30446;&#26631;LLM&#21487;&#20197;&#24456;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35760;&#24518;&#22686;&#24378;&#30340;&#36866;&#37197;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25554;&#25300;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;&#21487;&#25554;&#25300;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06029</link><description>&lt;p&gt;
&#36890;&#36807;&#35760;&#24518;&#22686;&#24378;&#30340;&#36866;&#37197;&#22120;&#23454;&#29616;&#21487;&#25554;&#25300;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pluggable Neural Machine Translation Models via Memory-augmented Adapters. (arXiv:2307.06029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06029
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35760;&#24518;&#22686;&#24378;&#30340;&#36866;&#37197;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25554;&#25300;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;&#21487;&#25554;&#25300;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#22312;&#26222;&#36890;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#25511;&#21046;&#20854;&#29983;&#25104;&#34892;&#20026;&#20197;&#28385;&#36275;&#19981;&#21516;&#29992;&#25143;&#38656;&#27714;&#20173;&#28982;&#20855;&#26377;&#19968;&#23450;&#25361;&#25112;&#24615;&#12290;&#37492;&#20110;&#27599;&#20010;&#29992;&#25143;&#38656;&#27714;&#37117;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#26032;&#27169;&#22411;&#30340;&#39640;&#26114;&#35757;&#32451;&#25104;&#26412;&#21644;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#36866;&#37197;&#22120;&#65292;&#20197;&#21487;&#25554;&#25300;&#30340;&#26041;&#24335;&#24341;&#23548;&#39044;&#35757;&#32451;&#30340;NMT&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#26679;&#26412;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#31890;&#24230;&#35760;&#24518;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#37197;&#22120;&#26550;&#26500;&#26469;&#32467;&#21512;&#27169;&#22411;&#34920;&#31034;&#21644;&#26816;&#32034;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35760;&#24518;&#20002;&#24323;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;NMT&#27169;&#22411;&#21644;&#35760;&#24518;&#20043;&#38388;&#30340;&#34394;&#20551;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#39118;&#26684;&#21644;&#39046;&#22495;&#29305;&#23450;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;&#21487;&#25554;&#25300;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although neural machine translation (NMT) models perform well in the general domain, it remains rather challenging to control their generation behavior to satisfy the requirement of different users. Given the expensive training cost and the data scarcity challenge of learning a new model from scratch for each user requirement, we propose a memory-augmented adapter to steer pretrained NMT models in a pluggable manner. Specifically, we construct a multi-granular memory based on the user-provided text samples and propose a new adapter architecture to combine the model representations and the retrieved results. We also propose a training strategy using memory dropout to reduce spurious dependencies between the NMT model and the memory. We validate our approach on both style- and domain-specific experiments and the results indicate that our method can outperform several representative pluggable baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21307;&#23398;&#23454;&#20307;&#12289;&#26631;&#20934;&#21270;&#23454;&#20307;&#21644;&#20998;&#37197;UMLS&#27010;&#24565;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;COVID-19&#30456;&#20851;&#25512;&#25991;&#30340;&#30151;&#29366;&#35789;&#20856;&#12290;</title><link>http://arxiv.org/abs/2306.16001</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#20197;&#25903;&#25345;&#20844;&#20849;&#21355;&#29983;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning. (arXiv:2306.16001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21307;&#23398;&#23454;&#20307;&#12289;&#26631;&#20934;&#21270;&#23454;&#20307;&#21644;&#20998;&#37197;UMLS&#27010;&#24565;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;COVID-19&#30456;&#20851;&#25512;&#25991;&#30340;&#30151;&#29366;&#35789;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#22312;&#27969;&#34892;&#30149;&#30417;&#27979;&#20013;&#30340;&#21033;&#29992;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#35777;&#23454;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#35789;&#27719;&#34920;&#26469;&#26816;&#32034;&#30456;&#20851;&#35821;&#26009;&#24211;&#26102;&#65292;&#24120;&#24120;&#20250;&#24341;&#20837;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#26500;&#24314;&#21307;&#23398;&#20439;&#35821;&#21644;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#27010;&#24565;&#30340;&#24191;&#27867;&#23383;&#20856;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#22522;&#20110;BERT&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#20013;&#35782;&#21035;&#20986;&#21307;&#23398;&#23454;&#20307;&#65307;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#26631;&#20934;&#21270;&#27169;&#22359;&#65292;&#29992;&#20110;&#23545;&#25552;&#21462;&#20986;&#30340;&#23454;&#20307;&#36827;&#34892;&#35268;&#33539;&#21270;&#22788;&#29702;&#65307;&#21322;&#30417;&#30563;&#32858;&#31867;&#27169;&#22359;&#65292;&#23558;&#26368;&#21487;&#33021;&#30340;UMLS&#27010;&#24565;&#20998;&#37197;&#32473;&#27599;&#20010;&#35268;&#33539;&#21270;&#23454;&#20307;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#20174;2020&#24180;2&#26376;1&#26085;&#21040;2022&#24180;4&#26376;30&#26085;&#26399;&#38388;&#19982;COVID-19&#30456;&#20851;&#30340;&#25512;&#25991;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#30151;&#29366;&#35789;&#20856;&#65288;&#21487;&#22312;https://github.com/ningkko/UMLS_colloquialism/&#19978;&#33719;&#21462;&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;9,249&#20010;&#26631;&#20934;&#21270;&#23454;&#20307;&#65292;&#26144;&#23556;&#21040;876&#20010;UMLS&#27010;&#24565;&#21644;38,175&#20010;&#20442;&#35821;&#34920;&#36798;&#12290;&#35813;&#26694;&#26550;&#30340;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
The utilization of social media in epidemic surveillance has been well established. Nonetheless, bias is often introduced when pre-defined lexicons are used to retrieve relevant corpus. This study introduces a framework aimed at curating extensive dictionaries of medical colloquialisms and Unified Medical Language System (UMLS) concepts. The framework comprises three modules: a BERT-based Named Entity Recognition (NER) model that identifies medical entities from social media content, a deep-learning powered normalization module that standardizes the extracted entities, and a semi-supervised clustering module that assigns the most probable UMLS concept to each standardized entity. We applied this framework to COVID-19-related tweets from February 1, 2020, to April 30, 2022, generating a symptom dictionary (available at https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249 standardized entities mapped to 876 UMLS concepts and 38,175 colloquial expressions. This framework demo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23601;&#19981;&#38656;&#35201;&#24494;&#35843;&#27169;&#22411;&#25110;&#35775;&#38382;&#19987;&#26377;&#20449;&#24687;&#30340;&#26041;&#27861;&#36827;&#34892;&#32622;&#20449;&#24230;&#24341;&#23548;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;LLMs&#24448;&#24448;&#23637;&#29616;&#20986;&#39640;&#24230;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;</title><link>http://arxiv.org/abs/2306.13063</link><description>&lt;p&gt;
LLM&#33021;&#34920;&#36798;&#23427;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#21527;&#65311;LMM&#33258;&#20449;&#24515;&#35780;&#20272;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. (arXiv:2306.13063v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23601;&#19981;&#38656;&#35201;&#24494;&#35843;&#27169;&#22411;&#25110;&#35775;&#38382;&#19987;&#26377;&#20449;&#24687;&#30340;&#26041;&#27861;&#36827;&#34892;&#32622;&#20449;&#24230;&#24341;&#23548;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;LLMs&#24448;&#24448;&#23637;&#29616;&#20986;&#39640;&#24230;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#20934;&#30830;&#34920;&#36798;&#20854;&#32622;&#20449;&#24230;&#30340;&#33021;&#21147;&#65292;&#21363;&#32622;&#20449;&#24230;&#24341;&#23548;&#20219;&#21153;&#65292;&#23545;&#30830;&#20445;&#21487;&#38752;&#21644;&#21487;&#20449;&#30340;&#20915;&#31574;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#38656;&#35201;&#24494;&#35843;&#27169;&#22411;&#25110;&#35775;&#38382;&#19987;&#26377;&#20449;&#24687;&#30340;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#19977;&#31181;&#26041;&#27861;&#65306;&#22522;&#20110;&#34920;&#36848;&#12289;&#22522;&#20110;&#19968;&#33268;&#24615;&#12289;&#20197;&#21450;&#23427;&#20204;&#30340;&#28151;&#21512;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#22312;&#20116;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#21644;&#22235;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;LLMs&#19978;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of empowering large language models (LLMs) to accurately express their confidence, referred to as confidence elicitation, is essential in ensuring reliable and trustworthy decision-making processes. Previous methods, which primarily rely on model logits, have become less suitable for LLMs and even infeasible with the rise of closed-source LLMs (e.g., commercialized LLM APIs). This leads to a growing need to explore the untapped area of \emph{non-logit-based} approaches to estimate the uncertainty of LLMs. Hence, in this study, we investigate approaches for confidence elicitation that do not require model fine-tuning or access to proprietary information. We introduce three categories of methods: verbalize-based, consistency-based, and their hybrid methods for benchmarking, and evaluate their performance across five types of datasets and four widely-used LLMs. Our analysis of these methods uncovers several key insights: 1) LLMs often exhibit a high degree of overconfidence when 
&lt;/p&gt;</description></item><item><title>MOFI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598; I2E&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#23398;&#20064;&#21040;&#20102;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07952</link><description>&lt;p&gt;
MOFI: &#20174;&#21547;&#22122;&#23454;&#20307;&#26631;&#27880;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07952
&lt;/p&gt;
&lt;p&gt;
MOFI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598; I2E&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#23398;&#20064;&#21040;&#20102;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411; MOFI&#65292;&#26088;&#22312;&#20174;&#21547;&#22122;&#23454;&#20307;&#26631;&#27880;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#12290;MOFI &#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#26377;&#20004;&#28857;&#19981;&#21516;&#65306;&#65288;i&#65289;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#65288;ii&#65289;&#35757;&#32451;&#37197;&#26041;&#12290;&#22312;&#25968;&#25454;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#20174; alt-text &#20013;&#25552;&#21462;&#23454;&#20307;&#65292;&#28982;&#21518;&#20351;&#29992; CLIP &#27169;&#22411;&#36873;&#25321;&#27491;&#30830;&#30340;&#23454;&#20307;&#20316;&#20026;&#22270;&#20687;&#30340;&#26631;&#31614;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#26131;&#34892;&#65292;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#20174; web &#19978;&#25366;&#25496;&#30340;&#25968;&#21313;&#20159;&#20010;&#22270;&#20687;&#25991;&#26412;&#23545;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102; Image-to-Entities&#65288;I2E&#65289;&#36825;&#19968;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547; 10 &#20159;&#24352;&#22270;&#20687;&#21644; 200 &#19975;&#20010;&#19981;&#21516;&#30340;&#23454;&#20307;&#65292;&#28085;&#30422;&#20102;&#37326;&#22806;&#20016;&#23500;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;&#22522;&#20110; I2E &#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#21253;&#25324;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#12289;&#23545;&#27604;&#24230;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web. Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.15852</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#65306;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#24187;&#24819;&#30340;&#25991;&#26412;&#12290;&#33258;&#30456;&#30683;&#30462;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24187;&#35273;&#24418;&#24335;&#65292;&#25351;&#30340;&#26159;&#35821;&#35328;&#27169;&#22411;&#22312;&#21516;&#19968;&#35821;&#22659;&#20013;&#29983;&#25104;&#20004;&#20010;&#30683;&#30462;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#38024;&#23545;&#26368;&#20808;&#36827;&#12289;&#32463;&#36807;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#33258;&#30456;&#30683;&#30462;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12289;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26377;&#25928;&#22320;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#33879;&#21517;&#30340;&#36824;&#26159;&#19981;&#22826;&#20986;&#21517;&#30340;&#35805;&#39064;&#65292;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#30456;&#30683;&#30462;&#37117;&#32463;&#24120;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (large LMs) are susceptible to producing text with hallucinated content. Self-contradiction, where the LM generates two contradictory sentences within the same context, is an important form of hallucination. In this work, we present a comprehensive analysis on self-contradiction for state-of-the-art, instruction-tuned LMs, including evaluation, detection, and mitigation. To effectively trigger self-contradictions, we design a framework that constrains LMs to generate appropriate sentence pairs. Our evaluation on these sentence pairs reveals that self-contradictions occur frequently across different LMs for both famous and lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our results indicate that ChatGPT and GPT-4 are able to accurately identify self-contradictions, while Vicuna-13B struggles to do so. For example, with our best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the sentence pairs generated by itself. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#22312;&#28151;&#21512;&#35821;&#35328;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#25216;&#26415;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#20219;&#21153;&#65292;&#20316;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;1030&#20010;&#25991;&#26412;&#30340;&#33521;&#35821;&#21644;&#32599;&#39532;&#20044;&#23572;&#28151;&#21512;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.14534</link><description>&lt;p&gt;
&#22312;&#28151;&#21512;&#35821;&#35328;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Detecting Propaganda Techniques in Code-Switched Social Media Text. (arXiv:2305.14534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#22312;&#28151;&#21512;&#35821;&#35328;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#25216;&#26415;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#20219;&#21153;&#65292;&#20316;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;1030&#20010;&#25991;&#26412;&#30340;&#33521;&#35821;&#21644;&#32599;&#39532;&#20044;&#23572;&#28151;&#21512;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23459;&#20256;&#26159;&#19968;&#31181;&#26088;&#22312;&#24433;&#21709;&#20844;&#20247;&#33286;&#35770;&#21644;&#24515;&#24577;&#20197;&#25512;&#24191;&#29305;&#23450;&#35758;&#31243;&#30340;&#27807;&#36890;&#24418;&#24335;&#12290;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#23835;&#36215;&#65292;&#23459;&#20256;&#24050;&#32463;&#36805;&#36895;&#20256;&#25773;&#65292;&#24341;&#21457;&#20102;&#23545;&#33258;&#21160;&#23459;&#20256;&#26816;&#27979;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#22823;&#22810;&#25968;&#23459;&#20256;&#26816;&#27979;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#33521;&#35821;&#65289;&#19978;&#65292;&#20960;&#20046;&#27809;&#26377;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#26816;&#27979;&#23459;&#20256;&#20570;&#20986;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#20132;&#27969;&#20013;&#21457;&#29616;&#22810;&#31181;&#35821;&#35328;&#30340;&#28151;&#21512;&#29616;&#35937;&#26159;&#24456;&#24120;&#35265;&#30340;&#65292;&#36825;&#34987;&#31216;&#20026;&#30721;&#28151;&#12290;&#30721;&#28151;&#22312;&#21516;&#19968;&#25991;&#26412;&#20013;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#65292;&#36825;&#23545;&#20110;&#33258;&#21160;&#31995;&#32479;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#27492;&#25552;&#20986;&#20102;&#26816;&#27979;&#28151;&#21512;&#25991;&#26412;&#20013;&#23459;&#20256;&#25216;&#26415;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;1030&#20010;&#25991;&#26412;&#30340;&#35821;&#26009;&#24211;&#65292;&#36825;&#20123;&#25991;&#26412;&#22312;&#33521;&#35821;&#21644;&#32599;&#39532;&#20044;&#23572;&#37117;&#36827;&#34892;&#20102;&#28151;&#21512;&#65292;&#24182;&#29992;20&#31181;&#23459;&#20256;&#25216;&#24039;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#25105;&#20204;&#24050;&#32463;&#20844;&#24320;&#20102;&#36825;&#20010;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#23545;&#27604;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#29305;&#24449;&#38598;&#21512;&#22312;&#27492;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Propaganda is a form of communication intended to influence the opinions and the mindset of the public to promote a particular agenda. With the rise of social media, propaganda has spread rapidly, leading to the need for automatic propaganda detection systems. Most work on propaganda detection has focused on high-resource languages, such as English, and little effort has been made to detect propaganda for low-resource languages. Yet, it is common to find a mix of multiple languages in social media communication, a phenomenon known as code-switching. Code-switching combines different languages within the same text, which poses a challenge for automatic systems. With this in mind, here we propose the novel task of detecting propaganda techniques in code-switched text. To support this task, we create a corpus of 1,030 texts code-switching between English and Roman Urdu, annotated with 20 propaganda techniques, which we make publicly available. We perform a number of experiments contrastin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11490</link><description>&lt;p&gt;
LLM&#33258;&#36523;&#21487;&#35835;&#21462;&#21644;&#29983;&#25104;CXR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#65292;&#20154;&#20204;&#27491;&#31215;&#26497;&#23581;&#35797;&#23558;LLMs&#30340;&#23454;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36830;&#25509;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#19988;&#20063;&#22312;&#19981;&#26029;&#23581;&#35797;&#20026;LLMs&#28155;&#21152;&#35270;&#35273;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21482;&#20351;&#29992;LLMs&#20316;&#20026;&#22270;&#20687;&#35299;&#30721;&#22120;&#65292;&#27809;&#26377;&#23581;&#35797;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;VQ-GAN&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#34920;&#31034;&#35270;&#20026;&#19968;&#31181;&#25991;&#26412;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#65292;&#20197;&#20687;&#25991;&#26412;&#19968;&#26679;&#35835;&#21462;&#21644;&#29983;&#25104;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#35757;&#32451;&#19987;&#38376;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#20449;&#24687;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#33258;&#21160;&#26631;&#27880;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;&#20154;&#31867;&#26631;&#27880;&#21592;&#12290;</title><link>http://arxiv.org/abs/2305.08339</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#36741;&#21161;&#27880;&#37322;&#36827;&#34892;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#30740;&#31350;&#65306;&#26412;&#22320;&#35821;&#27861;&#20998;&#26512;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis. (arXiv:2305.08339v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#33258;&#21160;&#26631;&#27880;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;&#20154;&#31867;&#26631;&#27880;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;LLMs&#22312;&#21327;&#21161;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#33258;&#21160;&#26631;&#27880;&#20026;&#29305;&#23450;&#35821;&#35328;&#20449;&#24687;&#31867;&#21035;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#30340;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;GPT-3.5&#30340;ChatGPT&#12289;&#22522;&#20110;GPT-4&#30340;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#32534;&#30721;&#22120;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#30528;&#20248;&#20110;ChatGPT&#12290;&#19982;&#20154;&#31867;&#26631;&#27880;&#21592;&#30456;&#27604;&#65292;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25972;&#20307;&#34920;&#29616;&#30053;&#20302;&#20110;&#20154;&#31867;&#26631;&#27880;&#21592;&#30340;&#34920;&#29616;&#65292;&#20294;&#24050;&#32463;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1&#24471;&#20998;:&#36947;&#27465;&#26631;&#35760;99.95&#65285;&#65292;&#21407;&#22240;&#26631;&#35760;91.91&#65285;&#65292;&#36947;&#27465;&#32773;&#26631;&#35760;95.35&#65285;&#65292;&#34987;&#36947;&#27465;&#32773;&#26631;&#35760;89.74&#65285;&#21644;&#21152;&#24378;&#26631;&#35760;96.47&#65285;&#12290;&#36825;&#34920;&#26126;&#65292;&#22312;&#35821;&#35328;&#31867;&#21035;&#28165;&#26224;&#19988;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;LLM&#36741;&#21161;&#27880;&#37322;&#36827;&#34892;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#30740;&#31350;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots based on Large Language Models (LLMs) have shown strong capabilities in language understanding. In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotation of texts with specific categories of linguistic information. Specifically, we examined to what extent LLMs understand the functional elements constituting the speech act of apology from a local grammar perspective, by comparing the performance of ChatGPT (powered by GPT-3.5), the Bing chatbot (powered by GPT-4), and a human coder in the annotation task. The results demonstrate that the Bing chatbot significantly outperformed ChatGPT in the task. Compared to human annotator, the overall performance of the Bing chatbot was slightly less satisfactory. However, it already achieved high F1 scores: 99.95% for the tag of APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for APOLOGISEE, and 96.47% for INTENSIFIER. This suggests that it is feasible to use LLM-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06626</link><description>&lt;p&gt;
&#24403;&#22810;&#25968;&#20154;&#26159;&#38169;&#35823;&#30340;&#65306;&#21033;&#29992;&#26631;&#27880;&#32773;&#19981;&#19968;&#33268;&#24615;&#36827;&#34892;&#20027;&#35266;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#34429;&#28982;&#36890;&#24120;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#22810;&#25968;&#25237;&#31080;&#26469;&#30830;&#23450;&#26631;&#31614;&#65292;&#20294;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#65292;&#26631;&#27880;&#32773;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#21487;&#33021;&#21453;&#26144;&#20986;&#32676;&#20307;&#35266;&#28857;&#30340;&#24046;&#24322;&#65292;&#32780;&#19981;&#26159;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#19968;&#20010;&#35821;&#21477;&#26159;&#21542;&#20882;&#29359;&#20102;&#23427;&#25152;&#38024;&#23545;&#30340;&#20154;&#32676;&#65292;&#32780;&#36825;&#21487;&#33021;&#21482;&#21344;&#26631;&#27880;&#32773;&#27744;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#39044;&#27979;&#21487;&#33021;&#20855;&#26377;&#20882;&#29359;&#24615;&#25991;&#26412;&#19978;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30340;&#39044;&#27979;&#30446;&#26631;&#32676;&#20307;&#26469;&#27169;&#25311;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#25552;&#39640;&#20102;22&#65285;&#22312;&#39044;&#27979;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#19978;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;33&#65285;&#22312;&#39044;&#27979;&#26631;&#27880;&#32773;&#20043;&#38388;&#26041;&#24046;&#19978;&#30340;&#24615;&#33021;&#65292;&#36825;&#25552;&#20379;&#20102;&#19979;&#28216;&#29992;&#26469;&#34913;&#37327;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#20854;&#22312;&#32447;&#24847;&#35265;&#26469;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though majority vote among annotators is typically used for ground truth labels in natural language processing, annotator disagreement in tasks such as hate speech detection may reflect differences among group opinions, not noise. Thus, a crucial problem in hate speech detection is whether a statement is offensive to the demographic group that it targets, which may constitute a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to model the opinions of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22% at predicting individual annotators' ratings and 33% at predicting variance among annotators, which provides a method of measuring model uncertainty downstream. We find that annotators' ratings can be predicted using their demographic information and opinions on online 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20307;&#31995;&#19979;&#20195;&#29702;&#20154;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24314;&#31435;&#20102;&#20840;&#32852;&#37030;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65288;NAW&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#32780;&#22522;&#20110;&#31163;&#32676;&#28857;&#26816;&#27979;&#30340;&#38450;&#24481;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;NAW&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20195;&#29702;&#20154;&#23398;&#20064;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14769</link><description>&lt;p&gt;
&#21521;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65311;&#24314;&#31435;&#25308;&#21344;&#24237;&#40065;&#26834;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#20307;&#31995;&#19979;&#30340;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied Agents under Federated Learning. (arXiv:2211.14769v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20307;&#31995;&#19979;&#20195;&#29702;&#20154;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24314;&#31435;&#20102;&#20840;&#32852;&#37030;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65288;NAW&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#32780;&#22522;&#20110;&#31163;&#32676;&#28857;&#26816;&#27979;&#30340;&#38450;&#24481;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;NAW&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20195;&#29702;&#20154;&#23398;&#20064;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#21270;&#20307;&#31995;&#19979;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#36890;&#36807;&#22312;&#26412;&#22320;&#23458;&#25143;&#31471;&#65288;&#21363;&#19981;&#21516;&#29615;&#22659;&#65289;&#20013;&#20445;&#25345;&#25968;&#25454;&#26469;&#20445;&#25252;&#20010;&#20154;&#35270;&#35273;&#29615;&#22659;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#19979;&#65292;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#23545;&#26381;&#21153;&#22120;&#26159;&#19981;&#21487;&#35775;&#38382;&#30340;&#65292;&#25915;&#20987;&#32773;&#21487;&#33021;&#36731;&#26131;&#22320;&#27745;&#26579;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#22312;&#19981;&#34987;&#36890;&#30693;&#30340;&#24773;&#20917;&#19979;&#22312;&#20195;&#29702;&#20154;&#20013;&#24314;&#31435;&#21518;&#38376;&#12290;&#20351;&#29992;&#36825;&#26679;&#30340;&#20195;&#29702;&#20154;&#20250;&#23545;&#20154;&#31867;&#26500;&#25104;&#28508;&#22312;&#21361;&#23475;&#65292;&#22240;&#20026;&#25915;&#20987;&#32773;&#21487;&#20197;&#36731;&#26494;&#22320;&#36890;&#36807;&#21518;&#38376;&#25805;&#32437;&#20195;&#29702;&#20154;&#36827;&#34892;&#23548;&#33322;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#23454;&#29616;&#20840;&#32852;&#37030;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20219;&#21153;&#20013;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#38656;&#35201;&#36319;&#38543;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#23548;&#33322;&#23460;&#20869;&#29615;&#22659;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21363;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65288;NAW&#65289;&#65292;&#20854;&#20013;&#24694;&#24847;&#23458;&#25143;&#31471;&#36890;&#36807;&#25805;&#32437;&#26412;&#22320;&#36712;&#36857;&#25968;&#25454;&#26469;&#21521;&#20840;&#23616;&#27169;&#22411;&#26893;&#20837;&#21518;&#38376;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;NAW&#21487;&#20197;&#23454;&#29616;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#32780;&#19988;&#24615;&#33021;&#19979;&#38477;&#24494;&#19981;&#36275;&#36947;&#12290;&#20026;&#20102;&#38450;&#27490;NAW&#25915;&#20987;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#24481;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31163;&#32676;&#28857;&#26816;&#27979;&#30340;&#27010;&#24565;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#12290;&#25105;&#20204;&#22312;VLN&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;NAW&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#32852;&#37030;&#21270;&#20307;&#31995;&#19979;&#20195;&#29702;&#20154;&#23398;&#20064;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated embodied agent learning protects the data privacy of individual visual environments by keeping data locally at each client (the individual environment) during training. However, since the local data is inaccessible to the server under federated learning, attackers may easily poison the training data of the local client to build a backdoor in the agent without notice. Deploying such an agent raises the risk of potential harm to humans, as the attackers may easily navigate and control the agent as they wish via the backdoor. Towards Byzantine-robust federated embodied agent learning, in this paper, we study the attack and defense for the task of vision-and-language navigation (VLN), where the agent is required to follow natural language instructions to navigate indoor environments. First, we introduce a simple but effective attack strategy, Navigation as Wish (NAW), in which the malicious client manipulates local trajectory data to implant a backdoor into the global model. Resu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#20851;&#20110;&#26234;&#33021;&#12289;&#20154;&#31867;&#35821;&#35328;&#21644;&#20154;&#31867;&#25968;&#23398;&#30340;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#20154;&#31867;&#35821;&#35328;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#33021;&#21542;&#23545;&#25105;&#20204;&#26080;&#27861;&#24819;&#35937;&#30340;&#20107;&#29289;&#26377;&#20219;&#20309;&#20102;&#35299;&#12290;</title><link>http://arxiv.org/abs/2208.03886</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#20102;&#35299;&#29978;&#33267;&#26080;&#27861;&#24819;&#35937;&#30340;&#20107;&#29289;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
What can we know about that which we cannot even imagine?. (arXiv:2208.03886v3 [physics.hist-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03886
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#20851;&#20110;&#26234;&#33021;&#12289;&#20154;&#31867;&#35821;&#35328;&#21644;&#20154;&#31867;&#25968;&#23398;&#30340;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#20154;&#31867;&#35821;&#35328;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#33021;&#21542;&#23545;&#25105;&#20204;&#26080;&#27861;&#24819;&#35937;&#30340;&#20107;&#29289;&#26377;&#20219;&#20309;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#23558;&#32771;&#34385;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#38382;&#39064;&#28041;&#21450;&#21040;&#26234;&#33021;&#30340;&#29983;&#29289;&#23398;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#35748;&#30693;&#20041;&#32930;&#12290;&#36825;&#23558;&#24341;&#20986;&#20851;&#20110;&#20154;&#31867;&#35821;&#35328;&#30340;&#38382;&#39064;&#65292;&#20063;&#35768;&#26159;&#20154;&#31867;&#36804;&#20170;&#20026;&#27490;&#24320;&#21457;&#30340;&#26368;&#37325;&#35201;&#30340;&#35748;&#30693;&#20041;&#32930;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#23545;&#20154;&#31867;&#35821;&#35328;&#25152;&#21253;&#21547;&#30340;&#35748;&#30693;&#33021;&#21147;&#36827;&#34892;&#36190;&#32654;&#65292;&#20294;&#25105;&#23558;&#24378;&#35843;&#20154;&#31867;&#35821;&#35328;&#22810;&#20040;&#26377;&#38480;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#35748;&#30693;&#33021;&#21147;&#20063;&#26159;&#26377;&#38480;&#30340;&#65292;&#23613;&#31649;&#35821;&#35328;&#23545;&#20854;&#36827;&#34892;&#20102;&#22686;&#24378;&#12290;&#36825;&#23558;&#24341;&#20986;&#20851;&#20110;&#20154;&#31867;&#25968;&#23398;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26368;&#32456;&#26159;&#20197;&#20154;&#31867;&#35821;&#35328;&#30340;&#24418;&#24335;&#26469;&#34920;&#36848;&#30340;&#65292;&#25152;&#20197;&#20063;&#23384;&#22312;&#28145;&#23618;&#27425;&#30340;&#38480;&#21046;&#12290;&#28982;&#21518;&#65292;&#25105;&#23558;&#32467;&#21512;&#36825;&#20123;&#38382;&#39064;&#65292;&#23545;&#36825;&#31687;&#25991;&#31456;&#30340;&#26680;&#24515;&#38382;&#39064;&#25552;&#20986;&#19968;&#20010;&#37096;&#20998;&#24615;&#30340;&#12289;&#26377;&#28857;&#20391;&#38754;&#30340;&#31572;&#26696;&#65306;&#25105;&#20204;&#33021;&#22815;&#23545;&#25105;&#20204;&#29978;&#33267;&#26080;&#27861;&#26500;&#24819;&#30340;&#20107;&#29289;&#26377;&#20309;&#20102;&#35299;&#65311;
&lt;/p&gt;
&lt;p&gt;
In this essay I will consider a sequence of questions. The first questions concern the biological function of intelligence in general, and cognitive prostheses of human intelligence in particular. These will lead into questions concerning human language, perhaps the most important cognitive prosthesis humanity has ever developed. While it is traditional to rhapsodize about the cognitive power encapsulated in human language, I will emphasize how horribly limited human language is -- and therefore how limited our cognitive abilities are, despite their being augmented with language. This will lead to questions of whether human mathematics, being ultimately formulated in terms of human language, is also deeply limited. I will then combine these questions to pose a partial, sort-of, sideways answer to the guiding concern of this essay: what we can ever discern about that we cannot even conceive?
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#30417;&#30563;&#22522;&#32447;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#20998;&#26512;&#20102;&#20256;&#32479;&#25163;&#24037;&#29305;&#24449;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2105.00815</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Weakly Supervised Relation Extraction. (arXiv:2105.00815v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.00815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#30417;&#30563;&#22522;&#32447;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#20998;&#26512;&#20102;&#20256;&#32479;&#25163;&#24037;&#29305;&#24449;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20449;&#24687;&#25277;&#21462;&#20197;&#21450;&#20854;&#20013;&#30340;&#23376;&#20219;&#21153;&#20851;&#31995;&#25277;&#21462;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#20851;&#31995;&#25277;&#21462;&#33021;&#22815;&#22312;&#21477;&#23376;&#20013;&#26816;&#27979;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#30446;&#21069;&#65292;&#35768;&#22810;&#39640;&#25928;&#30340;&#26041;&#27861;&#24050;&#34987;&#24212;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#12290;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#23588;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#38590;&#39064;&#12290;&#20854;&#20013;&#26368;&#20005;&#37325;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#25163;&#21160;&#26631;&#35760;&#25968;&#25454;&#38590;&#20197;&#33719;&#21462;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#31561;&#20110;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#21482;&#26377;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20851;&#27880;&#22914;&#20309;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#25105;&#20204;&#30340;&#30417;&#30563;&#22522;&#32447;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#29305;&#24449;&#26159;&#25913;&#36827;&#30417;&#30563;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#25163;&#24037;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#21644;&#26114;&#36149;&#30340;&#20154;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#21487;&#33021;&#20250;&#21463;&#21040;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen rapid development in Information Extraction, as well as its subtask, Relation Extraction. Relation Extraction is able to detect semantic relations between entities in sentences. Currently, many efficient approaches have been applied to relation extraction tasks. Supervised learning approaches especially have good performance. However, there are still many difficult challenges. One of the most serious problems is that manually labeled data is difficult to acquire. In most cases, limited data for supervised approaches equals lousy performance. Thus here, under the situation with only limited training data, we focus on how to improve the performance of our supervised baseline system with unsupervised pre-training. Feature is one of the key components in improving the supervised approaches. Traditional approaches usually apply hand-crafted features, which require expert knowledge and expensive human labor. However, this type of feature might suffer from data sparsity
&lt;/p&gt;</description></item></channel></rss>