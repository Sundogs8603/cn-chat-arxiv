<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>VisoGender&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#32844;&#19994;&#30456;&#20851;&#24615;&#21035;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#32570;&#20047;&#27491;&#30830;&#35299;&#26512;&#22797;&#26434;&#22330;&#26223;&#20013;&#24615;&#21035;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#29983;&#25104;&#23383;&#24149;&#30340;&#27169;&#22411;&#36890;&#24120;&#27604;&#31867;&#20284;CLIP&#30340;&#27169;&#22411;&#26356;&#31934;&#30830;&#21644;&#26356;&#23569;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2306.12424</link><description>&lt;p&gt;
VisoGender&#65306;&#19968;&#20221;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;-&#25991;&#26412;&#20195;&#35789;&#35299;&#26512;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution. (arXiv:2306.12424v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12424
&lt;/p&gt;
&lt;p&gt;
VisoGender&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#32844;&#19994;&#30456;&#20851;&#24615;&#21035;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#32570;&#20047;&#27491;&#30830;&#35299;&#26512;&#22797;&#26434;&#22330;&#26223;&#20013;&#24615;&#21035;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#29983;&#25104;&#23383;&#24149;&#30340;&#27169;&#22411;&#36890;&#24120;&#27604;&#31867;&#20284;CLIP&#30340;&#27169;&#22411;&#26356;&#31934;&#30830;&#21644;&#26356;&#23569;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;VisoGender&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#32844;&#19994;&#30456;&#20851;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#21463;Winograd&#21644;Winogender&#27169;&#24335;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#27599;&#20010;&#22270;&#20687;&#37117;&#19982;&#21253;&#21547;&#22330;&#26223;&#20013;&#20027;&#35821;&#21644;&#23486;&#35821;&#20195;&#35789;&#20851;&#31995;&#30340;&#26631;&#39064;&#30456;&#20851;&#32852;&#12290;VisoGender&#22312;&#32844;&#19994;&#35282;&#33394;&#20013;&#24179;&#34913;&#20102;&#24615;&#21035;&#20195;&#34920;&#65292;&#25903;&#25345;&#20004;&#31181;&#20559;&#35265;&#35780;&#20272;&#26041;&#24335;&#65306;i&#65289;&#35299;&#20915;&#20559;&#35265;&#65292;&#25105;&#20204;&#35780;&#20272;&#30007;&#24615;&#21644;&#22899;&#24615;&#35299;&#20915;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#65307;ii&#65289;&#26816;&#32034;&#20559;&#35265;&#65292;&#25105;&#20204;&#27604;&#36739;&#22312;&#24615;&#21035;&#20013;&#31435;&#30340;&#25628;&#32034;&#26597;&#35810;&#20013;&#26816;&#32034;&#21040;&#30340;&#30007;&#24615;&#21644;&#22899;&#24615;&#19987;&#19994;&#20154;&#21592;&#30340;&#27604;&#20363;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#32570;&#20047;&#27491;&#30830;&#35299;&#26512;&#22797;&#26434;&#22330;&#26223;&#20013;&#24615;&#21035;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#34429;&#28982;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#21521;&#21644;&#24133;&#24230;&#21462;&#20915;&#20110;&#20219;&#21153;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#20294;&#29983;&#25104;&#23383;&#24149;&#30340;&#27169;&#22411;&#36890;&#24120;&#27604;&#31867;&#20284;CLIP&#30340;&#27169;&#22411;&#26356;&#31934;&#30830;&#21644;&#26356;&#23569;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce VisoGender, a novel dataset for benchmarking gender bias in vision-language models. We focus on occupation-related gender biases, inspired by Winograd and Winogender schemas, where each image is associated with a caption containing a pronoun relationship of subjects and objects in the scene. VisoGender is balanced by gender representation in professional roles, supporting bias evaluation in two ways: i) resolution bias, where we evaluate the difference between gender resolution accuracies for men and women and ii) retrieval bias, where we compare ratios of male and female professionals retrieved for a gender-neutral search query. We benchmark several state-of-the-art vision-language models and find that they lack the reasoning abilities to correctly resolve gender in complex scenes. While the direction and magnitude of gender bias depends on the task and the model being evaluated, captioning models generally are more accurate and less biased than CLIP-like models. Dataset 
&lt;/p&gt;</description></item><item><title>LMFlow&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#36731;&#37327;&#32423;&#30340;&#24037;&#20855;&#21253;&#65292;&#20026;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#23436;&#25972;&#30340;&#24494;&#35843;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#25903;&#25345;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#24182;&#25903;&#25345;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#24494;&#35843;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19987;&#19994;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.12420</link><description>&lt;p&gt;
LMFlow&#65306;&#29992;&#20110;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#21487;&#25193;&#23637;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. (arXiv:2306.12420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12420
&lt;/p&gt;
&lt;p&gt;
LMFlow&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#36731;&#37327;&#32423;&#30340;&#24037;&#20855;&#21253;&#65292;&#20026;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#23436;&#25972;&#30340;&#24494;&#35843;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#25903;&#25345;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#24182;&#25903;&#25345;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#24494;&#35843;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19987;&#19994;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#23637;&#29616;&#20986;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#25509;&#36817;&#20154;&#31867;&#26234;&#33021;&#30340;&#33021;&#21147;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#27169;&#22411;&#22312;&#19987;&#19994;&#20219;&#21153;&#24212;&#29992;&#20013;&#20173;&#28982;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#32570;&#38519;&#65292;&#38656;&#35201;&#24494;&#35843;&#25165;&#33021;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#21487;&#29992;&#27169;&#22411;&#21644;&#19987;&#19994;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36890;&#29992;&#24494;&#35843;&#30340;&#24037;&#20316;&#21464;&#24471;&#38750;&#24120;&#26840;&#25163;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#31532;&#19968;&#27493;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#36731;&#37327;&#32423;&#30340;&#24037;&#20855;&#21253;LMFlow&#65292;&#26088;&#22312;&#31616;&#21270;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#25512;&#29702;&#12290;LMFlow&#20026;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#24494;&#35843;&#24037;&#20316;&#27969;&#31243;&#65292;&#25903;&#25345;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25903;&#25345;&#36830;&#32493;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#31561;&#21151;&#33021;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#19987;&#19994;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large foundation models have demonstrated a great ability to achieve general human-level intelligence far beyond traditional approaches. As the technique keeps attracting attention from the AI community, more and more large foundation models have become publically available. However, most of those models exhibit a major deficiency in specialized-task applications, where the step of finetuning is still required for obtaining satisfactory performance. As the number of available models and specialized tasks keeps growing, the job of general finetuning becomes highly nontrivial. In this paper, we take the first step to address this issue. We introduce an extensible and lightweight toolkit, LMFlow, which aims to simplify the finetuning and inference of general large foundation models. LMFlow offers a complete finetuning workflow for a large foundation model to support personalized training with limited computing resources. Furthermore, it supports continuous pretraining, instruction tuning,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#22522;&#20110;&#23545;&#35805;&#24341;&#23548;&#34892;&#21160;&#20219;&#21153;&#26469;&#22686;&#24378;AI&#31995;&#32479;&#23545;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#29702;&#35299;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#38024;&#23545;Minecraft&#25968;&#25454;&#38598;&#19978;&#38598;&#20307;&#24314;&#31569;&#20219;&#21153;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12387</link><description>&lt;p&gt;
&#37319;&#29992;&#36827;&#19968;&#27493;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#20915;&#27169;&#25311;&#29615;&#22659;&#19979;&#23545;&#35805;&#24341;&#23548;&#34892;&#21160;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Solving Dialogue Grounding Embodied Task in a Simulated Environment using Further Masked Language Modeling. (arXiv:2306.12387v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#22522;&#20110;&#23545;&#35805;&#24341;&#23548;&#34892;&#21160;&#20219;&#21153;&#26469;&#22686;&#24378;AI&#31995;&#32479;&#23545;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#29702;&#35299;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#38024;&#23545;Minecraft&#25968;&#25454;&#38598;&#19978;&#38598;&#20307;&#24314;&#31569;&#20219;&#21153;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#22320;&#36741;&#21161;&#20154;&#31867;&#29992;&#25143;&#65292;&#22686;&#24378;AI&#31995;&#32479;&#30340;&#26377;&#25928;&#27807;&#36890;&#25216;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#38656;&#35201;&#20174;&#31995;&#32479;&#26041;&#38754;&#37319;&#21462;&#31215;&#26497;&#30340;&#25514;&#26045;&#26469;&#36776;&#21035;&#29305;&#23450;&#24773;&#20917;&#24182;&#19982;&#29992;&#25143;&#24688;&#24403;&#22320;&#20114;&#21160;&#20197;&#35299;&#20915;&#36825;&#20123;&#22330;&#26223;&#12290;&#26412;&#30740;&#31350;&#36873;&#25321;Minecraft&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#20010;&#38598;&#20307;&#24314;&#31569;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#35821;&#35328;&#24314;&#27169;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#25552;&#39640;&#20219;&#21153;&#29702;&#35299;&#65292;&#36825;&#20123;&#27169;&#22411;&#19987;&#27880;&#20110;&#22522;&#20110;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#12290;&#36825;&#31181;&#19987;&#27880;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#35299;&#37322;&#21644;&#21709;&#24212;&#21508;&#31181;&#36755;&#20837;&#21644;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#25552;&#20379;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21331;&#36234;&#24615;&#30340;&#26377;&#21147;&#35777;&#25454;&#12290;&#36825;&#23637;&#31034;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#65292;&#24182;&#25351;&#21521;&#20102;&#26410;&#26469;&#30740;&#31350;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enhancing AI systems with efficient communication skills that align with human understanding is crucial for their effective assistance to human users. Proactive initiatives from the system side are needed to discern specific circumstances and interact aptly with users to solve these scenarios. In this research, we opt for a collective building assignment taken from the Minecraft dataset. Our proposed method employs language modeling to enhance task understanding through state-of-the-art (SOTA) methods using language models. These models focus on grounding multi-modal understandinging and task-oriented dialogue comprehension tasks. This focus aids in gaining insights into how well these models interpret and respond to a variety of inputs and tasks. Our experimental results provide compelling evidence of the superiority of our proposed method. This showcases a substantial improvement and points towards a promising direction for future research in this domain.
&lt;/p&gt;</description></item><item><title>&#36845;&#20195;&#20998;&#27573;&#20223;&#23556;&#25554;&#20540;&#65288;IPA&#65289;&#36924;&#36817;&#27861;&#21487;&#20197;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#65292;&#19982;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#26550;&#26500;&#31867;&#20284;&#65292;&#24182;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#19979;&#30340;&#23567;&#24207;&#21015;&#38271;&#24230;&#19979;&#20248;&#20110;&#21464;&#21387;&#22120;1.5&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.12317</link><description>&lt;p&gt;
&#36845;&#20195;&#20998;&#27573;&#20223;&#23556;&#25554;&#20540;&#65288;IPA&#65289;&#36924;&#36817;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Iterated Piecewise Affine (IPA) Approximation for Language Modeling. (arXiv:2306.12317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12317
&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#20998;&#27573;&#20223;&#23556;&#25554;&#20540;&#65288;IPA&#65289;&#36924;&#36817;&#27861;&#21487;&#20197;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#65292;&#19982;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#26550;&#26500;&#31867;&#20284;&#65292;&#24182;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#19979;&#30340;&#23567;&#24207;&#21015;&#38271;&#24230;&#19979;&#20248;&#20110;&#21464;&#21387;&#22120;1.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#19968;&#38454;&#27888;&#21202;&#23637;&#24320;&#27861;&#26469;&#36924;&#36817;&#19968;&#20010;&#36890;&#29992;&#30340;&#20989;&#25968;F: R^{n x m} -&gt; R^{n x m} &#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#12290;&#20026;&#20102;&#22686;&#24378;&#22522;&#26412;&#30340;&#27888;&#21202;&#23637;&#24320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;&#21644;&#20998;&#27573;&#24314;&#27169;&#65292;&#20174;&#32780;&#21629;&#21517;&#31639;&#27861;&#20026;&#36845;&#20195;&#20998;&#27573;&#20223;&#23556;&#25554;&#20540;&#65288;IPA&#65289;&#36924;&#36817;&#12290;&#26368;&#32456;&#31639;&#27861;&#34920;&#29616;&#20986;&#19982;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#26550;&#26500;&#30456;&#20284;&#30340;&#26377;&#36259;&#29305;&#24449;&#12290;&#36890;&#36807;&#27604;&#36739;IPA&#21644;&#21464;&#21387;&#22120;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#36739;&#23567;&#30340;&#24207;&#21015;&#38271;&#24230;&#19979;&#65292;IPA&#22312;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#20219;&#21153;&#20013;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#27604;&#21464;&#21387;&#22120;&#39640;1.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we demonstrate the application of a simple first-order Taylor expansion to approximate a generic function $F: R^{n \times m} \to R^{n \times m}$ and utilize it in language modeling. To enhance the basic Taylor expansion, we introduce iteration and piecewise modeling, leading us to name the algorithm the Iterative Piecewise Affine (IPA) approximation. The final algorithm exhibits interesting resemblances to the Transformers decoder architecture. By comparing parameter arrangements in IPA and Transformers, we observe a strikingly similar performance, with IPA outperforming Transformers by 1.5\% in the next token prediction task with cross-entropy loss for smaller sequence lengths.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Web&#25235;&#21462;&#23545;&#21307;&#30103;&#27835;&#30103;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#65292;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#30830;&#23450;&#24739;&#32773;&#26368;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.12310</link><description>&lt;p&gt;
&#36890;&#36807;Web&#25235;&#21462;&#36827;&#34892;&#21307;&#30103;&#27835;&#30103;
&lt;/p&gt;
&lt;p&gt;
Medical ministrations through web scraping. (arXiv:2306.12310v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12310
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Web&#25235;&#21462;&#23545;&#21307;&#30103;&#27835;&#30103;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#65292;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#30830;&#23450;&#24739;&#32773;&#26368;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Web&#25235;&#21462;&#26159;&#19968;&#31181;&#21487;&#20197;&#33258;&#21160;&#20174;&#32593;&#31449;&#20013;&#25552;&#21462;&#25968;&#25454;&#30340;&#25216;&#26415;&#12290;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;Web&#25235;&#21462;&#21487;&#20197;&#29992;&#20110;&#25910;&#38598;&#26377;&#20851;&#21307;&#30103;&#31243;&#24207;&#12289;&#27835;&#30103;&#21644;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#12289;&#30417;&#27979;&#21307;&#30103;&#26381;&#21153;&#36136;&#37327;&#24182;&#30830;&#23450;&#25913;&#36827;&#30340;&#39046;&#22495;&#12290;Web&#25235;&#21462;&#22312;&#21307;&#30103;&#27835;&#30103;&#20013;&#23588;&#20854;&#26377;&#29992;&#65292;&#21487;&#20197;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#30830;&#23450;&#24739;&#32773;&#26368;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Web scraping is a technique that allows us to extract data from websites automatically. in the field of medicine, web scraping can be used to collect information about medical procedures, treatments, and healthcare providers. this information can be used to improve patient care, monitor the quality of healthcare services, and identify areas for improvement. one area where web scraping can be particularly useful is in medical ministrations. medical ministrations are the actions taken to provide medical care to patients, and web scraping can help healthcare providers identify the most effective ministrations for their patients. for example, healthcare providers can use web scraping to collect data about the symptoms and medical histories of their patients, and then use this information to determine the most appropriate ministrations. they can also use web scraping to gather information about the latest medical research and clinical trials, which can help them stay up-to-date with the lat
&lt;/p&gt;</description></item><item><title>SIFTER &#26159;&#19968;&#31181;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#38656;&#27714;&#30340;&#20219;&#21153;&#29305;&#23450;&#23545;&#40784;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2306.12280</link><description>&lt;p&gt;
SIFTER: &#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#23884;&#20837;&#30340;&#20219;&#21153;&#29305;&#23450;&#23545;&#40784;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SIFTER: A Task-specific Alignment Strategy for Enhancing Sentence Embeddings. (arXiv:2306.12280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12280
&lt;/p&gt;
&lt;p&gt;
SIFTER &#26159;&#19968;&#31181;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#38656;&#27714;&#30340;&#20219;&#21153;&#29305;&#23450;&#23545;&#40784;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#21518;&#36827;&#34892;&#24494;&#35843;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#34429;&#28982;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#27867;&#21270;&#30340;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#30340;&#34920;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20219;&#21153;&#20013;&#20173;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#26159;&#22240;&#20026;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#23545;&#21477;&#23376;&#30340;&#25935;&#24863;&#31243;&#24230;&#20063;&#19981;&#21516;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#23884;&#20837;&#30340;&#20219;&#21153;&#29305;&#23450;&#23545;&#40784;&#31574;&#30053; SIFTER&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paradigm of pre-training followed by fine-tuning on downstream tasks has become the mainstream method in natural language processing tasks. Although pre-trained models have the advantage of generalization, their performance may still vary significantly across different domain tasks. This is because the data distribution in different domains varies. For example, the different parts of the sentence 'He married Smt. Dipali Ghosh in 1947 and led a very happy married life' may have different impact for downstream tasks. For similarity calculations, words such as 'led' and 'life' are more important. On the other hand, for sentiment analysis, the word 'happy' is crucial. This indicates that different downstream tasks have different levels of sensitivity to sentence components. Our starting point is to scale information of the model and data according to the specifics of downstream tasks, enhancing domain information of relevant parts for these tasks and reducing irrelevant elements for di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#35299;&#20915;&#21644;&#29983;&#25104; NPR Sunday Puzzles &#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#35768;&#22810; PUZZLEQA &#35868;&#39064;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#27861;&#29983;&#25104;&#35868;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.12255</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#21644;&#29983;&#25104; NPR Sunday Puzzles&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving and Generating NPR Sunday Puzzles with Large Language Models. (arXiv:2306.12255v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#35299;&#20915;&#21644;&#29983;&#25104; NPR Sunday Puzzles &#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#35768;&#22810; PUZZLEQA &#35868;&#39064;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#27861;&#29983;&#25104;&#35868;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992; PUZZLEQA &#25968;&#25454;&#38598;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#21644;&#29983;&#25104; NPR Sunday Puzzle &#28216;&#25103;&#33410;&#30446;&#35868;&#39064;&#30340;&#33021;&#21147;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547; 15 &#24180;&#30340;&#33410;&#30446;&#35868;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312; PUZZLEQA &#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#22810;&#39033;&#36873;&#25321;&#21644;&#33258;&#30001;&#22238;&#31572;&#26684;&#24335;&#65292;&#24182;&#25506;&#35752;&#20102;&#20004;&#31181;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#20197;&#25552;&#39640;&#33258;&#30001;&#22238;&#31572;&#34920;&#29616;&#65306;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#25552;&#31034;&#25688;&#35201;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#35768;&#22810; PUZZLEQA &#35868;&#39064;&#65306;&#26368;&#22909;&#30340;&#27169;&#22411; GPT-3.5 &#36798;&#21040;&#20102; 50.2% &#30340;&#26494;&#25955;&#20934;&#30830;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#30340;&#23569;&#37327;&#26679;&#26412;&#35868;&#39064;&#29983;&#25104;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#26080;&#27861;&#29983;&#25104;&#35868;&#39064;&#65306;GPT-3.5 &#29983;&#25104;&#30340;&#35868;&#39064;&#31572;&#26696;&#19981;&#31526;&#21512;&#29983;&#25104;&#30340;&#35268;&#21017;&#12290;&#35868;&#39064;&#29983;&#25104;&#20173;&#28982;&#26159;&#26410;&#26469;&#24037;&#20316;&#30340;&#19968;&#39033;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the ability of large language models to solve and generate puzzles from the NPR Sunday Puzzle game show using PUZZLEQA, a dataset comprising 15 years of on-air puzzles. We evaluate four large language models using PUZZLEQA, in both multiple choice and free response formats, and explore two prompt engineering techniques to improve free response performance: chain-of-thought reasoning and prompt summarization. We find that state-of-the-art large language models can solve many PUZZLEQA puzzles: the best model, GPT-3.5, achieves 50.2% loose accuracy. However, in our few-shot puzzle generation experiment, we find no evidence that models can generate puzzles: GPT-3.5 generates puzzles with answers that do not conform to the generated rules. Puzzle generation remains a challenging task for future work.
&lt;/p&gt;</description></item><item><title>BEER^2&#26159;&#19968;&#31181;&#29992;&#20110;Retriever&#21644;Reader&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#23398;&#20064;&#65292;&#20849;&#21516;&#36827;&#27493;&#65292;&#23454;&#29616;&#31471;&#21040;&#31471;EL&#12290;</title><link>http://arxiv.org/abs/2306.12245</link><description>&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#30340;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#33539;&#24335;&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking. (arXiv:2306.12245v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12245
&lt;/p&gt;
&lt;p&gt;
BEER^2&#26159;&#19968;&#31181;&#29992;&#20110;Retriever&#21644;Reader&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#23398;&#20064;&#65292;&#20849;&#21516;&#36827;&#27493;&#65292;&#23454;&#29616;&#31471;&#21040;&#31471;EL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26159;&#20449;&#24687;&#25552;&#21462;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#23427;&#30340;&#19968;&#33324;&#24418;&#24335;&#65288;&#21363;&#31471;&#21040;&#31471;EL&#65289;&#26088;&#22312;&#39318;&#20808;&#22312;&#32473;&#23450;&#36755;&#20837;&#25991;&#26723;&#20013;&#25214;&#21040;&#25552;&#21450;&#65292;&#24182;&#23558;&#25552;&#21450;&#38142;&#25509;&#21040;&#29305;&#23450;&#30693;&#35782;&#24211;&#20013;&#30340;&#30456;&#24212;&#23454;&#20307;&#12290;&#26368;&#36817;&#65292;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#33539;&#24335;&#20419;&#36827;&#20102;&#31471;&#21040;&#31471;EL&#30340;&#36827;&#23637;&#65292;&#21463;&#30410;&#20110;&#23494;&#38598;&#30340;&#23454;&#20307;&#26816;&#32034;&#21644;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20165;&#20197;&#27969;&#27700;&#32447;&#26041;&#24335;&#21333;&#29420;&#35757;&#32451;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#65292;&#24573;&#30053;&#20102;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20043;&#38388;&#20132;&#20114;&#24102;&#26469;&#30340;&#30410;&#22788;&#12290;&#20026;&#20102;&#20351;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#33539;&#24335;&#26356;&#23436;&#32654;&#22320;&#25191;&#34892;&#31471;&#21040;&#31471;EL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BEER$^2$&#65292;&#19968;&#31181;&#29992;&#20110;Retriever and Reader&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#12290;&#36890;&#36807;&#25105;&#20204;&#35774;&#35745;&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;BEER$^2$&#25351;&#23548;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20114;&#30456;&#23398;&#20064;&#65292;&#20849;&#21516;&#36827;&#27493;&#65292;&#24182;&#26368;&#32456;&#23454;&#29616;&#31471;&#21040;&#31471;EL&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Linking (EL) is a fundamental task for Information Extraction and Knowledge Graphs. The general form of EL (i.e., end-to-end EL) aims to first find mentions in the given input document and then link the mentions to corresponding entities in a specific knowledge base. Recently, the paradigm of retriever-reader promotes the progress of end-to-end EL, benefiting from the advantages of dense entity retrieval and machine reading comprehension. However, the existing study only trains the retriever and the reader separately in a pipeline manner, which ignores the benefit that the interaction between the retriever and the reader can bring to the task. To advance the retriever-reader paradigm to perform more perfectly on end-to-end EL, we propose BEER$^2$, a Bidirectional End-to-End training framework for Retriever and Reader. Through our designed bidirectional end-to-end training, BEER$^2$ guides the retriever and the reader to learn from each other, make progress together, and ultimate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#23398;&#20064;&#26576;&#20123;&#22522;&#26412;&#30340;&#35821;&#20041;&#23646;&#24615;&#65292;&#21253;&#25324;&#35821;&#20041;&#34164;&#21547;&#21644;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#26080;&#27861;&#23398;&#20064;&#36229;&#20986;&#21487;&#25968;&#38598;&#23618;&#27425;&#32467;&#26500;&#31532;&#19968;&#23618;&#30340;&#27010;&#24565;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#24847;&#20041;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.12213</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits for Learning with Language Models. (arXiv:2306.12213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#23398;&#20064;&#26576;&#20123;&#22522;&#26412;&#30340;&#35821;&#20041;&#23646;&#24615;&#65292;&#21253;&#25324;&#35821;&#20041;&#34164;&#21547;&#21644;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#26080;&#27861;&#23398;&#20064;&#36229;&#20986;&#21487;&#25968;&#38598;&#23618;&#27425;&#32467;&#26500;&#31532;&#19968;&#23618;&#30340;&#27010;&#24565;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#24847;&#20041;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;NLP&#30340;&#36235;&#21183;&#24050;&#32463;&#36716;&#21521;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#35757;&#32451;LLMs&#65292;&#20197;&#35299;&#20915;&#21508;&#31181;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;LLM&#30340;&#25104;&#21151;&#21015;&#34920;&#24456;&#38271;&#65292;&#20063;&#24456;&#22810;&#26679;&#21270;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#26368;&#36817;&#30340;&#20960;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#32463;&#39564;&#24615;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;LLMs&#26410;&#33021;&#25429;&#25417;&#35821;&#35328;&#24847;&#20041;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#26222;&#36941;&#37327;&#21270;&#65292;&#36890;&#36807;&#35777;&#26126;LLMs&#26080;&#27861;&#23398;&#20064;&#26576;&#20123;&#22522;&#26412;&#30340;&#35821;&#20041;&#23646;&#24615;&#65288;&#22914;&#27491;&#24335;&#35821;&#20041;&#20013;&#23450;&#20041;&#30340;&#35821;&#20041;&#34164;&#21547;&#21644;&#19968;&#33268;&#24615;&#65289;&#65292;&#20026;&#36825;&#20123;&#32463;&#39564;&#24615;&#21457;&#29616;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290; &#26356;&#26222;&#36941;&#22320;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#26080;&#27861;&#23398;&#20064;&#36229;&#20986;&#21487;&#25968;&#38598;&#23618;&#27425;&#32467;&#26500;&#65288;Borel Hierarchy&#65289;&#31532;&#19968;&#23618;&#30340;&#27010;&#24565;&#65292;&#36825;&#23545;&#20110;LLMs&#30340;&#33021;&#21147;&#26045;&#21152;&#20102;&#20005;&#26684;&#38480;&#21046;&#65292;&#26080;&#35770;&#20854;&#22823;&#23567;&#65292;&#37117;&#26080;&#27861;&#25429;&#25417;&#35768;&#22810;&#35821;&#35328;&#24847;&#20041;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;LLMs&#23558;&#32487;&#32493;&#22312;&#38656;&#35201;&#34164;&#21547;&#21644;&#28145;&#21051;&#35821;&#35328;&#29702;&#35299;&#30340;&#20219;&#21153;&#20013;&#27809;&#26377;&#27491;&#24335;&#20445;&#35777;&#22320;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of large language models (LLMs), the trend in NLP has been to train LLMs on vast amounts of data to solve diverse language understanding and generation tasks. The list of LLM successes is long and varied. Nevertheless, several recent papers provide empirical evidence that LLMs fail to capture important aspects of linguistic meaning. Focusing on universal quantification, we provide a theoretical foundation for these empirical findings by proving that LLMs cannot learn certain fundamental semantic properties including semantic entailment and consistency as they are defined in formal semantics. More generally, we show that LLMs are unable to learn concepts beyond the first level of the Borel Hierarchy, which imposes severe limits on the ability of LMs, both large and small, to capture many aspects of linguistic meaning. This means that LLMs will continue to operate without formal guarantees on tasks that require entailments and deep linguistic understanding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#26159;&#23454;&#29616;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.12205</link><description>&lt;p&gt;
&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#36808;&#21521;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI. (arXiv:2306.12205v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#26159;&#23454;&#29616;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#24494;&#35843;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#24403;&#27169;&#22411;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#24076;&#26395;&#20854;&#33021;&#33719;&#24471;&#38544;&#24335;&#30693;&#35782;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#27867;&#21270;&#21040;&#19981;&#21516;&#38750;&#35821;&#35328;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#20219;&#21153;&#65292;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20998;&#23618;&#25968;&#25454;&#25512;&#29702;&#21644;&#34507;&#30333;&#36136;&#25240;&#21472;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#22235;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;T5&#12289;BART&#12289;BERT &#21644; GPT-2 &#37117;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25104;&#32489;&#12290;&#23427;&#20204;&#30340;&#34920;&#29616;&#24456;&#30456;&#20284;&#65292;&#19988;&#23427;&#20204;&#30340;&#34920;&#29616;&#27604;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340; transformers &#35201;&#22909;&#24471;&#22810;&#12290;&#20363;&#22914;&#65292;&#22312; Listops &#25968;&#25454;&#38598;&#19978;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026; 58.7&#65285;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340; transformers &#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026; 29.0&#65285;&#12290;&#36328;&#19977;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#25152;&#23637;&#31034;&#30340;&#26174;&#33879;&#25913;&#36827;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26159;&#23454;&#29616;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26377;&#21069;&#36884;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have recently emerged as a powerful tool for fine-tuning a variety of language tasks. Ideally, when models are pre-trained on large amount of data, they are expected to gain implicit knowledge. In this paper, we investigate the ability of pre-trained language models to generalize to different non-language tasks. In particular, we test them on tasks from different domains such as computer vision, reasoning on hierarchical data, and protein fold prediction. The four pre-trained models that we used, T5, BART, BERT, and GPT-2 achieve outstanding results. They all have similar performance and they outperform transformers that are trained from scratch by a large margin. For instance, pre-trained language models perform better on the Listops dataset, with an average accuracy of 58.7\%, compared to transformers trained from scratch, which have an average accuracy of 29.0\%. The significant improvement demonstrated across three types of datasets suggests that pre-tra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#20855;&#20307;&#20351;&#29992;&#32422;&#26463;&#31639;&#26415;&#38382;&#39064;&#25506;&#32034;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#20998;&#25968;&#21644;&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#21457;&#29616;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#35813;&#27169;&#22411;&#20197;&#30053;&#24494;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#35299;&#20915;&#20998;&#23618;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35299;&#20915;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.12198</link><description>&lt;p&gt;
&#25581;&#24320;&#40657;&#21283;&#23376;&#65306;&#20998;&#26512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks. (arXiv:2306.12198v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#20855;&#20307;&#20351;&#29992;&#32422;&#26463;&#31639;&#26415;&#38382;&#39064;&#25506;&#32034;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#20998;&#25968;&#21644;&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#21457;&#29616;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#35813;&#27169;&#22411;&#20197;&#30053;&#24494;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#35299;&#20915;&#20998;&#23618;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35299;&#20915;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22810;&#25968;&#20808;&#36827;&#27169;&#22411;&#30340;&#8220;&#40657;&#21283;&#23376;&#8221;&#29305;&#24615;&#65292;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#38543;&#30528;&#22522;&#20110;transformers&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#36827;&#23637;&#21450;&#20854;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#19981;&#26029;&#38598;&#25104;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#32039;&#36843;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;AI&#27169;&#22411;&#65292;&#24517;&#39035;&#29702;&#35299;&#28041;&#21450;&#30340;&#36807;&#31243;&#27493;&#39588;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#36827;&#34892;&#27604;&#36739;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31616;&#21333;&#26131;&#25026;&#30340;&#38750;&#35821;&#35328;&#20219;&#21153;&#26469;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#37096;&#36816;&#20316;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20855;&#26377;&#20998;&#23618;&#32467;&#26500;&#30340;&#32422;&#26463;&#31639;&#26415;&#38382;&#39064;&#65292;&#20197;&#20998;&#26512;&#20854;&#27880;&#24847;&#21147;&#26435;&#37325;&#20998;&#25968;&#21644;&#38544;&#34255;&#29366;&#24577;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#27169;&#22411;&#20197;&#30053;&#24494;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#35299;&#20915;&#20998;&#23618;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35299;&#20915;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investigating deep learning language models has always been a significant research area due to the ``black box" nature of most advanced models. With the recent advancements in pre-trained language models based on transformers and their increasing integration into daily life, addressing this issue has become more pressing. In order to achieve an explainable AI model, it is essential to comprehend the procedural steps involved and compare them with human thought processes. Thus, in this paper, we use simple, well-understood non-language tasks to explore these models' inner workings. Specifically, we apply a pre-trained language model to constrained arithmetic problems with hierarchical structure, to analyze their attention weight scores and hidden states. The investigation reveals promising results, with the model addressing hierarchical problems in a moderately structured manner, similar to human problem-solving strategies. Additionally, by inspecting the attention weights layer by laye
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#29305;&#24449;&#20132;&#20114;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28784;&#31665;&#26041;&#27861;&#26469;&#20998;&#26512;&#20132;&#20114;&#29305;&#24449;&#30340;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#26576;&#20123;&#26041;&#27861;&#33021;&#22815;&#21453;&#26144;&#30446;&#26631;&#27169;&#22411;&#30340;&#20869;&#37096;&#36816;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.12181</link><description>&lt;p&gt;
&#29305;&#24449;&#20132;&#20114;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Feature Interactions Reveal Linguistic Structure in Language Models. (arXiv:2306.12181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12181
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#29305;&#24449;&#20132;&#20114;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28784;&#31665;&#26041;&#27861;&#26469;&#20998;&#26512;&#20132;&#20114;&#29305;&#24449;&#30340;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#26576;&#20123;&#26041;&#27861;&#33021;&#22815;&#21453;&#26144;&#30446;&#26631;&#27169;&#22411;&#30340;&#20869;&#37096;&#36816;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29305;&#24449;&#25351;&#27966;&#26041;&#27861;&#30340;&#29305;&#24449;&#20132;&#20114;&#12290;&#22312;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20013;&#65292;&#29702;&#35299;&#29305;&#24449;&#20132;&#20114;&#36234;&#26469;&#36234;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20132;&#20114;&#29305;&#24449;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#24449;&#20132;&#20114;&#35753;&#19968;&#20010;&#27169;&#22411;&#20026;&#20854;&#36755;&#20837;&#24314;&#31435;&#23618;&#27425;&#32467;&#26500;&#34920;&#31034;&#65292;&#24182;&#21487;&#33021;&#25104;&#20026;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#20013;&#35821;&#35328;&#32467;&#26500;&#30340;&#29702;&#24819;&#36215;&#28857;&#12290;&#28982;&#32780;&#65292;&#25581;&#31034;&#36825;&#20123;&#20132;&#20114;&#31934;&#30830;&#30340;&#20316;&#29992;&#20063;&#26159;&#22256;&#38590;&#30340;&#65292;&#32780;&#19988;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20132;&#20114;&#25351;&#27966;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#38382;&#39064;&#26159;&#21738;&#31181;&#26041;&#27861;&#26368;&#24544;&#23454;&#22320;&#21453;&#26144;&#30446;&#26631;&#27169;&#22411;&#30340;&#20869;&#37096;&#36816;&#20316;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#28784;&#31665;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;PCFGs&#22312;&#27491;&#24335;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#19978;&#23436;&#32654;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#29305;&#23450;&#30340;&#37197;&#32622;&#19979;&#65292;&#19968;&#20123;&#26041;&#27861;&#30830;&#23454;&#33021;&#22815;&#25581;&#31034;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study feature interactions in the context of feature attribution methods for post-hoc interpretability. In interpretability research, getting to grips with feature interactions is increasingly recognised as an important challenge, because interacting features are key to the success of neural networks. Feature interactions allow a model to build up hierarchical representations for its input, and might provide an ideal starting point for the investigation into linguistic structure in language models. However, uncovering the exact role that these interactions play is also difficult, and a diverse range of interaction attribution methods has been proposed. In this paper, we focus on the question which of these methods most faithfully reflects the inner workings of the target models. We work out a grey box methodology, in which we train models to perfection on a formal language classification task, using PCFGs. We show that under specific configurations, some methods are indeed able to u
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#26041;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;&#26174;&#24335;&#35821;&#38899;&#20998;&#31163;&#21644;&#30452;&#25509;&#22312;ASR&#27169;&#22359;&#20013;&#21512;&#24182;&#28151;&#21512;&#35821;&#38899;&#20449;&#24687;&#65292;&#36890;&#36807;&#20132;&#25442;&#36328;&#35828;&#35805;&#32773;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#23618;&#65292;&#23454;&#29616;&#20102;&#22312;SMS-WSJ&#20219;&#21153;&#19978;&#30456;&#23545;&#20110;&#32431;&#27169;&#22359;&#21270;&#35774;&#32622;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;7%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.12173</link><description>&lt;p&gt;
&#32852;&#21512;&#35821;&#38899;&#20998;&#31163;&#19982;&#35782;&#21035;&#30340;&#28151;&#21512;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mixture Encoder for Joint Speech Separation and Recognition. (arXiv:2306.12173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#26041;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;&#26174;&#24335;&#35821;&#38899;&#20998;&#31163;&#21644;&#30452;&#25509;&#22312;ASR&#27169;&#22359;&#20013;&#21512;&#24182;&#28151;&#21512;&#35821;&#38899;&#20449;&#24687;&#65292;&#36890;&#36807;&#20132;&#25442;&#36328;&#35828;&#35805;&#32773;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#23618;&#65292;&#23454;&#29616;&#20102;&#22312;SMS-WSJ&#20219;&#21153;&#19978;&#30456;&#23545;&#20110;&#32431;&#27169;&#22359;&#21270;&#35774;&#32622;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;7%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35828;&#35805;&#20154;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#38656;&#35201;&#29305;&#23450;&#30340;&#24314;&#27169;&#25216;&#26415;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#27169;&#22359;&#21270;&#21644;&#31471;&#21040;&#31471;&#26041;&#27861;&#12290;&#27169;&#22359;&#21270;&#26041;&#27861;&#20351;&#29992;&#21333;&#35828;&#35805;&#20154;ASR&#31995;&#32479;&#20998;&#31163;&#35828;&#35805;&#20154;&#24182;&#35782;&#21035;&#20182;&#20204;&#12290;&#31471;&#21040;&#31471;&#27169;&#22411;&#30452;&#25509;&#22312;&#19968;&#20010;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#22788;&#29702;&#37325;&#21472;&#30340;&#35821;&#38899;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20171;&#20110;&#20004;&#32773;&#20043;&#38388;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31867;&#20284;&#20110;&#27169;&#22359;&#21270;&#26041;&#27861;&#30340;&#26174;&#24335;&#35821;&#38899;&#20998;&#31163;&#65292;&#20294;&#20063;&#30452;&#25509;&#22312;ASR&#27169;&#22359;&#20013;&#21512;&#24182;&#28151;&#21512;&#35821;&#38899;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#35821;&#38899;&#20998;&#31163;&#22120;&#36896;&#25104;&#30340;&#38169;&#35823;&#20256;&#25773;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#20010;&#20307;&#35828;&#35805;&#20154;&#20449;&#24687;&#30340;&#23618;&#26469;&#20132;&#25442;&#36328;&#35828;&#35805;&#32773;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#36890;&#36807;&#21333;&#29420;&#21644;&#32852;&#21512;&#35757;&#32451;&#38454;&#27573;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;SMS-WSJ&#20219;&#21153;&#19978;&#30456;&#23545;&#20110;&#32431;&#27169;&#22359;&#21270;&#35774;&#32622;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#23454;&#29616;&#20102;7%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-speaker automatic speech recognition (ASR) is crucial for many real-world applications, but it requires dedicated modeling techniques. Existing approaches can be divided into modular and end-to-end methods. Modular approaches separate speakers and recognize each of them with a single-speaker ASR system. End-to-end models process overlapped speech directly in a single, powerful neural network. This work proposes a middle-ground approach that leverages explicit speech separation similarly to the modular approach but also incorporates mixture speech information directly into the ASR module in order to mitigate the propagation of errors made by the speech separator. We also explore a way to exchange cross-speaker context information through a layer that combines information of the individual speakers. Our system is optimized through separate and joint training stages and achieves a relative improvement of 7% in word error rate over a purely modular setup on the SMS-WSJ task.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;NLI&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#20154;&#26426;&#20132;&#20114;&#30340;&#20202;&#34920;&#26495;&#65292;&#21457;&#29616;&#20102;&#20960;&#31867;&#20266;&#30456;&#20851;&#24615;&#24182;&#23558;&#20854;&#20998;&#20026;&#19977;&#31867;&#65306;&#35821;&#20041;&#30456;&#20851;&#24615;&#12289;&#36923;&#36753;&#35884;&#35823;&#21644;&#20559;&#35265;&#65292;&#20854;&#20013;&#21253;&#25324;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#21644;&#21019;&#24314;&#23545;&#25239;&#27979;&#35797;&#22871;&#20214;&#26469;&#35780;&#20272;NLI&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12146</link><description>&lt;p&gt;
&#21738;&#20123;&#20266;&#30456;&#20851;&#23545;NLI&#27169;&#22411;&#30340;&#25512;&#29702;&#20135;&#29983;&#24433;&#21709;&#65311;&#22522;&#20110;&#25968;&#25454;&#38480;&#21046;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#35270;&#35273;&#20132;&#20114;&#24335;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals. (arXiv:2306.12146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12146
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;NLI&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#20154;&#26426;&#20132;&#20114;&#30340;&#20202;&#34920;&#26495;&#65292;&#21457;&#29616;&#20102;&#20960;&#31867;&#20266;&#30456;&#20851;&#24615;&#24182;&#23558;&#20854;&#20998;&#20026;&#19977;&#31867;&#65306;&#35821;&#20041;&#30456;&#20851;&#24615;&#12289;&#36923;&#36753;&#35884;&#35823;&#21644;&#20559;&#35265;&#65292;&#20854;&#20013;&#21253;&#25324;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#21644;&#21019;&#24314;&#23545;&#25239;&#27979;&#35797;&#22871;&#20214;&#26469;&#35780;&#20272;NLI&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#26426;&#20132;&#20114;&#30340;&#20202;&#34920;&#26495;&#65292;&#29992;&#20110;&#35786;&#26029;NLI&#27169;&#22411;&#20381;&#36182;&#20110;&#36827;&#34892;&#39044;&#27979;&#30340;&#28508;&#22312;&#20266;&#29305;&#24449;&#12290;&#35813;&#20202;&#34920;&#26495;&#20351;&#29992;&#25143;&#33021;&#22815;&#36890;&#36807;&#21463;GPT-3&#24314;&#35758;&#21551;&#21457;&#32780;&#29983;&#25104;&#19981;&#21516;&#30340;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20363;&#23376;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#21487;&#20197;&#20174;&#32463;&#36807;&#35757;&#32451;&#30340;NLI&#27169;&#22411;&#20013;&#33719;&#24471;&#26377;&#20851;&#26032;&#29983;&#25104;&#30340;&#31034;&#20363;&#30340;&#25361;&#25112;&#24615;&#30340;&#21453;&#39304;&#65292;&#24182;&#26681;&#25454;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#35843;&#26597;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20960;&#31867;&#24433;&#21709;NLI&#27169;&#22411;&#25512;&#29702;&#30340;&#20266;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#19977;&#31867;&#65306;&#35821;&#20041;&#30456;&#20851;&#24615;&#12289;&#36923;&#36753;&#35884;&#35823;&#21644;&#20559;&#35265;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#30830;&#23450;&#21644;&#25551;&#36848;&#20102;&#21508;&#31181;&#30740;&#31350;&#26426;&#20250;&#65292;&#21253;&#25324;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#21644;&#21019;&#24314;&#23545;&#25239;&#27979;&#35797;&#22871;&#20214;&#26469;&#35780;&#20272;NLI&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a human-in-the-loop dashboard tailored to diagnosing potential spurious features that NLI models rely on for predictions. The dashboard enables users to generate diverse and challenging examples by drawing inspiration from GPT-3 suggestions. Additionally, users can receive feedback from a trained NLI model on how challenging the newly created example is and make refinements based on the feedback. Through our investigation, we discover several categories of spurious correlations that impact the reasoning of NLI models, which we group into three categories: Semantic Relevance, Logical Fallacies, and Bias. Based on our findings, we identify and describe various research opportunities, including diversifying training data and assessing NLI models' robustness by creating adversarial test suites.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;MultiMon&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#22810;&#27169;&#24577;&#31995;&#32479;&#20013;&#30340;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#25581;&#31034;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;14&#20010;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#27599;&#20010;&#37117;&#30001;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#36755;&#20837;&#32452;&#25104;&#65292;&#36825;&#20123;&#36755;&#20837;&#20250;&#23548;&#33268;&#20854;&#20182;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2306.12105</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#25209;&#37327;&#29983;&#20135;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Mass-Producing Failures of Multimodal Systems with Language Models. (arXiv:2306.12105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;MultiMon&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#22810;&#27169;&#24577;&#31995;&#32479;&#20013;&#30340;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#25581;&#31034;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;14&#20010;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#27599;&#20010;&#37117;&#30001;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#36755;&#20837;&#32452;&#25104;&#65292;&#36825;&#20123;&#36755;&#20837;&#20250;&#23548;&#33268;&#20854;&#20182;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#21487;&#33021;&#20197;&#35780;&#20272;&#20154;&#21592;&#26410;&#26366;&#39044;&#35265;&#30340;&#26041;&#24335;&#22833;&#36133;&#12290;&#20026;&#20102;&#22312;&#37096;&#32626;&#21069;&#25214;&#21040;&#36825;&#20123;&#22833;&#36133;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MultiMon&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;&#24615;&#22833;&#36133;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#25552;&#20379;&#21487;&#25512;&#24191;&#30340;&#12289;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27169;&#22411;&#22833;&#36133;&#27169;&#24335;&#30340;&#20363;&#23376;&#12290;&#20026;&#20102;&#25581;&#31034;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;MultiMon&#20174;&#35821;&#26009;&#24211;&#20013;&#25235;&#21462;&#38169;&#35823;&#21327;&#35758;&#30340;&#31034;&#20363;&#65306;&#36755;&#20837;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65292;&#20294;&#19981;&#24212;&#35813;&#22914;&#27492;&#12290;&#28982;&#21518;&#23427;&#20250;&#28608;&#27963;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-4&#65289;&#26469;&#26597;&#25214;&#31995;&#32479;&#24615;&#22833;&#36133;&#30340;&#27169;&#24335;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#23427;&#20204;&#12290;&#25105;&#20204;&#20351;&#29992;MultiMon&#25214;&#21040;&#20102;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;14&#20010;&#31995;&#32479;&#24615;&#22833;&#36133;&#65288;&#20363;&#22914;&#8220;&#24573;&#30053;&#37327;&#35789;&#8221;&#65292;&#27599;&#20010;&#37117;&#30001;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#36755;&#20837;&#32452;&#25104;&#65288;&#20363;&#22914;&#8220;&#19968;&#20010;&#24102;&#26377;&#19968;&#20123;/&#35768;&#22810;&#20070;&#30340;&#20070;&#26550;&#8221;&#65289;&#12290;&#22240;&#20026;CLIP&#26159;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22522;&#30784;&#65292;&#36825;&#20123;&#36755;&#20837;&#20250;&#23548;&#33268;Midjourney 5.1&#12289;DALL-E&#12289;VideoFusion&#31561;&#31995;&#32479;&#22833;&#36133;&#12290;MultiMon&#20063;&#21487;&#20197;&#25351;&#23548;&#38024;&#23545;&#29305;&#23450;&#29992;&#20363;&#30340;&#30456;&#20851;&#25925;&#38556;&#65292;&#20363;&#22914;&#33258;&#39550;&#36710;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployed multimodal systems can fail in ways that evaluators did not anticipate. In order to find these failures before deployment, we introduce MultiMon, a system that automatically identifies systematic failures -generalizable, natural-language descriptions of patterns of model failures. To uncover systematic failures, MultiMon scrapes a corpus for examples of erroneous agreement: inputs that produce the same output, but should not. It then prompts a language model (e.g., GPT-4) to find systematic patterns of failure and describe them in natural language. We use MultiMon to find 14 systematic failures (e.g., "ignores quantifiers") of the CLIP text-encoder, each comprising hundreds of distinct inputs (e.g., "a shelf with a few/many books"). Because CLIP is the backbone for most state-of-the-art multimodal systems, these inputs produce failures in Midjourney 5.1, DALL-E, VideoFusion, and others. MultiMon can also steer towards failures relevant to specific use cases, such as self-dri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#22522;&#20110;&#35821;&#20041;&#30456;&#20851;&#30340;&#35789;&#27861;&#32422;&#26463;&#22312;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#35299;&#20915;&#21516;&#24418;&#24322;&#20041;&#35789;&#38382;&#39064;&#30340;&#21516;&#24418;&#24322;&#20041;&#35789;&#28040;&#27495;&#27169;&#22359;&#21644;PLUMCOT&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#35780;&#20272;&#22522;&#20934;HOLLY&#12290;</title><link>http://arxiv.org/abs/2306.12089</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#30456;&#20851;&#30340;&#35789;&#27861;&#32422;&#26463;&#22312;&#31934;&#30830;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Accurate Translation via Semantically Appropriate Application of Lexical Constraints. (arXiv:2306.12089v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#22522;&#20110;&#35821;&#20041;&#30456;&#20851;&#30340;&#35789;&#27861;&#32422;&#26463;&#22312;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#35299;&#20915;&#21516;&#24418;&#24322;&#20041;&#35789;&#38382;&#39064;&#30340;&#21516;&#24418;&#24322;&#20041;&#35789;&#28040;&#27495;&#27169;&#22359;&#21644;PLUMCOT&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#35780;&#20272;&#22522;&#20934;HOLLY&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27861;&#32422;&#26463;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(LNMT)&#26088;&#22312;&#23558;&#29992;&#25143;&#25552;&#20379;&#30340;&#26415;&#35821;&#32435;&#20837;&#32763;&#35793;&#20013;&#12290;&#23613;&#31649;&#20855;&#26377;&#23454;&#29992;&#20248;&#21183;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#24182;&#26410;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#26465;&#20214;&#19979;&#35780;&#20272;LNMT&#27169;&#22411;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#24403;&#21069;LNMT&#30740;&#31350;&#35780;&#20272;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#37325;&#35201;&#20294;&#40092;&#26377;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#27169;&#22411;&#38656;&#35201;&#24212;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#25361;&#25112;&#24615;&#35789;&#27719;&#32422;&#26463;&#65292;&#20363;&#22914;&#21516;&#24418;&#24322;&#20041;&#35789;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#21516;&#24418;&#24322;&#20041;&#35789;&#28040;&#27495;&#27169;&#22359;&#26469;&#21306;&#20998;&#21516;&#24418;&#24322;&#20041;&#35789;&#30340;&#21547;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PLUMCOT&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#24378;&#21270;&#25351;&#38024;&#32593;&#32476;&#30340;&#22797;&#21046;&#26426;&#21046;&#26469;&#30452;&#25509;&#30417;&#30563;&#22797;&#21046;&#35780;&#20998;&#65292;&#20174;&#32780;&#25972;&#21512;&#20102;&#19978;&#19979;&#25991;&#20016;&#23500;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;HOLLY&#65292;&#19968;&#31181;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#24212;&#23545;&#21516;&#24418;&#24322;&#20041;&#35789;&#21450;&#35757;&#32451;&#20013;&#26410;&#35265;&#35789;&#27719;&#38480;&#21046;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#22312;HOLLY&#21644;&#20043;&#21069;&#30340;&#27979;&#35797;&#29615;&#22659;&#20013;&#36827;&#34892;
&lt;/p&gt;
&lt;p&gt;
Lexically-constrained NMT (LNMT) aims to incorporate user-provided terminology into translations. Despite its practical advantages, existing work has not evaluated LNMT models under challenging real-world conditions. In this paper, we focus on two important but under-studied issues that lie in the current evaluation process of LNMT studies. The model needs to cope with challenging lexical constraints that are "homographs" or "unseen" during training. To this end, we first design a homograph disambiguation module to differentiate the meanings of homographs. Moreover, we propose PLUMCOT, which integrates contextually rich information about unseen lexical constraints from pre-trained language models and strengthens a copy mechanism of the pointer network via direct supervision of a copying score. We also release HOLLY, an evaluation benchmark for assessing the ability of a model to cope with "homographic" and "unseen" lexical constraints. Experiments on HOLLY and the previous test setup s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20840;&#23616;&#22270;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26082;&#22788;&#29702;&#35805;&#35821;&#23618;&#38754;&#21448;&#22788;&#29702;&#21333;&#35789;&#23618;&#38754;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#20132;&#20114;&#26426;&#21046;&#23545;&#33410;&#28857;&#32423;&#21035;&#21644;&#31867;&#22411;&#32423;&#21035;&#30340;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#25552;&#20379;&#26356;&#32454;&#31890;&#24230;&#30340;&#20851;&#31995;&#25552;&#21462;&#65292;&#35813;&#26041;&#27861;&#22312;&#36923;&#36753;&#25512;&#29702;QA&#25968;&#25454;&#38598;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.12069</link><description>&lt;p&gt;
&#23558;&#35805;&#35821;&#21333;&#20803;&#21644;&#20851;&#38190;&#35789;&#32852;&#31995;&#36215;&#26469;&#23545;&#38405;&#35835;&#29702;&#35299;&#36827;&#34892;&#23618;&#27425;&#25512;&#29702;&#38142;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension. (arXiv:2306.12069v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12069
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20840;&#23616;&#22270;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26082;&#22788;&#29702;&#35805;&#35821;&#23618;&#38754;&#21448;&#22788;&#29702;&#21333;&#35789;&#23618;&#38754;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#20132;&#20114;&#26426;&#21046;&#23545;&#33410;&#28857;&#32423;&#21035;&#21644;&#31867;&#22411;&#32423;&#21035;&#30340;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#25552;&#20379;&#26356;&#32454;&#31890;&#24230;&#30340;&#20851;&#31995;&#25552;&#21462;&#65292;&#35813;&#26041;&#27861;&#22312;&#36923;&#36753;&#25512;&#29702;QA&#25968;&#25454;&#38598;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#38754;&#20020;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#26032;&#25361;&#25112;&#65292;&#26088;&#22312;&#29702;&#35299;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#25152;&#28041;&#21450;&#30340;&#38544;&#21547;&#36923;&#36753;&#20851;&#31995;&#24182;&#23545;&#20854;&#36827;&#34892;&#25512;&#29702;&#12290;&#30001;&#20110;&#36923;&#36753;&#22797;&#26434;&#24615;&#65292;&#36923;&#36753;&#20851;&#31995;&#23384;&#22312;&#20110;&#19981;&#21516;&#30340;&#31890;&#24230;&#32423;&#21035;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25512;&#29702;&#26041;&#27861;&#20998;&#21035;&#20851;&#27880;&#23454;&#20307;&#24863;&#30693;&#25110;&#35805;&#35821;&#20026;&#22522;&#30784;&#30340;&#20449;&#24687;&#65292;&#20294;&#24573;&#30053;&#20102;&#21487;&#33021;&#20855;&#26377;&#30456;&#20114;&#24433;&#21709;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20840;&#23616;&#22270;&#32593;&#32476;&#65288;HGN&#65289;&#30340;&#26041;&#27861;&#65292;&#26082;&#22788;&#29702;&#35805;&#35821;&#23618;&#38754;&#21448;&#22788;&#29702;&#21333;&#35789;&#23618;&#38754;&#30340;&#19978;&#19979;&#25991;&#65292;&#20316;&#20026;&#36923;&#36753;&#25512;&#29702;&#30340;&#22522;&#30784;&#65292;&#20197;&#25552;&#20379;&#26356;&#32454;&#31890;&#24230;&#30340;&#20851;&#31995;&#25552;&#21462;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#20998;&#23618;&#20132;&#20114;&#26426;&#21046;&#23545;&#33410;&#28857;&#32423;&#21035;&#21644;&#31867;&#22411;&#32423;&#21035;&#30340;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#20123;&#20851;&#31995;&#21487;&#20197;&#35299;&#37322;&#20026;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#26725;&#26753;&#65292;&#20197;&#25913;&#36827;MRC&#31995;&#32479;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36923;&#36753;&#25512;&#29702;QA&#25968;&#25454;&#38598;&#65288;ReClor&#21644;LogiQA&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;RACE&#21644;SWAG&#65289;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine reading comprehension (MRC) poses new challenges over logical reasoning, which aims to understand the implicit logical relations entailed in the given contexts and perform inference over them. Due to the complexity of logic, logical relations exist at different granularity levels. However, most existing methods of logical reasoning individually focus on either entity-aware or discourse-based information but ignore the hierarchical relations that may even have mutual effects. In this paper, we propose a holistic graph network (HGN) which deals with context at both discourse level and word level, as the basis for logical reasoning, to provide a more fine-grained relation extraction. Specifically, node-level and type-level relations, which can be interpreted as bridges in the reasoning process, are modeled by a hierarchical interaction mechanism to improve the interpretation of MRC systems. Experimental results on logical reasoning QA datasets (ReClor and LogiQA) and natural langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#21738;&#20123;&#26679;&#26412;&#26368;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#25110;&#26368;&#20855;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#8220;&#26679;&#26412;&#25915;&#20987;&#33021;&#21147;/&#40065;&#26834;&#24615;&#8221;&#30340;&#23450;&#20041;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26816;&#27979;&#22120;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20986;&#19968;&#20010;&#30475;&#19981;&#35265;&#30340;&#30446;&#26631;&#27169;&#22411;&#20013;&#26368;&#26131;&#21463;&#25915;&#20987;&#21644;&#26368;&#20855;&#40065;&#26834;&#24615;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.12043</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#26679;&#26412;&#25915;&#20987;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sample Attackability in Natural Language Adversarial Attacks. (arXiv:2306.12043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#21738;&#20123;&#26679;&#26412;&#26368;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#25110;&#26368;&#20855;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#8220;&#26679;&#26412;&#25915;&#20987;&#33021;&#21147;/&#40065;&#26834;&#24615;&#8221;&#30340;&#23450;&#20041;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26816;&#27979;&#22120;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20986;&#19968;&#20010;&#30475;&#19981;&#35265;&#30340;&#30446;&#26631;&#27169;&#22411;&#20013;&#26368;&#26131;&#21463;&#25915;&#20987;&#21644;&#26368;&#20855;&#40065;&#26834;&#24615;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#23545;&#25239;&#25915;&#20987;&#30740;&#31350;&#24050;&#32463;&#22312;&#35774;&#35745;&#24378;&#22823;&#30340;&#25915;&#20987;&#26041;&#27861;&#21644;&#38450;&#24481;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#21162;&#21147;&#21435;&#30830;&#23450;&#21738;&#20123;&#28304;&#26679;&#26412;&#26159;&#26368;&#26131;&#21463;&#25915;&#20987;&#25110;&#26368;&#20855;&#40065;&#26834;&#24615;&#65292;&#21363;&#22312;&#19968;&#20010;&#30475;&#19981;&#35265;&#30340;&#30446;&#26631;&#27169;&#22411;&#19978;&#65292;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#26368;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#26412;&#25991;&#27491;&#24335;&#25193;&#23637;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25915;&#20987;&#20013;&#26679;&#26412;&#25915;&#20987;&#33021;&#21147;/&#40065;&#26834;&#24615;&#30340;&#23450;&#20041;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#12289;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21644;&#22235;&#31181;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26679;&#26412;&#19981;&#30830;&#23450;&#24615;&#19981;&#33021;&#20805;&#20998;&#25551;&#36848;&#25915;&#20987;&#24615;/&#40065;&#26834;&#24615;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#22240;&#27492;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26816;&#27979;&#22120;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20986;&#19968;&#20010;&#30475;&#19981;&#35265;&#30340;&#30446;&#26631;&#27169;&#22411;&#20013;&#26368;&#26131;&#21463;&#25915;&#20987;&#21644;&#26368;&#20855;&#40065;&#26834;&#24615;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25915;&#20987;&#26041;&#27861;&#20013;&#65292;&#23545;&#20110;&#21738;&#20123;&#26679;&#26412;&#34987;&#35748;&#20026;&#26159;&#26368;&#26131;&#21463;&#25915;&#20987;/&#26368;&#20855;&#40065;&#26834;&#24615;&#65292;&#19981;&#23384;&#22312;&#26126;&#26174;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attack research in natural language processing (NLP) has made significant progress in designing powerful attack methods and defence approaches. However, few efforts have sought to identify which source samples are the most attackable or robust, i.e. can we determine for an unseen target model, which samples are the most vulnerable to an adversarial attack. This work formally extends the definition of sample attackability/robustness for NLP attacks. Experiments on two popular NLP datasets, four state of the art models and four different NLP adversarial attack methods, demonstrate that sample uncertainty is insufficient for describing characteristics of attackable/robust samples and hence a deep learning based detector can perform much better at identifying the most attackable and robust samples for an unseen target model. Nevertheless, further analysis finds that there is little agreement in which samples are considered the most attackable/robust across different NLP attack 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;TTS&#20013;&#20351;&#29992;&#22522;&#20110;PHOIBLE&#30340;&#38899;&#32032;&#26144;&#23556;&#26041;&#27861;&#21644;&#20351;&#29992;&#35821;&#38899;&#29305;&#24449;&#36755;&#20837;&#30340;&#36716;&#31227;&#23398;&#20064;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#25552;&#39640;&#36755;&#20986;&#36136;&#37327;&#65292;&#20294;&#21518;&#32773;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#20013;&#36873;&#25321;&#28304;&#35821;&#35328;&#30340;&#26631;&#20934;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#35282;&#39057;&#29575;&#30456;&#20284;&#24615;&#30340;&#38899;&#39057;&#39057;&#29575;&#26159;&#26377;&#25928;&#30340;&#65292;&#32780;&#35821;&#35328;&#36317;&#31163;&#21017;&#27809;&#26377;&#39044;&#26399;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12040</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#38899;&#21512;&#25104;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#31574;&#30053;&#65306;&#38899;&#32032;&#26144;&#23556;&#65292;&#29305;&#24449;&#36755;&#20837;&#21644;&#28304;&#35821;&#35328;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Strategies in Transfer Learning for Low-Resource Speech Synthesis: Phone Mapping, Features Input, and Source Language Selection. (arXiv:2306.12040v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;TTS&#20013;&#20351;&#29992;&#22522;&#20110;PHOIBLE&#30340;&#38899;&#32032;&#26144;&#23556;&#26041;&#27861;&#21644;&#20351;&#29992;&#35821;&#38899;&#29305;&#24449;&#36755;&#20837;&#30340;&#36716;&#31227;&#23398;&#20064;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#25552;&#39640;&#36755;&#20986;&#36136;&#37327;&#65292;&#20294;&#21518;&#32773;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#20013;&#36873;&#25321;&#28304;&#35821;&#35328;&#30340;&#26631;&#20934;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#35282;&#39057;&#29575;&#30456;&#20284;&#24615;&#30340;&#38899;&#39057;&#39057;&#29575;&#26159;&#26377;&#25928;&#30340;&#65292;&#32780;&#35821;&#35328;&#36317;&#31163;&#21017;&#27809;&#26377;&#39044;&#26399;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;PHOIBLE&#30340;&#38899;&#32032;&#26144;&#23556;&#26041;&#27861;&#21644;&#20351;&#29992;&#35821;&#38899;&#29305;&#24449;&#36755;&#20837;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;TTS&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#28304;&#35821;&#35328;&#65288;&#33521;&#35821;&#65292;&#33452;&#20848;&#35821;&#65292;&#21360;&#22320;&#35821;&#65292;&#26085;&#35821;&#21644;&#20420;&#35821;&#65289;&#21644;&#30446;&#26631;&#35821;&#35328;&#65288;&#20445;&#21152;&#21033;&#20122;&#35821;&#65292;&#26684;&#40065;&#21513;&#20122;&#35821;&#65292;&#21704;&#33832;&#20811;&#35821;&#65292;&#26031;&#29926;&#24076;&#37324;&#35821;&#65292;&#20044;&#23572;&#37117;&#35821;&#21644;&#20044;&#20857;&#21035;&#20811;&#35821;&#65289;&#27979;&#35797;&#26041;&#27861;&#30340;&#35821;&#35328;&#29420;&#31435;&#24615;&#65292;&#22686;&#24378;&#30740;&#31350;&#32467;&#26524;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#23383;&#31526;&#35823;&#24046;&#29575;&#21644;&#39044;&#27979;&#30340;&#24179;&#22343;&#24847;&#35265;&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38899;&#32032;&#26144;&#23556;&#21644;&#29305;&#24449;&#36755;&#20837;&#37117;&#21487;&#20197;&#25552;&#39640;&#36755;&#20986;&#36136;&#37327;&#65292;&#21518;&#32773;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#36825;&#20123;&#25928;&#26524;&#20063;&#21462;&#20915;&#20110;&#20855;&#20307;&#30340;&#35821;&#35328;&#32452;&#21512;&#12290;&#25105;&#20204;&#36824;&#23558;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#35282;&#39057;&#29575;&#30456;&#20284;&#24615;&#30340;&#38899;&#39057;&#39057;&#29575;&#65288;ASPF&#65289;&#19982;&#22522;&#20110;&#23478;&#26063;&#26641;&#30340;&#36317;&#31163;&#24230;&#37327;&#36827;&#34892;&#27604;&#36739;&#65292;&#20316;&#20026;&#22312;&#36716;&#31227;&#23398;&#20064;&#20013;&#36873;&#25321;&#28304;&#35821;&#35328;&#30340;&#26631;&#20934;&#12290;&#22914;&#26524;&#20351;&#29992;&#22522;&#20110;&#26631;&#31614;&#30340;&#30005;&#35805;&#36755;&#20837;&#65292;&#21017;ASPF&#35777;&#26126;&#26377;&#25928;&#65292;&#32780;&#35821;&#35328;&#36317;&#31163;&#21017;&#27809;&#26377;&#39044;&#26399;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We compare using a PHOIBLE-based phone mapping method and using phonological features input in transfer learning for TTS in low-resource languages. We use diverse source languages (English, Finnish, Hindi, Japanese, and Russian) and target languages (Bulgarian, Georgian, Kazakh, Swahili, Urdu, and Uzbek) to test the language-independence of the methods and enhance the findings' applicability. We use Character Error Rates from automatic speech recognition and predicted Mean Opinion Scores for evaluation. Results show that both phone mapping and features input improve the output quality and the latter performs better, but these effects also depend on the specific language combination. We also compare the recently-proposed Angular Similarity of Phone Frequencies (ASPF) with a family tree-based distance measure as a criterion to select source languages in transfer learning. ASPF proves effective if label-based phone input is used, while the language distance does not have expected effects.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24863;&#30693;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;VA-TTS&#65289;&#20219;&#21153;&#65292;&#29992;&#20110;&#22312;&#38754;&#23545;&#38754;&#30340;&#20132;&#27969;&#20013;&#65292;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#21644;&#25509;&#25910;&#32773;&#30340;&#24207;&#21015;&#35270;&#35273;&#21453;&#39304;&#26469;&#21512;&#25104;&#35821;&#38899;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#35821;&#38899;&#21512;&#25104;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#26356;&#33258;&#28982;&#30340;&#38899;&#39057;&#65292;&#20855;&#26377;&#22330;&#26223;&#36866;&#24403;&#30340;&#38901;&#24459;&#21644;&#35821;&#35843;&#12290;</title><link>http://arxiv.org/abs/2306.12020</link><description>&lt;p&gt;
&#35270;&#35273;&#24863;&#30693;&#30340;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Visual-Aware Text-to-Speech. (arXiv:2306.12020v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24863;&#30693;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;VA-TTS&#65289;&#20219;&#21153;&#65292;&#29992;&#20110;&#22312;&#38754;&#23545;&#38754;&#30340;&#20132;&#27969;&#20013;&#65292;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#21644;&#25509;&#25910;&#32773;&#30340;&#24207;&#21015;&#35270;&#35273;&#21453;&#39304;&#26469;&#21512;&#25104;&#35821;&#38899;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#35821;&#38899;&#21512;&#25104;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#26356;&#33258;&#28982;&#30340;&#38899;&#39057;&#65292;&#20855;&#26377;&#22330;&#26223;&#36866;&#24403;&#30340;&#38901;&#24459;&#21644;&#35821;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22320;&#21512;&#25104;&#33021;&#22815;&#20027;&#21160;&#21709;&#24212;&#21548;&#20247;&#30340;&#35821;&#38899;&#22312;&#38754;&#23545;&#38754;&#30340;&#20132;&#20114;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#28436;&#35762;&#32773;&#21487;&#20197;&#21033;&#29992;&#21548;&#20247;&#30340;&#38754;&#37096;&#34920;&#24773;&#26469;&#35843;&#25972;&#35821;&#35843;&#12289;&#37325;&#35835;&#38899;&#33410;&#25110;&#20572;&#39039;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24863;&#30693;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;VA-TTS&#65289;&#20219;&#21153;&#65292;&#29992;&#20110;&#22312;&#38754;&#23545;&#38754;&#30340;&#20132;&#27969;&#20013;&#65292;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#21644;&#25509;&#25910;&#32773;&#30340;&#24207;&#21015;&#35270;&#35273;&#21453;&#39304;&#65288;&#22914;&#28857;&#22836;&#12289;&#24494;&#31505;&#65289;&#26469;&#21512;&#25104;&#35821;&#38899;&#12290;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65292;VA-TTS&#24378;&#35843;&#20102;&#35270;&#35273;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#20010;&#26032;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27169;&#22411;&#65292;&#29992;&#20110;&#34701;&#21512;&#38899;&#32032;&#35821;&#35328;&#20449;&#24687;&#21644;&#21548;&#20247;&#30340;&#35270;&#35273;&#20449;&#21495;&#26469;&#36827;&#34892;&#35821;&#38899;&#21512;&#25104;&#12290;&#23545;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;ViCo-X&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#29983;&#25104;&#20102;&#26356;&#33258;&#28982;&#30340;&#38899;&#39057;&#65292;&#24182;&#20855;&#26377;&#22330;&#26223;&#36866;&#24403;&#30340;&#38901;&#24459;&#21644;&#35821;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamically synthesizing talking speech that actively responds to a listening head is critical during the face-to-face interaction. For example, the speaker could take advantage of the listener's facial expression to adjust the tones, stressed syllables, or pauses. In this work, we present a new visual-aware text-to-speech (VA-TTS) task to synthesize speech conditioned on both textual inputs and sequential visual feedback (e.g., nod, smile) of the listener in face-to-face communication. Different from traditional text-to-speech, VA-TTS highlights the impact of visual modality. On this newly-minted task, we devise a baseline model to fuse phoneme linguistic information and listener visual signals for speech synthesis. Extensive experiments on multimodal conversation dataset ViCo-X verify our proposal for generating more natural audio with scenario-appropriate rhythm and prosody.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#33258;&#22238;&#24402;&#30340;&#20381;&#23384;&#35299;&#26512;&#22120;&#65292;&#25104;&#21151;&#25429;&#33719;&#20102;&#33410;&#28857;&#21644;&#36793;&#32536;&#20043;&#38388;&#30340;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12018</link><description>&lt;p&gt;
&#19968;&#31181;&#21322;&#33258;&#22238;&#24402;&#22270;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#20381;&#23384;&#20851;&#31995;&#22270;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Semi-Autoregressive Graph Generative Model for Dependency Graph Parsing. (arXiv:2306.12018v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12018
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#33258;&#22238;&#24402;&#30340;&#20381;&#23384;&#35299;&#26512;&#22120;&#65292;&#25104;&#21151;&#25429;&#33719;&#20102;&#33410;&#28857;&#21644;&#36793;&#32536;&#20043;&#38388;&#30340;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#20381;&#23384;&#35299;&#26512;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#26681;&#25454;&#23545;&#22270;&#32852;&#21512;&#27010;&#29575;&#30340;&#19981;&#21516;&#20998;&#35299;&#26041;&#27861;&#65292;&#29616;&#26377;&#35299;&#26512;&#22120;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#26377;&#21521;&#36793;&#35270;&#20026;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20381;&#23384;&#22270;&#20013;&#23384;&#22312;&#29420;&#31435;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#32452;&#20214;&#28151;&#21512;&#65292;&#26631;&#24535;&#30528;&#20808;&#21069;&#30340;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#33410;&#28857;&#21644;&#36793;&#32536;&#20043;&#38388;&#30340;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#19968;&#29305;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21322;&#33258;&#22238;&#24402;&#30340;&#20381;&#23384;&#35299;&#26512;&#22120;&#65292;&#36890;&#36807;&#21152;&#20837;&#33410;&#28857;&#32452;&#21644;&#36793;&#32452;&#33258;&#22238;&#24402;&#22320;&#24182;&#23558;&#33410;&#28857;&#23884;&#20837;&#19968;&#27425;&#24615;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#20381;&#23384;&#22270;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#25429;&#33719;&#20102;&#33410;&#28857;&#21644;&#36793;&#32536;&#20043;&#38388;&#30340;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the impressive progress in Neural Dependency Parsing. According to the different factorization approaches to the graph joint probabilities, existing parsers can be roughly divided into autoregressive and non-autoregressive patterns. The former means that the graph should be factorized into multiple sequentially dependent components, then it can be built up component by component. And the latter assumes these components to be independent so that they can be outputted in a one-shot manner. However, when treating the directed edge as an explicit dependency relationship, we discover that there is a mixture of independent and interdependent components in the dependency graph, signifying that both aforementioned models fail to precisely capture the explicit dependencies among nodes and edges. Based on this property, we design a Semi-Autoregressive Dependency Parser to generate dependency graphs via adding node groups and edge groups autoregressively while pouring 
&lt;/p&gt;</description></item><item><title>3HAN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34394;&#20551;&#26032;&#38395;&#33258;&#21160;&#26816;&#27979;&#22120;&#12290;&#36890;&#36807;&#19977;&#23618;&#20998;&#23618;&#27880;&#24847;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#23545;&#26032;&#38395;&#36827;&#34892;&#23436;&#25972;&#26377;&#25928;&#34920;&#31034;&#30340;&#26032;&#38395;&#21521;&#37327;&#65292;&#24182;&#32473;&#25991;&#31456;&#19981;&#21516;&#37096;&#20998;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12014</link><description>&lt;p&gt;
3HAN&#65306;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
3HAN: A Deep Neural Network for Fake News Detection. (arXiv:2306.12014v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12014
&lt;/p&gt;
&lt;p&gt;
3HAN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34394;&#20551;&#26032;&#38395;&#33258;&#21160;&#26816;&#27979;&#22120;&#12290;&#36890;&#36807;&#19977;&#23618;&#20998;&#23618;&#27880;&#24847;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#23545;&#26032;&#38395;&#36827;&#34892;&#23436;&#25972;&#26377;&#25928;&#34920;&#31034;&#30340;&#26032;&#38395;&#21521;&#37327;&#65292;&#24182;&#32473;&#25991;&#31456;&#19981;&#21516;&#37096;&#20998;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#26032;&#38395;&#30340;&#36805;&#36895;&#20256;&#25773;&#26159;&#19968;&#20010;&#38656;&#35201;AI&#35299;&#20915;&#26041;&#26696;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19977;&#23618;&#20998;&#23618;&#27880;&#24847;&#32593;&#32476;&#65288;3HAN&#65289;&#26500;&#24314;&#19968;&#20010;&#33258;&#21160;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#26816;&#27979;&#34394;&#20551;&#26032;&#38395;&#12290; 3HAN&#36890;&#36807;&#36880;&#23618;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#24335;&#26500;&#24314;&#26032;&#38395;&#21521;&#37327;&#65306;&#23545;&#36755;&#20837;&#26032;&#38395;&#25991;&#31456;&#30340;&#26377;&#25928;&#34920;&#31034;&#12290;&#22312;&#26032;&#38395;&#20013;&#65292;&#26631;&#39064;&#26159;&#34394;&#20551;&#26032;&#38395;&#30340;&#19968;&#20010;&#21306;&#21035;&#29305;&#24449;&#65292;&#21516;&#26102;&#25991;&#31456;&#20013;&#30340;&#30456;&#23545;&#36739;&#23569;&#30340;&#21333;&#35789;&#21644;&#21477;&#23376;&#27604;&#20854;&#20313;&#37096;&#20998;&#26356;&#20026;&#37325;&#35201;&#12290;3HAN&#30001;&#20110;&#20855;&#26377;&#19977;&#23618;&#27880;&#24847;&#21147;&#30340;&#26426;&#21046;&#65292;&#32473;&#25991;&#31456;&#30340;&#19981;&#21516;&#37096;&#20998;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;&#22823;&#22411;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;3HAN&#30340;&#39640;&#25928;&#24615;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;96.77&#65285;&#12290;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19981;&#21516;&#65292;3HAN&#36890;&#36807;&#23545;&#25991;&#31456;&#19981;&#21516;&#37096;&#20998;&#30340;&#27880;&#24847;&#26435;&#37325;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid spread of fake news is a serious problem calling for AI solutions. We employ a deep learning based automated detector through a three level hierarchical attention network (3HAN) for fast, accurate detection of fake news. 3HAN has three levels, one each for words, sentences, and the headline, and constructs a news vector: an effective representation of an input news article, by processing an article in an hierarchical bottom-up manner. The headline is known to be a distinguishing feature of fake news, and furthermore, relatively few words and sentences in an article are more important than the rest. 3HAN gives a differential importance to parts of an article, on account of its three layers of attention. By experiments on a large real-world data set, we observe the effectiveness of 3HAN with an accuracy of 96.77%. Unlike some other deep learning models, 3HAN provides an understandable output through the attention weights given to different parts of an article, which can be visu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#32534;&#36753;&#30446;&#26631;&#20998;&#23376;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;ChatMol&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#20998;&#23376;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#22312;&#20998;&#23376;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11976</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#20132;&#20114;&#24335;&#20998;&#23376;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Interactive Molecular Discovery with Natural Language. (arXiv:2306.11976v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#32534;&#36753;&#30446;&#26631;&#20998;&#23376;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;ChatMol&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#20998;&#23376;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#22312;&#20998;&#23376;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#34987;&#26399;&#26395;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#21508;&#31181;&#20154;&#26426;&#20132;&#20114;&#30340;&#20851;&#38190;&#23186;&#20171;&#12290;&#22312;&#29983;&#29289;&#21270;&#23398;&#39046;&#22495;&#65292;&#22260;&#32469;&#20998;&#23376;&#65288;&#20363;&#22914;&#65292;&#23646;&#24615;&#39044;&#27979;&#12289;&#20998;&#23376;&#25366;&#25496;&#31561;&#65289;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20294;&#25216;&#26415;&#38376;&#27099;&#24456;&#39640;&#12290;&#23558;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#20998;&#23376;&#34920;&#36798;&#19982;&#21270;&#23398;&#35821;&#35328;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#20219;&#21153;&#30340;&#35299;&#37322;&#24615;&#12289;&#38477;&#20302;&#25805;&#20316;&#38590;&#24230;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#34701;&#21512;&#25955;&#24067;&#22312;&#20114;&#34917;&#26448;&#26009;&#20013;&#30340;&#21270;&#23398;&#30693;&#35782;&#65292;&#28145;&#20837;&#29702;&#35299;&#20998;&#23376;&#12290;&#22522;&#20110;&#36825;&#20123;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#20998;&#23376;&#35774;&#35745;&#65292;&#36825;&#26159;&#19968;&#39033;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#32534;&#36753;&#30446;&#26631;&#20998;&#23376;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#26356;&#22909;&#22320;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;ChatMol&#65292;&#19968;&#31181;&#30693;&#35782;&#20016;&#23500;&#19988;&#22810;&#21151;&#33021;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#27880;&#20837;&#23454;&#39564;&#24615;&#36136;&#20449;&#24687;&#65292;&#20998;&#23376;&#31354;&#38388;&#30693;&#35782;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#21644;&#20998;&#23376;&#20043;&#38388;&#30340;&#20851;&#32852;&#36827;&#34892;&#22686;&#24378;&#12290; ChatMol&#21487;&#20197;&#20026;&#20998;&#23376;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#25551;&#36848;&#65292;&#26356;&#27491;&#32473;&#23450;&#30340;&#20998;&#23376;&#65292;&#29978;&#33267;&#21487;&#20197;&#36890;&#36807;&#20114;&#21160;&#23545;&#35805;&#36845;&#20195;&#22320;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20998;&#23376;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;ChatMol&#65292;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language is expected to be a key medium for various human-machine interactions in the era of large language models. When it comes to the biochemistry field, a series of tasks around molecules (e.g., property prediction, molecule mining, etc.) are of great significance while having a high technical threshold. Bridging the molecule expressions in natural language and chemical language can not only hugely improve the interpretability and reduce the operation difficulty of these tasks, but also fuse the chemical knowledge scattered in complementary materials for a deeper comprehension of molecules. Based on these benefits, we propose the conversational molecular design, a novel task adopting natural language for describing and editing target molecules. To better accomplish this task, we design ChatMol, a knowledgeable and versatile generative pre-trained model, enhanced by injecting experimental property information, molecular spatial knowledge, and the associations between natural
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#36229;&#36234;&#34920;&#38754;&#24418;&#24335;&#29305;&#24449;&#65292;&#23398;&#20064;&#31934;&#30830;&#32780;&#24418;&#24335;&#21270;&#23450;&#20041;&#30340;&#20195;&#30721;&#30340;&#35745;&#31639;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.11943</link><description>&lt;p&gt;
&#25506;&#31350;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#25152;&#23398;&#20064;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding What Code Language Models Learned. (arXiv:2306.11943v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#36229;&#36234;&#34920;&#38754;&#24418;&#24335;&#29305;&#24449;&#65292;&#23398;&#20064;&#31934;&#30830;&#32780;&#24418;&#24335;&#21270;&#23450;&#20041;&#30340;&#20195;&#30721;&#30340;&#35745;&#31639;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#37117;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26377;&#20154;&#35748;&#20026;&#23427;&#20204;&#30340;&#33021;&#21147;&#19981;&#36275;&#20197;&#23436;&#20840;&#23398;&#20064;&#35821;&#35328;&#30340;&#24847;&#20041;&#25110;&#29702;&#35299;&#35821;&#35328;&#12290;&#20026;&#20102;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#26576;&#31181;&#24418;&#24335;&#30340;&#24847;&#20041;&#30340;&#31243;&#24230;&#65292;&#25105;&#20204;&#30740;&#31350;&#23427;&#20204;&#25429;&#25417;&#20195;&#30721;&#35821;&#20041;&#30340;&#33021;&#21147;&#65292;&#36229;&#36234;&#34920;&#23618;&#39057;&#29575;&#21644;&#20849;&#29616;&#30340;&#38480;&#21046;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#27169;&#22411;&#35821;&#35328;&#29305;&#24449;&#30340;&#25506;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#19968;&#31181;&#21487;&#20197;&#23458;&#35266;&#22320;&#12289;&#31616;&#21333;&#26126;&#20102;&#22320;&#35780;&#20272;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;&#33021;&#21147;&#30340;&#29615;&#22659;&#19979;&#30740;&#31350;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#25429;&#25417;&#31934;&#30830;&#32780;&#24418;&#24335;&#21270;&#23450;&#20041;&#30340;&#20195;&#30721;&#30340;&#35821;&#20041;&#12290;&#36890;&#36807;&#23545;&#20195;&#30721;&#29255;&#27573;&#30340;&#25805;&#32437;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#30721;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#20102;&#20195;&#30721;&#30340;&#35745;&#31639;&#35821;&#20041;&#30340;&#24378;&#26377;&#21147;&#30340;&#34920;&#24449;&#65292;&#36229;&#36234;&#20102;&#20195;&#30721;&#34920;&#38754;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models are effective in a variety of natural language tasks, but it has been argued their capabilities fall short of fully learning meaning or understanding language. To understand the extent to which language models can learn some form of meaning, we investigate their ability to capture semantics of code beyond superficial frequency and co-occurrence. In contrast to previous research on probing models for linguistic features, we study pre-trained models in a setting that allows for objective and straightforward evaluation of a model's ability to learn semantics. In this paper, we examine whether such models capture the semantics of code, which is precisely and formally defined. Through experiments involving the manipulation of code fragments, we show that code pre-trained models of code learn a robust representation of the computational semantics of code that goes beyond superficial features of form alone
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#20419;&#36827;&#12289;&#35843;&#33410;&#21644;&#24635;&#32467;Polis&#21487;&#25193;&#23637;&#21327;&#21830;&#20013;&#30340;&#26426;&#20250;&#21644;&#39118;&#38505;&#65292;&#35797;&#39564;&#34920;&#26126;&#24635;&#32467;&#33021;&#21147;&#20351;&#24471;&#20844;&#20849;&#21442;&#19982;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#26159;LLMs&#30340;&#19978;&#19979;&#25991;&#38480;&#21046;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21407;&#21017;&#21644;&#25216;&#26415;&#26469;&#34920;&#24449;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.11932</link><description>&lt;p&gt;
LLMs&#22312;Polis&#21487;&#25193;&#23637;&#21327;&#21830;&#20013;&#30340;&#26426;&#20250;&#19982;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Opportunities and Risks of LLMs for Scalable Deliberation with Polis. (arXiv:2306.11932v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#20419;&#36827;&#12289;&#35843;&#33410;&#21644;&#24635;&#32467;Polis&#21487;&#25193;&#23637;&#21327;&#21830;&#20013;&#30340;&#26426;&#20250;&#21644;&#39118;&#38505;&#65292;&#35797;&#39564;&#34920;&#26126;&#24635;&#32467;&#33021;&#21147;&#20351;&#24471;&#20844;&#20849;&#21442;&#19982;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#26159;LLMs&#30340;&#19978;&#19979;&#25991;&#38480;&#21046;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21407;&#21017;&#21644;&#25216;&#26415;&#26469;&#34920;&#24449;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Polis&#26159;&#19968;&#20010;&#21033;&#29992;&#26426;&#22120;&#26234;&#33021;&#25193;&#22823;&#21327;&#21830;&#36807;&#31243;&#30340;&#24179;&#21488;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#20419;&#36827;&#12289;&#35843;&#33410;&#21644;&#24635;&#32467;Polis&#21442;&#19982;&#36807;&#31243;&#20013;&#38754;&#20020;&#30340;&#26426;&#20250;&#21644;&#39118;&#38505;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#20351;&#29992;Anthropic&#30340;Claude&#36827;&#34892;&#35797;&#28857;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;LLMs&#30830;&#23454;&#21487;&#20197;&#22686;&#24378;&#20154;&#31867;&#26234;&#33021;&#65292;&#24110;&#21161;&#26356;&#26377;&#25928;&#22320;&#36816;&#34892;Polis&#23545;&#35805;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24635;&#32467;&#33021;&#21147;&#20351;&#24471;&#38598;&#20307;&#24847;&#20041;&#24418;&#25104;&#32451;&#20064;&#20013;&#30340;&#20844;&#20849;&#21442;&#19982;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#19978;&#19979;&#25991;&#38480;&#21046;&#23545;&#32467;&#26524;&#30340;&#27934;&#35265;&#21644;&#36136;&#37327;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26426;&#20250;&#20063;&#20276;&#38543;&#30528;&#39118;&#38505;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20854;&#20013;&#19968;&#20123;&#39118;&#38505;&#65292;&#20197;&#21450;&#34920;&#24449;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#21407;&#21017;&#21644;&#25216;&#26415;&#65292;&#20197;&#21450;&#21487;&#33021;&#37319;&#29992;LLMs&#30340;&#20854;&#20182;&#21327;&#21830;&#25110;&#25919;&#27835;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#36884;&#24452;&#65292;&#21253;&#25324;&#24320;&#21457;&#32467;&#21512;&#20154;&#31867;&#21644;LLM&#26234;&#33021;&#30340;&#28151;&#21512;&#26041;&#27861;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polis is a platform that leverages machine intelligence to scale up deliberative processes. In this paper, we explore the opportunities and risks associated with applying Large Language Models (LLMs) towards challenges with facilitating, moderating and summarizing the results of Polis engagements. In particular, we demonstrate with pilot experiments using Anthropic's Claude that LLMs can indeed augment human intelligence to help more efficiently run Polis conversations. In particular, we find that summarization capabilities enable categorically new methods with immense promise to empower the public in collective meaning-making exercises. And notably, LLM context limitations have a significant impact on insight and quality of these results.  However, these opportunities come with risks. We discuss some of these risks, as well as principles and techniques for characterizing and mitigating them, and the implications for other deliberative or political systems that may employ LLMs. Finally
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;Google&#32763;&#35793;&#23545;&#24773;&#24863;&#21270;&#25991;&#26412;&#30340;&#20013;&#33521;&#32763;&#35793;&#25928;&#26524;&#65292;&#21457;&#29616;50%&#30340;&#32763;&#35793;&#36755;&#20986;&#27809;&#26377;&#20445;&#30041;&#21407;&#22987;&#24773;&#24863;&#20449;&#24687;&#65292;&#24773;&#24863;&#35789;&#27719;&#21644;&#35821;&#35328;&#29616;&#35937;&#26159;&#36825;&#20123;&#32763;&#35793;&#38169;&#35823;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2306.11900</link><description>&lt;p&gt;
&#24773;&#24863;&#21270;&#24494;&#21338;&#25991;&#26412;&#30340;&#20013;&#33521;&#26426;&#22120;&#32763;&#35793;&#30340;&#35780;&#20272;&#65306;&#19968;&#20010;&#29992;&#20110;&#24773;&#24863;&#32763;&#35793;&#30340;&#36136;&#37327;&#35780;&#20272;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Chinese-English Machine Translation of Emotion-Loaded Microblog Texts: A Human Annotated Dataset for the Quality Assessment of Emotion Translation. (arXiv:2306.11900v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;Google&#32763;&#35793;&#23545;&#24773;&#24863;&#21270;&#25991;&#26412;&#30340;&#20013;&#33521;&#32763;&#35793;&#25928;&#26524;&#65292;&#21457;&#29616;50%&#30340;&#32763;&#35793;&#36755;&#20986;&#27809;&#26377;&#20445;&#30041;&#21407;&#22987;&#24773;&#24863;&#20449;&#24687;&#65292;&#24773;&#24863;&#35789;&#27719;&#21644;&#35821;&#35328;&#29616;&#35937;&#26159;&#36825;&#20123;&#32763;&#35793;&#38169;&#35823;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;Google&#32763;&#35793;&#22312;&#24773;&#24863;&#21270;&#25991;&#26412;&#32763;&#35793;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#32500;&#24230;&#36136;&#37327;&#24230;&#37327;&#21644;&#35814;&#32454;&#38169;&#35823;&#20998;&#26512;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;&#36890;&#36807;&#20998;&#26512;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#32422;50%&#30340;&#26426;&#22120;&#32763;&#35793;&#36755;&#20986;&#27809;&#26377;&#20445;&#30041;&#21407;&#22987;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;&#22312;&#36827;&#19968;&#27493;&#20998;&#26512;&#38169;&#35823;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24773;&#24863;&#35789;&#27719;&#21644;&#35821;&#35328;&#29616;&#35937;&#65288;&#22914;&#22810;&#20041;&#35789;&#12289;&#21542;&#23450;&#12289;&#32553;&#20889;&#31561;&#65289;&#26159;&#36825;&#20123;&#32763;&#35793;&#38169;&#35823;&#30340;&#24120;&#35265;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on how current Machine Translation (MT) tools perform on the translation of emotion-loaded texts by evaluating outputs from Google Translate according to a framework proposed in this paper. We propose this evaluation framework based on the Multidimensional Quality Metrics (MQM) and perform a detailed error analysis of the MT outputs. From our analysis, we observe that about 50% of the MT outputs fail to preserve the original emotion. After further analysis of the errors, we find that emotion carrying words and linguistic phenomena such as polysemous words, negation, abbreviation etc., are common causes for these translation errors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#21033;&#29992;&#19982;&#39135;&#21697;&#30456;&#20851;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#20026;&#22522;&#30784;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#35821;&#20041;&#21305;&#37197;&#20219;&#21153;&#12290;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;AgriBERT&#65292;&#24182;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22914;FoodOn&#26412;&#20307;&#35770;&#31561;&#65292;&#21516;&#26102;&#20511;&#21161;ChatGPT&#22806;&#37096;&#30693;&#35782;&#28304;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#27169;&#22411;&#23545;&#19982;&#39135;&#21697;&#30456;&#20851;&#30340;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.11892</link><description>&lt;p&gt;
&#25506;&#32034;&#20892;&#19994;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26032;&#21069;&#27839;&#65306;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39135;&#21697;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring New Frontiers in Agricultural NLP: Investigating the Potential of Large Language Models for Food Applications. (arXiv:2306.11892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#21033;&#29992;&#19982;&#39135;&#21697;&#30456;&#20851;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#20026;&#22522;&#30784;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#35821;&#20041;&#21305;&#37197;&#20219;&#21153;&#12290;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;AgriBERT&#65292;&#24182;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22914;FoodOn&#26412;&#20307;&#35770;&#31561;&#65292;&#21516;&#26102;&#20511;&#21161;ChatGPT&#22806;&#37096;&#30693;&#35782;&#28304;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#27169;&#22411;&#23545;&#19982;&#39135;&#21697;&#30456;&#20851;&#30340;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#36816;&#29992;&#19982;&#39135;&#21697;&#30456;&#20851;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#20026;&#22522;&#30784;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#24320;&#25299;&#20892;&#19994;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26032;&#21069;&#27839;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#35821;&#20041;&#21305;&#37197;&#36825;&#20010;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#24314;&#31435;&#39135;&#21697;&#25551;&#36848;&#21644;&#33829;&#20859;&#25968;&#25454;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;FoodOn&#26412;&#20307;&#35770;&#31561;&#22806;&#37096;&#30693;&#35782;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;AgriBERT&#12290;&#20026;&#20102;&#25512;&#36827;&#20892;&#19994;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25506;&#32034;&#36884;&#24452;&#65306;(1) &#21033;&#29992;&#22522;&#20110;GPT&#30340;&#27169;&#22411;&#20316;&#20026;&#22522;&#20934;&#21644;(2) &#21033;&#29992;ChatGPT&#20316;&#20026;&#22806;&#37096;&#30693;&#35782;&#28304;&#12290;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;ChatGPT&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#22522;&#20934;&#65292;&#25105;&#20204;&#30456;&#20449;&#23427;&#20855;&#26377;&#28508;&#21147;&#25552;&#39640;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35821;&#20041;&#21305;&#37197;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#23545;&#19982;&#39135;&#21697;&#30456;&#20851;&#30340;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23454;&#39564;&#20102;&#20854;&#20182;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores new frontiers in agricultural natural language processing by investigating the effectiveness of using food-related text corpora for pretraining transformer-based language models. In particular, we focus on the task of semantic matching, which involves establishing mappings between food descriptions and nutrition data. To accomplish this, we fine-tune a pre-trained transformer-based language model, AgriBERT, on this task, utilizing an external source of knowledge, such as the FoodOn ontology. To advance the field of agricultural NLP, we propose two new avenues of exploration: (1) utilizing GPT-based models as a baseline and (2) leveraging ChatGPT as an external source of knowledge. ChatGPT has shown to be a strong baseline in many NLP tasks, and we believe it has the potential to improve our model in the task of semantic matching and enhance our model's understanding of food-related concepts and relationships. Additionally, we experiment with other applications, such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#20803;&#20998;&#24067;&#26041;&#27861;&#65288;MDM&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#30340;&#23545;&#27604;&#26144;&#23556;&#21040;&#36136;&#37327;&#24230;&#37327;&#19978;&#65292;&#21487;&#20197;&#35270;&#20026;&#20998;&#24067;&#30340;&#20998;&#24067;&#65292;&#20854;&#21487;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.11879</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#20998;&#24067;&#24314;&#27169;&#30340;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Open-Domain Text Evaluation via Meta Distribution Modeling. (arXiv:2306.11879v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#20803;&#20998;&#24067;&#26041;&#27861;&#65288;MDM&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#30340;&#23545;&#27604;&#26144;&#23556;&#21040;&#36136;&#37327;&#24230;&#37327;&#19978;&#65292;&#21487;&#20197;&#35270;&#20026;&#20998;&#24067;&#30340;&#20998;&#24067;&#65292;&#20854;&#21487;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25511;&#21046;&#21644;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#36798;&#21040;&#25152;&#38656;&#23646;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22522;&#20110;&#21442;&#32771;&#25991;&#26412;&#30340;&#24230;&#37327;&#26631;&#20934;&#22914;BLEU&#12289;ROUGE&#21644;METEOR&#23545;&#20110;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#26469;&#35828;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#21516;&#26679;&#22320;&#65292;&#34429;&#28982;&#20855;&#22791;&#35757;&#32451;&#37492;&#21035;&#22120;&#30340;&#24230;&#37327;&#26631;&#20934;&#34920;&#29616;&#20986;&#20102;&#24076;&#26395;&#30340;&#21069;&#26223;&#65292;&#20294;&#26159;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#21017;&#26159;&#19968;&#39033;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#8212;&#8212;&#20803;&#20998;&#24067;&#26041;&#27861;&#65288;MDM&#65289;&#12290;&#36890;&#36807;&#32771;&#34385;LLMs&#21442;&#25968;&#25968;&#37327;&#19978;&#21319;&#21644;&#24615;&#33021;&#25552;&#21319;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;MDM &#21019;&#36896;&#20102;&#19968;&#20010;&#26144;&#23556;&#65292;&#23558;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#30340;&#23545;&#27604;&#65288;&#19968;&#20010;&#24050;&#30693;&#20248;&#20110;&#21478;&#19968;&#20010;&#65289;&#26144;&#23556;&#21040;&#36136;&#37327;&#24230;&#37327;&#19978;&#65292;&#35813;&#24230;&#37327;&#21487;&#20197;&#35270;&#20026;&#20998;&#24067;&#30340;&#20998;&#24067;&#65292;&#21363;&#20803;&#20998;&#24067;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;MDM&#22312;&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in open-domain text generation models powered by large pre-trained language models (LLMs) have achieved remarkable performance. However, evaluating and controlling these models for desired attributes remains a challenge, as traditional reference-based metrics such as BLEU, ROUGE, and METEOR are insufficient for open-ended generation tasks. Similarly, while trainable discriminator-based evaluation metrics show promise, obtaining high-quality training data is a non-trivial task. In this paper, we introduce a novel approach to evaluate open-domain generation - the Meta-Distribution Methods (MDM). Drawing on the correlation between the rising parameter counts and the improving performance of LLMs, MDM creates a mapping from the contrast of two probabilistic distributions -- one known to be superior to the other -to quality measures, which can be viewed as a distribution of distributions i.e. Meta-Distribution. We investigate MDM for open-domain text generation evaluation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#22522;&#20110;&#26816;&#32034;&#30340;Transformer&#27169;&#22411;&#26469;&#35299;&#20915;&#34920;&#26684;&#22686;&#24378;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#20197;&#37325;&#26500;&#21407;&#22987;&#20540;&#25110;&#26631;&#39064;&#65292;&#20197;&#20415;&#20943;&#36731;&#25968;&#25454;&#20998;&#26512;&#24072;&#22312;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.11843</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#30340;Transformer&#27169;&#22411;&#29992;&#20110;&#34920;&#26684;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Based Transformer for Table Augmentation. (arXiv:2306.11843v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#22522;&#20110;&#26816;&#32034;&#30340;Transformer&#27169;&#22411;&#26469;&#35299;&#20915;&#34920;&#26684;&#22686;&#24378;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#20197;&#37325;&#26500;&#21407;&#22987;&#20540;&#25110;&#26631;&#39064;&#65292;&#20197;&#20415;&#20943;&#36731;&#25968;&#25454;&#20998;&#26512;&#24072;&#22312;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20934;&#22791;&#65288;&#20063;&#31216;&#25968;&#25454;&#25972;&#29702;&#65289;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#36827;&#34892;&#20998;&#26512;&#25110;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#26368;&#32791;&#36153;&#26102;&#38388;&#21644;&#31934;&#21147;&#30340;&#27493;&#39588;&#20043;&#19968;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35797;&#22270;&#20943;&#36731;&#26368;&#32456;&#29992;&#25143;&#65288;&#20363;&#22914;&#25968;&#25454;&#20998;&#26512;&#24072;&#65289;&#22312;&#20174;&#25968;&#25454;&#28246;&#20013;&#26500;&#24314;&#21160;&#24577;&#34920;&#26684;&#25968;&#25454;&#30340;&#36807;&#31243;&#20013;&#30340;&#24037;&#20316;&#37327;&#12290;&#25105;&#20204;&#26088;&#22312;&#35299;&#20915;&#34920;&#26684;&#22686;&#24378;&#20219;&#21153;&#65292;&#21253;&#25324;&#34892;/&#21015;&#22635;&#20805;&#21644;&#25968;&#25454;&#25554;&#34917;&#12290;&#32473;&#23450;&#19968;&#32452;&#34920;&#26684;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#33258;&#23398;&#20064;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#33258;&#23398;&#20064;&#31574;&#30053;&#26159;&#20174;&#35821;&#26009;&#24211;&#20013;&#38543;&#26426;&#21435;&#38500;&#34920;&#26684;&#65292;&#24182;&#35757;&#32451;&#26816;&#32034;&#27169;&#22411;&#20197;&#22312;&#32473;&#23450;&#37096;&#20998;&#34920;&#26684;&#20316;&#20026;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#37325;&#26500;&#21407;&#22987;&#20540;&#25110;&#26631;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#39318;&#20808;&#35757;&#32451;&#23494;&#38598;&#30340;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Data preparation, also called data wrangling, is considered one of the most expensive and time-consuming steps when performing analytics or building machine learning models. Preparing data typically involves collecting and merging data from complex heterogeneous, and often large-scale data sources, such as data lakes. In this paper, we introduce a novel approach toward automatic data wrangling in an attempt to alleviate the effort of end-users, e.g. data analysts, in structuring dynamic views from data lakes in the form of tabular data. We aim to address table augmentation tasks, including row/column population and data imputation. Given a corpus of tables, we propose a retrieval augmented self-trained transformer model. Our self-learning strategy consists in randomly ablating tables from the corpus and training the retrieval-based model to reconstruct the original values or headers given the partial tables as input. We adopt this strategy to first train the dense neural retrieval mode
&lt;/p&gt;</description></item><item><title>EvolveMT&#26159;&#19968;&#20010;&#21487;&#20197;&#39640;&#25928;&#32452;&#21512;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#24341;&#25806;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#21160;&#24577;&#36866;&#24212;&#39046;&#22495;&#25110;&#26426;&#22120;&#32763;&#35793;&#24341;&#25806;&#30340;&#25913;&#21464;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#38752;&#30340;&#32763;&#35793;&#32467;&#26524;&#65292;&#24182;&#19988;&#21482;&#38656;&#20351;&#29992;&#26412;&#36523;&#36827;&#34892;&#25913;&#36827;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11823</link><description>&lt;p&gt;
EvolveMT&#65306;&#19968;&#31181;&#20165;&#36890;&#36807;&#20351;&#29992;&#25913;&#36827;&#33258;&#36523;&#30340;&#38598;&#25104;MT&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
EvolveMT: an Ensemble MT Engine Improving Itself with Usage Only. (arXiv:2306.11823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11823
&lt;/p&gt;
&lt;p&gt;
EvolveMT&#26159;&#19968;&#20010;&#21487;&#20197;&#39640;&#25928;&#32452;&#21512;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#24341;&#25806;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#21160;&#24577;&#36866;&#24212;&#39046;&#22495;&#25110;&#26426;&#22120;&#32763;&#35793;&#24341;&#25806;&#30340;&#25913;&#21464;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#38752;&#30340;&#32763;&#35793;&#32467;&#26524;&#65292;&#24182;&#19988;&#21482;&#38656;&#20351;&#29992;&#26412;&#36523;&#36827;&#34892;&#25913;&#36827;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;EvolveMT&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#32452;&#21512;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#24341;&#25806;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#20026;&#27599;&#20010;&#27573;&#33853;&#36873;&#25321;&#21333;&#20010;&#24341;&#25806;&#30340;&#36755;&#20986;&#65292;&#20197;&#39044;&#27979;&#27599;&#20010;&#32763;&#35793;&#35831;&#27714;&#30340;&#26368;&#21512;&#36866;&#31995;&#32479;&#12290;&#31070;&#32463;&#36136;&#37327;&#20272;&#35745;&#24230;&#37327;&#30417;&#30563;&#35813;&#26041;&#27861;&#65292;&#26080;&#38656;&#21442;&#32771;&#32763;&#35793;&#12290;&#35813;&#31995;&#32479;&#30340;&#22312;&#32447;&#23398;&#20064;&#33021;&#21147;&#20801;&#35768;&#21160;&#24577;&#36866;&#24212;&#39046;&#22495;&#25110;&#26426;&#22120;&#32763;&#35793;&#24341;&#25806;&#30340;&#25913;&#21464;&#65292;&#22240;&#27492;&#28040;&#38500;&#20102;&#39069;&#22806;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#12290; EvolveMT&#22522;&#20110;&#28304;&#35821;&#21477;&#29305;&#24449;&#36873;&#25321;&#35201;&#35843;&#29992;&#30340;&#32763;&#35793;&#24341;&#25806;&#23376;&#38598;&#12290;&#25506;&#32034;&#31243;&#24230;&#21487;&#26681;&#25454;&#25152;&#38656;&#30340;&#36136;&#37327;-&#25104;&#26412;&#24179;&#34913;&#36827;&#34892;&#37197;&#32622;&#12290;&#23450;&#21046;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;EvolveMT&#22312;&#25104;&#26412;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#20174;&#25152;&#26377;&#32763;&#35793;&#20013;&#36873;&#25321;&#27599;&#20010;&#27573;&#33853;&#30340;&#26368;&#20339;&#32763;&#35793;&#30340;MT&#36136;&#37327;&#20272;&#35745;&#22120;&#25152;&#23454;&#29616;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;EvolveMT&#26159;&#31532;&#19968;&#20010;&#20165;&#36890;&#36807;&#20351;&#29992;&#33258;&#36523;&#25913;&#36827;&#30340;&#38598;&#25104;MT&#24341;&#25806;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents EvolveMT for efficiently combining multiple machine translation (MT) engines. The proposed system selects the output from a single engine for each segment by utilizing online learning techniques to predict the most suitable system for every translation request. A neural quality estimation metric supervises the method without requiring reference translations. The online learning capability of this system allows for dynamic adaptation to alterations in the domain or machine translation engines, thereby obviating the necessity for additional training. EvolveMT selects a subset of translation engines to be called based on the source sentence features. The degree of exploration is configurable according to the desired quality-cost trade-off. Results from custom datasets demonstrate that EvolveMT achieves similar translation accuracy at a lower cost than selecting the best translation of each segment from all translations using an MT quality estimator. To our knowledge, E
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; RLGF&#65292;&#29992;&#20110;&#22312; GPT-3 &#31561;&#21160;&#24577;&#40657;&#21283;&#23376;&#25351;&#23548;&#19979;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; LLM &#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#36890;&#29992; RL &#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312; IMDB &#21644; CommonGen &#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.11816</link><description>&lt;p&gt;
&#23398;&#20250;&#29983;&#25104;&#27604;&#20320;&#30340;LMM&#26356;&#22909;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Better Than Your LLM. (arXiv:2306.11816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; RLGF&#65292;&#29992;&#20110;&#22312; GPT-3 &#31561;&#21160;&#24577;&#40657;&#21283;&#23376;&#25351;&#23548;&#19979;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; LLM &#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#36890;&#29992; RL &#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312; IMDB &#21644; CommonGen &#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;(RL)&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#20363;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#12290;&#29305;&#21035;&#22320;&#65292;&#26368;&#36817;&#30340;LLM&#65292;&#22914;ChatGPT&#21644;GPT - 4&#33021;&#22815;&#19982;&#29992;&#25143;&#36827;&#34892;&#27969;&#30021;&#30340;&#23545;&#35805;&#65292;&#24182;&#34701;&#21512;&#20102;RL&#21644;&#20154;&#31867;&#21453;&#39304;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#23398;&#20064;&#25628;&#32034;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#29983;&#25104;&#30340;&#20851;&#38190;&#29305;&#24615;&#65292;&#25506;&#32034;&#20102;&#36229;&#20986;&#36890;&#29992;RL&#31639;&#27861;&#22914;PPO&#20043;&#22806;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;RL&#31639;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#21160;&#24577;&#40657;&#21283;&#23376;&#30340;&#25351;&#23548;LLM&#22914;GPT-3&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#24341;&#23548;&#21453;&#39304;&#30340;RL(RLGF)&#65292;&#36825;&#26159;&#19968;&#22871;&#29992;&#20110;LLM&#24494;&#35843;&#30340;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;GRUE&#22522;&#20934;&#27979;&#35797;&#30340;IMDB&#27491;&#21521;&#35780;&#35770;&#21644;CommonGen&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;RL&#31639;&#27861;&#27604;&#30417;&#30563;&#23398;&#20064;(SL)&#21644;&#40664;&#35748;PPO&#22522;&#32447;&#34920;&#29616;&#26356;&#39640;&#65292;&#35777;&#26126;&#20102;&#19982;&#25351;&#23548;LLM&#20114;&#21160;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning Large Language Models (LLMs) for conditional text generation. In particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with users by incorporating RL and feedback from humans. Inspired by learning-to-search algorithms and capitalizing on key properties of text generation, we seek to investigate reinforcement learning algorithms beyond general purpose algorithms such as Proximal policy optimization (PPO). In particular, we extend RL algorithms to allow them to interact with a dynamic black-box guide LLM such as GPT-3 and propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM fine-tuning. We experiment on the IMDB positive review and CommonGen text generation task from the GRUE benchmark. We show that our RL algorithms achieve higher performance than supervised learning (SL) and default PPO baselines, demonstrating the benefit of interaction with the guide LLM. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#21482;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26032;&#35789;&#27719;&#21450;&#20854;&#35270;&#35273;&#34920;&#31034;&#30340;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#65292;&#21487;&#24212;&#29992;&#20110;Yoruba&#31561;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.11371</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#22522;&#30784;&#23569;&#37327;&#31034;&#20363;&#35789;&#27719;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Visually grounded few-shot word learning in low-resource settings. (arXiv:2306.11371v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#21482;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26032;&#35789;&#27719;&#21450;&#20854;&#35270;&#35273;&#34920;&#31034;&#30340;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#65292;&#21487;&#24212;&#29992;&#20110;Yoruba&#31561;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#65292;&#22312;&#21482;&#26377;&#23569;&#37327;&#35789;-&#22270;&#20687;&#23454;&#20363;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#21333;&#35789;&#21450;&#20854;&#35270;&#35273;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#33258;&#28982;&#35789;-&#22270;&#20687;&#23545;&#19978;&#24037;&#20316;&#65292;&#20294;&#20351;&#29992;&#26356;&#23569;&#30340;&#31034;&#20363;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#23454;&#38469;&#20302;&#36164;&#28304;&#35821;&#35328;Yoruba&#30340;&#22810;&#27169;&#24577;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a visually grounded speech model that learns new words and their visual depictions from just a few word-image example pairs. Given a set of test images and a spoken query, we ask the model which image depicts the query word. Previous work has simplified this few-shot learning problem by either using an artificial setting with digit word-image pairs or by using a large number of examples per class. Moreover, all previous studies were performed using English speech-image data. We propose an approach that can work on natural word-image pairs but with less examples, i.e. fewer shots, and then illustrate how this approach can be applied for multimodal few-shot learning in a real low-resource language, Yoruba. Our approach involves using the given word-image example pairs to mine new unsupervised word-image training pairs from large collections of unlabelledspeech and images. Additionally, we use a word-to-image attention mechanism to determine word-image similarity. With this new
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20960;&#31181;FSL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#25351;&#20986;&#65292;&#19982;&#20840;&#24494;&#35843;&#27169;&#22411;&#30456;&#27604;&#65292;&#32431;FSL&#27169;&#22411;&#38754;&#23545;&#23545;&#25239;&#25200;&#21160;&#20250;&#24102;&#26469;&#19981;&#21487;&#24573;&#35270;&#30340;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#12290;&#20294;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#25552;&#31034;&#25110;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#32988;&#36807;&#20840;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.11066</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding. (arXiv:2306.11066v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20960;&#31181;FSL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#25351;&#20986;&#65292;&#19982;&#20840;&#24494;&#35843;&#27169;&#22411;&#30456;&#27604;&#65292;&#32431;FSL&#27169;&#22411;&#38754;&#23545;&#23545;&#25239;&#25200;&#21160;&#20250;&#24102;&#26469;&#19981;&#21487;&#24573;&#35270;&#30340;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#12290;&#20294;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#25552;&#31034;&#25110;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#32988;&#36807;&#20840;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;Few-shot learning&#65292;FSL&#65289;&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;FSL&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#20110;&#23545;&#25239;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20840;&#24494;&#35843;&#27169;&#22411;&#65292;&#32431;FSL&#26041;&#27861;&#38754;&#23545;&#23545;&#25239;&#25200;&#21160;&#20250;&#20986;&#29616;&#26126;&#26174;&#30340;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#25552;&#31034;&#29983;&#25104;&#25110;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;FSL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#29978;&#33267;&#36229;&#36807;&#20840;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#31867;&#22411;&#36873;&#25321;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#19978;&#20063;&#26377;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art few-shot learning (FSL) methods leverage prompt-based fine-tuning to obtain remarkable results for natural language understanding (NLU) tasks. While much of the prior FSL methods focus on improving downstream task performance, there is a limited understanding of the adversarial robustness of such methods. In this work, we conduct an extensive study of several state-of-the-art FSL methods to assess their robustness to adversarial perturbations. To better understand the impact of various factors towards robustness (or the lack of it), we evaluate prompt-based FSL methods against fully fine-tuned models for aspects such as the use of unlabeled data, multiple prompts, number of few-shot examples, model size and type. Our results on six GLUE tasks indicate that compared to fully fine-tuned models, vanilla FSL methods lead to a notable relative drop in task performance (i.e., are less robust) in the face of adversarial perturbations. However, using (i) unlabeled data for pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31185;&#25216;&#20889;&#20316;&#25903;&#25345;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#21253;&#25324;&#35780;&#20272;&#21477;&#23376;&#31185;&#23398;&#24615;&#12289;&#21010;&#20998;&#27573;&#33853;&#21644;&#28070;&#33394;&#24314;&#35758;&#31561;&#21151;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#20559;&#35265;&#27979;&#35797;&#21644;&#19978;&#19979;&#25991;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.10974</link><description>&lt;p&gt;
&#38754;&#21521;&#31185;&#25216;&#20889;&#20316;&#25903;&#25345;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models for Scientific Writing Support. (arXiv:2306.10974v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31185;&#25216;&#20889;&#20316;&#25903;&#25345;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#21253;&#25324;&#35780;&#20272;&#21477;&#23376;&#31185;&#23398;&#24615;&#12289;&#21010;&#20998;&#27573;&#33853;&#21644;&#28070;&#33394;&#24314;&#35758;&#31561;&#21151;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#20559;&#35265;&#27979;&#35797;&#21644;&#19978;&#19979;&#25991;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;&#31185;&#25216;&#20316;&#32773;&#25552;&#20379;&#31185;&#25216;&#24615;&#21028;&#26029;&#65292;&#21010;&#20998;&#27573;&#33853;&#65292;&#20197;&#21450;&#25552;&#20379;&#28070;&#33394;&#24314;&#35758;&#30340;&#36741;&#21161;&#21151;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22238;&#24402;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#26681;&#25454;&#20174;&#21516;&#34892;&#35780;&#23457;&#30340;&#31185;&#25216;&#35770;&#25991;&#21644;&#38750;&#31185;&#25216;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#31185;&#25216;&#21477;&#23376;&#35821;&#26009;&#35757;&#32451;&#24471;&#21040;&#30340;&#65292;&#29992;&#20110;&#35780;&#20272;&#19968;&#20010;&#21477;&#23376;&#30340;&#31185;&#23398;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26041;&#31243;&#24335;&#21644;&#24341;&#25991;&#23545;&#35813;&#35780;&#20998;&#30340;&#24433;&#21709;&#65292;&#20197;&#27979;&#35797;&#35813;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#20559;&#35265;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26144;&#23556;&#65292;&#23558;AI&#21644;&#26426;&#22120;&#23398;&#20064;&#26631;&#20934;&#35770;&#25991;&#24067;&#23616;&#20013;&#30340;&#26631;&#39064;&#26144;&#23556;&#21040;&#19968;&#20010;&#26368;&#21487;&#33021;&#30340;&#27573;&#33853;&#65292;&#20197;&#23558;&#21477;&#23376;&#20998;&#31867;&#21040;&#20854;&#25152;&#23646;&#30340;&#27573;&#33853;&#20013;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23545;&#27573;&#33853;&#20998;&#31867;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28070;&#33394;&#22120;&#65292;&#23427;&#20026;&#32473;&#23450;&#30340;&#21477;&#23376;&#25552;&#20379;&#20102;&#26367;&#20195;&#26041;&#26696;&#65292;&#21253;&#25324;&#21333;&#35789;&#26367;&#25442;&#12289;&#28155;&#21152;&#35821;&#21477;&#21644;&#32467;&#26500;&#21464;&#21270;&#65292;&#20197;&#25913;&#21892;&#20889;&#20316;&#39118;&#26684;&#12290;&#25105;&#20204;&#22312;&#21477;&#23376;&#32423;&#21035;&#23545;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We support scientific writers in determining whether a written sentence is scientific, to which section it belongs, and suggest paraphrasings to improve the sentence. Firstly, we propose a regression model trained on a corpus of scientific sentences extracted from peer-reviewed scientific papers and non-scientific text to assign a score that indicates the scientificness of a sentence. We investigate the effect of equations and citations on this score to test the model for potential biases. Secondly, we create a mapping of section titles to a standard paper layout in AI and machine learning to classify a sentence to its most likely section. We study the impact of context, i.e., surrounding sentences, on the section classification performance. Finally, we propose a paraphraser, which suggests an alternative for a given sentence that includes word substitutions, additions to the sentence, and structural changes to improve the writing style. We train various large language models on senten
&lt;/p&gt;</description></item><item><title>BayLing&#26159;&#19968;&#20010;&#25351;&#20196;&#36981;&#24490;&#30340;LLM&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#32763;&#35793;&#23558;&#35821;&#35328;&#29983;&#25104;&#21644;&#25351;&#20196;&#36981;&#24490;&#30340;&#33021;&#21147;&#20174;&#33521;&#35821;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#65292;&#20351;&#24471;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#20063;&#33021;&#22815;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;LLMs&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#25968;&#25454;&#25110;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2306.10968</link><description>&lt;p&gt;
BayLing&#65306;&#36890;&#36807;&#20132;&#20114;&#24335;&#32763;&#35793;&#36830;&#25509;&#36328;&#35821;&#35328;&#23545;&#40784;&#21644;&#25351;&#20196;&#36861;&#36394;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models. (arXiv:2306.10968v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10968
&lt;/p&gt;
&lt;p&gt;
BayLing&#26159;&#19968;&#20010;&#25351;&#20196;&#36981;&#24490;&#30340;LLM&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#32763;&#35793;&#23558;&#35821;&#35328;&#29983;&#25104;&#21644;&#25351;&#20196;&#36981;&#24490;&#30340;&#33021;&#21147;&#20174;&#33521;&#35821;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#65292;&#20351;&#24471;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#20063;&#33021;&#22815;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;LLMs&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#25968;&#25454;&#25110;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#20174;&#22522;&#30784;LLMs&#21457;&#23637;&#20026;&#25351;&#20196;&#36861;&#36394;LLMs&#65292;&#25351;&#20196;&#35843;&#20248;&#22312;&#23558;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#36890;&#24120;&#19987;&#27880;&#20110;&#33521;&#35821;&#65292;&#23548;&#33268;&#38750;&#33521;&#35821;&#35821;&#35328;&#34920;&#29616;&#36739;&#24046;&#12290;&#20026;&#20102;&#25552;&#39640;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#24615;&#33021;&#65292;&#38656;&#35201;&#25910;&#38598;&#22522;&#30784;LLMs&#30340;&#35821;&#35328;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#35821;&#35328;&#29305;&#23450;&#30340;&#25351;&#20196;&#20197;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#65292;&#36825;&#20004;&#32773;&#37117;&#26159;&#27785;&#37325;&#30340;&#36127;&#25285;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#20154;&#31867;&#24037;&#20316;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20132;&#20114;&#24335;&#32763;&#35793;&#20219;&#21153;&#23558;&#35821;&#35328;&#29983;&#25104;&#21644;&#25351;&#20196;&#36981;&#24490;&#30340;&#33021;&#21147;&#20174;&#33521;&#35821;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;BayLing&#65292;&#19968;&#31181;&#25351;&#20196;&#36981;&#24490;LLM&#65292;&#21033;&#29992;LLaMA&#20316;&#20026;&#22522;&#30784;LLM&#65292;&#33258;&#21160;&#26500;&#24314;&#20132;&#20114;&#24335;&#32763;&#35793;&#25351;&#20196;&#20197;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#12290;&#23545;&#33521;&#35821;&#12289;&#20013;&#25991;&#21644;&#35199;&#29677;&#29273;&#35821;&#19977;&#31181;&#35821;&#35328;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;BayLing&#22312;&#19981;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#25968;&#25454;&#25110;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;LLMs&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable prowess in language understanding and generation. Advancing from foundation LLMs to instructionfollowing LLMs, instruction tuning plays a vital role in aligning LLMs to human preferences. However, the existing LLMs are usually focused on English, leading to inferior performance in non-English languages. In order to improve the performance for non-English languages, it is necessary to collect language-specific training data for foundation LLMs and construct language-specific instructions for instruction tuning, both of which are heavy loads. To minimize human workload, we propose to transfer the capabilities of language generation and instruction following from English to other languages through an interactive translation task. We have developed BayLing, an instruction-following LLM by utilizing LLaMA as the foundation LLM and automatically constructing interactive translation instructions for instructing tuning. Extensive assess
&lt;/p&gt;</description></item><item><title>ChatGPT&#20351;&#29992;&#22312;&#36234;&#21335;&#39640;&#20013;&#27605;&#19994;&#32771;&#35797;&#20013;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#27700;&#24179;&#65292;&#23637;&#31034;&#20102;AI&#21160;&#21147;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09170</link><description>&lt;p&gt;
ChatGPT &#33021;&#36890;&#36807;&#36234;&#21335;&#39640;&#20013;&#27605;&#19994;&#32771;&#35797;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT pass the Vietnamese National High School Graduation Examination?. (arXiv:2306.09170v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09170
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20351;&#29992;&#22312;&#36234;&#21335;&#39640;&#20013;&#27605;&#19994;&#32771;&#35797;&#20013;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#27700;&#24179;&#65292;&#23637;&#31034;&#20102;AI&#21160;&#21147;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25991;&#31456;&#24378;&#35843;&#20102; AI &#21160;&#21147;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#25945;&#32946;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20171;&#32461;&#20102;&#20351;&#29992; ChatGPT&#65292;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23436;&#25104;&#36234;&#21335;&#39640;&#20013;&#27605;&#19994;&#32771;&#35797;&#65288;VNHSGE&#65289;&#30340;&#32467;&#26524;&#12290;&#30740;&#31350;&#25968;&#25454;&#38598;&#21253;&#25324;&#25991;&#23398;&#27979;&#35797;&#26696;&#20363;&#20013;&#30340; 30 &#31687;&#25991;&#31456;&#21644;&#35774;&#35745;&#32473;&#20854;&#20182;&#31185;&#30446;&#30340; 1,700 &#36947;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#32467;&#26524;&#26174;&#31034; ChatGPT &#33021;&#22815;&#36890;&#36807;&#32771;&#35797;&#65292;&#24182;&#24471;&#21040;&#20102;&#24179;&#22343; 6-7 &#20998;&#30340;&#25104;&#32489;&#65292;&#23637;&#31034;&#20102;&#36825;&#39033;&#25216;&#26415;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290; ChatGPT &#34920;&#29616;&#30340;&#20998;&#26512;&#26174;&#31034;&#20854;&#22312;&#21253;&#25324;&#25968;&#23398;&#12289;&#33521;&#35821;&#12289;&#29289;&#29702;&#12289;&#21270;&#23398;&#12289;&#29983;&#29289;&#12289;&#21382;&#21490;&#12289;&#22320;&#29702;&#12289;&#20844;&#27665;&#25945;&#32946;&#21644;&#25991;&#23398;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#23398;&#31185;&#20013;&#37117;&#24456;&#29087;&#32451;&#65292;&#36825;&#34920;&#26126;&#20854;&#25903;&#25345;&#23398;&#20064;&#32773;&#20855;&#26377;&#28508;&#21147;&#12290;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350; ChatGPT &#22312;&#26356;&#22797;&#26434;&#30340;&#32771;&#35797;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#20197;&#21450;&#20854;&#25903;&#25345;&#19981;&#21516;&#32972;&#26223;&#23398;&#20064;&#32773;&#30340;&#28508;&#21147;&#12290;&#38543;&#30528;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;AI &#21160;&#21147;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#26159;&#24191;&#27867;&#32780;&#26377;&#21069;&#26223;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research article highlights the potential of AI-powered chatbots in education and presents the results of using ChatGPT, a large language model, to complete the Vietnamese National High School Graduation Examination (VNHSGE). The study dataset included 30 essays in the literature test case and 1,700 multiple-choice questions designed for other subjects. The results showed that ChatGPT was able to pass the examination with an average score of 6-7, demonstrating the technology's potential to revolutionize the educational landscape. The analysis of ChatGPT performance revealed its proficiency in a range of subjects, including mathematics, English, physics, chemistry, biology, history, geography, civic education, and literature, which suggests its potential to provide effective support for learners. However, further research is needed to assess ChatGPT performance on more complex exam questions and its potential to support learners in different contexts. As technology continues to evo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21830;&#19994;&#29615;&#22659;&#30340;&#12289;&#33021;&#22815;&#24179;&#34913;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102; $\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$ &#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#23545;&#35805;&#31995;&#32479;&#21709;&#24212;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03361</link><description>&lt;p&gt;
&#35774;&#35745;&#29992;&#25143;&#35282;&#33394;&#24863;&#30693;&#30340;&#23545;&#35805;&#20195;&#29702;&#36827;&#34892;&#26377;&#36259;&#30340;&#23545;&#35805;&#65306;$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground
&lt;/p&gt;
&lt;p&gt;
$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue. (arXiv:2306.03361v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21830;&#19994;&#29615;&#22659;&#30340;&#12289;&#33021;&#22815;&#24179;&#34913;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102; $\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$ &#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#23545;&#35805;&#31995;&#32479;&#21709;&#24212;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#31435;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20197;&#35299;&#20915;&#21830;&#19994;&#35774;&#32622;&#20013;&#28041;&#21450;&#20010;&#24615;&#21270;&#23545;&#35805;&#21709;&#24212;&#19982;&#38750;&#27491;&#24335;&#21709;&#24212;&#20132;&#26367;&#30340;$\textit{WWH}$&#65288;$\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$&#65289;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20197;&#24212;&#23545;&#20010;&#24615;&#21270;&#12289;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;$\textit{WWH}$&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#21709;&#24212;&#31867;&#22411;&#26631;&#31614;&#26469;&#25552;&#39640;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#32452;&#21512;&#23548;&#33268;&#20102;&#26356;&#21152;&#27969;&#30021;&#30340;&#23545;&#35805;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#20027;&#35266;&#20154;&#31867;&#35780;&#20272;&#21644;&#23458;&#35266;&#35780;&#20272;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for building a personalized open-domain dialogue system to address the $\textit{WWH}$ ($\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of $\textit{WWH}$ in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#24178;&#20928;&#30340;OpenPI-C&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22522;&#20110;&#32858;&#31867;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#27169;&#22411;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#23454;&#20307;&#24863;&#30693;&#33021;&#21147;&#26469;&#25552;&#39640;&#24320;&#25918;&#35789;&#27719;&#29366;&#24577;&#36319;&#36394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00887</link><description>&lt;p&gt;
OpenPI-C&#65306;&#24320;&#25918;&#35789;&#27719;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#22909;&#22522;&#20934;&#21644;&#26356;&#24378;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
OpenPI-C: A Better Benchmark and Stronger Baseline for Open-Vocabulary State Tracking. (arXiv:2306.00887v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#24178;&#20928;&#30340;OpenPI-C&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22522;&#20110;&#32858;&#31867;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#27169;&#22411;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#23454;&#20307;&#24863;&#30693;&#33021;&#21147;&#26469;&#25552;&#39640;&#24320;&#25918;&#35789;&#27719;&#29366;&#24577;&#36319;&#36394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#29366;&#24577;&#36319;&#36394;&#26159;&#19968;&#31181;&#22312;&#19981;&#38480;&#21046;&#29366;&#24577;&#31354;&#38388;&#21644;&#23454;&#20307;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#36319;&#36394;&#23454;&#20307;&#29366;&#24577;&#21464;&#21270;&#30340;&#26356;&#23454;&#29992;&#30340;&#29366;&#24577;&#36319;&#36394;&#26041;&#27861;&#12290;&#30446;&#21069;&#65292;OpenPI&#26159;&#21807;&#19968;&#20026;&#24320;&#25918;&#35789;&#27719;&#29366;&#24577;&#36319;&#36394;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#35780;&#20272;&#25351;&#26631;&#23384;&#22312;&#38382;&#39064;&#12290;&#38024;&#23545;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20998;&#21035;&#22312;&#31243;&#24207;&#32423;&#21035;&#12289;&#27493;&#39588;&#32423;&#21035;&#21644;&#29366;&#24577;&#21464;&#21270;&#32423;&#21035;&#19978;&#20998;&#31867;&#20102;3&#31181;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22810;&#36718;&#20154;&#24037;&#21028;&#26029;&#26500;&#24314;&#20102;&#24178;&#20928;&#30340;OpenPI-C&#25968;&#25454;&#38598;&#12290;&#38024;&#23545;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#25351;&#26631;&#65292;&#20197;&#20462;&#22797;&#21407;&#22987;&#25351;&#26631;&#23545;&#37325;&#22797;&#30340;&#20559;&#22909;&#12290;&#22312;&#27169;&#22411;&#26041;&#38754;&#65292;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#29366;&#24577;&#36319;&#36394;&#30340;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#65292;&#21363;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#23454;&#20307;&#24863;&#30693;&#33021;&#21147;&#65292;&#22686;&#24378;&#20102;seq2seq&#29983;&#25104;&#22522;&#32447;&#27169;&#22411;&#12290;&#22312;&#25191;&#34892;&#25805;&#20316;&#21518;&#65292;&#19990;&#30028;&#29366;&#24577;&#20174;&#26681;&#26412;&#19978;&#20381;&#36182;&#20110;&#20808;&#21069;&#29366;&#24577;&#12290;&#25105;&#20204;&#36890;&#36807;&#21160;&#24577;&#35760;&#24518;&#24211;&#23545;&#36825;&#31181;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20801;&#35768;&#27169;&#22411;&#27880;&#24847;&#21040;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-vocabulary state tracking is a more practical version of state tracking that aims to track state changes of entities throughout a process without restricting the state space and entity space. OpenPI is to date the only dataset annotated for open-vocabulary state tracking. However, we identify issues with the dataset quality and evaluation metric. For the dataset, we categorize 3 types of problems on the procedure level, step level and state change level respectively, and build a clean dataset OpenPI-C using multiple rounds of human judgment. For the evaluation metric, we propose a cluster-based metric to fix the original metric's preference for repetition.  Model-wise, we enhance the seq2seq generation baseline by reinstating two key properties for state tracking: temporal dependency and entity awareness. The state of the world after an action is inherently dependent on the previous state. We model this dependency through a dynamic memory bank and allow the model to attend to the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#22686;&#24378;&#31639;&#27861;&#65292;&#20197;&#36866;&#24403;&#25913;&#21464;&#26032;&#22686;&#36755;&#20837;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#35823;&#23548;&#24615;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#23569;&#25968;&#32676;&#20307;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24635;&#20307;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16863</link><description>&lt;p&gt;
&#25511;&#21046;&#23398;&#20064;&#25928;&#24212;&#20197;&#38477;&#20302;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#30340;&#35823;&#23548;&#24615;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Controlling Learned Effects to Reduce Spurious Correlations in Text Classifiers. (arXiv:2305.16863v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16863
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#22686;&#24378;&#31639;&#27861;&#65292;&#20197;&#36866;&#24403;&#25913;&#21464;&#26032;&#22686;&#36755;&#20837;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#35823;&#23548;&#24615;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#23569;&#25968;&#32676;&#20307;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24635;&#20307;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;NLP&#20998;&#31867;&#22120;&#23398;&#20064;&#35757;&#32451;&#29305;&#24449;&#21644;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#35823;&#23548;&#24615;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#27169;&#22411;&#30340;&#39044;&#27979;&#23545;&#36825;&#20123;&#29305;&#24449;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#29305;&#24449;&#23545;&#30446;&#26631;&#26631;&#31614;&#26377;&#38750;&#38646;&#22240;&#26524;&#25928;&#24212;&#24182;&#19988;&#23545;&#39044;&#27979;&#24456;&#37325;&#35201;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#20135;&#29983;&#21453;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#22240;&#26524;&#25512;&#26029;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#25928;&#24212;&#35268;&#33539;&#21270;&#20026;&#29305;&#24449;&#23545;&#26631;&#31614;&#30340;&#20272;&#35745;&#25928;&#24212;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#29305;&#24449;&#30340;&#20272;&#35745;&#25928;&#24212;&#36866;&#24403;&#22320;&#25913;&#21464;&#26032;&#22686;&#30340;&#36755;&#20837;&#30340;&#26631;&#31614;&#12290;&#22312;&#27602;&#24615;&#21644;IMDB&#35780;&#35770;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#26368;&#23567;&#21270;&#20102;&#35823;&#23548;&#24615;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#23569;&#25968;&#32676;&#20307;&#65288;&#21363;&#25171;&#30772;&#35823;&#23548;&#24615;&#30456;&#20851;&#24615;&#30340;&#26679;&#26412;&#65289;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#24635;&#20307;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the problem of NLP classifiers learning spurious correlations between training features and target labels, a common approach is to make the model's predictions invariant to these features. However, this can be counter-productive when the features have a non-zero causal effect on the target label and thus are important for prediction. Therefore, using methods from the causal inference literature, we propose an algorithm to regularize the learnt effect of the features on the model's prediction to the estimated effect of feature on label. This results in an automated augmentation method that leverages the estimated effect of a feature to appropriately change the labels for new augmented inputs. On toxicity and IMDB review datasets, the proposed algorithm minimises spurious correlations and improves the minority group (i.e., samples breaking spurious correlations) accuracy, while also improving the total accuracy compared to standard training.
&lt;/p&gt;</description></item><item><title>ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09770</link><description>&lt;p&gt;
ConvXAI&#65306;&#36890;&#36807;&#23545;&#35805;&#25552;&#20379;&#24322;&#26500;&#30340;AI&#35299;&#37322;&#65292;&#25903;&#25345;&#20154;&#26426;&#31185;&#25216;&#20889;&#20316;
&lt;/p&gt;
&lt;p&gt;
ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09770
&lt;/p&gt;
&lt;p&gt;
ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#35299;&#37322;AI&#31995;&#32479;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21542;&#23545;&#20154;&#31867;&#23454;&#29992;&#20173;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#25913;&#21892;XAI&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#19968;&#31995;&#21015;&#30740;&#31350;&#30830;&#23450;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#29992;&#25143;&#38656;&#27714;&#19982;&#29616;&#26377;XAI&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#35774;&#24819;&#23558;&#22810;&#31181;XAI&#26041;&#27861;&#38598;&#25104;&#21040;&#36890;&#29992;XAI&#30028;&#38754;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#23545;&#35805;&#25110;GUI&#30340;XAI&#31995;&#32479;&#65289;&#20013;&#20197;&#20943;&#36731;&#36825;&#20123;&#24046;&#36317;&#65292;&#20294;&#32570;&#23569;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#35774;&#35745;&#20197;&#28385;&#36275;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvXAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#36890;&#36807;&#36890;&#29992;&#30340;XAI&#23545;&#35805;&#30028;&#38754;&#25552;&#20986;&#21508;&#31181;XAI&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#65288;&#21363;&#65292;&#22522;&#20110;&#26684;&#24335;&#30740;&#31350;&#30340;&#22235;&#20010;&#21407;&#21017;&#65289;&#23884;&#20837;ConvXAI&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#23450;&#20041;&#12289;&#21457;&#23637;&#21644;&#25361;&#25112;&#65292;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#26500;&#24314;&#26356;&#22909;&#24615;&#33021;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.07611</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis: A Survey. (arXiv:2305.07611v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#23450;&#20041;&#12289;&#21457;&#23637;&#21644;&#25361;&#25112;&#65292;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#26500;&#24314;&#26356;&#22909;&#24615;&#33021;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36825;&#39033;&#25216;&#26415;&#24050;&#32463;&#36798;&#21040;&#20102;&#26032;&#30340;&#39640;&#24230;&#12290;&#23427;&#22312;&#24212;&#29992;&#21644;&#30740;&#31350;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#25104;&#20026;&#20102;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#23450;&#20041;&#12289;&#32972;&#26223;&#21644;&#21457;&#23637;&#27010;&#36848;&#12290;&#23427;&#36824;&#28085;&#30422;&#20102;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#20808;&#36827;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#35813;&#25216;&#26415;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21069;&#26223;&#12290;&#26368;&#21518;&#65292;&#23427;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#38656;&#35201;&#25351;&#20986;&#30340;&#26159;&#65292;&#26412;&#32508;&#36848;&#20026;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#26500;&#24314;&#26356;&#22909;&#24615;&#33021;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#25552;&#20379;&#20102;&#24314;&#35774;&#24615;&#30340;&#24314;&#35758;&#65292;&#26377;&#21161;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sentiment analysis has become an important research area in the field of artificial intelligence. With the latest advances in deep learning, this technology has reached new heights. It has great potential for both application and research, making it a popular research topic. This review provides an overview of the definition, background, and development of multimodal sentiment analysis. It also covers recent datasets and advanced models, emphasizing the challenges and future prospects of this technology. Finally, it looks ahead to future research directions. It should be noted that this review provides constructive suggestions for promising research directions and building better performing multimodal sentiment analysis models, which can help researchers in this field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;GPT&#22312;&#23721;&#22303;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30528;&#37325;&#35752;&#35770;&#21450;&#26102;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#23721;&#22303;&#24037;&#31243;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2304.02138</link><description>&lt;p&gt;
&#22303;&#26408;&#40550;&#40521;&#20256;&#22855;(GPT)&#65306;&#21033;&#29992;&#21450;&#26102;&#24037;&#31243;&#20811;&#26381;GPT&#24187;&#35273;&#20197;&#22312;&#23721;&#22303;&#24037;&#31243;&#20013;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Geotechnical Parrot Tales (GPT): Overcoming GPT hallucinations with prompt engineering for geotechnical applications. (arXiv:2304.02138v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;GPT&#22312;&#23721;&#22303;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30528;&#37325;&#35752;&#35770;&#21450;&#26102;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#23721;&#22303;&#24037;&#31243;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26222;&#21450;&#65292;&#22914;OpenAI&#30340;ChatGPT&#65292;&#21487;&#33021;&#20250;&#24443;&#24213;&#25913;&#21464;&#21253;&#25324;&#23721;&#22303;&#24037;&#31243;&#22312;&#20869;&#30340;&#21508;&#20010;&#34892;&#19994;&#12290; &#20294;&#26159;&#65292;GPT&#27169;&#22411;&#26377;&#26102;&#20250;&#29983;&#25104;&#21548;&#36215;&#26469;&#24456;&#26377;&#36947;&#29702;&#20294;&#38169;&#35823;&#30340;&#36755;&#20986;&#65292;&#23548;&#33268;&#24187;&#35273;&#20135;&#29983;&#12290; &#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#32531;&#35299;&#36825;&#20123;&#39118;&#38505;&#21644;&#21033;&#29992;GPT&#22312;&#23721;&#22303;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#20840;&#37096;&#28508;&#21147;&#26041;&#38754;&#65292;&#21450;&#26102;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290; &#25105;&#20204;&#25506;&#35752;&#20102;&#19982;LLM&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#38519;&#38449;&#65292;&#24182;&#24378;&#35843;&#20102;&#19978;&#19979;&#25991;&#22312;&#30830;&#20445;&#20934;&#30830;&#21644;&#26377;&#20215;&#20540;&#30340;&#21709;&#24212;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#29305;&#23450;&#20110;&#19978;&#19979;&#25991;&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#24320;&#21457;&#20197;&#21450;LLM&#25104;&#20026;&#22797;&#26434;&#20219;&#21153;&#65288;&#20363;&#22914;&#25968;&#25454;&#20998;&#26512;&#21644;&#35774;&#35745;&#65289;&#30340;&#33258;&#28982;&#30028;&#38754;&#30340;&#28508;&#21147;&#12290; &#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#23721;&#22303;&#24037;&#31243;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290; &#36890;&#36807;&#23558;GPT&#38598;&#25104;&#21040;&#23721;&#22303;&#24037;&#31243;&#24037;&#20316;&#27969;&#20013;&#65292;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#31616;&#21270;&#20182;&#20204;&#30340;&#24037;&#20316;&#24182;&#21457;&#23637;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of large language models (LLMs), such as OpenAI's ChatGPT, could revolutionized various industries, including geotechnical engineering. However, GPT models can sometimes generate plausible-sounding but false outputs, leading to hallucinations. In this article, we discuss the importance of prompt engineering in mitigating these risks and harnessing the full potential of GPT for geotechnical applications. We explore the challenges and pitfalls associated with LLMs and highlight the role of context in ensuring accurate and valuable responses. Furthermore, we examine the development of context-specific search engines and the potential of LLMs to become a natural interface for complex tasks, such as data analysis and design. We also develop a unified interface using natural language to handle complex geotechnical engineering tasks and data analysis. By integrating GPT into geotechnical engineering workflows, professionals can streamline their work and develop sustain
&lt;/p&gt;</description></item><item><title>PATCorrect&#26159;&#19968;&#31181;&#22522;&#20110;&#38899;&#32032;&#22686;&#24378;&#30340;&#38750;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#38899;&#32032;&#27169;&#24577;&#30340;&#34920;&#31034;&#26469;&#26368;&#22823;&#38480;&#24230;&#22320;&#38477;&#20302;ASR&#31995;&#32479;&#20013;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#65292;&#24182;&#22312;&#20302;&#24310;&#36831;&#38656;&#27714;&#30340;&#23454;&#38469;&#29983;&#20135;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.05040</link><description>&lt;p&gt;
PATCorrect&#65306;&#22522;&#20110;&#38899;&#32032;&#22686;&#24378;&#30340;&#38750;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#29992;&#20110;ASR&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction. (arXiv:2302.05040v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05040
&lt;/p&gt;
&lt;p&gt;
PATCorrect&#26159;&#19968;&#31181;&#22522;&#20110;&#38899;&#32032;&#22686;&#24378;&#30340;&#38750;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#38899;&#32032;&#27169;&#24577;&#30340;&#34920;&#31034;&#26469;&#26368;&#22823;&#38480;&#24230;&#22320;&#38477;&#20302;ASR&#31995;&#32479;&#20013;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#65292;&#24182;&#22312;&#20302;&#24310;&#36831;&#38656;&#27714;&#30340;&#23454;&#38469;&#29983;&#20135;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#31995;&#32479;&#20135;&#29983;&#30340;&#35821;&#38899;&#21040;&#25991;&#26412;&#38169;&#35823;&#20250;&#23545;&#19979;&#28216;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#38169;&#35823;&#20462;&#27491;&#27169;&#22411;&#20316;&#20026;&#21518;&#22788;&#29702;&#25991;&#26412;&#32534;&#36753;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;ASR&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26410;&#23545;&#31526;&#21512;&#24037;&#19994;&#32423;&#29983;&#20135;&#31995;&#32479;&#20302;&#24310;&#36831;&#38656;&#27714;&#30340;&#39640;&#25928;&#27169;&#22411;&#36827;&#34892;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PATCorrect&#36825;&#19968;&#26032;&#22411;&#38750;&#33258;&#22238;&#24402;(NAR)&#26041;&#27861;&#65292;&#22522;&#20110;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#38899;&#32032;&#27169;&#24577;&#30340;&#34920;&#31034;&#65292;&#20197;&#38477;&#20302;&#21333;&#35789;&#38169;&#35823;&#29575;(WER)&#65292;&#24182;&#22312;&#19981;&#21516;&#36136;&#37327;&#30340;&#36755;&#20837;&#36716;&#24405;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;PATCorrect&#22312;&#33521;&#35821;&#35821;&#26009;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;NAR&#26041;&#27861;&#65292;&#19982;&#20165;&#20351;&#29992;&#25991;&#26412;&#27169;&#24577;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#24635;&#20307;WER&#38477;&#20302;&#20102;11.62%&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#30340;WER&#38477;&#20302;&#29575;&#20026;9.46%&#12290;&#27492;&#22806;&#65292;&#20854;&#25512;&#29702;&#24310;&#36831;&#22312;&#27627;&#31186;&#32423;&#21035;&#65292;&#38750;&#24120;&#36866;&#21512;&#38656;&#35201;&#20302;&#24310;&#36831;&#31995;&#32479;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-to-text errors made by automatic speech recognition (ASR) systems negatively impact downstream models. Error correction models as a post-processing text editing method have been recently developed for refining the ASR outputs. However, efficient models that meet the low latency requirements of industrial grade production systems have not been well studied. We propose PATCorrect-a novel non-autoregressive (NAR) approach based on multi-modal fusion leveraging representations from both text and phoneme modalities, to reduce word error rate (WER) and perform robustly with varying input transcription quality. We demonstrate that PATCorrect consistently outperforms state-of-the-art NAR method on English corpus across different upstream ASR systems, with an overall 11.62% WER reduction (WERR) compared to 9.46% WERR achieved by other methods using text only modality. Besides, its inference latency is at tens of milliseconds, making it ideal for systems with low latency requirements.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#25512;&#29305;&#25968;&#25454;&#36827;&#34892;&#36873;&#20030;&#32467;&#26524;&#39044;&#27979;&#65292;&#32467;&#26524;&#34920;&#26126;&#24102;&#26377;&#22320;&#29702;&#23646;&#24615;&#30340;&#25968;&#25454;&#27169;&#22411;&#27604;&#20256;&#32479;&#27665;&#24847;&#35843;&#26597;&#26041;&#27861;&#26356;&#33021;&#20934;&#30830;&#39044;&#27979;&#36873;&#20030;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.00626</link><description>&lt;p&gt;
&#22522;&#20110;&#25512;&#29305;&#30340;&#36873;&#20030;&#27169;&#22411;&#22312;2021&#24180;&#22696;&#35199;&#21733;&#31435;&#27861;&#36873;&#20030;&#20013;&#30340;&#35774;&#35745;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Design and analysis of tweet-based election models for the 2021 Mexican legislative election. (arXiv:2301.00626v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00626
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#25512;&#29305;&#25968;&#25454;&#36827;&#34892;&#36873;&#20030;&#32467;&#26524;&#39044;&#27979;&#65292;&#32467;&#26524;&#34920;&#26126;&#24102;&#26377;&#22320;&#29702;&#23646;&#24615;&#30340;&#25968;&#25454;&#27169;&#22411;&#27604;&#20256;&#32479;&#27665;&#24847;&#35843;&#26597;&#26041;&#27861;&#26356;&#33021;&#20934;&#30830;&#39044;&#27979;&#36873;&#20030;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#27169;&#25311;&#21644;&#39044;&#27979;&#30495;&#23454;&#20154;&#31867;&#34892;&#20026;&#26159;&#25919;&#27835;&#12289;&#25919;&#24220;&#12289;&#23398;&#26415;&#30028;&#21644;&#20135;&#19994;&#30028;&#24863;&#20852;&#36259;&#30340;&#31215;&#26497;&#23581;&#35797;&#12290;&#33258;2006&#24180;&#21019;&#24314;&#20197;&#26469;&#65292;Twitter&#19968;&#30452;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#28508;&#22312;&#30340;&#23454;&#39564;&#23460;&#65292;&#21487;&#29992;&#20110;&#34913;&#37327;&#21644;&#39044;&#27979;&#31038;&#20250;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;2021&#24180;&#22696;&#35199;&#21733;&#31435;&#27861;&#36873;&#20030;&#30340;&#29992;&#25143;&#22522;&#30784;&#65292;&#20351;&#29992;&#20102;&#19968;&#32452;&#22312;&#36873;&#20030;&#26085;&#21069;&#20845;&#20010;&#26376;&#20869;&#28041;&#21450;&#36873;&#20030;&#30340;1500&#19975;&#26465;&#25512;&#25991;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#36873;&#20030;&#27169;&#22411;&#65292;&#23558;&#25919;&#27835;&#20559;&#22909;&#20998;&#37197;&#32473;&#25191;&#25919;&#20826;&#25110;&#21453;&#23545;&#27966;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#24102;&#26377;&#22320;&#29702;&#23646;&#24615;&#30340;&#25968;&#25454;&#30340;&#27169;&#22411;&#27604;&#20256;&#32479;&#30340;&#27665;&#24847;&#35843;&#26597;&#26041;&#27861;&#26356;&#33021;&#20934;&#30830;&#22320;&#30830;&#23450;&#36873;&#20030;&#32467;&#26524;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#65292;&#22312;&#32447;&#20844;&#20849;&#25968;&#25454;&#30340;&#20998;&#26512;&#21487;&#20197;&#20248;&#20110;&#20256;&#32479;&#30340;&#27665;&#24847;&#35843;&#26597;&#26041;&#27861;&#26469;&#39044;&#27979;&#36873;&#20030;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling and forecasting real-life human behaviour using online social media is an active endeavour of interest in politics, government, academia, and industry. Since its creation in 2006, Twitter has been proposed as a potential laboratory that could be used to gauge and predict social behaviour. During the last decade, the user base of Twitter has been growing and becoming more representative of the general population. Here we analyse this user base in the context of the 2021 Mexican Legislative Election. To do so, we use a dataset of 15 million election-related tweets in the six months preceding election day. We explore different election models that assign political preference to either the ruling parties or the opposition. We find that models using data with geographical attributes determine the results of the election with better precision and accuracy than conventional polling methods. These results demonstrate that analysis of public online data can outperform conventional pol
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VRDU&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#26356;&#20840;&#38754;&#22320;&#21453;&#26144;&#23454;&#38469;&#25991;&#26723;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20016;&#23500;&#27169;&#24335;&#12289;&#22797;&#26434;&#27169;&#26495;&#21644;&#22810;&#26679;&#30340;&#24067;&#23616;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26723;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.15421</link><description>&lt;p&gt;
VRDU&#65306;&#38754;&#21521;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#29702;&#35299;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
VRDU: A Benchmark for Visually-rich Document Understanding. (arXiv:2211.15421v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VRDU&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#26356;&#20840;&#38754;&#22320;&#21453;&#26144;&#23454;&#38469;&#25991;&#26723;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20016;&#23500;&#27169;&#24335;&#12289;&#22797;&#26434;&#27169;&#26495;&#21644;&#22810;&#26679;&#30340;&#24067;&#23616;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26723;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20016;&#23500;&#35270;&#35273;&#21270;&#19994;&#21153;&#25991;&#26723;&#20197;&#25552;&#21462;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#33258;&#21160;&#21270;&#19994;&#21153;&#24037;&#20316;&#27969;&#31243;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#21463;&#21040;&#20851;&#27880;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#21453;&#26144;&#24037;&#19994;&#20013;&#23454;&#38469;&#25991;&#26723;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Visually Rich Document Understanding (VRDU)&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;VRDU&#21253;&#21547;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20195;&#34920;&#20102;&#22810;&#31181;&#25361;&#25112;&#65306;&#20016;&#23500;&#30340;&#27169;&#24335;&#65292;&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#20197;&#21450;&#20998;&#23618;&#23454;&#20307;; &#22797;&#26434;&#30340;&#27169;&#26495;&#65292;&#21253;&#25324;&#34920;&#26684;&#21644;&#22810;&#21015;&#24067;&#23616;; &#20197;&#21450;&#21333;&#20010;&#25991;&#26723;&#31867;&#22411;&#20013;&#19981;&#21516;&#24067;&#23616;&#65288;&#27169;&#26495;&#65289;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#23569;&#26679;&#26412;&#21644;&#24120;&#35268;&#23454;&#39564;&#35774;&#32622;&#65292;&#20197;&#21450;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#21305;&#37197;&#31639;&#27861;&#26469;&#35780;&#20272;&#25552;&#21462;&#32467;&#26524;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24378;&#22522;&#32447;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#35266;&#23519;&#32467;&#26524;&#65306;(1)&#36890;&#29992;n&#30340;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry. Although recent multi-modal language models have achieved impressive results, we find that existing benchmarks do not reflect the complexity of real documents seen in industry. In this work, we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding (VRDU). VRDU contains two datasets that represent several challenges: rich schema including diverse data types as well as hierarchical entities, complex templates including tables and multi-column layouts, and diversity of different layouts (templates) within a single document type. We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results. We report the performance of strong baselines and offer three observations: (1) generalizing to n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Peekaboo&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#20351;&#29992;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#35821;&#20041;&#32454;&#20998;&#24182;&#22522;&#30784;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#37325;&#26032;&#22521;&#35757;&#12290;&#36825;&#39033;&#25216;&#26415;&#30340;&#25512;&#29702;&#26102;&#38388;&#20248;&#21270;&#36807;&#31243;&#21487;&#20197;&#22312;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#20851;&#32852;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20998;&#21106;&#25513;&#27169;&#12290;</title><link>http://arxiv.org/abs/2211.13224</link><description>&lt;p&gt;
Peekaboo&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26159;&#38646;-shot&#32454;&#20998;&#22120;
&lt;/p&gt;
&lt;p&gt;
Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors. (arXiv:2211.13224v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Peekaboo&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#20351;&#29992;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#35821;&#20041;&#32454;&#20998;&#24182;&#22522;&#30784;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#37325;&#26032;&#22521;&#35757;&#12290;&#36825;&#39033;&#25216;&#26415;&#30340;&#25512;&#29702;&#26102;&#38388;&#20248;&#21270;&#36807;&#31243;&#21487;&#20197;&#22312;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#20851;&#32852;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20998;&#21106;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#21019;&#24314;&#36924;&#30495;&#22270;&#20687;&#26041;&#38754;&#30340;&#26174;&#30528;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#40092;&#26377;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#23450;&#20301;&#25110;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#27809;&#26377;&#25509;&#35302;&#21040;&#26412;&#22320;&#21270;&#20449;&#24687;&#30340;&#35757;&#32451;&#22914;&#20309;&#22312;&#26080;&#38656;&#32454;&#20998;&#29305;&#23450;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20026;&#26465;&#20214;&#24314;&#31435;&#21508;&#31181;&#35821;&#20041;&#30701;&#35821;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25512;&#29702;&#26102;&#38388;&#20248;&#21270;&#36807;&#31243;&#65292;&#33021;&#22815;&#22312;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#20851;&#32852;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20998;&#21106;&#25513;&#27169;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#8212;&#8212;Peekaboo&#65292;&#26159;&#19968;&#31181;&#39318;&#27454;&#26080;&#35757;&#32451;&#24320;&#25918;&#35789;&#27719;&#26080;&#30417;&#30563;&#35821;&#20041;&#25509;&#22320;&#25216;&#26415;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Pascal VOC&#25968;&#25454;&#38598;&#19978;&#23545;Peekaboo&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#35821;&#20041;&#32454;&#20998;&#65292;&#20197;&#21450;&#22312;RefCOCO&#25968;&#25454;&#38598;&#19978;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#29992;&#20110;&#24341;&#29992;&#32454;&#20998;&#65292;&#26174;&#31034;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#30340;&#31454;&#20105;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;Peekaboo&#36890;&#36807;&#26465;&#20214;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#29983;&#25104;&#24102;&#36879;&#26126;&#32972;&#26223;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, text-to-image diffusion models have shown remarkable capabilities in creating realistic images from natural language prompts. However, few works have explored using these models for semantic localization or grounding. In this work, we explore how an off-the-shelf text-to-image diffusion model, trained without exposure to localization information, can ground various semantic phrases without segmentation-specific re-training. We introduce an inference time optimization process capable of generating segmentation masks conditioned on natural language prompts. Our proposal, Peekaboo, is a first-of-its-kind zero-shot, open-vocabulary, unsupervised semantic grounding technique leveraging diffusion models without any training. We evaluate Peekaboo on the Pascal VOC dataset for unsupervised semantic segmentation and the RefCOCO dataset for referring segmentation, showing results competitive with promising results. We also demonstrate how Peekaboo can be used to generate images with tr
&lt;/p&gt;</description></item></channel></rss>