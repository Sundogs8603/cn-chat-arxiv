<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07922</link><description>&lt;p&gt;
RAVEN&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models. (arXiv:2308.07922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07922
&lt;/p&gt;
&lt;p&gt;
RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;ATLAS&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#39044;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#38271;&#24230;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RAVEN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26356;&#22810;&#19978;&#19979;&#25991;&#31034;&#20363;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#27169;&#22411;&#20462;&#25913;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RAVEN&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#26126;&#26174;&#20248;&#20110;ATLAS&#65292;&#24182;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder lang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-4&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26174;&#24335;&#30340;&#22522;&#20110;&#20195;&#30721;&#30340;&#33258;&#25105;&#39564;&#35777;&#65288;CSV&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36827;&#19968;&#27493;&#25552;&#21319;GPT-4 Code Interpreter&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07921</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#20195;&#30721;&#35299;&#37322;&#22120;&#21644;&#22522;&#20110;&#20195;&#30721;&#30340;&#33258;&#25105;&#39564;&#35777;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification. (arXiv:2308.07921v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-4&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26174;&#24335;&#30340;&#22522;&#20110;&#20195;&#30721;&#30340;&#33258;&#25105;&#39564;&#35777;&#65288;CSV&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36827;&#19968;&#27493;&#25552;&#21319;GPT-4 Code Interpreter&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#21644;PaLM-2&#22312;&#35299;&#20915;&#25968;&#23398;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23588;&#20854;&#26159;OpenAI&#30340;&#26368;&#26032;&#29256;&#26412;GPT-4 Code Interpreter&#22312;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#25506;&#35752;&#20102;&#20195;&#30721;&#23545;&#25552;&#21319;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#20854;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#20854;&#22312;&#29983;&#25104;&#21644;&#25191;&#34892;&#20195;&#30721;&#12289;&#35780;&#20272;&#20195;&#30721;&#25191;&#34892;&#32467;&#26524;&#20197;&#21450;&#20462;&#27491;&#35299;&#20915;&#26041;&#26696;&#26102;&#30340;&#24378;&#22823;&#25216;&#24039;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21363;&#26174;&#24335;&#30340;&#22522;&#20110;&#20195;&#30721;&#30340;&#33258;&#25105;&#39564;&#35777;&#65288;CSV&#65289;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;GPT-4 Code Interpreter&#30340;&#25968;&#23398;&#25512;&#29702;&#28508;&#21147;&#12290;&#35813;&#26041;&#27861;&#22312;GPT-4 Code Interpreter&#19978;&#37319;&#29992;&#38646;-shot&#25552;&#31034;&#65292;&#40723;&#21169;&#20854;&#20351;&#29992;&#20195;&#30721;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the \textit{Code Usage Frequency} of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit \uline{c}ode-based \uline{s}elf-\uline{v}erification~(CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#26680;&#24515;&#31454;&#20105;&#21147;&#30340;&#35270;&#35282;&#65292;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#35780;&#20272;&#30340;&#22810;&#31687;&#35770;&#25991;&#65292;&#24635;&#32467;&#20986;LLM&#30340;4&#20010;&#26680;&#24515;&#31454;&#20105;&#21147;&#65306;&#25512;&#29702;&#33021;&#21147;&#12289;&#30693;&#35782;&#33021;&#21147;&#12289;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#23545;&#24212;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.07902</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#24515;&#31454;&#20105;&#21147;&#30340;&#35270;&#35282;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Through the Lens of Core Competency: Survey on Evaluation of Large Language Models. (arXiv:2308.07902v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#26680;&#24515;&#31454;&#20105;&#21147;&#30340;&#35270;&#35282;&#65292;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#35780;&#20272;&#30340;&#22810;&#31687;&#35770;&#25991;&#65292;&#24635;&#32467;&#20986;LLM&#30340;4&#20010;&#26680;&#24515;&#31454;&#20105;&#21147;&#65306;&#25512;&#29702;&#33021;&#21147;&#12289;&#30693;&#35782;&#33021;&#21147;&#12289;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#23545;&#24212;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24050;&#32463;&#35265;&#35777;&#20102;&#24555;&#36895;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#23545;&#20854;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#26497;&#20854;&#22256;&#38590;&#65292;&#20027;&#35201;&#26377;&#20004;&#20010;&#21407;&#22240;&#65306;&#39318;&#20808;&#65292;&#20256;&#32479;&#30340;NLP&#20219;&#21153;&#22312;LLM&#34920;&#29616;&#20986;&#33394;&#20043;&#21518;&#21464;&#24471;&#19981;&#22815;&#36866;&#29992;&#65307;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#20219;&#21153;&#38590;&#20197;&#36319;&#19978;&#23454;&#38469;&#22330;&#26223;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#36895;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24050;&#26377;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20934;&#26469;&#26356;&#22909;&#22320;&#35780;&#20272;LLM&#12290;&#20026;&#20102;&#28548;&#28165;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#20013;&#19982;LLM&#35780;&#20272;&#30456;&#20851;&#30340;&#20247;&#22810;&#35770;&#25991;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22810;&#31687;&#20851;&#20110;LLM&#35780;&#20272;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;LLM&#30340;4&#20010;&#26680;&#24515;&#31454;&#20105;&#21147;&#65292;&#21253;&#25324;&#25512;&#29702;&#33021;&#21147;&#12289;&#30693;&#35782;&#33021;&#21147;&#12289;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#23545;&#20110;&#27599;&#20010;&#26680;&#24515;&#31454;&#20105;&#21147;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20854;&#23450;&#20041;&#12289;&#23545;&#24212;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#22522;&#20110;&#36825;&#20010;&#26680;&#24515;&#31454;&#20105;&#21147;&#20307;&#31995;&#65292;&#31867;&#20284;&#30340;&#20219;&#21153;&#21487;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
From pre-trained language model (PLM) to large language model (LLM), the field of natural language processing (NLP) has witnessed steep performance gains and wide practical uses. The evaluation of a research field guides its direction of improvement. However, LLMs are extremely hard to thoroughly evaluate for two reasons. First of all, traditional NLP tasks become inadequate due to the excellent performance of LLM. Secondly, existing evaluation tasks are difficult to keep up with the wide range of applications in real-world scenarios. To tackle these problems, existing works proposed various benchmarks to better evaluate LLMs. To clarify the numerous evaluation tasks in both academia and industry, we investigate multiple papers concerning LLM evaluations. We summarize 4 core competencies of LLM, including reasoning, knowledge, reliability, and safety. For every competency, we introduce its definition, corresponding benchmarks, and metrics. Under this competency architecture, similar ta
&lt;/p&gt;</description></item><item><title>&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#29702;&#25361;&#25112;&#26159;&#19968;&#20010;&#20197;&#25214;&#21040;&#26368;&#23567;&#27491;&#21017;&#34920;&#36798;&#24335;&#20026;&#30446;&#26631;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#24212;&#29992;&#24191;&#27867;&#12289;&#38590;&#24230;&#21487;&#35843;&#12289;&#36866;&#29992;&#20110;&#20195;&#30721;/&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.07899</link><description>&lt;p&gt;
&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#29702;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Regular Expression Inference Challenge. (arXiv:2308.07899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07899
&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#29702;&#25361;&#25112;&#26159;&#19968;&#20010;&#20197;&#25214;&#21040;&#26368;&#23567;&#27491;&#21017;&#34920;&#36798;&#24335;&#20026;&#30446;&#26631;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#24212;&#29992;&#24191;&#27867;&#12289;&#38590;&#24230;&#21487;&#35843;&#12289;&#36866;&#29992;&#20110;&#20195;&#30721;/&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#23558;&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#29702;&#65288;REI&#65289;&#20316;&#20026;&#20195;&#30721;/&#35821;&#35328;&#24314;&#27169;&#20197;&#21450;&#26356;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#25361;&#25112;&#12290;REI&#26159;&#19968;&#20010;&#26377;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#31243;&#24207;&#21512;&#25104;&#20219;&#21153;&#65292;&#23427;&#25552;&#20986;&#20102;&#20174;&#31034;&#20363;&#20013;&#25214;&#21040;&#26368;&#23567;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#20004;&#20010;&#26377;&#38480;&#23383;&#31526;&#20018;&#38598;&#21512;P&#21644;N&#20197;&#21450;&#19968;&#20010;&#25104;&#26412;&#20989;&#25968;cost(&#183;)&#65292;&#20219;&#21153;&#26159;&#29983;&#25104;&#19968;&#20010;&#25509;&#21463;P&#20013;&#25152;&#26377;&#23383;&#31526;&#20018;&#24182;&#25298;&#32477;N&#20013;&#25152;&#26377;&#23383;&#31526;&#20018;&#30340;&#34920;&#36798;&#24335;r&#65292;&#32780;&#19981;&#23384;&#22312;&#20854;&#20182;&#34920;&#36798;&#24335;r'&#65292;&#20351;&#24471;cost(r')&lt;cost(r)&#12290;REI&#20316;&#20026;&#19968;&#20010;&#25361;&#25112;&#38382;&#39064;&#20855;&#26377;&#20197;&#19979;&#20248;&#21183;&#65306;&#65288;i&#65289;&#27491;&#21017;&#34920;&#36798;&#24335;&#26159;&#20247;&#25152;&#21608;&#30693;&#12289;&#24191;&#27867;&#20351;&#29992;&#30340;&#65292;&#26159;&#20195;&#30721;&#30340;&#33258;&#28982;&#29702;&#24819;&#21270;&#65307;&#65288;ii&#65289;REI&#30340;&#28176;&#36817;&#26368;&#22351;&#24773;&#20917;&#22797;&#26434;&#24615;&#24050;&#34987;&#20805;&#20998;&#29702;&#35299;&#65307;&#65288;iii&#65289;REI&#20855;&#26377;&#19968;&#23567;&#37096;&#20998;&#26131;&#20110;&#29702;&#35299;&#30340;&#21442;&#25968;&#65288;&#20363;&#22914;P&#25110;N&#30340;&#22522;&#25968;&#12289;&#31034;&#20363;&#30340;&#23383;&#31526;&#20018;&#38271;&#24230;&#25110;&#25104;&#26412;&#20989;&#25968;&#65289;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;REI&#30340;&#38590;&#24230;&#65307;&#65288;iv&#65289;&#23545;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;M&#27169;&#22411;&#32780;&#35328;&#65292;REI&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose \emph{regular expression inference (REI)} as a challenge for code/language modelling, and the wider machine learning community. REI is a supervised machine learning (ML) and program synthesis task, and poses the problem of finding minimal regular expressions from examples: Given two finite sets of strings $P$ and $N$ and a cost function $\text{cost}(\cdot)$, the task is to generate an expression $r$ that accepts all strings in $P$ and rejects all strings in $N$, while no other such expression $r'$ exists with $\text{cost}(r')&lt;\text{cost}(r)$.  REI has advantages as a challenge problem: (i) regular expressions are well-known, widely used, and a natural idealisation of code; (ii) REI's asymptotic worst-case complexity is well understood; (iii) REI has a small number of easy to understand parameters (e.g.~$P$ or $N$ cardinality, string lengths of examples, or the cost function); this lets us easily finetune REI-hardness; (iv) REI is an unsolved problem for deep learning based M
&lt;/p&gt;</description></item><item><title>&#38142;&#25509;&#19978;&#19979;&#25991;&#23398;&#20064;(LCL)&#36890;&#36807;&#20174;&#22240;&#26524;&#20851;&#31995;&#20013;&#25512;&#29702;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#24341;&#23548;&#27169;&#22411;&#36776;&#21035;&#31867;&#27604;&#20851;&#31995;&#24182;&#25581;&#31034;&#28508;&#22312;&#30340;&#22240;&#26524;&#20851;&#32852;&#65292;&#25552;&#21319;&#20102;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#30340;&#35782;&#21035;&#21644;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.07891</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;LLM&#30340;&#38142;&#25509;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Link-Context Learning for Multimodal LLMs. (arXiv:2308.07891v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07891
&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#19978;&#19979;&#25991;&#23398;&#20064;(LCL)&#36890;&#36807;&#20174;&#22240;&#26524;&#20851;&#31995;&#20013;&#25512;&#29702;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#24341;&#23548;&#27169;&#22411;&#36776;&#21035;&#31867;&#27604;&#20851;&#31995;&#24182;&#25581;&#31034;&#28508;&#22312;&#30340;&#22240;&#26524;&#20851;&#32852;&#65292;&#25552;&#21319;&#20102;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#30340;&#35782;&#21035;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#23545;&#35805;&#20013;&#65292;&#20174;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23613;&#31649;&#30446;&#21069;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#20294;&#22312;&#26080;&#38656;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#25110;&#29702;&#35299;&#26032;&#27010;&#24565;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#25506;&#32034;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#40723;&#21169;&#27169;&#22411;&#20174;&#26377;&#38480;&#30340;&#20219;&#21153;&#20013;&#8220;&#23398;&#20250;&#23398;&#20064;&#8221;&#24182;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38142;&#25509;&#19978;&#19979;&#25991;&#23398;&#20064;(LCL)&#65292;&#24378;&#35843;&#8220;&#20174;&#22240;&#26524;&#20851;&#31995;&#20013;&#25512;&#29702;&#8221;&#20197;&#22686;&#24378;MLLMs&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;LCL&#36229;&#36234;&#20256;&#32479;&#30340;ICL&#65292;&#36890;&#36807;&#26126;&#30830;&#21152;&#24378;&#25903;&#25345;&#38598;&#21644;&#26597;&#35810;&#38598;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#20379;&#20855;&#26377;&#22240;&#26524;&#38142;&#25509;&#30340;&#28436;&#31034;&#65292;&#24341;&#23548;&#27169;&#22411;&#19981;&#20165;&#36776;&#21035;&#31867;&#27604;&#20851;&#31995;&#65292;&#36824;&#25581;&#31034;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#28508;&#22312;&#22240;&#26524;&#20851;&#32852;&#65292;&#25552;&#21319;&#20102;MLLMs&#23454;&#29616;&#30456;&#24212;&#25512;&#33616;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes "reasoning from cause and effect" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to reco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20027;&#35201;&#36890;&#36807;&#23545;7&#20010;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#22312;4&#31181;&#24120;&#35265;&#20851;&#31995;&#27169;&#24335;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#20197;&#21450;&#29702;&#35770;&#12289;&#23454;&#20307;&#39057;&#29575;&#21644;&#25972;&#20307;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#22312;&#19981;&#21516;&#20851;&#31995;&#27169;&#24335;&#19979;&#30340;&#33021;&#21147;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#20855;&#26377;&#35773;&#21050;&#24847;&#21619;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2308.07889</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#23398;&#20064;&#30340;&#20851;&#31995;&#27169;&#24335;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Knowledge Graph Embedding over Relational Patterns Based on Rule Learning. (arXiv:2308.07889v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#36890;&#36807;&#23545;7&#20010;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#22312;4&#31181;&#24120;&#35265;&#20851;&#31995;&#27169;&#24335;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#20197;&#21450;&#29702;&#35770;&#12289;&#23454;&#20307;&#39057;&#29575;&#21644;&#25972;&#20307;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#22312;&#19981;&#21516;&#20851;&#31995;&#27169;&#24335;&#19979;&#30340;&#33021;&#21147;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#20855;&#26377;&#35773;&#21050;&#24847;&#21619;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGE&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#30693;&#35782;&#22270;&#34917;&#20840;&#65288;KGC&#65289;&#20219;&#21153;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#20851;&#31995;&#27169;&#24335;&#26159;&#25351;&#20855;&#26377;&#29305;&#23450;&#35821;&#20041;&#30340;&#20851;&#31995;&#65292;&#23637;&#31034;&#22270;&#24418;&#27169;&#24335;&#65292;&#26159;&#24433;&#21709;KGE&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#23613;&#31649;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;KGE&#27169;&#22411;&#22312;&#19981;&#21516;&#20851;&#31995;&#27169;&#24335;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#24314;&#31435;&#20102;&#26356;&#22909;&#30340;&#20851;&#31995;&#27169;&#24335;&#24314;&#27169;&#19982;KGC&#26356;&#22909;&#24615;&#33021;&#20043;&#38388;&#30340;&#31895;&#30053;&#20851;&#32852;&#65292;&#20294;&#23545;&#20110;KGE&#27169;&#22411;&#22312;&#20851;&#31995;&#27169;&#24335;&#19978;&#30340;&#32508;&#21512;&#23450;&#37327;&#20998;&#26512;&#23578;&#26410;&#23436;&#25104;&#65292;&#22240;&#27492;&#19981;&#30830;&#23450;KGE&#23545;&#20110;&#20851;&#31995;&#27169;&#24335;&#30340;&#29702;&#35770;&#25903;&#25345;&#22914;&#20309;&#24433;&#21709;&#19982;&#27492;&#20851;&#31995;&#27169;&#24335;&#30456;&#20851;&#30340;&#19977;&#20803;&#32452;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;7&#20010;KGE&#27169;&#22411;&#22312;4&#20010;&#24120;&#35265;&#20851;&#31995;&#27169;&#24335;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;2&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#29702;&#35770;&#12289;&#23454;&#20307;&#39057;&#29575;&#21644;&#25972;&#20307;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#19968;&#20123;&#30452;&#35266;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding (KGE) has proven to be an effective approach to solving the Knowledge Graph Completion (KGC) task. Relational patterns which refer to relations with specific semantics exhibiting graph patterns are an important factor in the performance of KGE models. Though KGE models' capabilities are analyzed over different relational patterns in theory and a rough connection between better relational patterns modeling and better performance of KGC has been built, a comprehensive quantitative analysis on KGE models over relational patterns remains absent so it is uncertain how the theoretical support of KGE to a relational pattern contributes to the performance of triples associated to such a relational pattern. To address this challenge, we evaluate the performance of 7 KGE models over 4 common relational patterns on 2 benchmarks, then conduct an analysis in theory, entity frequency, and part-to-whole three aspects and get some counterintuitive conclusions. Finally, we int
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;</title><link>http://arxiv.org/abs/2308.07876</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#30721;&#26412;&#30693;&#35782;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;ChatGPT&#26469;&#21512;&#25104;&#25919;&#27835;&#38646;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20107;&#20214;&#32534;&#30721;&#30340;&#30417;&#30563;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#36828;&#36828;&#36229;&#36807;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#20165;&#20381;&#36182;&#20110;&#26032;&#30340;&#27880;&#37322;&#65292;&#24573;&#35270;&#20102;&#19987;&#23478;&#25968;&#25454;&#24211;&#20013;&#30340;&#22823;&#37327;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;ChatGPT&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#19978;&#19979;&#25991;&#12289;&#35821;&#24577;&#21644;&#31867;&#21035;&#28040;&#27495;&#30340;&#19981;&#21516;&#23618;&#27425;&#12290;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#26032;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;ChatGPT&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#31361;&#20986;&#20102;ZSP&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;ZSP&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#30340;F1&#24471;&#20998;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25552;&#39640;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classific
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#24773;&#24863;&#23884;&#20837;&#65292;&#29420;&#31435;&#20110;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#20132;&#27969;&#26041;&#24335;&#12289;&#23186;&#20307;&#25110;&#26631;&#31614;&#24418;&#24335;&#65292;&#20174;&#32780;&#23558;&#20197;&#24448;&#23545;&#19981;&#21516;&#31867;&#22411;&#24322;&#36136;&#24773;&#24863;&#25968;&#25454;&#30340;&#30740;&#31350;&#25972;&#21512;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2308.07871</link><description>&lt;p&gt;
&#24773;&#24863;&#23884;&#20837;&#8212;&#8212;&#20174;&#24322;&#36136;&#24773;&#24863;&#25968;&#25454;&#20013;&#23398;&#20064;&#31283;&#23450;&#19988;&#22343;&#21248;&#30340;&#25277;&#35937;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Emotion Embeddings $\unicode{x2014}$ Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets. (arXiv:2308.07871v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07871
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#24773;&#24863;&#23884;&#20837;&#65292;&#29420;&#31435;&#20110;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#20132;&#27969;&#26041;&#24335;&#12289;&#23186;&#20307;&#25110;&#26631;&#31614;&#24418;&#24335;&#65292;&#20174;&#32780;&#23558;&#20197;&#24448;&#23545;&#19981;&#21516;&#31867;&#22411;&#24322;&#36136;&#24773;&#24863;&#25968;&#25454;&#30340;&#30740;&#31350;&#25972;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#24773;&#24863;&#36890;&#36807;&#22810;&#31181;&#20132;&#27969;&#26041;&#24335;&#21644;&#23186;&#20307;&#26684;&#24335;&#34920;&#36798;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#35745;&#31639;&#30740;&#31350;&#21516;&#26679;&#22810;&#26679;&#21270;&#65292;&#28041;&#21450;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#38899;&#39057;&#20449;&#21495;&#20998;&#26512;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#31561;&#12290;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#24773;&#24863;&#34987;&#20197;&#19981;&#21516;&#30340;&#24418;&#24335;&#36827;&#34892;&#25551;&#36848;&#65288;&#26497;&#24615;&#23610;&#24230;&#12289;&#22522;&#26412;&#24773;&#24863;&#31867;&#21035;&#12289;&#32500;&#24230;&#26041;&#27861;&#12289;&#35780;&#20215;&#29702;&#35770;&#31561;&#65289;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#12289;&#39044;&#27979;&#27169;&#22411;&#21644;&#24773;&#24863;&#20998;&#26512;&#36719;&#20214;&#24037;&#20855;&#30340;&#22810;&#26679;&#21270;&#22686;&#38271;&#12290;&#30001;&#20110;&#36825;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#24322;&#36136;&#24615;&#65292;&#22312;&#34920;&#36798;&#21644;&#34920;&#31034;&#23618;&#38754;&#19978;&#65292;&#36843;&#20999;&#38656;&#35201;&#32479;&#19968;&#20197;&#24448;&#23545;&#36234;&#26469;&#36234;&#20998;&#25955;&#30340;&#25968;&#25454;&#21644;&#26631;&#31614;&#31867;&#22411;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#31181;&#20849;&#20139;&#30340;&#24773;&#24863;&#28508;&#22312;&#34920;&#31034;&#65292;&#21363;&#25152;&#35859;&#24773;&#24863;&#23884;&#20837;&#65292;&#19981;&#20381;&#36182;&#20110;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#12289;&#20132;&#27969;&#26041;&#24335;&#12289;&#23186;&#20307;&#25110;&#34920;&#31034;&#26631;&#31614;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human emotion is expressed in many communication modalities and media formats and so their computational study is equally diversified into natural language processing, audio signal analysis, computer vision, etc. Similarly, the large variety of representation formats used in previous research to describe emotions (polarity scales, basic emotion categories, dimensional approaches, appraisal theory, etc.) have led to an ever proliferating diversity of datasets, predictive models, and software tools for emotion analysis. Because of these two distinct types of heterogeneity, at the expressional and representational level, there is a dire need to unify previous work on increasingly diverging data and label types. This article presents such a unifying computational model. We propose a training procedure that learns a shared latent representation for emotions, so-called emotion embeddings, independent of different natural languages, communication modalities, media or representation label form
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20449;&#24687;&#25552;&#21462;&#36807;&#31243;&#20013;&#24212;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#28040;&#38500;&#20102;&#24187;&#35273;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2308.07791</link><description>&lt;p&gt;
&#20026;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20449;&#24687;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Informed Named Entity Recognition Decoding for Generative Language Models. (arXiv:2308.07791v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07791
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20449;&#24687;&#25552;&#21462;&#36807;&#31243;&#20013;&#24212;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#28040;&#38500;&#20102;&#24187;&#35273;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36234;&#26469;&#36234;&#24378;&#30340;&#33021;&#21147;&#65292;&#24050;&#25104;&#20026;&#34987;&#24191;&#27867;&#24212;&#29992;&#30340;&#25991;&#26412;&#22788;&#29702;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20173;&#28982;&#21463;&#21040;&#20043;&#21069;&#19968;&#20195;&#20165;&#32534;&#30721;&#22120;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Informed Named Entity Recognition Decoding&#65288;iNERD&#65289;&#65292;&#23427;&#23558;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#35270;&#20026;&#19968;&#31181;&#29983;&#25104;&#36807;&#31243;&#12290;&#23427;&#20197;&#38754;&#21521;&#26410;&#26469;&#30340;&#26041;&#24335;&#21033;&#29992;&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#25552;&#21462;&#30340;&#26377;&#38480;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#28040;&#38500;&#20102;&#20219;&#20309;&#24187;&#35273;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#22312;&#21512;&#24182;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#19978;&#31895;&#35843;&#20248;&#21270;&#20102;&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;&#20116;&#20010;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#20843;&#20010;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ever-larger language models with ever-increasing capabilities are by now well-established text processing tools. Alas, information extraction tasks such as named entity recognition are still largely unaffected by this progress as they are primarily based on the previous generation of encoder-only transformer models. Here, we propose a simple yet effective approach, Informed Named Entity Recognition Decoding (iNERD), which treats named entity recognition as a generative process. It leverages the language understanding capabilities of recent generative models in a future-proof manner and employs an informed decoding scheme incorporating the restricted nature of information extraction into open-ended text generation, improving performance and eliminating any risk of hallucinations. We coarse-tune our model on a merged named entity corpus to strengthen its performance, evaluate five generative language models on eight named entity recognition datasets, and achieve remarkable results, espec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;GraphLayoutLM&#65292;&#36890;&#36807;&#24314;&#27169;&#24067;&#23616;&#32467;&#26500;&#22270;&#23558;&#25991;&#26723;&#24067;&#23616;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#20013;&#65292;&#24182;&#21033;&#29992;&#24067;&#23616;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#26469;&#23398;&#20064;&#25991;&#26723;&#24067;&#23616;&#30693;&#35782;&#12290;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07777</link><description>&lt;p&gt;
&#36890;&#36807;&#24067;&#23616;&#32467;&#26500;&#24314;&#27169;&#22686;&#24378;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Enhancing Visually-Rich Document Understanding via Layout Structure Modeling. (arXiv:2308.07777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;GraphLayoutLM&#65292;&#36890;&#36807;&#24314;&#27169;&#24067;&#23616;&#32467;&#26500;&#22270;&#23558;&#25991;&#26723;&#24067;&#23616;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#20013;&#65292;&#24182;&#21033;&#29992;&#24067;&#23616;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#26469;&#23398;&#20064;&#25991;&#26723;&#24067;&#23616;&#30693;&#35782;&#12290;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#30340;&#20351;&#29992;&#22312;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#25991;&#26412;&#21644;&#22270;&#20687;&#31561;&#29305;&#24449;&#65292;&#24573;&#35270;&#20102;&#25991;&#26412;&#33410;&#28857;&#20043;&#38388;&#24067;&#23616;&#20851;&#31995;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphLayoutLM&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#24067;&#23616;&#32467;&#26500;&#22270;&#30340;&#24314;&#27169;&#23558;&#25991;&#26723;&#24067;&#23616;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#20013;&#12290;GraphLayoutLM&#21033;&#29992;&#22270;&#37325;&#25490;&#24207;&#31639;&#27861;&#65292;&#26681;&#25454;&#22270;&#32467;&#26500;&#35843;&#25972;&#25991;&#26412;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#19968;&#20010;&#24067;&#23616;&#24863;&#30693;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#23618;&#26469;&#23398;&#20064;&#25991;&#26723;&#24067;&#23616;&#30693;&#35782;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#25991;&#26412;&#20803;&#32032;&#30340;&#31354;&#38388;&#25490;&#21015;&#65292;&#25552;&#39640;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;FUNSD&#12289;XFUND&#21644;CORD&#31561;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the use of multi-modal pre-trained Transformers has led to significant advancements in visually-rich document understanding. However, existing models have mainly focused on features such as text and vision while neglecting the importance of layout relationship between text nodes. In this paper, we propose GraphLayoutLM, a novel document understanding model that leverages the modeling of layout structure graph to inject document layout knowledge into the model. GraphLayoutLM utilizes a graph reordering algorithm to adjust the text sequence based on the graph structure. Additionally, our model uses a layout-aware multi-head self-attention layer to learn document layout knowledge. The proposed model enables the understanding of the spatial arrangement of text elements, improving document comprehension. We evaluate our model on various benchmarks, including FUNSD, XFUND and CORD, and achieve state-of-the-art results among these datasets. Our experimental results demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07758</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#65288;Chain-of-Though, CoT&#65289;&#25552;&#31034;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;Self-Consistency&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#36825;&#20123;&#38142;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#26102;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#21363;``&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19978;&#36848;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#20505;&#36873;&#31572;&#26696;&#65292;&#37027;&#20040;&#26410;&#30693;&#21464;&#37327;x&#30340;&#20540;&#26159;&#22810;&#23569;&#65311;''&#65292;&#23558;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#23631;&#34109;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#27491;&#30830;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;FOBAR&#26041;&#27861;&#65292;&#23558;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;Meituan&#25628;&#32034;&#20013;&#36827;&#34892;&#30456;&#20851;&#24615;&#24314;&#27169;&#30340;&#26032;&#39062;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#21644;&#21305;&#37197;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.07711</link><description>&lt;p&gt;
SPM: Meituan&#25628;&#32034;&#20013;&#29992;&#20110;&#30456;&#20851;&#24615;&#24314;&#27169;&#30340;&#32467;&#26500;&#21270;&#39044;&#35757;&#32451;&#21644;&#21305;&#37197;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SPM: Structured Pretraining and Matching Architectures for Relevance Modeling in Meituan Search. (arXiv:2308.07711v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;Meituan&#25628;&#32034;&#20013;&#36827;&#34892;&#30456;&#20851;&#24615;&#24314;&#27169;&#30340;&#26032;&#39062;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#21644;&#21305;&#37197;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#21830;&#25628;&#32034;&#20013;&#65292;&#26597;&#35810;&#21644;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#28385;&#36275;&#29992;&#25143;&#20307;&#39564;&#30340;&#22522;&#26412;&#35201;&#27714;&#12290;&#19982;&#20256;&#32479;&#30340;&#30005;&#21830;&#24179;&#21488;&#19981;&#21516;&#65292;&#29992;&#25143;&#22312;&#32654;&#22242;&#31561;&#29983;&#27963;&#26381;&#21153;&#24179;&#21488;&#19978;&#36827;&#34892;&#25628;&#32034;&#20027;&#35201;&#26159;&#20026;&#20102;&#20135;&#21697;&#20379;&#24212;&#21830;&#65292;&#36825;&#20123;&#20379;&#24212;&#21830;&#36890;&#24120;&#25317;&#26377;&#20016;&#23500;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#20363;&#22914;&#21517;&#31216;&#12289;&#22320;&#22336;&#12289;&#31867;&#21035;&#12289;&#25104;&#21315;&#19978;&#19975;&#30340;&#20135;&#21697;&#12290;&#20351;&#29992;&#36825;&#20123;&#20016;&#23500;&#30340;&#32467;&#26500;&#21270;&#20869;&#23481;&#36827;&#34892;&#25628;&#32034;&#30456;&#20851;&#24615;&#24314;&#27169;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20027;&#35201;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19981;&#21516;&#23383;&#27573;&#30340;&#32467;&#26500;&#21270;&#25991;&#26723;&#23384;&#22312;&#35821;&#35328;&#20998;&#24067;&#24046;&#24322;&#65292;&#26080;&#27861;&#30452;&#25509;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65288;&#22914;BERT&#65289;&#12290;&#65288;2&#65289;&#19981;&#21516;&#23383;&#27573;&#36890;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#65292;&#19988;&#38271;&#24230;&#24046;&#24322;&#24456;&#22823;&#65292;&#24456;&#38590;&#25552;&#21462;&#23545;&#30456;&#20851;&#24615;&#21305;&#37197;&#26377;&#24110;&#21161;&#30340;&#25991;&#26723;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#21644;&#21305;&#37197;&#26550;&#26500;&#65292;&#29992;&#20110;&#20016;&#23500;&#32467;&#26500;&#30340;&#30456;&#20851;&#24615;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
In e-commerce search, relevance between query and documents is an essential requirement for satisfying user experience. Different from traditional e-commerce platforms that offer products, users search on life service platforms such as Meituan mainly for product providers, which usually have abundant structured information, e.g. name, address, category, thousands of products. Modeling search relevance with these rich structured contents is challenging due to the following issues: (1) there is language distribution discrepancy among different fields of structured document, making it difficult to directly adopt off-the-shelf pretrained language model based methods like BERT. (2) different fields usually have different importance and their length vary greatly, making it difficult to extract document information helpful for relevance matching.  To tackle these issues, in this paper we propose a novel two-stage pretraining and matching architecture for relevance matching with rich structure
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2308.07706</link><description>&lt;p&gt;
&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25506;&#32034;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25972;&#21512;&#25991;&#26412;&#25351;&#23548;&#20197;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#20173;&#28982;&#26159;&#19968;&#20010;&#36827;&#23637;&#26377;&#38480;&#30340;&#39046;&#22495;&#12290;&#29616;&#26377;&#21033;&#29992;&#25991;&#26412;&#25351;&#23548;&#30340;&#20998;&#21106;&#27169;&#22411;&#20027;&#35201;&#22312;&#24320;&#25918;&#39046;&#22495;&#22270;&#20687;&#19978;&#35757;&#32451;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#30452;&#25509;&#24212;&#29992;&#30340;&#38590;&#39064;&#65292;&#38656;&#35201;&#25163;&#21160;&#20171;&#20837;&#25110;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20174;&#22270;&#20687;&#25551;&#36848;&#21644;&#22270;&#20687;&#20013;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#24471;&#33021;&#22815;&#23545;&#22810;&#26679;&#21270;&#30340;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#12290;&#35813;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#20197;&#35780;&#20272;&#20854;&#20174;&#24320;&#25918;&#39046;&#22495;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#20013;&#20197;&#21069;&#26410;&#35265;&#22270;&#20687;&#30340;&#22270;&#20687;&#25551;&#36848;&#24341;&#20837;&#20102;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.  To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#65292;&#30740;&#31350;&#35780;&#20272;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#22312;&#22810;&#20010;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#36229;&#36234;&#20102;&#26631;&#20934;&#30340;&#38646;-shot&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.07702</link><description>&lt;p&gt;
&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#25552;&#39640;&#38646;-shot&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Better Zero-Shot Reasoning with Role-Play Prompting. (arXiv:2308.07702v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07702
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#65292;&#30740;&#31350;&#35780;&#20272;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#22312;&#22810;&#20010;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#36229;&#36234;&#20102;&#26631;&#20934;&#30340;&#38646;-shot&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#65292;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#25198;&#28436;&#19981;&#20165;&#26159;&#20154;&#31867;&#35282;&#33394;&#65292;&#36824;&#21253;&#25324;&#20687;Linux&#32456;&#31471;&#36825;&#26679;&#30340;&#38750;&#20154;&#35282;&#33394;&#12290;&#36825;&#31181;&#22810;&#21151;&#33021;&#24615;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#20132;&#20114;&#21644;&#34892;&#20026;&#65292;&#24182;&#20223;&#30495;&#29305;&#23450;&#30340;&#23545;&#35937;&#25110;&#31995;&#32479;&#12290;&#23613;&#31649;&#36825;&#20123;&#33021;&#21147;&#22686;&#24378;&#20102;&#29992;&#25143;&#21442;&#19982;&#24230;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#20132;&#20114;&#27169;&#24335;&#65292;&#20294;&#35282;&#33394;&#25198;&#28436;&#23545;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#20173;&#26377;&#24453;&#28145;&#20837;&#25506;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31574;&#30053;&#24615;&#35774;&#35745;&#30340;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#22312;&#21313;&#20108;&#20010;&#19981;&#21516;&#30340;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20854;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#65292;&#28085;&#30422;&#20102;&#31639;&#26415;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#31526;&#21495;&#25512;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#21033;&#29992;ChatGPT&#21644;Llama 2&#31561;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#22312;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;&#26631;&#20934;&#30340;&#38646;-shot&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Modern large language models (LLMs), such as ChatGPT, exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities like a Linux terminal. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs' reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks, encompassing arithmetic, commonsense reasoning, symbolic reasoning, and more. Leveraging models such as ChatGPT and Llama 2, our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Extractor&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#65292;&#29992;&#20110;&#21462;&#20195;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;Extractor&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#30701;&#30340;&#35745;&#31639;&#20851;&#38190;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2308.07661</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#19981;&#20877;&#26159;&#21807;&#19968;&#38656;&#35201;&#30340;&#19996;&#35199;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention Is Not All You Need Anymore. (arXiv:2308.07661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Extractor&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#65292;&#29992;&#20110;&#21462;&#20195;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;Extractor&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#30701;&#30340;&#35745;&#31639;&#20851;&#38190;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#27969;&#34892;&#30340;Transformer&#26550;&#26500;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#24615;&#33021;&#24179;&#34913;&#26469;&#20943;&#23569;Transformer&#20013;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#22797;&#26434;&#24230;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#23545;&#20110;Transformer&#30340;&#25345;&#32493;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21462;&#20195;Transformer&#20013;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#65288;Extractor&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;Extractor&#26367;&#25442;&#33258;&#27880;&#24847;&#26426;&#21046;&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;Extractor&#20855;&#26377;&#26356;&#30701;&#30340;&#35745;&#31639;&#20851;&#38190;&#36335;&#24452;&#65292;&#22240;&#27492;&#26377;&#28508;&#21147;&#27604;&#33258;&#27880;&#24847;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#20351;&#29992;&#21487;&#21464;&#38271;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#23545;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#24182;&#38024;&#23545;&#25105;&#20204;&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#23545;Transformer&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the popular Transformer architecture has achieved great success in many application areas, including natural language processing and computer vision. Many existing works aim to reduce the computational and memory complexity of the self-attention mechanism in the Transformer by trading off performance. However, performance is key for the continuing success of the Transformer. In this paper, a drop-in replacement for the self-attention mechanism in the Transformer, called the Extractor, is proposed. Experimental results show that replacing the self-attention mechanism with the Extractor improves the performance of the Transformer. Furthermore, the proposed Extractor has the potential to run faster than the self-attention since it has a much shorter critical path of computation. Additionally, the sequence prediction problem in the context of text generation is formulated using variable-length discrete-time Markov chains, and the Transformer is reviewed based on our unders
&lt;/p&gt;</description></item><item><title>SEER&#26159;&#19968;&#20010;&#22522;&#20110;E&#22270;&#37325;&#20889;&#21644;MLIR&#30340;&#36229;&#32423;&#20248;&#21270;&#25506;&#32034;&#22120;&#65292;&#29992;&#20110;&#23558;&#36719;&#20214;&#31243;&#24207;&#33258;&#21160;&#36716;&#25442;&#20026;&#39640;&#25928;&#30340;&#30828;&#20214;&#25551;&#36848;&#65292;&#22635;&#34917;&#20102;&#39640;&#32423;&#32508;&#21512;&#24037;&#20855;&#29983;&#25104;&#30340;&#30828;&#20214;&#35774;&#35745;&#19982;&#25163;&#21160;&#23454;&#29616;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2308.07654</link><description>&lt;p&gt;
SEER&#65306;&#20351;&#29992;MLIR&#36827;&#34892;E&#22270;&#37325;&#20889;&#30340;HLS&#36229;&#32423;&#20248;&#21270;&#25506;&#32034;&#22120;
&lt;/p&gt;
&lt;p&gt;
SEER: Super-Optimization Explorer for HLS using E-graph Rewriting with MLIR. (arXiv:2308.07654v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07654
&lt;/p&gt;
&lt;p&gt;
SEER&#26159;&#19968;&#20010;&#22522;&#20110;E&#22270;&#37325;&#20889;&#21644;MLIR&#30340;&#36229;&#32423;&#20248;&#21270;&#25506;&#32034;&#22120;&#65292;&#29992;&#20110;&#23558;&#36719;&#20214;&#31243;&#24207;&#33258;&#21160;&#36716;&#25442;&#20026;&#39640;&#25928;&#30340;&#30828;&#20214;&#25551;&#36848;&#65292;&#22635;&#34917;&#20102;&#39640;&#32423;&#32508;&#21512;&#24037;&#20855;&#29983;&#25104;&#30340;&#30828;&#20214;&#35774;&#35745;&#19982;&#25163;&#21160;&#23454;&#29616;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#32508;&#21512;&#65288;HLS&#65289;&#26159;&#19968;&#20010;&#23558;&#39640;&#32423;&#35821;&#35328;&#32534;&#20889;&#30340;&#36719;&#20214;&#31243;&#24207;&#33258;&#21160;&#36716;&#25442;&#20026;&#20302;&#32423;&#30828;&#20214;&#25551;&#36848;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#30001;HLS&#24037;&#20855;&#29983;&#25104;&#30340;&#30828;&#20214;&#35774;&#35745;&#19982;&#25163;&#21160;&#23454;&#29616;&#30456;&#27604;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#36825;&#26159;&#22240;&#20026;&#36755;&#20837;&#30340;HLS&#31243;&#24207;&#20173;&#24517;&#39035;&#25353;&#29031;&#30828;&#20214;&#35774;&#35745;&#21407;&#21017;&#32534;&#20889;&#12290;&#29616;&#26377;&#30340;&#25216;&#26415;&#35201;&#20040;&#20445;&#25345;&#31243;&#24207;&#28304;&#20195;&#30721;&#19981;&#21464;&#65292;&#35201;&#20040;&#25191;&#34892;&#19968;&#31995;&#21015;&#22266;&#23450;&#30340;&#28304;&#20195;&#30721;&#36716;&#25442;&#27493;&#39588;&#65292;&#21487;&#33021;&#20250;&#38169;&#36807;&#23547;&#25214;&#26368;&#20248;&#35774;&#35745;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;HLS&#30340;&#36229;&#32423;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#20219;&#24847;&#36719;&#20214;&#31243;&#24207;&#37325;&#20889;&#20026;&#39640;&#25928;&#30340;HLS&#20195;&#30721;&#65292;&#29992;&#20110;&#29983;&#25104;&#20248;&#21270;&#30340;&#30828;&#20214;&#35774;&#35745;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;SEER&#30340;&#24037;&#20855;&#27969;&#31243;&#65292;&#22522;&#20110;E&#22270;&#25968;&#25454;&#32467;&#26500;&#65292;&#20197;&#39640;&#25928;&#22320;&#25506;&#32034;&#31243;&#24207;&#30340;&#31561;&#25928;&#23454;&#29616;&#12290;SEER&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#21327;&#35843;&#29616;&#26377;&#30340;&#36719;&#20214;&#32534;&#35793;&#22120;&#20256;&#36882;&#21644;&#30828;
&lt;/p&gt;
&lt;p&gt;
High-level synthesis (HLS) is a process that automatically translates a software program in a high-level language into a low-level hardware description. However, the hardware designs produced by HLS tools still suffer from a significant performance gap compared to manual implementations. This is because the input HLS programs must still be written using hardware design principles.  Existing techniques either leave the program source unchanged or perform a fixed sequence of source transformation passes, potentially missing opportunities to find the optimal design. We propose a super-optimization approach for HLS that automatically rewrites an arbitrary software program into efficient HLS code that can be used to generate an optimized hardware design. We developed a toolflow named SEER, based on the e-graph data structure, to efficiently explore equivalent implementations of a program at scale. SEER provides an extensible framework, orchestrating existing software compiler passes and har
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#19987;&#23478;&#25351;&#23548;&#21644;&#36127;&#38754;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#36830;&#36143;&#21644;&#22810;&#26679;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#25805;&#20316;&#65292;&#21033;&#29992;&#23545;&#27604;&#24418;&#24335;&#25351;&#23548;LLMs&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#21644;&#19982;&#20808;&#21069;&#31034;&#20363;&#30340;&#20559;&#31163;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.07645</link><description>&lt;p&gt;
&#24341;&#23548;&#35821;&#35328;&#29983;&#25104;&#65306;&#21033;&#29992;&#23545;&#27604;&#19987;&#23478;&#25351;&#23548;&#21644;&#36127;&#38754;&#25552;&#31034;&#36827;&#34892;&#19968;&#33268;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation. (arXiv:2308.07645v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07645
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#19987;&#23478;&#25351;&#23548;&#21644;&#36127;&#38754;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#36830;&#36143;&#21644;&#22810;&#26679;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#25805;&#20316;&#65292;&#21033;&#29992;&#23545;&#27604;&#24418;&#24335;&#25351;&#23548;LLMs&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#21644;&#19982;&#20808;&#21069;&#31034;&#20363;&#30340;&#20559;&#31163;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#23454;&#29992;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#36825;&#22312;&#20174;&#19979;&#28216;&#27169;&#22411;&#35757;&#32451;&#21040;&#23454;&#38469;&#25968;&#25454;&#21033;&#29992;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26377;&#24456;&#22810;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#29616;&#20195;&#27169;&#22411;&#30340;&#33021;&#21147;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#21364;&#32463;&#24120;&#38590;&#20197;&#20135;&#29983;&#26082;&#36830;&#36143;&#21448;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36830;&#36143;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#24418;&#24335;&#19987;&#23478;&#25351;&#23548;&#65292;&#24378;&#35843;&#20102;&#31934;&#32454;&#35843;&#25972;&#21644;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#36923;&#36753;&#20998;&#24067;&#24046;&#24322;&#65292;&#20197;&#30830;&#20445;&#39046;&#22495;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#20445;&#35777;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#30495;&#23454;&#21644;&#21512;&#25104;&#30340;&#20363;&#23376;&#20316;&#20026;&#27169;&#22411;&#30340;&#36127;&#38754;&#25552;&#31034;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#23545;&#36923;&#36753;&#37325;&#22609;&#30340;&#21452;&#37325;&#26041;&#27861;&#31216;&#20026;STEER&#65306;&#36890;&#36807;&#23884;&#20837;&#37325;&#26032;&#23450;&#20301;&#23454;&#29616;&#30340;&#35821;&#20041;&#25991;&#26412;&#22686;&#24378;&#12290;STEER&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#36816;&#34892;&#65292;&#24182;&#31995;&#32479;&#22320;&#25351;&#23548;LLMs&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#65288;&#30830;&#20445;&#35821;&#20041;&#20445;&#30495;&#24230;&#65289;&#19982;&#20808;&#21069;&#21512;&#25104;&#31034;&#20363;&#25110;&#29616;&#26377;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) hold immense potential to generate synthetic data of high quality and utility, which has numerous applications from downstream model training to practical data utilisation. However, contemporary models, despite their impressive capacities, consistently struggle to produce both coherent and diverse data. To address the coherency issue, we introduce contrastive expert guidance, where the difference between the logit distributions of fine-tuned and base language models is emphasised to ensure domain adherence. In order to ensure diversity, we utilise existing real and synthetic examples as negative prompts to the model. We deem this dual-pronged approach to logit reshaping as STEER: Semantic Text Enhancement via Embedding Repositioning. STEER operates at inference-time and systematically guides the LLMs to strike a balance between adherence to the data distribution (ensuring semantic fidelity) and deviation from prior synthetic examples or existing real datase
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35786;&#26029;&#23545;&#35805;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;LLM-specific Mini-CEX&#35780;&#20272;&#26631;&#20934;&#21644;&#20351;&#29992;&#24739;&#32773;&#27169;&#25311;&#22120;&#19982;ChatGPT&#33258;&#21160;&#35780;&#20272;&#35786;&#26029;&#23545;&#35805;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;LLMs&#35780;&#20272;&#20013;&#30340;&#32479;&#19968;&#21644;&#20840;&#38754;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07635</link><description>&lt;p&gt;
LLM-Mini-CEX: &#29992;&#20110;&#35786;&#26029;&#23545;&#35805;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation. (arXiv:2308.07635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35786;&#26029;&#23545;&#35805;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;LLM-specific Mini-CEX&#35780;&#20272;&#26631;&#20934;&#21644;&#20351;&#29992;&#24739;&#32773;&#27169;&#25311;&#22120;&#19982;ChatGPT&#33258;&#21160;&#35780;&#20272;&#35786;&#26029;&#23545;&#35805;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;LLMs&#35780;&#20272;&#20013;&#30340;&#32479;&#19968;&#21644;&#20840;&#38754;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#30340;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#20197;&#25552;&#39640;&#35786;&#26029;&#25928;&#29575;&#26041;&#38754;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#25216;&#26415;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#32479;&#19968;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#26631;&#20934;&#23548;&#33268;&#26080;&#27861;&#35780;&#20272;&#21307;&#23398;LLMs&#30340;&#36136;&#37327;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#36827;&#19968;&#27493;&#38459;&#30861;&#20102;LLMs&#22312;&#21307;&#30103;&#27835;&#30103;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#20005;&#37325;&#20381;&#36182;&#20110;&#19982;LLMs&#36827;&#34892;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#20132;&#20114;&#20197;&#33719;&#21462;&#35786;&#26029;&#23545;&#35805;&#21644;&#20154;&#24037;&#35780;&#20272;&#35786;&#26029;&#23545;&#35805;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#32570;&#20047;&#32479;&#19968;&#21644;&#20840;&#38754;&#35780;&#20272;&#26631;&#20934;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#21407;&#22987;&#30340;Mini-CEX&#24314;&#31435;&#20102;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#31216;&#20026;LLM-specific Mini-CEX&#65292;&#20197;&#26377;&#25928;&#35780;&#20272;LLMs&#30340;&#35786;&#26029;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#21171;&#21160;&#23494;&#38598;&#22411;&#20132;&#20114;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24739;&#32773;&#27169;&#25311;&#22120;&#19982;LLMs&#33258;&#21160;&#36827;&#34892;&#23545;&#35805;&#65292;&#24182;&#21033;&#29992;ChatGPT&#33258;&#21160;&#35780;&#20272;&#35786;&#26029;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an increasing interest in developing LLMs for medical diagnosis to improve diagnosis efficiency. Despite their alluring technological potential, there is no unified and comprehensive evaluation criterion, leading to the inability to evaluate the quality and potential risks of medical LLMs, further hindering the application of LLMs in medical treatment scenarios. Besides, current evaluations heavily rely on labor-intensive interactions with LLMs to obtain diagnostic dialogues and human evaluation on the quality of diagnosis dialogue. To tackle the lack of unified and comprehensive evaluation criterion, we first initially establish an evaluation criterion, termed LLM-specific Mini-CEX to assess the diagnostic capabilities of LLMs effectively, based on original Mini-CEX. To address the labor-intensive interaction problem, we develop a patient simulator to engage in automatic conversations with LLMs, and utilize ChatGPT for evaluating diagnosis dialogues automatically. Experimenta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2308.07633</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Model Compression for Large Language Models. (arXiv:2308.07633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#24778;&#20154;&#30340;&#25104;&#21151;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#20307;&#37327;&#21644;&#35745;&#31639;&#38656;&#27714;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#30340;&#23454;&#38469;&#37096;&#32626;&#20013;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#38543;&#30528;&#36825;&#20123;&#25361;&#25112;&#26085;&#30410;&#32039;&#36843;&#65292;&#27169;&#22411;&#21387;&#32553;&#39046;&#22495;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#19987;&#38376;&#38024;&#23545;LLMs&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#65292;&#20197;&#24212;&#23545;&#39640;&#25928;&#37096;&#32626;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#22312;&#27599;&#31181;&#25216;&#26415;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;LLM&#30740;&#31350;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#29992;&#20110;&#35780;&#20272;&#25928;&#26524;&#30340;&#22522;&#20934;&#31574;&#30053;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of 
&lt;/p&gt;</description></item><item><title>LogPrompt&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#35299;&#37322;&#30340;&#26085;&#24535;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#20026;&#26085;&#24535;&#20219;&#21153;&#35774;&#35745;&#30340;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#26085;&#24535;&#30340;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07610</link><description>&lt;p&gt;
LogPrompt: &#38646;&#26679;&#26412;&#21644;&#21487;&#35299;&#37322;&#30340;&#26085;&#24535;&#20998;&#26512;&#30340;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
LogPrompt: Prompt Engineering Towards Zero-Shot and Interpretable Log Analysis. (arXiv:2308.07610v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07610
&lt;/p&gt;
&lt;p&gt;
LogPrompt&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#35299;&#37322;&#30340;&#26085;&#24535;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#20026;&#26085;&#24535;&#20219;&#21153;&#35774;&#35745;&#30340;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#26085;&#24535;&#30340;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#36719;&#20214;&#23494;&#38598;&#22411;&#31995;&#32479;&#20013;&#65292;&#33258;&#21160;&#21270;&#26085;&#24535;&#20998;&#26512;&#23545;&#20110;&#30830;&#20445;&#36719;&#20214;&#32500;&#25252;&#21644;&#24037;&#31243;&#29983;&#21629;&#21608;&#26399;&#30340;&#21487;&#38752;&#24615;&#21644;&#24377;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#21333;&#20010;&#39044;&#27979;&#20540;&#32780;&#27809;&#26377;&#35299;&#37322;&#26469;&#25191;&#34892;&#35832;&#22914;&#26085;&#24535;&#35299;&#26512;&#21644;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#31995;&#32479;&#20107;&#20214;&#30340;&#22686;&#21152;&#65292;&#20998;&#26512;&#32467;&#26524;&#30340;&#26377;&#38480;&#21487;&#35299;&#37322;&#24615;&#38459;&#30861;&#20102;&#20998;&#26512;&#20154;&#21592;&#23545;&#20854;&#30340;&#20449;&#20219;&#24230;&#21644;&#37319;&#21462;&#36866;&#24403;&#34892;&#21160;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#39046;&#22495;&#20869;&#22521;&#35757;&#25968;&#25454;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#28041;&#21450;&#26032;&#22495;&#30340;&#26410;&#35265;&#36807;&#26085;&#24535;&#30340;&#22312;&#32447;&#22330;&#26223;&#20013;&#24613;&#21095;&#19979;&#38477;&#65288;&#26368;&#22810;&#19979;&#38477;62.5%&#65289;&#65292;&#36825;&#26159;&#30001;&#20110;&#36719;&#20214;&#26356;&#26032;&#30340;&#36805;&#36895;&#32780;&#24120;&#35265;&#30340;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#35299;&#37322;&#30340;&#26085;&#24535;&#20998;&#26512;&#26041;&#27861;LogPrompt&#12290;LogPrompt&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#19968;&#22871;&#38024;&#23545;&#26085;&#24535;&#20219;&#21153;&#30340;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#25191;&#34892;&#38646;&#26679;&#26412;&#26085;&#24535;&#20998;&#26512;&#20219;&#21153;&#65292;&#20174;&#32780;&#22686;&#24378;LLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated log analysis is crucial in modern software-intensive systems for ensuring reliability and resilience throughout software maintenance and engineering life cycles. Existing methods perform tasks such as log parsing and log anomaly detection by providing a single prediction value without interpretation. However, given the increasing volume of system events, the limited interpretability of analysis results hinders analysts' trust and their ability to take appropriate actions. Moreover, these methods require substantial in-domain training data, and their performance declines sharply (by up to 62.5%) in online scenarios involving unseen logs from new domains, a common occurrence due to rapid software updates. In this paper, we propose LogPrompt, a novel zero-shot and interpretable log analysis approach. LogPrompt employs large language models (LLMs) to perform zero-shot log analysis tasks via a suite of advanced prompt strategies tailored for log tasks, which enhances LLMs' perform
&lt;/p&gt;</description></item><item><title>VBD-MT&#26159;VLSP 2022&#20013;&#36234;&#32763;&#35793;&#20219;&#21153;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#27169;&#22411;&#21644;mBART&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#37319;&#29992;&#21453;&#21521;&#32763;&#35793;&#21644;&#20854;&#20182;&#26041;&#27861;&#26469;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#20844;&#20849;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2308.07601</link><description>&lt;p&gt;
VBD-MT &#29992;&#20110;VLSP 2022&#30340;&#20013;&#36234;&#32763;&#35793;&#31995;&#32479;&#65288;arXiv:2308.07601v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
VBD-MT Chinese-Vietnamese Translation Systems for VLSP 2022. (arXiv:2308.07601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07601
&lt;/p&gt;
&lt;p&gt;
VBD-MT&#26159;VLSP 2022&#20013;&#36234;&#32763;&#35793;&#20219;&#21153;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#27169;&#22411;&#21644;mBART&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#37319;&#29992;&#21453;&#21521;&#32763;&#35793;&#21644;&#20854;&#20182;&#26041;&#27861;&#26469;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#20844;&#20849;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#21442;&#21152;VLSP 2022&#26426;&#22120;&#32763;&#35793;&#20849;&#20139;&#20219;&#21153;&#30340;&#31995;&#32479;&#12290;&#22312;&#20170;&#24180;&#30340;&#20849;&#20139;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21442;&#21152;&#20102;&#20013;&#36234;&#21644;&#36234;&#20013;&#20004;&#31181;&#32763;&#35793;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22522;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;Transformer&#27169;&#22411;&#21644;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#21435;&#22122;&#39044;&#35757;&#32451;&#27169;&#22411;mBART&#12290;&#31995;&#32479;&#36890;&#36807;&#37319;&#26679;&#27861;&#36827;&#34892;&#21453;&#21521;&#32763;&#35793;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#21487;&#29992;&#30340;&#21333;&#35821;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#20854;&#20182;&#20960;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#21253;&#25324;&#38598;&#25104;&#21644;&#21518;&#22788;&#29702;&#12290;&#22312;&#20844;&#20849;&#27979;&#35797;&#38598;&#19978;&#65292;&#25105;&#20204;&#22312;&#20013;&#36234;&#32763;&#35793;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;38.9 BLEU&#65292;&#22312;&#36234;&#20013;&#32763;&#35793;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;38.0 BLEU&#65292;&#36229;&#36807;&#20102;&#20960;&#20010;&#24378;&#26377;&#21147;&#30340;&#22522;&#20934;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present our systems participated in the VLSP 2022 machine translation shared task. In the shared task this year, we participated in both translation tasks, i.e., Chinese-Vietnamese and Vietnamese-Chinese translations. We build our systems based on the neural-based Transformer model with the powerful multilingual denoising pre-trained model mBART. The systems are enhanced by a sampling method for backtranslation, which leverage large scale available monolingual data. Additionally, several other methods are applied to improve the translation quality including ensembling and postprocessing. We achieve 38.9 BLEU on ChineseVietnamese and 38.0 BLEU on VietnameseChinese on the public test sets, which outperform several strong baselines.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#35199;&#29677;&#29273;&#25991;&#25991;&#26412;&#31616;&#21270;&#30340;&#29992;&#25143;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#29992;&#25143;&#20559;&#22909;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#19981;&#22914;&#35199;&#29677;&#29273;&#25991;&#27169;&#22411;&#22312;&#21516;&#19968;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35780;&#20272;&#20013;&#30340;&#35821;&#26009;&#24211;&#65292;&#24076;&#26395;&#25512;&#21160;&#35199;&#29677;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2308.07556</link><description>&lt;p&gt;
&#35199;&#29677;&#29273;&#25991;&#25991;&#26412;&#31616;&#21270;&#30340;&#29992;&#25143;&#20013;&#24515;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A User-Centered Evaluation of Spanish Text Simplification. (arXiv:2308.07556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07556
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#35199;&#29677;&#29273;&#25991;&#25991;&#26412;&#31616;&#21270;&#30340;&#29992;&#25143;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#29992;&#25143;&#20559;&#22909;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#19981;&#22914;&#35199;&#29677;&#29273;&#25991;&#27169;&#22411;&#22312;&#21516;&#19968;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35780;&#20272;&#20013;&#30340;&#35821;&#26009;&#24211;&#65292;&#24076;&#26395;&#25512;&#21160;&#35199;&#29677;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#35821;&#26009;&#24211;&#65292;&#19968;&#20010;&#38024;&#23545;&#22797;&#26434;&#21477;&#23376;&#21644;&#19968;&#20010;&#38024;&#23545;&#22797;&#26434;&#35789;&#35821;&#30340;&#35782;&#21035;&#65292;&#23545;&#35199;&#29677;&#29273;&#25991;&#25991;&#26412;&#31616;&#21270;&#36827;&#34892;&#20102;&#19968;&#20010;&#29983;&#20135;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#23558;&#26368;&#27969;&#34892;&#30340;&#35199;&#29677;&#29273;&#29305;&#23450;&#21487;&#35835;&#24615;&#35780;&#20998;&#19982;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#21518;&#32773;&#22312;&#39044;&#27979;&#29992;&#25143;&#23545;&#25991;&#26412;&#31616;&#21270;&#30340;&#20559;&#22909;&#26041;&#38754;&#22987;&#32456;&#26356;&#22909;&#12290;&#20316;&#20026;&#25105;&#20204;&#20998;&#26512;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#22914;&#20165;&#38480;&#20110;&#35199;&#29677;&#29273;&#25991;&#30340;&#27169;&#22411;&#65292;&#20294;&#25152;&#26377;&#27169;&#22411;&#36807;&#20110;&#39057;&#32321;&#22320;&#20851;&#27880;&#32479;&#35745;&#29305;&#24449;&#65292;&#22914;&#21477;&#23376;&#38271;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#35780;&#20272;&#20013;&#30340;&#35821;&#26009;&#24211;&#21457;&#24067;&#32473;&#26356;&#24191;&#27867;&#30340;&#31038;&#21306;&#65292;&#24076;&#26395;&#25512;&#21160;&#35199;&#29677;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an evaluation of text simplification (TS) in Spanish for a production system, by means of two corpora focused in both complex-sentence and complex-word identification. We compare the most prevalent Spanish-specific readability scores with neural networks, and show that the latter are consistently better at predicting user preferences regarding TS. As part of our analysis, we find that multilingual models underperform against equivalent Spanish-only models on the same task, yet all models focus too often on spurious statistical features, such as sentence length. We release the corpora in our evaluation to the broader community with the hopes of pushing forward the state-of-the-art in Spanish natural language processing.
&lt;/p&gt;</description></item><item><title>CALYPSO&#26159;&#19968;&#20010;&#20197;LLMs&#20026;&#22522;&#30784;&#30340;&#25509;&#21475;&#31995;&#32479;&#65292;&#26088;&#22312;&#25903;&#25345;Dungeon Masters&#22312;&#40857;&#19982;&#22320;&#19979;&#22478;&#21644;&#26700;&#38754;&#28216;&#25103;&#20013;&#12290;&#23427;&#23558;&#28216;&#25103;&#32972;&#26223;&#20449;&#24687;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#25955;&#25991;&#65292;&#24182;&#25552;&#20379;&#20449;&#24687;&#21644;&#28789;&#24863;&#25903;&#25345;&#65292;&#20197;&#24110;&#21161;DM&#22312;&#28216;&#25103;&#20013;&#32500;&#25345;&#36830;&#36143;&#24615;&#24182;&#36827;&#34892;&#22836;&#33041;&#39118;&#26292;&#12290;</title><link>http://arxiv.org/abs/2308.07540</link><description>&lt;p&gt;
CALYPSO: &#23558;LLMs&#20316;&#20026;Dungeon Masters&#30340;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
CALYPSO: LLMs as Dungeon Masters' Assistants. (arXiv:2308.07540v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07540
&lt;/p&gt;
&lt;p&gt;
CALYPSO&#26159;&#19968;&#20010;&#20197;LLMs&#20026;&#22522;&#30784;&#30340;&#25509;&#21475;&#31995;&#32479;&#65292;&#26088;&#22312;&#25903;&#25345;Dungeon Masters&#22312;&#40857;&#19982;&#22320;&#19979;&#22478;&#21644;&#26700;&#38754;&#28216;&#25103;&#20013;&#12290;&#23427;&#23558;&#28216;&#25103;&#32972;&#26223;&#20449;&#24687;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#25955;&#25991;&#65292;&#24182;&#25552;&#20379;&#20449;&#24687;&#21644;&#28789;&#24863;&#25903;&#25345;&#65292;&#20197;&#24110;&#21161;DM&#22312;&#28216;&#25103;&#20013;&#32500;&#25345;&#36830;&#36143;&#24615;&#24182;&#36827;&#34892;&#22836;&#33041;&#39118;&#26292;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28216;&#25103;&#40857;&#19982;&#22320;&#19979;&#22478;&#20013;&#65292;Dungeon Master&#65288;DM&#65289;&#30340;&#35282;&#33394;&#26159;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;&#20219;&#21153;&#12290;DM&#24517;&#39035;&#20102;&#35299;&#28216;&#25103;&#32972;&#26223;&#21644;&#24618;&#29289;&#30340;&#20449;&#24687;&#65292;&#32508;&#21512;&#22330;&#26223;&#23637;&#31034;&#32473;&#20854;&#20182;&#29609;&#23478;&#65292;&#24182;&#23545;&#29609;&#23478;&#19982;&#22330;&#26223;&#30340;&#20114;&#21160;&#20316;&#20986;&#21709;&#24212;&#12290;&#22312;&#20445;&#25345;&#25925;&#20107;&#36830;&#36143;&#24615;&#30340;&#21516;&#26102;&#23436;&#25104;&#25152;&#26377;&#20219;&#21153;&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#26469;&#35828;&#26159;&#19968;&#39033;&#19981;&#23567;&#30340;&#25361;&#25112;&#65292;&#20351;&#26032;&#29609;&#23478;&#38590;&#20197;&#25509;&#36817;&#21644;&#29702;&#35299;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;ChatGPT&#34920;&#29616;&#20986;&#29983;&#25104;&#36830;&#36143;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30340;&#38750;&#20961;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19982;DM&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#35780;&#20272;&#65292;&#20197;&#30830;&#23450;&#22312;&#40857;&#19982;&#22320;&#19979;&#22478;&#21644;&#26700;&#38754;&#28216;&#25103;&#31561;&#39046;&#22495;&#20013;&#20351;&#29992;LLMs&#30340;&#29992;&#20363;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;CALYPSO&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#30028;&#38754;&#31995;&#32479;&#65292;&#20026;DM&#25552;&#20379;&#19982;&#20182;&#20204;&#33258;&#24049;&#30340;&#22330;&#26223;&#30456;&#20851;&#30340;&#20449;&#24687;&#21644;&#28789;&#24863;&#25903;&#25345;&#12290;CALYPSO&#23558;&#28216;&#25103;&#32972;&#26223;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#25955;&#25991;&#65292;&#24182;&#24110;&#21161;DM&#22312;&#28216;&#25103;&#36807;&#31243;&#20013;&#36827;&#34892;&#22836;&#33041;&#39118;&#26292;&#65292;&#32780;&#19981;&#20250;&#20998;&#25955;DM&#30340;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The role of a Dungeon Master, or DM, in the game Dungeons &amp; Dragons is to perform multiple tasks simultaneously. The DM must digest information about the game setting and monsters, synthesize scenes to present to other players, and respond to the players' interactions with the scene. Doing all of these tasks while maintaining consistency within the narrative and story world is no small feat of human cognition, making the task tiring and unapproachable to new players. Large language models (LLMs) like GPT-3 and ChatGPT have shown remarkable abilities to generate coherent natural language text. In this paper, we conduct a formative evaluation with DMs to establish the use cases of LLMs in D&amp;D and tabletop gaming generally. We introduce CALYPSO, a system of LLM-powered interfaces that support DMs with information and inspiration specific to their own scenario. CALYPSO distills game context into bite-sized prose and helps brainstorm ideas without distracting the DM from the game. When give
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32463;&#36807;&#20248;&#21270;&#30340;BERT&#21644;LSTM&#27169;&#22411;&#65292;&#20174;&#19978;&#24066;&#20844;&#21496;&#30340;10-K&#25253;&#21578;&#20013;&#35782;&#21035;&#20986;&#21033;&#30410;&#30456;&#20851;&#26041;-&#26448;&#26009;&#20449;&#24687;&#65292;&#26174;&#33879;&#20248;&#20110;&#20851;&#38190;&#35789;&#25628;&#32034;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07522</link><description>&lt;p&gt;
&#20351;&#29992;&#32463;&#36807;&#20248;&#21270;&#30340;BERT&#21644;LSTM&#27169;&#22411;&#20174;10-K&#25253;&#21578;&#20013;&#25214;&#21040;&#21033;&#30410;&#30456;&#20851;&#26041;-&#26448;&#26009;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Finding Stakeholder-Material Information from 10-K Reports using Fine-Tuned BERT and LSTM Models. (arXiv:2308.07522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32463;&#36807;&#20248;&#21270;&#30340;BERT&#21644;LSTM&#27169;&#22411;&#65292;&#20174;&#19978;&#24066;&#20844;&#21496;&#30340;10-K&#25253;&#21578;&#20013;&#35782;&#21035;&#20986;&#21033;&#30410;&#30456;&#20851;&#26041;-&#26448;&#26009;&#20449;&#24687;&#65292;&#26174;&#33879;&#20248;&#20110;&#20851;&#38190;&#35789;&#25628;&#32034;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#19978;&#24066;&#20844;&#21496;&#37117;&#35201;&#25353;&#29031;&#32852;&#37030;&#35777;&#21048;&#27861;&#30340;&#35268;&#23450;&#22312;&#24180;&#24230;10-K&#25253;&#21578;&#20013;&#25259;&#38706;&#20854;&#19994;&#21153;&#21644;&#36130;&#21153;&#27963;&#21160;&#12290;&#27599;&#20221;&#25253;&#21578;&#36890;&#24120;&#26377;&#25968;&#30334;&#39029;&#65292;&#20351;&#24471;&#20154;&#24037;&#35835;&#32773;&#24456;&#38590;&#39640;&#25928;&#22320;&#35782;&#21035;&#21644;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20351;&#29992;&#32463;&#36807;&#20248;&#21270;&#30340;BERT&#27169;&#22411;&#21644;&#24102;&#26377;LSTM&#23618;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#35782;&#21035;&#21033;&#30410;&#30456;&#20851;&#26041;-&#26448;&#26009;&#20449;&#24687;&#65292;&#21363;&#25658;&#24102;&#26377;&#20851;&#20844;&#21496;&#23545;&#20854;&#21033;&#30410;&#30456;&#20851;&#26041;&#65288;&#21253;&#25324;&#23458;&#25143;&#12289;&#21592;&#24037;&#12289;&#25237;&#36164;&#32773;&#20197;&#21450;&#31038;&#21306;&#21644;&#33258;&#28982;&#29615;&#22659;&#65289;&#24433;&#21709;&#30340;&#38472;&#36848;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#20851;&#38190;&#35789;&#25628;&#32034;&#26469;&#35782;&#21035;&#36825;&#31181;&#20449;&#24687;&#65292;&#36825;&#26159;&#25105;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;2022&#24180;&#21457;&#24067;&#30340;62&#20221;10-K&#25253;&#21578;&#20013;&#36817;6000&#20010;&#21477;&#23376;&#36827;&#34892;&#19994;&#21153;&#19987;&#23478;&#26631;&#27880;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#26368;&#20339;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#20013;&#36798;&#21040;&#20102;0.904&#30340;&#20934;&#30830;&#29575;&#21644;0.899&#30340;F1&#20998;&#25968;&#65292;&#36828;&#39640;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;0.781&#21644;0.749&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30340;&#24037;&#20316;&#24050;&#32463;
&lt;/p&gt;
&lt;p&gt;
All public companies are required by federal securities law to disclose their business and financial activities in their annual 10-K reports. Each report typically spans hundreds of pages, making it difficult for human readers to identify and extract the material information efficiently. To solve the problem, I have fine-tuned BERT models and RNN models with LSTM layers to identify stakeholder-material information, defined as statements that carry information about a company's influence on its stakeholders, including customers, employees, investors, and the community and natural environment. The existing practice uses keyword search to identify such information, which is my baseline model. Using business expert-labeled training data of nearly 6,000 sentences from 62 10-K reports published in 2022, the best model has achieved an accuracy of 0.904 and an F1 score of 0.899 in test data, significantly above the baseline model's 0.781 and 0.749 respectively. Furthermore, the same work was r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#21457;&#29616;LLMs&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#30340;&#21487;&#34892;&#26041;&#27861;&#65292;&#20294;&#22312;&#38656;&#35201;&#35814;&#32454;&#20449;&#24687;&#26102;&#20173;&#19981;&#22914;&#20256;&#32479;&#24037;&#20855;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2308.07505</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Data Race Detection Using Large Language Models. (arXiv:2308.07505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#21457;&#29616;LLMs&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#30340;&#21487;&#34892;&#26041;&#27861;&#65292;&#20294;&#22312;&#38656;&#35201;&#35814;&#32454;&#20449;&#24687;&#26102;&#20173;&#19981;&#22914;&#20256;&#32479;&#24037;&#20855;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#22312;&#20998;&#26512;&#21644;&#20248;&#21270;&#39640;&#24615;&#33021;&#35745;&#31639;&#31243;&#24207;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#36991;&#20813;&#20102;&#36164;&#28304;&#23494;&#38598;&#22411;&#25163;&#21160;&#24037;&#20855;&#30340;&#21019;&#24314;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#25216;&#26415;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;DRB-ML&#30340;&#19987;&#29992;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#28304;&#33258;DataRaceBench&#65292;&#24182;&#20855;&#26377;&#31934;&#32454;&#30340;&#26631;&#31614;&#65292;&#26174;&#31034;&#20102;&#25968;&#25454;&#31454;&#20105;&#23545;&#21450;&#20854;&#30456;&#20851;&#21464;&#37327;&#12289;&#34892;&#21495;&#21644;&#35835;/&#20889;&#20449;&#24687;&#30340;&#23384;&#22312;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;DRB-ML&#35780;&#20272;&#20102;&#20195;&#34920;&#24615;&#30340;LLMs&#24182;&#24494;&#35843;&#20102;&#24320;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#30340;&#21487;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#38656;&#35201;&#26377;&#20851;&#24341;&#36215;&#25968;&#25454;&#31454;&#20105;&#30340;&#21464;&#37327;&#23545;&#30340;&#35814;&#32454;&#20449;&#24687;&#26102;&#65292;&#23427;&#20204;&#20173;&#26080;&#27861;&#19982;&#20256;&#32479;&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#24037;&#20855;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. In this paper, we explore a novel LLM-based data race detection approach combining prompting engineering and fine-tuning techniques. We create a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. DRB-ML is then used to evaluate representative LLMs and fine-tune open-source ones. Our experiment shows that LLMs can be a viable approach to data race detection. However, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.
&lt;/p&gt;</description></item><item><title>SOTASTREAM&#26159;&#19968;&#31181;&#26426;&#22120;&#32763;&#35793;&#35757;&#32451;&#30340;&#27969;&#24335;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26080;&#38480;&#25490;&#21015;&#30340;&#35757;&#32451;&#25968;&#25454;&#27969;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#30340;&#38745;&#24577;&#25968;&#25454;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#20197;&#21450;&#32791;&#26102;&#12289;&#26114;&#36149;&#21644;&#32321;&#29712;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07489</link><description>&lt;p&gt;
SOTASTREAM&#65306;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#35757;&#32451;&#30340;&#27969;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SOTASTREAM: A Streaming Approach to Machine Translation Training. (arXiv:2308.07489v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07489
&lt;/p&gt;
&lt;p&gt;
SOTASTREAM&#26159;&#19968;&#31181;&#26426;&#22120;&#32763;&#35793;&#35757;&#32451;&#30340;&#27969;&#24335;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26080;&#38480;&#25490;&#21015;&#30340;&#35757;&#32451;&#25968;&#25454;&#27969;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#30340;&#38745;&#24577;&#25968;&#25454;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#20197;&#21450;&#32791;&#26102;&#12289;&#26114;&#36149;&#21644;&#32321;&#29712;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#32763;&#35793;&#24037;&#20855;&#21253;&#20351;&#29992;&#25968;&#25454;&#20934;&#22791;&#27493;&#39588;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#21487;&#20197;&#30452;&#25509;&#34987;&#35757;&#32451;&#22120;&#20351;&#29992;&#30340;&#24352;&#37327;&#26684;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20934;&#22791;&#27493;&#39588;&#19982;&#29616;&#20195;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#23454;&#36341;&#36234;&#26469;&#36234;&#19981;&#30456;&#31526;&#65292;&#22240;&#20026;&#36825;&#20010;&#36807;&#31243;&#20135;&#29983;&#20102;&#19968;&#20010;&#38745;&#24577;&#30340;&#12289;&#19981;&#21487;&#26356;&#25913;&#30340;&#35757;&#32451;&#25968;&#25454;&#29256;&#26412;&#65292;&#20351;&#24471;&#24120;&#35265;&#30340;&#35757;&#32451;&#26102;&#38656;&#27714;&#21464;&#24471;&#22256;&#38590;&#65288;&#20363;&#22914;&#65292;&#23376;&#35789;&#37319;&#26679;&#65289;&#12289;&#32791;&#26102;&#65288;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#21487;&#33021;&#38656;&#35201;&#20960;&#22825;&#26102;&#38388;&#65289;&#12289;&#26114;&#36149;&#65288;&#20363;&#22914;&#65292;&#30913;&#30424;&#31354;&#38388;&#65289;&#21644;&#32321;&#29712;&#65288;&#31649;&#29702;&#23454;&#39564;&#32452;&#21512;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#30340;&#29983;&#25104;&#19982;&#25968;&#25454;&#30340;&#20351;&#29992;&#20998;&#31163;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#27809;&#26377;&#21333;&#29420;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#65307;&#25968;&#25454;&#30340;&#29983;&#25104;&#20135;&#29983;&#20102;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26080;&#38480;&#25490;&#21015;&#27969;&#65292;&#35757;&#32451;&#22120;&#22312;&#28040;&#36153;&#25968;&#25454;&#26102;&#23558;&#20854;&#36716;&#25442;&#20026;&#24352;&#37327;&#24182;&#36827;&#34892;&#25209;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#25968;&#25454;&#27969;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#21487;&#23450;&#20041;&#30340;&#25805;&#20316;&#31526;&#36827;&#34892;&#23454;&#26102;&#20462;&#25913;&#65292;&#20363;&#22914;&#23376;&#35789;&#37319;&#26679;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine translation toolkits make use of a data preparation step wherein raw data is transformed into a tensor format that can be used directly by the trainer. This preparation step is increasingly at odds with modern research and development practices because this process produces a static, unchangeable version of the training data, making common training-time needs difficult (e.g., subword sampling), time-consuming (preprocessing with large data can take days), expensive (e.g., disk space), and cumbersome (managing experiment combinatorics). We propose an alternative approach that separates the generation of data from the consumption of that data. In this approach, there is no separate pre-processing step; data generation produces an infinite stream of permutations of the raw training data, which the trainer tensorizes and batches as it is consumed. Additionally, this data stream can be manipulated by a set of user-definable operators that provide on-the-fly modifications, such 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#35757;&#32451;&#30446;&#26631;O-1&#65292;&#36890;&#36807;&#22686;&#24378;&#31070;&#35861;&#20551;&#35774;&#26469;&#20943;&#23569;&#35757;&#32451;&#20559;&#24046;&#65292;&#24182;&#32479;&#19968;&#35757;&#32451;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;O-1&#22312;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#30456;&#23545;&#20110;EMBR&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.07486</link><description>&lt;p&gt;
O-1: &#33258;&#21160;&#26631;&#27880;&#19982;1-best&#20551;&#35774;&#30340;&#33258;&#25105;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
O-1: Self-training with Oracle and 1-best Hypothesis. (arXiv:2308.07486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07486
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#35757;&#32451;&#30446;&#26631;O-1&#65292;&#36890;&#36807;&#22686;&#24378;&#31070;&#35861;&#20551;&#35774;&#26469;&#20943;&#23569;&#35757;&#32451;&#20559;&#24046;&#65292;&#24182;&#32479;&#19968;&#35757;&#32451;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;O-1&#22312;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#30456;&#23545;&#20110;EMBR&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;O-1&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#33258;&#25105;&#35757;&#32451;&#30446;&#26631;&#65292;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;&#20559;&#24046;&#65292;&#32479;&#19968;&#35821;&#38899;&#35782;&#21035;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;O-1&#26159;&#26399;&#26395;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;EMBR&#65289;&#30340;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;&#21464;&#20307;&#65292;&#23427;&#22686;&#24378;&#20102;&#31070;&#35861;&#20551;&#35774;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#24212;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;SpeechStew&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#20869;&#37096;&#25968;&#25454;&#38598;&#36827;&#34892;&#35782;&#21035;&#25928;&#26524;&#30340;&#23454;&#35777;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;Speechstew&#19978;&#65292;&#30456;&#23545;&#20110;&#23558;&#23454;&#38469;&#24615;&#33021;&#19982;&#31070;&#35861;&#24615;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;43%&#30340;EMBR&#65292;O-1&#30446;&#26631;&#36890;&#36807;80%&#30340;&#30456;&#23545;&#32553;&#23567;&#20102;&#36825;&#19968;&#24046;&#36317;&#12290;&#22312;SpeechStew&#30340;&#21508;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;O-1&#30456;&#23545;&#20110;EMBR&#23454;&#29616;&#20102;13%&#21040;25%&#30340;&#30456;&#23545;&#25913;&#36827;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#22312;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;EMBR&#35757;&#32451;&#30340;&#31070;&#35861;&#35789;&#38169;&#35823;&#29575;&#30456;&#23545;&#32553;&#23567;&#20102;12%&#30340;&#24046;&#36317;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;O-1&#30456;&#23545;&#20110;EMBR&#22312;&#35789;&#38169;&#35823;&#29575;&#19978;&#23454;&#29616;&#20102;9%&#30340;&#30456;&#23545;&#25913;&#36827;&#65292;&#36825;&#34920;&#26126;&#20102;&#35813;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce O-1, a new self-training objective to reduce training bias and unify training and evaluation metrics for speech recognition. O-1 is a faster variant of Expected Minimum Bayes Risk (EMBR), that boosts the oracle hypothesis and can accommodate both supervised and unsupervised data. We demonstrate the effectiveness of our approach in terms of recognition on publicly available SpeechStew datasets and a large-scale, in-house data set. On Speechstew, the O-1 objective closes the gap between the actual and oracle performance by 80\% relative compared to EMBR which bridges the gap by 43\% relative. O-1 achieves 13\% to 25\% relative improvement over EMBR on the various datasets that SpeechStew comprises of, and a 12\% relative gap reduction with respect to the oracle WER over EMBR training on the in-house dataset. Overall, O-1 results in a 9\% relative improvement in WER over EMBR, thereby speaking to the scalability of the proposed objective for large-scale datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20154;&#31867;&#22312;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#20250;&#23545;&#35789;&#27719;&#20351;&#29992;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#20135;&#29983;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#35821;&#35328;&#28436;&#21464;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.07462</link><description>&lt;p&gt;
&#29609;&#24324;&#25991;&#23383;&#65306;&#27604;&#36739;ChatGPT&#21644;&#20154;&#31867;&#30340;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans. (arXiv:2308.07462v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07462
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20154;&#31867;&#22312;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#20250;&#23545;&#35789;&#27719;&#20351;&#29992;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#20135;&#29983;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#35821;&#35328;&#28436;&#21464;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT&#65289;&#21644;ChatGPT&#31561;&#24037;&#20855;&#30340;&#24341;&#20837;&#24341;&#21457;&#20102;&#19968;&#22330;&#38761;&#21629;&#65292;&#21487;&#20197;&#25913;&#21464;&#25991;&#26412;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#36825;&#23545;&#35835;&#32773;&#30340;&#35821;&#35328;&#33021;&#21147;&#20197;&#21450;&#26032;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#22521;&#35757;&#26159;&#21542;&#20250;&#20135;&#29983;&#24433;&#21709;&#20855;&#26377;&#35768;&#22810;&#21547;&#20041;&#65311;&#23427;&#26159;&#21542;&#20250;&#24433;&#21709;&#35821;&#35328;&#30340;&#28436;&#21464;&#65311;&#25105;&#20204;&#20851;&#27880;&#35821;&#35328;&#30340;&#19968;&#20010;&#29305;&#23450;&#26041;&#38754;&#65306;&#35789;&#35821;&#65307;&#22312;&#32534;&#20889;&#32473;&#23450;&#25991;&#26412;&#26102;&#65292;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#20250;&#22686;&#21152;&#25110;&#20943;&#23569;&#20351;&#29992;&#30340;&#35789;&#27719;&#37327;&#25110;&#35789;&#27719;&#20016;&#23500;&#24230;&#65288;&#29702;&#35299;&#20026;&#20070;&#38754;&#25110;&#21475;&#22836;&#34920;&#36798;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#35789;&#27719;&#25968;&#37327;&#65289;&#65311;&#36825;&#23545;&#35789;&#35821;&#26377;&#24433;&#21709;&#65292;&#22240;&#20026;&#26410;&#21253;&#21547;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#30340;&#35789;&#35821;&#24448;&#24448;&#20250;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#21463;&#27426;&#36814;&#65292;&#24182;&#26368;&#32456;&#21487;&#33021;&#28040;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#21644;&#20154;&#31867;&#30340;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of Artificial Intelligence (AI) generative language models such as GPT (Generative Pre-trained Transformer) and tools such as ChatGPT has triggered a revolution that can transform how text is generated. This has many implications, for example, as AI-generated text becomes a significant fraction of the text in many disciplines, would this have an effect on the language capabilities of readers and also on the training of newer AI tools? Would it affect the evolution of languages? Focusing on one specific aspect of the language: words; will the use of tools such as ChatGPT increase or reduce the vocabulary used or the lexical richness (understood as the number of different words used in a written or oral production) when writing a given text? This has implications for words, as those not included in AI-generated content will tend to be less and less popular and may eventually be lost. In this work, we perform an initial comparison of the vocabulary and lexical richness of
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20026;&#20135;&#21518;&#25252;&#29702;&#20154;&#21592;&#25552;&#20379;&#19978;&#19979;&#25991;&#29305;&#23450;&#30340;&#20849;&#24773;&#25903;&#25345;&#65292;&#20854;&#20013;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#65292;&#20154;&#31867;&#29992;&#25143;&#26356;&#21916;&#27426;&#23427;&#30340;&#22238;&#22797;&#65292;&#29983;&#25104;&#27169;&#22411;&#30340;&#22238;&#22797;&#26377;&#26102;&#28151;&#28102;&#25110;&#26080;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.07407</link><description>&lt;p&gt;
&#12298;&#20026;&#20135;&#21518;&#24773;&#32490;&#19982;&#28966;&#34385;&#38556;&#30861;&#24320;&#21457;&#21644;&#35780;&#20272;&#19977;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#12299;
&lt;/p&gt;
&lt;p&gt;
Development and Evaluation of Three Chatbots for Postpartum Mood and Anxiety Disorders. (arXiv:2308.07407v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07407
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20026;&#20135;&#21518;&#25252;&#29702;&#20154;&#21592;&#25552;&#20379;&#19978;&#19979;&#25991;&#29305;&#23450;&#30340;&#20849;&#24773;&#25903;&#25345;&#65292;&#20854;&#20013;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#65292;&#20154;&#31867;&#29992;&#25143;&#26356;&#21916;&#27426;&#23427;&#30340;&#22238;&#22797;&#65292;&#29983;&#25104;&#27169;&#22411;&#30340;&#22238;&#22797;&#26377;&#26102;&#28151;&#28102;&#25110;&#26080;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20135;&#21518;&#25903;&#25345;&#22269;&#38469;&#65288;PSI&#65289;&#38750;&#30408;&#21033;&#32452;&#32455;&#21512;&#20316;&#65292;&#35813;&#32452;&#32455;&#33268;&#21147;&#20110;&#20026;&#20135;&#21518;&#25252;&#29702;&#20154;&#21592;&#25552;&#20379;&#24773;&#32490;&#21644;&#28966;&#34385;&#38556;&#30861;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20197;&#25552;&#20379;&#19978;&#19979;&#25991;&#29305;&#23450;&#30340;&#20849;&#24773;&#25903;&#25345;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#35268;&#21017;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#23545;&#25105;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#27979;&#37327;&#21644;&#20154;&#31867;&#38382;&#21367;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#65292;&#36755;&#20986;&#25509;&#36817;&#22320;&#38754;&#30495;&#20540;&#21442;&#32771;&#65292;&#24182;&#19988;&#21547;&#26377;&#26368;&#39640;&#27700;&#24179;&#30340;&#20849;&#24773;&#12290;&#20154;&#31867;&#29992;&#25143;&#26356;&#21916;&#27426;&#22522;&#20110;&#35268;&#21017;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#19978;&#19979;&#25991;&#29305;&#23450;&#30340;&#12289;&#20154;&#31867;&#21270;&#30340;&#22238;&#22797;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#20063;&#33021;&#20135;&#29983;&#20849;&#24773;&#22238;&#22797;&#65292;&#24182;&#34987;&#20154;&#31867;&#29992;&#25143;&#25551;&#36848;&#20026;&#26377;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#24120;&#24120;&#23548;&#33268;&#22238;&#22797;&#28151;&#28102;&#25110;&#26080;&#24847;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#35752;&#35770;&#22522;&#20110;&#35268;&#21017;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#38469;&#22909;&#22788;&#20316;&#20986;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In collaboration with Postpartum Support International (PSI), a non-profit organization dedicated to supporting caregivers with postpartum mood and anxiety disorders, we developed three chatbots to provide context-specific empathetic support to postpartum caregivers, leveraging both rule-based and generative models. We present and evaluate the performance of our chatbots using both machine-based metrics and human-based questionnaires. Overall, our rule-based model achieves the best performance, with outputs that are close to ground truth reference and contain the highest levels of empathy. Human users prefer the rule-based chatbot over the generative chatbot for its context-specific and human-like replies. Our generative chatbot also produced empathetic responses and was described by human users as engaging. However, limitations in the training dataset often result in confusing or nonsensical responses. We conclude by discussing practical benefits of rule-based vs. generative models fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35821;&#38899;&#27169;&#22411;&#20013;&#21033;&#29992;&#25991;&#26412;&#27880;&#20837;&#36827;&#34892;&#38750;ASR&#20219;&#21153;&#30340;&#36741;&#21161;&#65292;&#24182;&#20351;&#29992;&#32852;&#21512;&#35757;&#32451;&#31639;&#27861;&#25552;&#21319;&#22823;&#20889;&#20934;&#30830;&#29575;&#21644;&#20132;&#26367;&#26816;&#27979;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07395</link><description>&lt;p&gt;
&#35821;&#38899;&#27169;&#22411;&#20013;&#29992;&#20110;&#22823;&#20889;&#21644;&#20132;&#26367;&#39044;&#27979;&#30340;&#25991;&#26412;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Text Injection for Capitalization and Turn-Taking Prediction in Speech Models. (arXiv:2308.07395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35821;&#38899;&#27169;&#22411;&#20013;&#21033;&#29992;&#25991;&#26412;&#27880;&#20837;&#36827;&#34892;&#38750;ASR&#20219;&#21153;&#30340;&#36741;&#21161;&#65292;&#24182;&#20351;&#29992;&#32852;&#21512;&#35757;&#32451;&#31639;&#27861;&#25552;&#21319;&#22823;&#20889;&#20934;&#30830;&#29575;&#21644;&#20132;&#26367;&#26816;&#27979;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#27880;&#20837;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#20854;&#20013;&#38750;&#37197;&#23545;&#30340;&#32431;&#25991;&#26412;&#25968;&#25454;&#29992;&#20110;&#34917;&#20805;&#38899;&#39057;-&#25991;&#26412;&#25968;&#25454;&#65292;&#24050;&#26174;&#31034;&#20986;&#23545;&#35789;&#38169;&#35823;&#29575;&#26377;&#26174;&#33879;&#25913;&#21892;&#12290;&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#25991;&#26412;&#27880;&#20837;&#29992;&#20110;&#36741;&#21161;&#20219;&#21153;&#65292;&#22312;E2E&#27169;&#22411;&#20013;&#24120;&#29992;&#20110;&#38750;ASR&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32852;&#21512;&#31471;&#21040;&#31471;&#21644;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#65288;JEIT&#65289;&#20316;&#20026;&#25105;&#20204;&#30340;&#25991;&#26412;&#27880;&#20837;&#31639;&#27861;&#65292;&#35757;&#32451;&#19968;&#20010;ASR&#27169;&#22411;&#26469;&#23436;&#25104;&#20004;&#20010;&#36741;&#21161;&#20219;&#21153;&#12290;&#31532;&#19968;&#20010;&#20219;&#21153;&#26159;&#22823;&#20889;&#65292;&#26159;&#19968;&#31181;&#21435;&#26631;&#20934;&#21270;&#20219;&#21153;&#12290;&#31532;&#20108;&#20010;&#20219;&#21153;&#26159;&#20132;&#26367;&#39044;&#27979;&#65292;&#23581;&#35797;&#35782;&#21035;&#29992;&#25143;&#26159;&#21542;&#24050;&#23436;&#25104;&#23545;&#35805;&#20132;&#26367;&#65292;&#22312;&#25968;&#23383;&#21161;&#29702;&#20114;&#21160;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25991;&#26412;&#27880;&#20837;&#26041;&#27861;&#33021;&#25552;&#21319;&#38271;&#23614;&#25968;&#25454;&#30340;&#22823;&#20889;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#20102;&#20132;&#26367;&#26816;&#27979;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text injection for automatic speech recognition (ASR), wherein unpaired text-only data is used to supplement paired audio-text data, has shown promising improvements for word error rate. This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model. In this work, we use joint end-to-end and internal language model training (JEIT) as our text injection algorithm to train an ASR model which performs two auxiliary tasks. The first is capitalization, which is a de-normalization task. The second is turn-taking prediction, which attempts to identify whether a user has completed their conversation turn in a digital assistant interaction. We show results demonstrating that our text injection method boosts capitalization performance for long-tail data, and improves turn-taking detection recall.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#27880;&#20837;&#25216;&#26415;&#65292;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#25913;&#21892;&#20010;&#20154;&#35782;&#21035;&#31526;&#31867;&#21035;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#22312;&#21307;&#30103;&#35760;&#24405;&#20013;&#25552;&#39640;&#20102;&#20154;&#21517;&#21644;&#26085;&#26399;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07393</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#27880;&#20837;&#26469;&#25552;&#39640;&#35821;&#38899;&#20013;&#20010;&#20154;&#35782;&#21035;&#31526;&#30340;&#35782;&#21035;&#29575;
&lt;/p&gt;
&lt;p&gt;
Using Text Injection to Improve Recognition of Personal Identifiers in Speech. (arXiv:2308.07393v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07393
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#27880;&#20837;&#25216;&#26415;&#65292;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#25913;&#21892;&#20010;&#20154;&#35782;&#21035;&#31526;&#31867;&#21035;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#22312;&#21307;&#30103;&#35760;&#24405;&#20013;&#25552;&#39640;&#20102;&#20154;&#21517;&#21644;&#26085;&#26399;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#35782;&#21035;&#29305;&#23450;&#31867;&#21035;&#65288;&#20363;&#22914;&#20154;&#21517;&#12289;&#26085;&#26399;&#25110;&#20854;&#20182;&#26631;&#35782;&#31526;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#36825;&#20123;&#31867;&#21035;&#20195;&#34920;&#20010;&#20154;&#20449;&#24687;&#65292;&#23545;&#20110;&#21253;&#25324;&#25910;&#38598;&#12289;&#36716;&#24405;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#22312;&#20869;&#30340;&#25968;&#25454;&#30340;&#21512;&#35268;&#20351;&#29992;&#38656;&#35201;&#29305;&#21035;&#27880;&#24847;&#12290;&#30830;&#20445;&#20010;&#20154;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#23436;&#20840;&#21024;&#38500;&#25110;&#20462;&#39280;&#25910;&#38598;&#21040;&#30340;&#21487;&#35782;&#21035;&#20010;&#20154;&#20449;&#24687;&#65288;PII&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#20250;&#23548;&#33268;ASR&#27169;&#22411;&#22312;&#36825;&#20123;&#31867;&#21035;&#19978;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#25105;&#20204;&#20351;&#29992;&#25991;&#26412;&#27880;&#20837;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;PII&#31867;&#21035;&#30340;&#34394;&#20551;&#25991;&#26412;&#26367;&#20195;&#29289;&#65292;&#20197;&#25552;&#39640;PII&#31867;&#21035;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21307;&#30103;&#35760;&#24405;&#20013;&#20154;&#21517;&#21644;&#26085;&#26399;&#30340;&#21484;&#22238;&#29575;&#26174;&#33879;&#25552;&#39640;&#65292;&#21516;&#26102;&#25913;&#21892;&#20102;&#25972;&#20307;&#30340;&#35782;&#21035;&#38169;&#35823;&#29575;&#12290;&#23545;&#20110;&#23383;&#27597;&#25968;&#23383;&#24207;&#21015;&#65292;&#25105;&#20204;&#36824;&#25913;&#21892;&#20102;&#23383;&#31526;&#38169;&#35823;&#29575;&#21644;&#21477;&#23376;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate recognition of specific categories, such as persons' names, dates or other identifiers is critical in many Automatic Speech Recognition (ASR) applications. As these categories represent personal information, ethical use of this data including collection, transcription, training and evaluation demands special care. One way of ensuring the security and privacy of individuals is to redact or eliminate Personally Identifiable Information (PII) from collection altogether. However, this results in ASR models that tend to have lower recognition accuracy of these categories. We use text-injection to improve the recognition of PII categories by including fake textual substitutes of PII categories in the training data using a text injection method. We demonstrate substantial improvement to Recall of Names and Dates in medical notes while improving overall WER. For alphanumeric digit sequences we show improvements to Character Error Rate and Sentence Accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#20852;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31227;&#21160;&#22686;&#24378;&#29616;&#23454;&#20013;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#24182;&#37319;&#29992;&#25913;&#36827;&#30340;Lewis&#20449;&#20196;&#21338;&#24328;&#35757;&#32451;&#20004;&#20010;&#20195;&#29702;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#20004;&#20010;&#20195;&#29702;&#33021;&#22815;&#22312;&#22024;&#26434;&#30340;&#20449;&#36947;&#20013;&#20351;&#29992;&#26497;&#23567;&#30340;&#28040;&#24687;&#36827;&#34892;&#20851;&#20110;&#35270;&#35273;&#25968;&#25454;&#30340;&#25277;&#35937;&#24605;&#24819;&#30340;&#20132;&#27969;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#35270;&#35273;&#25968;&#25454;&#36890;&#20449;&#24310;&#36831;&#23545;&#20307;&#39564;&#36136;&#37327;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.07342</link><description>&lt;p&gt;
AR&#30340;&#26032;&#20852;&#36890;&#35759;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Emergent communication for AR. (arXiv:2308.07342v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#20852;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31227;&#21160;&#22686;&#24378;&#29616;&#23454;&#20013;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#24182;&#37319;&#29992;&#25913;&#36827;&#30340;Lewis&#20449;&#20196;&#21338;&#24328;&#35757;&#32451;&#20004;&#20010;&#20195;&#29702;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#20004;&#20010;&#20195;&#29702;&#33021;&#22815;&#22312;&#22024;&#26434;&#30340;&#20449;&#36947;&#20013;&#20351;&#29992;&#26497;&#23567;&#30340;&#28040;&#24687;&#36827;&#34892;&#20851;&#20110;&#35270;&#35273;&#25968;&#25454;&#30340;&#25277;&#35937;&#24605;&#24819;&#30340;&#20132;&#27969;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#35270;&#35273;&#25968;&#25454;&#36890;&#20449;&#24310;&#36831;&#23545;&#20307;&#39564;&#36136;&#37327;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#22686;&#24378;&#29616;&#23454;&#65288;MAR&#65289;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#25968;&#23383;&#23402;&#29983;&#21644;&#20803;&#23431;&#23449;&#30340;&#26222;&#36941;&#25509;&#21475;&#20043;&#19968;&#65292;&#35201;&#27714;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#24310;&#36831;&#12289;&#35745;&#31639;&#33021;&#21147;&#21644;&#33021;&#25928;&#12290;&#29616;&#26377;&#30340;&#23454;&#29616;MAR&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;&#36793;&#32536;&#35745;&#31639;&#12289;&#20113;&#35745;&#31639;&#21644;&#31532;&#20116;&#20195;&#65288;5G&#65289;&#32593;&#32476;&#31561;&#22810;&#31181;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#25968;&#25454;&#30340;&#22266;&#26377;&#36890;&#20449;&#24310;&#36831;&#23545;&#20307;&#39564;&#36136;&#37327;&#65288;QoE&#65289;&#36896;&#25104;&#26126;&#26174;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;MAR&#20013;&#30340;&#36890;&#20449;&#21327;&#35758;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25913;&#36827;&#30340;Lewis&#20449;&#20196;&#21338;&#24328;&#26469;&#35757;&#32451;&#20004;&#20010;&#20195;&#29702;&#65292;&#20197;&#24418;&#25104;&#31163;&#25955;&#30340;&#36890;&#20449;&#21327;&#35758;&#12290;&#22522;&#20110;&#36825;&#20010;&#21327;&#35758;&#65292;&#20004;&#20010;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#26497;&#23567;&#25968;&#25454;&#22823;&#23567;&#30340;&#28040;&#24687;&#22312;&#22024;&#26434;&#30340;&#20449;&#36947;&#20013;&#30456;&#20114;&#20256;&#36798;&#26377;&#20851;&#35270;&#35273;&#25968;&#25454;&#30340;&#25277;&#35937;&#24605;&#24819;&#65292;&#20174;&#32780;&#23548;&#33268;&#28040;&#24687;&#38169;&#35823;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#23558;&#20449;&#36947;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile augmented reality (MAR) is widely acknowledged as one of the ubiquitous interfaces to the digital twin and Metaverse, demanding unparalleled levels of latency, computational power, and energy efficiency. The existing solutions for realizing MAR combine multiple technologies like edge, cloud computing, and fifth-generation (5G) networks. However, the inherent communication latency of visual data imposes apparent limitations on the quality of experience (QoE). To address the challenge, we propose an emergent semantic communication framework to learn the communication protocols in MAR. Specifically, we train two agents through a modified Lewis signaling game to emerge a discrete communication protocol spontaneously. Based on this protocol, two agents can communicate about the abstract idea of visual data through messages with extremely small data sizes in a noisy channel, which leads to message errors. To better simulate real-world scenarios, we incorporate channel uncertainty into
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07336</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#21644;&#24418;&#24335;&#36923;&#36753;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26041;&#27861;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#20855;&#20307;&#30340;&#28436;&#32462;&#35268;&#21017;&#26469;&#29983;&#25104;&#28436;&#32462;&#31034;&#20363;&#65292;&#20294;&#36825;&#20123;&#35268;&#21017;&#21463;&#38480;&#25110;&#32773;&#26159;&#20219;&#24847;&#30340;&#12290;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#25152;&#33719;&#24471;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#24182;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#19968;&#32452;&#33391;&#22909;&#22522;&#30784;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#24403;&#36825;&#20123;&#35268;&#21017;&#20197;&#22810;&#27493;&#26041;&#24335;&#32452;&#21512;&#26102;&#65292;&#21487;&#20197;&#25512;&#23548;&#20986;&#20219;&#20309;&#20854;&#20182;&#28436;&#32462;&#35268;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#25552;&#20986;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;LMs&#65292;&#21363;$\textbf{FLD}$&#65288;$\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction&#65289;&#65292;&#33719;&#24471;&#20102;&#26356;&#20855;&#27867;&#21270;&#24615;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#28436;&#32462;&#25512;&#29702;&#35821;&#26009;&#24211;&#21487;&#20197;&#22686;&#24378;LMs&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#38754;&#65292;&#20197;&#21450;&#19981;&#21516;&#26041;&#38754;&#26080;&#27861;&#22686;&#24378;&#30340;&#26041;&#38754;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#28436;&#32462;&#35821;&#26009;&#24211;&#25110;&#20854;&#20182;&#26041;&#27861;&#24212;&#29992;&#20110;&#27599;&#20010;&#26041;&#38754;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25805;&#25511;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#34892;&#20026;&#24515;&#29702;&#23398;&#26694;&#26550;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#33021;&#22815;&#26681;&#25454;&#25552;&#31034;&#23637;&#29616;&#29305;&#23450;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#29305;&#24449;&#19978;&#30340;&#34920;&#29616;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21306;&#20998;&#24230;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#22797;&#21046;&#21382;&#21490;&#20154;&#29289;&#30340;&#20010;&#24615;&#21644;&#23545;&#35805;&#39118;&#26684;&#12290;</title><link>http://arxiv.org/abs/2308.07326</link><description>&lt;p&gt;
AI&#25991;&#26412;-&#34892;&#20026;&#65306;&#21487;&#25805;&#25511;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AI Text-to-Behavior: A Study In Steerability. (arXiv:2308.07326v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25805;&#25511;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#34892;&#20026;&#24515;&#29702;&#23398;&#26694;&#26550;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#33021;&#22815;&#26681;&#25454;&#25552;&#31034;&#23637;&#29616;&#29305;&#23450;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#29305;&#24449;&#19978;&#30340;&#34920;&#29616;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21306;&#20998;&#24230;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#22797;&#21046;&#21382;&#21490;&#20154;&#29289;&#30340;&#20010;&#24615;&#21644;&#23545;&#35805;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;ChatGPT&#36845;&#20195;&#29256;&#26412;&#30340;&#21487;&#25805;&#25511;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#21517;&#20026;OCEAN&#65288;&#24320;&#25918;&#24615;&#65292;&#36131;&#20219;&#24515;&#65292;&#22806;&#21521;&#24615;&#65292;&#23452;&#20154;&#24615;&#65292;&#31070;&#32463;&#36136;&#65289;&#30340;&#34892;&#20026;&#24515;&#29702;&#23398;&#26694;&#26550;&#65292;&#25105;&#20204;&#37327;&#21270;&#35780;&#20272;&#20102;&#27169;&#22411;&#23545;&#23450;&#21046;&#25552;&#31034;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;&#24403;&#35201;&#27714;&#29983;&#25104;&#31867;&#20284;&#20110;&#22806;&#21521;&#20154;&#26684;&#30340;&#25991;&#26412;&#26102;&#65292;OCEAN&#24471;&#20998;&#23545;&#40784;&#21040;&#20102;&#35813;&#34892;&#20026;&#29305;&#36136;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#8220;&#24320;&#25918;&#24615;&#8221;&#21576;&#29616;&#20102;&#35821;&#35328;&#30340;&#27169;&#31946;&#24615;&#65292;&#32780;&#8220;&#36131;&#20219;&#24515;&#8221;&#21644;&#8220;&#31070;&#32463;&#36136;&#8221;&#22312;OCEAN&#26694;&#26550;&#20013;&#26126;&#30830;&#22320;&#34987;&#21796;&#36215;&#65292;&#32780;&#8220;&#22806;&#21521;&#24615;&#8221;&#21644;&#8220;&#23452;&#20154;&#24615;&#8221;&#21017;&#23637;&#31034;&#20102;&#19982;&#20854;&#20182;&#29305;&#24449;&#26126;&#26174;&#37325;&#21472;&#20294;&#21448;&#26377;&#26126;&#26174;&#20998;&#31163;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;GPT&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#35782;&#21035;&#20197;&#21450;&#36866;&#24212;&#24494;&#22937;&#25351;&#23548;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#21382;&#21490;&#20154;&#29289;&#27169;&#25311;&#31361;&#20986;&#20102;LLM&#20869;&#21270;&#21644;&#25237;&#23556;&#21487;&#25351;&#23548;&#20010;&#24615;&#30340;&#33021;&#21147;&#65292;&#31934;&#30830;&#22797;&#21046;&#20102;&#20182;&#20204;&#30340;&#21746;&#23398;&#21644;&#23545;&#35805;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research explores the steerability of Large Language Models (LLMs), particularly OpenAI's ChatGPT iterations. By employing a behavioral psychology framework called OCEAN (Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism), we quantitatively gauged the model's responsiveness to tailored prompts. When asked to generate text mimicking an extroverted personality, OCEAN scored the language alignment to that behavioral trait. In our analysis, while "openness" presented linguistic ambiguity, "conscientiousness" and "neuroticism" were distinctly evoked in the OCEAN framework, with "extroversion" and "agreeableness" showcasing a notable overlap yet distinct separation from other traits. Our findings underscore GPT's versatility and ability to discern and adapt to nuanced instructions. Furthermore, historical figure simulations highlighted the LLM's capacity to internalize and project instructible personas, precisely replicating their philosophies and dialogic styles. How
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26816;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21363;&#35753;&#27169;&#22411;&#33258;&#34892;&#36807;&#28388;&#22238;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#26410;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#65292;&#20173;&#28982;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.07308</link><description>&lt;p&gt;
LLM&#33258;&#21355;&#65306;&#36890;&#36807;&#33258;&#26816;&#65292;LLMs&#24847;&#35782;&#21040;&#23427;&#20204;&#34987;&#24858;&#24324;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked. (arXiv:2308.07308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26816;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21363;&#35753;&#27169;&#22411;&#33258;&#34892;&#36807;&#28388;&#22238;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#26410;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#65292;&#20173;&#28982;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33021;&#22815;&#23545;&#20154;&#31867;&#25552;&#31034;&#20570;&#20986;&#39640;&#36136;&#37327;&#25991;&#26412;&#22238;&#24212;&#32780;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22238;&#24212;&#29992;&#25143;&#25552;&#31034;&#26102;&#21487;&#33021;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#32473;&#29992;&#25143;&#25552;&#20379;&#29359;&#32618;&#25351;&#23548;&#65289;&#12290;&#25991;&#29486;&#20013;&#24050;&#32463;&#30528;&#37325;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#26041;&#27861;&#65288;&#20363;&#22914;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23558;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65289;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#32469;&#36807;&#29983;&#25104;&#26377;&#23475;&#25991;&#26412;&#38480;&#21046;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#38450;&#24481;&#36825;&#20123;&#25915;&#20987;&#65292;&#21363;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#33258;&#24049;&#30340;&#22238;&#24212;&#36827;&#34892;&#36807;&#28388;&#12290;&#25105;&#20204;&#30446;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#27809;&#26377;&#34987;&#24494;&#35843;&#20197;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#26469;&#38450;&#27490;&#20854;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have skyrocketed in popularity in recent years due to their ability to generate high-quality text in response to human prompting. However, these models have been shown to have the potential to generate harmful content in response to user prompting (e.g., giving users instructions on how to commit crimes). There has been a focus in the literature on mitigating these risks, through methods like aligning models with human values through reinforcement learning. However, it has been shown that even aligned language models are susceptible to adversarial attacks that bypass their restrictions on generating harmful text. We propose a simple approach to defending against these attacks by having a large language model filter its own responses. Our current results show that even if a model is not fine-tuned to be aligned with human values, it is possible to stop it from presenting harmful content to users by validating the content using a language model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;AI&#30340;&#21305;&#37197;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#21592;&#30340;&#21512;&#20316;&#35774;&#35745;&#65292;&#21457;&#29616;&#24182;&#35299;&#20915;&#20107;&#23454;&#26680;&#26597;&#21592;&#19982;&#25216;&#26415;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#21512;&#20316;&#35774;&#35745;&#20250;&#35758;&#20135;&#29983;&#20102;11&#20010;&#26032;&#30340;&#35774;&#35745;&#24605;&#36335;&#65292;&#21253;&#25324;&#25552;&#39640;&#25928;&#29575;&#21644;&#20010;&#24615;&#21270;&#30340;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#65292;&#24110;&#21161;&#20107;&#23454;&#26680;&#26597;&#21592;&#20934;&#22791;&#26410;&#26469;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#30417;&#27979;&#20559;&#35265;&#65292;&#20197;&#21450;&#25903;&#25345;&#20869;&#37096;&#32452;&#32455;&#12290;</title><link>http://arxiv.org/abs/2308.07213</link><description>&lt;p&gt;
&#20154;&#26412;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20107;&#23454;&#26680;&#26597;&#65306;&#20351;&#29992;AI&#30340;&#21305;&#37197;&#35774;&#35745;&#19982;&#20107;&#23454;&#26680;&#26597;&#21592;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using Matchmaking for AI. (arXiv:2308.07213v1 [cs.HC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;AI&#30340;&#21305;&#37197;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#21592;&#30340;&#21512;&#20316;&#35774;&#35745;&#65292;&#21457;&#29616;&#24182;&#35299;&#20915;&#20107;&#23454;&#26680;&#26597;&#21592;&#19982;&#25216;&#26415;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#21512;&#20316;&#35774;&#35745;&#20250;&#35758;&#20135;&#29983;&#20102;11&#20010;&#26032;&#30340;&#35774;&#35745;&#24605;&#36335;&#65292;&#21253;&#25324;&#25552;&#39640;&#25928;&#29575;&#21644;&#20010;&#24615;&#21270;&#30340;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#65292;&#24110;&#21161;&#20107;&#23454;&#26680;&#26597;&#21592;&#20934;&#22791;&#26410;&#26469;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#30417;&#27979;&#20559;&#35265;&#65292;&#20197;&#21450;&#25903;&#25345;&#20869;&#37096;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#22312;&#24212;&#23545;&#22823;&#37327;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#26469;&#22686;&#24378;&#20107;&#23454;&#26680;&#26597;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#23398;&#26415;&#30740;&#31350;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#22343;&#25253;&#21578;&#20102;&#23545;&#27492;&#31867;&#24037;&#20855;&#30340;&#26377;&#38480;&#37319;&#29992;&#65292;&#22240;&#20026;&#36825;&#20123;&#24037;&#20855;&#19981;&#36275;&#20197;&#19982;&#20107;&#23454;&#26680;&#26597;&#21592;&#30340;&#23454;&#36341;&#12289;&#20215;&#20540;&#35266;&#21644;&#38656;&#27714;&#20445;&#25345;&#19968;&#33268;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#21512;&#20316;&#35774;&#35745;&#26041;&#27861;&#65292;&#21363;AI&#30340;&#21305;&#37197;&#35774;&#35745;&#65292;&#35813;&#26041;&#27861;&#20419;&#36827;&#20107;&#23454;&#26680;&#26597;&#21592;&#12289;&#35774;&#35745;&#24072;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#20849;&#21516;&#21457;&#29616;&#24212;&#20197;&#20309;&#31181;&#26041;&#24335;&#35299;&#20915;&#20107;&#23454;&#26680;&#26597;&#21592;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#19982;22&#21517;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#21592;&#36827;&#34892;&#30340;&#21512;&#20316;&#35774;&#35745;&#20250;&#35758;&#20135;&#29983;&#20102;11&#20010;&#26032;&#30340;&#35774;&#35745;&#24605;&#36335;&#12290;&#36825;&#20123;&#24605;&#36335;&#26377;&#21161;&#20110;&#25552;&#39640;&#20449;&#24687;&#25628;&#32034;&#12289;&#22788;&#29702;&#21644;&#25776;&#20889;&#25928;&#29575;&#20197;&#21450;&#20010;&#24615;&#21270;&#30340;&#20107;&#23454;&#26680;&#26597;&#65307;&#24110;&#21161;&#20107;&#23454;&#26680;&#26597;&#21592;&#20027;&#21160;&#20934;&#22791;&#26410;&#26469;&#30340;&#34394;&#20551;&#20449;&#24687;&#65307;&#30417;&#27979;&#28508;&#22312;&#30340;&#20559;&#35265;&#65307;&#24182;&#25903;&#25345;&#20869;&#37096;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in professional fact-checking is its limited scalability in relation to the magnitude of false information. While many Natural Language Processing (NLP) tools have been proposed to enhance fact-checking efficiency and scalability, both academic research and fact-checking organizations report limited adoption of such tooling due to insufficient alignment with fact-checker practices, values, and needs. To address this gap, we investigate a co-design method, Matchmaking for AI, which facilitates fact-checkers, designers, and NLP researchers to collaboratively discover what fact-checker needs should be addressed by technology and how. Our co-design sessions with 22 professional fact-checkers yielded a set of 11 novel design ideas. They assist in information searching, processing, and writing tasks for efficient and personalized fact-checking; help fact-checkers proactively prepare for future misinformation; monitor their potential biases; and support internal organization c
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#21495;&#21644;&#35821;&#20041;&#32454;&#24494;&#20043;&#22788;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#19982;&#20256;&#32479;&#26816;&#32034;&#26041;&#27861;&#30340;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07107</link><description>&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Information Retrieval: A Survey. (arXiv:2308.07107v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#21495;&#21644;&#35821;&#20041;&#32454;&#24494;&#20043;&#22788;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#19982;&#20256;&#32479;&#26816;&#32034;&#26041;&#27861;&#30340;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20449;&#24687;&#33719;&#21462;&#30340;&#20027;&#35201;&#25163;&#27573;&#65292;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31995;&#32479;&#65292;&#22914;&#25628;&#32034;&#24341;&#25806;&#65292;&#24050;&#32463;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;&#36825;&#20123;&#31995;&#32479;&#36824;&#20316;&#20026;&#23545;&#35805;&#12289;&#38382;&#31572;&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;IR&#30340;&#21457;&#23637;&#36712;&#36857;&#20174;&#22522;&#20110;&#35789;&#39033;&#30340;&#26041;&#27861;&#36215;&#27493;&#65292;&#36880;&#28176;&#21457;&#23637;&#25104;&#19982;&#20808;&#36827;&#30340;&#31070;&#32463;&#27169;&#22411;&#30456;&#34701;&#21512;&#12290;&#23613;&#31649;&#31070;&#32463;&#27169;&#22411;&#25797;&#38271;&#25429;&#25417;&#22797;&#26434;&#30340;&#19978;&#19979;&#25991;&#20449;&#21495;&#21644;&#35821;&#20041;&#32454;&#24494;&#20043;&#22788;&#65292;&#20174;&#32780;&#25913;&#21464;&#20102;IR&#30340;&#26684;&#23616;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#30528;&#25968;&#25454;&#31232;&#32570;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#29983;&#25104;&#19978;&#19979;&#25991;&#21512;&#29702;&#20294;&#28508;&#22312;&#19981;&#20934;&#30830;&#21709;&#24212;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#28436;&#21464;&#38656;&#35201;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#22522;&#20110;&#35789;&#39033;&#30340;&#31232;&#30095;&#26816;&#32034;&#26041;&#27861;&#19982;&#24555;&#36895;&#21709;&#24212;&#65289;&#21644;&#29616;&#20195;&#31070;&#32463;&#26550;&#26500;&#65288;&#22914;&#20855;&#26377;&#24378;&#22823;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#32467;&#21512;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#30340;&#20986;&#29616;&#65292;&#24341;&#36215;&#20102;&#19968;&#22330;&#38761;&#21629;
&lt;/p&gt;
&lt;p&gt;
As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;InsTag&#65292;&#19968;&#31181;&#29992;&#20110;&#26631;&#35760;&#22522;&#20110;&#35821;&#20041;&#21644;&#24847;&#22270;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#25968;&#25454;&#38598;&#26679;&#26412;&#30340;&#24320;&#25918;&#24335;&#32454;&#31890;&#24230;&#26631;&#27880;&#22120;&#12290;&#36890;&#36807;&#20998;&#26512;&#24320;&#28304;SFT&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#21147;&#20250;&#38543;&#30528;&#26356;&#22810;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#21270;&#30340;&#25968;&#25454;&#32780;&#22686;&#38271;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#20351;&#29992;InsTag&#36873;&#25321;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#65292;&#24471;&#21040;&#30340;TagLM&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;SFT&#25968;&#25454;&#19978;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#39564;&#35777;&#20102;&#26597;&#35810;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07074</link><description>&lt;p&gt;
#InsTag:&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30417;&#30563;&#24494;&#35843;&#30340;&#25351;&#20196;&#26631;&#27880;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models. (arXiv:2308.07074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;InsTag&#65292;&#19968;&#31181;&#29992;&#20110;&#26631;&#35760;&#22522;&#20110;&#35821;&#20041;&#21644;&#24847;&#22270;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#25968;&#25454;&#38598;&#26679;&#26412;&#30340;&#24320;&#25918;&#24335;&#32454;&#31890;&#24230;&#26631;&#27880;&#22120;&#12290;&#36890;&#36807;&#20998;&#26512;&#24320;&#28304;SFT&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#21147;&#20250;&#38543;&#30528;&#26356;&#22810;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#21270;&#30340;&#25968;&#25454;&#32780;&#22686;&#38271;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#20351;&#29992;InsTag&#36873;&#25321;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#65292;&#24471;&#21040;&#30340;TagLM&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;SFT&#25968;&#25454;&#19978;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#39564;&#35777;&#20102;&#26597;&#35810;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#65292;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#20102;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#34987;&#35748;&#20026;&#26159;&#25104;&#21151;&#30340;SFT&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#20294;&#20854;&#23450;&#20041;&#20173;&#28982;&#27169;&#31946;&#19981;&#28165;&#65292;&#32570;&#20047;&#23450;&#37327;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InsTag&#65292;&#19968;&#31181;&#24320;&#25918;&#30340;&#32454;&#31890;&#24230;&#26631;&#27880;&#22120;&#65292;&#26681;&#25454;&#35821;&#20041;&#21644;&#24847;&#22270;&#23545;SFT&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#35760;&#65292;&#24182;&#19988;&#36890;&#36807;&#26631;&#31614;&#26469;&#23450;&#20041;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;6.6K&#20010;&#26631;&#31614;&#26469;&#25551;&#36848;&#32508;&#21512;&#29992;&#25143;&#26597;&#35810;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#24320;&#28304;SFT&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#30340;&#33021;&#21147;&#38543;&#30528;&#26356;&#22810;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#21270;&#30340;&#25968;&#25454;&#32780;&#22686;&#38271;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;InsTag&#30340;&#25968;&#25454;&#36873;&#25321;&#22120;&#65292;&#20174;&#24320;&#28304;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;6K&#20010;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#26679;&#26412;&#65292;&#24182;&#22312;InsTag&#36873;&#25321;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TagLM&#27169;&#22411;&#22312;MT-Bench&#35780;&#20272;&#30340;&#22823;&#35268;&#27169;SFT&#25968;&#25454;&#19978;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#39564;&#35777;&#20102;&#26597;&#35810;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation language models obtain the instruction-following ability through supervised fine-tuning (SFT). Diversity and complexity are considered critical factors of a successful SFT dataset, while their definitions remain obscure and lack quantitative analyses. In this work, we propose InsTag, an open-set fine-grained tagger, to tag samples within SFT datasets based on semantics and intentions and define instruction diversity and complexity regarding tags. We obtain 6.6K tags to describe comprehensive user queries. Then we analyze popular open-sourced SFT datasets and find that the model ability grows with more diverse and complex data. Based on this observation, we propose a data selector based on InsTag to select 6K diverse and complex samples from open-source datasets and fine-tune models on InsTag-selected data. The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of query diversity and compl
&lt;/p&gt;</description></item><item><title>Thresh&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#21487;&#23450;&#21046;&#30340;&#21644;&#21487;&#37096;&#32626;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#35780;&#20272;&#24179;&#21488;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;YAML&#37197;&#32622;&#25991;&#20214;&#65292;&#29992;&#25143;&#21487;&#20197;&#24555;&#36895;&#26500;&#24314;&#21644;&#27979;&#35797;&#20219;&#20309;&#26694;&#26550;&#30340;&#27880;&#37322;&#30028;&#38754;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#31038;&#21306;&#20013;&#24515;&#26469;&#20419;&#36827;&#21327;&#20316;&#21644;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2308.06953</link><description>&lt;p&gt;
Thresh&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#21487;&#23450;&#21046;&#30340;&#21644;&#21487;&#37096;&#32626;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#35780;&#20272;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained Text Evaluation. (arXiv:2308.06953v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06953
&lt;/p&gt;
&lt;p&gt;
Thresh&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#21487;&#23450;&#21046;&#30340;&#21644;&#21487;&#37096;&#32626;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#35780;&#20272;&#24179;&#21488;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;YAML&#37197;&#32622;&#25991;&#20214;&#65292;&#29992;&#25143;&#21487;&#20197;&#24555;&#36895;&#26500;&#24314;&#21644;&#27979;&#35797;&#20219;&#20309;&#26694;&#26550;&#30340;&#27880;&#37322;&#30028;&#38754;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#31038;&#21306;&#20013;&#24515;&#26469;&#20419;&#36827;&#21327;&#20316;&#21644;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#31890;&#24230;&#30340;&#12289;&#36328;&#24230;&#32423;&#21035;&#30340;&#20154;&#24037;&#35780;&#20272;&#24050;&#32463;&#25104;&#20026;&#35780;&#20215;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#21487;&#38752;&#21644;&#31283;&#20581;&#30340;&#26041;&#27861;&#65292;&#22914;&#25688;&#35201;&#12289;&#31616;&#21270;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#26032;&#38395;&#29983;&#25104;&#65292;&#24182;&#19988;&#24471;&#21040;&#30340;&#26631;&#27880;&#23545;&#35757;&#32451;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20026;&#36825;&#20123;&#35780;&#20272;&#26694;&#26550;&#23454;&#26045;&#30340;&#26631;&#27880;&#24037;&#20855;&#32570;&#20047;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#25110;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#25110;&#32773;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#20462;&#25913;&#26631;&#27880;&#35774;&#32622;&#12290;&#32570;&#20047;&#32479;&#19968;&#30340;&#26631;&#27880;&#25968;&#25454;&#26684;&#24335;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Thresh&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#21487;&#23450;&#21046;&#30340;&#21644;&#21487;&#37096;&#32626;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;&#24179;&#21488;&#12290;&#21482;&#38656;&#21019;&#24314;&#19968;&#20010;YAML&#37197;&#32622;&#25991;&#20214;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#26500;&#24314;&#21644;&#27979;&#35797;&#20219;&#20309;&#26694;&#26550;&#30340;&#27880;&#37322;&#30028;&#38754; - &#25152;&#26377;&#36825;&#20123;&#37117;&#22312;&#19968;&#20010;web&#27983;&#35272;&#22120;&#31383;&#21475;&#20013;&#12290;&#20026;&#20102;&#20419;&#36827;&#21327;&#20316;&#21644;&#20849;&#20139;&#65292;Thresh&#25552;&#20379;&#20102;&#19968;&#20010;&#31038;&#21306;&#20013;&#24515;&#65292;&#36825;&#20010;&#20013;&#24515;&#25176;&#31649;&#20102;&#19968;&#31995;&#21015;&#32454;&#31890;&#24230;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained, span-level human evaluation has emerged as a reliable and robust method for evaluating text generation tasks such as summarization, simplification, machine translation and news generation, and the derived annotations have been useful for training automatic metrics and improving language models. However, existing annotation tools implemented for these evaluation frameworks lack the adaptability to be extended to different domains or languages, or modify annotation settings according to user needs. And the absence of a unified annotated data format inhibits the research in multi-task learning. In this paper, we introduce Thresh, a unified, customizable and deployable platform for fine-grained evaluation. By simply creating a YAML configuration file, users can build and test an annotation interface for any framework within minutes -- all in one web browser window. To facilitate collaboration and sharing, Thresh provides a community hub that hosts a collection of fine-grained
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#33647;&#29289;&#30456;&#20851;&#25991;&#26723;&#20013;&#25552;&#21462;&#33647;&#29289;&#20107;&#20214;/&#23454;&#20307;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#25429;&#25417;&#24182;&#23545;&#40784;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;/&#35821;&#35328;/&#30693;&#35782;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#33647;&#29289;&#20107;&#20214;&#20449;&#24687;&#30340;&#20840;&#38754;&#26816;&#27979;&#21644;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.06546</link><description>&lt;p&gt;
MC-DRE: &#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#29992;&#20110;&#33647;&#29289;&#20107;&#20214;/&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
MC-DRE: Multi-Aspect Cross Integration for Drug Event/Entity Extraction. (arXiv:2308.06546v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#33647;&#29289;&#30456;&#20851;&#25991;&#26723;&#20013;&#25552;&#21462;&#33647;&#29289;&#20107;&#20214;/&#23454;&#20307;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#25429;&#25417;&#24182;&#23545;&#40784;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;/&#35821;&#35328;/&#30693;&#35782;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#33647;&#29289;&#20107;&#20214;&#20449;&#24687;&#30340;&#20840;&#38754;&#26816;&#27979;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#33647;&#29289;&#30456;&#20851;&#20449;&#24687;&#22359;&#65292;&#22914;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#65288;ADE&#65289;&#65292;&#23545;&#20110;&#39044;&#38450;&#30142;&#30149;&#21644;&#25327;&#25937;&#35768;&#22810;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;ADE&#26159;&#36890;&#36807;&#21307;&#30103;&#32972;&#26223;&#19979;&#30340;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25253;&#21578;&#30340;&#12290;&#22240;&#27492;&#65292;&#24212;&#29992;&#36890;&#29992;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#20851;&#38190;&#22312;&#20110;&#22914;&#20309;&#25972;&#21512;&#21644;&#23545;&#40784;&#22810;&#20010;&#20851;&#38190;&#26041;&#38754;&#26469;&#26816;&#27979;&#33647;&#29289;&#20107;&#20214;&#20449;&#24687;&#65292;&#21253;&#25324;&#33647;&#29289;&#20107;&#20214;&#35821;&#20041;&#12289;&#21477;&#27861;&#32467;&#26500;&#21644;&#21307;&#23398;&#39046;&#22495;&#26415;&#35821;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#33647;&#29289;&#30456;&#20851;&#25991;&#26723;&#20013;&#25429;&#25417;&#21644;&#23545;&#40784;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;/&#35821;&#35328;/&#30693;&#35782;&#23646;&#24615;&#65292;&#29992;&#20110;&#33647;&#29289;&#23454;&#20307;/&#20107;&#20214;&#26816;&#27979;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#22810;&#26041;&#38754;&#32534;&#30721;&#22120;&#26469;&#25551;&#36848;&#35821;&#20041;&#12289;&#21477;&#27861;&#21644;&#21307;&#23398;&#25991;&#26723;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#26041;&#27861;&#21253;&#25324;&#27133;&#26631;&#27880;&#20219;&#21153;&#12289;&#20027;&#35201;&#33647;&#29289;&#23454;&#20307;/&#20107;&#20214;&#26816;&#27979;&#12289;&#35789;&#24615;&#26631;&#27880;&#21644;&#36890;&#29992;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#32534;&#30721;&#22120;&#36827;&#34892;&#20132;&#21449;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting meaningful drug-related information chunks, such as adverse drug events (ADE), is crucial for preventing morbidity and saving many lives. Most ADE are reported via an unstructured conversation with the medical context. Hence, applying a general entity recognition approach is not sufficient enough. The key is how to integrate and align multiple crucial aspects to detect drug event information, including drug event semantics, syntactic structures, and medical domain terminology. In this paper, we propose a new multi-aspect cross-integration framework for drug entity/event detection by capturing and aligning different context/language/knowledge properties from drug-related documents. We first construct multi-aspect encoders to describe semantic, syntactic, and medical document contextual information by conducting those slot tagging tasks, main drug entity/event detection, part-of-speech tagging, and general medical named entity recognition. Then, each encoder conducts cross int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#30693;&#35782;&#27880;&#20837;&#30340;&#30740;&#31350;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#65292;&#21253;&#25324;&#20027;&#27969;&#30693;&#35782;&#21644;&#30693;&#35782;&#27880;&#20837;&#21407;&#21017;&#30340;&#24635;&#32467;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#20934;&#27169;&#22411;&#30340;&#22238;&#39038;&#65292;&#24182;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#30693;&#35782;&#27880;&#20837;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04306</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#21947;&#26816;&#27979;&#30693;&#35782;&#27880;&#20837;&#65306;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive Review. (arXiv:2308.04306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#30693;&#35782;&#27880;&#20837;&#30340;&#30740;&#31350;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#65292;&#21253;&#25324;&#20027;&#27969;&#30693;&#35782;&#21644;&#30693;&#35782;&#27880;&#20837;&#21407;&#21017;&#30340;&#24635;&#32467;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#20934;&#27169;&#22411;&#30340;&#22238;&#39038;&#65292;&#24182;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#30693;&#35782;&#27880;&#20837;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#30740;&#31350;&#30340;&#21382;&#21490;&#20063;&#26631;&#24535;&#30528;&#30693;&#35782;&#27880;&#20837;&#30740;&#31350;&#30340;&#28436;&#21464;&#12290;&#38543;&#30528;&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#23545;&#23558;&#30693;&#35782;&#24212;&#29992;&#20110;&#22312;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#32467;&#26524;&#34920;&#29616;&#20986;&#26497;&#22823;&#20852;&#36259;&#12290;&#23613;&#31649;&#22312;&#38544;&#21947;&#35782;&#21035;&#39046;&#22495;&#28041;&#21450;&#30693;&#35782;&#27880;&#20837;&#30340;&#26041;&#27861;&#36880;&#28176;&#22686;&#21152;&#65292;&#20294;&#32570;&#20047;&#19968;&#31687;&#23436;&#25972;&#30340;&#20851;&#20110;&#22522;&#20110;&#30693;&#35782;&#27880;&#20837;&#30340;&#26041;&#27861;&#30340;&#32508;&#36848;&#25991;&#31456;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#32508;&#36848;&#28145;&#24230;&#23398;&#20064;&#22312;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#24212;&#29992;&#30693;&#35782;&#27880;&#20837;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#26412;&#25991;&#31995;&#32479;&#24635;&#32467;&#21644;&#27010;&#25324;&#20102;&#20027;&#27969;&#30340;&#30693;&#35782;&#21644;&#30693;&#35782;&#27880;&#20837;&#21407;&#21017;&#65292;&#21516;&#26102;&#22238;&#39038;&#20102;&#22312;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#20934;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;&#21069;&#38754;&#20020;&#30340;&#30693;&#35782;&#27880;&#20837;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The history of metaphor research also marks the evolution of knowledge infusion research. With the continued advancement of deep learning techniques in recent years, the natural language processing community has shown great interest in applying knowledge to successful results in metaphor recognition tasks. Although there has been a gradual increase in the number of approaches involving knowledge injection in the field of metaphor recognition, there is a lack of a complete review article on knowledge injection based approaches. Therefore, the goal of this paper is to provide a comprehensive review of research advances in the application of deep learning for knowledge injection in metaphor recognition tasks. In this paper, we systematically summarize and generalize the mainstream knowledge and knowledge injection principles, as well as review the datasets, evaluation metrics, and benchmark models used in metaphor recognition tasks. Finally, we explore the current issues facing knowledge 
&lt;/p&gt;</description></item><item><title>SynJax&#26159;&#19968;&#20010;&#38024;&#23545;JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#39640;&#25928;&#30340;&#21521;&#37327;&#21270;&#23454;&#29616;&#35299;&#20915;&#20102;&#23545;&#20110;&#32467;&#26500;&#21270;&#23545;&#35937;&#30340;&#38590;&#20197;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03291</link><description>&lt;p&gt;
SynJax: JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
SynJax: Structured Probability Distributions for JAX. (arXiv:2308.03291v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03291
&lt;/p&gt;
&lt;p&gt;
SynJax&#26159;&#19968;&#20010;&#38024;&#23545;JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#39640;&#25928;&#30340;&#21521;&#37327;&#21270;&#23454;&#29616;&#35299;&#20915;&#20102;&#23545;&#20110;&#32467;&#26500;&#21270;&#23545;&#35937;&#30340;&#38590;&#20197;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36719;&#20214;&#24211;&#30340;&#21457;&#23637;&#20351;&#24471;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#19987;&#27880;&#20110;&#24314;&#27169;&#65292;&#21516;&#26102;&#35753;&#24211;&#26469;&#22788;&#29702;&#38024;&#23545;&#29616;&#20195;&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#20248;&#21270;&#25191;&#34892;&#30340;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20165;&#23545;&#29305;&#23450;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#30410;&#65292;&#20363;&#22914;Transformer&#65292;&#20854;&#22522;&#26412;&#25805;&#20316;&#26131;&#20110;&#26144;&#23556;&#21040;&#21521;&#37327;&#21270;&#35745;&#31639;&#12290;&#32780;&#23545;&#20110;&#26174;&#24335;&#32771;&#34385;&#32467;&#26500;&#21270;&#23545;&#35937;&#65288;&#22914;&#26641;&#21644;&#20998;&#21106;&#65289;&#30340;&#27169;&#22411;&#65292;&#24182;&#27809;&#26377;&#21516;&#26679;&#30340;&#21463;&#30410;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23450;&#21046;&#30340;&#38590;&#20197;&#20197;&#21521;&#37327;&#21270;&#24418;&#24335;&#23454;&#29616;&#30340;&#31639;&#27861;&#12290;SynJax&#36890;&#36807;&#25552;&#20379;&#29992;&#20110;&#32467;&#26500;&#21270;&#20998;&#24067;&#30340;&#25512;&#29702;&#31639;&#27861;&#30340;&#39640;&#25928;&#21521;&#37327;&#21270;&#23454;&#29616;&#26469;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#23545;&#40784;&#12289;&#26631;&#35760;&#12289;&#20998;&#21106;&#12289;&#32452;&#25104;&#26641;&#21644;&#29983;&#25104;&#26641;&#30340;&#22788;&#29702;&#12290;&#20351;&#29992;SynJax&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;&#21487;&#24494;&#20998;&#27169;&#22411;&#65292;&#26174;&#24335;&#22320;&#23545;&#25968;&#25454;&#30340;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#12290;&#20195;&#30721;&#21487;&#22312;https://g&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form.  SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https://g
&lt;/p&gt;</description></item><item><title>LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.01413</link><description>&lt;p&gt;
LaFiCMIL&#65306;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22823;&#25991;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning. (arXiv:2308.01413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01413
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#31361;&#30772;&#12290;&#30452;&#35266;&#19978;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#26399;&#26395;&#25991;&#26412;&#20998;&#31867;&#65292;&#20316;&#20026;&#19981;&#38656;&#35201;&#20687;&#29983;&#25104;&#20219;&#21153;&#37027;&#26679;&#35768;&#22810;&#39640;&#32423;&#34920;&#31034;&#30340;&#20219;&#21153;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;Transformer&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#36827;&#34892;&#32508;&#21512;&#24615;&#30340;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#65292;&#22312;&#22810;&#31867;&#21035;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#38271;&#25991;&#26412;&#25991;&#26723;&#21644;&#20854;&#20182;&#22823;&#25991;&#20214;&#30340;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#36739;&#22823;&#30340;&#25913;&#36827;&#28508;&#21147;&#12290;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#20027;&#35201;&#21463;&#21040;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#30340;&#38459;&#30861;&#65306;&#26377;&#38480;&#30340;&#36755;&#20837;&#38271;&#24230;&#65292;&#27604;&#22914;BERT&#30340;512&#20010;&#26631;&#35760;&#12290;&#34429;&#28982;&#22686;&#21152;GPU&#20869;&#23384;&#21487;&#20197;&#31245;&#24494;&#25193;&#23637;&#36825;&#20010;&#38480;&#21046;&#65292;&#20294;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;GPU&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#36755;&#20837;&#38480;&#21046;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;LaFiCMIL&#65292;&#20316;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have revolutionized the performance of a wide range of language tasks. Intuitively, one might expect text classification, which does not necessitate as many high-level representations as generative tasks, to be comprehensively addressed with the powerful representation capabilities of Transformers. However, in reality, there remains significant potential for enhancement, particularly in the areas of multi-class and multi-label classification of lengthy textual documents and other large files. The performance of Transformer-based models is mainly hindered by a major limitation: a restricted input length, e.g., 512 tokens for BERT. While an increase in GPU memory can marginally extend this limit, practical real-world applications often operate under constrained GPU resources. In this work, we tackle the input limit problem from the perspective of correlated multiple instance learning. The proposed approach, LaFiCMIL, serves as a versatile framework applicable to 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#21512;&#25104;&#26679;&#24335;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#26080;&#28304;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#65292;&#24182;&#36890;&#36807;&#24378;&#21046;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#19982;&#20869;&#23481;&#29305;&#24449;&#38752;&#36817;&#26469;&#20445;&#35777;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.15199</link><description>&lt;p&gt;
PromptStyler&#65306;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#28304;&#22495;&#27867;&#21270;&#39118;&#26684;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization. (arXiv:2307.15199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15199
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#21512;&#25104;&#26679;&#24335;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#26080;&#28304;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#65292;&#24182;&#36890;&#36807;&#24378;&#21046;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#19982;&#20869;&#23481;&#29305;&#24449;&#38752;&#36817;&#26469;&#20445;&#35777;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#21512;&#35270;&#35273;&#35821;&#35328;&#31354;&#38388;&#20013;&#65292;&#25991;&#26412;&#29305;&#24449;&#65288;&#22914;&#8220;&#19968;&#24352;&#29399;&#30340;&#29031;&#29255;&#8221;&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#20854;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65288;&#22914;&#29399;&#30340;&#29031;&#29255;&#65289;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#26469;&#21512;&#25104;&#21508;&#31181;&#26679;&#24335;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#22270;&#20687;&#26469;&#22788;&#29702;&#26080;&#28304;&#22495;&#27867;&#21270;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#26679;&#24335;&#35789;&#21521;&#37327;&#20026;&#20266;&#35789;S*&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#29305;&#24449;&#65288;&#22914;&#8220;a S* style of a&#8221;&#65289;&#12290;&#20026;&#20102;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#65292;&#25105;&#20204;&#24378;&#21046;&#35201;&#27714;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#65288;&#22914;&#8220;a S* style of a [class]&#8221;&#65289;&#22312;&#32852;&#21512;&#35270;&#35273;&#35821;&#35328;&#31354;&#38388;&#20013;&#38752;&#36817;&#20854;&#23545;&#24212;&#30340;&#20869;&#23481;&#29305;&#24449;&#65288;&#22914;&#8220;[class]&#8221;&#65289;&#12290;&#22312;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#30340;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#23613;&#31649;PromptStyler&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#22270;&#20687;&#65292;&#24182;&#19988;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#20294;&#22312;PACS&#12289;VLCS&#12289;OfficeHome&#21644;DomainNet&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a joint vision-language space, a text feature (e.g., from "a photo of a dog") could effectively represent its relevant image features (e.g., from dog photos). Inspired by this, we propose PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. Our method learns to generate a variety of style features (from "a S* style of a") via learnable style word vectors for pseudo-words S*. To ensure that learned styles do not distort content information, we force style-content features (from "a S* style of a [class]") to be located nearby their corresponding content features (from "[class]") in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthesized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, although it does not require any images and take
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05695</link><description>&lt;p&gt;
&#20197;&#19981;&#21516;&#26041;&#24335;&#22534;&#21472;&#26356;&#22810;&#23618;&#65306;&#36890;&#36807;&#20302;&#31209;&#26356;&#26032;&#36827;&#34892;&#39640;&#31209;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#32593;&#32476;&#25317;&#26377;&#25968;&#30334;&#20159;&#20010;&#21442;&#25968;&#30340;&#35268;&#27169;&#24050;&#32463;&#21344;&#20027;&#23548;&#22320;&#20301;&#24182;&#19988;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#23545;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#24517;&#35201;&#24615;&#20173;&#28982;&#32570;&#20047;&#28165;&#26224;&#30340;&#29702;&#35299;&#65292;&#32780;&#26367;&#20195;&#26041;&#27861;&#19981;&#19968;&#23450;&#33021;&#22815;&#38477;&#20302;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#39640;&#31209;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;ReLoRA&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#21442;&#25968;&#37327;&#39640;&#36798;350M&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#30340;&#25928;&#29575;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#30340;&#28508;&#21147;&#21450;&#20854;&#23545;&#20110;&#32553;&#25918;&#23450;&#24459;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models. In this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for scaling laws.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#31867;&#21644;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#22996;&#22312;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#36755;&#20986;&#26102;&#30340;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#35780;&#20272;&#36807;&#31243;&#20013;&#23384;&#22312;&#20559;&#35265;&#65292;&#21363;&#23613;&#31649;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#65292;&#31572;&#26696;&#20173;&#28982;&#34987;&#26356;&#39640;&#22320;&#35780;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;</title><link>http://arxiv.org/abs/2307.03025</link><description>&lt;p&gt;
&#39118;&#26684;&#32988;&#36807;&#23454;&#36136;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Style Over Substance: Evaluation Biases for Large Language Models. (arXiv:2307.03025v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03025
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#31867;&#21644;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#22996;&#22312;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#36755;&#20986;&#26102;&#30340;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#35780;&#20272;&#36807;&#31243;&#20013;&#23384;&#22312;&#20559;&#35265;&#65292;&#21363;&#23613;&#31649;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#65292;&#31572;&#26696;&#20173;&#28982;&#34987;&#26356;&#39640;&#22320;&#35780;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#20934;&#30830;&#21644;&#20840;&#38754;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#19978;&#65292;&#20154;&#31867;&#35780;&#20272;&#34987;&#35748;&#20026;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#23558;&#26368;&#20808;&#36827;&#30340;LLMs&#32435;&#20837;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#20316;&#20026;&#20154;&#31867;&#35780;&#22996;&#30340;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#21644;LLMs&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#33021;&#21147;&#31243;&#24230;&#20173;&#28982;&#19981;&#30830;&#23450;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#20247;&#21253;&#20154;&#31867;&#35780;&#22996;&#21644;&#22522;&#20110;LLMs&#30340;&#35780;&#22996;&#22312;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#36755;&#20986;&#26102;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#25925;&#24847;&#26377;&#32570;&#38519;&#30340;&#26426;&#22120;&#29983;&#25104;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#20107;&#23454;&#19978;&#30340;&#38169;&#35823;&#21487;&#33021;&#24102;&#26469;&#26356;&#22823;&#30340;&#21361;&#38505;&#65292;&#20294;&#24102;&#26377;&#20107;&#23454;&#38169;&#35823;&#30340;&#31572;&#26696;&#20173;&#28982;&#27604;&#38271;&#24230;&#36807;&#30701;&#25110;&#21253;&#21547;&#35821;&#27861;&#38169;&#35823;&#30340;&#31572;&#26696;&#35780;&#20998;&#26356;&#39640;&#12290;&#36825;&#31361;&#26174;&#20102;&#35780;&#20272;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#20196;&#20154;&#25285;&#24551;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) continue to advance, accurately and comprehensively evaluating their performance becomes increasingly challenging. Conventionally, human evaluations are considered the gold standard in natural language generation. Recent advancements incorporate state-of-the-art LLMs as proxies for human judges in evaluation processes. Nonetheless, the extent to which humans and LLMs are capable evaluators remains uncertain. This study aims to investigate the behavior of both crowd-sourced human and LLM-based judges when comparing outputs from different models. To accomplish this, we curate a dataset comprising intentionally flawed machine-generated answers. Our findings indicate that despite the potentially greater danger posed by factual errors, answers with factual errors were still rated more favorably compared to answers that were too short or contained grammatical errors. This highlights a concerning bias in the evaluation process. To address this issue, we propose
&lt;/p&gt;</description></item><item><title>BatGPT&#26159;&#19968;&#20010;&#21452;&#21521;&#33258;&#22238;&#24402;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#21452;&#21521;&#24314;&#27169;&#21644;&#21442;&#25968;&#25193;&#23637;&#26041;&#27861;&#26469;&#25913;&#21892;&#23545;&#35805;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;&#36825;&#26159;&#19968;&#20010;&#22312;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#37325;&#35201;&#36129;&#29486;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.00360</link><description>&lt;p&gt;
BatGPT: &#20174;&#29983;&#25104;&#24615;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#20013;&#24471;&#21040;&#30340;&#21452;&#21521;&#33258;&#22238;&#24402;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer. (arXiv:2307.00360v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00360
&lt;/p&gt;
&lt;p&gt;
BatGPT&#26159;&#19968;&#20010;&#21452;&#21521;&#33258;&#22238;&#24402;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#21452;&#21521;&#24314;&#27169;&#21644;&#21442;&#25968;&#25193;&#23637;&#26041;&#27861;&#26469;&#25913;&#21892;&#23545;&#35805;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;&#36825;&#26159;&#19968;&#20010;&#22312;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#37325;&#35201;&#36129;&#29486;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BatGPT&#26159;&#30001;&#27494;&#27721;&#22823;&#23398;&#21644;&#19978;&#28023;&#20132;&#36890;&#22823;&#23398;&#20849;&#21516;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#65288;&#21253;&#25324;&#25991;&#26412;&#25552;&#31034;&#12289;&#22270;&#20687;&#21644;&#38899;&#39057;&#65289;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#22312;&#24314;&#27169;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21452;&#21521;&#33258;&#22238;&#24402;&#26550;&#26500;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#35821;&#35328;&#29983;&#25104;&#12289;&#23545;&#35805;&#31995;&#32479;&#21644;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#21452;&#21521;&#33258;&#22238;&#24402;&#27169;&#22411;&#19981;&#20165;&#20174;&#24038;&#21040;&#21491;&#36816;&#34892;&#65292;&#36824;&#20174;&#21491;&#21040;&#24038;&#36816;&#34892;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#22266;&#23450;&#35760;&#24518;&#25928;&#24212;&#24182;&#32531;&#35299;&#20102;&#27169;&#22411;&#20135;&#29983;&#34394;&#20551;&#36755;&#20986;&#30340;&#38382;&#39064;&#12290;&#22312;&#35757;&#32451;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#25193;&#23637;&#26041;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#36739;&#23567;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#20102;&#26469;&#33258;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#26088;&#22312;&#25913;&#21892;&#27169;&#22411;&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23545;&#35805;&#29983;&#25104;&#26041;&#21521;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
BatGPT is a large-scale language model designed and trained jointly by Wuhan University and Shanghai Jiao Tong University. It is capable of generating highly natural and fluent text in response to various types of input, including text prompts, images, and audio. In the modeling level, we employ a bidirectional autoregressive architecture that allows the model to efficiently capture the complex dependencies of natural language, making it highly effective in tasks such as language generation, dialog systems, and question answering. Moreover, the bidirectional autoregressive modeling not only operates from left to right but also from right to left, effectively reducing fixed memory effects and alleviating model hallucinations.  In the training aspect, we propose a novel parameter expansion method for leveraging the pre-training of smaller models and employ reinforcement learning from both AI and human feedback, aimed at improving the model's alignment performance. Overall, these approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PoetryDiffusion&#27169;&#22411;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#35799;&#27468;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#35821;&#20041;&#21644;&#38901;&#24459;&#26041;&#38754;&#30340;&#25511;&#21046;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#21019;&#26032;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08456</link><description>&lt;p&gt;
PoetryDiffusion: &#23454;&#29616;&#35799;&#27468;&#29983;&#25104;&#20013;&#30340;&#35821;&#20041;&#21644;&#38901;&#24459;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in Poetry Generation. (arXiv:2306.08456v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PoetryDiffusion&#27169;&#22411;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#35799;&#27468;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#35821;&#20041;&#21644;&#38901;&#24459;&#26041;&#38754;&#30340;&#25511;&#21046;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#21019;&#26032;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#24847;&#20041;&#37325;&#22823;&#30340;&#39046;&#22495;&#12290;&#23588;&#20854;&#26159;&#35799;&#27468;&#29983;&#25104;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#39046;&#22495;&#65292;&#23545;&#25991;&#26412;&#29983;&#25104;&#26377;&#30528;&#26126;&#30830;&#23450;&#20041;&#21644;&#20005;&#26684;&#30340;&#26465;&#20214;&#65292;&#26159;&#35780;&#20272;&#24403;&#21069;&#26041;&#27861;&#23398;&#30340;&#29702;&#24819;&#23454;&#39564;&#22330;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#25104;&#21151;&#22320;&#25511;&#21046;&#20102;&#35799;&#27468;&#29983;&#25104;&#30340;&#35821;&#20041;&#25110;&#38901;&#24459;&#26041;&#38754;&#65292;&#20294;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#26041;&#38754;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#21313;&#22235;&#34892;&#35799;&#21644;&#20013;&#22269;&#23435;&#35789;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23601;&#35821;&#20041;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;PoetryDiffusion&#27169;&#22411;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#23436;&#25972;&#30340;&#21477;&#23376;&#25110;&#35799;&#27468;&#65292;&#20840;&#38754;&#32771;&#34385;&#21477;&#23376;&#20449;&#24687;&#30340;&#25972;&#20307;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#35821;&#20041;&#34920;&#36798;&#65292;&#20351;&#20854;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#25152;&#21306;&#21035;&#12290;&#23601;&#38901;&#24459;&#25511;&#21046;&#32780;&#35328;&#65292;&#25193;&#25955;&#29983;&#25104;&#21644;&#20854;&#32422;&#26463;&#25511;&#21046;&#27169;&#22359;&#30340;&#20998;&#31163;&#29305;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#28789;&#27963;&#22320;&#25511;&#21046;&#38901;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable text generation is a challenging and meaningful field in natural language generation (NLG). Especially, poetry generation is a typical one with well-defined and strict conditions for text generation which is an ideal playground for the assessment of current methodologies. While prior works succeeded in controlling either semantic or metrical aspects of poetry generation, simultaneously addressing both remains a challenge. In this paper, we pioneer the use of the Diffusion model for generating sonnets and Chinese SongCi poetry to tackle such challenges. In terms of semantics, our PoetryDiffusion model, built upon the Diffusion model, generates entire sentences or poetry by comprehensively considering the entirety of sentence information. This approach enhances semantic expression, distinguishing it from autoregressive and large language models (LLMs). For metrical control, the separation feature of diffusion generation and its constraint control module enable us to flexibly
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#20559;&#32622;&#26041;&#27861;&#65292;&#22522;&#20110;Context-Aware Transformer Transducer (CATT) &#26469;&#36827;&#34892;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312; WER &#21644; CER &#19978;&#21487;&#20197;&#20998;&#21035;&#20943;&#23569;6.7%&#21644;20.7%&#65292;&#20943;&#23569;&#20102;96.7%&#21644;84.9% &#30340;&#30456;&#23545; WER &#21644; CER &#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2306.00804</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#20559;&#32622;&#30340;&#22522;&#20110;Transducer&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Adaptive Contextual Biasing for Transducer Based Streaming Speech Recognition. (arXiv:2306.00804v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#20559;&#32622;&#26041;&#27861;&#65292;&#22522;&#20110;Context-Aware Transformer Transducer (CATT) &#26469;&#36827;&#34892;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312; WER &#21644; CER &#19978;&#21487;&#20197;&#20998;&#21035;&#20943;&#23569;6.7%&#21644;20.7%&#65292;&#20943;&#23569;&#20102;96.7%&#21644;84.9% &#30340;&#30456;&#23545; WER &#21644; CER &#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21152;&#20837;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#20010;&#24615;&#21270;&#35805;&#35821;&#35782;&#21035;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#30340;&#35821;&#38899;&#21161;&#25163;&#20013;&#65292;&#24635;&#26159;&#23558;&#39640;&#39044;&#27979;&#20998;&#25968;&#30340;&#20010;&#24615;&#21270;&#35789;&#35821;&#36827;&#34892;&#20559;&#32622;&#22788;&#29702;&#20250;&#26174;&#33879;&#38477;&#20302;&#23545;&#24120;&#35265;&#35789;&#35821;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;Transformer Transducer&#65288;CATT&#65289;&#30340;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#20559;&#32622;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20559;&#32622;&#32534;&#30721;&#22120;&#21644;&#39044;&#27979;&#22120;&#23884;&#20837;&#26469;&#25191;&#34892;&#19978;&#19979;&#25991;&#30701;&#35821;&#20986;&#29616;&#30340;&#27969;&#24335;&#39044;&#27979;&#12290;&#36825;&#31181;&#39044;&#27979;&#28982;&#21518;&#34987;&#29992;&#26469;&#21160;&#24577;&#22320;&#20999;&#25442;&#20559;&#32622;&#21015;&#34920;&#30340;&#24320;&#20851;&#65292;&#20351;&#27169;&#22411;&#36866;&#24212;&#20010;&#24615;&#21270;&#21644;&#24120;&#35265;&#24773;&#26223;&#12290;&#22312;Librispeech&#21644;&#20869;&#37096;&#35821;&#38899;&#21161;&#25163;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;WER&#21644;CER&#19978;&#20998;&#21035;&#21487;&#20197;&#36798;&#21040;6.7%&#21644;20.7%&#30340;&#30456;&#23545;&#38477;&#20302;&#65292;&#23558;&#30456;&#23545;WER&#21644;CER&#30340;&#22686;&#21152;&#20943;&#23569;&#21040;96.7%&#21644;84.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
By incorporating additional contextual information, deep biasing methods have emerged as a promising solution for speech recognition of personalized words. However, for real-world voice assistants, always biasing on such personalized words with high prediction scores can significantly degrade the performance of recognizing common words. To address this issue, we propose an adaptive contextual biasing method based on Context-Aware Transformer Transducer (CATT) that utilizes the biased encoder and predictor embeddings to perform streaming prediction of contextual phrase occurrences. Such prediction is then used to dynamically switch the bias list on and off, enabling the model to adapt to both personalized and common scenarios. Experiments on Librispeech and internal voice assistant datasets show that our approach can achieve up to 6.7% and 20.7% relative reduction in WER and CER compared to the baseline respectively, mitigating up to 96.7% and 84.9% of the relative WER and CER increase 
&lt;/p&gt;</description></item><item><title>GripRank&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#29983;&#25104;&#24335;&#30693;&#35782;&#24212;&#29992;&#20110;&#27573;&#33853;&#25490;&#24207;&#65292;&#22635;&#34917;&#26816;&#32034;&#21644;&#25991;&#26412;&#29983;&#25104;&#20043;&#38388;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18144</link><description>&lt;p&gt;
GripRank: &#36890;&#36807;&#29983;&#25104;&#24335;&#30693;&#35782;&#25913;&#36827;&#30340;&#27573;&#33853;&#25490;&#24207;&#22635;&#34917;&#26816;&#32034;&#21644;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking. (arXiv:2305.18144v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18144
&lt;/p&gt;
&lt;p&gt;
GripRank&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#29983;&#25104;&#24335;&#30693;&#35782;&#24212;&#29992;&#20110;&#27573;&#33853;&#25490;&#24207;&#65292;&#22635;&#34917;&#26816;&#32034;&#21644;&#25991;&#26412;&#29983;&#25104;&#20043;&#38388;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20174;&#22823;&#22411;&#27573;&#33853;&#35821;&#26009;&#24211;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#26469;&#25552;&#20379;&#21512;&#36866;&#30340;&#31572;&#26696;&#65292;&#26816;&#32034;&#22686;&#24378;&#30340;&#25991;&#26412;&#29983;&#25104;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22914;&#24320;&#25918;&#22495;&#38382;&#31572;&#21644;&#30693;&#35782;&#22686;&#24378;&#23545;&#35805;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26816;&#32034;&#21644;&#29983;&#25104;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#21363;&#22312;&#26816;&#32034;&#36807;&#31243;&#20013;&#20505;&#36873;&#27573;&#33853;&#37117;&#34987;&#31561;&#21516;&#23545;&#24453;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#29983;&#25104;&#21512;&#36866;&#31572;&#26696;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#24182;&#19981;&#29702;&#24819;&#29992;&#20110;&#25351;&#23548;&#31572;&#26696;&#29983;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;GeneRative Knowledge Improved Passage Ranking (GripRank) &#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#24335;&#27573;&#33853;&#20272;&#35745;&#22120;(GPE)&#30340;&#30693;&#35782;&#25552;&#28860;&#21040;&#19968;&#20010;&#27573;&#33853;&#25490;&#24207;&#22120;&#20013;&#65292;&#20854;&#20013;GPE&#26159;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#21512;&#36866;&#31572;&#26696;&#30340;&#21487;&#33021;&#24615;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-enhanced text generation has shown remarkable progress on knowledge-intensive language tasks, such as open-domain question answering and knowledge-enhanced dialogue generation, by leveraging passages retrieved from a large passage corpus for delivering a proper answer given the input query. However, the retrieved passages are not ideal for guiding answer generation because of the discrepancy between retrieval and generation, i.e., the candidate passages are all treated equally during the retrieval procedure without considering their potential to generate a proper answer. This discrepancy makes a passage retriever deliver a sub-optimal collection of candidate passages to generate the answer. In this paper, we propose the GeneRative Knowledge Improved Passage Ranking (GripRank) approach, addressing the above challenge by distilling knowledge from a generative passage estimator (GPE) to a passage ranker, where the GPE is a generative language model used to measure how likely the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17401</link><description>&lt;p&gt;
&#19968;&#31181;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework For Refining Text Classification and Object Recognition from Academic Articles. (arXiv:2305.17401v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#39640;&#25928;&#22320;&#20174;&#22823;&#37327;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#21462;&#29305;&#23450;&#20449;&#24687;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25366;&#25496;&#23398;&#26415;&#35770;&#25991;&#30340;&#25968;&#25454;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33258;&#21160;&#20174;&#22797;&#26434;&#30340;&#38750;&#32467;&#26500;&#21270;&#24067;&#23616;&#25991;&#26723;&#20013;&#25552;&#21462;&#29305;&#23450;&#27169;&#24335;&#12290;&#24403;&#21069;&#30340;&#23398;&#26415;&#35770;&#25991;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#65288;RB&#65289;&#25110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#38656;&#35201;&#32534;&#20889;&#22797;&#26434;&#25490;&#29256;&#35770;&#25991;&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#23545;&#25991;&#31456;&#20013;&#22797;&#26434;&#20869;&#23481;&#31867;&#22411;&#36827;&#34892;&#27880;&#37322;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#23481;&#26131;&#35782;&#21035;&#30340;&#27169;&#24335;&#34987;&#38169;&#35823;&#25552;&#21462;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;&#20998;&#26512;&#25351;&#23450;&#33879;&#20316;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#24067;&#23616;&#21644;&#25490;&#29256;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of the internet, it has become increasingly crucial to extract specific information from vast amounts of academic articles efficiently. Data mining techniques are generally employed to solve this issue. However, data mining for academic articles is challenging since it requires automatically extracting specific patterns in complex and unstructured layout documents. Current data mining methods for academic articles employ rule-based(RB) or machine learning(ML) approaches. However, using rule-based methods incurs a high coding cost for complex typesetting articles. On the other hand, simply using machine learning methods requires annotation work for complex content types within the paper, which can be costly. Furthermore, only using machine learning can lead to cases where patterns easily recognized by rule-based methods are mistakenly extracted. To overcome these issues, from the perspective of analyzing the standard layout and typesetting used in the specified p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.04003</link><description>&lt;p&gt;
ANTONIO:&#38754;&#21521;NLP&#39564;&#35777;&#30340;&#31995;&#32479;&#21270;&#22522;&#20934;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39564;&#35777;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26041;&#27861;&#24120;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#20294;&#24182;&#19981;&#36866;&#29992;&#20110;NLP&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36896;&#25104;&#36825;&#19968;&#38382;&#39064;&#30340;&#25216;&#26415;&#21407;&#22240;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#23558;NLP&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20934;&#22791;&#20026;&#36866;&#21512;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#30340;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#23454;&#29616;&#20026;&#19968;&#20010;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#35813;&#24211;&#36830;&#25509;&#21040;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#22120;ERAN&#21644;Marabou&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;R-U-A-Robot&#30340;NLP&#25968;&#25454;&#38598;&#23545;&#24037;&#20855;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;&#25552;&#35758;&#20316;&#20026;&#39564;&#35777;&#20855;&#26377;&#27861;&#24459;&#37325;&#35201;&#24615;&#30340;NLP&#24212;&#29992;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24076;&#26395;&#65292;&#30001;&#20110;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. Here, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to known verification methods based on abstract interpretation. We implement these methods as a Python library called ANTONIO that links to the neural network verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems withi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LeafAI&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#25968;&#25454;&#27169;&#22411;&#19981;&#21463;&#38480;&#21046;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#20026;&#22797;&#26434;&#30340;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#26631;&#20934;&#25552;&#20379;&#26032;&#39062;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.06203</link><description>&lt;p&gt;
LeafAI&#65306;&#20020;&#24202;&#38431;&#21015;&#21457;&#29616;&#30340;&#26597;&#35810;&#29983;&#25104;&#22120;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#19981;&#30456;&#19978;&#19979;
&lt;/p&gt;
&lt;p&gt;
LeafAI: query generator for clinical cohort discovery rivaling a human programmer. (arXiv:2304.06203v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06203
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LeafAI&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#25968;&#25454;&#27169;&#22411;&#19981;&#21463;&#38480;&#21046;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#20026;&#22797;&#26434;&#30340;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#26631;&#20934;&#25552;&#20379;&#26032;&#39062;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#65292;&#30830;&#23450;&#30740;&#31350;&#36164;&#26684;&#30340;&#24739;&#32773;&#26159;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#30340;&#26597;&#35810;&#35774;&#35745;&#36890;&#24120;&#38656;&#35201;&#24191;&#27867;&#30340;&#25216;&#26415;&#21644;&#29983;&#29289;&#21307;&#23398;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#35797;&#22270;&#21019;&#24314;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#29983;&#25104;&#25968;&#25454;&#27169;&#22411;&#19981;&#21463;&#38480;&#21046;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#20026;&#22797;&#26434;&#30340;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#26631;&#20934;&#25552;&#20379;&#26032;&#39062;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#20174;&#36164;&#26684;&#26631;&#20934;&#21019;&#24314;&#26597;&#35810;&#30340;&#20219;&#21153;&#38656;&#35201;&#35299;&#20915;&#20960;&#20010;&#25991;&#26412;&#22788;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;&#12289;&#24207;&#21015;&#21040;&#24207;&#21015;&#36716;&#25442;&#12289;&#24402;&#19968;&#21270;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22359;&#20197;&#21450;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#21644;&#38142;&#25509;&#26412;&#20307;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#30693;&#35782;&#24211;&#12290;&#20026;&#20102;&#23454;&#29616;&#25968;&#25454;&#27169;&#22411;&#19981;&#21463;&#38480;&#21046;&#30340;&#26597;&#35810;&#21019;&#24314;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;UMLS&#27010;&#24565;&#26631;&#35760;&#25968;&#25454;&#24211;&#27169;&#24335;&#20803;&#32032;&#30340;&#26032;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#31995;&#32479;LeafAI&#65292;&#25105;&#20204;&#19982;&#20004;&#20010;&#20855;&#26377;&#20020;&#24202;&#24212;&#29992;&#32972;&#26223;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Identifying study-eligible patients within clinical databases is a critical step in clinical research. However, accurate query design typically requires extensive technical and biomedical expertise. We sought to create a system capable of generating data model-agnostic queries while also providing novel logical reasoning capabilities for complex clinical trial eligibility criteria.  Materials and Methods: The task of query creation from eligibility criteria requires solving several text-processing problems, including named entity recognition and relation extraction, sequence-to-sequence transformation, normalization, and reasoning. We incorporated hybrid deep learning and rule-based modules for these, as well as a knowledge base of the Unified Medical Language System (UMLS) and linked ontologies. To enable data-model agnostic query creation, we introduce a novel method for tagging database schema elements using UMLS concepts. To evaluate our system, called LeafAI, we compare
&lt;/p&gt;</description></item><item><title>SGL-PT&#26159;&#19968;&#20010;&#20855;&#26377;&#22270;&#24418;&#25552;&#31034;&#35843;&#20248;&#30340;&#24378;&#22823;&#22270;&#24418;&#23398;&#20064;&#22120;&#65292;&#20197;&#32553;&#23567;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#22270;&#24418;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#19968;&#33268;&#30340;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.12449</link><description>&lt;p&gt;
SGL-PT: &#19968;&#31181;&#20855;&#26377;&#22270;&#24418;&#25552;&#31034;&#35843;&#20248;&#30340;&#24378;&#22823;&#22270;&#24418;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
SGL-PT: A Strong Graph Learner with Graph Prompt Tuning. (arXiv:2302.12449v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12449
&lt;/p&gt;
&lt;p&gt;
SGL-PT&#26159;&#19968;&#20010;&#20855;&#26377;&#22270;&#24418;&#25552;&#31034;&#35843;&#20248;&#30340;&#24378;&#22823;&#22270;&#24418;&#23398;&#20064;&#22120;&#65292;&#20197;&#32553;&#23567;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#22270;&#24418;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#19968;&#33268;&#30340;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#20184;&#20986;&#20102;&#24456;&#22810;&#21162;&#21147;&#26469;&#35774;&#35745;&#22270;&#24418;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21069;&#25991;&#20219;&#21153;&#21644;&#19979;&#28216;&#22270;&#24418;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#24046;&#36317;&#65292;&#36825;&#19981;&#20805;&#20998;&#21457;&#25381;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#29978;&#33267;&#23548;&#33268;&#36127;&#20256;&#36882;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#19982;&#19968;&#33268;&#30340;&#35757;&#32451;&#30446;&#26631;&#23545;&#40784;&#65292;&#25552;&#31034;&#35843;&#20248;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22270;&#24418;&#25552;&#31034;&#35843;&#20248;&#30340;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#22270;&#39046;&#22495;&#20013;&#21508;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#20043;&#38388;&#32570;&#20047;&#24378;&#22823;&#19988;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#22312;&#20110;&#35774;&#35745;&#19968;&#33268;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#26082;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20063;&#36866;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#19978;&#36848;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SGL-PT&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36981;&#24490;&#23398;&#20064;&#31574;&#30053;&#8220;&#39044;&#35757;&#32451;&#12289;&#25552;&#31034;&#21644;&#39044;&#27979;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, much exertion has been paid to design graph self-supervised methods to obtain generalized pre-trained models, and adapt pre-trained models onto downstream tasks through fine-tuning. However, there exists an inherent gap between pretext and downstream graph tasks, which insufficiently exerts the ability of pre-trained models and even leads to negative transfer. Meanwhile, prompt tuning has seen emerging success in natural language processing by aligning pre-training and fine-tuning with consistent training objectives. In this paper, we identify the challenges for graph prompt tuning: The first is the lack of a strong and universal pre-training task across sundry pre-training methods in graph domain. The second challenge lies in the difficulty of designing a consistent training objective for both pre-training and downstream tasks. To overcome above obstacles, we propose a novel framework named SGL-PT which follows the learning strategy ``Pre-train, Prompt, and Predict''. Specif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SuS-X&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#21517;&#31216;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36801;&#31227;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.16198</link><description>&lt;p&gt;
SuS-X&#65306;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#21517;&#31216;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36801;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SuS-X: Training-Free Name-Only Transfer of Vision-Language Models. (arXiv:2211.16198v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SuS-X&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#21517;&#31216;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36801;&#31227;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#24050;&#25104;&#20026;&#35757;&#32451;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;CLIP&#22312;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#21644;&#26816;&#32034;&#26041;&#38754;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#35201;&#21457;&#25381;&#20854;&#20840;&#37096;&#28508;&#21147;&#65292;&#24494;&#35843;&#20173;&#28982;&#26159;&#24517;&#35201;&#30340;&#12290;&#24494;&#35843;&#25972;&#20010;CLIP&#27169;&#22411;&#20250;&#28040;&#32791;&#36164;&#28304;&#19988;&#19981;&#31283;&#23450;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#34429;&#28982;&#26088;&#22312;&#36991;&#20813;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#20173;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#20998;&#24067;&#20013;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#21478;&#19968;&#31181;&#26041;&#27861;&#8212;&#8212;&#26080;&#38656;&#35757;&#32451;&#30340;&#8220;&#20165;&#22522;&#20110;&#21517;&#31216;&#36801;&#31227;&#8221;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;SuS-X&#65292;&#30001;&#20004;&#20010;&#20851;&#38190;&#26500;&#24314;&#22359;&#8212;&#8212;SuS&#21644;TIP-X&#32452;&#25104;&#65292;&#26082;&#19981;&#38656;&#35201;&#23494;&#38598;&#30340;&#24494;&#35843;&#65292;&#20063;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;SuS-X&#22312;19&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet effective way to train large-scale vision-language models. CLIP demonstrates impressive zero-shot classification and retrieval on diverse downstream tasks. However, to leverage its full potential, fine-tuning still appears to be necessary. Fine-tuning the entire CLIP model can be resource-intensive and unstable. Moreover, recent methods that aim to circumvent this need for fine-tuning still require access to images from the target distribution. In this paper, we pursue a different approach and explore the regime of training-free "name-only transfer" in which the only knowledge we possess about the downstream task comprises the names of downstream target categories. We propose a novel method, SuS-X, consisting of two key building blocks -- SuS and TIP-X, that requires neither intensive fine-tuning nor costly labelled data. SuS-X achieves state-of-the-art zero-shot classification results on 19 benchmark datasets. 
&lt;/p&gt;</description></item></channel></rss>