<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;ChatGPT&#12289;GPT-4&#21644;&#20154;&#31867;&#23548;&#24072;&#22312;&#19981;&#21516;&#30340;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;GPT-4&#20248;&#20110;ChatGPT&#65292;&#25509;&#36817;&#20110;&#20154;&#31867;&#23548;&#24072;&#12290;</title><link>http://arxiv.org/abs/2306.17156</link><description>&lt;p&gt;
&#32534;&#31243;&#25945;&#32946;&#30340;&#29983;&#25104;AI&#65306;&#27604;&#36739;ChatGPT&#12289;GPT-4&#21644;&#20154;&#31867;&#23548;&#24072;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors. (arXiv:2306.17156v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17156
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;ChatGPT&#12289;GPT-4&#21644;&#20154;&#31867;&#23548;&#24072;&#22312;&#19981;&#21516;&#30340;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;GPT-4&#20248;&#20110;ChatGPT&#65292;&#25509;&#36817;&#20110;&#20154;&#31867;&#23548;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#39640;&#35745;&#31639;&#26426;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20026;&#21021;&#32423;&#32534;&#31243;&#25552;&#20379;&#19979;&#19968;&#20195;&#25945;&#32946;&#25216;&#26415;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19982;&#32534;&#31243;&#25945;&#32946;&#30456;&#20851;&#30340;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#30001;&#20110;&#22810;&#31181;&#21407;&#22240;&#32780;&#21463;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#32771;&#34385;&#30340;&#26159;&#24050;&#32463;&#36807;&#26102;&#30340;&#27169;&#22411;&#25110;&#20165;&#20165;&#29305;&#23450;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#30340;&#30740;&#31350;&#26469;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#20004;&#20010;&#27169;&#22411;&#65292;ChatGPT&#65288;&#22522;&#20110;GPT-3.5&#65289;&#21644;GPT-4&#65292;&#24182;&#23558;&#20854;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#19982;&#20154;&#31867;&#23548;&#24072;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#20010;&#21021;&#32423;Python&#32534;&#31243;&#38382;&#39064;&#21644;&#26469;&#33258;&#22312;&#32447;&#24179;&#21488;&#30340;&#30495;&#23454;&#38169;&#35823;&#31243;&#24207;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#19987;&#23478;&#35780;&#27880;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#26126;&#26174;&#20248;&#20110;ChatGPT&#65288;&#22522;&#20110;GPT-3.5&#65289;&#65292;&#24182;&#19988;&#25509;&#36817;&#20110;&#20154;&#31867;&#23548;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies for introductory programming. Recent works have studied these models for different scenarios relevant to programming education; however, these works are limited for several reasons, as they typically consider already outdated models or only specific scenario(s). Consequently, there is a lack of a systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios. We evaluate using five introductory Python programming problems and real-world buggy programs from an online platform, and assess performance using expert-based annotations. Our results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to hu
&lt;/p&gt;</description></item><item><title>LLaVAR&#26159;&#19968;&#20010;&#22686;&#24378;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#20016;&#23500;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#23427;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22312;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.17107</link><description>&lt;p&gt;
LLaVAR:&#22686;&#24378;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#29992;&#20110;&#25991;&#26412;&#20016;&#23500;&#30340;&#22270;&#20687;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding. (arXiv:2306.17107v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17107
&lt;/p&gt;
&lt;p&gt;
LLaVAR&#26159;&#19968;&#20010;&#22686;&#24378;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#20016;&#23500;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#23427;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22312;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#21487;&#20197;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;&#21253;&#25324;&#22270;&#20687;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#65292;&#25910;&#38598;&#22270;&#20687;&#25351;&#20196;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#19981;&#33021;&#24456;&#22909;&#22320;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#32454;&#33410;&#12290;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#24403;&#21069;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#27969;&#31243;&#65292;&#20351;&#29992;&#25991;&#26412;&#20016;&#23500;&#30340;&#22270;&#20687;&#65288;&#22914;&#30005;&#24433;&#28023;&#25253;&#12289;&#22270;&#20070;&#23553;&#38754;&#31561;&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;OCR&#24037;&#20855;&#20174;LAION&#25968;&#25454;&#38598;&#30340;422K&#20010;&#25991;&#26412;&#20016;&#23500;&#30340;&#22270;&#20687;&#19978;&#25552;&#21462;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#35782;&#21035;&#21040;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#26631;&#39064;&#26469;&#21551;&#21160;&#20165;&#25991;&#26412;&#30340;GPT-4&#29983;&#25104;16K&#20010;&#23545;&#35805;&#65292;&#27599;&#20010;&#23545;&#35805;&#21253;&#21547;&#25991;&#26412;&#20016;&#23500;&#30340;&#22270;&#20687;&#30340;&#38382;&#31572;&#23545;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#25910;&#38598;&#30340;&#25968;&#25454;&#19982;&#20808;&#21069;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#32452;&#21512;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;LLaVAR&#22312;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;VQA&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;LLaVA&#27169;&#22411;&#30340;&#33021;&#21147;&#65288;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;20%&#65289;&#21516;&#26102; achieving an accur
&lt;/p&gt;
&lt;p&gt;
Instruction tuning unlocks the superior capability of Large Language Models (LLM) to interact with humans. Furthermore, recent instruction-following datasets include images as visual inputs, collecting responses for image-based instructions. However, visual instruction-tuned models cannot comprehend textual details within images well. This work enhances the current visual instruction tuning pipeline with text-rich images (e.g., movie posters, book covers, etc.). Specifically, we first use publicly available OCR tools to collect results on 422K text-rich images from the LAION dataset. Moreover, we prompt text-only GPT-4 with recognized texts and image captions to generate 16K conversations, each containing question-answer pairs for text-rich images. By combining our collected data with previous multi-modal instruction-following data, our model, LLaVAR, substantially improves the LLaVA model's capability on text-based VQA datasets (up to 20% accuracy improvement) while achieving an accur
&lt;/p&gt;</description></item><item><title>LyricWhiz&#26159;&#19968;&#31181;&#40065;&#26834;&#12289;&#22810;&#35821;&#35328;&#12289;&#38646;&#23556;&#20987;&#30340;&#33258;&#21160;&#27468;&#35789;&#36716;&#24405;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Whisper&#20316;&#20026;"&#32819;&#26421;"&#21644;GPT-4&#20316;&#20026;"&#22823;&#33041;"&#65292;&#23427;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#27468;&#35789;&#36716;&#24405;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.17103</link><description>&lt;p&gt;
LyricWhiz: &#36890;&#36807;&#21521;ChatGPT&#32819;&#35821;&#36827;&#34892;&#40065;&#26834;&#30340;&#22810;&#35821;&#35328;&#38646;&#23556;&#20987;&#27468;&#35789;&#36716;&#24405;
&lt;/p&gt;
&lt;p&gt;
LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT. (arXiv:2306.17103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17103
&lt;/p&gt;
&lt;p&gt;
LyricWhiz&#26159;&#19968;&#31181;&#40065;&#26834;&#12289;&#22810;&#35821;&#35328;&#12289;&#38646;&#23556;&#20987;&#30340;&#33258;&#21160;&#27468;&#35789;&#36716;&#24405;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Whisper&#20316;&#20026;"&#32819;&#26421;"&#21644;GPT-4&#20316;&#20026;"&#22823;&#33041;"&#65292;&#23427;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#27468;&#35789;&#36716;&#24405;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LyricWhiz&#30340;&#40065;&#26834;&#12289;&#22810;&#35821;&#35328;&#12289;&#38646;&#23556;&#20987;&#30340;&#33258;&#21160;&#27468;&#35789;&#36716;&#24405;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27969;&#27966;&#22914;&#25671;&#28378;&#21644;&#37329;&#23646;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#30340;&#20840;&#26032;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Whisper&#65292;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#21450;GPT-4&#65292;&#24403;&#20170;&#26368;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;Whisper&#20805;&#24403;&#8220;&#32819;&#26421;&#8221;&#65292;&#36127;&#36131;&#36716;&#24405;&#35821;&#38899;&#65292;&#32780;GPT-4&#21017;&#20316;&#20026;&#8220;&#22823;&#33041;&#8221;&#65292;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#24378;&#22823;&#24615;&#33021;&#30340;&#19978;&#19979;&#25991;&#36755;&#20986;&#36873;&#25321;&#21644;&#26657;&#27491;&#30340;&#27880;&#37322;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;LyricWhiz&#22312;&#33521;&#35821;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#35789;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#36716;&#24405;&#22810;&#31181;&#35821;&#35328;&#30340;&#27468;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;LyricWhiz&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;CC-BY-NC-SA&#29256;&#26435;&#35768;&#21487;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;MTG-Jamendo&#65292;&#24182;&#25552;&#20379;&#20102;h
&lt;/p&gt;
&lt;p&gt;
We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today's most performant chat-based large language model. In the proposed method, Whisper functions as the "ear" by transcribing the audio, while GPT-4 serves as the "brain," acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copyright license, based on MTG-Jamendo, and offer a h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20174;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#21644;&#27010;&#24565;&#22270;&#12290;&#21516;&#26102;&#20063;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#36798;&#20154;&#31867;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.17089</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Concept-Oriented Deep Learning with Large Language Models. (arXiv:2306.17089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20174;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#21644;&#27010;&#24565;&#22270;&#12290;&#21516;&#26102;&#20063;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#36798;&#20154;&#31867;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#25991;&#26412;&#29983;&#25104;&#21644;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#23427;&#20204;&#20063;&#26159;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#65288;CODL&#65289;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26032;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#21069;&#25552;&#26159;LLMs&#35201;&#29702;&#35299;&#27010;&#24565;&#24182;&#30830;&#20445;&#27010;&#24565;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20197;&#21450;LLMs&#22312;CODL&#20013;&#30340;&#20027;&#35201;&#29992;&#36884;&#65292;&#21253;&#25324;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#27010;&#24565;&#12289;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#27010;&#24565;&#22270;&#21644;&#27010;&#24565;&#23398;&#20064;&#12290;&#20154;&#31867;&#30693;&#35782;&#21253;&#25324;&#31526;&#21495;&#65288;&#27010;&#24565;&#24615;&#65289;&#30693;&#35782;&#21644;&#20855;&#20307;&#65288;&#24863;&#24615;&#65289;&#30693;&#35782;&#12290;&#32780;&#20165;&#25991;&#26412;&#30340;LLMs&#21482;&#33021;&#34920;&#31034;&#31526;&#21495;&#65288;&#27010;&#24565;&#24615;&#65289;&#30693;&#35782;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22810;&#27169;&#24577;LLMs&#33021;&#22815;&#34920;&#31034;&#20154;&#31867;&#30693;&#35782;&#30340;&#23436;&#25972;&#33539;&#22260;&#65288;&#27010;&#24565;&#24615;&#21644;&#24863;&#24615;&#65289;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#35270;&#35273;-&#35821;&#35328;LLMs&#20013;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#36825;&#26159;&#26368;&#37325;&#35201;&#30340;&#22810;&#27169;&#24577;LLMs&#65292;&#24182;&#20171;&#32461;&#20102;&#23427;&#20204;&#22312;CODL&#20013;&#30340;&#20027;&#35201;&#29992;&#36884;&#65292;&#21253;&#25324;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#12289;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been successfully used in many natural-language tasks and applications including text generation and AI chatbots. They also are a promising new technology for concept-oriented deep learning (CODL). However, the prerequisite is that LLMs understand concepts and ensure conceptual consistency. We discuss these in this paper, as well as major uses of LLMs for CODL including concept extraction from text, concept graph extraction from text, and concept learning. Human knowledge consists of both symbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only LLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal LLMs, on the other hand, are capable of representing the full range (conceptual and sensory) of human knowledge. We discuss conceptual understanding in visual-language LLMs, the most important multimodal LLMs, and major uses of them for CODL including concept extraction from image, concept graph extraction from image
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#33014;&#22218;&#32593;&#32476;&#30340;&#38463;&#25289;&#20271;&#35821;&#21644;&#27874;&#26031;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#21333;&#29420;&#30340;&#33014;&#22218;&#32593;&#32476;&#24182;&#20351;&#29992;&#21152;&#26435;&#24230;&#37327;&#26469;&#23454;&#29616;&#24773;&#24863;&#20998;&#31867;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17068</link><description>&lt;p&gt;
&#22522;&#20110;&#21152;&#26435;CapsuleNet&#32593;&#32476;&#30340;&#38463;&#25289;&#20271;&#35821;&#21644;&#27874;&#26031;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Presenting an approach based on weighted CapsuleNet networks for Arabic and Persian multi-domain sentiment analysis. (arXiv:2306.17068v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#33014;&#22218;&#32593;&#32476;&#30340;&#38463;&#25289;&#20271;&#35821;&#21644;&#27874;&#26031;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#21333;&#29420;&#30340;&#33014;&#22218;&#32593;&#32476;&#24182;&#20351;&#29992;&#21152;&#26435;&#24230;&#37327;&#26469;&#23454;&#29616;&#24773;&#24863;&#20998;&#31867;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#23545;&#33258;&#30001;&#25991;&#26412;&#36827;&#34892;&#27491;&#38754;&#12289;&#36127;&#38754;&#25110;&#20013;&#24615;&#30340;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#39640;&#24230;&#20381;&#36182;&#20110;&#39046;&#22495;&#65292;&#20998;&#31867;&#22120;&#22312;&#19968;&#20010;&#39046;&#22495;&#20013;&#21487;&#33021;&#20855;&#26377;&#21512;&#29702;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#21478;&#19968;&#20010;&#39046;&#22495;&#20013;&#30001;&#20110;&#35789;&#35821;&#30340;&#35821;&#20041;&#22810;&#37325;&#24615;&#32780;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27874;&#26031;&#35821;/&#38463;&#25289;&#20271;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#32047;&#31215;&#21152;&#26435;&#33014;&#22218;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#21152;&#26435;&#33014;&#22218;&#38598;&#21512;&#30001;&#20026;&#27599;&#20010;&#39046;&#22495;&#35757;&#32451;&#30340;&#21333;&#29420;&#30340;&#33014;&#22218;&#32593;&#32476;&#21644;&#31216;&#20026;&#39046;&#22495;&#25152;&#23646;&#24230;&#65288;DBD&#65289;&#30340;&#21152;&#26435;&#24230;&#37327;&#32452;&#25104;&#12290;&#36825;&#20010;&#24230;&#37327;&#30001;TF&#21644;IDF&#32452;&#25104;&#65292;&#35745;&#31639;&#27599;&#20010;&#25991;&#26723;&#23545;&#20110;&#27599;&#20010;&#39046;&#22495;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#28982;&#21518;&#20056;&#20197;&#27599;&#20010;&#33014;&#22218;&#21019;&#24314;&#30340;&#21487;&#33021;&#36755;&#20986;&#12290;&#26368;&#32456;&#65292;&#36825;&#20123;&#20056;&#31215;&#30340;&#24635;&#21644;&#26159;&#26368;&#32456;&#36755;&#20986;&#30340;&#26631;&#31614;&#65292;&#24182;&#29992;&#20110;&#30830;&#23450;&#26497;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment classification is a fundamental task in natural language processing, assigning one of the three classes, positive, negative, or neutral, to free texts. However, sentiment classification models are highly domain dependent; the classifier may perform classification with reasonable accuracy in one domain but not in another due to the Semantic multiplicity of words getting poor accuracy. This article presents a new Persian/Arabic multi-domain sentiment analysis method using the cumulative weighted capsule networks approach. Weighted capsule ensemble consists of training separate capsule networks for each domain and a weighting measure called domain belonging degree (DBD). This criterion consists of TF and IDF, which calculates the dependency of each document for each domain separately; this value is multiplied by the possible output that each capsule creates. In the end, the sum of these multiplications is the title of the final output, and is used to determine the polarity. And 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mapKurator&#30340;&#31995;&#32479;&#65292;&#33021;&#23436;&#25972;&#22320;&#20174;&#21382;&#21490;&#22320;&#22270;&#20013;&#25552;&#21462;&#21644;&#38142;&#25509;&#25991;&#26412;&#20449;&#24687;&#12290;&#35813;&#31995;&#32479;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#20301;&#32622;&#30456;&#20851;&#35789;&#35821;&#30340;&#24573;&#30053;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#32771;&#34385;&#26356;&#24191;&#30340;&#20027;&#39064;&#33539;&#22260;&#65292;&#33021;&#22815;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.17059</link><description>&lt;p&gt;
The mapKurator&#31995;&#32479;&#65306;&#20174;&#21382;&#21490;&#22320;&#22270;&#20013;&#25552;&#21462;&#21644;&#38142;&#25509;&#25991;&#26412;&#30340;&#23436;&#25972;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
The mapKurator System: A Complete Pipeline for Extracting and Linking Text from Historical Maps. (arXiv:2306.17059v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17059
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mapKurator&#30340;&#31995;&#32479;&#65292;&#33021;&#23436;&#25972;&#22320;&#20174;&#21382;&#21490;&#22320;&#22270;&#20013;&#25552;&#21462;&#21644;&#38142;&#25509;&#25991;&#26412;&#20449;&#24687;&#12290;&#35813;&#31995;&#32479;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#20301;&#32622;&#30456;&#20851;&#35789;&#35821;&#30340;&#24573;&#30053;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#32771;&#34385;&#26356;&#24191;&#30340;&#20027;&#39064;&#33539;&#22260;&#65292;&#33021;&#22815;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#20855;&#26377;&#31354;&#38388;&#28966;&#28857;&#21644;&#26377;&#20215;&#20540;&#30340;&#22320;&#26041;&#29305;&#24449;&#12290;&#20363;&#22914;&#65292;&#25151;&#22320;&#20135;&#25110;&#26053;&#34892;&#21338;&#23458;&#20013;&#30340;&#21015;&#34920;&#25551;&#36848;&#21253;&#21547;&#26377;&#20851;&#29305;&#23450;&#22320;&#21306;&#31038;&#21306;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#25551;&#36848;&#20154;&#31867;&#22914;&#20309;&#24863;&#30693;&#20182;&#20204;&#30340;&#29615;&#22659;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#30340;&#31532;&#19968;&#27493;&#26159;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#65288;&#20363;&#22914;&#65292;&#22478;&#24066;&#65289;&#12290;&#20256;&#32479;&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#20381;&#36182;&#20110;&#20174;&#25991;&#26723;&#20013;&#26816;&#27979;&#21644;&#28040;&#27495;&#21270;&#22320;&#21517;&#12290;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#21253;&#21547;&#20301;&#32622;&#30701;&#35821;&#21644;&#20020;&#26102;&#35268;&#21017;&#30340;&#35789;&#27719;&#38598;&#65292;&#36825;&#20123;&#35268;&#21017;&#24573;&#30053;&#20102;&#19982;&#20301;&#32622;&#30456;&#20851;&#30340;&#37325;&#35201;&#35789;&#35821;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#36890;&#24120;&#32771;&#34385;&#20960;&#20010;&#24191;&#24230;&#30340;&#20027;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#21487;&#20197;&#26159;&#19968;&#20010;&#22269;&#23478;&#12289;&#19968;&#20010;&#22478;&#24066;&#65292;&#29978;&#33267;&#26159;&#19968;&#20010;&#31038;&#21306;&#65292;&#36825;&#20123;&#33539;&#22260;&#27604;&#36825;&#20123;&#26041;&#27861;&#32771;&#34385;&#30340;&#20027;&#39064;&#25968;&#35201;&#22823;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Documents hold spatial focus and valuable locality characteristics. For example, descriptions of listings in real estate or travel blogs contain information about specific local neighborhoods. This information is valuable to characterize how humans perceive their environment. However, the first step to making use of this information is to identify the spatial focus (e.g., a city) of a document. Traditional approaches for identifying the spatial focus of a document rely on detecting and disambiguating toponyms from the document. This approach requires a vocabulary set of location phrases and ad-hoc rules, which ignore important words related to location. Recent topic modeling approaches using large language models often consider a few topics, each with broad coverage. In contrast, the spatial focus of a document can be a country, a city, or even a neighborhood, which together, is much larger than the number of topics considered in these approaches. Additionally, topic modeling methods a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#32593;&#32476;&#23433;&#20840;&#27861;&#24459;&#35821;&#35328;&#30340;&#35821;&#27861;&#26631;&#27880;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21644;&#25163;&#21160;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#20174;&#27861;&#24459;&#25991;&#20214;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#24615;&#30340;&#33258;&#21160;&#21270;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#27861;&#24459;&#35821;&#35328;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17042</link><description>&lt;p&gt;
&#38754;&#21521;&#32593;&#32476;&#23433;&#20840;&#27861;&#24459;&#35821;&#35328;&#30340;&#35821;&#27861;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Towards Grammatical Tagging for the Legal Language of Cybersecurity. (arXiv:2306.17042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#32593;&#32476;&#23433;&#20840;&#27861;&#24459;&#35821;&#35328;&#30340;&#35821;&#27861;&#26631;&#27880;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21644;&#25163;&#21160;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#20174;&#27861;&#24459;&#25991;&#20214;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#24615;&#30340;&#33258;&#21160;&#21270;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#27861;&#24459;&#35821;&#35328;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#35821;&#35328;&#21487;&#20197;&#29702;&#35299;&#20026;&#24459;&#24072;&#32844;&#19994;&#20013;&#36890;&#24120;&#20351;&#29992;&#30340;&#35821;&#35328;&#65292;&#21487;&#20197;&#26159;&#21475;&#35821;&#25110;&#20070;&#38754;&#24418;&#24335;&#12290;&#26368;&#36817;&#20851;&#20110;&#32593;&#32476;&#23433;&#20840;&#30340;&#31435;&#27861;&#26174;&#28982;&#20351;&#29992;&#20102;&#20070;&#38754;&#30340;&#27861;&#24459;&#35821;&#35328;&#65292;&#22240;&#27492;&#32487;&#25215;&#20102;&#20854;&#35299;&#37322;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;&#26696;&#20363;&#21644;&#32454;&#33410;&#30340;&#20016;&#23500;&#24615;&#12290;&#26412;&#25991;&#38754;&#23545;&#30340;&#25361;&#25112;&#26159;&#23545;&#32593;&#32476;&#23433;&#20840;&#27861;&#24459;&#35821;&#35328;&#36827;&#34892;&#22522;&#26412;&#35299;&#37322;&#65292;&#21363;&#20174;&#19982;&#32593;&#32476;&#23433;&#20840;&#26377;&#20851;&#30340;&#27861;&#24459;&#25991;&#20214;&#20013;&#25552;&#21462;&#22522;&#26412;&#35789;&#24615;&#65288;POS&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#20811;&#26381;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#23427;&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#20197;&#21450;&#25163;&#21160;&#20998;&#26512;&#26469;&#39564;&#35777;&#24037;&#20855;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#65292;&#36825;&#20010;&#26041;&#27861;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#27861;&#24459;&#35821;&#35328;&#65292;&#21482;&#38656;&#23545;&#39044;&#22788;&#29702;&#27493;&#39588;&#36827;&#34892;&#36731;&#24494;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal language can be understood as the language typically used by those engaged in the legal profession and, as such, it may come both in spoken or written form. Recent legislation on cybersecurity obviously uses legal language in writing, thus inheriting all its interpretative complications due to the typical abundance of cases and sub-cases as well as to the general richness in detail. This paper faces the challenge of the essential interpretation of the legal language of cybersecurity, namely of the extraction of the essential Parts of Speech (POS) from the legal documents concerning cybersecurity. The challenge is overcome by our methodology for POS tagging of legal language. It leverages state-of-the-art open-source tools for Natural Language Processing (NLP) as well as manual analysis to validate the outcomes of the tools. As a result, the methodology is automated and, arguably, general for any legal language following minor tailoring of the preprocessing step. It is demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550; LR-GCN&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#34917;&#20840;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#25506;&#32034;&#39640;&#38454;&#22270;&#32467;&#26500;&#65292;&#33258;&#21160;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#36828;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#36923;&#36753;&#25512;&#29702;&#25552;&#28860;&#30693;&#35782;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#31232;&#30095;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.17034</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#21033;&#29992;&#39640;&#38454;&#22270;&#32467;&#26500;&#36827;&#34892;&#31232;&#30095;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Exploring &amp; Exploiting High-Order Graph Structure for Sparse Knowledge Graph Completion. (arXiv:2306.17034v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550; LR-GCN&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#34917;&#20840;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#25506;&#32034;&#39640;&#38454;&#22270;&#32467;&#26500;&#65292;&#33258;&#21160;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#36828;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#36923;&#36753;&#25512;&#29702;&#25552;&#28860;&#30693;&#35782;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#31232;&#30095;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#30693;&#35782;&#22270;&#35889;&#22330;&#26223;&#23545;&#20043;&#21069;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#21363;&#38543;&#30528;&#22270;&#30340;&#31232;&#30095;&#24615;&#22686;&#21152;&#65292;&#34917;&#20840;&#24615;&#33021;&#36805;&#36895;&#19979;&#38477;&#12290;&#30001;&#20110;&#31232;&#30095;&#30693;&#35782;&#22270;&#35889;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#36825;&#20010;&#38382;&#39064;&#20063;&#34987;&#21152;&#21095;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;LR-GCN&#65292;&#33021;&#22815;&#33258;&#21160;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#26377;&#20215;&#20540;&#30340;&#36828;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#34917;&#20805;&#19981;&#36275;&#30340;&#32467;&#26500;&#29305;&#24449;&#24182;&#25552;&#28860;&#36923;&#36753;&#25512;&#29702;&#30693;&#35782;&#29992;&#20110;&#31232;&#30095;&#22270;&#35889;&#34917;&#20840;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#22522;&#20110;GNN&#30340;&#39044;&#27979;&#22120;&#21644;&#25512;&#29702;&#36335;&#24452;&#25552;&#21462;&#22120;&#12290;&#25512;&#29702;&#36335;&#24452;&#25552;&#21462;&#22120;&#25506;&#32034;&#39640;&#38454;&#22270;&#32467;&#26500;&#65292;&#22914;&#25512;&#29702;&#36335;&#24452;&#65292;&#24182;&#23558;&#20854;&#32534;&#30721;&#20026;&#23500;&#35821;&#20041;&#36793;&#65292;&#26126;&#30830;&#22320;&#23558;&#36828;&#31243;&#20381;&#36182;&#20851;&#31995;&#32452;&#21512;&#21040;&#39044;&#27979;&#22120;&#20013;&#12290;&#27492;&#27493;&#39588;&#36824;&#22312;&#31232;&#30095;&#38382;&#39064;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#31232;&#30095;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36335;&#24452;&#25552;&#21462;&#22120;&#36824;&#21487;&#20197;&#24110;&#21161;&#23494;&#21270;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse knowledge graph (KG) scenarios pose a challenge for previous Knowledge Graph Completion (KGC) methods, that is, the completion performance decreases rapidly with the increase of graph sparsity. This problem is also exacerbated because of the widespread existence of sparse KGs in practical applications. To alleviate this challenge, we present a novel framework, LR-GCN, that is able to automatically capture valuable long-range dependency among entities to supplement insufficient structure features and distill logical reasoning knowledge for sparse KGC. The proposed approach comprises two main components: a GNN-based predictor and a reasoning path distiller. The reasoning path distiller explores high-order graph structures such as reasoning paths and encodes them as rich-semantic edges, explicitly compositing long-range dependencies into the predictor. This step also plays an essential role in densifying KGs, effectively alleviating the sparse issue. Furthermore, the path distiller
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;NLP&#22788;&#29702;&#26041;&#27861;&#30340;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#26679;&#26412;&#26469;&#24179;&#34913;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#32570;&#38519;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26469;&#36171;&#20104;&#27169;&#22411;&#23545;&#23567;&#25968;&#25454;&#38598;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17020</link><description>&lt;p&gt;
&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21028;&#20915;&#25991;&#20214;&#23545;&#29359;&#32618;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying Crime Types using Judgment Documents from Social Media. (arXiv:2306.17020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;NLP&#22788;&#29702;&#26041;&#27861;&#30340;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#26679;&#26412;&#26469;&#24179;&#34913;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#32570;&#38519;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26469;&#36171;&#20104;&#27169;&#22411;&#23545;&#23567;&#25968;&#25454;&#38598;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29359;&#32618;&#34892;&#20026;&#20107;&#23454;&#26469;&#30830;&#23450;&#29359;&#32618;&#31867;&#22411;&#30340;&#20219;&#21153;&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#21644;&#26377;&#24847;&#20041;&#12290;&#20294;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#29359;&#32618;&#26412;&#36523;&#30340;&#24615;&#36136;&#65292;&#25968;&#25454;&#26679;&#26412;&#26412;&#36523;&#20998;&#24067;&#19981;&#22343;&#21248;&#12290;&#21516;&#26102;&#65292;&#21496;&#27861;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#23569;&#26377;&#20844;&#24320;&#21487;&#29992;&#65292;&#26080;&#27861;&#20135;&#29983;&#29992;&#20110;&#30452;&#25509;&#35757;&#32451;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;NLP&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#29359;&#32618;&#20107;&#23454;&#25968;&#25454;&#39044;&#22788;&#29702;&#27169;&#22359;(CFDPM)&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#26679;&#26412;&#26469;&#24179;&#34913;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#32570;&#38519;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#24320;&#28304;&#25968;&#25454;&#38598;(CAIL-big)&#20316;&#20026;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#25105;&#20204;&#33258;&#24049;&#25910;&#38598;&#30340;&#19968;&#20010;&#23567;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#36171;&#20104;&#27169;&#22411;&#23545;&#19981;&#29087;&#24713;&#30340;&#23567;&#25968;&#25454;&#38598;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#25913;&#36827;&#30340;Bert&#27169;&#22411;&#21644;&#21160;&#24577;&#36974;&#34109;&#26469;&#25913;&#36827;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
The task of determining crime types based on criminal behavior facts has become a very important and meaningful task in social science. But the problem facing the field now is that the data samples themselves are unevenly distributed, due to the nature of the crime itself. At the same time, data sets in the judicial field are less publicly available, and it is not practical to produce large data sets for direct training. This article proposes a new training model to solve this problem through NLP processing methods. We first propose a Crime Fact Data Preprocessing Module (CFDPM), which can balance the defects of uneven data set distribution by generating new samples. Then we use a large open source dataset (CAIL-big) as our pretraining dataset and a small dataset collected by ourselves for Fine-tuning, giving it good generalization ability to unfamiliar small datasets. At the same time, we use the improved Bert model with dynamic masking to improve the model. Experiments show that the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#37197;&#38899;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#39044;&#27979;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#23398;&#20064;&#21644;&#26356;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21512;&#25104;&#65292;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.17005</link><description>&lt;p&gt;
&#20934;&#30830;&#23545;&#40784;&#30340;&#39640;&#36136;&#37327;&#33258;&#21160;&#37197;&#38899;&#65306;&#36890;&#36807;&#33258;&#30417;&#30563;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#23454;&#29616;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
High-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units. (arXiv:2306.17005v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#37197;&#38899;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#39044;&#27979;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#23398;&#20064;&#21644;&#26356;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21512;&#25104;&#65292;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#37197;&#38899;&#65288;AVO&#65289;&#30340;&#30446;&#26631;&#26159;&#26681;&#25454;&#38745;&#38899;&#35270;&#39057;&#30340;&#25991;&#26412;&#33050;&#26412;&#29983;&#25104;&#19982;&#20043;&#21516;&#27493;&#30340;&#35821;&#38899;&#12290;&#26368;&#36817;&#30340;AVO&#26694;&#26550;&#24314;&#31435;&#22312;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65288;TTS&#65289;&#20043;&#19978;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;AVO&#30340;&#23398;&#20064;&#30446;&#26631;&#26159;&#22768;&#23398;&#29305;&#24449;&#37325;&#24314;&#65292;&#20026;&#36328;&#27169;&#24577;&#23545;&#40784;&#23398;&#20064;&#24102;&#26469;&#20102;&#38388;&#25509;&#30417;&#30563;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21516;&#27493;&#24615;&#33021;&#21644;&#21512;&#25104;&#35821;&#38899;&#36136;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;AVO&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#39044;&#27979;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#19981;&#20165;&#20026;&#23545;&#40784;&#23398;&#20064;&#25552;&#20379;&#20102;&#26356;&#30452;&#25509;&#30340;&#30417;&#30563;&#65292;&#36824;&#20943;&#36731;&#20102;&#25991;&#26412;-&#35270;&#39057;&#19978;&#19979;&#25991;&#19982;&#22768;&#23398;&#29305;&#24449;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#22068;&#21767;-&#35821;&#38899;&#21516;&#27493;&#21644;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#12290;&#20195;&#30721;&#21644;&#35821;&#38899;&#26679;&#26412;&#24050;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Automatic Voice Over (AVO) is to generate speech in sync with a silent video given its text script. Recent AVO frameworks built upon text-to-speech synthesis (TTS) have shown impressive results. However, the current AVO learning objective of acoustic feature reconstruction brings in indirect supervision for inter-modal alignment learning, thus limiting the synchronization performance and synthetic speech quality. To this end, we propose a novel AVO method leveraging the learning objective of self-supervised discrete speech unit prediction, which not only provides more direct supervision for the alignment learning, but also alleviates the mismatch between the text-video context and acoustic features. Experimental results show that our proposed method achieves remarkable lip-speech synchronization and high speech quality by outperforming baselines in both objective and subjective evaluations. Code and speech samples are publicly available.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35201;&#32032;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#65288;MEMD-ABSA&#65289;&#65292;&#29992;&#20110;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#30340;&#30740;&#31350;&#12290;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20116;&#20010;&#39046;&#22495;&#30340;&#22235;&#20010;&#35201;&#32032;&#65292;&#21253;&#25324;&#36817;2&#19975;&#20010;&#35780;&#35770;&#21477;&#23376;&#21644;3&#19975;&#20010;&#24102;&#26377;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#38754;&#21644;&#35266;&#28857;&#30340;&#22235;&#20803;&#32452;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24320;&#25918;&#39046;&#22495;ABSA&#20197;&#21450;&#25366;&#25496;&#38544;&#21547;&#30340;&#26041;&#38754;&#21644;&#35266;&#28857;&#20173;&#28982;&#26159;&#24453;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.16956</link><description>&lt;p&gt;
MEMD-ABSA&#65306;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#30340;&#22810;&#35201;&#32032;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MEMD-ABSA: A Multi-Element Multi-Domain Dataset for Aspect-Based Sentiment Analysis. (arXiv:2306.16956v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16956
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35201;&#32032;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#65288;MEMD-ABSA&#65289;&#65292;&#29992;&#20110;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#30340;&#30740;&#31350;&#12290;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20116;&#20010;&#39046;&#22495;&#30340;&#22235;&#20010;&#35201;&#32032;&#65292;&#21253;&#25324;&#36817;2&#19975;&#20010;&#35780;&#35770;&#21477;&#23376;&#21644;3&#19975;&#20010;&#24102;&#26377;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#38754;&#21644;&#35266;&#28857;&#30340;&#22235;&#20803;&#32452;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24320;&#25918;&#39046;&#22495;ABSA&#20197;&#21450;&#25366;&#25496;&#38544;&#21547;&#30340;&#26041;&#38754;&#21644;&#35266;&#28857;&#20173;&#28982;&#26159;&#24453;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26159;&#24773;&#24863;&#25366;&#25496;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#30340;&#30740;&#31350;&#20852;&#36259;&#65292;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#36880;&#28176;&#23558;&#28966;&#28857;&#20174;&#31616;&#21333;&#30340;ABSA&#23376;&#20219;&#21153;&#36716;&#21521;&#31471;&#21040;&#31471;&#30340;&#22810;&#35201;&#32032;ABSA&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#23616;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#20010;&#21035;&#35201;&#32032;&#65292;&#36890;&#24120;&#20851;&#27880;&#20110;&#39046;&#22495;&#20869;&#35774;&#32622;&#65292;&#24573;&#30053;&#20102;&#38544;&#21547;&#30340;&#26041;&#38754;&#21644;&#35266;&#28857;&#65292;&#24182;&#19988;&#25968;&#25454;&#35268;&#27169;&#36739;&#23567;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35201;&#32032;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;(MEMD)&#65292;&#28085;&#30422;&#20102;&#20116;&#20010;&#39046;&#22495;&#30340;&#22235;&#20010;&#35201;&#32032;&#65292;&#21253;&#25324;&#36817;2&#19975;&#20010;&#35780;&#35770;&#21477;&#23376;&#21644;3&#19975;&#20010;&#24102;&#26377;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#38754;&#21644;&#35266;&#28857;&#30340;&#22235;&#20803;&#32452;&#65292;&#21487;&#29992;&#20110;ABSA&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#22312;&#24320;&#25918;&#39046;&#22495;&#35774;&#32622;&#19979;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#21644;&#38750;&#29983;&#25104;&#24335;&#22522;&#32447;&#27169;&#22411;&#22312;&#22810;&#20010;ABSA&#23376;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#24320;&#25918;&#39046;&#22495;ABSA&#20197;&#21450;&#25366;&#25496;&#38544;&#21547;&#30340;&#26041;&#38754;&#21644;&#35266;&#28857;&#20173;&#28982;&#26159;&#24453;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment analysis is a long-standing research interest in the field of opinion mining, and in recent years, researchers have gradually shifted their focus from simple ABSA subtasks to end-to-end multi-element ABSA tasks. However, the datasets currently used in the research are limited to individual elements of specific tasks, usually focusing on in-domain settings, ignoring implicit aspects and opinions, and with a small data scale. To address these issues, we propose a large-scale Multi-Element Multi-Domain dataset (MEMD) that covers the four elements across five domains, including nearly 20,000 review sentences and 30,000 quadruples annotated with explicit and implicit aspects and opinions for ABSA research. Meanwhile, we evaluate generative and non-generative baselines on multiple ABSA subtasks under the open domain setting, and the results show that open domain ABSA as well as mining implicit aspects and opinions remain ongoing challenges to be addressed. The datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;&#39044;&#27979;&#38899;&#20048;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#21644;&#20998;&#31867;&#22120;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#23558;&#38899;&#20048;&#24207;&#21015;&#35299;&#26512;&#20026;&#20381;&#36182;&#26641;&#65292;&#24182;&#22312;&#22810;&#20010;&#38899;&#20048;&#29305;&#24449;&#21644;&#39034;&#24207;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#37096;&#20998;&#32467;&#26524;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16955</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#35299;&#30721;&#22120;&#39044;&#27979;&#38899;&#20048;&#23618;&#27425;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Predicting Music Hierarchies with a Graph-Based Neural Decoder. (arXiv:2306.16955v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;&#39044;&#27979;&#38899;&#20048;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#21644;&#20998;&#31867;&#22120;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#23558;&#38899;&#20048;&#24207;&#21015;&#35299;&#26512;&#20026;&#20381;&#36182;&#26641;&#65292;&#24182;&#22312;&#22810;&#20010;&#38899;&#20048;&#29305;&#24449;&#21644;&#39034;&#24207;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#37096;&#20998;&#32467;&#26524;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#23558;&#38899;&#20048;&#24207;&#21015;&#35299;&#26512;&#20026;&#20381;&#36182;&#26641;&#65292;&#36825;&#26159;&#38899;&#20048;&#35748;&#30693;&#30740;&#31350;&#21644;&#38899;&#20048;&#20998;&#26512;&#20013;&#20351;&#29992;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#35299;&#26512;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#23558;&#36755;&#20837;&#24207;&#21015;&#36890;&#36807;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#20256;&#36882;&#65292;&#20197;&#20016;&#23500;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#20998;&#31867;&#22120;&#31579;&#36873;&#20986;&#25152;&#26377;&#21487;&#33021;&#30340;&#20381;&#36182;&#24359;&#30340;&#22270;&#65292;&#29983;&#25104;&#20381;&#36182;&#26641;&#12290;&#35813;&#31995;&#32479;&#30340;&#19968;&#20010;&#20027;&#35201;&#20248;&#28857;&#26159;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#20013;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23427;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#29305;&#23450;&#30340;&#31526;&#21495;&#35821;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#38899;&#20048;&#29305;&#24449;&#65292;&#21033;&#29992;&#39034;&#24207;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#38024;&#23545;&#22024;&#26434;&#30340;&#36755;&#20837;&#20135;&#29983;&#37096;&#20998;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#27979;&#35797;&#22312;&#20004;&#20010;&#38899;&#20048;&#26641;&#25968;&#25454;&#38598;&#19978; - &#21333;&#22768;&#37096;&#38899;&#31526;&#24207;&#21015;&#30340;&#26102;&#38388;&#36328;&#24230;&#26641;&#21644;&#29237;&#22763;&#21644;&#24358;&#24207;&#21015;&#30340;&#21644;&#22768;&#26641;&#19978;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes a data-driven framework to parse musical sequences into dependency trees, which are hierarchical structures used in music cognition research and music analysis. The parsing involves two steps. First, the input sequence is passed through a transformer encoder to enrich it with contextual information. Then, a classifier filters the graph of all possible dependency arcs to produce the dependency tree. One major benefit of this system is that it can be easily integrated into modern deep-learning pipelines. Moreover, since it does not rely on any particular symbolic grammar, it can consider multiple musical features simultaneously, make use of sequential context information, and produce partial results for noisy inputs. We test our approach on two datasets of musical trees -- time-span trees of monophonic note sequences and harmonic trees of jazz chord sequences -- and show that our approach outperforms previous methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;UMASS_BioNLP&#22242;&#38431;&#22312;MEDIQA-Chat 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#21442;&#19982;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;LLMs&#21327;&#20316;&#31995;&#32479;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;ChatGPT&#21644;GPT-4&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.16931</link><description>&lt;p&gt;
UMASS_BioNLP&#21442;&#21152;MEDIQA-Chat 2023&#65306;LLMs&#33021;&#21542;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21307;&#29983;-&#24739;&#32773;&#22522;&#20110;&#31508;&#35760;&#30340;&#23545;&#35805;&#65311;
&lt;/p&gt;
&lt;p&gt;
UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?. (arXiv:2306.16931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;UMASS_BioNLP&#22242;&#38431;&#22312;MEDIQA-Chat 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#21442;&#19982;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;LLMs&#21327;&#20316;&#31995;&#32479;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;ChatGPT&#21644;GPT-4&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;UMASS_BioNLP&#22242;&#38431;&#21442;&#19982;MEDIQA-Chat 2023&#20849;&#20139;&#20219;&#21153;&#30340;Task-A&#21644;Task-C&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;Task-C&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21307;&#29983;-&#24739;&#32773;&#24490;&#29615;&#30340;&#26032;&#22411;LLMs&#21327;&#20316;&#31995;&#32479;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ROUGE&#12289;&#21307;&#30103;&#27010;&#24565;&#21484;&#22238;&#29575;&#12289;BLEU&#21644;Self-BLEU&#31561;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#19979;&#34920;&#29616;&#21512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;ChatGPT&#21644;GPT-4&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#21033;&#29992;&#21327;&#20316;LLMs&#29983;&#25104;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023 shared task for Task-A and Task-C. We focus especially on Task-C and propose a novel LLMs cooperation system named a doctor-patient loop to generate high-quality conversation data sets. The experiment results demonstrate that our approaches yield reasonable performance as evaluated by automatic metrics such as ROUGE, medical concept recall, BLEU, and Self-BLEU. Furthermore, we conducted a comparative analysis between our proposed method and ChatGPT and GPT-4. This analysis also investigates the potential of utilizing cooperation LLMs to generate high-quality datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#36328;&#35805;&#35821;&#19978;&#19979;&#25991;&#26469;&#25552;&#21319;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#35299;&#30721;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#27874;&#26463;&#25628;&#32034;&#21644;&#38271;&#19978;&#19979;&#25991;&#36716;&#25442;&#22120;LMs&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36328;&#35805;&#35821;&#19978;&#19979;&#25991;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#35782;&#21035;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.16903</link><description>&lt;p&gt;
&#21033;&#29992;&#36328;&#35805;&#35821;&#19978;&#19979;&#25991;&#36827;&#34892;&#35821;&#38899;&#35782;&#21035;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Leveraging Cross-Utterance Context For ASR Decoding. (arXiv:2306.16903v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#36328;&#35805;&#35821;&#19978;&#19979;&#25991;&#26469;&#25552;&#21319;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#35299;&#30721;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#27874;&#26463;&#25628;&#32034;&#21644;&#38271;&#19978;&#19979;&#25991;&#36716;&#25442;&#22120;LMs&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36328;&#35805;&#35821;&#19978;&#19979;&#25991;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#35782;&#21035;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#24120;&#34987;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#35299;&#30721;&#38454;&#27573;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21482;&#20351;&#29992;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31532;&#20108;&#27425;&#37325;&#26032;&#35780;&#20998;&#26102;&#65292;&#36328;&#35805;&#35821;&#20449;&#24687;&#23545;&#25552;&#21319;&#24615;&#33021;&#26377;&#30410;&#65292;&#28982;&#32780;&#36825;&#20165;&#22522;&#20110;&#31532;&#19968;&#27425;&#35821;&#35328;&#27169;&#22411;&#21487;&#29992;&#30340;&#23616;&#37096;&#20449;&#24687;&#26469;&#38480;&#21046;&#20551;&#35774;&#31354;&#38388;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#27874;&#26463;&#25628;&#32034;&#23558;&#38271;&#19978;&#19979;&#25991;&#36716;&#25442;&#22120;LMs&#24212;&#29992;&#20110;&#22768;&#23398;&#27169;&#22411;&#30340;&#36328;&#35805;&#35821;&#35299;&#30721;&#65292;&#24182;&#19982;n-best&#37325;&#26032;&#35780;&#20998;&#30340;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27874;&#26463;&#25628;&#32034;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36328;&#35805;&#35821;&#19978;&#19979;&#25991;&#12290;&#22312;&#38271;&#26684;&#24335;&#25968;&#25454;&#38598;AMI&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#19982;&#21333;&#35805;&#35821;&#35774;&#32622;&#30456;&#27604;&#65292;dev&#21644;test&#38598;&#30340;&#32477;&#23545;&#20943;&#23569;&#20998;&#21035;&#20026;0.7&#65285;&#21644;0.3&#65285;&#65292;&#21253;&#25324;&#22810;&#36798;500&#20010;&#26631;&#35760;&#30340;&#20808;&#21069;&#19978;&#19979;&#25991;&#26102;&#36824;&#26377;&#25913;&#36827;&#12290;Tedlium-1&#20063;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25913;&#36827;&#24133;&#24230;&#32422;&#20026;0.1&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
While external language models (LMs) are often incorporated into the decoding stage of automated speech recognition systems, these models usually operate with limited context. Cross utterance information has been shown to be beneficial during second pass re-scoring, however this limits the hypothesis space based on the local information available to the first pass LM. In this work, we investigate the incorporation of long-context transformer LMs for cross-utterance decoding of acoustic models via beam search, and compare against results from n-best rescoring. Results demonstrate that beam search allows for an improved use of cross-utterance context. When evaluating on the long-format dataset AMI, results show a 0.7\% and 0.3\% absolute reduction on dev and test sets compared to the single-utterance setting, with improvements when including up to 500 tokens of prior context. Evaluations are also provided for Tedlium-1 with less significant improvements of around 0.1\% absolute.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#35745;&#31639;&#38656;&#27714;&#37327;&#22823;&#30340;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#19981;&#24179;&#31561;&#21644;&#25285;&#24551;&#65292;&#36890;&#36807;&#23545;NLP&#31038;&#21306;&#30340;312&#20301;&#21442;&#19982;&#32773;&#36827;&#34892;&#35843;&#26597;&#65292;&#21457;&#29616;&#20102;&#22312;&#36164;&#21382;&#12289;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#31561;&#26041;&#38754;&#23384;&#22312;&#30340;&#65288;&#19981;&#65289;&#24179;&#31561;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#32531;&#35299;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.16900</link><description>&lt;p&gt;
&#35843;&#26597;&#35745;&#31639;&#38656;&#27714;&#37327;&#22823;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#30340;&#19981;&#24179;&#31561;&#21644;&#25285;&#24551;
&lt;/p&gt;
&lt;p&gt;
Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research. (arXiv:2306.16900v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16900
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#35745;&#31639;&#38656;&#27714;&#37327;&#22823;&#30340;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#19981;&#24179;&#31561;&#21644;&#25285;&#24551;&#65292;&#36890;&#36807;&#23545;NLP&#31038;&#21306;&#30340;312&#20301;&#21442;&#19982;&#32773;&#36827;&#34892;&#35843;&#26597;&#65292;&#21457;&#29616;&#20102;&#22312;&#36164;&#21382;&#12289;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#31561;&#26041;&#38754;&#23384;&#22312;&#30340;&#65288;&#19981;&#65289;&#24179;&#31561;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#32531;&#35299;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#35768;&#22810;&#26368;&#26032;&#36827;&#23637;&#28304;&#20110;&#24320;&#21457;&#21644;&#20351;&#29992;&#20855;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#12290;&#22823;&#27169;&#22411;&#30340;&#35268;&#27169;&#20351;&#24471;&#35745;&#31639;&#25104;&#26412;&#25104;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#20043;&#19968;&#65307;&#24182;&#19988;&#23545;&#20110;&#30740;&#31350;PLMs&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#21253;&#23481;&#24615;&#24341;&#21457;&#20102;&#20005;&#37325;&#30340;&#25285;&#24551;&#12290;&#36825;&#20123;&#25285;&#24551;&#24448;&#24448;&#22522;&#20110;&#20010;&#20154;&#32463;&#39564;&#21644;&#35266;&#23519;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#36827;&#34892;&#22823;&#35268;&#27169;&#35843;&#26597;&#26469;&#35843;&#26597;&#36825;&#20123;&#25285;&#24551;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#37327;&#21270;&#19982;&#29615;&#22659;&#24433;&#21709;&#12289;&#20844;&#24179;&#24615;&#21644;&#21516;&#34892;&#35780;&#23457;&#24433;&#21709;&#30456;&#20851;&#30340;&#36825;&#20123;&#25285;&#24551;&#12290;&#36890;&#36807;&#23545;NLP&#31038;&#21306;&#30340;312&#20301;&#21442;&#19982;&#32773;&#36827;&#34892;&#35843;&#26597;&#65292;&#25105;&#20204;&#25429;&#25417;&#21040;&#19981;&#21516;&#32676;&#20307;&#20869;&#37096;&#21644;&#20043;&#38388;&#30340;&#29616;&#26377;&#65288;&#19981;&#65289;&#24179;&#31561;&#29616;&#35937;&#65292;&#21253;&#25324;&#36164;&#21382;&#12289;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#65292;&#20197;&#21450;&#23427;&#20204;&#23545;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#27599;&#20010;&#20027;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#26512;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#32531;&#35299;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent improvements in NLP stem from the development and use of large pre-trained language models (PLMs) with billions of parameters. Large model sizes makes computational cost one of the main limiting factors for training and evaluating such models; and has raised severe concerns about the sustainability, reproducibility, and inclusiveness for researching PLMs. These concerns are often based on personal experiences and observations. However, there had not been any large-scale surveys that investigate them. In this work, we provide a first attempt to quantify these concerns regarding three topics, namely, environmental impact, equity, and impact on peer reviewing. By conducting a survey with 312 participants from the NLP community, we capture existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process. For each topic, we provide an analysis and devise recommendations to mitigate found 
&lt;/p&gt;</description></item><item><title>&#20248;&#31168;&#30340;&#20998;&#35789;&#22120;&#33021;&#22815;&#23454;&#29616;&#36739;&#39640;&#30340;&#36890;&#36947;&#20351;&#29992;&#25928;&#29575;&#65292;&#24182;&#19988;R\'enyi&#29109;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#19982;\textsc{Bleu}&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16842</link><description>&lt;p&gt;
Tokenization&#21644;&#26080;&#22122;&#22768;&#36890;&#36947;
&lt;/p&gt;
&lt;p&gt;
Tokenization and the Noiseless Channel. (arXiv:2306.16842v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16842
&lt;/p&gt;
&lt;p&gt;
&#20248;&#31168;&#30340;&#20998;&#35789;&#22120;&#33021;&#22815;&#23454;&#29616;&#36739;&#39640;&#30340;&#36890;&#36947;&#20351;&#29992;&#25928;&#29575;&#65292;&#24182;&#19988;R\'enyi&#29109;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#19982;\textsc{Bleu}&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#35789;&#20998;&#35789;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20110;&#20026;&#20160;&#20040;&#26576;&#20123;&#20998;&#35789;&#22120;&#21644;&#36229;&#21442;&#25968;&#32452;&#21512;&#20250;&#27604;&#20854;&#20182;&#32452;&#21512;&#22312;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20248;&#31168;&#30340;&#20998;&#35789;&#22120;&#20250;&#23548;&#33268;\emph{&#25928;&#29575;}&#36739;&#39640;&#30340;&#36890;&#36947;&#20351;&#29992;&#65292;&#20854;&#20013;&#36890;&#36947;&#26159;&#25351;&#23558;&#26576;&#20123;&#36755;&#20837;&#20256;&#36882;&#32473;&#27169;&#22411;&#30340;&#26041;&#24335;&#65292;&#32780;&#25928;&#29575;&#21487;&#20197;&#29992;&#20449;&#24687;&#35770;&#26415;&#35821;&#20013;&#30340;Shannon&#29109;&#19982;&#20196;&#29260;&#20998;&#24067;&#30340;&#26368;&#22823;&#29109;&#20043;&#27604;&#26469;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;Shannon&#29109;&#36827;&#34892;&#30340;&#26368;&#20248;&#32534;&#30721;&#23558;&#25226;&#20302;&#39057;&#20196;&#29260;&#36171;&#20104;&#26497;&#38271;&#30340;&#32534;&#30721;&#65292;&#25226;&#39640;&#39057;&#20196;&#29260;&#36171;&#20104;&#26497;&#30701;&#30340;&#32534;&#30721;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29992;R\'enyi&#29109;&#26469;&#23450;&#20041;&#25928;&#29575;&#21017;&#20250;&#24809;&#32602;&#20855;&#26377;&#26497;&#39640;&#25110;&#26497;&#20302;&#39057;&#20196;&#29260;&#30340;&#20998;&#24067;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#22810;&#20010;&#20998;&#35789;&#22120;&#20013;&#65292;&#24403;$\alpha = 2.5$&#26102;&#65292;R\'enyi&#29109;&#19982;\textsc{Bleu}&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65288;$0.78$&#65289;&#65292;&#32780;&#30456;&#27604;&#20043;&#19979;&#65292;Shannon&#29109;&#19982;\textsc{Bleu}&#30340;&#30456;&#20851;&#24615;&#20165;&#20026;$-0.32$&#12290;
&lt;/p&gt;
&lt;p&gt;
Subword tokenization is a key part of many NLP pipelines. However, little is known about why some tokenizer and hyperparameter combinations lead to better downstream model performance than others. We propose that good tokenizers lead to \emph{efficient} channel usage, where the channel is the means by which some input is conveyed to the model and efficiency can be quantified in information-theoretic terms as the ratio of the Shannon entropy to the maximum possible entropy of the token distribution. Yet, an optimal encoding according to Shannon entropy assigns extremely long codes to low-frequency tokens and very short codes to high-frequency tokens. Defining efficiency in terms of R\'enyi entropy, on the other hand, penalizes distributions with either very high or very low-frequency tokens. In machine translation, we find that across multiple tokenizers, the R\'enyi entropy with $\alpha = 2.5$ has a very strong correlation with \textsc{Bleu}: $0.78$ in comparison to just $-0.32$ for co
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20174;&#24418;&#24335;&#21270;&#30340;&#35282;&#24230;&#23545;Byte-Pair&#32534;&#30721;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36845;&#20195;&#36138;&#23146;&#29256;&#26412;&#26159;&#23545;&#26368;&#20248;&#21512;&#24182;&#24207;&#21015;&#30340;&#36817;&#20284;&#35299;&#65292;&#24182;&#20248;&#21270;&#20102;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.16837</link><description>&lt;p&gt;
Byte-Pair&#32534;&#30721;&#30340;&#24418;&#24335;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Formal Perspective on Byte-Pair Encoding. (arXiv:2306.16837v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20174;&#24418;&#24335;&#21270;&#30340;&#35282;&#24230;&#23545;Byte-Pair&#32534;&#30721;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36845;&#20195;&#36138;&#23146;&#29256;&#26412;&#26159;&#23545;&#26368;&#20248;&#21512;&#24182;&#24207;&#21015;&#30340;&#36817;&#20284;&#35299;&#65292;&#24182;&#20248;&#21270;&#20102;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Byte-Pair&#32534;&#30721;&#65288;BPE&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25968;&#25454;&#26631;&#35760;&#31639;&#27861;&#65292;&#23613;&#31649;&#26368;&#21021;&#26159;&#20316;&#20026;&#19968;&#31181;&#21387;&#32553;&#26041;&#27861;&#32780;&#35774;&#35745;&#30340;&#12290;BPE&#34920;&#38754;&#19978;&#30475;&#36215;&#26469;&#26159;&#19968;&#31181;&#36138;&#23146;&#31639;&#27861;&#65292;&#20294;&#26159;BPE&#23547;&#27714;&#35299;&#20915;&#30340;&#24213;&#23618;&#20248;&#21270;&#38382;&#39064;&#23578;&#26410;&#26126;&#30830;&#12290;&#25105;&#20204;&#23558;BPE&#24418;&#24335;&#21270;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#23376;&#27169;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36845;&#20195;&#36138;&#23146;&#29256;&#26412;&#26159;&#19968;&#20010;&#23545;&#20110;&#26368;&#20248;&#21512;&#24182;&#24207;&#21015;&#30340;$\frac{1}{{\sigma(\boldsymbol{\mu}^\star)}}(1-e^{-{\sigma(\boldsymbol{\mu}^\star)}})$-&#36817;&#20284;&#35299;&#65292;&#20854;&#20013;${\sigma(\boldsymbol{\mu}^\star)}$&#26159;&#30456;&#23545;&#20110;&#26368;&#20248;&#21512;&#24182;&#24207;&#21015;$\boldsymbol{\mu}^\star$&#30340;&#24635;&#21521;&#21518;&#26354;&#29575;&#12290;&#32463;&#39564;&#35777;&#36817;&#20284;&#35299;&#30340;&#19979;&#30028;&#32422;&#20026;$\approx 0.37$&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#24555;&#30340;BPE&#23454;&#29616;&#65292;&#23558;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;$\mathcal{O}\left(N M\right)$&#20248;&#21270;&#20026;$\mathcal{O}\left(N \log M\right)$&#65292;&#20854;&#20013;$N$&#26159;&#24207;&#21015;&#38271;&#24230;&#65292;$M$&#26159;&#21512;&#24182;&#27425;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#26292;&#21147;&#25628;&#32034;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in NLP, despite being devised initially as a compression method. BPE appears to be a greedy algorithm at face value, but the underlying optimization problem that BPE seeks to solve has not yet been laid down. We formalize BPE as a combinatorial optimization problem. Via submodular functions, we prove that the iterative greedy version is a $\frac{1}{{\sigma(\boldsymbol{\mu}^\star)}}(1-e^{-{\sigma(\boldsymbol{\mu}^\star)}})$-approximation of an optimal merge sequence, where ${\sigma(\boldsymbol{\mu}^\star)}$ is the total backward curvature with respect to the optimal merge sequence $\boldsymbol{\mu}^\star$. Empirically the lower bound of the approximation is $\approx 0.37$.  We provide a faster implementation of BPE which improves the runtime complexity from $\mathcal{O}\left(N M\right)$ to $\mathcal{O}\left(N \log M\right)$, where $N$ is the sequence length and $M$ is the merge count. Finally, we optimize the brute
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#24863;&#30693;&#23545;&#40784;&#26799;&#24230;&#65288;PAG&#65289;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#35270;&#35273;-&#35821;&#35328;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#23545; CLIP &#36827;&#34892;&#40065;&#26834;&#24615;&#35843;&#25972;&#65292;&#23637;&#31034;&#20102;&#22312;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#38598;&#25104; CLIPAG &#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26080;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.16805</link><description>&lt;p&gt;
CLIPAG: &#36208;&#21521;&#26080;&#38656;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CLIPAG: Towards Generator-Free Text-to-Image Generation. (arXiv:2306.16805v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#24863;&#30693;&#23545;&#40784;&#26799;&#24230;&#65288;PAG&#65289;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#35270;&#35273;-&#35821;&#35328;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#23545; CLIP &#36827;&#34892;&#40065;&#26834;&#24615;&#35843;&#25972;&#65292;&#23637;&#31034;&#20102;&#22312;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#38598;&#25104; CLIPAG &#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26080;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#23545;&#40784;&#26799;&#24230; (Perceptually Aligned Gradients, PAG) &#26159;&#22312;&#20581;&#22766;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#19968;&#31181;&#26377;&#36259;&#23646;&#24615;&#65292;&#20854;&#20013;&#23427;&#20204;&#30340;&#36755;&#20837;&#28176;&#21464;&#19982;&#20154;&#31867;&#24863;&#30693;&#23545;&#40784;&#24182;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#12290;&#34429;&#28982;&#36825;&#19968;&#29616;&#35937;&#24341;&#36215;&#20102;&#26174;&#30528;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#20294;&#20165;&#20165;&#22312;&#21333;&#27169;&#24577;&#32431;&#35270;&#35273;&#26550;&#26500;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558; PAG &#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#35270;&#35273;-&#35821;&#35328;&#26550;&#26500;&#65292;&#36825;&#26159;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20219;&#21153;&#21644;&#24212;&#29992;&#30340;&#22522;&#30784;&#12290;&#36890;&#36807;&#23545; CLIP &#36827;&#34892;&#23545;&#25239;&#24615;&#40065;&#26834;&#24494;&#35843;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#40065;&#26834;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20854;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102; PAG&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102; CLIPAG &#22312;&#20960;&#31181;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#32541;&#38598;&#25104; CLIPAG &#30340; "&#21363;&#25554;&#21363;&#29992;" &#26041;&#24335;&#26174;&#33879;&#25913;&#36827;&#20102;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#20854; PAG &#23646;&#24615;&#65292;CLIPAG &#23454;&#29616;&#20102;&#26080;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceptually Aligned Gradients (PAG) refer to an intriguing property observed in robust image classification models, wherein their input gradients align with human perception and pose semantic meanings. While this phenomenon has gained significant research attention, it was solely studied in the context of unimodal vision-only architectures. In this work, we extend the study of PAG to Vision-Language architectures, which form the foundations for diverse image-text tasks and applications. Through an adversarial robustification finetuning of CLIP, we demonstrate that robust Vision-Language models exhibit PAG in contrast to their vanilla counterparts. This work reveals the merits of CLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, we show that seamlessly integrating CLIPAG in a "plug-n-play" manner leads to substantial improvements in vision-language generative applications. Furthermore, leveraging its PAG property, CLIPAG enables text-to-image generation witho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35780;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26465;&#20214;&#29983;&#25104;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#29983;&#25104;&#36136;&#37327;&#30340;&#35780;&#20272;&#21644;&#19982;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#22330;&#26223;&#30340;&#20851;&#32852;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16793</link><description>&lt;p&gt;
&#27979;&#35780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26465;&#20214;&#29983;&#25104;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Model Capabilities for Conditional Generation. (arXiv:2306.16793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35780;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26465;&#20214;&#29983;&#25104;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#29983;&#25104;&#36136;&#37327;&#30340;&#35780;&#20272;&#21644;&#19982;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#22330;&#26223;&#30340;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (PLMs) &#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#22810;&#25968;&#26032;&#21457;&#23637;&#30340;&#22522;&#30784;&#12290;&#23427;&#20204;&#23558;&#35813;&#39046;&#22495;&#20174;&#24212;&#29992;&#29305;&#23450;&#30340;&#27169;&#22411;&#27969;&#31243;&#36716;&#21464;&#20026;&#19968;&#20010;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;&#19982;&#20998;&#31867;&#25110;&#22238;&#24402;&#19981;&#21516;&#65292;&#33258;&#22238;&#24402; PLMs&#65288;&#20363;&#22914;GPT-3&#25110;PaLM&#65289;&#20197;&#21450;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#25216;&#26415;&#23558;&#36755;&#20986;&#26041;&#24335;&#36827;&#19968;&#27493;&#36716;&#21464;&#20026;&#29983;&#25104;&#12290;&#23613;&#31649;&#23427;&#20204;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#22312;&#24341;&#20837;&#36825;&#20123;&#27169;&#22411;&#26102;&#24456;&#23569;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#36136;&#37327;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#29616;&#26377;&#30340;&#29983;&#25104;&#20219;&#21153;&#8212;&#8212;&#23613;&#31649;&#21487;&#20197;&#29992;&#20110;&#27604;&#36739;&#31995;&#32479;&#8212;&#8212;&#22914;&#20309;&#19982;&#20154;&#20204;&#37319;&#29992;&#23427;&#20204;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#22330;&#26223;&#30456;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#22914;&#20309;&#23558;&#29616;&#26377;&#30340;&#24212;&#29992;&#29305;&#23450;&#29983;&#25104;&#22522;&#20934;&#36866;&#24212;PLMs&#65292;&#24182;&#23545;PLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#21644;&#33021;&#21147;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#21253;&#25324;&#35268;&#27169;&#12289;&#26550;&#26500;&#31561;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (PLMs) underlie most new developments in natural language processing. They have shifted the field from application-specific model pipelines to a single model that is adapted to a wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside techniques like few-shot learning, have additionally shifted the output modality to generation instead of classification or regression. Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced. Additionally, it is unclear how existing generation tasks--while they can be used to compare systems at a high level--relate to the real world use cases for which people have been adopting them. In this work, we discuss how to adapt existing application-specific generation benchmarks to PLMs and provide an in-depth, empirical study of the limitations and capabilities of PLMs in natural language generation tasks along dimensions such as scale, archite
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23558;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20110;&#26410;&#35265;&#35821;&#35328;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36328;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#20803;&#23884;&#20837;&#23545;&#40784;&#25216;&#26415;&#65292;&#26080;&#38656;&#22270;&#20687;&#36755;&#20837;&#21644;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#12289;&#35270;&#35273;&#34164;&#28085;&#21644;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#30340;&#33391;&#22909;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16774</link><description>&lt;p&gt;
&#20572;&#27490;&#39044;&#35757;&#32451;&#65306;&#23558;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20110;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages. (arXiv:2306.16774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16774
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23558;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20110;&#26410;&#35265;&#35821;&#35328;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36328;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#20803;&#23884;&#20837;&#23545;&#40784;&#25216;&#26415;&#65292;&#26080;&#38656;&#22270;&#20687;&#36755;&#20837;&#21644;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#12289;&#35270;&#35273;&#34164;&#28085;&#21644;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#30340;&#33391;&#22909;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#24050;&#32463;&#25552;&#39640;&#20102;&#35768;&#22810;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#27604;&#22914;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#12289;&#35270;&#35273;&#34164;&#28085;&#21644;&#35270;&#35273;&#25512;&#29702;&#12290;&#39044;&#35757;&#32451;&#20027;&#35201;&#21033;&#29992;&#33521;&#35821;&#30340;&#35789;&#27719;&#25968;&#25454;&#24211;&#21644;&#22270;&#20687;&#26597;&#35810;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38646;&#23556;&#26410;&#35265;&#35821;&#35328;&#20013;&#65292;&#33521;&#35821;&#30340;&#39044;&#35757;&#32451;&#25928;&#26524;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(MPLM)&#22312;&#21508;&#31181;&#21333;&#27169;&#24577;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;MPLM&#23558;VLP&#36866;&#24212;&#20110;&#26410;&#35265;&#35821;&#35328;&#12290;&#25105;&#20204;&#21033;&#29992;&#36328;&#35821;&#35328;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#20803;&#23884;&#20837;&#23545;&#40784;&#26041;&#27861;&#26469;&#35757;&#32451;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22270;&#20687;&#36755;&#20837;&#65292;&#20027;&#35201;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#65292;&#28040;&#38500;&#20102;&#23545;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;(&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#12289;&#35270;&#35273;&#34164;&#28085;&#21644;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;)&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Pre-training (VLP) has advanced the performance of many vision-language tasks, such as image-text retrieval, visual entailment, and visual reasoning. The pre-training mostly utilizes lexical databases and image queries in English. Previous work has demonstrated that the pre-training in English does not transfer well to other languages in a zero-shot setting. However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks. In this paper, we propose a simple yet efficient approach to adapt VLP to unseen languages using MPLM. We utilize a cross-lingual contextualized token embeddings alignment approach to train text encoders for non-English languages. Our approach does not require image input and primarily uses machine translation, eliminating the need for target language data. Our evaluation across three distinct tasks (image-text retrieval, visual entailment, and natural language visual reasoning) demonstrates that this 
&lt;/p&gt;</description></item><item><title>DialoGPS&#26159;&#31532;&#19968;&#20010;&#22312;&#36830;&#32493;&#35821;&#20041;&#31354;&#38388;&#20013;&#36827;&#34892;&#23545;&#35805;&#36335;&#24452;&#37319;&#26679;&#30340;&#22810;&#23545;&#22810;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#30340;&#25968;&#25454;&#22686;&#24378;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.16770</link><description>&lt;p&gt;
DialoGPS: &#22312;&#36830;&#32493;&#35821;&#20041;&#31354;&#38388;&#20013;&#23545;&#35805;&#36335;&#24452;&#37319;&#26679;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations. (arXiv:2306.16770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16770
&lt;/p&gt;
&lt;p&gt;
DialoGPS&#26159;&#31532;&#19968;&#20010;&#22312;&#36830;&#32493;&#35821;&#20041;&#31354;&#38388;&#20013;&#36827;&#34892;&#23545;&#35805;&#36335;&#24452;&#37319;&#26679;&#30340;&#22810;&#23545;&#22810;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#30340;&#25968;&#25454;&#22686;&#24378;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#20013;&#30340;&#19978;&#19979;&#25991;&#21644;&#22238;&#22797;&#26159;&#19968;&#23545;&#19968;&#30340;&#26144;&#23556;&#65292;&#36829;&#21453;&#20102;&#37325;&#35201;&#30340;&#22810;&#23545;&#22810;&#29305;&#24615;&#65306;&#19978;&#19979;&#25991;&#26377;&#22810;&#31181;&#22238;&#22797;&#65292;&#22238;&#22797;&#22238;&#31572;&#22810;&#20010;&#19978;&#19979;&#25991;&#12290;&#32570;&#20047;&#36825;&#26679;&#30340;&#27169;&#24335;&#65292;&#27169;&#22411;&#24456;&#38590;&#27867;&#21270;&#24182;&#20542;&#21521;&#20110;&#23433;&#20840;&#22238;&#22797;&#12290;&#24050;&#32463;&#26377;&#35768;&#22810;&#23581;&#35797;&#20197;&#19968;&#23545;&#22810;&#30340;&#35282;&#24230;&#22788;&#29702;&#22810;&#36718;&#23545;&#35805;&#25110;&#20197;&#22810;&#23545;&#22810;&#30340;&#35282;&#24230;&#22788;&#29702;&#21333;&#36718;&#23545;&#35805;&#65292;&#20294;&#23545;&#20110;&#22810;&#23545;&#22810;&#30340;&#22810;&#36718;&#23545;&#35805;&#30340;&#22686;&#24378;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DialoGPS&#26041;&#27861;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#30340;&#22810;&#23545;&#22810;&#22686;&#24378;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#23545;&#35805;&#26144;&#23556;&#21040;&#25105;&#20204;&#30340;&#25193;&#23637;&#24067;&#26391;&#26725;&#65288;Brownian Bridge&#65289;&#65292;&#19968;&#20010;&#29305;&#27530;&#30340;&#39640;&#26031;&#36807;&#31243;&#12290;&#25105;&#20204;&#37319;&#26679;&#28508;&#21464;&#37327;&#20197;&#24418;&#25104;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#36830;&#36143;&#23545;&#35805;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
In open-domain dialogue generation tasks, contexts and responses in most datasets are one-to-one mapped, violating an important many-to-many characteristic: a context leads to various responses, and a response answers multiple contexts. Without such patterns, models poorly generalize and prefer responding safely. Many attempts have been made in either multi-turn settings from a one-to-many perspective or in a many-to-many perspective but limited to single-turn settings. The major challenge to many-to-many augment multi-turn dialogues is that discretely replacing each turn with semantic similarity breaks fragile context coherence. In this paper, we propose DialoGue Path Sampling (DialoGPS) method in continuous semantic space, the first many-to-many augmentation method for multi-turn dialogues. Specifically, we map a dialogue to our extended Brownian Bridge, a special Gaussian process. We sample latent variables to form coherent dialogue paths in the continuous space. A dialogue path cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#21363;&#23558;&#22270;&#20687;&#21644;&#34920;&#26684;&#36716;&#21270;&#20026;&#32479;&#19968;&#30340;&#35821;&#35328;&#34920;&#31034;&#65292;&#36890;&#36807;&#26816;&#32034;&#12289;&#25490;&#24207;&#21644;&#29983;&#25104;&#19977;&#20010;&#27493;&#39588;&#35299;&#20915;&#25991;&#26412;&#38382;&#31572;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;WebQA&#27036;&#21333;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.16762</link><description>&lt;p&gt;
&#25991;&#26412;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#30340;&#32479;&#19968;&#35821;&#35328;&#34920;&#31034;&#22312;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Unified Language Representation for Question Answering over Text, Tables, and Images. (arXiv:2306.16762v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#21363;&#23558;&#22270;&#20687;&#21644;&#34920;&#26684;&#36716;&#21270;&#20026;&#32479;&#19968;&#30340;&#35821;&#35328;&#34920;&#31034;&#65292;&#36890;&#36807;&#26816;&#32034;&#12289;&#25490;&#24207;&#21644;&#29983;&#25104;&#19977;&#20010;&#27493;&#39588;&#35299;&#20915;&#25991;&#26412;&#38382;&#31572;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;WebQA&#27036;&#21333;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35797;&#22270;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#26102;&#65292;&#20154;&#20204;&#32463;&#24120;&#20381;&#36182;&#20110;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#22914;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#34920;&#26684;&#25968;&#25454;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#22810;&#27169;&#24577;&#31354;&#38388;&#30340;&#36755;&#20837;&#29305;&#24449;&#25110;&#27169;&#22411;&#32467;&#26500;&#65292;&#36825;&#23545;&#20110;&#36328;&#27169;&#24577;&#25512;&#29702;&#25110;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;&#26469;&#35828;&#26159;&#19981;&#28789;&#27963;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#23558;&#22270;&#20687;&#21644;&#34920;&#26684;&#36716;&#21270;&#20026;&#32479;&#19968;&#30340;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#23558;&#20219;&#21153;&#31616;&#21270;&#20026;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#25991;&#26412;&#38382;&#31572;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#19977;&#20010;&#27493;&#39588;&#35299;&#20915;&#65306;&#26816;&#32034;&#12289;&#25490;&#24207;&#21644;&#29983;&#25104;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#22312;&#35821;&#35328;&#31354;&#38388;&#20869;&#36827;&#34892;&#12290;&#36825;&#20010;&#24819;&#27861;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#19968;&#20010;&#21517;&#20026;Solar&#30340;&#26694;&#26550;&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;Solar&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;MultimodalQA&#21644;MMCoQA&#19978;&#30456;&#23545;&#20110;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#30340;&#25351;&#26631;&#25552;&#39640;&#20102;10.6-32.3&#20010;&#30334;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;Solar&#22312;WebQA&#27036;&#21333;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
When trying to answer complex questions, people often rely on multiple sources of information, such as visual, textual, and tabular data. Previous approaches to this problem have focused on designing input features or model structure in the multi-modal space, which is inflexible for cross-modal reasoning or data-efficient training. In this paper, we call for an alternative paradigm, which transforms the images and tables into unified language representations, so that we can simplify the task into a simpler textual QA problem that can be solved using three steps: retrieval, ranking, and generation, all within a language space. This idea takes advantage of the power of pre-trained language models and is implemented in a framework called Solar. Our experimental results show that Solar outperforms all existing methods by 10.6-32.3 pts on two datasets, MultimodalQA and MMCoQA, across ten different metrics. Additionally, Solar achieves the best performance on the WebQA leaderboard
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PaRTE&#65292;&#19968;&#20010;&#21253;&#21547;1,126&#23545;&#25991;&#26412;&#34164;&#28085;&#31034;&#20363;&#30340;&#38598;&#21512;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#25913;&#20889;&#21477;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#20195;&#27169;&#22411;&#22312;8-16&#65285;&#30340;&#25913;&#20889;&#31034;&#20363;&#19978;&#25913;&#21464;&#20102;&#20182;&#20204;&#30340;&#39044;&#27979;&#65292;&#35828;&#26126;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.16722</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#34164;&#28085;&#27169;&#22411;&#23545;&#25913;&#20889;&#21477;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Paraphrastic Robustness in Textual Entailment Models. (arXiv:2306.16722v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PaRTE&#65292;&#19968;&#20010;&#21253;&#21547;1,126&#23545;&#25991;&#26412;&#34164;&#28085;&#31034;&#20363;&#30340;&#38598;&#21512;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#25913;&#20889;&#21477;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#20195;&#27169;&#22411;&#22312;8-16&#65285;&#30340;&#25913;&#20889;&#31034;&#20363;&#19978;&#25913;&#21464;&#20102;&#20182;&#20204;&#30340;&#39044;&#27979;&#65292;&#35828;&#26126;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PaRTE&#65292;&#19968;&#20010;&#21253;&#21547;1,126&#23545;&#25991;&#26412;&#34164;&#28085;&#65288;RTE&#65289;&#31034;&#20363;&#30340;&#38598;&#21512;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#25913;&#20889;&#21477;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;RTE&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#35821;&#35328;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#24212;&#35813;&#22312;&#20855;&#26377;&#30456;&#21516;&#21547;&#20041;&#30340;&#36755;&#20837;&#19978;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#20351;&#29992;&#35780;&#20272;&#38598;&#26469;&#30830;&#23450;&#24403;&#31034;&#20363;&#34987;&#25913;&#20889;&#26102;&#65292;RTE&#27169;&#22411;&#30340;&#39044;&#27979;&#26159;&#21542;&#21457;&#29983;&#21464;&#21270;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#29616;&#20195;&#27169;&#22411;&#22312;8-16&#65285;&#30340;&#25913;&#20889;&#31034;&#20363;&#19978;&#25913;&#21464;&#20102;&#20182;&#20204;&#30340;&#39044;&#27979;&#65292;&#36825;&#35828;&#26126;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present PaRTE, a collection of 1,126 pairs of Recognizing Textual Entailment (RTE) examples to evaluate whether models are robust to paraphrasing. We posit that if RTE models understand language, their predictions should be consistent across inputs that share the same meaning. We use the evaluation set to determine if RTE models' predictions change when examples are paraphrased. In our experiments, contemporary models change their predictions on 8-16\% of paraphrased examples, indicating that there is still room for improvement.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#20004;&#31181;&#26368;&#26032;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#38750;&#27597;&#35821;&#20799;&#31461;&#35821;&#38899;&#23398;&#20064;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#25552;&#20379;&#23545;&#20799;&#31461;&#21457;&#38899;&#21644;&#27969;&#21033;&#24615;&#30340;&#21453;&#39304;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16710</link><description>&lt;p&gt;
&#38754;&#21521;&#35821;&#35328;&#23398;&#20064;&#24212;&#29992;&#30340;&#38750;&#27597;&#35821;&#20799;&#31461;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition of Non-Native Child Speech for Language Learning Applications. (arXiv:2306.16710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#20004;&#31181;&#26368;&#26032;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#38750;&#27597;&#35821;&#20799;&#31461;&#35821;&#38899;&#23398;&#20064;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#25552;&#20379;&#23545;&#20799;&#31461;&#21457;&#38899;&#21644;&#27969;&#21033;&#24615;&#30340;&#21453;&#39304;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#26426;&#22120;&#20154;&#22312;&#25903;&#25345;&#35821;&#35328;&#25216;&#33021;&#21457;&#23637;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#36884;&#24452;&#65292;&#23588;&#20854;&#26159;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899;&#26426;&#22120;&#20154;&#20027;&#35201;&#38754;&#21521;&#27597;&#35821;&#25104;&#24180;&#20154;&#12290;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;ASR&#31995;&#32479;&#65288;Wav2Vec2.0&#21644;Whisper AI&#65289;&#30340;&#24615;&#33021;&#65292;&#20197;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#25903;&#25345;&#20799;&#31461;&#20064;&#24471;&#22806;&#35821;&#30340;&#35821;&#38899;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#27597;&#35821;&#21644;&#38750;&#27597;&#35821;&#33655;&#20848;&#20799;&#31461;&#30340;&#26391;&#35835;&#21644;&#21363;&#20852;&#28436;&#35762;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;ASR&#25216;&#26415;&#25552;&#20379;&#23545;&#20799;&#31461;&#21457;&#38899;&#21644;&#27969;&#21033;&#24615;&#30340;&#27934;&#23519;&#30340;&#23454;&#29992;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#26032;&#30340;&#39044;&#35757;&#32451;ASR&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#21487;&#25509;&#21463;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25552;&#21462;&#26377;&#20851;&#38899;&#32032;&#21457;&#38899;&#36136;&#37327;&#30340;&#35814;&#32454;&#21453;&#39304;&#65292;&#23613;&#31649;&#20799;&#31461;&#21644;&#38750;&#27597;&#35821;&#30340;&#35821;&#38899;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voicebots have provided a new avenue for supporting the development of language skills, particularly within the context of second language learning. Voicebots, though, have largely been geared towards native adult speakers. We sought to assess the performance of two state-of-the-art ASR systems, Wav2Vec2.0 and Whisper AI, with a view to developing a voicebot that can support children acquiring a foreign language. We evaluated their performance on read and extemporaneous speech of native and non-native Dutch children. We also investigated the utility of using ASR technology to provide insight into the children's pronunciation and fluency. The results show that recent, pre-trained ASR transformer-based models achieve acceptable performance from which detailed feedback on phoneme pronunciation quality can be extracted, despite the challenging nature of child and non-native speech.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#35821;&#20041;&#22270;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#29983;&#25104;&#26041;&#26696;&#65288;TEAM&#65289;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#25552;&#21462;&#23545;&#35937;&#32423;&#35821;&#20041;&#20803;&#25968;&#25454;&#21644;&#24341;&#20837;&#22806;&#37096;&#30456;&#20851;&#30693;&#35782;&#27010;&#24565;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#35270;&#35273;&#29305;&#24449;&#19982;&#35299;&#30721;&#22120;&#35821;&#20041;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#20197;&#21450;&#28508;&#22312;&#30340;&#22806;&#37096;&#30693;&#35782;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.16650</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#28304;&#35821;&#20041;&#22270;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multi-source Semantic Graph-based Multimodal Sarcasm Explanation Generation. (arXiv:2306.16650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#35821;&#20041;&#22270;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#29983;&#25104;&#26041;&#26696;&#65288;TEAM&#65289;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#25552;&#21462;&#23545;&#35937;&#32423;&#35821;&#20041;&#20803;&#25968;&#25454;&#21644;&#24341;&#20837;&#22806;&#37096;&#30456;&#20851;&#30693;&#35782;&#27010;&#24565;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#35270;&#35273;&#29305;&#24449;&#19982;&#35299;&#30721;&#22120;&#35821;&#20041;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#20197;&#21450;&#28508;&#22312;&#30340;&#22806;&#37096;&#30693;&#35782;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#65288;MuSE&#65289;&#26159;&#19968;&#20010;&#26032;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#22810;&#27169;&#24577;&#31038;&#20132;&#24086;&#23376;&#65288;&#21253;&#25324;&#22270;&#20687;&#21644;&#20854;&#26631;&#39064;&#65289;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#65292;&#35299;&#37322;&#20026;&#20160;&#20040;&#23427;&#21253;&#21547;&#35773;&#21050;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20808;&#39537;&#30740;&#31350;&#22312;&#20351;&#29992;BART&#26694;&#26550;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#24573;&#35270;&#20102;&#22270;&#20687;&#30340;&#23545;&#35937;&#32423;&#20803;&#25968;&#25454;&#19982;&#35299;&#30721;&#22120;&#35821;&#20041;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#21450;&#28508;&#22312;&#30340;&#22806;&#37096;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#28304;&#35821;&#20041;&#22270;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#26041;&#26696;&#65292;&#31216;&#20026;TEAM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TEAM&#25552;&#21462;&#20102;&#36755;&#20837;&#22270;&#20687;&#30340;&#23545;&#35937;&#32423;&#35821;&#20041;&#20803;&#25968;&#25454;&#32780;&#19981;&#26159;&#20256;&#32479;&#20840;&#23616;&#35270;&#35273;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;TEAM&#21033;&#29992;ConceptNet&#33719;&#21462;&#36755;&#20837;&#25991;&#26412;&#21644;&#25552;&#21462;&#30340;&#23545;&#35937;&#20803;&#25968;&#25454;&#30340;&#30456;&#20851;&#22806;&#37096;&#30693;&#35782;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;TEAM&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#28304;&#35821;&#20041;&#22270;&#65292;&#20840;&#38754;&#22320;&#21051;&#30011;&#20102;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sarcasm Explanation (MuSE) is a new yet challenging task, which aims to generate a natural language sentence for a multimodal social post (an image as well as its caption) to explain why it contains sarcasm. Although the existing pioneer study has achieved great success with the BART backbone, it overlooks the gap between the visual feature space and the decoder semantic space, the object-level metadata of the image, as well as the potential external knowledge. To solve these limitations, in this work, we propose a novel mulTi-source sEmantic grAph-based Multimodal sarcasm explanation scheme, named TEAM. In particular, TEAM extracts the object-level semantic meta-data instead of the traditional global visual features from the input image. Meanwhile, TEAM resorts to ConceptNet to obtain the external related knowledge concepts for the input text and the extracted object meta-data. Thereafter, TEAM introduces a multi-source semantic graph that comprehensively characterize the m
&lt;/p&gt;</description></item><item><title>ZeroGen&#26159;&#19968;&#31181;&#38646;&#23556;&#20987;&#30340;&#22810;&#27169;&#24577;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#21495;&#30340;&#25511;&#21046;&#65292;&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;&#32479;&#19968;&#30340;&#27010;&#29575;&#31354;&#38388;&#24182;&#36890;&#36807;&#21152;&#26435;&#28155;&#21152;&#33258;&#23450;&#20041;LM&#36755;&#20986;&#23454;&#29616;&#39640;&#25928;&#29575;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#26469;&#33258;&#19981;&#21516;&#27169;&#24335;&#20449;&#21495;&#20043;&#38388;&#30340;&#28145;&#24230;&#19982;&#23485;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.16649</link><description>&lt;p&gt;
ZeroGen: &#38646;&#23556;&#20987;&#22810;&#27169;&#24577;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#19982;&#22810;&#20010;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
ZeroGen: Zero-shot Multimodal Controllable Text Generation with Multiple Oracles. (arXiv:2306.16649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16649
&lt;/p&gt;
&lt;p&gt;
ZeroGen&#26159;&#19968;&#31181;&#38646;&#23556;&#20987;&#30340;&#22810;&#27169;&#24577;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#21495;&#30340;&#25511;&#21046;&#65292;&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;&#32479;&#19968;&#30340;&#27010;&#29575;&#31354;&#38388;&#24182;&#36890;&#36807;&#21152;&#26435;&#28155;&#21152;&#33258;&#23450;&#20041;LM&#36755;&#20986;&#23454;&#29616;&#39640;&#25928;&#29575;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#26469;&#33258;&#19981;&#21516;&#27169;&#24335;&#20449;&#21495;&#20043;&#38388;&#30340;&#28145;&#24230;&#19982;&#23485;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#29983;&#25104;&#24102;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#25991;&#26412;&#20869;&#23481;&#26159;&#19968;&#20010;&#38596;&#24515;&#21187;&#21187;&#30340;&#20219;&#21153;&#65292;&#20154;&#20204;&#19968;&#30452;&#22312;&#36861;&#27714;&#36825;&#19968;&#30446;&#26631;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#22312;&#23558;&#21333;&#27169;&#24577;&#25511;&#21046;&#24341;&#20837;&#35821;&#35328;&#27169;&#22411;(LMs)&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#31995;&#21015;&#36827;&#23637;&#65292;&#28982;&#32780;&#22914;&#20309;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#21495;&#21644;&#39640;&#25928;&#22320;&#29983;&#25104;&#21487;&#25511;&#21477;&#23376;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#23556;&#20987;&#22810;&#27169;&#24577;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#33539;&#24335;(ZeroGen)&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ZeroGen&#20174;&#20196;&#29260;&#32423;&#21035;&#21040;&#21477;&#23376;&#32423;&#21035;&#36830;&#32493;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25511;&#21046;&#65292;&#24182;&#22312;&#35299;&#30721;&#26102;&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;&#32479;&#19968;&#30340;&#27010;&#29575;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#21152;&#26435;&#28155;&#21152;&#33258;&#23450;&#20041;LM&#36755;&#20986;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#36328;&#27169;&#24577;&#26435;&#34913;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#21160;&#24577;&#21152;&#26435;&#26426;&#21046;&#26469;&#35843;&#33410;&#25152;&#26377;&#25511;&#21046;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#26469;&#33258;&#19981;&#21516;&#27169;&#24335;&#20449;&#21495;&#20043;&#38388;&#30340;&#28145;&#24230;&#19982;&#23485;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generating textual content with desired attributes is an ambitious task that people have pursued long. Existing works have made a series of progress in incorporating unimodal controls into language models (LMs), whereas how to generate controllable sentences with multimodal signals and high efficiency remains an open question. To tackle the puzzle, we propose a new paradigm of zero-shot controllable text generation with multimodal signals (\textsc{ZeroGen}). Specifically, \textsc{ZeroGen} leverages controls of text and image successively from token-level to sentence-level and maps them into a unified probability space at decoding, which customizes the LM outputs by weighted addition without extra training. To achieve better inter-modal trade-offs, we further introduce an effective dynamic weighting mechanism to regulate all control weights. Moreover, we conduct substantial experiments to probe the relationship of being in-depth or in-width between signals from distinct mo
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#26631;&#35760;&#32423;&#25991;&#26412;&#22686;&#24378;&#30340;&#26377;&#25928;&#24615;&#21644;&#27010;&#29575;&#35821;&#35328;&#30693;&#35782;&#30340;&#20316;&#29992;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#30740;&#31350;&#30340;&#20116;&#31181;&#26631;&#35760;&#32423;&#25991;&#26412;&#22686;&#24378;&#25216;&#26415;&#22312;&#35821;&#35328;&#35780;&#20272;&#29615;&#22659;&#19979;&#19981;&#20855;&#22791;&#26222;&#36941;&#26377;&#25928;&#24615;&#65292;&#32780;&#19988;&#19982;&#19981;&#21516;&#20998;&#31867;&#27169;&#22411;&#31867;&#22411;&#26080;&#20851;&#12290;</title><link>http://arxiv.org/abs/2306.16644</link><description>&lt;p&gt;
&#27010;&#29575;&#35821;&#35328;&#30693;&#35782;&#19982;&#26631;&#35760;&#32423;&#25991;&#26412;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Linguistic Knowledge and Token-level Text Augmentation. (arXiv:2306.16644v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16644
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#26631;&#35760;&#32423;&#25991;&#26412;&#22686;&#24378;&#30340;&#26377;&#25928;&#24615;&#21644;&#27010;&#29575;&#35821;&#35328;&#30693;&#35782;&#30340;&#20316;&#29992;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#30740;&#31350;&#30340;&#20116;&#31181;&#26631;&#35760;&#32423;&#25991;&#26412;&#22686;&#24378;&#25216;&#26415;&#22312;&#35821;&#35328;&#35780;&#20272;&#29615;&#22659;&#19979;&#19981;&#20855;&#22791;&#26222;&#36941;&#26377;&#25928;&#24615;&#65292;&#32780;&#19988;&#19982;&#19981;&#21516;&#20998;&#31867;&#27169;&#22411;&#31867;&#22411;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#23398;&#39537;&#21160;&#30340;&#35780;&#20272;&#29615;&#22659;&#19979;&#65292;&#26631;&#35760;&#32423;&#25991;&#26412;&#22686;&#24378;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#27010;&#29575;&#35821;&#35328;&#30693;&#35782;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#25991;&#26412;&#22686;&#24378;&#31243;&#24207;REDA&#21644;REDA$_{NG}$&#65292;&#23427;&#20204;&#37117;&#23454;&#29616;&#20102;&#20116;&#31181;&#26631;&#35760;&#32423;&#25991;&#26412;&#32534;&#36753;&#25805;&#20316;&#65306;&#21516;&#20041;&#35789;&#26367;&#25442;(SR)&#12289;&#38543;&#26426;&#20132;&#25442;(RS)&#12289;&#38543;&#26426;&#25554;&#20837;(RI)&#12289;&#38543;&#26426;&#21024;&#38500;(RD)&#21644;&#38543;&#26426;&#28151;&#21512;(RM)&#12290;REDA$_{NG}$&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;n-gram&#35821;&#35328;&#27169;&#22411;&#20174;REDA&#30340;&#36755;&#20986;&#20013;&#36873;&#25321;&#26368;&#21487;&#33021;&#30340;&#22686;&#24378;&#25991;&#26412;&#12290;&#25105;&#20204;&#23545;&#20013;&#25991;&#21644;&#33521;&#25991;&#30340;&#20108;&#20803;&#38382;&#39064;&#21305;&#37197;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#24378;&#28872;&#21542;&#23450;&#20102;&#25152;&#30740;&#31350;&#30340;&#20116;&#31181;&#26631;&#35760;&#32423;&#25991;&#26412;&#22686;&#24378;&#25216;&#26415;&#30340;&#26222;&#36941;&#26377;&#25928;&#24615;&#65292;&#26080;&#35770;&#26159;&#21516;&#26102;&#24212;&#29992;&#36824;&#26159;&#20998;&#21035;&#24212;&#29992;&#65292;&#20063;&#26080;&#35770;&#20351;&#29992;&#20102;&#21738;&#31181;&#24120;&#35265;&#30340;&#20998;&#31867;&#27169;&#22411;&#31867;&#22411;&#65292;&#21253;&#25324;transformers&#12290;&#27492;&#22806;&#65292;&#27010;&#29575;&#35821;&#35328;&#30693;&#35782;&#30340;&#20316;&#29992;&#26159;...
&lt;/p&gt;
&lt;p&gt;
This paper investigates the effectiveness of token-level text augmentation and the role of probabilistic linguistic knowledge within a linguistically-motivated evaluation context. Two text augmentation programs, REDA and REDA$_{NG}$, were developed, both implementing five token-level text editing operations: Synonym Replacement (SR), Random Swap (RS), Random Insertion (RI), Random Deletion (RD), and Random Mix (RM). REDA$_{NG}$ leverages pretrained $n$-gram language models to select the most likely augmented texts from REDA's output. Comprehensive and fine-grained experiments were conducted on a binary question matching classification task in both Chinese and English. The results strongly refute the general effectiveness of the five token-level text augmentation techniques under investigation, whether applied together or separately, and irrespective of various common classification model types used, including transformers. Furthermore, the role of probabilistic linguistic knowledge is 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-2&#12289;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#21542;&#23450;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;GPT-4&#34920;&#29616;&#26368;&#20248;&#65292;&#32780;GPT-3.5&#34920;&#29616;&#19979;&#38477;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;GPT&#27169;&#22411;&#22312;&#21542;&#23450;&#26816;&#27979;&#26041;&#38754;&#30340;&#33021;&#21147;&#30456;&#23545;&#26377;&#38480;&#65292;&#36825;&#34920;&#26126;&#35813;&#20219;&#21153;&#25361;&#25112;&#20102;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#19981;&#20165;&#31361;&#26174;&#20102;GPT&#27169;&#22411;&#22312;&#22788;&#29702;&#21542;&#23450;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#36824;&#24378;&#35843;&#20102;&#36923;&#36753;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16638</link><description>&lt;p&gt;
GPT&#27169;&#22411;&#22312;&#21542;&#23450;&#26816;&#27979;&#26041;&#38754;&#30340;&#35780;&#20272;&#65306;&#20197;xNot360&#25968;&#25454;&#38598;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A negation detection assessment of GPTs: analysis with the xNot360 dataset. (arXiv:2306.16638v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-2&#12289;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#21542;&#23450;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;GPT-4&#34920;&#29616;&#26368;&#20248;&#65292;&#32780;GPT-3.5&#34920;&#29616;&#19979;&#38477;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;GPT&#27169;&#22411;&#22312;&#21542;&#23450;&#26816;&#27979;&#26041;&#38754;&#30340;&#33021;&#21147;&#30456;&#23545;&#26377;&#38480;&#65292;&#36825;&#34920;&#26126;&#35813;&#20219;&#21153;&#25361;&#25112;&#20102;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#19981;&#20165;&#31361;&#26174;&#20102;GPT&#27169;&#22411;&#22312;&#22788;&#29702;&#21542;&#23450;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#36824;&#24378;&#35843;&#20102;&#36923;&#36753;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21542;&#23450;&#26159;&#33258;&#28982;&#35821;&#35328;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#22312;&#20132;&#27969;&#21644;&#29702;&#35299;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Generative Pre-trained Transformer&#65288;GPT&#65289;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;GPT-2&#12289;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#65289;&#22312;&#21542;&#23450;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#38646;&#26679;&#26412;&#39044;&#27979;&#26041;&#27861;&#22312;&#25105;&#20204;&#33258;&#23450;&#20041;&#30340;xNot360&#25968;&#25454;&#38598;&#19978;&#35782;&#21035;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#21542;&#23450;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#23519;&#20102;&#26631;&#35760;&#20026;&#31532;&#20108;&#20010;&#21477;&#23376;&#26159;&#21542;&#21542;&#23450;&#20102;&#31532;&#19968;&#20010;&#21477;&#23376;&#30340;&#21477;&#23376;&#23545;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;GPT&#27169;&#22411;&#20043;&#38388;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#20854;&#20013;GPT-4&#34920;&#29616;&#20248;&#24322;&#65292;&#32780;GPT-3.5&#21017;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;GPT&#27169;&#22411;&#22312;&#21542;&#23450;&#26816;&#27979;&#26041;&#38754;&#25972;&#20307;&#30340;&#33021;&#21147;&#30456;&#23545;&#26377;&#38480;&#65292;&#34920;&#26126;&#36825;&#19968;&#20219;&#21153;&#25361;&#25112;&#20102;&#23427;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#19981;&#20165;&#31361;&#26174;&#20102;GPT&#27169;&#22411;&#22312;&#22788;&#29702;&#21542;&#23450;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#32780;&#19988;&#24378;&#35843;&#20102;&#36923;&#36753;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Negation is a fundamental aspect of natural language, playing a critical role in communication and comprehension. Our study assesses the negation detection performance of Generative Pre-trained Transformer (GPT) models, specifically GPT-2, GPT-3, GPT-3.5, and GPT-4. We focus on the identification of negation in natural language using a zero-shot prediction approach applied to our custom xNot360 dataset. Our approach examines sentence pairs labeled to indicate whether the second sentence negates the first. Our findings expose a considerable performance disparity among the GPT models, with GPT-4 surpassing its counterparts and GPT-3.5 displaying a marked performance reduction. The overall proficiency of the GPT models in negation detection remains relatively modest, indicating that this task pushes the boundaries of their natural language understanding capabilities. We not only highlight the constraints of GPT models in handling negation but also emphasize the importance of logical relia
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20013;&#22269;&#23567;&#23398;&#25968;&#23398;&#24212;&#29992;&#39064;&#65288;CMATH&#65289;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23567;&#23398;&#25968;&#23398;&#19981;&#21516;&#24180;&#32423;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#21482;&#26377;GPT-4&#22312;&#25152;&#26377;&#24180;&#32423;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#22312;&#19981;&#21516;&#24180;&#32423;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.16636</link><description>&lt;p&gt;
CMATH&#65306;&#20320;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#36890;&#36807;&#20013;&#22269;&#23567;&#23398;&#25968;&#23398;&#27979;&#35797;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?. (arXiv:2306.16636v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16636
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20013;&#22269;&#23567;&#23398;&#25968;&#23398;&#24212;&#29992;&#39064;&#65288;CMATH&#65289;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23567;&#23398;&#25968;&#23398;&#19981;&#21516;&#24180;&#32423;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#21482;&#26377;GPT-4&#22312;&#25152;&#26377;&#24180;&#32423;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#22312;&#19981;&#21516;&#24180;&#32423;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20013;&#22269;&#23567;&#23398;&#25968;&#23398;&#24212;&#29992;&#39064;&#65288;CMATH&#65289;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;1.7k&#20010;&#20855;&#26377;&#35814;&#32454;&#27880;&#37322;&#30340;&#23567;&#23398;&#27700;&#24179;&#25968;&#23398;&#24212;&#29992;&#39064;&#65292;&#26469;&#28304;&#20110;&#20013;&#22269;&#23454;&#38469;&#30340;&#32451;&#20064;&#21644;&#32771;&#35797;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#35780;&#20272;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#36798;&#21040;&#23567;&#23398;&#25968;&#23398;&#21738;&#20010;&#24180;&#32423;&#27700;&#24179;&#30340;&#22522;&#20934;&#24037;&#20855;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;LLMs&#65292;&#21253;&#25324;&#21830;&#19994;&#21644;&#24320;&#28304;&#36873;&#39033;&#65292;&#24182;&#21457;&#29616;&#21482;&#26377;GPT-4&#22312;&#25152;&#26377;&#20845;&#20010;&#23567;&#23398;&#24180;&#32423;&#20013;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65288;&#20934;&#30830;&#29575;&#8805;60%&#65289;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#22312;&#19981;&#21516;&#24180;&#32423;&#19978;&#30340;&#34920;&#29616;&#27424;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#24178;&#25200;&#20449;&#24687;&#26469;&#35780;&#20272;&#20960;&#20010;&#34920;&#29616;&#26368;&#20339;&#30340;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;GPT-4&#33021;&#22815;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#22833;&#36133;&#12290;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#30740;&#31350;&#23558;&#25581;&#31034;LLMs&#22312;&#31639;&#26415;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Chinese Elementary School Math Word Problems (CMATH) dataset, comprising 1.7k elementary school-level math word problems with detailed annotations, source from actual Chinese workbooks and exams. This dataset aims to provide a benchmark tool for assessing the following question: to what grade level of elementary school math do the abilities of popular large language models (LLMs) correspond? We evaluate a variety of popular LLMs, including both commercial and open-source options, and discover that only GPT-4 achieves success (accuracy $\geq$ 60\%) across all six elementary school grades, while other models falter at different grade levels. Furthermore, we assess the robustness of several top-performing LLMs by augmenting the original problems in the CMATH dataset with distracting information. Our findings reveal that GPT-4 is able to maintains robustness, while other model fail. We anticipate that our study will expose limitations in LLMs' arithmetic and reasoning capabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#31232;&#30095;&#25512;&#26029;&#36719;&#20214;&#21152;&#36895;&#22120;&#65292;&#22312;CPU&#19978;&#21033;&#29992;Intel Deep Learning Boost&#23454;&#29616;&#20102;&#31232;&#30095;&#30697;&#38453;-&#31264;&#23494;&#30697;&#38453;&#20056;&#27861;&#30340;&#20248;&#21270;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#31232;&#30095;&#24211;&#65292;&#22312;&#21508;&#31181;&#24418;&#29366;&#21644;&#31232;&#30095;&#24230;&#19979;&#37117;&#33719;&#24471;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.16601</link><description>&lt;p&gt;
&#29992;&#20110;CPU&#19978;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#31232;&#30095;&#25512;&#26029;&#36719;&#20214;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs. (arXiv:2306.16601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#31232;&#30095;&#25512;&#26029;&#36719;&#20214;&#21152;&#36895;&#22120;&#65292;&#22312;CPU&#19978;&#21033;&#29992;Intel Deep Learning Boost&#23454;&#29616;&#20102;&#31232;&#30095;&#30697;&#38453;-&#31264;&#23494;&#30697;&#38453;&#20056;&#27861;&#30340;&#20248;&#21270;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#31232;&#30095;&#24211;&#65292;&#22312;&#21508;&#31181;&#24418;&#29366;&#21644;&#31232;&#30095;&#24230;&#19979;&#37117;&#33719;&#24471;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#20005;&#26684;&#30340;&#21534;&#21520;&#37327;&#21644;&#24310;&#36831;&#35201;&#27714;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#37319;&#29992;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#31561;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#26469;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#36816;&#34892;&#26102;&#23545;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#32570;&#20047;&#20805;&#20998;&#30340;&#25903;&#25345;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#25512;&#26029;&#36719;&#20214;&#22534;&#26632;&#65292;&#29992;&#20110;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#26435;&#37325;&#20351;&#29992;&#24658;&#23450;&#30340;&#22359;&#22823;&#23567;&#36827;&#34892;&#21098;&#26525;&#12290;&#25105;&#20204;&#30340;&#31232;&#30095;&#36719;&#20214;&#21152;&#36895;&#22120;&#21033;&#29992;Intel Deep Learning Boost&#22312;CPU&#19978;&#26368;&#22823;&#21270;&#31232;&#30095;&#30697;&#38453;-&#31264;&#23494;&#30697;&#38453;&#20056;&#27861;&#65288;&#36890;&#24120;&#34987;&#32553;&#20889;&#20026;SpMM&#65289;&#30340;&#24615;&#33021;&#12290;&#22312;&#24191;&#27867;&#30340;GEMM&#24418;&#29366;&#21644;5&#20010;&#20195;&#34920;&#24615;&#31232;&#30095;&#24230;&#27700;&#24179;&#19979;&#65292;&#25105;&#20204;&#30340;SpMM&#20869;&#26680;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#31232;&#30095;&#24211;&#65288;oneMKL&#12289;TVM&#21644;LIBXSMM&#65289;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Transformer-based language models have become the standard approach for natural language processing tasks. However, stringent throughput and latency requirements in industrial applications are limiting their adoption. To mitigate the gap, model compression techniques such as structured pruning are being used to improve inference efficiency. However, most existing neural network inference runtimes lack adequate support for structured sparsity. In this paper, we propose an efficient sparse deep learning inference software stack for Transformer-based language models where the weights are pruned with constant block size. Our sparse software accelerator leverages Intel Deep Learning Boost to maximize the performance of sparse matrix - dense matrix multiplication (commonly abbreviated as SpMM) on CPUs. Our SpMM kernel outperforms the existing sparse libraries (oneMKL, TVM, and LIBXSMM) by an order of magnitude on a wide range of GEMM shapes under 5 representative sparsity ra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Pareto Optimal&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#29992;&#30340;&#32534;&#31243;&#30417;&#30563;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21709;&#24212;&#36827;&#34892;&#31995;&#32479;&#26657;&#20934;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21709;&#24212;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.16564</link><description>&lt;p&gt;
&#36890;&#36807;Pareto Optimal&#33258;&#30417;&#30563;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#26657;&#20934;&#21644;&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision. (arXiv:2306.16564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Pareto Optimal&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#29992;&#30340;&#32534;&#31243;&#30417;&#30563;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21709;&#24212;&#36827;&#34892;&#31995;&#32479;&#26657;&#20934;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21709;&#24212;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#32463;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;&#26159;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#22686;&#38271;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#31561;&#20851;&#38190;&#39046;&#22495;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;LLM&#21709;&#24212;&#30340;&#32622;&#20449;&#27700;&#24179;&#65292;&#23545;&#20110;&#33258;&#21160;&#26816;&#27979;&#38169;&#35823;&#24182;&#20419;&#36827;&#20154;&#26426;&#21327;&#20316;&#39564;&#35777;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#26657;&#20934;&#20449;&#21495;&#26469;&#28304;&#26159;&#19987;&#23478;&#25351;&#23450;&#30340;&#32534;&#31243;&#30417;&#30563;&#65292;&#36890;&#24120;&#20855;&#26377;&#36739;&#20302;&#30340;&#25104;&#26412;&#65292;&#20294;&#20063;&#26377;&#20854;&#33258;&#36523;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#22122;&#22768;&#21644;&#35206;&#30422;&#33539;&#22260;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;Pareto Optimal&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#21487;&#29992;&#30340;&#32534;&#31243;&#30417;&#30563;&#26469;&#31995;&#32479;&#22320;&#26657;&#20934;LLM&#21709;&#24212;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21709;&#24212;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;&#36825;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#35843;&#21644;&#27169;&#22411;&#26469;&#23454;&#29616;&#65292;&#23558;LLM&#36755;&#20986;&#19982;&#20854;&#20182;&#21487;&#29992;&#30340;&#30417;&#30563;&#26469;&#28304;&#30456;&#21327;&#35843;&#65292;&#23558;&#26356;&#19981;&#30830;&#23450;&#30340;&#21709;&#24212;&#20998;&#37197;&#26356;&#39640;&#30340;&#39118;&#38505;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the confidence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop verification. An important source of calibration signals stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain L
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.16533</link><description>&lt;p&gt;
ICSVR: &#22312;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30740;&#31350;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ICSVR: Investigating Compositional and Semantic Understanding in Video Retrieval Models. (arXiv:2306.16533v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16533
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#26816;&#32034;&#65288;VR&#65289;&#28041;&#21450;&#26681;&#25454;&#25991;&#26412;&#26631;&#39064;&#26816;&#32034;&#35270;&#39057;&#25968;&#25454;&#24211;&#20013;&#30340;&#30495;&#23454;&#35270;&#39057;&#65292;&#25110;&#21453;&#20043;&#20134;&#28982;&#12290;&#21512;&#25104;&#24615;&#30340;&#20004;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#23545;&#35937;&#21644;&#23646;&#24615;&#20197;&#21450;&#21160;&#20316;&#65292;&#20351;&#29992;&#27491;&#30830;&#30340;&#35821;&#20041;&#32852;&#32467;&#20197;&#24418;&#25104;&#27491;&#30830;&#30340;&#25991;&#26412;&#26597;&#35810;&#12290;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#65288;&#23545;&#35937;&#21644;&#23646;&#24615;&#12289;&#21160;&#20316;&#21644;&#35821;&#20041;&#65289;&#21508;&#33258;&#22312;&#24110;&#21161;&#21306;&#20998;&#35270;&#39057;&#21644;&#26816;&#32034;&#27491;&#30830;&#30340;&#30495;&#23454;&#35270;&#39057;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#22914;MSRVTT&#12289;MSVD&#21644;DIDEMO&#12290;&#35813;&#30740;&#31350;&#38024;&#23545;&#20004;&#31867;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#20102;&#65292;&#19968;&#31867;&#26159;&#22312;&#35270;&#39057;&#25991;&#26412;&#23545;&#19978;&#39044;&#35757;&#32451;&#24182;&#22312;&#19979;&#28216;&#35270;&#39057;&#26816;&#32034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#65288;&#20363;&#22914;&#65292;Frozen-in-Time&#12289;Violet&#12289;MCQ&#31561;&#65289;&#65292;&#21478;&#19968;&#31867;&#26159;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25991;&#26412;&#34920;&#31034;&#65288;&#22914;CLIP&#65289;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video retrieval (VR) involves retrieving the ground truth video from the video database given a text caption or vice-versa. The two important components of compositionality: objects \&amp; attributes and actions are joined using correct semantics to form a proper text query. These components (objects \&amp; attributes, actions and semantics) each play an important role to help distinguish among videos and retrieve the correct ground truth video. However, it is unclear what is the effect of these components on the video retrieval performance. We therefore, conduct a systematic study to evaluate the compositional and semantic understanding of video retrieval models on standard benchmarks such as MSRVTT, MSVD and DIDEMO. The study is performed on two categories of video retrieval models: (i) which are pre-trained on video-text pairs and fine-tuned on downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.) (ii) which adapt pre-trained image-text representations like CLIP for vid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20026;&#22806;&#37096;&#30693;&#35782;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#39044;&#35757;&#32451;&#30340;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#30340;&#33258;&#21160;&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;Precision@5&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#26816;&#32034;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.16478</link><description>&lt;p&gt;
&#20026;&#22806;&#37096;&#30693;&#35782;&#35270;&#35273;&#38382;&#31572;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#31264;&#23494;&#26816;&#32034;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pre-Training Multi-Modal Dense Retrievers for Outside-Knowledge Visual Question Answering. (arXiv:2306.16478v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20026;&#22806;&#37096;&#30693;&#35782;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#39044;&#35757;&#32451;&#30340;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#30340;&#33258;&#21160;&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;Precision@5&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#26816;&#32034;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#65292;&#20854;&#20013;&#35775;&#38382;&#22806;&#37096;&#30693;&#35782;&#23545;&#20110;&#22238;&#31572;&#38382;&#39064;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#20010;&#31867;&#21035;&#34987;&#31216;&#20026;&#22806;&#37096;&#30693;&#35782;&#35270;&#35273;&#38382;&#31572;&#65288;OK-VQA&#65289;&#12290;&#24320;&#21457;OK-VQA&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#26159;&#20026;&#32473;&#23450;&#30340;&#22810;&#27169;&#24577;&#26597;&#35810;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#12290;&#30446;&#21069;&#27492;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#30340;&#38750;&#23545;&#31216;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#20351;&#29992;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26597;&#35810;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#21333;&#27169;&#24577;&#25991;&#26723;&#32534;&#30721;&#22120;&#30340;&#26550;&#26500;&#12290;&#36825;&#26679;&#30340;&#26550;&#26500;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#23454;&#29616;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#35757;&#32451;OK-VQA&#20219;&#21153;&#30340;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#30340;&#33258;&#21160;&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#12290;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#38750;&#23545;&#31216;&#26550;&#26500;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;Precision@5&#25552;&#21319;&#20102;26.9%&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#26816;&#32034;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a category of visual question answering tasks, in which accessing external knowledge is necessary for answering the questions. This category is called outside-knowledge visual question answering (OK-VQA). A major step in developing OK-VQA systems is to retrieve relevant documents for the given multi-modal query. Current state-of-the-art asymmetric dense retrieval model for this task uses an architecture with a multi-modal query encoder and a uni-modal document encoder. Such an architecture requires a large amount of training data for effective performance. We propose an automatic data generation pipeline for pre-training passage retrieval models for OK-VQA tasks. The proposed approach leads to 26.9% Precision@5 improvements compared to the current state-of-the-art asymmetric architecture. Additionally, the proposed pre-training approach exhibits a good ability in zero-shot retrieval scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#22235;&#20010;&#39044;&#27979;&#23376;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#20316;&#20026;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#36755;&#20837;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16125</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#26694;&#26550;&#65306;MentalRiskES@IberLEF 2023
&lt;/p&gt;
&lt;p&gt;
A Framework for Identifying Depression on Social Media: MentalRiskES@IberLEF 2023. (arXiv:2306.16125v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16125
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#22235;&#20010;&#39044;&#27979;&#23376;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#20316;&#20026;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#36755;&#20837;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#21442;&#19982;IberLEF 2023&#30340;MentalRiskES&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#26681;&#25454;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#27963;&#21160;&#26469;&#39044;&#27979;&#20182;&#20204;&#21487;&#33021;&#24739;&#25233;&#37057;&#30151;&#30340;&#21487;&#33021;&#24615;&#12290;&#25968;&#25454;&#38598;&#30001;175&#20010;Telegram&#29992;&#25143;&#30340;&#23545;&#35805;&#32452;&#25104;&#65292;&#27599;&#20010;&#29992;&#25143;&#26681;&#25454;&#20182;&#20204;&#24739;&#30149;&#35777;&#25454;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#22235;&#20010;&#39044;&#27979;&#23376;&#20219;&#21153;&#65306;&#20108;&#20998;&#31867;&#12289;&#31616;&#21333;&#22238;&#24402;&#12289;&#22810;&#31867;&#21035;&#20998;&#31867;&#21644;&#22810;&#31867;&#21035;&#22238;&#24402;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#31867;&#21035;&#22238;&#24402;&#38382;&#39064;&#65292;&#28982;&#21518;&#23558;&#39044;&#27979;&#32467;&#26524;&#36716;&#25442;&#20026;&#36866;&#29992;&#20110;&#20854;&#20182;&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#24314;&#27169;&#26041;&#27861;&#30340;&#24615;&#33021;&#65306;&#23545;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21644;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#20316;&#20026;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#36755;&#20837;&#65292;&#21518;&#32773;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#22797;&#29616;&#25105;&#20204;&#32467;&#26524;&#30340;&#20195;&#30721;&#65306;https://github.com/simonsanvil/EarlyDep
&lt;/p&gt;
&lt;p&gt;
This paper describes our participation in the MentalRiskES task at IberLEF 2023. The task involved predicting the likelihood of an individual experiencing depression based on their social media activity. The dataset consisted of conversations from 175 Telegram users, each labeled according to their evidence of suffering from the disorder. We used a combination of traditional machine learning and deep learning techniques to solve four predictive subtasks: binary classification, simple regression, multiclass classification, and multiclass regression. We approached this by training a model to solve the multiclass regression case and then transforming the predictions to work for the other three subtasks. We compare the performance of two different modeling approaches: fine-tuning a BERT-based model and using sentence embeddings as inputs to a linear regressor, with the latter yielding better results. The code to reproduce our results can be found at: https://github.com/simonsanvil/EarlyDep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#21270;&#20026;&#26356;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.14122</link><description>&lt;p&gt;
&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#25552;&#21462;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#21644;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Prompt Distillation for Multimodal Named Entity and Multimodal Relation Extraction. (arXiv:2306.14122v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#21270;&#20026;&#26356;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MNER&#65289;&#21644;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;&#65288;MRE&#65289;&#38656;&#35201;&#22788;&#29702;&#22797;&#26434;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#22522;&#26412;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#25552;&#28860;&#20026;&#26356;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#31995;&#21015;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#28085;&#30422;&#22810;&#31890;&#24230;&#65288;&#21517;&#35789;&#12289;&#21477;&#23376;&#12289;&#22810;&#27169;&#24577;&#65289;&#21644;&#25968;&#25454;&#22686;&#24378;&#65288;&#26679;&#24335;&#12289;&#23454;&#20307;&#12289;&#22270;&#20687;&#65289;&#32500;&#24230;&#30340;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#65292;&#23637;&#31034;&#20102;&#20174;LLMs&#20013;&#24341;&#23548;&#27492;&#31867;&#25512;&#29702;&#33021;&#21147;&#30340;&#31034;&#20363;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#25552;&#31034;&#25552;&#21462;&#26041;&#27861;&#65292;&#20197;&#21560;&#25910;LLMs&#20013;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#22312;&#22788;&#29702;&#20165;&#25991;&#26412;&#36755;&#20837;&#26102;&#30340;&#23454;&#29992;&#24615;&#65292;&#32780;&#26080;&#38656;&#28155;&#21152;&#22270;&#20687;&#21644;&#38142;&#24335;&#24605;&#32500;&#30693;&#35782;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction (MRE) necessitate the fundamental reasoning capacity for intricate linguistic and multimodal comprehension. In this study, we explore distilling the reasoning ability of large language models (LLMs) into a more compact student model by generating a \textit{chain of thought} (CoT) -- a sequence of intermediate reasoning steps. Specifically, we commence by exemplifying the elicitation of such reasoning ability from LLMs through CoT prompts covering multi-grain (noun, sentence, multimodality) and data-augmentation (style, entity, image) dimensions. Subsequently, we present a novel conditional prompt distillation method to assimilate the commonsense reasoning ability from LLMs, thereby enhancing the utility of the student model in addressing text-only inputs without the requisite addition of image and CoT knowledge. Extensive experiments reveal that our approach attains state-of-the-art accuracy and manifests a p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#19978;&#30340;&#21464;&#25442;&#23545;LLMs&#30340;&#28789;&#25935;&#24230;&#25110;&#19981;&#21464;&#24615;&#65292;&#30452;&#25509;&#30417;&#25511;LLMs&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.13651</link><description>&lt;p&gt;
&#33258;&#24102;&#25968;&#25454;&#65281;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Bring Your Own Data! Self-Supervised Evaluation for Large Language Models. (arXiv:2306.13651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#19978;&#30340;&#21464;&#25442;&#23545;LLMs&#30340;&#28789;&#25935;&#24230;&#25110;&#19981;&#21464;&#24615;&#65292;&#30452;&#25509;&#30417;&#25511;LLMs&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#20197;&#21450;&#23427;&#20204;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#34892;&#20026;&#21464;&#24471;&#19981;&#21487;&#25110;&#32570;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#19978;&#30340;&#21464;&#25442;&#23545;LLMs&#30340;&#28789;&#25935;&#24230;&#25110;&#19981;&#21464;&#24615;&#65292;&#30452;&#25509;&#30417;&#25511;LLM&#22312;&#37326;&#22806;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#25110;&#22312;&#27169;&#22411;&#37096;&#32626;&#26399;&#38388;&#36827;&#34892;&#30340;&#27969;&#25968;&#25454;&#30340;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#35780;&#20272;LLMs&#30340;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of Large Language Models (LLMs) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. For example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations. To bypass these drawbacks, we propose a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text. Self-supervised evaluation can directly monitor LLM behavior on datasets collected in the wild or streamed during live model deployment. We demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, tox
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#20998;&#31867;&#27861;&#12289;&#20248;&#21270;&#31574;&#30053;&#21644;&#24615;&#33021;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.19915</link><description>&lt;p&gt;
&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation Approaches for Source Code Models: A Survey. (arXiv:2305.19915v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#20998;&#31867;&#27861;&#12289;&#20248;&#21270;&#31574;&#30053;&#21644;&#24615;&#33021;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#20195;&#30721;&#22312;&#35768;&#22810;&#20851;&#38190;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#20419;&#36827;&#20102;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20197;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#24182;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#21508;&#31181;&#33021;&#21147;&#65288;&#20363;&#22914;&#20581;&#22766;&#24615;&#21644;&#21487;&#27867;&#21270;&#24615;&#65289;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#24182;&#38024;&#23545;&#28304;&#20195;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;DA&#26041;&#27861;&#30340;&#35843;&#25972;&#65292;&#20294;&#32570;&#20047;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#21644;&#23457;&#26597;&#20197;&#29702;&#35299;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#21547;&#20041;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20840;&#38754;&#32780;&#32508;&#21512;&#30340;&#35843;&#26597;&#65292;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25972;&#29702;&#21644;&#27010;&#36848;&#29616;&#26377;&#25991;&#29486;&#65292;&#20197;&#25552;&#20379;&#35813;&#39046;&#22495;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#20998;&#31867;&#27861;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#33879;&#21517;&#30340;&#12289;&#26041;&#27861;&#19978;&#20855;&#26377;&#35828;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20248;&#21270;DA&#36136;&#37327;&#30340;&#19968;&#33324;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21457;&#25381;&#20316;&#29992;&#30340;&#25216;&#26415;&#65292;&#24182;&#21576;&#29616;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;DA&#29992;&#20110;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly popular adoption of source code in many critical tasks motivates the development of data augmentation (DA) techniques to enhance training data and improve various capabilities (e.g., robustness and generalizability) of these models. Although a series of DA methods have been proposed and tailored for source code models, there lacks a comprehensive survey and examination to understand their effectiveness and implications. This paper fills this gap by conducting a comprehensive and integrative survey of data augmentation for source code, wherein we systematically compile and encapsulate existing literature to provide a comprehensive overview of the field. We start by constructing a taxonomy of DA for source code models model approaches, followed by a discussion on prominent, methodologically illustrative approaches. Next, we highlight the general strategies and techniques to optimize the DA quality. Subsequently, we underscore techniques that find utility in widely-accept
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#21644;&#29616;&#26377;&#27169;&#22411;&#22312;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;ChatGPT&#22312;&#25152;&#26377;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#25991;&#26723;&#38271;&#24230;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2304.14177</link><description>&lt;p&gt;
ChatGPT&#19982;&#29616;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ChatGPT vs State-of-the-Art Models: A Benchmarking Study in Keyphrase Generation Task. (arXiv:2304.14177v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#21644;&#29616;&#26377;&#27169;&#22411;&#22312;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;ChatGPT&#22312;&#25152;&#26377;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#25991;&#26723;&#38271;&#24230;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;ChatGPT&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#22312;&#35780;&#20272;ChatGPT&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#36824;&#27809;&#26377;&#22810;&#23569;&#30740;&#31350;&#65292;&#36825;&#28041;&#21450;&#21040;&#20934;&#30830;&#21453;&#26144;&#25991;&#26723;&#20869;&#23481;&#30340;&#20449;&#24687;&#24615;&#30701;&#35821;&#30340;&#35782;&#21035;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#23558;ChatGPT&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#34920;&#29616;&#19982;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#27979;&#35797;&#20102;&#23427;&#20316;&#20026;&#35299;&#20915;&#39046;&#22495;&#36866;&#24212;&#21644;&#38271;&#25991;&#26723;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#31185;&#23398;&#25991;&#31456;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#20845;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#22312;&#30701;&#25991;&#26723;&#21644;&#38271;&#25991;&#26723;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#65292;ChatGPT&#30340;&#24615;&#33021;&#20248;&#20110;&#24403;&#21069;&#29616;&#26377;&#27169;&#22411;&#65292;&#20135;&#29983;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#21644;&#25991;&#26723;&#38271;&#24230;&#30340;&#39640;&#36136;&#37327;&#20851;&#38190;&#30701;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models, including ChatGPT, have demonstrated exceptional performance in various natural language generation tasks. However, there has been limited research evaluating ChatGPT's keyphrase generation ability, which involves identifying informative phrases that accurately reflect a document's content. This study seeks to address this gap by comparing ChatGPT's keyphrase generation performance with state-of-the-art models, while also testing its potential as a solution for two significant challenges in the field: domain adaptation and keyphrase generation from long documents. We conducted experiments on six publicly available datasets from scientific articles and news domains, analyzing performance on both short and long documents. Our results show that ChatGPT outperforms current state-of-the-art models in all tested datasets and environments, generating high-quality keyphrases that adapt well to diverse domains and document lengths.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InstructGPT&#36741;&#21161;&#21307;&#29983;&#39044;&#31579;&#36873;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#12290;&#36890;&#36807;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#12289;&#21333;&#29420;&#20998;&#31867;&#12289;&#25972;&#20307;&#20998;&#31867;&#12289;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.07396</link><description>&lt;p&gt;
&#25913;&#21892;&#20020;&#24202;&#35797;&#39564;&#30340;&#24739;&#32773;&#39044;&#31579;&#36873;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#21307;&#29983;
&lt;/p&gt;
&lt;p&gt;
Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models. (arXiv:2304.07396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InstructGPT&#36741;&#21161;&#21307;&#29983;&#39044;&#31579;&#36873;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#12290;&#36890;&#36807;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#12289;&#21333;&#29420;&#20998;&#31867;&#12289;&#25972;&#20307;&#20998;&#31867;&#12289;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#24739;&#32773;&#30340;&#20020;&#24202;&#35797;&#39564;&#65292;&#21307;&#29983;&#38656;&#35201;&#36827;&#34892;&#32321;&#29712;&#30340;&#26816;&#26597;&#65292;&#20197;&#30830;&#23450;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#25991;&#26412;&#22522;&#20934;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#21644;&#20020;&#24202;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23578;&#26410;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;InstructGPT&#36741;&#21161;&#21307;&#29983;&#26681;&#25454;&#24739;&#32773;&#30340;&#21307;&#30103;&#31616;&#20917;&#30830;&#23450;&#20854;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#30340;&#36164;&#26684;&#12290;&#20351;&#29992;&#19968;&#27425;&#24615;&#12289;&#36873;&#25321;-&#25512;&#29702;&#21644;&#24605;&#32500;&#38142;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#22235;&#20010;&#32423;&#21035;&#19978;&#35780;&#20272;&#20102;&#24615;&#33021;&#65306;&#33021;&#21542;&#20174;&#20020;&#24202;&#35797;&#39564;&#20013;&#32473;&#20986;&#30340;&#21307;&#30103;&#31616;&#20917;&#20013;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#65307;&#33021;&#21542;&#20026;&#27599;&#20010;&#21333;&#29420;&#30340;&#26631;&#20934;&#20998;&#31867;&#26159;&#21542;&#31526;&#21512;&#24739;&#32773;&#65307;&#25972;&#20307;&#20998;&#31867;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physicians considering clinical trials for their patients are met with the laborious process of checking many text based eligibility criteria. Large Language Models (LLMs) have shown to perform well for clinical information extraction and clinical reasoning, including medical tests, but not yet in real-world scenarios. This paper investigates the use of InstructGPT to assist physicians in determining eligibility for clinical trials based on a patient's summarised medical profile. Using a prompting strategy combining one-shot, selection-inference and chain-of-thought techniques, we investigate the performance of LLMs on 10 synthetically created patient profiles. Performance is evaluated at four levels: ability to identify screenable eligibility criteria from a trial given a medical profile; ability to classify for each individual criterion whether the patient qualifies; the overall classification whether a patient is eligible for a clinical trial and the percentage of criteria to be scr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#19981;&#21487;&#38752;&#12290;&#25913;&#20889;&#25915;&#20987;&#21487;&#20197;&#30772;&#35299;&#22810;&#31181;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#65292;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21487;&#38752;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.11156</link><description>&lt;p&gt;
AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#21542;&#21487;&#38752;&#22320;&#26816;&#27979;&#20986;&#26469;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI-Generated Text be Reliably Detected?. (arXiv:2303.11156v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#19981;&#21487;&#38752;&#12290;&#25913;&#20889;&#25915;&#20987;&#21487;&#20197;&#30772;&#35299;&#22810;&#31181;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#65292;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21487;&#38752;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23454;&#35777;&#21644;&#29702;&#35770;&#20004;&#20010;&#26041;&#38754;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#24182;&#19981;&#21487;&#38752;&#12290;&#20174;&#23454;&#36341;&#19978;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36731;&#37327;&#32423;&#30340;&#25913;&#20889;&#22120;&#24212;&#29992;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#21487;&#20197;&#30772;&#35299;&#19968;&#31995;&#21015;&#30340;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#20351;&#29992;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26088;&#22312;&#36530;&#36991;&#25913;&#20889;&#25915;&#20987;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26816;&#27979;&#22120;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36882;&#24402;&#25913;&#20889;&#30340;&#25915;&#20987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#65292;&#25351;&#20986;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#26356;&#25797;&#38271;&#27169;&#20223;&#20154;&#31867;&#25991;&#26412;&#65292;&#22312;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#23545;&#20110;&#19968;&#20010;&#36275;&#22815;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#27169;&#20223;&#20154;&#31867;&#25991;&#26412;&#65292;&#21363;&#20351;&#26368;&#20339;&#30340;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#21482;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#22909;&#19978;&#19968;&#28857;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36275;&#22815;&#27010;&#25324;&#29305;&#23450;&#30340;&#22330;&#26223;&#65292;&#22914;&#25913;&#20889;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, both empirically and theoretically, we show that several AI-text detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of a large language model (LLM), can break a whole range of detectors, including ones using watermarking schemes as well as neural network-based detectors and zero-shot classifiers. Our experiments demonstrate that retrieval-based detectors, designed to evade paraphrasing attacks, are still vulnerable to recursive paraphrasing. We then provide a theoretical impossibility result indicating that as language models become more sophisticated and better at emulating human text, the performance of even the best-possible detector decreases. For a sufficiently advanced language model seeking to imitate human text, even the best-possible detector may only perform marginally better than a random classifier. Our result is general enough to capture specific scenarios such as par
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#26102;&#65292;&#25968;&#25454;&#22686;&#24378;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25552;&#21319;&#26576;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20294;&#25928;&#26524;&#22240;&#25216;&#26415;&#21644;&#20219;&#21153;&#32780;&#24322;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#36739;&#22823;&#27169;&#22411;&#21644;&#26356;&#38590;&#30340;&#20219;&#21153;&#26102;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2303.02577</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Effectiveness of Data Augmentation for Parameter Efficient Tuning with Limited Data. (arXiv:2303.02577v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#26102;&#65292;&#25968;&#25454;&#22686;&#24378;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25552;&#21319;&#26576;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20294;&#25928;&#26524;&#22240;&#25216;&#26415;&#21644;&#20219;&#21153;&#32780;&#24322;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#36739;&#22823;&#27169;&#22411;&#21644;&#26356;&#38590;&#30340;&#20219;&#21153;&#26102;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#65292;&#22914;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#21069;&#32512;&#35843;&#25972;&#65288;&#25110;P-tuning&#65289;&#65292;&#21487;&#20197;&#22312;&#22823;&#22823;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#21516;&#26102;&#65292;&#20135;&#29983;&#19982;&#24494;&#35843;&#30456;&#23218;&#32654;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#25968;&#25454;&#22686;&#24378;&#30340;&#32972;&#26223;&#19979;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20004;&#31181;&#36890;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#8212;&#8212;P-tuning v2&#21644;LoRA&#26102;&#65292;&#20960;&#31181;&#24120;&#29992;&#30340;&#20219;&#21153;&#26080;&#20851;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21363;EDA&#65292;&#21518;&#32763;&#35793;&#21644;&#28151;&#21512;&#65292;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#29992;&#20110;&#25552;&#21319;P-tuning&#21644;LoRA&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#21508;&#31181;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#19988;&#26576;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#36739;&#22823;&#27169;&#22411;&#21644;&#26356;&#38590;&#30340;&#20219;&#21153;&#26102;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#21477;&#23376;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has demonstrated that using parameter efficient tuning techniques such as prefix tuning (or P-tuning) on pretrained language models can yield performance that is comparable or superior to fine-tuning while dramatically reducing trainable parameters. Nevertheless, the effectiveness of such methods under the context of data augmentation, a common strategy to improve learning under low data regimes, has not been fully explored. In this paper, we examine the effectiveness of several popular task-agnostic data augmentation techniques, i.e., EDA, Back Translation, and Mixup, when using two general parameter efficient tuning methods, P-tuning v2 and LoRA, under data scarcity. We show that data augmentation can be used to boost the performance of P-tuning and LoRA models, but the effectiveness of each technique varies and certain methods can lead to a notable degradation in performance, particularly when using larger models and on harder tasks. We further analyze the sentence repre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25972;&#20010;&#23186;&#20307;&#30340;&#32454;&#31890;&#24230;&#21487;&#38752;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;&#25163;&#21160;&#21046;&#20316;&#30340;&#8220;FactNews&#8221;&#25968;&#25454;&#24211;&#19978;&#65292;&#36890;&#36807; fine-tuning BERT &#27169;&#22411;&#39044;&#27979;&#26032;&#38395;&#25253;&#36947;&#30340;&#21477;&#23376;&#32423;&#21035;&#20107;&#23454;&#24615;&#21644;&#23186;&#20307;&#20542;&#21521;&#12290;&#27492;&#26041;&#27861;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#20854;&#20182;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2301.11850</link><description>&lt;p&gt;
&#39044;&#27979;&#26032;&#38395;&#20107;&#23454;&#24615;&#21644;&#23186;&#20307;&#20542;&#21521;&#30340;&#21477;&#23376;&#32423;&#21035;&#21487;&#38752;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Predicting Sentence-Level Factuality of News and Bias of Media Outlets. (arXiv:2301.11850v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25972;&#20010;&#23186;&#20307;&#30340;&#32454;&#31890;&#24230;&#21487;&#38752;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;&#25163;&#21160;&#21046;&#20316;&#30340;&#8220;FactNews&#8221;&#25968;&#25454;&#24211;&#19978;&#65292;&#36890;&#36807; fine-tuning BERT &#27169;&#22411;&#39044;&#27979;&#26032;&#38395;&#25253;&#36947;&#30340;&#21477;&#23376;&#32423;&#21035;&#20107;&#23454;&#24615;&#21644;&#23186;&#20307;&#20542;&#21521;&#12290;&#27492;&#26041;&#27861;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#20854;&#20182;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#26032;&#38395;&#25253;&#36947;&#30340;&#20107;&#23454;&#24615;&#21644;&#23186;&#20307;&#20542;&#21521;&#23545;&#20110;&#33258;&#21160;&#21270;&#30340;&#26032;&#38395;&#20449;&#35465;&#21644;&#20107;&#23454;&#26680;&#26597;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#25972;&#20010;&#23186;&#20307;&#36827;&#34892;&#32454;&#31890;&#24230;&#21487;&#38752;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#27979;&#26032;&#38395;&#25253;&#36947;&#30340;&#21477;&#23376;&#32423;&#21035;&#20107;&#23454;&#24615;&#21644;&#23186;&#20307;&#20542;&#21521;&#65292;&#36825;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#35299;&#37322;&#25972;&#20010; source &#30340;&#21487;&#38752;&#31243;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#25163;&#21160;&#21046;&#20316;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#21477;&#23376;&#32423;&#21035;&#25968;&#25454;&#24211;&#65292;&#8220;FactNews&#8221;&#65292;&#30001; 6191 &#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#21477;&#23376;&#32452;&#25104;&#65292;&#27880;&#37322;&#20381;&#25454;&#26469;&#33258; AllSides &#30340;&#20107;&#23454;&#24615;&#21644;&#23186;&#20307;&#20542;&#21521;&#23450;&#20041;&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#24052;&#35199;&#23384;&#22312;&#20005;&#37325;&#30340;&#34394;&#20551;&#26032;&#38395;&#21644;&#25919;&#27835;&#26497;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#20854;&#20182;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the factuality of news reporting and bias of media outlets is surely relevant for automated news credibility and fact-checking. While prior work has focused on the veracity of news, we propose a fine-grained reliability analysis of the entire media. Specifically, we study the prediction of sentence-level factuality of news reporting and bias of media outlets, which may explain more accurately the overall reliability of the entire source. We first manually produced a large sentence-level dataset, titled "FactNews", composed of 6,191 sentences expertly annotated according to factuality and media bias definitions from AllSides. As a result, baseline models for sentence-level factuality prediction were presented by fine-tuning BERT. Finally, due to the severity of fake news and political polarization in Brazil, both dataset and baseline were proposed for Portuguese. However, our approach may be applied to any other language.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#35821;&#38899;&#24230;&#37327;&#23398;&#27169;&#22411;MooseNet&#65292;&#20351;&#29992;PLDA&#27169;&#22359;&#22312;SSL&#27169;&#22411;&#20013;&#36827;&#34892;&#23884;&#20837;&#23618;&#29983;&#25104;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21548;&#20247;&#30340;&#24179;&#22343;&#24847;&#35265;&#20998;&#25968;&#65288;MOS&#65289;&#12290;&#36890;&#36807;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#23545;PLDA&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30456;&#23545;&#20110;SSL&#27169;&#22411;&#24494;&#35843;&#30340;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#20248;&#21270;&#22120;&#21644;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25913;&#36827;&#20102;SSL&#27169;&#22411;&#30340;&#24494;&#35843;&#25928;&#26524;&#12290;&#32463;&#36807;PLDA&#27169;&#22359;&#24494;&#35843;&#30340;MooseNet&#22312;VoiceMOS Challenge&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36234;&#20102;SSL&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.07087</link><description>&lt;p&gt;
MooseNet&#65306;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#21512;&#25104;&#35821;&#38899;&#24230;&#37327;&#23398;&#27169;&#22411;&#19982;PLDA&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
MooseNet: A Trainable Metric for Synthesized Speech with a PLDA Module. (arXiv:2301.07087v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07087
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#35821;&#38899;&#24230;&#37327;&#23398;&#27169;&#22411;MooseNet&#65292;&#20351;&#29992;PLDA&#27169;&#22359;&#22312;SSL&#27169;&#22411;&#20013;&#36827;&#34892;&#23884;&#20837;&#23618;&#29983;&#25104;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21548;&#20247;&#30340;&#24179;&#22343;&#24847;&#35265;&#20998;&#25968;&#65288;MOS&#65289;&#12290;&#36890;&#36807;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#23545;PLDA&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30456;&#23545;&#20110;SSL&#27169;&#22411;&#24494;&#35843;&#30340;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#20248;&#21270;&#22120;&#21644;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25913;&#36827;&#20102;SSL&#27169;&#22411;&#30340;&#24494;&#35843;&#25928;&#26524;&#12290;&#32463;&#36807;PLDA&#27169;&#22359;&#24494;&#35843;&#30340;MooseNet&#22312;VoiceMOS Challenge&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36234;&#20102;SSL&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#35821;&#38899;&#24230;&#37327;&#23398;&#27169;&#22411;MooseNet&#65292;&#29992;&#20110;&#39044;&#27979;&#21548;&#20247;&#30340;&#24179;&#22343;&#24847;&#35265;&#20998;&#25968;&#65288;MOS&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#20351;&#29992;&#22522;&#20110;&#27010;&#29575;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;PLDA&#65289;&#29983;&#25104;&#27169;&#22411;&#24471;&#21040;&#30340;&#23884;&#20837;&#23618;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20165;&#20351;&#29992;136&#20010;&#21477;&#23376;&#65288;&#22823;&#32422;&#19968;&#20998;&#38047;&#30340;&#35757;&#32451;&#26102;&#38388;&#65289;&#35757;&#32451;&#30340;&#38750;&#24494;&#35843;SSL&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;PLDA&#33021;&#22815;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;PLDA&#25345;&#32493;&#25913;&#36827;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#30340;MOS&#39044;&#27979;&#27169;&#22411;&#65292;&#29978;&#33267;&#21253;&#25324;&#20855;&#26377;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;PLDA&#30340;&#35757;&#32451;&#22312;SSL&#27169;&#22411;&#24494;&#35843;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#25913;&#36827;&#20102;SSL&#27169;&#22411;&#24494;&#35843;&#65292;&#37319;&#29992;&#20102;&#21512;&#36866;&#30340;&#20248;&#21270;&#22120;&#36873;&#25321;&#21644;&#39069;&#22806;&#30340;&#23545;&#27604;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#30446;&#26631;&#12290;&#32463;&#36807;PLDA&#27169;&#22359;&#24494;&#35843;&#30340;MooseNet&#31070;&#32463;&#32593;&#32476;&#22312;VoiceMOS Challenge&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#65292;&#36229;&#36807;&#20102;SSL&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MooseNet, a trainable speech metric that predicts the listeners' Mean Opinion Score (MOS). We propose a novel approach where the Probabilistic Linear Discriminative Analysis (PLDA) generative model is used on top of an embedding obtained from a self-supervised learning (SSL) neural network (NN) model. We show that PLDA works well with a non-finetuned SSL model when trained only on 136 utterances (ca. one minute training time) and that PLDA consistently improves various neural MOS prediction models, even state-of-the-art models with task-specific fine-tuning. Our ablation study shows PLDA training superiority over SSL model fine-tuning in a low-resource scenario. We also improve SSL model fine-tuning using a convenient optimizer choice and additional contrastive and multi-task training objectives. The fine-tuned MooseNet NN with the PLDA module achieves the best results, surpassing the SSL baseline on the VoiceMOS Challenge data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#23545;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2210.10012</link><description>&lt;p&gt;
&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#24433;&#21709;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Log-linear Guardedness and its Implications. (arXiv:2210.10012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#23545;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#21457;&#29616;&#65292;&#22312;&#20551;&#35774;&#21487;&#32447;&#24615;&#30340;&#31070;&#32463;&#34920;&#31034;&#20013;&#65292;&#20174;&#20013;&#21024;&#38500;&#21487;&#20154;&#35299;&#37322;&#30340;&#27010;&#24565;&#30340;&#26041;&#27861;&#26159;&#21487;&#34892;&#21644;&#26377;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21024;&#38500;&#23545;&#20110;&#22522;&#20110;&#20462;&#25913;&#21518;&#34920;&#31034;&#36827;&#34892;&#35757;&#32451;&#30340;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#23545;&#25163;&#26080;&#27861;&#30452;&#25509;&#20174;&#34920;&#31034;&#20013;&#39044;&#27979;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#24182;&#30740;&#31350;&#20854;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#65292;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#65292;&#36825;&#25351;&#20986;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#20316;&#20026;&#19979;&#28216;&#20559;&#24046;&#32531;&#35299;&#25216;&#26415;&#30340;&#20869;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#21487;&#35299;&#37322;&#31070;&#32463;&#34920;&#31034;&#19982;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of log-linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept. However, we demonstrate that a multiclass log-linear model \emph{can} be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of log-linear guardedness as a downstream bias mitigation technique. These findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between int
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26641;&#30340;&#24179;&#38754;&#32447;&#24615;&#21270;&#20013;&#36793;&#38271;&#24230;&#30340;&#26399;&#26395;&#21644;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#24179;&#38754;&#25490;&#21015;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#24179;&#38754;&#25490;&#21015;&#19982;&#25237;&#24433;&#25490;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2207.05564</link><description>&lt;p&gt;
&#26641;&#30340;&#24179;&#38754;&#32447;&#24615;&#21270;&#20013;&#36793;&#38271;&#24230;&#30340;&#26399;&#26395;&#21644;&#65306;&#29702;&#35770;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The expected sum of edge lengths in planar linearizations of trees. Theory and applications. (arXiv:2207.05564v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26641;&#30340;&#24179;&#38754;&#32447;&#24615;&#21270;&#20013;&#36793;&#38271;&#24230;&#30340;&#26399;&#26395;&#21644;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#24179;&#38754;&#25490;&#21015;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#24179;&#38754;&#25490;&#21015;&#19982;&#25237;&#24433;&#25490;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#26641;&#24050;&#34987;&#35777;&#26126;&#26159;&#34920;&#31034;&#20154;&#31867;&#35821;&#35328;&#21477;&#23376;&#30340;&#21477;&#27861;&#32467;&#26500;&#30340;&#38750;&#24120;&#25104;&#21151;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#20123;&#32467;&#26500;&#20013;&#65292;&#39030;&#28857;&#26159;&#21333;&#35789;&#65292;&#36793;&#36830;&#25509;&#35821;&#27861;&#30456;&#20851;&#30340;&#21333;&#35789;&#12290;&#20351;&#29992;&#38543;&#26426;&#22522;&#32447;&#26469;&#35745;&#31639;&#36793;&#38271;&#24230;&#20043;&#21644;&#25110;&#20854;&#21464;&#20307;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#30340;&#20542;&#21521;&#26159;&#30701;&#30340;&#12290;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#22522;&#32447;&#26159;&#22312;&#25237;&#24433;&#25490;&#24207;&#20013;&#30340;&#26399;&#26395;&#21644;&#65288;&#20854;&#20013;&#36793;&#19981;&#30456;&#20132;&#65292;&#24182;&#19988;&#21477;&#23376;&#30340;&#26681;&#35789;&#27809;&#26377;&#34987;&#20219;&#20309;&#36793;&#35206;&#30422;&#65289;&#65292;&#21487;&#20197;&#22312;$O(n)$&#26102;&#38388;&#20869;&#35745;&#31639;&#24471;&#21040;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20851;&#27880;&#19968;&#20010;&#36739;&#24369;&#30340;&#24418;&#24335;&#32422;&#26463;&#65292;&#21363;&#24179;&#38754;&#24615;&#12290;&#22312;&#29702;&#35770;&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21051;&#30011;&#24179;&#38754;&#24615;&#30340;&#26041;&#27861;&#65292;&#32473;&#23450;&#19968;&#20010;&#21477;&#23376;&#65292;&#21487;&#20197;&#24471;&#21040;&#24179;&#38754;&#25490;&#21015;&#30340;&#25968;&#37327;&#25110;&#20197;&#22343;&#21248;&#38543;&#26426;&#26041;&#24335;&#29983;&#25104;&#24179;&#38754;&#25490;&#21015;&#30340;&#26377;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24179;&#38754;&#25490;&#21015;&#20013;&#30340;&#26399;&#26395;&#24635;&#21644;&#19982;&#25237;&#24433;&#25490;&#21015;&#20013;&#30340;&#26399;&#26395;&#24635;&#21644;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dependency trees have proven to be a very successful model to represent the syntactic structure of sentences of human languages. In these structures, vertices are words and edges connect syntactically-dependent words. The tendency of these dependencies to be short has been demonstrated using random baselines for the sum of the lengths of the edges or its variants. A ubiquitous baseline is the expected sum in projective orderings (wherein edges do not cross and the root word of the sentence is not covered by any edge), that can be computed in time $O(n)$. Here we focus on a weaker formal constraint, namely planarity. In the theoretical domain, we present a characterization of planarity that, given a sentence, yields either the number of planar permutations or an efficient algorithm to generate uniformly random planar permutations of the words. We also show the relationship between the expected sum in planar arrangements and the expected sum in projective arrangements. In the domain of a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#30340;&#26041;&#27861;LMKE&#65292;&#23427;&#26088;&#22312;&#25552;&#39640;&#23545;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.12617</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Language Models as Knowledge Embeddings. (arXiv:2206.12617v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#30340;&#26041;&#27861;LMKE&#65292;&#23427;&#26088;&#22312;&#25552;&#39640;&#23545;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#23884;&#20837;&#26159;&#36890;&#36807;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#23884;&#20837;&#21040;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#26469;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#32467;&#26500;&#25110;&#22522;&#20110;&#25551;&#36848;&#12290;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#23398;&#20064;&#34920;&#31034;&#65292;&#20197;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#29616;&#23454;&#19990;&#30028;&#30693;&#35782;&#22270;&#35889;&#20013;&#26377;&#38480;&#32467;&#26500;&#20449;&#24687;&#19979;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#12290;&#22522;&#20110;&#25551;&#36848;&#30340;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#20808;&#21069;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#36229;&#36234;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23384;&#22312;&#26114;&#36149;&#30340;&#36127;&#37319;&#26679;&#21644;&#38480;&#21046;&#24615;&#25551;&#36848;&#38656;&#27714;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LMKE&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#65292;&#26088;&#22312;&#20016;&#23500;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#34920;&#36848;&#22522;&#20110;&#25551;&#36848;&#30340;&#30693;&#35782;&#23884;&#20837;&#23398;&#20064;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21644;&#35780;&#20215;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LMKE&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#22522;&#20110;&#32467;&#26500;&#21644;&#22522;&#20110;&#20808;&#21069;&#25551;&#36848;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding entities and relations into continuous vector spaces. Existing methods are mainly structure-based or description-based. Structure-based methods learn representations that preserve the inherent structure of KGs. They cannot well represent abundant long-tail entities in real-world KGs with limited structural information. Description-based methods leverage textual information and language models. Prior approaches in this direction barely outperform structure-based ones, and suffer from problems like expensive negative sampling and restrictive description demand. In this paper, we propose LMKE, which adopts Language Models to derive Knowledge Embeddings, aiming at both enriching representations of long-tail entities and solving problems of prior description-based methods. We formulate description-based KE learning with a contrastive learning framework to improve efficiency in training and evaluation. Experimental resul
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#35821;&#35328;&#29983;&#25104;&#22120;&#35780;&#20272;&#20013;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#38382;&#39064;&#65292;&#20197;&#21450;&#30446;&#21069;&#23384;&#22312;&#30340;Mauve&#24230;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#36817;&#20284;&#35745;&#31639;&#26469;&#34913;&#37327;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#26159;&#19968;&#20010;&#20005;&#26684;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2205.16001</link><description>&lt;p&gt;
&#20851;&#20110;&#23884;&#20837;&#12289;&#32858;&#31867;&#21644;&#23383;&#31526;&#20018;&#22312;&#25991;&#26412;&#29983;&#25104;&#22120;&#35780;&#20272;&#20013;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation. (arXiv:2205.16001v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.16001
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#35821;&#35328;&#29983;&#25104;&#22120;&#35780;&#20272;&#20013;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#38382;&#39064;&#65292;&#20197;&#21450;&#30446;&#21069;&#23384;&#22312;&#30340;Mauve&#24230;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#36817;&#20284;&#35745;&#31639;&#26469;&#34913;&#37327;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#26159;&#19968;&#20010;&#20005;&#26684;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22909;&#30340;&#33258;&#21160;&#35780;&#20272;&#35821;&#35328;&#29983;&#25104;&#24230;&#37327;&#26631;&#20934;&#24212;&#35813;&#19982;&#20154;&#31867;&#23545;&#25991;&#26412;&#36136;&#37327;&#30340;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#24230;&#37327;&#26631;&#20934;&#24456;&#23569;&#65292;&#36825;&#38459;&#30861;&#20102;&#35821;&#35328;&#29983;&#25104;&#22120;&#30340;&#24555;&#36895;&#21644;&#39640;&#25928;&#21457;&#23637;&#12290;&#19968;&#20010;&#20363;&#22806;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;Mauve&#24230;&#37327;&#26631;&#20934;&#12290;&#29702;&#35770;&#19978;&#65292;Mauve&#24230;&#37327;&#30340;&#26159;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#20449;&#24687;&#35770;&#24046;&#24322;&#65306;&#19968;&#20010;&#34920;&#31034;&#34987;&#35780;&#20272;&#30340;&#35821;&#35328;&#29983;&#25104;&#22120;&#65292;&#21478;&#19968;&#20010;&#34920;&#31034;&#30495;&#27491;&#30340;&#33258;&#28982;&#35821;&#35328;&#20998;&#24067;&#12290;Mauve&#30340;&#20316;&#32773;&#35748;&#20026;&#20854;&#25104;&#21151;&#26469;&#33258;&#20110;&#25152;&#25552;&#20986;&#24046;&#24322;&#30340;&#23450;&#24615;&#29305;&#24615;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#36825;&#20010;&#24046;&#24322;&#19981;&#21487;&#35745;&#31639;&#65292;Mauve&#36890;&#36807;&#34913;&#37327;&#32858;&#31867;&#19978;&#22810;&#39033;&#24335;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#36817;&#20284;&#34920;&#31034;&#65292;&#20854;&#20013;&#32858;&#31867;&#20998;&#37197;&#26159;&#36890;&#36807;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#36827;&#34892;&#20998;&#32452;&#23383;&#31526;&#20018;&#33719;&#24471;&#30340;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#65292;&#36825;&#24182;&#19981;&#26159;&#19968;&#20010;&#20005;&#26684;&#30340;&#36817;&#20284;&#8212;&#8212;&#26080;&#35770;&#26159;&#22312;&#29702;&#35770;&#36824;&#26159;&#23454;&#36341;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
A good automatic evaluation metric for language generation ideally correlates highly with human judgements of text quality. Yet, there is a dearth of such metrics, which inhibits the rapid and efficient progress of language generators. One exception is the recently proposed Mauve. In theory, Mauve measures an information-theoretic divergence between two probability distributions over strings: one representing the language generator under evaluation; the other representing the true natural language distribution. Mauve's authors argue that its success comes from the qualitative properties of their proposed divergence. Yet in practice, as this divergence is uncomputable, Mauve approximates it by measuring the divergence between multinomial distributions over clusters instead, where cluster assignments are attained by grouping strings based on a pre-trained language model's embeddings. As we show, however, this is not a tight approximation -- in either theory or practice. This begs the que
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#25239;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#35782;&#21035;&#30446;&#26631;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20013;&#30340;&#27169;&#24335;&#26469;&#25913;&#36827;&#23545;&#25239;&#36755;&#20837;&#30340;&#35782;&#21035;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#36739;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2204.04636</link><description>&lt;p&gt;
&#8220;&#36825;&#26159;&#19968;&#20010;&#21487;&#30097;&#30340;&#21453;&#24212;&#65281;&#8221;&#65306;&#35299;&#35835;&#27010;&#29575;&#21464;&#21270;&#20197;&#26816;&#27979;NLP&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
"That Is a Suspicious Reaction!": Interpreting Logits Variation to Detect NLP Adversarial Attacks. (arXiv:2204.04636v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04636
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#25239;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#35782;&#21035;&#30446;&#26631;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20013;&#30340;&#27169;&#24335;&#26469;&#25913;&#36827;&#23545;&#25239;&#36755;&#20837;&#30340;&#35782;&#21035;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#36739;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#26159;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#36825;&#20123;&#26377;&#24847;&#21046;&#20316;&#30340;&#36755;&#20837;&#29978;&#33267;&#21487;&#20197;&#27450;&#39575;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#26080;&#27861;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#37096;&#32626;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#20197;&#24320;&#21457;&#21487;&#38752;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#21516;&#26679;&#30340;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#28145;&#20837;&#25506;&#31350;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#25991;&#26412;&#31034;&#20363;&#30340;&#27169;&#22411;&#26080;&#20851;&#26816;&#27979;&#22120;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25200;&#21160;&#36755;&#20837;&#25991;&#26412;&#26102;&#22312;&#30446;&#26631;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20013;&#35782;&#21035;&#27169;&#24335;&#12290;&#25152;&#25552;&#20986;&#30340;&#26816;&#27979;&#22120;&#22312;&#35782;&#21035;&#23545;&#25239;&#36755;&#20837;&#26041;&#38754;&#25552;&#39640;&#20102;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#30340;NLP&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#35789;&#32423;&#25915;&#20987;&#20013;&#20855;&#26377;&#36739;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks are a major challenge faced by current machine learning research. These purposely crafted inputs fool even the most advanced models, precluding their deployment in safety-critical applications. Extensive research in computer vision has been carried to develop reliable defense strategies. However, the same issue remains less explored in natural language processing. Our work presents a model-agnostic detector of adversarial text examples. The approach identifies patterns in the logits of the target classifier when perturbing the input text. The proposed detector improves the current state-of-the-art performance in recognizing adversarial inputs and exhibits strong generalization capabilities across different NLP models, datasets, and word-level attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#31185;&#23398;&#25991;&#31456;&#26356;&#20542;&#21521;&#20110;&#31215;&#26497;&#30340;&#31435;&#22330;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#25345;&#28040;&#26497;&#31435;&#22330;&#30340;&#35770;&#25991;&#12290;</title><link>http://arxiv.org/abs/2202.13610</link><description>&lt;p&gt;
AI&#26368;&#36817;&#21464;&#24471;&#26356;&#28040;&#26497;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Did AI get more negative recently?. (arXiv:2202.13610v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#31185;&#23398;&#25991;&#31456;&#26356;&#20542;&#21521;&#20110;&#31215;&#26497;&#30340;&#31435;&#22330;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#25345;&#28040;&#26497;&#31435;&#22330;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26680;&#24515;&#23376;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#31185;&#23398;&#25991;&#31456;&#20998;&#31867;&#20026;&#20004;&#31181;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#24341;&#20837;&#26032;&#25216;&#26415;&#36229;&#36234;&#29616;&#26377;&#27169;&#22411;&#30340;&#25991;&#31456;&#65292;&#34987;&#31216;&#20026;&#8220;&#31215;&#26497;&#31435;&#22330;&#8221;&#65307;&#21478;&#19968;&#31181;&#26159;&#20027;&#35201;&#25209;&#35780;&#29616;&#26377;&#25216;&#26415;&#19981;&#36275;&#30340;&#25991;&#31456;&#65292;&#34987;&#31216;&#20026;&#8220;&#28040;&#26497;&#31435;&#22330;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#36229;&#36807;1500&#31687;NLP&#21644;ML&#35770;&#25991;&#36827;&#34892;&#26631;&#27880;&#65292;&#20351;&#29992;&#22522;&#20110;SciBERT&#30340;&#27169;&#22411;&#33258;&#21160;&#39044;&#27979;&#35770;&#25991;&#30340;&#31435;&#22330;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36817;35&#24180;&#26469;NLP&#21644;ML&#39046;&#22495;&#30340;&#36229;&#36807;41000&#31687;&#35770;&#25991;&#30340;&#22823;&#35268;&#27169;&#36235;&#21183;&#65292;&#21457;&#29616;&#35770;&#25991;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21464;&#24471;&#26356;&#31215;&#26497;&#65292;&#20294;&#20063;&#26377;&#19968;&#20123;&#28040;&#26497;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we classify scientific articles in the domain of natural language processing (NLP) and machine learning (ML), as core subfields of artificial intelligence (AI), into whether (i) they extend the current state-of-the-art by the introduction of novel techniques which beat existing models or whether (ii) they mainly criticize the existing state-of-the-art, i.e. that it is deficient with respect to some property (e.g. wrong evaluation, wrong datasets, misleading task specification). We refer to contributions under (i) as having a 'positive stance' and contributions under (ii) as having a 'negative stance' (to related work). We annotate over 1.5 k papers from NLP and ML to train a SciBERT-based model to automatically predict the stance of a paper based on its title and abstract. We then analyse large-scale trends on over 41 k papers from the last approximately 35 years in NLP and ML, finding that papers have become substantially more positive over time, but negative papers als
&lt;/p&gt;</description></item></channel></rss>