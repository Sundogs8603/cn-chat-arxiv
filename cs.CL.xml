<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;MildTriple Loss&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#24182;&#27169;&#25311;&#36328;&#27169;&#24577;&#20154;&#31867;&#21160;&#20316;&#24207;&#21015;&#19982;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.04195</link><description>&lt;p&gt;
MildTriple Loss&#27169;&#22411;&#19979;&#30340;&#36816;&#21160;&#21644;&#25991;&#26412;&#36328;&#27169;&#24577;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Retrieval for Motion and Text via MildTriple Loss. (arXiv:2305.04195v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;MildTriple Loss&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#24182;&#27169;&#25311;&#36328;&#27169;&#24577;&#20154;&#31867;&#21160;&#20316;&#24207;&#21015;&#19982;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26816;&#32034;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#65292;&#38543;&#30528;&#22270;&#20687;&#25991;&#26412;&#21644;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;&#23613;&#31649;&#22312;&#34394;&#25311;&#29616;&#23454;&#31561;&#24191;&#27867;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#20294;&#20154;&#31867;&#21160;&#20316;&#24207;&#21015;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#23578;&#26410;&#24341;&#36215;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#36825;&#20010;&#20219;&#21153;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#23545;&#20004;&#31181;&#35821;&#35328;&#30340;&#20849;&#21516;&#24314;&#27169;&#65292;&#35201;&#27714;&#20174;&#25991;&#26412;&#20013;&#29702;&#35299;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20449;&#24687;&#65292;&#24182;&#20174;&#19977;&#32500;&#20154;&#20307;&#36816;&#21160;&#24207;&#21015;&#20013;&#23398;&#20064;&#34892;&#20026;&#29305;&#24449;&#12290;&#20197;&#24448;&#30340;&#36816;&#21160;&#25968;&#25454;&#24314;&#27169;&#20027;&#35201;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36825;&#21487;&#33021;&#20250;&#36951;&#24536;&#20197;&#21069;&#30340;&#20449;&#24687;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#36816;&#21160;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#20174;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#20013;&#23398;&#20064;&#34920;&#31034;&#24182;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
Cross-modal retrieval has become a prominent research topic in computer vision and natural language processing with advances made in image-text and video-text retrieval technologies. However, cross-modal retrieval between human motion sequences and text has not garnered sufficient attention despite the extensive application value it holds, such as aiding virtual reality applications in better understanding users' actions and language. This task presents several challenges, including joint modeling of the two modalities, demanding the understanding of person-centered information from text, and learning behavior features from 3D human motion sequences. Previous work on motion data modeling mainly relied on autoregressive feature extractors that may forget previous information, while we propose an innovative model that includes simple yet powerful transformer-based motion and text encoders, which can learn representations from the two different modalities and capture long-term dependencie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#8212;&#8212;OpenViVQA&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#34701;&#21512;&#38382;&#39064;&#21644;&#22270;&#20687;&#29305;&#24449;&#29983;&#25104;&#31572;&#26696;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;OpenViVQA&#25968;&#25454;&#38598;&#22312;&#26410;&#26469;&#20302;&#36164;&#28304;&#35821;&#35328;VQA&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.04183</link><description>&lt;p&gt;
OpenViVQA&#65306;&#36234;&#21335;&#35821;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OpenViVQA: Task, Dataset, and Multimodal Fusion Models for Visual Question Answering in Vietnamese. (arXiv:2305.04183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#8212;&#8212;OpenViVQA&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#34701;&#21512;&#38382;&#39064;&#21644;&#22270;&#20687;&#29305;&#24449;&#29983;&#25104;&#31572;&#26696;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;OpenViVQA&#25968;&#25454;&#38598;&#22312;&#26410;&#26469;&#20302;&#36164;&#28304;&#35821;&#35328;VQA&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#24180;&#26469;&#65292;&#35270;&#35273;&#38382;&#31572; (VQA) &#30001;&#20110;&#20854;&#39640;&#28508;&#22312;&#24212;&#29992;&#65288;&#20363;&#22914;&#26234;&#33021;&#36710;&#19978;&#30340;&#34394;&#25311;&#21161;&#25163;&#12289;&#30450;&#20154;&#36741;&#21161;&#35013;&#32622;&#25110;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#26597;&#35810;&#30340;&#25991;&#26723;&#22270;&#20687;&#20449;&#24687;&#26816;&#32034;&#31561;&#65289;&#21644;&#25361;&#25112;&#24615;&#32780;&#24341;&#36215;&#30740;&#31350;&#30028;&#30340;&#20851;&#27880;&#12290;VQA &#20219;&#21153;&#38656;&#35201;&#20855;&#26377;&#20174;&#38382;&#39064;&#21644;&#22270;&#20687;&#20013;&#34701;&#21512;&#20449;&#24687;&#20197;&#29983;&#25104;&#36866;&#24403;&#31572;&#26696;&#30340;&#26041;&#27861;&#12290; &#31070;&#32463;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22810;&#25968;&#20026;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#65292;&#22914;&#33521;&#35821;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#23558; VQA &#20219;&#21153;&#32553;&#23567;&#20026;&#31572;&#26696;&#36873;&#25321;&#20219;&#21153;&#25110;&#31572;&#26696;&#20998;&#31867;&#20219;&#21153;&#65292;&#32780;&#36825;&#31181;&#24418;&#24335;&#30340; VQA &#36828;&#36828;&#19981;&#33021;&#19982;&#20154;&#31867;&#33021;&#21147;&#30456;&#25552;&#24182;&#35770;&#65292;&#36890;&#36807;&#20165;&#36873;&#25321;&#31572;&#26696;&#32780;&#19981;&#26159;&#29983;&#25104;&#31572;&#26696;&#26469;&#28040;&#38500; VQA &#20219;&#21153;&#20013;&#30340;&#22238;&#31572;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; OpenViVQA &#65288;&#24320;&#25918;&#39046;&#22495;&#36234;&#21335;&#35821;&#35270;&#35273;&#38382;&#31572;&#65289;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#36825;&#26088;&#22312;&#25512;&#24191;&#36234;&#21335;&#35821;&#21644;&#20854;&#20182;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340; VQA &#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#32452;&#21512;&#19981;&#21516;&#30340;&#38382;&#39064;&#21644;&#22270;&#20687;&#29305;&#24449;&#20197;&#29983;&#25104; OpenViVQA &#20219;&#21153;&#30340;&#31572;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450; OpenViVQA &#25968;&#25454;&#38598;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328; VQA &#30340;&#26410;&#26469;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, visual question answering (VQA) has attracted attention from the research community because of its highly potential applications (such as virtual assistance on intelligent cars, assistant devices for blind people, or information retrieval from document images using natural language as queries) and challenge. The VQA task requires methods that have the ability to fuse the information from questions and images to produce appropriate answers. Neural visual question answering models have achieved tremendous growth on large-scale datasets which are mostly for resource-rich languages such as English. However, available datasets narrow the VQA task as the answers selection task or answer classification task. We argue that this form of VQA is far from human ability and eliminates the challenge of the answering aspect in the VQA task by just selecting answers rather than generating them. In this paper, we introduce the OpenViVQA (Open-domain Vietnamese Visual Question Answering
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#20013;&#30340;&#29468;&#27979;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#20803;&#32452;&#32423;&#21035;&#29468;&#27979;&#26816;&#27979;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21517;&#20026;OIE-Spec&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.04181</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#30456;&#20449;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#25552;&#21462;&#30340;&#25152;&#26377;&#20851;&#31995;&#20803;&#32452;&#21527;&#65311;&#23545;&#29468;&#27979;&#26816;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Shall We Trust All Relational Tuples by Open Information Extraction? A Study on Speculation Detection. (arXiv:2305.04181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#20013;&#30340;&#29468;&#27979;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#20803;&#32452;&#32423;&#21035;&#29468;&#27979;&#26816;&#27979;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21517;&#20026;OIE-Spec&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#65288;OIE&#65289;&#26088;&#22312;&#20174;&#24320;&#25918;&#22495;&#21477;&#23376;&#20013;&#25552;&#21462;&#20107;&#23454;&#20851;&#31995;&#20803;&#32452;&#12290;&#19979;&#28216;&#20219;&#21153;&#20351;&#29992;&#25552;&#21462;&#30340;OIE&#20803;&#32452;&#20316;&#20026;&#20107;&#23454;&#65292;&#32780;&#19981;&#32771;&#34385;&#36825;&#20123;&#20107;&#23454;&#30340;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#30830;&#23450;&#24615;/&#29468;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35821;&#35328;&#29616;&#35937;&#12290;&#29616;&#26377;&#30340;&#29468;&#27979;&#26816;&#27979;&#30740;&#31350;&#26159;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#23450;&#20041;&#30340;&#65292;&#20294;&#21363;&#20351;&#30830;&#23450;&#20102;&#19968;&#20010;&#21477;&#23376;&#26159;&#29468;&#27979;&#30340;&#65292;&#20063;&#19981;&#26159;&#20174;&#20854;&#20013;&#25552;&#21462;&#30340;&#25152;&#26377;&#20803;&#32452;&#37117;&#26159;&#29468;&#27979;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30740;&#31350;OIE&#20013;&#30340;&#29468;&#27979;&#65292;&#24182;&#26088;&#22312;&#30830;&#23450;&#25552;&#21462;&#30340;&#20803;&#32452;&#26159;&#21542;&#23384;&#22312;&#29468;&#27979;&#12290;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#20803;&#32452;&#32423;&#21035;&#29468;&#27979;&#26816;&#27979;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#23545;&#21253;&#21547;&#29468;&#27979;&#20803;&#32452;&#26631;&#31614;&#30340;LSOIE&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OIE-Spec&#30340;&#22522;&#20934;&#27169;&#22411;&#29992;&#20110;&#36825;&#20010;&#26032;&#30340;&#30740;&#31350;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open Information Extraction (OIE) aims to extract factual relational tuples from open-domain sentences. Downstream tasks use the extracted OIE tuples as facts, without examining the certainty of these facts. However, uncertainty/speculation is a common linguistic phenomenon. Existing studies on speculation detection are defined at sentence level, but even if a sentence is determined to be speculative, not all tuples extracted from it may be speculative. In this paper, we propose to study speculations in OIE and aim to determine whether an extracted tuple is speculative. We formally define the research problem of tuple-level speculation detection and conduct a detailed data analysis on the LSOIE dataset which contains labels for speculative tuples. Lastly, we propose a baseline model OIE-Spec for this new research task.
&lt;/p&gt;</description></item><item><title>MIReAD&#26159;&#36890;&#36807;&#24494;&#35843;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#26469;&#39044;&#27979;&#26399;&#21002;&#31867;&#21035;&#65292;&#20174;&#32780;&#23398;&#20064;&#31185;&#23398;&#35770;&#25991;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35770;&#25991;&#26816;&#32034;&#21644;&#25991;&#29486;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.04177</link><description>&lt;p&gt;
MIReAD: &#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents. (arXiv:2305.04177v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04177
&lt;/p&gt;
&lt;p&gt;
MIReAD&#26159;&#36890;&#36807;&#24494;&#35843;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#26469;&#39044;&#27979;&#26399;&#21002;&#31867;&#21035;&#65292;&#20174;&#32780;&#23398;&#20064;&#31185;&#23398;&#35770;&#25991;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35770;&#25991;&#26816;&#32034;&#21644;&#25991;&#29486;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#23398;&#20064;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#21487;&#20197;&#20419;&#36827;&#23398;&#26415;&#25991;&#29486;&#25628;&#32034;&#24182;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MIReAD&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#26469;&#39044;&#27979;&#22522;&#20110;&#25688;&#35201;&#30340;&#30446;&#26631;&#26399;&#21002;&#31867;&#21035;&#65292;&#20174;&#32780;&#23398;&#20064;&#31185;&#23398;&#35770;&#25991;&#30340;&#39640;&#36136;&#37327;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#36229;&#36807;2,000&#20010;&#26399;&#21002;&#31867;&#21035;&#30340;500,000&#22810;&#20010;PubMed&#21644;arXiv&#25688;&#35201;&#19978;&#23545;MIReAD&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#34920;&#26126;MIReAD&#20135;&#29983;&#30340;&#34920;&#31034;&#21487;&#29992;&#20110;&#31867;&#20284;&#35770;&#25991;&#26816;&#32034;&#12289;&#20027;&#39064;&#20998;&#31867;&#21644;&#25991;&#29486;&#25628;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#35780;&#20215;&#26631;&#20934;&#19979;&#20248;&#20110;&#29616;&#26377;&#30340;&#20845;&#31181;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning semantically meaningful representations from scientific documents can facilitate academic literature search and improve performance of recommendation systems. Pre-trained language models have been shown to learn rich textual representations, yet they cannot provide powerful document-level representations for scientific articles. We propose MIReAD, a simple method that learns high-quality representations of scientific papers by fine-tuning transformer model to predict the target journal class based on the abstract. We train MIReAD on more than 500,000 PubMed and arXiv abstracts across over 2,000 journal classes. We show that MIReAD produces representations that can be used for similar papers retrieval, topic categorization and literature search. Our proposed approach outperforms six existing models for representation learning on scientific documents across four evaluation standards.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36234;&#21335;&#35821;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;UIT-OpenViIC&#65292;&#36825;&#26159;&#20026;&#20102;&#35299;&#20915;&#30446;&#21069;&#22312;&#36234;&#21335;&#20302;&#36164;&#28304;&#30740;&#31350;&#31038;&#21306;&#20013;&#23384;&#22312;&#30340;&#22256;&#22659;&#32780;&#24341;&#20837;&#30340;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#36234;&#21335;&#30340;&#22797;&#26434;&#22330;&#26223;&#65292;&#24182;&#20165;&#30001;&#36234;&#21335;&#20154;&#26681;&#25454;&#20005;&#26684;&#30340;&#35268;&#21017;&#21644;&#30417;&#30563;&#36827;&#34892;&#25163;&#21160;&#27880;&#37322;&#12290;&#25968;&#25454;&#38598;&#23545;&#20110;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32763;&#35793;&#27169;&#22411;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04166</link><description>&lt;p&gt;
UIT-OpenViIC&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#36234;&#21335;&#35821;&#22270;&#20687;&#23383;&#24149;&#30340;&#26032;&#22411;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in Vietnamese. (arXiv:2305.04166v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04166
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36234;&#21335;&#35821;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;UIT-OpenViIC&#65292;&#36825;&#26159;&#20026;&#20102;&#35299;&#20915;&#30446;&#21069;&#22312;&#36234;&#21335;&#20302;&#36164;&#28304;&#30740;&#31350;&#31038;&#21306;&#20013;&#23384;&#22312;&#30340;&#22256;&#22659;&#32780;&#24341;&#20837;&#30340;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#36234;&#21335;&#30340;&#22797;&#26434;&#22330;&#26223;&#65292;&#24182;&#20165;&#30001;&#36234;&#21335;&#20154;&#26681;&#25454;&#20005;&#26684;&#30340;&#35268;&#21017;&#21644;&#30417;&#30563;&#36827;&#34892;&#25163;&#21160;&#27880;&#37322;&#12290;&#25968;&#25454;&#38598;&#23545;&#20110;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32763;&#35793;&#27169;&#22411;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#26159;&#19968;&#31181;&#20173;&#28982;&#21560;&#24341;&#20840;&#29699;&#30740;&#31350;&#31038;&#21306;&#20852;&#36259;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12290;&#34429;&#28982;MS-COCO&#23383;&#24149;&#22522;&#20934;&#26159;&#22312;2015&#24180;&#21457;&#24067;&#30340;&#65292;&#20294;&#23427;&#20173;&#28982;&#34987;&#24191;&#27867;&#20351;&#29992;&#26469;&#35780;&#20272;&#39640;&#32423;&#23383;&#24149;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20165;&#22312;MS-COCO&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#26368;&#26032;&#23383;&#24149;&#27169;&#22411;&#20165;&#22312;&#33521;&#35821;&#35821;&#35328;&#27169;&#24335;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65307;&#23427;&#20204;&#22312;&#36234;&#21335;&#25429;&#25417;&#30340;&#19978;&#19979;&#25991;&#25110;&#20351;&#29992;&#36234;&#21335;&#35821;&#27969;&#30021;&#23383;&#24149;&#22270;&#20687;&#26041;&#38754;&#30340;&#34920;&#29616;&#24182;&#19981;&#22909;&#12290;&#20026;&#20102;&#36129;&#29486;&#20110;&#20687;&#36234;&#21335;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#30740;&#31350;&#31038;&#21306;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36234;&#21335;&#35821;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;&#65292;UIT-OpenViIC&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#36234;&#21335;&#30340;&#22797;&#26434;&#22330;&#26223;&#65292;&#24182;&#30001;&#36234;&#21335;&#20154;&#26681;&#25454;&#20005;&#26684;&#30340;&#35268;&#21017;&#21644;&#30417;&#30563;&#36827;&#34892;&#25163;&#21160;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26356;&#35814;&#32454;&#22320;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#36807;&#31243;&#12290;&#20174;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32763;&#35793;&#27169;&#22411;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning is one of the vision-language tasks that still interest the research community worldwide in the 2020s. MS-COCO Caption benchmark is commonly used to evaluate the performance of advanced captioning models, although it was published in 2015. Recent captioning models trained on the MS-COCO Caption dataset only have good performance in language patterns of English; they do not have such good performance in contexts captured in Vietnam or fluently caption images using Vietnamese. To contribute to the low-resources research community as in Vietnam, we introduce a novel image captioning dataset in Vietnamese, the Open-domain Vietnamese Image Captioning dataset (UIT-OpenViIC). The introduced dataset includes complex scenes captured in Vietnam and manually annotated by Vietnamese under strict rules and supervision. In this paper, we present in more detail the dataset creation process. From preliminary analysis, we show that our dataset is challenging to recent state-of-the-art 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-LLM&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#23545;&#20110;LLM&#21152;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#31350;&#21644;&#25299;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.04160</link><description>&lt;p&gt;
X-LLM: &#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#35270;&#20026;&#22806;&#35821;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21551;&#21160;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages. (arXiv:2305.04160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-LLM&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#23545;&#20110;LLM&#21152;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#31350;&#21644;&#25299;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#22522;&#20110;&#39640;&#32423;LLM&#30340;GPT-4&#34920;&#29616;&#20986;&#36229;&#24120;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#24402;&#21151;&#20110;&#19982;&#20197;&#21069;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#20351;&#29992;&#20102;&#26356;&#20808;&#36827;&#30340;LLM&#12290;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;GPT-4&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#26159;&#26410;&#30693;&#30340;&#12290;&#20026;&#20102;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;X-LLM&#65292;&#36890;&#36807;&#20351;&#29992;X2L&#25509;&#21475;&#23558;&#22810;&#27169;&#24577;&#65288;&#22270;&#20687;&#12289;&#35821;&#38899;&#12289;&#35270;&#39057;&#65289;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ChatGLM&#65289;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;X-LLM&#20351;&#29992;X2L&#25509;&#21475;&#23558;&#22810;&#20010;&#20923;&#32467;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#20923;&#32467;&#30340;LLM&#23545;&#40784;&#65292;&#20854;&#20013;&#8220;X&#8221;&#34920;&#31034;&#22810;&#27169;&#24577;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35270;&#39057;&#65292;&#8220;L&#8221;&#34920;&#31034;&#35821;&#35328;&#12290;X-LLM&#30340;&#35757;&#32451;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#65288;1&#65289;&#36716;&#25442;&#22810;&#27169;&#24577;&#20449;&#24687;&#65306;&#31532;&#19968;&#38454;&#27573;&#20998;&#21035;&#35757;&#32451;&#27599;&#20010;X2L&#25509;&#21475;&#19982;&#20854;&#21508;&#33258;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#23545;&#40784;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#36755;&#20837;&#21040;ChatGLM&#20013;&#12290;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#26041;&#24335;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26465;&#20214;&#29983;&#25104;&#24494;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#31034;&#26500;&#36896;&#26469;&#23454;&#29616;&#21487;&#25511;&#30340;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#29983;&#25104;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.04147</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#23454;&#29616;&#21487;&#25511;&#30340;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controllable Mixed-Initiative Dialogue Generation through Prompting. (arXiv:2305.04147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04147
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#26041;&#24335;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26465;&#20214;&#29983;&#25104;&#24494;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#31034;&#26500;&#36896;&#26469;&#23454;&#29616;&#21487;&#25511;&#30340;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#29983;&#25104;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#20219;&#21153;&#28041;&#21450;&#37325;&#22797;&#20132;&#25442;&#20449;&#24687;&#21644;&#23545;&#35805;&#25511;&#21046;&#12290;&#20250;&#35805;&#20195;&#29702;&#36890;&#36807;&#29983;&#25104;&#21709;&#24212;&#26469;&#33719;&#24471;&#25511;&#21046;&#65292;&#36825;&#20123;&#21709;&#24212;&#25353;&#29031;&#31574;&#30053;&#35268;&#21010;&#22120;&#35268;&#23450;&#30340;&#29305;&#23450;&#23545;&#35805;&#24847;&#22270;&#25110;&#31574;&#30053;&#36827;&#34892;&#12290;&#26631;&#20934;&#26041;&#27861;&#26159;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#25191;&#34892;&#22522;&#20110;&#36825;&#20123;&#24847;&#22270;&#30340;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21463;&#30417;&#30563;&#30340;&#29983;&#25104;&#27169;&#22411;&#21463;&#25968;&#25454;&#27880;&#37322;&#25104;&#26412;&#21644;&#36136;&#37327;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26465;&#20214;&#29983;&#25104;&#24494;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#20026;&#21487;&#25511;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#24418;&#24335;&#21270;&#25552;&#31034;&#26500;&#36896;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;PersuasionForGood&#21644;Emotional Support Conversations&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#31867;&#35780;&#20272;&#21644;&#33258;&#21160;&#25351;&#26631;&#26041;&#38754;&#22343;&#26174;&#31034;&#20986;&#27604;&#24494;&#35843;&#21644;&#30495;&#23454;&#21709;&#24212;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control. Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner. The standard approach has been fine-tuning pre-trained language models to perform generation conditioned on these intents. However, these supervised generation models are limited by the cost and quality of data annotation. We instead prompt large language models as a drop-in replacement to fine-tuning on conditional generation. We formalize prompt construction for controllable mixed-initiative dialogue. Our findings show improvements over fine-tuning and ground truth responses according to human evaluation and automatic metrics for two tasks: PersuasionForGood and Emotional Support Conversations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MAPS&#30340;&#26694;&#26550;&#65292;&#20351;LLMs&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#32763;&#35793;&#30340;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#21253;&#25324;&#20998;&#26512;&#28304;&#25991;&#26412;&#24182;&#25552;&#21462;&#20851;&#38190;&#35789;&#12289;&#20027;&#39064;&#21644;&#30456;&#20851;&#28436;&#31034;&#20197;&#25351;&#23548;&#32763;&#35793;&#36807;&#31243;&#12290;&#35813;&#26694;&#26550;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26126;&#26174;&#20248;&#20110;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#20026;&#24320;&#23637;&#20351;&#29992;LLM&#23454;&#29616;&#20154;&#31867;&#21270;&#32763;&#35793;&#31574;&#30053;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.04118</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#20154;&#31867;&#21270;&#32763;&#35793;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Exploring Human-Like Translation Strategy with Large Language Models. (arXiv:2305.04118v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MAPS&#30340;&#26694;&#26550;&#65292;&#20351;LLMs&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#32763;&#35793;&#30340;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#21253;&#25324;&#20998;&#26512;&#28304;&#25991;&#26412;&#24182;&#25552;&#21462;&#20851;&#38190;&#35789;&#12289;&#20027;&#39064;&#21644;&#30456;&#20851;&#28436;&#31034;&#20197;&#25351;&#23548;&#32763;&#35793;&#36807;&#31243;&#12290;&#35813;&#26694;&#26550;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26126;&#26174;&#20248;&#20110;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#20026;&#24320;&#23637;&#20351;&#29992;LLM&#23454;&#29616;&#20154;&#31867;&#21270;&#32763;&#35793;&#31574;&#30053;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#34920;&#29616;&#20986;&#20102;&#25509;&#36817;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#27700;&#24179;&#12290;&#22312;&#20854;&#22810;&#31181;&#25216;&#33021;&#20013;&#65292;LLM&#30340;&#32763;&#35793;&#33021;&#21147;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#32763;&#35793;&#20165;&#20851;&#27880;&#28304;&#30446;&#26631;&#26144;&#23556;&#19981;&#21516;&#65292;&#22522;&#20110;LLM&#30340;&#32763;&#35793;&#21487;&#20197;&#28508;&#22312;&#22320;&#27169;&#20223;&#20154;&#31867;&#32763;&#35793;&#30340;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#20250;&#37319;&#21462;&#35768;&#22810;&#20934;&#22791;&#27493;&#39588;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;MAPS&#26694;&#26550;&#65288;Multi-Aspect Prompting and Selection&#65289;&#25506;&#32034;&#36825;&#31181;&#21487;&#33021;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;LLM&#39318;&#20808;&#20998;&#26512;&#32473;&#23450;&#28304;&#25991;&#26412;&#24182;&#25552;&#21462;&#19977;&#20010;&#19982;&#32763;&#35793;&#30456;&#20851;&#30340;&#30693;&#35782;&#26041;&#38754;&#65306;&#20851;&#38190;&#35789;&#12289;&#20027;&#39064;&#21644;&#30456;&#20851;&#28436;&#31034;&#20197;&#25351;&#23548;&#32763;&#35793;&#36807;&#31243;&#12290;&#20026;&#20102;&#36807;&#28388;&#25481;&#22122;&#22768;&#21644;&#26080;&#29992;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#36136;&#37327;&#20272;&#35745;&#30340;&#36873;&#25321;&#26426;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#22810;&#20010;&#35821;&#35328;&#23545;&#21644;&#32763;&#35793;&#26041;&#21521;&#19978;&#26174;&#30528;&#20248;&#20110;&#22810;&#20010;&#24378;&#22522;&#32447;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#24320;&#23637;&#20351;&#29992;LLM&#23454;&#29616;&#20154;&#31867;&#21270;&#32763;&#35793;&#31574;&#30053;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence. Among their numerous skills, the translation abilities of LLMs have received considerable attention. In contrast to traditional machine translation that focuses solely on source-target mapping, LLM-based translation can potentially mimic the human translation process that takes many preparatory steps to ensure high-quality translation. This work aims to explore this possibility by proposing the MAPS framework, which stands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs to first analyze the given source text and extract three aspects of translation-related knowledge: keywords, topics and relevant demonstrations to guide the translation process. To filter out the noisy and unhelpful knowledge, we employ a selection mechanism based on quality estimation. Experiments sug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24773;&#24863;&#21453;&#36716;&#21644;&#35821;&#20041;&#19981;&#19968;&#33268;&#24615;&#20174;&#38750;&#35773;&#21050;&#24615;&#36755;&#20837;&#21477;&#23376;&#29983;&#25104;&#24102;&#26377;&#34920;&#24773;&#31526;&#21495;&#30340;&#35773;&#21050;&#24615;&#21477;&#23376;&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.04105</link><description>&lt;p&gt;
"&#24403;&#25991;&#23383;&#38590;&#20197;&#34920;&#36798;&#26102;&#65292;&#34920;&#24773;&#31526;&#21495;&#31216;&#38712;": &#21033;&#29992;&#24773;&#24863;&#21453;&#36716;&#21644;&#35821;&#20041;&#19981;&#19968;&#33268;&#29983;&#25104;&#24102;&#26377;&#34920;&#24773;&#31526;&#21495;&#30340;&#35773;&#21050;&#35805;&#35821;
&lt;/p&gt;
&lt;p&gt;
"When Words Fail, Emojis Prevail": Generating Sarcastic Utterances with Emoji Using Valence Reversal and Semantic Incongruity. (arXiv:2305.04105v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24773;&#24863;&#21453;&#36716;&#21644;&#35821;&#20041;&#19981;&#19968;&#33268;&#24615;&#20174;&#38750;&#35773;&#21050;&#24615;&#36755;&#20837;&#21477;&#23376;&#29983;&#25104;&#24102;&#26377;&#34920;&#24773;&#31526;&#21495;&#30340;&#35773;&#21050;&#24615;&#21477;&#23376;&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35773;&#21050;&#26159;&#20010;&#20307;&#29992;&#26469;&#34920;&#36798;&#19982;&#21547;&#20041;&#30456;&#21453;&#30340;&#20107;&#24773;&#30340;&#24494;&#22937;&#24418;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#20174;&#38750;&#35773;&#21050;&#24615;&#36755;&#20837;&#21477;&#23376;&#29983;&#25104;&#35773;&#21050;&#24615;&#21477;&#23376;&#30340;&#26032;&#39062;&#26550;&#26500;&#12290;&#25105;&#20204;&#23558;&#29983;&#25104;&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#35773;&#21050;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#25910;&#38598;&#19982;&#36825;&#20123;&#35773;&#21050;&#21477;&#23376;&#30456;&#20851;&#30340;&#34920;&#24773;&#31526;&#21495;&#12290;&#35773;&#21050;&#30340;&#20004;&#20010;&#20851;&#38190;&#20803;&#32032;&#34987;&#32435;&#20837;&#21040;&#25991;&#26412;&#35773;&#21050;&#29983;&#25104;&#20219;&#21153;&#20013;&#65306;&#24773;&#24863;&#21453;&#36716;&#21644;&#35821;&#20041;&#19981;&#19968;&#33268;&#24615;&#19982;&#35821;&#22659;&#65292;&#20854;&#20013;&#35821;&#22659;&#21487;&#33021;&#28041;&#21450;&#28436;&#35762;&#32773;&#21644;&#21548;&#20247;&#20043;&#38388;&#20849;&#20139;&#30340;&#24120;&#35782;&#25110;&#19968;&#33324;&#30693;&#35782;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35773;&#21050;&#29983;&#25104;&#20316;&#21697;&#37117;&#38598;&#20013;&#22312;&#36825;&#31181;&#25991;&#26412;&#24418;&#24335;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#24403;&#20070;&#38754;&#25991;&#26412;&#26080;&#27861;&#26377;&#25928;&#22320;&#25429;&#25417;&#21475;&#22836;&#21644;&#38754;&#23545;&#38754;&#20132;&#27969;&#30340;&#24773;&#24863;&#32447;&#32034;&#26102;&#65292;&#20154;&#20204;&#32463;&#24120;&#36873;&#25321;&#34920;&#24773;&#31526;&#21495;&#26469;&#20934;&#30830;&#22320;&#34920;&#36798;&#20182;&#20204;&#30340;&#24773;&#24863;&#12290;&#30001;&#20110;&#34920;&#24773;&#31526;&#21495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#21512;&#29702;&#22320;&#32435;&#20837;&#36866;&#24403;&#30340;&#34920;&#24773;&#31526;&#21495;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sarcasm pertains to the subtle form of language that individuals use to express the opposite of what is implied. We present a novel architecture for sarcasm generation with emoji from a non-sarcastic input sentence. We divide the generation task into two sub tasks: one for generating textual sarcasm and another for collecting emojis associated with those sarcastic sentences. Two key elements of sarcasm are incorporated into the textual sarcasm generation task: valence reversal and semantic incongruity with context, where the context may involve shared commonsense or general knowledge between the speaker and their audience. The majority of existing sarcasm generation works have focused on this textual form. However, in the real world, when written texts fall short of effectively capturing the emotional cues of spoken and face-to-face communication, people often opt for emojis to accurately express their emotions. Due to the wide range of applications of emojis, incorporating appropriate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#25913;&#36827;&#27861;&#24459;&#25991;&#20214;&#30340;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#20197;&#35299;&#20915;&#38271;&#12289;&#23494;&#38598;&#12289;&#21253;&#21547;&#34892;&#35805;&#26415;&#35821;&#30340;&#27861;&#24459;&#25991;&#20214;&#30340;&#20462;&#36766;&#35282;&#33394;&#26631;&#27880;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.04100</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27861;&#24459;&#25991;&#20214;&#20462;&#36766;&#35282;&#33394;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Rhetorical Role Labeling of Legal Documents using Transformers and Graph Neural Networks. (arXiv:2305.04100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#25913;&#36827;&#27861;&#24459;&#25991;&#20214;&#30340;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#20197;&#35299;&#20915;&#38271;&#12289;&#23494;&#38598;&#12289;&#21253;&#21547;&#34892;&#35805;&#26415;&#35821;&#30340;&#27861;&#24459;&#25991;&#20214;&#30340;&#20462;&#36766;&#35282;&#33394;&#26631;&#27880;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#27861;&#24459;&#25991;&#20214;&#20887;&#38271;&#32780;&#23494;&#38598;&#65292;&#38656;&#35201;&#20154;&#21147;&#25104;&#26412;&#26469;&#35299;&#26512;&#12290;&#27492;&#22806;&#65292;&#27861;&#24459;&#25991;&#20214;&#36824;&#21253;&#21547;&#22823;&#37327;&#34892;&#35805;&#26415;&#35821;&#65292;&#20351;&#29992;&#29616;&#26377;&#27169;&#22411;&#26469;&#20174;&#20013;&#33719;&#24471;&#27934;&#23519;&#30340;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312; SemEval Task 6: understanding legal texts, shared subtask A &#30340;&#21360;&#24230;&#27861;&#38498;&#21028;&#20915;&#20013;&#36827;&#34892;&#20462;&#36766;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#25152;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#22914;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;BERT&#30340;&#21464;&#20307;&#65292;&#20197;&#25552;&#39640;&#38024;&#23545;&#22797;&#26434;&#27861;&#24459;&#25991;&#20214;&#30340;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
A legal document is usually long and dense requiring human effort to parse it. It also contains significant amounts of jargon which make deriving insights from it using existing models a poor approach. This paper presents the approaches undertaken to perform the task of rhetorical role labelling on Indian Court Judgements as part of SemEval Task 6: understanding legal texts, shared subtask A. We experiment with graph based approaches like Graph Convolutional Networks and Label Propagation Algorithm, and transformer-based approaches including variants of BERT to improve accuracy scores on text classification of complex legal documents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#21010;&#21644;&#35299;&#20915;&#30340;&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#21892;&#38646;&#26679;&#26412;&#24605;&#32771;&#38142;&#25512;&#29702;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#21046;&#23450;&#35745;&#21010;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#25353;&#35745;&#21010;&#25191;&#34892;&#23376;&#20219;&#21153;&#65307;&#23558;&#36755;&#20837;&#25552;&#31034;&#25193;&#23637;&#21040;&#21253;&#25324;&#31616;&#21333;&#31639;&#26415;&#35745;&#31639;&#30340;&#31034;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#20102;&#38646;&#26679;&#26412;-CoT&#12290;</title><link>http://arxiv.org/abs/2305.04091</link><description>&lt;p&gt;
&#35745;&#21010;&#21644;&#35299;&#20915;&#25552;&#31034;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#21892;&#38646;&#26679;&#26412;&#24605;&#32771;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. (arXiv:2305.04091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#21010;&#21644;&#35299;&#20915;&#30340;&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#21892;&#38646;&#26679;&#26412;&#24605;&#32771;&#38142;&#25512;&#29702;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#21046;&#23450;&#35745;&#21010;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#25353;&#35745;&#21010;&#25191;&#34892;&#23376;&#20219;&#21153;&#65307;&#23558;&#36755;&#20837;&#25552;&#31034;&#25193;&#23637;&#21040;&#21253;&#25324;&#31616;&#21333;&#31639;&#26415;&#35745;&#31639;&#30340;&#31034;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#20102;&#38646;&#26679;&#26412;-CoT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#22810;&#27493;&#39588;&#25512;&#29702;&#20219;&#21153;&#65292;&#23569;&#26679;&#26412;&#30340;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#21253;&#25324;&#19968;&#20123;&#25163;&#24037;&#21046;&#20316;&#30340;&#36880;&#27493;&#25512;&#29702;&#28436;&#31034;&#65292;&#20351;LLMs&#33021;&#22815;&#26126;&#30830;&#29983;&#25104;&#25512;&#29702;&#27493;&#39588;&#24182;&#25552;&#39640;&#20854;&#25512;&#29702;&#20219;&#21153;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#28040;&#38500;&#25163;&#21160;&#21171;&#21160;&#65292;&#38646;&#26679;&#26412;&#24605;&#32500;&#38142;&#23558;&#30446;&#26631;&#38382;&#39064;&#38472;&#36848;&#19982;&#8220;&#35753;&#25105;&#20204;&#36880;&#27493;&#24605;&#32771;&#8221;&#36830;&#25509;&#36215;&#26469;&#20316;&#20026;&#36755;&#20837;&#25552;&#31034;LLMs&#12290;&#23613;&#31649;&#38646;&#26679;&#26412;-CoT&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#35745;&#31639;&#38169;&#35823;&#12289;&#32570;&#22833;&#27493;&#39588;&#38169;&#35823;&#21644;&#35821;&#20041;&#35823;&#35299;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#32570;&#22833;&#27493;&#39588;&#38169;&#35823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35745;&#21010;&#21644;&#35299;&#20915;&#65288;PS&#65289;&#25552;&#31034;&#12290;&#23427;&#21253;&#21547;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#39318;&#20808;&#65292;&#21046;&#23450;&#35745;&#21010;&#23558;&#25972;&#20010;&#20219;&#21153;&#21010;&#20998;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#25353;&#29031;&#35745;&#21010;&#25191;&#34892;&#23376;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#31639;&#38169;&#35823;&#24182;&#25552;&#39640;&#29983;&#25104;&#25512;&#29702;&#27493;&#39588;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#23558;&#36755;&#20837;&#25552;&#31034;&#25193;&#23637;&#21040;&#21253;&#25324;&#31616;&#21333;&#31639;&#26415;&#35745;&#31639;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PS&#25552;&#31034;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#32988;&#36807;&#20102;&#38646;&#26679;&#26412;CoT&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zero-shot-CoT concatenates the target problem statement with "Let's think step by step" as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#24182;&#23558;&#25191;&#34892;&#32467;&#26524;&#21253;&#21547;&#22312;&#22312;&#27880;&#37322;&#20013;&#26469;&#20248;&#21270;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#30340;&#20195;&#30721;&#36136;&#37327;&#65292;&#36890;&#36807;&#19982;&#20061;&#20010;&#19981;&#21516;&#30340;LLMs&#36827;&#34892;&#27604;&#36739;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#20010;&#31454;&#25216;&#32534;&#31243;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04087</link><description>&lt;p&gt;
&#33258;&#25105;&#32534;&#36753;&#65306;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;
&lt;/p&gt;
&lt;p&gt;
Self-Edit: Fault-Aware Code Editor for Code Generation. (arXiv:2305.04087v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#24182;&#23558;&#25191;&#34892;&#32467;&#26524;&#21253;&#21547;&#22312;&#22312;&#27880;&#37322;&#20013;&#26469;&#20248;&#21270;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#30340;&#20195;&#30721;&#36136;&#37327;&#65292;&#36890;&#36807;&#19982;&#20061;&#20010;&#19981;&#21516;&#30340;LLMs&#36827;&#34892;&#27604;&#36739;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#20010;&#31454;&#25216;&#32534;&#31243;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#20013;&#29983;&#25104;&#20195;&#30721;&#30340;&#33021;&#21147;&#24050;&#32463;&#24471;&#21040;&#35777;&#26126;&#65292;&#20294;&#30001;&#20110;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#21463;&#20154;&#31867;&#32534;&#31243;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#25191;&#34892;&#32467;&#26524;&#26469;&#25552;&#39640;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#30340;&#20195;&#30721;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#38382;&#39064;&#20013;&#25552;&#20379;&#30340;&#31034;&#20363;&#27979;&#35797;&#29992;&#20363;&#19978;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#24182;&#23558;&#25191;&#34892;&#32467;&#26524;&#21253;&#21547;&#22312;&#34917;&#20805;&#24615;&#27880;&#37322;&#20013;&#12290;&#21033;&#29992;&#36825;&#20010;&#27880;&#37322;&#20316;&#20026;&#25351;&#23548;&#65292;&#25105;&#20204;&#30340;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;&#29992;&#20110;&#32416;&#27491;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#31454;&#25216;&#32534;&#31243;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20061;&#20010;&#19981;&#21516;&#30340;LLMs&#12290;&#19982;&#30452;&#25509;&#20174;LLMs&#29983;&#25104;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;APPS-dev&#19978;&#23558;pass@1&#30340;&#24179;&#22343;&#20540;&#25552;&#39640;89&#65285;&#65292;&#22312;APPS-test&#19978;&#25552;&#39640;31&#65285;&#65292;&#22312;HumanEval&#19978;&#25552;&#39640;48&#65285;&#65292;&#36229;&#36807;&#20102;&#20061;&#20010;&#27969;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;LLMs&#65292;&#21442;&#25968;&#22823;&#23567;&#33539;&#22260;&#20026;110M-t&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89\% on APPS-dev, 31\% on APPS-test, and 48\% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#33258;&#28982;&#35821;&#35328;&#21160;&#20316;&#31354;&#38388;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102; &#949;-&#21487;&#25509;&#21463;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#35821;&#35328;&#27169;&#22411;&#25110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25991;&#26412;&#35282;&#33394;-&#35780;&#35770;&#65288;TAC&#65289;&#20195;&#29702;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.04082</link><description>&lt;p&gt;
&#19968;&#31181;&#25991;&#26412;&#28216;&#25103;&#20013;&#33258;&#28982;&#35821;&#35328;&#21160;&#20316;&#31354;&#38388;&#30340;&#31616;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Minimal Approach for Natural Language Action Space in Text-based Games. (arXiv:2305.04082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#33258;&#28982;&#35821;&#35328;&#21160;&#20316;&#31354;&#38388;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102; &#949;-&#21487;&#25509;&#21463;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#35821;&#35328;&#27169;&#22411;&#25110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25991;&#26412;&#35282;&#33394;-&#35780;&#35770;&#65288;TAC&#65289;&#20195;&#29702;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#28216;&#25103;&#26159;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#24378;&#21270;&#23398;&#20064;&#20132;&#20114;&#29615;&#22659;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#36890;&#24120;&#34987;&#29992;&#20110;&#22788;&#29702;&#25991;&#26412;&#28216;&#25103;&#20013;&#30340;&#22823;&#37327;&#21160;&#20316;&#31354;&#38388;&#65292;&#20294;&#30446;&#21069;&#20173;&#19981;&#30830;&#23450;&#36825;&#20123;&#25216;&#26415;&#26159;&#21542;&#24517;&#35201;&#25110;&#34987;&#36807;&#24230;&#20351;&#29992;&#12290;&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#25506;&#32034;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102; &#949;-&#21487;&#25509;&#21463;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#21487;&#25509;&#21463;&#30340;&#21160;&#20316;&#30340;&#26368;&#23567;&#21270;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#35282;&#33394;-&#35780;&#35770;&#65288;TAC&#65289;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#20165;&#20174;&#28216;&#25103;&#35266;&#23519;&#20013;&#29983;&#25104;&#25991;&#26412;&#21629;&#20196;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#25110;&#30693;&#35782;&#22270;&#35889;&#12290;&#22312; Jericho &#30340; 10 &#31181;&#28216;&#25103;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#20248;&#20110;&#24378;&#22522;&#32447;&#21644;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20984;&#26174;&#20986;&#65292;&#23545;&#20110;&#26377;&#25928;&#22320;&#25506;&#32034;&#25351;&#25968;&#32423;&#21160;&#20316;&#31354;&#38388;&#65292;&#26356;&#36731;&#37327;&#21270;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#21033;&#29992;&#29615;&#22659;&#20449;&#24687;&#30340;&#26032;&#35270;&#35282;&#26159;&#36275;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based games (TGs) are language-based interactive environments for reinforcement learning. While language models (LMs) and knowledge graphs (KGs) are commonly used for handling large action space in TGs, it is unclear whether these techniques are necessary or overused. In this paper, we revisit the challenge of exploring the action space in TGs and propose $ \epsilon$-admissible exploration, a minimal approach of utilizing admissible actions, for training phase. Additionally, we present a text-based actor-critic (TAC) agent that produces textual commands for game, solely from game observations, without requiring any KG or LM. Our method, on average across 10 games from Jericho, outperforms strong baselines and state-of-the-art agents that use LM and KG. Our approach highlights that a much lighter model design, with a fresh perspective on utilizing the information within the environments, suffices for an effective exploration of exponentially large action spaces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;Distantly-Supervised Named Entity Recognition&#20013;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#22122;&#22768;&#30340;&#20998;&#31163;&#31574;&#30053;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#26500;&#24314;&#26469;&#24212;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2305.04076</link><description>&lt;p&gt;
SANTA&#65306;Distantly-Supervised Named Entity Recognition&#20013;&#22788;&#29702;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#22122;&#22768;&#30340;&#20998;&#31163;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition. (arXiv:2305.04076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;Distantly-Supervised Named Entity Recognition&#20013;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#22122;&#22768;&#30340;&#20998;&#31163;&#31574;&#30053;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#26500;&#24314;&#26469;&#24212;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36828;&#31243;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#30417;&#30563;&#35774;&#32622;&#20013;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#27880;&#37322;&#36127;&#25285;&#65292;&#20294;&#26159;&#26080;&#19978;&#19979;&#25991;&#30340;&#21305;&#37197;&#36807;&#31243;&#21644;&#30693;&#35782;&#24211;&#30340;&#26377;&#38480;&#35206;&#30422;&#24341;&#20837;&#20102;&#19981;&#20934;&#30830;&#21644;&#19981;&#23436;&#25972;&#30340;&#26631;&#27880;&#22122;&#38899;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#22788;&#29702;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#30340;SANTA&#65292;&#20197;&#35299;&#20915;&#30001;&#19981;&#20934;&#30830;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distantly-Supervised Named Entity Recognition effectively alleviates the burden of time-consuming and expensive annotation in the supervised setting. But the context-free matching process and the limited coverage of knowledge bases introduce inaccurate and incomplete annotation noise respectively. Previous studies either considered only incomplete annotation noise or indiscriminately handle two types of noise with the same strategy. In this paper, we argue that the different causes of two types of noise bring up the requirement of different strategies in model architecture. Therefore, we propose the SANTA to handle these two types of noise separately with (1) Memory-smoothed Focal Loss and Entity-aware KNN to relieve the entity ambiguity problem caused by inaccurate annotation, and (2) Boundary Mixup to alleviate decision boundary shifting problem caused by incomplete annotation and a noise-tolerant loss to improve the robustness. Benefiting from our separate tailored strategies, we co
&lt;/p&gt;</description></item><item><title>RPD&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28431;&#27934;&#36827;&#34892;&#20102;&#38450;&#24481;&#65292;&#36890;&#36807;&#35782;&#21035;&#23545;&#25239;&#24615;&#31034;&#20363;&#24182;&#27880;&#20837;&#23433;&#20840;&#25200;&#21160;&#26469;&#20943;&#23569;&#35823;&#38450;&#24481;&#65292;&#25104;&#21151;&#22320;&#20462;&#22797;&#20102;&#39640;&#36798;97%&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#22312;&#33258;&#28982;&#31034;&#20363;&#30340;&#24615;&#33021;&#20165;&#38477;&#20302;&#20102;&#32422;2%&#12290;</title><link>http://arxiv.org/abs/2305.04067</link><description>&lt;p&gt;
&#21453;&#24212;&#24615;&#25668;&#20837;&#25200;&#21160;&#65306;&#23545;&#25239;&#25991;&#26412;&#38450;&#24481;&#30340;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reactive Perturbation Defocusing for Textual Adversarial Defense. (arXiv:2305.04067v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04067
&lt;/p&gt;
&lt;p&gt;
RPD&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28431;&#27934;&#36827;&#34892;&#20102;&#38450;&#24481;&#65292;&#36890;&#36807;&#35782;&#21035;&#23545;&#25239;&#24615;&#31034;&#20363;&#24182;&#27880;&#20837;&#23433;&#20840;&#25200;&#21160;&#26469;&#20943;&#23569;&#35823;&#38450;&#24481;&#65292;&#25104;&#21151;&#22320;&#20462;&#22797;&#20102;&#39640;&#36798;97%&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#22312;&#33258;&#28982;&#31034;&#20363;&#30340;&#24615;&#33021;&#20165;&#38477;&#20302;&#20102;&#32422;2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35797;&#22270;&#37325;&#26500;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#38450;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#26041;&#38754;&#24615;&#33021;&#26377;&#38480;&#65292;&#21516;&#26102;&#20063;&#20250;&#23545;&#33258;&#28982;&#31034;&#20363;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21453;&#24212;&#24615;&#25668;&#20837;&#25200;&#21160; (RPD) &#30340;&#26041;&#27861;&#12290;RPD &#20351;&#29992;&#23545;&#25239;&#26816;&#27979;&#22120;&#35782;&#21035;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24182;&#20943;&#23569;&#22312;&#33258;&#28982;&#31034;&#20363;&#19978;&#30340;&#35823;&#38450;&#24481;&#12290;RPD &#19981;&#26159;&#37325;&#26500;&#23545;&#25163;&#65292;&#32780;&#26159;&#22312;&#23545;&#25239;&#24615;&#31034;&#20363;&#20013;&#27880;&#20837;&#23433;&#20840;&#25200;&#21160;&#65292;&#20197;&#20998;&#25955;&#30446;&#26631;&#27169;&#22411;&#23545;&#24694;&#24847;&#25200;&#21160;&#30340;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#12289;&#20004;&#20010;&#30446;&#26631;&#27169;&#22411;&#21644;&#21508;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#25104;&#21151;&#22320;&#20462;&#22797;&#20102;&#22823;&#32422; 97% &#30340;&#27491;&#30830;&#35782;&#21035;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24182;&#19988;&#33258;&#28982;&#31034;&#20363;&#30340;&#24615;&#33021;&#20165;&#38477;&#20302;&#20102;&#32422; 2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that large pre-trained language models are vulnerable to adversarial attacks. Existing methods attempt to reconstruct the adversarial examples. However, these methods usually have limited performance in defense against adversarial examples, while also negatively impacting the performance on natural examples. To overcome this problem, we propose a method called Reactive Perturbation Defocusing (RPD). RPD uses an adversarial detector to identify adversarial examples and reduce false defenses on natural examples. Instead of reconstructing the adversaries, RPD injects safe perturbations into adversarial examples to distract the objective models from the malicious perturbations. Our experiments on three datasets, two objective models, and various adversarial attacks show that our proposed framework successfully repairs up to approximately 97% of correctly identified adversarial examples with only about a 2% performance decrease on natural examples. We also provide 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20027;&#21160;&#21457;&#29616;&#38754;&#21521;&#20219;&#21153;&#23545;&#35805;&#30340;&#26032;&#27133;&#20301;&#65292;&#20197;&#25552;&#39640;&#20132;&#20114;&#24615;&#33021;&#12290;&#30001;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#26032;&#30340;&#29992;&#25143;&#38656;&#27714;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#22330;&#26223;&#65292;&#20351;&#29616;&#26377;&#30340;&#23545;&#35805;&#25628;&#32034;&#31995;&#32479;&#21464;&#24471;&#26080;&#27861;&#28385;&#36275;&#12290;&#20854;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26032;&#25554;&#27133;&#21457;&#29616;&#20219;&#21153;&#65292;&#36890;&#36807;&#20449;&#24687;&#25552;&#21462;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.04049</link><description>&lt;p&gt;
&#20027;&#21160;&#21457;&#29616;&#38754;&#21521;&#20219;&#21153;&#23545;&#35805;&#30340;&#26032;&#27133;&#20301;
&lt;/p&gt;
&lt;p&gt;
Actively Discovering New Slots for Task-oriented Conversation. (arXiv:2305.04049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04049
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20027;&#21160;&#21457;&#29616;&#38754;&#21521;&#20219;&#21153;&#23545;&#35805;&#30340;&#26032;&#27133;&#20301;&#65292;&#20197;&#25552;&#39640;&#20132;&#20114;&#24615;&#33021;&#12290;&#30001;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#26032;&#30340;&#29992;&#25143;&#38656;&#27714;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#22330;&#26223;&#65292;&#20351;&#29616;&#26377;&#30340;&#23545;&#35805;&#25628;&#32034;&#31995;&#32479;&#21464;&#24471;&#26080;&#27861;&#28385;&#36275;&#12290;&#20854;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26032;&#25554;&#27133;&#21457;&#29616;&#20219;&#21153;&#65292;&#36890;&#36807;&#20449;&#24687;&#25552;&#21462;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#25628;&#32034;&#31995;&#32479;&#20005;&#37325;&#20381;&#36182;&#20110;&#20855;&#26377;&#39044;&#23450;&#20041;&#25554;&#27133;&#21644;&#20505;&#36873;&#20540;&#38598;&#30340;&#39046;&#22495;&#26412;&#20307;&#35770;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#26032;&#30340;&#29992;&#25143;&#38656;&#27714;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#22330;&#26223;&#65292;&#36825;&#20123;&#20808;&#20915;&#26465;&#20214;&#24456;&#38590;&#28385;&#36275;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20132;&#20114;&#24615;&#33021;&#65292;&#19968;&#20123;&#24037;&#20316;&#33268;&#21147;&#20110;&#22312;&#26080;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#19979;&#26816;&#27979;&#35789;&#27719;&#34920;&#22806;&#30340;&#20540;&#25110;&#21457;&#29616;&#26032;&#30340;&#25554;&#27133;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#36807;&#20998;&#24378;&#35843;&#23545;&#35805;&#25968;&#25454;&#27169;&#24335;&#20250;&#23548;&#33268;&#36825;&#20123;&#26041;&#27861;&#20135;&#29983;&#22024;&#26434;&#21644;&#20219;&#24847;&#30340;&#25554;&#27133;&#32467;&#26524;&#12290;&#20026;&#20102;&#20419;&#36827;&#23454;&#38469;&#24212;&#29992;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479; tend to &#25552;&#20379;&#20005;&#26684;&#30340;&#20154;&#24037;&#26631;&#27880;&#37197;&#39069;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26435;&#23041;&#30340;&#26041;&#24335;&#26469;&#33719;&#24471;&#20934;&#30830;&#21644;&#26377;&#24847;&#20041;&#30340;&#25554;&#27133;&#20998;&#37197;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#25552;&#20986;&#20102;&#39640;&#35201;&#27714;&#65292;&#21363;&#39640;&#25928;&#21033;&#29992;&#36825;&#31181;&#37197;&#39069;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20197;&#20449;&#24687;&#25552;&#21462;&#30340;&#26041;&#24335;&#24418;&#25104;&#19968;&#20010;&#36890;&#29992;&#30340;&#26032;&#25554;&#27133;&#21457;&#29616;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing task-oriented conversational search systems heavily rely on domain ontologies with pre-defined slots and candidate value sets. In practical applications, these prerequisites are hard to meet, due to the emerging new user requirements and ever-changing scenarios. To mitigate these issues for better interaction performance, there are efforts working towards detecting out-of-vocabulary values or discovering new slots under unsupervised or semi-supervised learning paradigm. However, overemphasizing on the conversation data patterns alone induces these methods to yield noisy and arbitrary slot results. To facilitate the pragmatic utility, real-world systems tend to provide a stringent amount of human labelling quota, which offers an authoritative way to obtain accurate and meaningful slot assignments. Nonetheless, it also brings forward the high requirement of utilizing such quota efficiently. Hence, we formulate a general new slot discovery task in an information extraction fashio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-NAT&#65292;&#23558;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#24341;&#20837;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#24182;&#32467;&#21512;BART&#23454;&#29616;&#32479;&#19968;&#30340;&#25512;&#29702;&#21644;&#21435;&#22122;&#36807;&#31243;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#36845;&#20195;&#30340;&#33258;&#25105;&#25552;&#31034;&#31574;&#30053;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.04044</link><description>&lt;p&gt;
Diffusion-NAT: &#33258;&#25105;&#21551;&#21457;&#31163;&#25955;&#25193;&#25955;&#30340;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation. (arXiv:2305.04044v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-NAT&#65292;&#23558;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#24341;&#20837;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#24182;&#32467;&#21512;BART&#23454;&#29616;&#32479;&#19968;&#30340;&#25512;&#29702;&#21644;&#21435;&#22122;&#36807;&#31243;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#36845;&#20195;&#30340;&#33258;&#25105;&#25552;&#31034;&#31574;&#30053;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#24341;&#20837;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#20294;&#25991;&#26412;&#30340;&#31163;&#25955;&#24615;&#22686;&#21152;&#20102;&#29983;&#25104;&#36830;&#36143;&#21644;&#27969;&#30021;&#25991;&#26412;&#30340;&#38590;&#24230;&#65292;&#24182;&#24341;&#36215;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;NLP&#25216;&#26415;&#20013;&#30340;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diffusion-NAT&#65292;&#23558;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#24341;&#20837;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#24182;&#38598;&#25104;BART&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#20462;&#25913;BART&#35299;&#30721;&#36807;&#31243;&#21644;DDM&#30340;&#20856;&#22411;&#35774;&#32622;&#65292;&#25105;&#20204;&#23558;BART&#30340;&#25512;&#29702;&#36807;&#31243;&#21644;DDM&#30340;&#21435;&#22122;&#36807;&#31243;&#32479;&#19968;&#20026;&#30456;&#21516;&#30340;NAR&#25513;&#30721;&#20196;&#29260;&#36824;&#21407;&#20219;&#21153;&#12290;&#36825;&#26679;&#65292;DDM&#23601;&#21487;&#20197;&#20381;&#38752;BART&#25191;&#34892;&#21435;&#22122;&#65292;&#26082;&#21487;&#20197;&#20174;BART&#30340;&#20016;&#23500;&#39044;&#23398;&#20064;&#30693;&#35782;&#20013;&#21463;&#30410;&#65292;&#20063;&#21487;&#20197;&#20174;DDM&#30340;&#36845;&#20195;&#32454;&#21270;&#33539;&#20363;&#20013;&#21463;&#30410;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#36845;&#20195;&#30340;&#33258;&#25105;&#25552;&#31034;&#31574;&#30053;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, continuous diffusion models (CDM) have been introduced into non-autoregressive (NAR) text-to-text generation. However, the discrete nature of text increases the difficulty of CDM to generate coherent and fluent texts, and also causes the incompatibility problem between CDM and advanced NLP techniques, especially the popular pre-trained language models~(PLMs). To solve it, we propose Diffusion-NAT, which introduces discrete diffusion models~(DDM) into NAR text-to-text generation and integrates BART to improve the performance. By revising the decoding process of BART and the typical settings of DDM, we unify the inference process of BART and the denoising process of DDM into the same NAR masked tokens recovering task. In this way, DDM can rely on BART to perform denoising, which can benefit from both the rich pre-learned knowledge of BART and the iterative refining paradigm of DDM. Besides, we also propose the iterative self-prompting strategy to further improve the generation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#20248;&#21270;&#26426;&#21046;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;GPT-3.5&#27169;&#22411;&#19978;&#20351;&#29992;&#27492;&#26041;&#27861;&#65292;&#29983;&#25104;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#20197;&#19982;&#29978;&#33267;&#36229;&#36807;GPT-4&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.04039</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#20248;&#21270;&#26469;&#25552;&#21319;LLMs&#30340;&#21709;&#24212;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Refining the Responses of LLMs by Themselves. (arXiv:2305.04039v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#20248;&#21270;&#26426;&#21046;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;GPT-3.5&#27169;&#22411;&#19978;&#20351;&#29992;&#27492;&#26041;&#27861;&#65292;&#29983;&#25104;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#20197;&#19982;&#29978;&#33267;&#36229;&#36807;GPT-4&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#24037;&#31243;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#26469;&#20248;&#21270;&#20854;&#31572;&#26696;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#36741;&#21161;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#33258;&#25105;&#35780;&#20272;&#20248;&#21270;&#26426;&#21046;&#65292;&#38543;&#30528;&#36845;&#20195;&#30340;&#25512;&#36827;&#65292;&#20855;&#26377;&#25913;&#21892;&#36755;&#20986;&#36136;&#37327;&#30340;&#28508;&#21147;&#65292;&#26080;&#38656;&#25163;&#21160;&#24178;&#39044;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;GPT-3.5&#27169;&#22411;&#19978;&#20351;&#29992;&#25105;&#20204;&#30340;&#21709;&#24212;&#20248;&#21270;&#26694;&#26550;&#20135;&#29983;&#30340;&#32467;&#26524;&#19982;&#29978;&#33267;&#36229;&#36807;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#23454;&#26045;&#31574;&#30053;&#21644;&#35828;&#26126;&#24615;&#31034;&#20363;&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a simple yet efficient approach based on prompt engineering that leverages the large language model itself to optimize its answers without relying on auxiliary models. We introduce an iterative self-evaluating optimization mechanism, with the potential for improved output quality as iterations progress, removing the need for manual intervention. The experiment's findings indicate that utilizing our response refinement framework on the GPT-3.5 model yields results that are on par with, or even surpass, those generated by the cutting-edge GPT-4 model. Detailed implementation strategies and illustrative examples are provided to demonstrate the superiority of our proposed solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.04003</link><description>&lt;p&gt;
ANTONIO:&#38754;&#21521;NLP&#39564;&#35777;&#30340;&#31995;&#32479;&#21270;&#22522;&#20934;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39564;&#35777;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26041;&#27861;&#24120;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#20294;&#24182;&#19981;&#36866;&#29992;&#20110;NLP&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36896;&#25104;&#36825;&#19968;&#38382;&#39064;&#30340;&#25216;&#26415;&#21407;&#22240;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#23558;NLP&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20934;&#22791;&#20026;&#36866;&#21512;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#30340;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#23454;&#29616;&#20026;&#19968;&#20010;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#35813;&#24211;&#36830;&#25509;&#21040;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#22120;ERAN&#21644;Marabou&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;R-U-A-Robot&#30340;NLP&#25968;&#25454;&#38598;&#23545;&#24037;&#20855;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;&#25552;&#35758;&#20316;&#20026;&#39564;&#35777;&#20855;&#26377;&#27861;&#24459;&#37325;&#35201;&#24615;&#30340;NLP&#24212;&#29992;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24076;&#26395;&#65292;&#30001;&#20110;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. Here, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to known verification methods based on abstract interpretation. We implement these methods as a Python library called ANTONIO that links to the neural network verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems withi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#30417;&#30563;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22797;&#21046;&#20154;&#31867;&#22797;&#26434;&#30340;&#23545;&#35805;&#31574;&#30053;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03987</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#19982;&#30417;&#30563;&#27491;&#21017;&#21270;&#22797;&#21046;&#20154;&#31867;&#22797;&#26434;&#30340;&#23545;&#35805;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Replicating Complex Dialogue Policy of Humans via Offline Imitation Learning with Supervised Regularization. (arXiv:2305.03987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#30417;&#30563;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22797;&#21046;&#20154;&#31867;&#22797;&#26434;&#30340;&#23545;&#35805;&#31574;&#30053;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#30417;&#30563;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#65292;&#20174;&#32780;&#23398;&#20064;&#30495;&#23454;&#23545;&#35805;&#25968;&#25454;&#38598;&#20013;&#30340;&#31574;&#30053;&#12290;&#36825;&#20010;&#27169;&#22411;&#19981;&#38656;&#35201;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy learning (PL) is a module of a task-oriented dialogue system that trains an agent to make actions in each dialogue turn. Imitating human action is a fundamental problem of PL. However, both supervised learning (SL) and reinforcement learning (RL) frameworks cannot imitate humans well. Training RL models require online interactions with user simulators, while simulating complex human policy is hard. Performances of SL-based models are restricted because of the covariate shift problem. Specifically, a dialogue is a sequential decision-making process where slight differences in current utterances and actions will cause significant differences in subsequent utterances. Therefore, the generalize ability of SL models is restricted because statistical characteristics of training and testing dialogue data gradually become different. This study proposed an offline imitation learning model that learns policy from real dialogue datasets and does not require user simulators. It also utilize
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35838;&#31243;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19977;&#20010;&#33258;&#30417;&#30563;&#35838;&#31243;&#21644;&#20004;&#20010;&#33258;&#25105;&#20462;&#27491;&#35838;&#31243;&#65292;&#20197;&#24179;&#34913;&#26631;&#31614;&#21644;&#20805;&#20998;&#21033;&#29992;&#29983;&#25104;&#22120;&#21644;&#36776;&#21035;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2305.03981</link><description>&lt;p&gt;
&#20316;&#20026;&#22810;&#35282;&#24230;&#35838;&#31243;&#23398;&#20064;&#26426;&#22120;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pre-training Language Model as a Multi-perspective Course Learner. (arXiv:2305.03981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35838;&#31243;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19977;&#20010;&#33258;&#30417;&#30563;&#35838;&#31243;&#21644;&#20004;&#20010;&#33258;&#25105;&#20462;&#27491;&#35838;&#31243;&#65292;&#20197;&#24179;&#34913;&#26631;&#31614;&#21644;&#20805;&#20998;&#21033;&#29992;&#29983;&#25104;&#22120;&#21644;&#36776;&#21035;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ELECTRA&#26159;&#19968;&#20010;&#29983;&#25104;&#22120;-&#37492;&#21035;&#22120;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#20041;&#26500;&#24314;&#33021;&#21147;&#12290;&#23613;&#31649;&#24615;&#33021;&#20196;&#20154;&#20449;&#26381;&#65292;&#20294;ELECTRA&#20173;&#38754;&#20020;&#21333;&#35843;&#30340;&#35757;&#32451;&#21644;&#19981;&#36275;&#30340;&#20132;&#20114;&#25361;&#25112;&#12290;&#21482;&#26377;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30340;&#29983;&#25104;&#22120;&#20250;&#23548;&#33268;&#23545;&#20110;&#36776;&#21035;&#22120;&#30340;&#23398;&#20064;&#25928;&#29575;&#19979;&#38477;&#30340;&#20559;&#21521;&#24615;&#23398;&#20064;&#21644;&#26631;&#31614;&#19981;&#24179;&#34913;;&#21516;&#26102;&#27809;&#26377;&#26126;&#30830;&#30340;&#36776;&#21035;&#22120;&#21040;&#29983;&#25104;&#22120;&#30340;&#21453;&#39304;&#29615;&#36335;&#23548;&#33268;&#36825;&#20004;&#20010;&#32452;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#26410;&#20805;&#20998;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35838;&#31243;&#23398;&#20064;&#65288;MCL&#65289;&#26041;&#27861;&#65292;&#20197;&#22810;&#20010;&#35282;&#24230;&#33719;&#21462;&#26679;&#26412;&#39640;&#25928;&#39044;&#35757;&#32451;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#29983;&#25104;&#22120;&#21644;&#36776;&#21035;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35774;&#35745;&#20102;&#19977;&#20010;&#33258;&#30417;&#30563;&#35838;&#31243;&#26469;&#20943;&#36731;MLM&#30340;&#22266;&#26377;&#32570;&#38519;&#65292;&#24182;&#20197;&#22810;&#35282;&#24230;&#26041;&#24335;&#24179;&#34913;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#33258;&#25105;&#20462;&#27491;&#35838;&#31243;&#26469;&#24357;&#21512;&#36776;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#20805;&#20998;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
ELECTRA, the generator-discriminator pre-training framework, has achieved impressive semantic construction capability among various downstream tasks. Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm between these two components, underutilizing the course learning. In this study, a multi-perspective course learning (MCL) method is proposed to fetch a many degrees and visual angles for sample-efficient pre-training, and to fully leverage the relationship between generator and discriminator. Concretely, three self-supervision courses are designed to alleviate inherent flaws of MLM and balance the label in a multi-perspective way. Besides, two self-correction courses are proposed to bridge the cha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#38750;&#33258;&#22238;&#24402;Transformer&#27169;&#22411;&#29992;&#20110;&#23545;&#19981;&#23436;&#25972;&#20449;&#24687;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#20854;&#20855;&#26377;&#20301;&#32622;&#24863;&#30693;&#33258;&#35843;&#33410;&#21644;&#20381;&#36182;&#25509;&#21475;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#19982;&#20854;&#20182;&#20027;&#27969;&#27169;&#22411;&#30456;&#27604;&#26356;&#24555;&#30340;&#35299;&#30721;&#26102;&#38388;&#20869;&#33719;&#24471;&#21487;&#27604;&#36739;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#28508;&#22312;&#25554;&#20540;&#31561;&#24212;&#29992;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03977</link><description>&lt;p&gt;
&#19968;&#31181;&#24102;&#26377;&#19981;&#23436;&#25972;&#20449;&#24687;&#30340;&#25991;&#26412;&#29983;&#25104;&#23545;&#25239;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Non-Autoregressive Model for Text Generation with Incomplete Information. (arXiv:2305.03977v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03977
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#38750;&#33258;&#22238;&#24402;Transformer&#27169;&#22411;&#29992;&#20110;&#23545;&#19981;&#23436;&#25972;&#20449;&#24687;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#20854;&#20855;&#26377;&#20301;&#32622;&#24863;&#30693;&#33258;&#35843;&#33410;&#21644;&#20381;&#36182;&#25509;&#21475;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#19982;&#20854;&#20182;&#20027;&#27969;&#27169;&#22411;&#30456;&#27604;&#26356;&#24555;&#30340;&#35299;&#30721;&#26102;&#38388;&#20869;&#33719;&#24471;&#21487;&#27604;&#36739;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#28508;&#22312;&#25554;&#20540;&#31561;&#24212;&#29992;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#23436;&#25972;&#20449;&#24687;&#24773;&#20917;&#65288;CIS&#65289;&#19979;&#24050;&#24191;&#27867;&#30740;&#31350;&#65292;&#20854;&#20013;&#27169;&#22411;&#20855;&#26377;&#23436;&#25972;&#30340;&#36755;&#20837;&#20449;&#24687;&#26469;&#33719;&#21462;&#30456;&#24212;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19981;&#23436;&#25972;&#20449;&#24687;&#24773;&#20917;&#65288;IIS&#65289;&#19979;&#30340;&#25506;&#32034;&#26497;&#20026;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;IIS&#20013;&#19981;&#23436;&#25972;&#30340;&#36755;&#20837;&#20449;&#24687;&#23558;&#22686;&#21152;&#22312;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#19979;&#35757;&#32451;&#30340;&#29616;&#26377;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;IIS&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#38750;&#33258;&#22238;&#24402;Transformer &#65288;ANT&#65289;&#27169;&#22411;&#65292;&#20855;&#26377;&#20004;&#20010;&#26032;&#29305;&#24615;&#65306;1&#65289;&#20301;&#32622;&#24863;&#30693;&#33258;&#35843;&#33410;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#21512;&#29702;&#30340;&#38544;&#34255;&#34920;&#31034;&#65307;2&#65289;&#20381;&#36182;&#24615;&#21069;&#39304;&#32593;&#32476;&#65292;&#21487;&#20197;&#22686;&#24378;&#20854;&#20381;&#36182;&#24615;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;ANT&#19982;IIS&#20013;&#30340;&#20854;&#20182;&#20027;&#27969;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;ANT&#21487;&#20197;&#23454;&#29616;&#21487;&#27604;&#36739;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#24555;&#22320;&#36827;&#34892;&#35299;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ANT&#22312;&#28508;&#22312;&#25554;&#20540;&#31561;&#21508;&#31181;&#24212;&#29992;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive models have been widely studied in the Complete Information Scenario (CIS), in which the models have complete input information to obtain corresponding output. However, their explorations in the Incomplete Information Scenario (IIS) are extremely limited. Our analyses reveal that the IIS's incomplete input information will augment the inherent limitations of existing non-autoregressive models trained under Maximum Likelihood Estimation. In this paper, we propose for the IIS an Adversarial Non-autoregressive Transformer (ANT) which has two novel features: 1) Position Aware Self-Modulation to provide more reasonable hidden representations, and 2) Dependency Feed Forward Network to strengthen its capacity in dependency modeling. We compare ANT with other mainstream models in the IIS and demonstrate that ANT can achieve comparable performance with much fewer decoding iterations. Furthermore, we show its great potential in various applications like latent interpolation an
&lt;/p&gt;</description></item><item><title>DiscoPrompt&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#23618;&#27425;&#32467;&#26500;&#20869;&#37096;&#30340;&#36335;&#24452;&#26469;&#35782;&#21035;&#38544;&#21547;&#35821;&#31687;&#20851;&#31995;&#65292;&#30456;&#27604;&#20110;&#20197;&#24448;&#30340;&#32431;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.03973</link><description>&lt;p&gt;
DiscoPrompt: &#38544;&#21547;&#35821;&#31687;&#20851;&#31995;&#35782;&#21035;&#20013;&#30340;&#36335;&#24452;&#39044;&#27979;&#25552;&#31034;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
DiscoPrompt: Path Prediction Prompt Tuning for Implicit Discourse Relation Recognition. (arXiv:2305.03973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03973
&lt;/p&gt;
&lt;p&gt;
DiscoPrompt&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#23618;&#27425;&#32467;&#26500;&#20869;&#37096;&#30340;&#36335;&#24452;&#26469;&#35782;&#21035;&#38544;&#21547;&#35821;&#31687;&#20851;&#31995;&#65292;&#30456;&#27604;&#20110;&#20197;&#24448;&#30340;&#32431;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21547;&#35821;&#31687;&#20851;&#31995;&#35782;&#21035;(IDRR)&#26159;&#19968;&#39033;&#22797;&#26434;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#35782;&#21035;&#21442;&#25968;&#20043;&#38388;&#30340;&#35805;&#35821;&#20851;&#31995;&#65292;&#20854;&#20013;&#19981;&#23384;&#22312;&#35805;&#35821;&#36830;&#25509;&#35789;&#12290;&#22312;&#27880;&#37322;&#36807;&#31243;&#20013;&#65292;&#27599;&#20010;&#35805;&#35821;&#20851;&#31995;&#30340;&#35821;&#20041;&#26631;&#31614;&#36981;&#24490;&#19968;&#31181;&#23618;&#27425;&#20998;&#31867;&#20307;&#31995;(Prasad&#31561;&#20154;&#65292;2008)&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#20316;&#21697;&#27809;&#26377;&#24456;&#22909;&#22320;&#25972;&#21512;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#32431;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#21477;&#27861;&#29305;&#24449;&#21644;&#36830;&#35789;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#39044;&#27979;&#23618;&#27425;&#32467;&#26500;&#20869;&#37096;&#30340;&#36335;&#24452;(&#20363;&#22914;&#65292;&#8220;&#27604;&#36739;-&gt;&#23545;&#27604;-&gt;&#28982;&#32780;&#8221;)&#27604;&#39044;&#27979;&#24179;&#38754;&#26631;&#31614;(&#20363;&#22914;&#65292;&#23545;&#27604;)&#25110;&#36830;&#35789;(&#20363;&#22914;&#65292;&#28982;&#32780;)&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#36335;&#24452;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;IDRR&#20013;&#23618;&#27425;&#32467;&#26500;&#20043;&#38388;&#30340;&#20132;&#20114;&#20449;&#24687;&#21644;&#20869;&#22312;&#35821;&#20041;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#25552;&#31034;&#20248;&#21270;&#23558;&#36825;&#31181;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Discourse Relation Recognition (IDRR) is a sophisticated and challenging task to recognize the discourse relations between the arguments with the absence of discourse connectives. The sense labels for each discourse relation follow a hierarchical classification scheme in the annotation process (Prasad et al., 2008), forming a hierarchy structure. Most existing works do not well incorporate the hierarchy structure but focus on the syntax features and the prior knowledge of connectives in the manner of pure text classification. We argue that it is more effective to predict the paths inside the hierarchical tree (e.g., "Comparison -&gt; Contrast -&gt; however") rather than flat labels (e.g., Contrast) or connectives (e.g., however). We propose a prompt-based path prediction method to utilize the interactive information and intrinsic senses among the hierarchy in IDRR. This is the first work that injects such structure information into pre-trained language models via prompt tuning, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#23485;&#26494;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20026;&#38382;&#31572;&#31995;&#32479;&#32508;&#21512;&#20869;&#22806;&#20998;&#24067;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#26174;&#31034;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03971</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23485;&#26494;&#20248;&#21270;&#29992;&#20110;&#24378;&#38887;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Adaptive loose optimization for robust question answering. (arXiv:2305.03971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#23485;&#26494;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20026;&#38382;&#31572;&#31995;&#32479;&#32508;&#21512;&#20869;&#22806;&#20998;&#24067;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#26174;&#31034;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#26041;&#27861;&#20197;&#21033;&#29992;&#25968;&#25454;&#20559;&#24046;&#20026;&#29305;&#28857;&#65292;&#22914;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#35821;&#35328;&#20808;&#39564;&#21644;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;&#25277;&#21462;&#24335;&#38382;&#31572;&#65289;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#30446;&#21069;&#30340;&#21435;&#20559;&#26041;&#27861;&#24448;&#24448;&#20197;&#22312;&#20998;&#24067;&#20869;&#34920;&#29616;&#19981;&#20339;&#20026;&#20195;&#20215;&#33719;&#24471;&#26377;&#21033;&#30340;&#20998;&#24067;&#22806;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#21435;&#20559;&#26041;&#27861;&#21017;&#22312;&#33719;&#24471;&#39640;&#20998;&#24067;&#20869;&#34920;&#29616;&#30340;&#21516;&#26102;&#29306;&#29298;&#20102;&#30456;&#24403;&#25968;&#37327;&#30340;&#20998;&#24067;&#22806;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#38590;&#20197;&#24212;&#23545;&#22797;&#26434;&#21464;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#23485;&#26494;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#20026;&#38382;&#31572;&#31995;&#32479;&#32508;&#21512;&#20004;&#32773;&#26368;&#20339;&#34920;&#29616;&#32780;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#26159;&#26681;&#25454;&#23567;&#25209;&#37327;&#35757;&#32451;&#25968;&#25454;&#19978;&#20808;&#21069;&#21644;&#24403;&#21069;&#20248;&#21270;&#29366;&#24577;&#20043;&#38388;&#30340;&#27604;&#29575;&#33258;&#36866;&#24212;&#22320;&#20943;&#23569;&#25439;&#22833;&#12290;&#36825;&#31181;&#23485;&#26494;&#20248;&#21270;&#21487;&#20197;&#29992;&#26469;&#38450;&#27490;&#38750;&#20984;&#20248;&#21270;&#38519;&#20837;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#24182;&#24110;&#21161;&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering methods are well-known for leveraging data bias, such as the language prior in visual question answering and the position bias in machine reading comprehension (extractive question answering). Current debiasing methods often come at the cost of significant in-distribution performance to achieve favorable out-of-distribution generalizability, while non-debiasing methods sacrifice a considerable amount of out-of-distribution performance in order to obtain high in-distribution performance. Therefore, it is challenging for them to deal with the complicated changing real-world situations. In this paper, we propose a simple yet effective novel loss function with adaptive loose optimization, which seeks to make the best of both worlds for question answering. Our main technical contribution is to reduce the loss adaptively according to the ratio between the previous and current optimization state on mini-batch training data. This loose optimization can be used to prevent non
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;NER-to-MRC&#30340;&#26041;&#27861;&#65292;&#23558;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;&#23436;&#20840;&#20316;&#20026;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#36328;&#24230;&#30456;&#20851;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;NER&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21482;&#20351;&#29992;&#20102;&#19968;&#23567;&#37096;&#20998;&#39069;&#22806;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.03970</link><description>&lt;p&gt;
&#20174;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21040;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65306;&#19968;&#20010;&#23436;&#25972;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NER-to-MRC: Named-Entity Recognition Completely Solving as Machine Reading Comprehension. (arXiv:2305.03970v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;NER-to-MRC&#30340;&#26041;&#27861;&#65292;&#23558;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;&#23436;&#20840;&#20316;&#20026;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#36328;&#24230;&#30456;&#20851;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;NER&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21482;&#20351;&#29992;&#20102;&#19968;&#23567;&#37096;&#20998;&#39069;&#22806;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#27169;&#22359;&#65292;&#29992;&#20110;&#26816;&#27979;&#24102;&#26377;&#39044;&#23450;&#20041;&#35821;&#20041;&#26631;&#31614;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;NER&#30740;&#31350;&#38598;&#20013;&#20110;&#21033;&#29992;&#22823;&#37327;&#39069;&#22806;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21644;&#38598;&#25104;&#25628;&#32034;&#24341;&#25806;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#19982;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#35757;&#32451;&#30456;&#20851;&#30340;&#39640;&#25104;&#26412;&#65292;&#20197;&#21450;&#20174;&#25628;&#32034;&#24341;&#25806;&#26816;&#32034;&#30340;&#25968;&#25454;&#30340;&#39069;&#22806;&#35757;&#32451;&#36807;&#31243;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#20511;&#37492;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#21644;&#23427;&#30340;&#26377;&#25928;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#23558;NER&#23436;&#20840;&#26694;&#26550;&#20026;MRC&#38382;&#39064;&#65292;&#31216;&#20043;&#20026;NER-to-MRC&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;MRC&#30740;&#31350;&#33268;&#21147;&#20110;&#20351;&#29992;MRC&#35299;&#20915;NER&#38382;&#39064;&#65292;&#20294;&#20173;&#23384;&#22312;&#20960;&#20010;&#25361;&#25112;&#65306;i&#65289;&#20381;&#36182;&#25163;&#21160;&#35774;&#35745;&#30340;&#25552;&#31034;&#65307;ii&#65289;&#38480;&#21046;MRC&#26041;&#27861;&#23545;&#25968;&#25454;&#37325;&#24314;&#30340;&#33021;&#21147;&#65292;&#26080;&#27861;&#36798;&#21040;&#21033;&#29992;&#22823;&#37327;&#39069;&#22806;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;NER-to-MRC&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;NER&#38382;&#39064;&#37325;&#26032;&#38416;&#37322;&#20026;MRC&#20013;&#30340;&#36328;&#24230;&#25552;&#21462;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#24230;&#30456;&#20851;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#20102;&#19968;&#23567;&#37096;&#20998;&#39069;&#22806;&#25968;&#25454;&#65292;&#23601;&#23454;&#29616;&#20102;&#27604;&#20197;&#21069;&#30340;NER&#20316;&#21697;&#26356;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named-entity recognition (NER) detects texts with predefined semantic labels and is an essential building block for natural language processing (NLP). Notably, recent NER research focuses on utilizing massive extra data, including pre-training corpora and incorporating search engines. However, these methods suffer from high costs associated with data collection and pre-training, and additional training process of the retrieved data from search engines. To address the above challenges, we completely frame NER as a machine reading comprehension (MRC) problem, called NER-to-MRC, by leveraging MRC with its ability to exploit existing data efficiently. Several prior works have been dedicated to employing MRC-based solutions for tackling the NER problem, several challenges persist: i) the reliance on manually designed prompts; ii) the limited MRC approaches to data reconstruction, which fails to achieve performance on par with methods utilizing extensive additional data. Thus, our NER-to-MRC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;PET&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32858;&#31867;&#27969;&#31243;&#23454;&#20307;&#30340;&#25552;&#21450;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#25216;&#26415;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#21160;&#21019;&#24314;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#30340;&#32321;&#29712;&#24037;&#20316;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#21516;&#19968;&#27969;&#31243;&#23454;&#20307;&#37325;&#22797;&#25552;&#21450;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03960</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#29983;&#25104;&#27969;&#31243;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#35268;&#21017;&#20043;&#22806;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;(arXiv:2305.03960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Beyond Rule-based Named Entity Recognition and Relation Extraction for Process Model Generation from Natural Language Text. (arXiv:2305.03960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;PET&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32858;&#31867;&#27969;&#31243;&#23454;&#20307;&#30340;&#25552;&#21450;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#25216;&#26415;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#21160;&#21019;&#24314;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#30340;&#32321;&#29712;&#24037;&#20316;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#21516;&#19968;&#27969;&#31243;&#23454;&#20307;&#37325;&#22797;&#25552;&#21450;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#33258;&#21160;&#29983;&#25104;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#20852;&#26041;&#27861;&#65292;&#21487;&#36991;&#20813;&#25163;&#21160;&#21019;&#24314;&#24418;&#24335;&#21270;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#38656;&#35201;&#20174;&#25991;&#26412;&#27969;&#31243;&#25551;&#36848;&#20013;&#25552;&#21462;&#20986;&#27969;&#31243;&#23454;&#20307;&#65288;&#22914;&#21442;&#19982;&#32773;&#12289;&#27963;&#21160;&#12289;&#23545;&#35937;&#31561;&#65289;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#24102;&#26377;&#25991;&#26412;&#27969;&#31243;&#25551;&#36848;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;(PET)&#24050;&#32463;&#20986;&#29256;&#65292;&#20854;&#20276;&#38543;&#30528;&#19968;&#31181;&#22522;&#26412;&#30340;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#24403;&#21069;&#29366;&#24577;&#19979;&#65292;PET&#32570;&#20047;&#26377;&#20851;&#20004;&#20010;&#25552;&#21450;&#26159;&#21542;&#25351;&#20195;&#20102;&#30456;&#21516;&#25110;&#19981;&#21516;&#30340;&#27969;&#31243;&#23454;&#20307;&#30340;&#20449;&#24687;&#65292;&#36825;&#23545;&#20110;&#26159;&#21542;&#22312;&#30446;&#26631;&#27169;&#22411;&#20013;&#21019;&#24314;&#19968;&#20010;&#25110;&#20004;&#20010;&#24314;&#27169;&#20803;&#32032;&#30340;&#37325;&#35201;&#20915;&#31574;&#30456;&#23545;&#24212;&#12290;&#22240;&#27492;&#65292;&#20363;&#22914;&#65292;&#20004;&#20010;&#25968;&#25454;&#22788;&#29702;&#30340;&#25552;&#21450;&#26159;&#21542;&#24847;&#21619;&#30528;&#22788;&#29702;&#19981;&#21516;&#25110;&#30456;&#21516;&#30340;&#25968;&#25454;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#31867;&#27969;&#31243;&#23454;&#20307;&#30340;&#25552;&#21450;&#26469;&#25193;&#23637;PET&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#25216;&#26415;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Automated generation of business process models from natural language text is an emerging methodology for avoiding the manual creation of formal business process models. For this purpose, process entities like actors, activities, objects etc., and relations among them are extracted from textual process descriptions. A high-quality annotated corpus of textual process descriptions (PET) has been published accompanied with a basic process extraction approach. In its current state, however, PET lacks information about whether two mentions refer to the same or different process entities, which corresponds to the crucial decision of whether to create one or two modeling elements in the target model. Consequently, it is ambiguous whether, for instance, two mentions of data processing mean processing of different, or the same data. In this paper, we extend the PET dataset by clustering mentions of process entities and by proposing a new baseline technique for process extraction equipped with a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#37325;&#21442;&#25968;&#21270;&#30340;Prompt Tuning&#25913;&#36827;&#26041;&#27861;-Residual Prompt Tuning&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35843;&#20248;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#22312;&#36229;&#36807;Prompt Tuning 7&#20010;&#28857;&#65292;&#19988;&#21487;&#20197;&#32553;&#30701;Prompt&#38271;&#24230;10&#20493;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#21516;&#26102;&#23545;&#20110;&#23398;&#20064;&#29575;&#21644;Prompt&#21021;&#22987;&#21270;&#30340;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03937</link><description>&lt;p&gt;
&#22522;&#20110;&#27531;&#24046;&#37325;&#21442;&#25968;&#21270;&#30340;Prompt Tuning&#25913;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization. (arXiv:2305.03937v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#37325;&#21442;&#25968;&#21270;&#30340;Prompt Tuning&#25913;&#36827;&#26041;&#27861;-Residual Prompt Tuning&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35843;&#20248;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#22312;&#36229;&#36807;Prompt Tuning 7&#20010;&#28857;&#65292;&#19988;&#21487;&#20197;&#32553;&#30701;Prompt&#38271;&#24230;10&#20493;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#21516;&#26102;&#23545;&#20110;&#23398;&#20064;&#29575;&#21644;Prompt&#21021;&#22987;&#21270;&#30340;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Tuning&#26159;&#30446;&#21069;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#25928;&#29575;&#30340;&#19968;&#31181;&#25104;&#21151;&#26041;&#27861;&#12290;&#23613;&#31649;&#20854;&#21442;&#25968;&#25928;&#29575;&#26368;&#39640;&#65288;&#35843;&#25972;&#30340;soft prompts&#19981;&#21040;&#24635;&#21442;&#25968;&#30340;0.1%&#65289;&#65292;&#20294;&#23427;&#36890;&#24120;&#34920;&#29616;&#27604;&#20854;&#20182;&#25928;&#29575;&#39640;&#30340;&#35843;&#20248;&#26041;&#27861;&#26356;&#24046;&#65292;&#24182;&#19988;&#23545;&#36229;&#21442;&#25968;&#38750;&#24120;&#25935;&#24863;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;Residual Prompt Tuning&#26041;&#27861;&#65292;&#21487;&#26174;&#33879;&#25913;&#21892;Prompt Tuning&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24102;&#26377;&#27531;&#24046;&#36830;&#25509;&#30340;&#27973;&#23618;&#32593;&#32476;&#23545;&#36719;Prompt&#30340;&#37325;&#21442;&#25968;&#21270;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;Residual Prompt Tuning&#26126;&#26174;&#20248;&#20110;Prompt Tuning&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;T5-Base&#30456;&#27604;&#65292;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23558;Prompt&#38271;&#24230;&#32553;&#30701;&#20102;10&#20493;&#65292;&#19988;&#23545;&#20110;&#23398;&#20064;&#29575;&#21644;Prompt&#21021;&#22987;&#21270;&#30340;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#20063;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning is one of the successful approaches for parameter-efficient tuning of pre-trained language models. Despite being arguably the most parameter-efficient (tuned soft prompts constitute &lt;0.1% of total parameters), it typically performs worse than other efficient tuning methods and is quite sensitive to hyper-parameters. In this work, we introduce Residual Prompt Tuning - a simple and efficient method that significantly improves the performance and stability of prompt tuning. We propose to reparameterize soft prompt embeddings using a shallow network with a residual connection. Our experiments show that Residual Prompt Tuning significantly outperforms prompt tuning on SuperGLUE benchmark. Notably, our method reaches +7 points improvement over prompt tuning with T5-Base and allows to reduce the prompt length by 10x without hurting performance. In addition, we show that our approach is robust to the choice of learning rate and prompt initialization, and is effective in few-shot 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#20027;&#21160;&#23398;&#20064;&#20219;&#21153;&#30340;&#20027;&#21160;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#22810;&#31181;&#20027;&#21160;&#21644;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#26377;&#25928;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#36951;&#24536;-&#23398;&#20064;&#26354;&#32447;&#26041;&#27861;&#26469;&#24179;&#34913;&#19981;&#24536;&#26087;&#30693;&#35782;&#21644;&#24555;&#36895;&#23398;&#20064;&#30340;&#20004;&#20010;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.03923</link><description>&lt;p&gt;
&#20027;&#21160;&#30340;&#36830;&#32493;&#23398;&#20064;&#65306;&#22312;&#20219;&#21153;&#24207;&#21015;&#20013;&#26631;&#35760;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Continual Learning: Labelling Queries in a Sequence of Tasks. (arXiv:2305.03923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#20027;&#21160;&#23398;&#20064;&#20219;&#21153;&#30340;&#20027;&#21160;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#22810;&#31181;&#20027;&#21160;&#21644;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#26377;&#25928;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#36951;&#24536;-&#23398;&#20064;&#26354;&#32447;&#26041;&#27861;&#26469;&#24179;&#34913;&#19981;&#24536;&#26087;&#30693;&#35782;&#21644;&#24555;&#36895;&#23398;&#20064;&#30340;&#20004;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#20013;&#65292;&#33719;&#21462;&#26032;&#30693;&#35782;&#24182;&#19981;&#24536;&#35760;&#24050;&#23398;&#20869;&#23481;&#26159;&#20854;&#26680;&#24515;&#12290;&#32780;&#20219;&#21153;&#26159;&#25353;&#39034;&#24207;&#20986;&#29616;&#30340;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#20934;&#22791;&#21644;&#27880;&#37322;&#21017;&#36890;&#24120;&#26159;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#36830;&#32493;&#23398;&#20064;&#26469;&#36866;&#24212;&#26032;&#30340;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#20219;&#21153;&#30340;&#20027;&#21160;&#36830;&#32493;&#23398;&#20064;&#65288;ACL&#65289;&#20013;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#20219;&#21153;&#21253;&#25324;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#27744;&#21644;&#19968;&#20010;&#27880;&#37322;&#39044;&#31639;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;AL&#21644;CL&#31639;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#65292;&#31867;&#21035;&#21644;&#20219;&#21153;&#22686;&#37327;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#25581;&#31034;&#20102;&#19981;&#24536;&#26087;&#30693;&#35782;&#21644;&#24555;&#36895;&#23398;&#20064;&#22312;CL&#21644;AL&#20013;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23613;&#31649;&#22312;&#20197;&#21069;&#20219;&#21153;&#30340;&#27880;&#37322;&#25910;&#38598;&#19978;&#26465;&#20214;&#26597;&#35810;&#31574;&#30053;&#20250;&#25552;&#39640;&#39046;&#22495;&#21644;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;&#36951;&#24536;-&#23398;&#20064;&#26354;&#32447;&#21017;&#26356;&#22909;&#22320;&#24179;&#34913;&#20102;&#36825;&#20004;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring new knowledge without forgetting what has been learned in a sequence of tasks is the central focus of continual learning (CL). While tasks arrive sequentially, the training data are often prepared and annotated independently, leading to CL of incoming supervised learning tasks. This paper considers the under-explored problem of active continual learning (ACL) for a sequence of active learning (AL) tasks, where each incoming task includes a pool of unlabelled data and an annotation budget. We investigate the effectiveness and interplay between several AL and CL algorithms in the domain, class and task-incremental scenarios. Our experiments reveal the trade-off between two contrasting goals of not forgetting the old knowledge and the ability to quickly learn in CL and AL. While conditioning the query strategy on the annotations collected for the previous tasks leads to improved task performance on the domain and task incremental learning, our proposed forgetting-learning profil
&lt;/p&gt;</description></item><item><title>&#20167;&#24680;&#35328;&#35770;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25991;&#26412;&#23186;&#20307;&#19978;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#26816;&#27979;&#20167;&#24680;&#35270;&#39057;&#24182;&#20174;&#35270;&#39057;&#20849;&#20139;&#24179;&#21488;&#20013;&#21024;&#38500;&#20167;&#24680;&#20869;&#23481;&#25104;&#20026;&#37325;&#28857;&#12290;&#22240;&#20026;&#20167;&#24680;&#35270;&#39057;&#30340;&#22270;&#20687;&#21644;&#38899;&#39057;&#20013;&#23384;&#22312;&#21508;&#31181;&#32447;&#32034;&#65292;&#22240;&#27492;&#25105;&#20204;&#26500;&#24314;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.03915</link><description>&lt;p&gt;
HateMM&#65306;&#29992;&#20110;&#20167;&#24680;&#35270;&#39057;&#20998;&#31867;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HateMM: A Multi-Modal Dataset for Hate Video Classification. (arXiv:2305.03915v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03915
&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25991;&#26412;&#23186;&#20307;&#19978;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#26816;&#27979;&#20167;&#24680;&#35270;&#39057;&#24182;&#20174;&#35270;&#39057;&#20849;&#20139;&#24179;&#21488;&#20013;&#21024;&#38500;&#20167;&#24680;&#20869;&#23481;&#25104;&#20026;&#37325;&#28857;&#12290;&#22240;&#20026;&#20167;&#24680;&#35270;&#39057;&#30340;&#22270;&#20687;&#21644;&#38899;&#39057;&#20013;&#23384;&#22312;&#21508;&#31181;&#32447;&#32034;&#65292;&#22240;&#27492;&#25105;&#20204;&#26500;&#24314;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#24050;&#25104;&#20026;&#29616;&#20195;&#31038;&#20250;&#20013;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#65292;&#23545;&#32447;&#19978;&#21644;&#32447;&#19979;&#19990;&#30028;&#37117;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#30001;&#20110;&#36825;&#19968;&#28857;&#65292;&#36817;&#26469;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#25991;&#26412;&#23186;&#20307;&#19978;&#65292;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26089;&#26399;&#30340;&#33258;&#21160;&#21270;&#35270;&#39057;&#23457;&#26680;&#25216;&#26415;&#26469;&#22788;&#29702;&#34987;&#19978;&#20256;&#30340;&#35270;&#39057;&#65292;&#20197;&#20445;&#25345;&#24179;&#21488;&#30340;&#23433;&#20840;&#21644;&#20581;&#24247;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#20351;&#29992;&#22810;&#27169;&#24335;&#26816;&#27979;&#20167;&#24680;&#35270;&#39057;&#24182;&#20174;&#35270;&#39057;&#20849;&#20139;&#24179;&#21488;&#20013;&#26816;&#27979;&#21644;&#21024;&#38500;&#20167;&#24680;&#20869;&#23481;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;BitChute&#20013;&#31574;&#21010;&#20102;&#32422;43&#20010;&#23567;&#26102;&#30340;&#35270;&#39057;&#65292;&#24182;&#25163;&#21160;&#27880;&#37322;&#20102;&#23427;&#20204;&#26159;&#21542;&#23646;&#20110;&#20167;&#24680;&#25110;&#38750;&#20167;&#24680;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#20197;&#35299;&#37322;&#26631;&#27880;&#20915;&#31574;&#30340;&#24103;&#36328;&#24230;&#12290;&#25105;&#20204;&#21033;&#29992;&#20167;&#24680;&#35789;&#27719;&#34920;&#30340;&#25628;&#32034;&#20851;&#38190;&#23383;&#25910;&#38598;&#30456;&#20851;&#30340;&#35270;&#39057;&#65292;&#35266;&#23519;&#21040;&#22312;&#20167;&#24680;&#35270;&#39057;&#30340;&#22270;&#20687;&#21644;&#38899;&#39057;&#20013;&#23384;&#22312;&#21508;&#31181;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech has become one of the most significant issues in modern society, having implications in both the online and the offline world. Due to this, hate speech research has recently gained a lot of traction. However, most of the work has primarily focused on text media with relatively little work on images and even lesser on videos. Thus, early stage automated video moderation techniques are needed to handle the videos that are being uploaded to keep the platform safe and healthy. With a view to detect and remove hateful content from the video sharing platforms, our work focuses on hate video detection using multi-modalities. To this end, we curate ~43 hours of videos from BitChute and manually annotate them as hate or non-hate, along with the frame spans which could explain the labelling decision. To collect the relevant videos we harnessed search keywords from hate lexicons. We observe various cues in images and audio of hateful videos. Further, we build deep learning multi-modal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#24403;&#21069;&#30340;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#23384;&#22312;&#30456;&#24403;&#20005;&#37325;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#35265;&#26041;&#27861;&#20197;&#20943;&#36731;&#27492;&#31867;&#20559;&#35265;&#24182;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03881</link><description>&lt;p&gt;
&#22270;&#20687;&#25628;&#32034;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#20851;&#20110;&#20174;&#22270;&#20687;&#26816;&#32034;&#19982;&#21435;&#20559;&#35265;&#35282;&#24230;&#25506;&#31350;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness in Image Search: A Study of Occupational Stereotyping in Image Retrieval and its Debiasing. (arXiv:2305.03881v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#24403;&#21069;&#30340;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#23384;&#22312;&#30456;&#24403;&#20005;&#37325;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#35265;&#26041;&#27861;&#20197;&#20943;&#36731;&#27492;&#31867;&#20559;&#35265;&#24182;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#25628;&#32034;&#24341;&#25806;&#36817;&#24180;&#26469;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#21644;&#24191;&#27867;&#30340;&#20351;&#29992;&#65292;&#25104;&#20026;&#32487;&#20449;&#24687;&#26816;&#32034;&#20043;&#21518;&#31532;&#20108;&#24120;&#35265;&#30340;&#20114;&#32852;&#32593;&#20351;&#29992;&#26041;&#24335;&#12290;&#23613;&#31649;&#25628;&#32034;&#24341;&#25806;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26381;&#21153;&#65292;&#20294;&#22270;&#20687;&#25628;&#32034;&#39046;&#22495;&#26368;&#36817;&#25104;&#20026;&#20449;&#24687;&#26816;&#32034;&#31038;&#21306;&#30340;&#28966;&#28857;&#65292;&#22240;&#20026;&#24120;&#35328;&#36947;&#8220;&#19968;&#22270;&#32988;&#21315;&#35328;&#8221;&#12290;&#34429;&#28982;&#20687;&#35895;&#27468;&#36825;&#26679;&#30340;&#27969;&#34892;&#25628;&#32034;&#24341;&#25806;&#22312;&#22270;&#20687;&#25628;&#32034;&#31934;&#24230;&#21644;&#25935;&#25463;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#25628;&#32034;&#32467;&#26524;&#26159;&#21542;&#20250;&#23384;&#22312;&#24615;&#21035;&#12289;&#35821;&#35328;&#12289;&#20154;&#21475;&#32479;&#35745;&#12289;&#31038;&#20250;&#25991;&#21270;&#26041;&#38754;&#30340;&#20559;&#35265;&#23384;&#22312;&#20105;&#35758;&#12290;&#36825;&#31181;&#28508;&#22312;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#23545;&#20010;&#20154;&#30340;&#35748;&#30693;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#24433;&#21709;&#20182;&#20204;&#30340;&#35270;&#35282;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#38754;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#25628;&#32034;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#20960;&#31181;&#20559;&#35265;&#31867;&#22411;&#20197;&#21450;&#20026;&#20160;&#20040;&#26377;&#24517;&#35201;&#21152;&#20197;&#32531;&#35299;&#12290;&#25105;&#20204;&#23558;&#30740;&#31350;&#37325;&#28857;&#32553;&#23567;&#21040;&#35780;&#20272;&#21644;&#32531;&#35299;&#22270;&#20687;&#26816;&#32034;&#20013;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#23384;&#22312;&#30456;&#24403;&#20005;&#37325;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#65292;&#36825;&#21487;&#33021;&#23545;&#20010;&#20154;&#21644;&#25972;&#20010;&#31038;&#20250;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#27492;&#31867;&#20559;&#35265;&#24182;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal search engines have experienced significant growth and widespread use in recent years, making them the second most common internet use. While search engine systems offer a range of services, the image search field has recently become a focal point in the information retrieval community, as the adage goes, "a picture is worth a thousand words". Although popular search engines like Google excel at image search accuracy and agility, there is an ongoing debate over whether their search results can be biased in terms of gender, language, demographics, socio-cultural aspects, and stereotypes. This potential for bias can have a significant impact on individuals' perceptions and influence their perspectives.  In this paper, we present our study on bias and fairness in web search, with a focus on keyword-based image search. We first discuss several kinds of biases that exist in search systems and why it is important to mitigate them. We narrow down our study to assessing and mitigat
&lt;/p&gt;</description></item><item><title>NorBench&#26159;&#29992;&#26469;&#35780;&#20272;&#25386;&#23041;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#25324;&#31616;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21644;&#25506;&#27979;&#22871;&#20214;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#25386;&#23041;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#20934;&#25968;&#25454;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.03880</link><description>&lt;p&gt;
NorBench--&#25386;&#23041;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
NorBench -- A Benchmark for Norwegian Language Models. (arXiv:2305.03880v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03880
&lt;/p&gt;
&lt;p&gt;
NorBench&#26159;&#29992;&#26469;&#35780;&#20272;&#25386;&#23041;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#25324;&#31616;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21644;&#25506;&#27979;&#22871;&#20214;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#25386;&#23041;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#20934;&#25968;&#25454;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;NorBench&#65306;&#19968;&#20010;&#31616;&#21270;&#30340;NLP&#20219;&#21153;&#21644;&#25506;&#27979;&#22871;&#20214;&#65292;&#29992;&#20110;&#22312;&#26631;&#20934;&#25968;&#25454;&#21010;&#20998;&#21644;&#35780;&#20272;&#25351;&#26631;&#19978;&#35780;&#20272;&#25386;&#23041;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#12290; &#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#25386;&#23041;&#35821;&#35328;&#27169;&#22411;&#65288;&#22522;&#20110;&#32534;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65289;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#21644;&#20998;&#26512;&#23427;&#20204;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#20854;&#20182;&#29616;&#26377;&#30340;LMs&#65292;&#22312;NorBench&#30340;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present NorBench: a streamlined suite of NLP tasks and probes for evaluating Norwegian language models (LMs) on standardized data splits and evaluation metrics. We also introduce a range of new Norwegian language models (both encoder and encoder-decoder based). Finally, we compare and analyze their performance, along with other existing LMs, across the different benchmark tests of NorBench.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22810;&#31181;&#20016;&#23500;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36164;&#28304;&#26469;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#26497;&#23569;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#20197;&#20943;&#23569;&#20154;&#24037;&#32763;&#35793;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.03873</link><description>&lt;p&gt;
&#20840;&#29699;&#35757;&#32451;&#65292;&#26412;&#22320;&#23450;&#21046;&#65306;&#26497;&#23569;&#36164;&#28304;&#35821;&#35328;&#30340;&#26497;&#31616;&#22810;&#35821;&#35328;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Train Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages. (arXiv:2305.03873v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03873
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22810;&#31181;&#20016;&#23500;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36164;&#28304;&#26469;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#26497;&#23569;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#20197;&#20943;&#23569;&#20154;&#24037;&#32763;&#35793;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20154;&#36947;&#20027;&#20041;&#22330;&#26223;&#20013;&#65292;&#21521;&#26497;&#24230;&#20302;&#36164;&#28304;&#30340;&#35821;&#35328;&#36827;&#34892;&#32763;&#35793;&#36890;&#24120;&#19981;&#38656;&#35201;&#36890;&#29992;&#30340;&#32763;&#35793;&#24341;&#25806;&#65292;&#32780;&#26159;&#38656;&#35201;&#19987;&#38376;&#30340;&#25991;&#26412;&#29305;&#23450;&#32763;&#35793;&#24341;&#25806;&#12290;&#25105;&#20204;&#23581;&#35797;&#21033;&#29992;&#22810;&#31181;&#20016;&#23500;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36164;&#28304;&#65292;&#39640;&#25928;&#22320;&#22312;&#26032;&#30340;&#26497;&#23569;&#36164;&#28304;&#35821;&#35328;&#20013;&#20026;&#19968;&#20010;&#24050;&#30693;&#30340;&#25991;&#26412;&#20135;&#29983;&#26368;&#22909;&#30340;&#21487;&#33021;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many humanitarian scenarios, translation into severely low resource languages often does not require a universal translation engine, but a dedicated text-specific translation engine. For example, healthcare records, hygienic procedures, government communication, emergency procedures and religious texts are all limited texts. While generic translation engines for all languages do not exist, translation of multilingually known limited texts into new, endangered languages may be possible and reduce human translation effort. We attempt to leverage translation resources from many rich resource languages to efficiently produce best possible translation quality for a well known text, which is available in multiple languages, in a new, severely low resource language. We examine two approaches: 1. best selection of seed sentences to jump start translations in a new language in view of best generalization to the remainder of a larger targeted text(s), and 2. we adapt large general multilingua
&lt;/p&gt;</description></item><item><title>&#36816;&#21160;&#31185;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#25903;&#25345;&#21644;&#22686;&#24378;&#20174;&#19994;&#32773;&#30693;&#35782;&#30340;&#28508;&#21147;&#65292;&#20026;&#20010;&#24615;&#21270;&#35757;&#32451;&#35745;&#21010;&#25552;&#20379;&#24314;&#35758;&#65292;&#24182;&#21521;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#20174;&#19994;&#32773;&#20998;&#21457;&#39640;&#36136;&#37327;&#20449;&#24687;&#65292;&#20294;&#20351;&#29992;&#21644;&#24320;&#21457;LLM&#23384;&#22312;&#39118;&#38505;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.03851</link><description>&lt;p&gt;
&#36816;&#21160;&#31185;&#23398;&#19982;&#21307;&#23398;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#26426;&#36935;&#12289;&#39118;&#38505;&#21644;&#32771;&#34385;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Sport Science &amp; Medicine: Opportunities, Risks and Considerations. (arXiv:2305.03851v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03851
&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#31185;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#25903;&#25345;&#21644;&#22686;&#24378;&#20174;&#19994;&#32773;&#30693;&#35782;&#30340;&#28508;&#21147;&#65292;&#20026;&#20010;&#24615;&#21270;&#35757;&#32451;&#35745;&#21010;&#25552;&#20379;&#24314;&#35758;&#65292;&#24182;&#21521;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#20174;&#19994;&#32773;&#20998;&#21457;&#39640;&#36136;&#37327;&#20449;&#24687;&#65292;&#20294;&#20351;&#29992;&#21644;&#24320;&#21457;LLM&#23384;&#22312;&#39118;&#38505;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#36816;&#21160;&#31185;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25152;&#38754;&#20020;&#30340;&#28508;&#22312;&#26426;&#36935;&#12289;&#39118;&#38505;&#21644;&#25361;&#25112;&#12290;LLM&#26159;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#65292;&#37319;&#29992;&#36716;&#25442;&#22120;&#39118;&#26684;&#30340;&#26550;&#26500;&#65292;&#22312;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#24182;&#36890;&#24120;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#12290;LLM&#21487;&#20197;&#25191;&#34892;&#22823;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#22312;&#36816;&#21160;&#31185;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#65292;LLM&#20855;&#26377;&#25903;&#25345;&#21644;&#22686;&#24378;&#36816;&#21160;&#21307;&#23398;&#20174;&#19994;&#32773;&#30693;&#35782;&#30340;&#28508;&#21147;&#65292;&#20026;&#20010;&#24615;&#21270;&#35757;&#32451;&#35745;&#21010;&#25552;&#20379;&#24314;&#35758;&#65292;&#24182;&#28508;&#22312;&#22320;&#21521;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#20174;&#19994;&#32773;&#20998;&#21457;&#39640;&#36136;&#37327;&#20449;&#24687;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#21644;&#24320;&#21457;LLM&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#65292;&#21253;&#25324;&#22522;&#20110;&#29992;&#20110;&#21019;&#24314;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#65292;&#27844;&#38706;&#26426;&#23494;&#25968;&#25454;&#30340;&#39118;&#38505;&#65292;&#29983;&#25104;&#26377;&#23475;&#36755;&#20986;&#30340;&#39118;&#38505;&#65292;&#20197;&#21450;&#38656;&#35201;&#36890;&#36807;&#21453;&#39304;&#20351;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#31867;&#21916;&#22909;&#30456;&#19968;&#33268;&#30340;&#38656;&#27714;&#12290;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#26469;&#20840;&#38754;&#20102;&#35299;LLM&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the potential opportunities, risks, and challenges associated with the use of large language models (LLMs) in sports science and medicine. LLMs are large neural networks with transformer style architectures trained on vast amounts of textual data, and typically refined with human feedback. LLMs can perform a large range of natural language processing tasks. In sports science and medicine, LLMs have the potential to support and augment the knowledge of sports medicine practitioners, make recommendations for personalised training programs, and potentially distribute high-quality information to practitioners in developing countries. However, there are also potential risks associated with the use and development of LLMs, including biases in the dataset used to create the model, the risk of exposing confidential data, the risk of generating harmful output, and the need to align these models with human preferences through feedback. Further research is needed to fully unde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#24207;&#21015;&#26631;&#35760;&#21644;&#36328;&#24230;&#39044;&#27979;&#20004;&#31181;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#20013;&#36328;&#24230;&#39044;&#27979;&#34920;&#29616;&#30053;&#20248;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#22823;&#29256;&#26412;&#30340;XLM RoBERTa&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03845</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;2&#20013;CLaC&#65306;&#27604;&#36739;&#24207;&#21015;&#26631;&#35760;&#21644;&#36328;&#24230;&#39044;&#27979;&#26041;&#27861;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
CLaC at SemEval-2023 Task 2: Comparing Span-Prediction and Sequence-Labeling approaches for NER. (arXiv:2305.03845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#24207;&#21015;&#26631;&#35760;&#21644;&#36328;&#24230;&#39044;&#27979;&#20004;&#31181;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#20013;&#36328;&#24230;&#39044;&#27979;&#34920;&#29616;&#30053;&#20248;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#22823;&#29256;&#26412;&#30340;XLM RoBERTa&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;CLaC&#23545;&#20110;MultiCoNER 2&#20219;&#21153;&#30340;&#25552;&#20132;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#22797;&#26434;&#30340;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#20004;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#21363;&#24207;&#21015;&#26631;&#35760;&#21644;&#36328;&#24230;&#39044;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#26368;&#22909;&#30340;&#36328;&#24230;&#39044;&#27979;&#31995;&#32479;&#30340;&#34920;&#29616;&#30053;&#20248;&#20110;&#25105;&#20204;&#26368;&#22909;&#30340;&#24207;&#21015;&#26631;&#35760;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#26356;&#22823;&#29256;&#26412;&#30340;XLM RoBERTa&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#21518;&#32493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;XLM-RoBERTa&#30340;&#29305;&#27530;&#36755;&#20837;&#26631;&#35760;&#65288;&lt;s&gt;&#21644;&lt;/s&gt;&#65289;&#26102;&#65292;&#36328;&#24230;&#39044;&#27979;&#21644;&#24207;&#21015;&#26631;&#35760;&#26041;&#27861;&#37117;&#20250;&#24471;&#21040;&#25913;&#36827;&#12290;&#25152;&#26377;&#27169;&#22411;&#35757;&#32451;&#65292;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#30340;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/harshshredding/semeval2023-multiconer-paper&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper summarizes the CLaC submission for the MultiCoNER 2 task which concerns the recognition of complex, fine-grained named entities. We compare two popular approaches for NER, namely Sequence Labeling and Span Prediction. We find that our best Span Prediction system performs slightly better than our best Sequence Labeling system on test data. Moreover, we find that using the larger version of XLM RoBERTa significantly improves performance. Post-competition experiments show that Span Prediction and Sequence Labeling approaches improve when they use special input tokens (&lt;s&gt; and &lt;/s&gt;) of XLM-RoBERTa. The code for training all models, preprocessing, and post-processing is available at https://github.com/harshshredding/semeval2023-multiconer-paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#29992;&#20110;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#32852;&#21512;&#25277;&#21462;&#12290;&#36890;&#36807;&#25506;&#32034;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#33258;&#25105;&#38598;&#25104;&#27491;&#21017;&#21270;&#22120;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#26089;&#26399;&#24555;&#36895;&#25910;&#25947;&#24182;&#19988;&#32531;&#35299;&#20102;&#22122;&#22768;&#26631;&#31614;&#20135;&#29983;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#20307;&#23545;&#25277;&#21462;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20998;&#21035;&#25552;&#39640;&#20102;4.43%&#21644;4.92%&#12290;</title><link>http://arxiv.org/abs/2305.03827</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#29992;&#20110;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#32852;&#21512;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data. (arXiv:2305.03827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#29992;&#20110;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#32852;&#21512;&#25277;&#21462;&#12290;&#36890;&#36807;&#25506;&#32034;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#33258;&#25105;&#38598;&#25104;&#27491;&#21017;&#21270;&#22120;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#26089;&#26399;&#24555;&#36895;&#25910;&#25947;&#24182;&#19988;&#32531;&#35299;&#20102;&#22122;&#22768;&#26631;&#31614;&#20135;&#29983;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#20307;&#23545;&#25277;&#21462;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20998;&#21035;&#25552;&#39640;&#20102;4.43%&#21644;4.92%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#24102;&#26377;&#27169;&#31946;&#25110;&#22122;&#22768;&#26631;&#31614;&#30340;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#26102;&#65292;&#32852;&#21512;&#25277;&#21462;&#23454;&#20307;&#23545;&#21450;&#20854;&#20851;&#31995;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#65292;&#20854;&#21160;&#26426;&#26159;&#26681;&#25454;&#30452;&#35273;&#65292;&#19968;&#20010;&#23454;&#20363;&#30340;&#19981;&#30830;&#23450;&#24615;&#36234;&#39640;&#65292;&#27169;&#22411;&#32622;&#20449;&#24230;&#19982;&#30495;&#23454;&#26631;&#31614;&#19981;&#19968;&#33268;&#30340;&#21487;&#33021;&#24615;&#23601;&#36234;&#22823;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#32034;&#23454;&#20363;&#32423;&#21035;&#30340;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#39640;&#32622;&#20449;&#30340;&#21021;&#22987;&#26679;&#20363;&#38598;&#12290;&#36825;&#26679;&#30340;&#23376;&#38598;&#29992;&#20110;&#36807;&#28388;&#22122;&#22768;&#23454;&#20363;&#65292;&#24182;&#26377;&#21161;&#20110;&#27169;&#22411;&#22312;&#26089;&#26399;&#24555;&#36895;&#25910;&#25947;&#12290;&#22312;Bootstrap&#23398;&#20064;&#26399;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#33258;&#25105;&#38598;&#25104;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#20943;&#36731;&#22122;&#22768;&#26631;&#31614;&#20135;&#29983;&#30340;&#27169;&#22411;&#38388;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23450;&#20041;&#32852;&#21512;&#26631;&#35760;&#27010;&#29575;&#30340;&#27010;&#29575;&#26041;&#24046;&#65292;&#20197;&#20272;&#35745;&#20869;&#37096;&#27169;&#22411;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#29992;&#20110;&#36873;&#25321;&#21644;&#24314;&#31435;&#26032;&#30340;&#21487;&#38752;&#35757;&#32451;&#23454;&#20363;&#36827;&#34892;&#19979;&#19968;&#27425;&#36845;&#20195;&#12290;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#22312;&#23454;&#20307;&#23545;&#25277;&#21462;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20998;&#21035;&#25552;&#39640;&#20102;4.43%&#21644;4.92%&#12290;
&lt;/p&gt;
&lt;p&gt;
Jointly extracting entity pairs and their relations is challenging when working on distantly-supervised data with ambiguous or noisy labels. To mitigate such impact, we propose uncertainty-aware bootstrap learning, which is motivated by the intuition that the higher uncertainty of an instance, the more likely the model confidence is inconsistent with the ground truths. Specifically, we first explore instance-level data uncertainty to create an initial high-confident examples. Such subset serves as filtering noisy instances and facilitating the model to converge fast at the early stage. During bootstrap learning, we propose self-ensembling as a regularizer to alleviate inter-model uncertainty produced by noisy labels. We further define probability variance of joint tagging probabilities to estimate inner-model parametric uncertainty, which is used to select and build up new reliable training instances for the next iteration. Experimental results on two large datasets reveal that our app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#33041;&#26426;&#30028;&#38754;&#39044;&#27979;&#24335;&#25171;&#23383;&#20219;&#21153;&#20013;&#12290;&#22312;&#35780;&#20272;&#20960;&#20010;&#21333;&#35789;&#32423;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#21518;&#65292;GPT-2&#22312;&#24178;&#20928;&#21360;&#21047;&#20307;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#26159;&#19981;&#21516;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22024;&#26434;&#30340;&#21382;&#21490;&#36712;&#36857;&#19978;&#26377;&#19981;&#21516;&#30340;&#21453;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.03819</link><description>&lt;p&gt;
&#36866;&#24212;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#33041;&#26426;&#30028;&#38754;&#39044;&#27979;&#24335;&#25171;&#23383;
&lt;/p&gt;
&lt;p&gt;
Adapting Transformer Language Models for Predictive Typing in Brain-Computer Interfaces. (arXiv:2305.03819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#33041;&#26426;&#30028;&#38754;&#39044;&#27979;&#24335;&#25171;&#23383;&#20219;&#21153;&#20013;&#12290;&#22312;&#35780;&#20272;&#20960;&#20010;&#21333;&#35789;&#32423;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#21518;&#65292;GPT-2&#22312;&#24178;&#20928;&#21360;&#21047;&#20307;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#26159;&#19981;&#21516;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22024;&#26434;&#30340;&#21382;&#21490;&#36712;&#36857;&#19978;&#26377;&#19981;&#21516;&#30340;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#30028;&#38754;&#26159;&#35768;&#22810;&#20154;&#37325;&#35201;&#30340;&#26367;&#20195;&#21644;&#36741;&#21161;&#20132;&#27969;&#26041;&#24335;&#12290;&#19982;&#38190;&#30424;&#19981;&#21516;&#65292;&#35768;&#22810;&#33041;&#26426;&#30028;&#38754;&#31995;&#32479;&#19981;&#20250;&#19968;&#27425;&#26174;&#31034;&#29978;&#33267;&#33521;&#35821;&#20013;&#30340;26&#20010;&#23383;&#27597;&#65292;&#26356;&#19981;&#35201;&#35828;&#25152;&#26377;&#31526;&#21495;&#20102;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23383;&#31526;&#32423;&#39044;&#27979;&#21487;&#20197;&#26497;&#22823;&#22320;&#21152;&#36895;BCI&#25171;&#23383;&#12290;&#26412;&#25991;&#23558;&#20960;&#20010;&#21333;&#35789;&#32423;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20026;&#23383;&#31526;&#39044;&#27979;&#65292;&#24182;&#22312;&#25171;&#23383;&#20219;&#21153;&#20013;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#12290;&#20854;&#20013;&#65292;GPT-2&#22312;&#24178;&#20928;&#25991;&#26412;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#22024;&#26434;&#30340;&#21382;&#21490;&#21453;&#24212;&#19981;&#21516;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#21333;&#35789;&#20013;&#30340;&#23383;&#31526;&#20301;&#32622;&#21644;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain-computer interfaces (BCI) are an important mode of alternative and augmentative communication for many people. Unlike keyboards, many BCI systems do not display even the 26 letters of English at one time, let alone all the symbols in more complex systems. Using language models to make character-level predictions, therefore, can greatly speed up BCI typing (Ghosh and Kristensson, 2017). While most existing BCI systems employ character n-gram models or no LM at all, this paper adapts several wordpiece-level Transformer LMs to make character predictions and evaluates them on typing tasks. GPT-2 fares best on clean text, but different LMs react differently to noisy histories. We further analyze the effect of character positions in a word and context lengths.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RegularGPT&#30340;&#26032;&#22411;Transformer&#21464;&#20307;&#65292;&#20854;&#36890;&#36807;Weigh-Sharing&#12289;Adaptive-Depth&#21644;Sliding-Dilated-Attention&#30340;&#26032;&#39062;&#32452;&#21512;&#65292;&#27839;&#28145;&#24230;&#32500;&#24230;&#26500;&#24314;&#24037;&#20316;&#35760;&#24518;&#65292;&#20174;&#32780;&#25104;&#21151;&#22320;&#24314;&#27169;&#20102;&#27491;&#21017;&#35821;&#35328;&#65292;&#22914;PARITY&#65292;&#24182;&#22312;&#33258;&#28982;&#35821;&#35328;&#38271;&#24230;&#22806;&#25512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03796</link><description>&lt;p&gt;
Transformer&#24037;&#20316;&#35760;&#24518;&#20351;&#24471;&#27491;&#21017;&#35821;&#35328;&#25512;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#38271;&#24230;&#22806;&#25512;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation. (arXiv:2305.03796v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RegularGPT&#30340;&#26032;&#22411;Transformer&#21464;&#20307;&#65292;&#20854;&#36890;&#36807;Weigh-Sharing&#12289;Adaptive-Depth&#21644;Sliding-Dilated-Attention&#30340;&#26032;&#39062;&#32452;&#21512;&#65292;&#27839;&#28145;&#24230;&#32500;&#24230;&#26500;&#24314;&#24037;&#20316;&#35760;&#24518;&#65292;&#20174;&#32780;&#25104;&#21151;&#22320;&#24314;&#27169;&#20102;&#27491;&#21017;&#35821;&#35328;&#65292;&#22914;PARITY&#65292;&#24182;&#22312;&#33258;&#28982;&#35821;&#35328;&#38271;&#24230;&#22806;&#25512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#20110;&#24490;&#29615;&#27169;&#22411;&#65292;&#26222;&#36941;&#35748;&#20026;Transformer&#19981;&#33021;&#23436;&#32654;&#22320;&#24314;&#27169;&#27491;&#21017;&#35821;&#35328;&#12290;&#21463;&#21040;&#24037;&#20316;&#35760;&#24518;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RegularGPT&#30340;&#26032;&#22411;Transformer&#21464;&#31181;&#12290;&#36890;&#36807;Weigh-Sharing&#12289;Adaptive-Depth&#21644;Sliding-Dilated-Attention&#30340;&#26032;&#39062;&#32452;&#21512;&#65292;RegularGPT&#27839;&#28145;&#24230;&#32500;&#24230;&#26500;&#24314;&#24037;&#20316;&#35760;&#24518;&#65292;&#20174;&#32780;&#33021;&#22815;&#26377;&#25928;&#22320;&#25104;&#21151;&#22320;&#24314;&#27169;&#27491;&#21017;&#35821;&#35328;&#65292;&#22914;PARITY&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#33258;&#28982;&#35821;&#35328;&#38271;&#24230;&#22806;&#25512;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;RegularGPT&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#37325;&#26032;&#21457;&#29616;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#34987;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#22806;&#25512;&#33267;&#20851;&#37325;&#35201;&#30340;&#23616;&#37096;&#31383;&#21475;&#27880;&#24847;&#21147;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OpenFSP&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#20123;&#31616;&#21333;&#30340;&#26631;&#31614;&#26041;&#20415;&#22320;&#21019;&#24314;&#26032;&#39046;&#22495;&#65292;&#24182;&#22312;&#32473;&#23450;&#27880;&#37322;&#21518;&#65292;&#21033;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#30340;&#21305;&#37197;&#31639;&#27861;&#39044;&#27979;&#30001;&#32456;&#31471;&#29992;&#25143;&#23450;&#20041;&#30340;&#39046;&#22495;&#30340;&#24847;&#22270;&#21644;&#25554;&#27133;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03793</link><description>&lt;p&gt;
&#37319;&#29992;&#20219;&#21153;&#19981;&#21487;&#30693;&#26412;&#20307;&#21644;&#31616;&#21333;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#24103;&#35821;&#20041;&#35299;&#26512;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Towards Zero-Shot Frame Semantic Parsing with Task Agnostic Ontologies and Simple Labels. (arXiv:2305.03793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OpenFSP&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#20123;&#31616;&#21333;&#30340;&#26631;&#31614;&#26041;&#20415;&#22320;&#21019;&#24314;&#26032;&#39046;&#22495;&#65292;&#24182;&#22312;&#32473;&#23450;&#27880;&#37322;&#21518;&#65292;&#21033;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#30340;&#21305;&#37197;&#31639;&#27861;&#39044;&#27979;&#30001;&#32456;&#31471;&#29992;&#25143;&#23450;&#20041;&#30340;&#39046;&#22495;&#30340;&#24847;&#22270;&#21644;&#25554;&#27133;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24103;&#35821;&#20041;&#35299;&#26512;&#26159;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#24403;&#21069;&#30340;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#25104;&#21151;&#22320;&#35782;&#21035;&#29992;&#25143;&#36755;&#20837;&#35805;&#35821;&#20013;&#30340;&#24847;&#22270;&#21644;&#25554;&#27133;&#12290;&#36825;&#22312;&#23558;&#26032;&#39046;&#22495;&#28155;&#21152;&#21040;&#34394;&#25311;&#21161;&#25163;&#21151;&#33021;&#26102;&#20250;&#20135;&#29983;&#37325;&#35201;&#38556;&#30861;&#65292;&#22240;&#20026;&#36825;&#20123;&#25968;&#25454;&#30340;&#21019;&#24314;&#38656;&#35201;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OpenFSP&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#23569;&#37327;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;NLP&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30340;&#31616;&#21333;&#26631;&#31614;&#20013;&#26041;&#20415;&#22320;&#21019;&#24314;&#26032;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#21019;&#24314;&#19968;&#20010;&#23567;&#32780;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#12289;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#25554;&#27133;&#31867;&#22411;&#38598;&#65292;&#20197;&#23454;&#29616;&#23545;&#26032;&#39046;&#22495;&#30340;&#31616;&#21333;&#27880;&#37322;&#12290;&#22312;&#32473;&#23450;&#36825;&#26679;&#30340;&#27880;&#37322;&#21518;&#65292;&#20381;&#36182;&#20110;&#21477;&#23376;&#32534;&#30721;&#22120;&#30340;&#21305;&#37197;&#31639;&#27861;&#39044;&#27979;&#30001;&#32456;&#31471;&#29992;&#25143;&#23450;&#20041;&#30340;&#39046;&#22495;&#30340;&#24847;&#22270;&#21644;&#25554;&#27133;&#12290;&#22312;TopV2&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#36825;&#31181;&#31616;&#21333;&#26631;&#31614;&#35774;&#32622;&#19979;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Frame semantic parsing is an important component of task-oriented dialogue systems. Current models rely on a significant amount training data to successfully identify the intent and slots in the user's input utterance. This creates a significant barrier for adding new domains to virtual assistant capabilities, as creation of this data requires highly specialized NLP expertise. In this work we propose OpenFSP, a framework that allows for easy creation of new domains from a handful of simple labels that can be generated without specific NLP knowledge. Our approach relies on creating a small, but expressive, set of domain agnostic slot types that enables easy annotation of new domains. Given such annotation, a matching algorithm relying on sentence encoders predicts the intent and slots for domains defined by end-users. Extensive experiments on the TopV2 dataset shows that our model outperforms strong baselines in this simple labels setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22303;&#32819;&#20854;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#22823;&#37327;&#36890;&#29992;&#39046;&#22495;&#35821;&#26009;&#24211;&#30693;&#35782;&#30340;BERTurk&#21644;TurkRadBERT-task v1&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.03788</link><description>&lt;p&gt;
&#22312;&#22303;&#32819;&#20854;&#20020;&#24202;&#39046;&#22495;&#20013;&#21033;&#29992;BERT&#30340;&#21147;&#37327;&#65306;&#38754;&#21521;&#26377;&#38480;&#25968;&#25454;&#22330;&#26223;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios. (arXiv:2305.03788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22303;&#32819;&#20854;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#22823;&#37327;&#36890;&#29992;&#39046;&#22495;&#35821;&#26009;&#24211;&#30693;&#35782;&#30340;BERTurk&#21644;TurkRadBERT-task v1&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#26041;&#38754;&#30340;&#20027;&#35201;&#36827;&#23637;&#21463;&#21040;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25512;&#21160;&#65292;&#36825;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#20013;&#26377;&#30528;&#26174;&#33879;&#30340;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#36825;&#19968;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#26041;&#27861;&#23545;&#22303;&#32819;&#20854;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#22312;&#21253;&#21547;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#35299;&#20915;&#26377;&#38480;&#35821;&#35328;&#36164;&#28304;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31532;&#19968;&#27425;&#21033;&#29992;&#26377;&#38480;&#30340;&#20020;&#24202;&#20219;&#21153;&#25968;&#25454;&#26469;&#35780;&#20272;&#21516;&#26102;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22235;&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;TurkRadBERT-task v1&#12289;TurkRadBERT-task v2&#12289;TurkRadBERT-sim v1&#21644;TurkRadBERT-sim v2&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#22823;&#37327;&#36890;&#29992;&#39046;&#22495;&#35821;&#26009;&#24211;&#30693;&#35782;&#30340;&#27867;&#29992;&#24615;&#22303;&#32819;&#20854;BERT&#27169;&#22411;(BERTurk)&#21644;TurkRadBERT-task v1&#34920;&#29616;&#26368;&#20339;&#12290;&#34429;&#28982;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#24615;&#33021;&#20063;&#26377;&#25152;&#25552;&#39640;&#65292;&#20294;&#20173;&#19981;&#22914;&#19978;&#36848;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, major advancements in natural language processing (NLP) have been driven by the emergence of large language models (LLMs), which have significantly revolutionized research and development within the field. Building upon this progress, our study delves into the effects of various pre-training methodologies on Turkish clinical language models' performance in a multi-label classification task involving radiology reports, with a focus on addressing the challenges posed by limited language resources. Additionally, we evaluated the simultaneous pretraining approach by utilizing limited clinical task data for the first time. We developed four models, including TurkRadBERT-task v1, TurkRadBERT-task v2, TurkRadBERT-sim v1, and TurkRadBERT-sim v2. Our findings indicate that the general Turkish BERT model (BERTurk) and TurkRadBERT-task v1, both of which utilize knowledge from a substantial general-domain corpus, demonstrate the best overall performance. Although the task-adaptive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#31526;&#21495;&#25512;&#29702;&#26694;&#26550;&#65292;DSR-LM&#65292;&#29992;&#20110;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#19981;&#20687;&#20197;&#24448;&#30340;&#30740;&#31350;&#20381;&#36182;&#25163;&#24037;&#21046;&#23450;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#22320;&#23398;&#20064;&#21152;&#26435;&#35268;&#21017;&#65292;&#24182;&#24212;&#29992;&#35821;&#20041;&#25439;&#22833;&#36827;&#19968;&#27493;&#25913;&#21892;LMs&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03742</link><description>&lt;p&gt;
&#19981;&#21516;iable&#31526;&#21495;&#32534;&#31243;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming. (arXiv:2305.03742v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#31526;&#21495;&#25512;&#29702;&#26694;&#26550;&#65292;DSR-LM&#65292;&#29992;&#20110;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#19981;&#20687;&#20197;&#24448;&#30340;&#30740;&#31350;&#20381;&#36182;&#25163;&#24037;&#21046;&#23450;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#22320;&#23398;&#20064;&#21152;&#26435;&#35268;&#21017;&#65292;&#24182;&#24212;&#29992;&#35821;&#20041;&#25439;&#22833;&#36827;&#19968;&#27493;&#25913;&#21892;LMs&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#22312;&#35268;&#27169;&#21644;&#32452;&#21512;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#21487;&#38752;&#22320;&#25191;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;&#26412;&#25991;&#22522;&#20110;&#31526;&#21495;&#32534;&#31243;&#30340;&#35270;&#35282;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DSR-LM&#65292;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#31526;&#21495;&#25512;&#29702;&#26694;&#26550;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#30340;LMs&#31649;&#29702;&#20107;&#23454;&#30693;&#35782;&#30340;&#24863;&#30693;&#65292;&#31526;&#21495;&#27169;&#22359;&#25191;&#34892;&#28436;&#32462;&#25512;&#29702;&#12290;&#19982;&#20381;&#36182;&#25163;&#24037;&#21046;&#23450;&#30340;&#36923;&#36753;&#35268;&#21017;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#21487;&#24494;&#20998;&#31526;&#21495;&#25512;&#29702;&#26694;&#26550;&#26377;&#25928;&#22320;&#23398;&#20064;&#21152;&#26435;&#35268;&#21017;&#65292;&#24182;&#24212;&#29992;&#35821;&#20041;&#25439;&#22833;&#36827;&#19968;&#27493;&#25913;&#21892;LMs&#12290;DSR-LM&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20801;&#35768;&#36731;&#26494;&#38598;&#25104;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25903;&#25345;&#24191;&#27867;&#30340;&#31526;&#21495;&#32534;&#31243;&#65292;&#20197;&#31283;&#20581;&#22320;&#25512;&#20986;&#36923;&#36753;&#32467;&#35770;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DSR-LM&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#28436;&#32462;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#20102;20%&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;DSR-LM&#36824;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#38382;&#39064;&#65292;&#21253;&#25324;&#24320;&#25918;&#24335;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;tf-idf&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30340;SVM&#27169;&#22411;&#22312;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03737</link><description>&lt;p&gt;
&#35843;&#25972;&#20256;&#32479;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20197;&#36866;&#29992;&#20110;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Tuning Traditional Language Processing Approaches for Pashto Text Classification. (arXiv:2305.03737v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;tf-idf&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30340;SVM&#27169;&#22411;&#22312;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#22312;&#24456;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22810;&#39033;&#30740;&#31350;&#26469;&#24320;&#21457;&#22269;&#38469;&#21644;&#26412;&#22320;&#35821;&#35328;&#30340;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#20026;&#26412;&#22320;&#35821;&#35328;&#24314;&#31435;&#19968;&#20010;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#24314;&#31435;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;, &#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#30001;&#20110;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#25991;&#26723;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19981;&#21487;&#29992;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#21253;&#25324;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#12289;K&#36817;&#37051;(KNN)&#12289;&#20915;&#31574;&#26641;, &#39640;&#26031;&#26420;&#32032;&#36125;&#21494;&#26031;, &#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;, &#38543;&#26426;&#26862;&#26519;&#21644;&#36923;&#36753;&#22238;&#24402;&#22312;&#20869;&#30340;&#22810;&#20010;&#27169;&#22411;&#65292;&#20197;&#21457;&#29616;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#21253;&#25324;&#35789;&#34955;&#21644;tf-idf&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#20351;&#29992;tf-idf&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30340;SVM&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;97.19%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today text classification becomes critical task for concerned individuals for numerous purposes. Hence, several researches have been conducted to develop automatic text classification for national and international languages. However, the need for an automatic text categorization system for local languages is felt. The main aim of this study is to establish a Pashto automatic text classification system. In order to pursue this work, we built a Pashto corpus which is a collection of Pashto documents due to the unavailability of public datasets of Pashto text documents. Besides, this study compares several models containing both statistical and neural network machine learning techniques including Multilayer Perceptron (MLP), Support Vector Machine (SVM), K Nearest Neighbor (KNN), decision tree, gaussian na\"ive Bayes, multinomial na\"ive Bayes, random forest, and logistic regression to discover the most effective approach. Moreover, this investigation evaluates two different feature extr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2305.03731</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Assessing Working Memory Capacity of ChatGPT. (arXiv:2305.03731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#35760;&#24518;&#26159;&#20154;&#31867;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#20316;&#20026;&#20449;&#24687;&#20020;&#26102;&#23384;&#20648;&#21644;&#25805;&#20316;&#30340;&#24037;&#20316;&#31354;&#38388;&#12290;&#26412;&#25991;&#36890;&#36807;&#26816;&#26597;ChatGPT&#22312;N-back&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35843;&#26597;&#20102;&#36825;&#19968;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#24037;&#20316;&#35760;&#24518;&#23545;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#25509;&#30528;&#20171;&#32461;&#20102;&#35780;&#20272;ChatGPT&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#22312;&#35328;&#35821;&#21644;&#31354;&#38388;N- back&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#25991;&#29486;&#25253;&#36947;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24403;&#21069;&#36827;&#23637;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#65292;&#24182;&#20026;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29702;&#35299;&#20154;&#31867;&#24037;&#20316;&#35760;&#24518;&#30340;&#26410;&#26469;&#21162;&#21147;&#25552;&#20379;&#20102;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Working memory is a critical aspect of both human intelligence and artificial intelligence (AI), serving as a workspace for the temporary storage and manipulation of information. This paper investigates working memory capacity of ChatGPT, a state-of-the-art language model, by examining its performance on N-back tasks. We begin by discussing the importance of working memory to humans and AI, followed by the methods employed to assess working memory capacity of ChatGPT. Our study compares behavioral performance of ChatGPT on verbal and spatial N-back tasks to that of human participants reported in the literature, revealing notable similarities. Our findings offer crucial insights into the current progress in designing AI systems with human-level cognitive abilities and hold promise for informing future endeavors aimed at enhancing AI working memory and understanding human working memory through AI models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30333;&#30418;&#22810;&#30446;&#26631;&#23545;&#35805;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;DGSlow&#12290;&#36890;&#36807;&#24179;&#34913;&#29983;&#25104;&#20934;&#30830;&#24615;&#21644;&#38271;&#24230;&#20004;&#20010;&#30446;&#26631;&#65292;DGSlow&#21033;&#29992;&#29983;&#25104;&#26356;&#38271;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03655</link><description>&lt;p&gt;
&#30333;&#30418;&#22810;&#30446;&#26631;&#23545;&#35805;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
White-Box Multi-Objective Adversarial Attack on Dialogue Generation. (arXiv:2305.03655v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30333;&#30418;&#22810;&#30446;&#26631;&#23545;&#35805;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;DGSlow&#12290;&#36890;&#36807;&#24179;&#34913;&#29983;&#25104;&#20934;&#30830;&#24615;&#21644;&#38271;&#24230;&#20004;&#20010;&#30446;&#26631;&#65292;DGSlow&#21033;&#29992;&#29983;&#25104;&#26356;&#38271;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#22312;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#29983;&#25104;&#31995;&#32479;&#20013;&#24456;&#21463;&#27426;&#36814;&#12290; &#28982;&#32780;&#65292;&#36825;&#31181;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#36825;&#22312;&#20256;&#32479;&#20219;&#21153;&#65288;&#22914;&#25991;&#26412;&#20998;&#31867;&#65289;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#30740;&#31350;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#23545;&#23427;&#20204;&#22312;DG&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#22909;&#22855;&#24515;&#12290; &#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#25915;&#20987;DG&#27169;&#22411;&#30340;&#26102;&#20505;&#65292;&#23545;&#24403;&#21069;&#21477;&#23376;&#30340;&#25200;&#21160;&#20960;&#20046;&#19981;&#20250;&#38477;&#20302;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#65292;&#22240;&#20026;&#26410;&#25913;&#21464;&#30340;&#32842;&#22825;&#35760;&#24405;&#20063;&#20250;&#34987;&#32771;&#34385;&#36827;&#34892;&#20915;&#31574;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36890;&#36807;&#31934;&#24515;&#21046;&#20316;&#23545;&#25239;&#26679;&#26412;&#26469;&#36843;&#20351;&#29983;&#25104;&#26356;&#38271;&#30340;&#36755;&#20986;&#65292;&#26377;&#21033;&#20110;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615; - &#29983;&#25104;&#30340;&#21709;&#24212;&#36890;&#24120;&#26159;&#19981;&#30456;&#20851;&#12289;&#20887;&#38271;&#21644;&#37325;&#22797;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGSlow&#30340;&#30333;&#30418;&#22810;&#30446;&#26631;&#25915;&#20987;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DGSlow&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#22120;&#24179;&#34913;&#20004;&#20010;&#30446;&#26631; - &#29983;&#25104;&#20934;&#30830;&#24230;&#21644;&#38271;&#24230;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#26041;&#27861;&#23454;&#26045;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformers are popular in state-of-the-art dialogue generation (DG) systems. Such language models are, however, vulnerable to various adversarial samples as studied in traditional tasks such as text classification, which inspires our curiosity about their robustness in DG systems. One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for decision-making. Instead of merely pursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that crafting adversarial samples to force longer generation outputs benefits attack effectiveness -- the generated responses are typically irrelevant, lengthy, and repetitive. To this end, we propose a white-box multi-objective attack method called DGSlow. Specifically, DGSlow balances two objectives -- generation accuracy and length, via a gradient-based multi-objective optimizer and applies an adapti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;&#22823;&#25968;&#24615;&#36136;&#23545;&#20110;&#35299;&#37322;&#40784;&#26222;&#22827;&#23450;&#24459;&#30340;&#24433;&#21709;&#65292;&#25351;&#20986;&#40784;&#26222;&#22827;&#23450;&#24459;&#22122;&#38899;&#26159;&#36825;&#31181;&#24433;&#21709;&#30340;&#20363;&#23376;&#65292;&#24182;&#20998;&#26512;&#20102;&#26435;&#21147;&#20998;&#24067;&#21644;&#31867;&#20284;&#20998;&#24067;&#22312;&#31181;&#32676;&#26159;&#26377;&#38480;&#30340;&#12289;&#25490;&#21517;&#21644;&#20803;&#32032;&#35745;&#25968;&#26159;&#33258;&#28982;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02687</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#21644;&#22823;&#25968;&#30340;&#35299;&#37322;&#65306;&#35299;&#35835;&#40784;&#26222;&#22827;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Big Data and Large Numbers. Interpreting Zipf's Law. (arXiv:2305.02687v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02687
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;&#22823;&#25968;&#24615;&#36136;&#23545;&#20110;&#35299;&#37322;&#40784;&#26222;&#22827;&#23450;&#24459;&#30340;&#24433;&#21709;&#65292;&#25351;&#20986;&#40784;&#26222;&#22827;&#23450;&#24459;&#22122;&#38899;&#26159;&#36825;&#31181;&#24433;&#21709;&#30340;&#20363;&#23376;&#65292;&#24182;&#20998;&#26512;&#20102;&#26435;&#21147;&#20998;&#24067;&#21644;&#31867;&#20284;&#20998;&#24067;&#22312;&#31181;&#32676;&#26159;&#26377;&#38480;&#30340;&#12289;&#25490;&#21517;&#21644;&#20803;&#32032;&#35745;&#25968;&#26159;&#33258;&#28982;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#22823;&#25968;&#25454;&#39046;&#22495;&#30340;&#23454;&#35777;&#20107;&#23454;&#23646;&#20110;&#22823;&#25968;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;&#40784;&#26222;&#22827;&#23450;&#24459;&#22122;&#38899;&#23601;&#26159;&#36825;&#31181;&#29616;&#35937;&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#26435;&#21147;&#20998;&#24067;&#21644;&#31867;&#20284;&#20998;&#24067;&#30340;&#20960;&#20010;&#29305;&#24615;&#65292;&#36825;&#20123;&#20998;&#24067;&#21457;&#29983;&#22312;&#31181;&#32676;&#26159;&#26377;&#38480;&#30340;&#12289;&#25490;&#21517;&#21644;&#20803;&#32032;&#35745;&#25968;&#26159;&#33258;&#28982;&#25968;&#30340;&#24773;&#20917;&#19979;&#12290;&#35752;&#35770;&#20102;&#36825;&#20123;&#29305;&#24615;&#23545;&#35299;&#37322;&#40784;&#26222;&#22827;&#23450;&#24459;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
It turns out that some empirical facts in Big Data are the effects of properties of large numbers. Zipf's law noise is an example of such an artefact. We expose several properties of the power law distributions and of similar distribution that occur when the population is finite and the rank and counts of elements in the population are natural numbers. Consequences in the interpretation of Zipf's law are discussed.
&lt;/p&gt;</description></item><item><title>&#30456;&#37051;&#21333;&#35789;&#21487;&#20197;&#24433;&#21709;&#35299;&#37322;&#32773;&#23545;&#21333;&#35789;&#37325;&#35201;&#24615;&#30340;&#29702;&#35299;&#65292;&#24212;&#35813;&#32771;&#34385;&#25991;&#26412;&#20013;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#26469;&#26367;&#20195;&#21333;&#35789;&#32423;&#21035;&#30340;&#26174;&#33879;&#24615;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02679</link><description>&lt;p&gt;
&#30456;&#37051;&#21333;&#35789;&#24433;&#21709;&#20154;&#31867;&#23545;&#26174;&#33879;&#24615;&#35299;&#37322;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Neighboring Words Affect Human Interpretation of Saliency Explanations. (arXiv:2305.02679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02679
&lt;/p&gt;
&lt;p&gt;
&#30456;&#37051;&#21333;&#35789;&#21487;&#20197;&#24433;&#21709;&#35299;&#37322;&#32773;&#23545;&#21333;&#35789;&#37325;&#35201;&#24615;&#30340;&#29702;&#35299;&#65292;&#24212;&#35813;&#32771;&#34385;&#25991;&#26412;&#20013;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#26469;&#26367;&#20195;&#21333;&#35789;&#32423;&#21035;&#30340;&#26174;&#33879;&#24615;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35789;&#32423;&#21035;&#30340;&#26174;&#33879;&#24615;&#35299;&#37322;&#65288;&#8220;&#21333;&#35789;&#28909;&#22270;&#8221;&#65289;&#32463;&#24120;&#29992;&#20110;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#20013;&#20256;&#36798;&#29305;&#24449;&#24402;&#22240;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#35832;&#22914;&#21333;&#35789;&#38271;&#24230;&#31561;&#34920;&#38754;&#22240;&#32032;&#21487;&#33021;&#20250;&#25197;&#26354;&#20256;&#36798;&#30340;&#26174;&#33879;&#24615;&#35780;&#20998;&#30340;&#20154;&#31867;&#35299;&#35835;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#19968;&#20010;&#21333;&#35789;&#30340;&#30456;&#37051;&#21333;&#35789;&#30340;&#26631;&#35760;&#22914;&#20309;&#24433;&#21709;&#35299;&#37322;&#32773;&#23545;&#26174;&#33879;&#24615;&#35299;&#37322;&#20013;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#30340;&#24863;&#30693;&#12290;&#25105;&#20204;&#21457;&#29616;&#30456;&#37051;&#21333;&#35789;&#23545;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#35780;&#20998;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24433;&#21709;&#22522;&#20110;&#30456;&#37051;&#26041;&#21521;&#65288;&#24038;&#20391;&#19982;&#21491;&#20391;&#65289;&#20197;&#21450;&#30701;&#35821;&#21644;&#25645;&#37197;&#30340;&#20808;&#39564;&#35821;&#35328;&#21644;&#35745;&#31639;&#24230;&#37327;&#20540;&#65288;&#19982;&#19981;&#30456;&#20851;&#30340;&#30456;&#37051;&#21333;&#35789;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35753;&#20154;&#36136;&#30097;&#22312;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#26174;&#33879;&#24615;&#35299;&#37322;&#26159;&#21542;&#24212;&#35813;&#32487;&#32493;&#22312;&#21333;&#35789;&#32423;&#21035;&#19978;&#36827;&#34892;&#20256;&#36798;&#65292;&#24182;&#20026;&#26367;&#20195;&#26174;&#33879;&#24615;&#35299;&#37322;&#26041;&#27861;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word-level saliency explanations ("heat maps over words") are often used to communicate feature-attribution in text-based models. Recent studies found that superficial factors such as word length can distort human interpretation of the communicated saliency scores. We conduct a user study to investigate how the marking of a word's neighboring words affect the explainee's perception of the word's importance in the context of a saliency explanation. We find that neighboring words have significant effects on the word's importance rating. Concretely, we identify that the influence changes based on neighboring direction (left vs. right) and a-priori linguistic and computational measures of phrases and collocations (vs. unrelated neighboring words). Our results question whether text-based saliency explanations should be continued to be communicated at word level, and inform future research on alternative saliency explanation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Plan&#65292;Eliminate&#65292;&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24110;&#21161;&#26234;&#33021;&#20307;&#31616;&#21270;&#25511;&#21046;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#38480;&#21046;&#21644;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02412</link><description>&lt;p&gt;
&#35745;&#21010;&#12289;&#28040;&#38500;&#21644;&#36319;&#36394;&#8212;&#8212;&#35821;&#35328;&#27169;&#22411;&#26159;&#20855;&#22791;&#20307;&#39564;&#30340;&#26234;&#33021;&#20307;&#30340;&#33391;&#24072;&#30410;&#21451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents. (arXiv:2305.02412v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Plan&#65292;Eliminate&#65292;&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24110;&#21161;&#26234;&#33021;&#20307;&#31616;&#21270;&#25511;&#21046;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#38480;&#21046;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#25429;&#25417;&#21040;&#20851;&#20110;&#19990;&#30028;&#30340;&#31243;&#24207;&#21270;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;LLM&#20135;&#29983;&#30340;&#25277;&#35937;&#35745;&#21010;&#26469;&#31616;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#20219;&#21153;&#65292;&#36890;&#36807;&#21160;&#20316;&#25171;&#20998;&#25110;&#21160;&#20316;&#24314;&#27169;&#65288;&#24494;&#35843;&#65289;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#26550;&#26500;&#32487;&#25215;&#20102;&#20960;&#20010;&#38480;&#21046;&#65292;&#20351;&#24471;LLM&#38590;&#20197;&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#65306;&#20363;&#22914;&#26377;&#38480;&#30340;&#36755;&#20837;&#38271;&#24230;&#65292;&#24494;&#35843;&#30340;&#25928;&#29575;&#65292;&#39044;&#35757;&#32451;&#30340;&#20559;&#35265;&#20197;&#21450;&#19982;&#38750;&#25991;&#26412;&#29615;&#22659;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#20026;&#20102;&#19982;&#20302;&#32423;&#21035;&#21487;&#35757;&#32451;&#30340;&#25191;&#34892;&#22120;&#20445;&#25345;&#20860;&#23481;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;LLMs&#20013;&#30340;&#30693;&#35782;&#26469;&#31616;&#21270;&#25511;&#21046;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#35299;&#20915;&#38382;&#39064;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;Plan&#65292;Eliminate&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#12290;&#35745;&#21010;&#27169;&#22359;&#23558;&#20219;&#21153;&#25551;&#36848;&#36716;&#21270;&#20026;&#39640;&#23618;&#27425;&#23376;&#20219;&#21153;&#30340;&#21015;&#34920;&#12290;&#28040;&#38500;&#27169;&#22359;&#20174;&#24403;&#21069;&#23376;&#20219;&#21153;&#30340;&#35266;&#23519;&#20013;&#23631;&#34109;&#19981;&#30456;&#20851;&#30340;&#23545;&#35937;&#21644;&#23481;&#22120;&#12290;&#26368;&#21518;&#65292;&#36319;&#36394;&#27169;&#22359;&#30830;&#23450;&#26234;&#33021;&#20307;&#26159;&#21542;&#24050;&#32463;&#23454;&#29616;&#20102;&#24403;&#21069;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accompli
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#22266;&#23450;&#35821;&#35328;&#27169;&#22411;&#26469;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#19981;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#22312;&#26356;&#24555;&#30340;&#35757;&#32451;&#20013;&#20135;&#29983;&#31454;&#20105;&#24615;&#30340;&#25928;&#26524;&#65292;&#20165;&#38656;&#35201;&#21407;&#20808;&#20869;&#23384;&#30340;&#22235;&#20998;&#20043;&#19968;&#21363;&#21487;&#12290;</title><link>http://arxiv.org/abs/2305.02350</link><description>&lt;p&gt;
&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Using Language Models on Low-end Hardware. (arXiv:2305.02350v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#22266;&#23450;&#35821;&#35328;&#27169;&#22411;&#26469;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#19981;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#22312;&#26356;&#24555;&#30340;&#35757;&#32451;&#20013;&#20135;&#29983;&#31454;&#20105;&#24615;&#30340;&#25928;&#26524;&#65292;&#20165;&#38656;&#35201;&#21407;&#20808;&#20869;&#23384;&#30340;&#22235;&#20998;&#20043;&#19968;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#22266;&#23450;&#35821;&#35328;&#27169;&#22411;&#26469;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;CNN&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#24182;&#32452;&#25104;&#20102;&#21253;&#25324;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#35805;&#39064;&#12289;&#24773;&#24863;&#21644;&#39118;&#26684;&#30340;8&#32452;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#24635;&#32467;&#25104;&#19968;&#20010;&#26435;&#34913;&#21015;&#34920;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#21363;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#19981;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#22312;&#26356;&#24555;&#30340;&#35757;&#32451;&#20013;&#20135;&#29983;&#31454;&#20105;&#24615;&#30340;&#25928;&#26524;&#65292;&#20165;&#38656;&#35201;&#21407;&#20808;&#20869;&#23384;&#30340;&#22235;&#20998;&#20043;&#19968;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper evaluates the viability of using fixed language models for training text classification networks on low-end hardware. We combine language models with a CNN architecture and put together a comprehensive benchmark with 8 datasets covering single-label and multi-label classification of topic, sentiment, and genre. Our observations are distilled into a list of trade-offs, concluding that there are scenarios, where not fine-tuning a language model yields competitive effectiveness at faster training, requiring only a quarter of the memory compared to fine-tuning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01876</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#21477;&#23376;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#26377;&#21161;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20294;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#36828;&#26410;&#23436;&#21892;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#27010;&#24565;&#25552;&#21462;&#65288;CE&#65289;&#12290;&#28982;&#32780;&#65292;PLM&#24448;&#24448;&#20174;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#20851;&#32852;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#30693;&#35782;&#25366;&#25496;&#65292;&#32780;&#38750;Token&#20043;&#38388;&#30340;&#30495;&#23454;&#22240;&#26524;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#30693;&#35782;&#28151;&#28102;&#20102;PLM&#65292;&#23548;&#33268;&#25552;&#21462;&#22522;&#20110;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#30340;&#26377;&#20559;&#27010;&#24565;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20302;&#31934;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;PLM&#30340;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#20943;&#36731;&#27010;&#24565;&#20559;&#24046;&#12290;&#25552;&#31034;&#37319;&#29992;&#29616;&#26377;KG&#20013;&#30340;&#32473;&#23450;&#23454;&#20307;&#20027;&#39064;&#26469;&#32531;&#35299;&#23454;&#20307;&#21644;&#26377;&#20559;&#27010;&#24565;&#20043;&#38388;&#30340;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#26174;&#33879;&#25913;&#36827;&#20102;&#25552;&#21462;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens.As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;ICASSP&#20449;&#21495;&#22788;&#29702;&#22823;&#25361;&#25112;2023&#20013;&#30340;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#22823;&#25361;&#25112;&#30340;&#36136;&#37327;&#36712;&#36857;&#65288;Track 1&#65289;&#25552;&#20986;&#30340;&#21475;&#35821;&#35821;&#20041;&#35299;&#26512;&#31995;&#32479;&#12290;&#20351;&#29992;&#20102;&#31471;&#21040;&#31471;&#21644;pipeline&#31995;&#32479;&#65292;&#24182;&#22312;SLU&#26694;&#26550;&#20013;&#20351;&#29992;&#20102;&#24378;&#22823;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#31561;&#26041;&#27861;&#65292;&#26368;&#32456;&#33719;&#24471;&#20102;80.8&#30340;&#31934;&#30830;&#21305;&#37197;&#24230;&#65292;&#33719;&#24471;&#20102;&#25361;&#25112;&#30340;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2305.01620</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#23558;Pipeline&#21644;E2E SLU&#31995;&#32479;&#25972;&#21512;&#29992;&#20110;&#21475;&#35821;&#35821;&#20041;&#35299;&#26512;&#30340;&#30740;&#31350;&#65306;STOP&#21697;&#36136;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Study on the Integration of Pipeline and E2E SLU systems for Spoken Semantic Parsing toward STOP Quality Challenge. (arXiv:2305.01620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;ICASSP&#20449;&#21495;&#22788;&#29702;&#22823;&#25361;&#25112;2023&#20013;&#30340;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#22823;&#25361;&#25112;&#30340;&#36136;&#37327;&#36712;&#36857;&#65288;Track 1&#65289;&#25552;&#20986;&#30340;&#21475;&#35821;&#35821;&#20041;&#35299;&#26512;&#31995;&#32479;&#12290;&#20351;&#29992;&#20102;&#31471;&#21040;&#31471;&#21644;pipeline&#31995;&#32479;&#65292;&#24182;&#22312;SLU&#26694;&#26550;&#20013;&#20351;&#29992;&#20102;&#24378;&#22823;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#31561;&#26041;&#27861;&#65292;&#26368;&#32456;&#33719;&#24471;&#20102;80.8&#30340;&#31934;&#30830;&#21305;&#37197;&#24230;&#65292;&#33719;&#24471;&#20102;&#25361;&#25112;&#30340;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26377;&#19968;&#20123;&#26032;&#30340;&#22522;&#20934;&#20219;&#21153;&#34987;&#24341;&#20837;&#29992;&#20110;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#65292;&#20363;&#22914;&#35821;&#20041;&#35299;&#26512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545;ICASSP&#20449;&#21495;&#22788;&#29702;&#22823;&#25361;&#25112;2023&#20013;&#30340;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#22823;&#25361;&#25112;&#30340;&#36136;&#37327;&#36712;&#36857;&#65288;Track 1&#65289;&#25552;&#20986;&#30340;&#21475;&#35821;&#35821;&#20041;&#35299;&#26512;&#31995;&#32479;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#31471;&#21040;&#31471;&#21644;pipeline&#31995;&#32479;&#26469;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;SLU&#26694;&#26550;&#20013;&#20351;&#29992;&#20102;&#24378;&#22823;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#65292;&#22914;Whisper&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#65292;&#22914;BART&#65292;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#36755;&#20986;&#32423;&#21035;&#32452;&#21512;&#65292;&#20197;&#33719;&#24471;80.8&#30340;&#31934;&#30830;&#21305;&#37197;&#24230;&#65292;&#36825;&#20351;&#25105;&#20204;&#22312;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01219</link><description>&lt;p&gt;
&#35302;&#21457;&#35789;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#35302;&#21457;&#22120;&#65306;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#33539;&#20363;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20960;&#20010;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#27880;&#20837;&#35302;&#21457;&#22120;&#24182;&#20462;&#25913;&#26631;&#31614;&#26469;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#26377;&#38024;&#23545;&#24615;&#30340;&#28431;&#27934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35302;&#21457;&#22120;&#30340;&#23384;&#22312;&#21644;&#27602;&#30244;&#25968;&#25454;&#26631;&#27880;&#19981;&#27491;&#30830;&#31561;&#32570;&#38519;&#65292;&#36825;&#31181;&#25915;&#20987;&#23384;&#22312;&#24322;&#24120;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#65292;&#22522;&#20110;&#25552;&#31034;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#12290;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#36164;&#28304;&#21644;&#23569;&#26679;&#26412;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ProAttack&#26041;&#27861;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#65289;&#30340;&#22238;&#24212;&#65292;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#22823;&#37327;&#30340;L2&#29992;&#25143;&#24433;&#21709;&#35821;&#35328;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00217</link><description>&lt;p&gt;
&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#24180;&#65289;&#30340;&#22238;&#24212;&#65306;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#35777;&#26126;&#38750;&#27597;&#35821;&#29992;&#25143;&#27604;&#20363;&#23545;&#35821;&#35328;&#22797;&#26434;&#24230;&#26377;&#24433;&#21709;&#65288;arXiv:2305.00217v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus &amp; Walkden (2023). (arXiv:2305.00217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#65289;&#30340;&#22238;&#24212;&#65292;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#22823;&#37327;&#30340;L2&#29992;&#25143;&#24433;&#21709;&#35821;&#35328;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#12298;&#35821;&#35328;&#36827;&#21270;&#26434;&#24535;&#12299;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#20013;&#65292;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;https://doi.org/10.1093/jole/lzad005&#65292;KEW&#65289;&#25361;&#25112;&#20102;&#25105;&#22312;&#19968;&#31687;&#35770;&#25991;&#20013;&#65288;Koplenig&#65292;Royal Society Open Science&#65292;6&#65292;181274&#65288;2019&#65289;&#65292;https://doi.org/10.1098/rsos.181274&#65289;&#25152;&#21576;&#29616;&#30340;&#32467;&#26524;&#12290;&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#25105;&#35797;&#22270;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#32479;&#35745;&#20998;&#26512;&#26469;&#34920;&#26126;&#22823;&#37327;L2&#65288;&#31532;&#20108;&#35821;&#35328;&#65289;&#29992;&#25143;&#20284;&#20046;&#19981;&#20250;&#24433;&#21709;&#35821;&#35328;&#30340;&#65288;&#35821;&#27861;&#25110;&#32479;&#35745;&#65289;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#19987;&#27880;&#20110;Ethnologue&#35780;&#20272;&#35821;&#35328;&#22320;&#20301;&#30340;&#26041;&#24335;&#65306;&#22914;&#26524;&#19968;&#31181;&#35821;&#35328;&#38500;&#20102;&#34987;L1&#65288;&#31532;&#19968;&#35821;&#35328;&#65289;&#20351;&#29992;&#32773;&#20043;&#22806;&#65292;&#36824;&#24212;&#35813;&#26377;&#22823;&#37327;&#30340;L2&#20351;&#29992;&#32773;&#65292;&#37027;&#20040;&#35813;&#35821;&#35328;&#23601;&#34987;&#25551;&#36848;&#20026;&#20256;&#25773;&#24615;&#30340;&#12290;KEW&#25209;&#35780;&#20102;&#23558;&#20256;&#25773;&#24615;&#20316;&#20026;&#35821;&#35328;&#26159;&#21542;&#25317;&#26377;&#22823;&#37327;L2&#20351;&#29992;&#32773;&#65288;&#20108;&#20803;&#65289;&#25351;&#26631;&#30340;&#20351;&#29992;&#65292;&#20197;&#21450;&#22312;&#30452;&#25509;&#20272;&#35745;L2&#27604;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;L2&#29992;&#25143;&#27604;&#20363;&#24402;&#20026;&#38750;&#20256;&#25773;&#24615;&#35821;&#35328;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a recent paper published in the Journal of Language Evolution, Kauhanen, Einhaus &amp; Walkden (https://doi.org/10.1093/jole/lzad005, KEW) challenge the results presented in one of my papers (Koplenig, Royal Society Open Science, 6, 181274 (2019), https://doi.org/10.1098/rsos.181274), in which I tried to show through a series of statistical analyses that large numbers of L2 (second language) speakers do not seem to affect the (grammatical or statistical) complexity of a language. To this end, I focus on the way in which the Ethnologue assesses language status: a language is characterised as vehicular if, in addition to being used by L1 (first language) speakers, it should also have a significant number of L2 users. KEW criticise both the use of vehicularity as a (binary) indicator of whether a language has a significant number of L2 users and the idea of imputing a zero proportion of L2 speakers to non-vehicular languages whenever a direct estimate of that proportion is unavailable. Whi
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.00050</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24320;&#21551;&#22240;&#26524;&#30740;&#31350;&#30340;&#26032;&#31687;&#31456;
&lt;/p&gt;
&lt;p&gt;
Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. (arXiv:2305.00050v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00050
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#33021;&#21147;&#22791;&#21463;&#20105;&#35758;&#65292;&#24182;&#19988;&#23545;&#23558;&#20854;&#24212;&#29992;&#20110;&#21307;&#23398;&#12289;&#31185;&#23398;&#12289;&#27861;&#24459;&#21644;&#25919;&#31574;&#31561;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#21147;&#30340;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#21450;&#20854;&#22240;&#26524;&#25512;&#29702;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#28508;&#22312;&#30340;&#24314;&#26500;&#21644;&#27979;&#37327;&#25928;&#24230;&#23041;&#32961;&#12290;&#22522;&#20110;GPT-3.5&#21644;4&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#22240;&#26524;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LLMs&#23637;&#31034;&#20102;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#35299;&#37322;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.  Crucially, LLMs perform these causal tasks while relying on sources of knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.14391</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#22330;&#26223;&#37325;&#25490;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30456;&#23545;&#23545;&#35937;&#25490;&#21015;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#34920;&#31034;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#12290;&#35821;&#35328;&#35299;&#26512;&#22120;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#21442;&#25968;&#22522;&#20110;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#23545;&#35937;&#36827;&#34892;&#20462;&#27491;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27714;&#35299;&#33021;&#37327;&#20989;&#25968;&#30340;&#24635;&#21644;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#22320;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#31574;&#30053;&#23558;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#21040;&#25512;&#26029;&#30340;&#30446;&#26631;&#20301;&#32622;&#65292;&#21363;&#21487;&#29983;&#25104;&#30446;&#26631;&#22330;&#26223;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#32489;&#25928;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#65306;&#36890;&#36807;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#26377;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.13273</link><description>&lt;p&gt;
&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#65306;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#30340;&#32431;&#25991;&#26412;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping. (arXiv:2304.13273v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#65306;&#36890;&#36807;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#26377;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20197;CLIP&#21644;ALIGN&#20026;&#20195;&#34920;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;CLIP&#30340;&#38646;-shot&#33021;&#21147;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#31561;&#22522;&#20110;&#20851;&#32852;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#20294;&#26159;&#65292;CLIP&#38590;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#30001;&#20110;&#32570;&#20047;&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;K&#26368;&#36817;&#37051;&#36328;&#27169;&#24577;&#26144;&#23556;&#65288;Knight&#65289;&#65292;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#12290;&#36890;&#36807;&#31364;&#23383;&#24149;&#20219;&#21153;&#30340;&#32431;&#25991;&#26412;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Knight&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#19988;&#23558;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of Vision-Language Pre-training Models (VLPMs) represented by CLIP and ALIGN, significant breakthroughs have been achieved for association-based visual tasks such as image classification and image-text retrieval by the zero-shot capability of CLIP without fine-tuning. However, CLIP is hard to apply to generation-based tasks. This is due to the lack of decoder architecture and pre-training tasks for generation. Although previous works have created generation capacity for CLIP through additional language models, a modality gap between the CLIP representations of different modalities and the inability of CLIP to model the offset of this gap, which fails the concept to transfer across modalities. To solve the problem, we try to map images/videos to the language modality and generate captions from the language modality. In this paper, we propose the K-nearest-neighbor Cross-modality Mapping (Knight), a zero-shot method from association to generation. With text-only unsu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#26041;&#27861;&#8212;&#8212;\textsc{RenderDiffusion}&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#12290;&#23427;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#25955;&#25991;&#26412;&#24182;&#23454;&#29616;&#20102;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20316;&#20026;&#23383;&#24418;&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12519</link><description>&lt;p&gt;
RenderDiffusion: &#25991;&#26412;&#29983;&#25104;&#20316;&#20026;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
RenderDiffusion: Text Generation as Image Generation. (arXiv:2304.12519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#26041;&#27861;&#8212;&#8212;\textsc{RenderDiffusion}&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#12290;&#23427;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#25955;&#25991;&#26412;&#24182;&#23454;&#29616;&#20102;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20316;&#20026;&#23383;&#24418;&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#25991;&#26412;&#29983;&#25104;&#30340;&#26032;&#29983;&#25104;&#33539;&#24335;&#12290;&#32771;&#34385;&#21040;&#25991;&#26412;&#30340;&#31163;&#25955;&#20998;&#31867;&#29305;&#24615;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26041;&#27861;&#8212;&#8212;\textsc{RenderDiffusion}&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#36335;&#26159;&#23558;&#30446;&#26631;&#25991;&#26412;&#21576;&#29616;&#20026;&#21253;&#21547;&#35270;&#35273;&#35821;&#35328;&#20869;&#23481;&#30340;"&#23383;&#24418;&#22270;&#20687;"&#12290;&#36825;&#26679;&#65292;&#26465;&#20214;&#21270;&#30340;&#25991;&#26412;&#29983;&#25104;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#23383;&#24418;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#28982;&#21518;&#33258;&#28982;&#22320;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#25955;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have become a new generative paradigm for text generation. Considering the discrete categorical nature of text, in this paper, we propose \textsc{RenderDiffusion}, a novel diffusion approach for text generation via text-guided image generation. Our key idea is to render the target text as a \emph{glyph image} containing visual language content. In this way, conditional text generation can be cast as a glyph image generation task, and it is then natural to apply continuous diffusion models to discrete texts. Specially, we utilize a cascaded architecture (\ie a base and a super-resolution diffusion model) to generate high-fidelity glyph images, conditioned on the input text. Furthermore, we design a text grounding module to transform and refine the visual language content from generated glyph images into the final texts. In experiments over four conditional text generation tasks and two classes of metrics (\ie quality and diversity), \textsc{RenderDiffusion} can achieve 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22810;&#35821;&#35328;&#23545;&#25239;&#35757;&#32451;&#21644;&#27178;&#21521;&#25233;&#21046;&#25216;&#26415;&#23545;&#32599;&#39532;&#23612;&#20122;&#35821;&#22810;&#35789;&#34920;&#36798;&#36827;&#34892;&#33258;&#21160;&#35782;&#21035;&#65292;&#25552;&#39640;&#20102;&#24050;&#26377;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#22810;&#35789;&#34920;&#36798;&#19978;&#30340;F1&#20998;&#25968;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.11350</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#35821;&#35328;&#23545;&#25239;&#35757;&#32451;&#21644;&#27178;&#21521;&#25233;&#21046;&#25216;&#26415;&#30340;&#26041;&#27861;&#20248;&#21270;&#32599;&#39532;&#23612;&#20122;&#35821;&#22810;&#35789;&#34920;&#36798;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Romanian Multiword Expression Detection Using Multilingual Adversarial Training and Lateral Inhibition. (arXiv:2304.11350v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22810;&#35821;&#35328;&#23545;&#25239;&#35757;&#32451;&#21644;&#27178;&#21521;&#25233;&#21046;&#25216;&#26415;&#23545;&#32599;&#39532;&#23612;&#20122;&#35821;&#22810;&#35789;&#34920;&#36798;&#36827;&#34892;&#33258;&#21160;&#35782;&#21035;&#65292;&#25552;&#39640;&#20102;&#24050;&#26377;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#22810;&#35789;&#34920;&#36798;&#19978;&#30340;F1&#20998;&#25968;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35789;&#34920;&#36798;&#26159;&#24320;&#21457;&#22823;&#35268;&#27169;&#12289;&#35821;&#35328;&#23398;&#19978;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;PARSEM v1.2&#20849;&#20139;&#20219;&#21153;&#35821;&#26009;&#24211;&#19978;&#33258;&#21160;&#35782;&#21035;&#32599;&#39532;&#23612;&#20122;&#35821;&#22810;&#35789;&#34920;&#36798;&#26041;&#38754;&#25152;&#20570;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#24341;&#20837;&#30340;&#27178;&#21521;&#25233;&#21046;&#23618;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35270;&#35282;&#65292;&#20197;&#25552;&#39640;&#25152;&#37319;&#29992;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#24110;&#21161;&#19979;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;XLM-RoBERTa&#22312;&#26410;&#35265;&#36807;&#30340;&#22810;&#35789;&#34920;&#36798;&#26041;&#38754;&#30340;F1&#20998;&#25968;&#65292;&#20063;&#23601;&#26159;PARSEME 1.2&#29256;&#26412;&#30340;&#20027;&#35201;&#20219;&#21153;&#65292;&#32422;&#20026;2.7%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#22240;&#20026;&#23427;&#20204;&#36229;&#36807;&#20102;&#26412;&#27425;&#27604;&#36187;&#20013;&#21442;&#36187;&#32773;&#22312;&#32599;&#39532;&#23612;&#20122;&#35821;&#26041;&#38754;&#30340;&#20808;&#21069;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiword expressions are a key ingredient for developing large-scale and linguistically sound natural language processing technology. This paper describes our improvements in automatically identifying Romanian multiword expressions on the corpus released for the PARSEME v1.2 shared task. Our approach assumes a multilingual perspective based on the recently introduced lateral inhibition layer and adversarial training to boost the performance of the employed multilingual language models. With the help of these two methods, we improve the F1-score of XLM-RoBERTa by approximately 2.7% on unseen multiword expressions, the main task of the PARSEME 1.2 edition. In addition, our results can be considered SOTA performance, as they outperform the previous results on Romanian obtained by the participants in this competition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#27604;&#36739;&#30740;&#31350;&#34920;&#26126;&#65292;COVID-19&#38169;&#35823;&#20449;&#24687;&#30340;&#20998;&#24067;&#12289;&#20256;&#25773;&#33021;&#21147;&#12289;&#35821;&#35328;&#20998;&#26512;&#19982;&#20934;&#30830;&#20449;&#24687;&#19981;&#21516;&#65292;&#24182;&#19988;&#30740;&#21046;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#24179;&#22343;&#25552;&#39640;&#20102;9%&#20197;&#19978;&#30340;&#38169;&#35823;&#20449;&#24687;&#20998;&#31867;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.04811</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;COVID-19&#20449;&#24687;&#19982;&#38169;&#35823;&#20449;&#24687;&#30340;&#22823;&#35268;&#27169;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Large-Scale Comparative Study of Accurate COVID-19 Information versus Misinformation. (arXiv:2304.04811v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#27604;&#36739;&#30740;&#31350;&#34920;&#26126;&#65292;COVID-19&#38169;&#35823;&#20449;&#24687;&#30340;&#20998;&#24067;&#12289;&#20256;&#25773;&#33021;&#21147;&#12289;&#35821;&#35328;&#20998;&#26512;&#19982;&#20934;&#30830;&#20449;&#24687;&#19981;&#21516;&#65292;&#24182;&#19988;&#30740;&#21046;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#24179;&#22343;&#25552;&#39640;&#20102;9%&#20197;&#19978;&#30340;&#38169;&#35823;&#20449;&#24687;&#20998;&#31867;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#23548;&#33268;&#20449;&#24687;&#27867;&#28389;&#65292;&#22823;&#37327;&#19982;COVID-19&#30456;&#20851;&#30340;&#20869;&#23481;&#34987;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#39640;&#36895;&#20256;&#25773;&#65292;&#36825;&#35753;&#20844;&#20247;&#38590;&#20197;&#21306;&#20998;COVID-19&#30340;&#20934;&#30830;&#21644;&#38169;&#35823;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#23545;&#36229;&#36807;2.42&#20159;&#26465;&#25512;&#29305;&#36827;&#34892;&#22823;&#35268;&#27169;&#35745;&#31639;&#20998;&#26512;&#65292;&#23545;COVID-19&#38169;&#35823;&#21644;&#20934;&#30830;&#20449;&#24687;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;&#30740;&#31350;&#28085;&#30422;&#22235;&#20010;&#26041;&#38754;: 1)&#20027;&#39064;&#30340;&#20998;&#24067;&#65292;2)&#25512;&#29305;&#30340;&#23454;&#26102;&#24615;&#65292;3)&#35821;&#35328;&#20998;&#26512;&#21644;4)&#26102;&#38388;&#19978;&#30340;&#20256;&#25773;&#33021;&#21147;&#12290;&#26412;&#25991;&#36824;&#21019;&#36896;&#20102;&#19968;&#20010;&#26032;&#30340;COVID-19&#38169;&#35823;&#20449;&#24687;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#26368;&#21518;&#28436;&#31034;&#20102;&#35813;&#25968;&#25454;&#38598;&#33021;&#22815;&#36890;&#36807;&#24179;&#22343;F1&#25351;&#26631;&#23558;&#38169;&#35823;&#20449;&#24687;&#20998;&#31867;&#25552;&#39640;9%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic led to an infodemic where an overwhelming amount of COVID-19 related content was being disseminated at high velocity through social media. This made it challenging for citizens to differentiate between accurate and inaccurate information about COVID-19. This motivated us to carry out a comparative study of the characteristics of COVID-19 misinformation versus those of accurate COVID-19 information through a large-scale computational analysis of over 242 million tweets. The study makes comparisons alongside four key aspects: 1) the distribution of topics, 2) the live status of tweets, 3) language analysis and 4) the spreading power over time. An added contribution of this study is the creation of a COVID-19 misinformation classification dataset. Finally, we demonstrate that this new dataset helps improve misinformation classification by more than 9% based on average F1 measure.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32771;&#34385;&#20102;&#26102;&#38388;&#24615;&#23545;COVID-19&#30123;&#33495;&#24577;&#24230;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26102;&#38388;&#20998;&#21106;&#26174;&#33879;&#38477;&#20302;&#20102;&#31435;&#22330;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04806</link><description>&lt;p&gt;
&#32771;&#23519;COVID-19&#30123;&#33495;&#24577;&#24230;&#26816;&#27979;&#20013;&#30340;&#26102;&#38388;&#24615;
&lt;/p&gt;
&lt;p&gt;
Examining Temporalities on Stance Detection Towards COVID-19 Vaccination. (arXiv:2304.04806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04806
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32771;&#34385;&#20102;&#26102;&#38388;&#24615;&#23545;COVID-19&#30123;&#33495;&#24577;&#24230;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26102;&#38388;&#20998;&#21106;&#26174;&#33879;&#38477;&#20302;&#20102;&#31435;&#22330;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25351;&#20986;&#30123;&#33495;&#25509;&#31181;&#26159;&#25511;&#21046;COVID-19&#20256;&#25773;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#20102;&#35299;&#20844;&#20247;&#30340;&#30123;&#33495;&#24577;&#24230;&#23545;&#20915;&#31574;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340; COVID-19 &#30123;&#33495;&#24577;&#24230;&#65288;&#22914;&#25903;&#25345;&#21644;&#29369;&#35947;&#65289;&#20250;&#38543;&#26102;&#38388;&#32780;&#28436;&#21464;&#65292;&#22240;&#27492;&#22312;&#20998;&#26512;&#36825;&#20123;&#31435;&#22330;&#26102;&#38656;&#35201;&#32771;&#34385;&#21487;&#33021;&#30340;&#26102;&#38388;&#28418;&#31227;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#26816;&#26597;&#26102;&#38388;&#27010;&#24565;&#28418;&#31227;&#23545;&#25512;&#29305;&#19978; COVID-19 &#30123;&#33495;&#31435;&#22330;&#26816;&#27979;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#23545;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#36827;&#34892;&#20102;&#38543;&#26426;&#21644;&#26102;&#38388;&#20998;&#21106;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25152;&#26377;&#21333;&#35821;&#21644;&#22810;&#35821;&#25968;&#25454;&#38598;&#30340;&#38543;&#26426;&#21644;&#26102;&#38388;&#20998;&#21106;&#20043;&#38388;&#65292;&#27169;&#22411;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#26102;&#38388;&#20998;&#21106;&#26174;&#33879;&#38477;&#20302;&#20102;&#31435;&#22330;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have highlighted the importance of vaccination as an effective strategy to control the transmission of the COVID-19 virus. It is crucial for policymakers to have a comprehensive understanding of the public's stance towards vaccination on a large scale. However, attitudes towards COVID-19 vaccination, such as pro-vaccine or vaccine hesitancy, have evolved over time on social media. Thus, it is necessary to account for possible temporal shifts when analysing these stances. This study aims to examine the impact of temporal concept drift on stance detection towards COVID-19 vaccination on Twitter. To this end, we evaluate a range of transformer-based models using chronological and random splits of social media data. Our findings demonstrate significant discrepancies in model performance when comparing random and chronological splits across all monolingual and multilingual datasets. Chronological splits significantly reduce the accuracy of stance classification. Therefore, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LLM-Adapters&#65292;&#19968;&#20010;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;LLMs&#20013;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.01933</link><description>&lt;p&gt;
LLM-Adapters&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#36866;&#37197;&#22120;&#31995;&#21015;
&lt;/p&gt;
&lt;p&gt;
LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. (arXiv:2304.01933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLM-Adapters&#65292;&#19968;&#20010;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;LLMs&#20013;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;GPT-3&#21644;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#23548;&#33268;&#20102;&#35768;&#22810;&#32463;&#27982;&#23454;&#24800;&#21644;&#26131;&#20110;&#35775;&#38382;&#30340;&#26367;&#20195;&#21697;&#30340;&#24320;&#21457;&#65292;&#36825;&#20123;&#26367;&#20195;&#21697;&#26159;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#65288;&#20363;&#22914;ChatDoctor&#65289;&#25110;&#25351;&#23548;&#25968;&#25454;&#65288;&#20363;&#22914;Alpaca&#65289;&#24494;&#35843;&#24320;&#25918;&#24335;LLMs&#32780;&#21019;&#24314;&#30340;&#12290;&#22312;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26080;&#30097;&#26159;&#26368;&#26377;&#21560;&#24341;&#21147;&#30340;&#30740;&#31350;&#20027;&#39064;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#24494;&#35843;&#23569;&#37327;&#22806;&#37096;&#21442;&#25968;&#32780;&#19981;&#26159;&#25972;&#20010;LLMs&#65292;&#21516;&#26102;&#23454;&#29616;&#21487;&#27604;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;LLMs&#30340;PEFT&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;LLM-Adapters&#65292;&#36825;&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#21508;&#31181;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;LLMs&#20013;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#19981;&#21516;&#20219;&#21153;&#25191;&#34892;&#36825;&#20123;&#36866;&#37197;&#22120;&#30340;PEFT&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#24335;LLMs&#65292;&#20363;&#22914;LLaMA&#65292;BLOOM&#65292;OPT&#21644;GPT-J&#65292;&#20197;&#21450;&#24191;&#27867;&#20351;&#29992;&#30340;&#36866;&#37197;&#22120;&#65292;&#20363;&#22914;&#20018;&#32852;&#36866;&#37197;&#22120;&#65292;&#24182;&#32852;&#36866;&#37197;&#22120;&#21644;LoRA&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#39640;&#25928;&#19988;&#28789;&#27963;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#26368;&#23569;&#30340;&#38468;&#21152;&#35757;&#32451;&#25968;&#25454;&#25110;&#35745;&#31639;&#36164;&#28304;&#36731;&#26494;&#24494;&#35843;LLMs&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LLM-Adapters&#21487;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of large language models (LLMs), like GPT-3 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by fine-tuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, OPT, and GPT-J, as well as widely used adapters such as Series adapter, Parallel adapter, and LoRA. The framework is designed to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#32463;&#36807;&#25913;&#36827;&#21644;&#24494;&#35843;&#30340;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01238</link><description>&lt;p&gt;
Spam-T5&#65306;&#22522;&#20110;&#23567;&#26679;&#26412;&#30340;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection. (arXiv:2304.01238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#32463;&#36807;&#25913;&#36827;&#21644;&#24494;&#35843;&#30340;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;BERT-like&#12289;Sentence Transformers&#21644;Seq2Seq&#65289;&#20197;&#21450;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;LightGBM&#65289;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65288;&#23436;&#25972;&#35757;&#32451;&#38598;&#21644;&#23567;&#26679;&#26412;&#65289;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290; &#21457;&#29616;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;LLMs&#20248;&#20110;&#22522;&#32447;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#23567;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#36825;&#31181;&#36866;&#24212;&#24615;&#20351;LLMs&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#65292;&#22240;&#20026;&#26631;&#35760;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;&#24182;&#19988;&#27169;&#22411;&#38656;&#35201;&#32463;&#24120;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#19987;&#38376;&#20026;&#26816;&#27979;&#30005;&#23376;&#37038;&#20214;&#22403;&#22334;&#32780;&#36827;&#34892;&#20102;&#25913;&#36827;&#21644;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Spam-T5&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the effectiveness of large language models (LLMs) in email spam detection by comparing prominent models from three distinct families: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we examine well-established machine learning techniques for spam detection, such as Na\"ive Bayes and LightGBM, as baseline methods. We assess the performance of these models across four public datasets, utilizing different numbers of training samples (full training set and few-shot settings). Our findings reveal that, in the majority of cases, LLMs surpass the performance of the popular baseline techniques, particularly in few-shot scenarios. This adaptability renders LLMs uniquely suited to spam detection tasks, where labeled samples are limited in number and models require frequent updates. Additionally, we introduce Spam-T5, a Flan-T5 model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results demonstrate that Spam-T5 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;180&#19975;&#20221;&#32654;&#22269;&#20027;&#35201;&#23186;&#20307;&#26426;&#26500;&#30340;&#26032;&#38395;&#26631;&#39064;&#65292;&#25581;&#31034;&#20102;&#32654;&#22269;&#26032;&#38395;&#23186;&#20307;&#20013;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#22312;&#22269;&#20869;&#25919;&#27835;&#21644;&#31038;&#20250;&#38382;&#39064;&#19978;&#65292;&#24046;&#24322;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#23186;&#20307;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2303.15708</link><description>&lt;p&gt;
&#20559;&#35265;&#36824;&#26159;&#22810;&#26679;&#24615;&#65311;&#25581;&#31034;&#32654;&#22269;&#26032;&#38395;&#26631;&#39064;&#20013;&#30340;&#35821;&#20041;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Bias or Diversity? Unraveling Semantic Discrepancy in U.S. News Headlines. (arXiv:2303.15708v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;180&#19975;&#20221;&#32654;&#22269;&#20027;&#35201;&#23186;&#20307;&#26426;&#26500;&#30340;&#26032;&#38395;&#26631;&#39064;&#65292;&#25581;&#31034;&#20102;&#32654;&#22269;&#26032;&#38395;&#23186;&#20307;&#20013;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#22312;&#22269;&#20869;&#25919;&#27835;&#21644;&#31038;&#20250;&#38382;&#39064;&#19978;&#65292;&#24046;&#24322;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#23186;&#20307;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36941;&#19968;&#33268;&#35748;&#20026;&#26032;&#38395;&#23186;&#20307;&#22312;&#20854;&#26032;&#38395;&#25991;&#31456;&#20013;&#37319;&#29992;&#24847;&#35782;&#24418;&#24577;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#22312;&#27979;&#37327;&#23186;&#20307;&#26426;&#26500;&#20043;&#38388;&#30340;&#24046;&#24322;&#24182;&#36827;&#19968;&#27493;&#35299;&#21078;&#35821;&#20041;&#24046;&#24322;&#30340;&#28304;&#22836;&#26041;&#38754;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#26679;&#26412;&#22823;&#23567;&#30340;&#38480;&#21046;&#21644;&#33539;&#22260;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;180&#19975;&#20221;&#32654;&#22269;&#20027;&#35201;&#23186;&#20307;&#26426;&#26500;&#20174;2014&#24180;&#33267;2022&#24180;&#30340;&#26032;&#38395;&#26631;&#39064;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20197;&#20840;&#38754;&#36319;&#36394;&#21644;&#35299;&#21078;&#32654;&#22269;&#26032;&#38395;&#23186;&#20307;&#20013;&#30340;&#35821;&#20041;&#24046;&#24322;&#12290;&#25105;&#20204;&#37319;&#29992;&#22810;&#20803;&#23545;&#24212;&#20998;&#26512;(MCA)&#26469;&#37327;&#21270;&#19982;&#22235;&#20010;&#31361;&#20986;&#20027;&#39064;&#30456;&#20851;&#30340;&#35821;&#20041;&#24046;&#24322; - &#22269;&#20869;&#25919;&#27835;&#12289;&#32463;&#27982;&#38382;&#39064;&#12289;&#31038;&#20250;&#38382;&#39064;&#21644;&#22806;&#20132;&#20107;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#23186;&#20307;&#26631;&#39064;&#20013;&#26368;&#24120;&#35265;&#30340;n-gram&#65292;&#25552;&#20379;&#36827;&#19968;&#27493;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22269;&#20869;&#25919;&#27835;&#21644;&#31038;&#20250;&#38382;&#39064;&#19978;&#65292;&#24046;&#24322;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#23186;&#20307;&#20559;&#35265;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22806;&#20132;&#25253;&#36947;&#20013;&#30340;&#24046;&#24322;&#21017;&#26356;&#22810;&#22320;&#21453;&#26144;&#20102;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a broad consensus that news media outlets incorporate ideological biases in their news articles. However, prior studies on measuring the discrepancies among media outlets and further dissecting the origins of semantic differences suffer from small sample sizes and limited scope. In this study, we collect a large dataset of 1.8 million news headlines from major U.S. media outlets spanning from 2014 to 2022 to thoroughly track and dissect the semantic discrepancy in U.S. news media. We employ multiple correspondence analysis (MCA) to quantify the semantic discrepancy relating to four prominent topics - domestic politics, economic issues, social issues, and foreign affairs. Additionally, we compare the most frequent n-grams in media headlines to provide further qualitative insights into our analysis. Our findings indicate that on domestic politics and social issues, the discrepancy can be attributed to a certain degree of media bias. Meanwhile, the discrepancy in reporting foreig
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#65292;&#20174;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#34892;&#20026;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2303.13465</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep RL with Hierarchical Action Exploration for Dialogue Generation. (arXiv:2303.13465v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#65292;&#20174;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#34892;&#20026;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#30340;&#34892;&#20026;&#31354;&#38388;&#26497;&#20854;&#24222;&#22823;&#65292;&#22240;&#27492;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#65292;&#36817;&#20284;&#21160;&#24577;&#35268;&#21010;&#24517;&#39035;&#20351;&#29992;&#31574;&#30053;&#25913;&#36827;&#21644;&#34892;&#20026;&#37319;&#26679;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#26377;&#20215;&#20540;&#30340;&#22238;&#24212;&#38750;&#24120;&#31232;&#30095;&#65292;&#22240;&#27492;&#20351;&#29992;&#38543;&#26426;&#37319;&#26679;&#30340;&#36138;&#24515;&#31574;&#30053;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#31890;&#24230;&#30340; Q-function &#24182;&#36890;&#36807;&#25506;&#32034;&#26368;&#26377;&#21069;&#36884;&#30340;&#22238;&#24212;&#31867;&#21035;&#26469;&#32531;&#35299;&#36825;&#20010;&#23616;&#38480;&#24615;&#12290;&#35813;&#31639;&#27861;&#20174;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#30340;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventionally, since the natural language action space is astronomical, approximate dynamic programming applied to dialogue generation involves policy improvement with action sampling. However, such a practice is inefficient for reinforcement learning (RL) because the eligible (high action value) responses are very sparse, and the greedy policy sustained by the random sampling is flabby. This paper shows that the performance of dialogue policy positively correlated with sampling size by theoretical and experimental. We introduce a novel dual-granularity Q-function to alleviate this limitation by exploring the most promising response category to intervene in the sampling. It extracts the actions following the grained hierarchy, which can achieve the optimum with fewer policy iterations. Our approach learns in the way of offline RL from multiple reward functions designed to recognize human emotional details. Empirical studies demonstrate that our algorithm outperforms the baseline metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33521;&#22269;&#22269;&#23478;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#27604;&#21407;&#22987;BERT&#27169;&#22411;&#36798;&#21040;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#22312;&#20844;&#24179;&#12289;&#21487;&#37325;&#22797;&#19988;&#25968;&#25454;&#26377;&#25928;&#30340;&#27604;&#36739;&#30740;&#31350;&#20013;&#65292;&#20182;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#35821;&#26009;&#24211;&#26377;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;LM&#20307;&#31995;&#32467;&#26500;&#31216;&#20026;LTG-BERT&#12290;</title><link>http://arxiv.org/abs/2303.09859</link><description>&lt;p&gt;
&#32463;&#36807;&#35757;&#32451;&#30340;1&#20159;&#21333;&#35789;&#20173;&#28982;&#20445;&#25345;&#29366;&#24577;&#65306;BERT&#32467;&#21512;&#33521;&#22269;&#22269;&#23478;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Trained on 100 million words and still in shape: BERT meets British National Corpus. (arXiv:2303.09859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33521;&#22269;&#22269;&#23478;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#27604;&#21407;&#22987;BERT&#27169;&#22411;&#36798;&#21040;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#22312;&#20844;&#24179;&#12289;&#21487;&#37325;&#22797;&#19988;&#25968;&#25454;&#26377;&#25928;&#30340;&#27604;&#36739;&#30740;&#31350;&#20013;&#65292;&#20182;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#35821;&#26009;&#24211;&#26377;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;LM&#20307;&#31995;&#32467;&#26500;&#31216;&#20026;LTG-BERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#29616;&#20195;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35757;&#32451;&#30340;&#35821;&#26009;&#24211;&#35268;&#27169;&#36234;&#26469;&#36234;&#22823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#32553;&#23567;&#35757;&#32451;&#35268;&#27169;&#21040;&#19968;&#20010;&#35268;&#27169;&#36866;&#20013;&#12289;&#20195;&#34920;&#24615;&#22909;&#12289;&#24179;&#34913;&#24615;&#22909;&#19988;&#20844;&#24320;&#21487;&#29992;&#30340;&#33521;&#25991;&#25991;&#26412;&#28304;-&#33521;&#22269;&#22269;&#23478;&#35821;&#26009;&#24211;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#27604;&#21407;&#22987;BERT&#27169;&#22411;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#31867;&#22411;&#30340;&#35821;&#26009;&#24211;&#20855;&#26377;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#28508;&#21147;&#65292;&#25105;&#20204;&#20197;&#20844;&#24179;&#12289;&#21487;&#37325;&#22797;&#21644;&#25968;&#25454;&#26377;&#25928;&#30340;&#27604;&#36739;&#30740;&#31350;&#20026;&#29305;&#33394;&#65292;&#22312;&#20854;&#20013;&#35780;&#20272;&#20102;&#20960;&#20010;&#35757;&#32451;&#30446;&#26631;&#21644;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#20197;&#31995;&#32479;&#24615;&#30340;&#26041;&#24335;&#22797;&#21046;&#20102;&#20808;&#21069;&#30340;&#32463;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;LM&#20307;&#31995;&#32467;&#26500;&#31216;&#20026;LTG-BERT&#12290;
&lt;/p&gt;
&lt;p&gt;
While modern masked language models (LMs) are trained on ever larger corpora, we here explore the effects of down-scaling training to a modestly-sized but representative, well-balanced, and publicly available English text source -the British National Corpus. We show that pre-training on this carefully curated corpus can reach better performance than the original BERT model. We argue that this type of corpora has great potential as a language modeling benchmark. To showcase this potential, we present fair, reproducible and data-efficient comparative studies of LMs, in which we evaluate several training objectives and model architectures and replicate previous empirical results in a systematic way. We propose an optimized LM architecture called LTG-BERT.
&lt;/p&gt;</description></item><item><title>SmartBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#65292;&#20197;&#21152;&#36895;BERT&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.09266</link><description>&lt;p&gt;
SmartBERT&#65306;&#29992;&#20110;&#21152;&#36895;BERT&#25512;&#29702;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#26426;&#21046;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference. (arXiv:2303.09266v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09266
&lt;/p&gt;
&lt;p&gt;
SmartBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#65292;&#20197;&#21152;&#36895;BERT&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#26679;&#26412;&#22312;&#26089;&#26399;&#36864;&#20986;&#20043;&#21069;&#37117;&#24517;&#39035;&#32463;&#36807;&#25152;&#26377;&#36830;&#32493;&#23618;&#65292;&#36739;&#22797;&#26434;&#30340;&#26679;&#26412;&#36890;&#24120;&#20250;&#32463;&#21382;&#26356;&#22810;&#30340;&#23618;&#65292;&#20173;&#28982;&#23384;&#22312;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SmartBERT&#30340;Bert&#25512;&#29702;&#30340;&#26032;&#22411;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#30456;&#32467;&#21512;&#30340;&#26426;&#21046;&#65292;&#23427;&#23558;&#36339;&#36807;&#38376;&#21644;&#36864;&#20986;&#31639;&#23376;&#21152;&#20837;&#21040;BERT&#30340;&#27599;&#19968;&#23618;&#20013;&#12290;SmartBERT&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#23618;&#23545;&#27604;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#32467;&#21512;&#21040;&#25105;&#20204;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#65292;&#20197;&#25552;&#39640;&#20013;&#38388;&#23618;&#21644;&#20998;&#31867;&#22120;&#65292;&#36825;&#23545;&#20110;&#26089;&#26399;&#36864;&#20986;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#20445;&#25345;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#36339;&#36807;&#38376;&#30340;&#19968;&#33268;&#20351;&#29992;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#38454;&#27573;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#26435;&#37325;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#20843;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic early exiting has been proven to improve the inference speed of the pre-trained language model like BERT. However, all samples must go through all consecutive layers before early exiting and more complex samples usually go through more layers, which still exists redundant computation. In this paper, we propose a novel dynamic early exiting combined with layer skipping for BERT inference named SmartBERT, which adds a skipping gate and an exiting operator into each layer of BERT. SmartBERT can adaptively skip some layers and adaptively choose whether to exit. Besides, we propose cross-layer contrastive learning and combine it into our training phases to boost the intermediate layers and classifiers which would be beneficial for early exiting. To keep the consistent usage of skipping gates between training and inference phases, we propose a hard weight mechanism during training phase. We conduct experiments on eight classification datasets of the GLUE benchmark. Experimental resul
&lt;/p&gt;</description></item><item><title>SelfCheckGPT&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#38646;&#36164;&#28304;&#30340;&#26041;&#24335;&#26816;&#26597;&#40657;&#30418;&#27169;&#22411;&#30340;&#24187;&#35273;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.08896</link><description>&lt;p&gt;
SelfCheckGPT: &#38646;&#36164;&#28304;&#40657;&#30418;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#29992;&#20110;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. (arXiv:2303.08896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08896
&lt;/p&gt;
&lt;p&gt;
SelfCheckGPT&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#38646;&#36164;&#28304;&#30340;&#26041;&#24335;&#26816;&#26597;&#40657;&#30418;&#27169;&#22411;&#30340;&#24187;&#35273;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20363;&#22914;GPT-3&#65292;&#33021;&#22815;&#23545;&#21508;&#31181;&#29992;&#25143;&#25552;&#31034;&#36827;&#34892;&#39640;&#24230;&#27969;&#30021;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;LLM&#24050;&#30693;&#20250;&#20135;&#29983;&#24187;&#35273;&#20107;&#23454;&#21644;&#38750;&#20107;&#23454;&#38472;&#36848;&#65292;&#36825;&#21487;&#33021;&#20250;&#21066;&#24369;&#23545;&#23427;&#20204;&#30340;&#36755;&#20986;&#30340;&#20449;&#20219;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#26816;&#26597;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#35775;&#38382;&#20196;&#29260;&#32423;&#36755;&#20986;&#27010;&#29575;&#20998;&#24067;&#65288;&#36825;&#21487;&#33021;&#23545;&#20110;ChatGPT&#31561;&#31995;&#32479;&#26469;&#35828;&#19981;&#21487;&#29992;&#65289;&#65292;&#35201;&#20040;&#38656;&#35201;&#36890;&#36807;&#21333;&#29420;&#30340;&#36890;&#24120;&#22797;&#26434;&#30340;&#27169;&#22359;&#25509;&#21475;&#30340;&#22806;&#37096;&#25968;&#25454;&#24211;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;SelfCheckGPT&#8221;&#65292;&#21487;&#20197;&#20197;&#38646;&#36164;&#28304;&#30340;&#26041;&#24335;&#26816;&#26597;&#40657;&#30418;&#27169;&#22411;&#65292;&#21363;&#19981;&#38656;&#35201;&#22806;&#37096;&#25968;&#25454;&#24211;&#12290; SelfCheckGPT&#21033;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#24605;&#24819;&#65306;&#22914;&#26524;LLM&#20855;&#26377;&#29305;&#23450;&#27010;&#24565;&#30340;&#30693;&#35782;&#65292;&#21017;&#37319;&#26679;&#30340;&#21709;&#24212;&#21487;&#33021;&#31867;&#20284;&#24182;&#21253;&#21547;&#19968;&#33268;&#30340;&#20107;&#23454;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#24187;&#35273;&#30340;&#20107;&#23454;&#65292;&#38543;&#26426;&#37319;&#26679;&#30340;&#21709;&#24212;&#21487;&#33021;&#20250;&#21457;&#25955;&#24182;&#30456;&#20114;&#30683;&#30462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;GP-T-3&#27169;&#22411;&#20026;&#20363;&#26469;&#30740;&#31350;&#27492;&#26041;&#27861;&#65292;&#24182;&#22312;&#24120;&#35265;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SelfCheckGPT&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#27169;&#22411;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#19988;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to token-level output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GP
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07865</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Geolocation Predicting of Tweets Using BERT-Based Models. (arXiv:2303.07865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25512;&#25991;/&#29992;&#25143;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#22788;&#29702;&#25991;&#26412;&#22823;&#25968;&#25454;&#22320;&#29702;&#26631;&#35760;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#20272;&#35745;&#22352;&#26631;&#23545;&#65288;&#32463;&#24230;&#65292;&#32428;&#24230;&#65289;&#21644;&#20108;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#33539;&#22260;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#12290;&#24615;&#33021;&#25351;&#26631;&#34920;&#26126;&#65292;&#23545;&#20110;&#22312;&#25512;&#25991;&#20869;&#23481;&#21644;&#20803;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;30&#20844;&#37324;&#65292;&#32654;&#22269;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;15&#20844;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research is aimed to solve the tweet/user geolocation prediction task and provide a flexible methodology for the geotagging of textual big data. The suggested approach implements neural networks for natural language processing (NLP) to estimate the location as coordinate pairs (longitude, latitude) and two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder Representations from Transformers (BERT) as base models. Performance metrics show a median error of fewer than 30 km on a worldwide-level, and fewer than 15 km on the US-level datasets for the models trained and evaluated on text features of tweets' content and metadata context.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#25910;&#38598;&#29992;&#25143;&#23545;&#39033;&#30446;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#23545;&#35805;&#24335;&#25773;&#25918;&#21015;&#34920;&#26500;&#24314;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#33021;&#22815;&#36229;&#36234;&#21333;&#39033;&#20559;&#22909;&#65292;&#26356;&#21152;&#20840;&#38754;&#26377;&#25928;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.06791</link><description>&lt;p&gt;
&#36229;&#36234;&#21333;&#39033;&#65306;&#20351;&#29992;&#23545;&#35805;&#24335;&#38899;&#20048;&#25773;&#25918;&#21015;&#34920;&#26500;&#24314;&#25968;&#25454;&#25506;&#32034;&#29992;&#25143;&#23545;&#39033;&#30446;&#38598;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Beyond Single Items: Exploring User Preferences in Item Sets with the Conversational Playlist Curation Dataset. (arXiv:2303.06791v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06791
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#25910;&#38598;&#29992;&#25143;&#23545;&#39033;&#30446;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#23545;&#35805;&#24335;&#25773;&#25918;&#21015;&#34920;&#26500;&#24314;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#33021;&#22815;&#36229;&#36234;&#21333;&#39033;&#20559;&#22909;&#65292;&#26356;&#21152;&#20840;&#38754;&#26377;&#25928;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38899;&#20048;&#31561;&#28040;&#36153;&#39046;&#22495;&#65292;&#29992;&#25143;&#36890;&#24120;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25552;&#20379;&#23545;&#19968;&#32452;&#39033;&#30446;&#65288;&#20363;&#22914;&#25773;&#25918;&#21015;&#34920;&#25110;&#24191;&#25773;&#65289;&#30340;&#20559;&#22909;&#32780;&#19981;&#26159;&#23545;&#21333;&#20010;&#39033;&#30446;&#65288;&#20363;&#22914;&#27468;&#26354;&#65289;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#20165;&#38480;&#20110;&#20102;&#35299;&#21333;&#20010;&#39033;&#30446;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#39033;&#30446;&#32423;&#21644;&#38598;&#21512;&#32423;&#21453;&#39304;&#65292;&#26377;&#25928;&#22320;&#22312;&#20250;&#35805;&#29615;&#22659;&#20013;&#25910;&#38598;&#20851;&#20110;&#39033;&#30446;&#38598;&#30340;&#30495;&#23454;&#20559;&#22909;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#39033;&#20219;&#21153;&#31216;&#20026;&#23545;&#35805;&#24335;&#39033;&#30446;&#38598;&#21512;&#20316;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38899;&#20048;&#25512;&#33616;&#65292;&#26500;&#24314;&#20102;&#23545;&#35805;&#24335;&#25773;&#25918;&#21015;&#34920;&#26500;&#24314;&#25968;&#25454;&#38598;&#65288;CPC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users in consumption domains, like music, are often able to more efficiently provide preferences over a set of items (e.g. a playlist or radio) than over single items (e.g. songs). Unfortunately, this is an underexplored area of research, with most existing recommendation systems limited to understanding preferences over single items. Curating an item set exponentiates the search space that recommender systems must consider (all subsets of items!): this motivates conversational approaches-where users explicitly state or refine their preferences and systems elicit preferences in natural language-as an efficient way to understand user needs. We call this task conversational item set curation and present a novel data collection methodology that efficiently collects realistic preferences about item sets in a conversational setting by observing both item-level and set-level feedback. We apply this methodology to music recommendation to build the Conversational Playlist Curation Dataset (CPC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#23545;&#35937;&#27880;&#37322;&#30340;&#31934;&#32454;&#21270;VLP&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#21516;&#20041;&#35789;&#21477;&#23376;&#37325;&#20889;&#31639;&#27861;&#65292;&#35774;&#35745;&#20102;&#19977;&#31181;&#31934;&#32454;&#20219;&#21153;&#20197;&#25552;&#39640;&#31934;&#32454;&#30417;&#30563;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05313</link><description>&lt;p&gt;
&#31934;&#32454;&#21270;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Refined Vision-Language Modeling for Fine-grained Multi-modal Pre-training. (arXiv:2303.05313v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#23545;&#35937;&#27880;&#37322;&#30340;&#31934;&#32454;&#21270;VLP&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#21516;&#20041;&#35789;&#21477;&#23376;&#37325;&#20889;&#31639;&#27861;&#65292;&#35774;&#35745;&#20102;&#19977;&#31181;&#31934;&#32454;&#20219;&#21153;&#20197;&#25552;&#39640;&#31934;&#32454;&#30417;&#30563;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#35937;&#27880;&#37322;&#30340;&#31934;&#32454;&#21270;&#30417;&#30563;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#24050;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#36890;&#24120;&#20197;&#22270;&#20687;&#26631;&#39064;&#26684;&#24335;&#21576;&#29616;&#65292;&#20165;&#25552;&#20379;&#31895;&#31961;&#30340;&#30417;&#30563;&#12290;&#25910;&#38598;&#23545;&#35937;&#27880;&#37322;&#24182;&#20026;&#19981;&#21516;&#22330;&#26223;&#26500;&#24314;&#23545;&#35937;&#27880;&#37322;&#39044;&#25552;&#21462;&#22120;&#26082;&#26114;&#36149;&#21448;&#35745;&#31639;&#26114;&#36149;&#12290;&#26412;&#25991;&#20174;&#35821;&#35328;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#23545;&#35937;&#27880;&#37322;&#30340;&#31934;&#32454;&#21270;VLP&#26041;&#26696;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#21516;&#20041;&#35789;&#21477;&#23376;&#37325;&#20889;&#31639;&#27861;HSR&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#35789;&#20803;&#32423;&#21035;&#30340;&#30417;&#30563;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31934;&#32454;&#21270;&#35270;&#35273;-&#35821;&#35328;&#24314;&#27169;&#65288;RVLM&#65289;&#26694;&#26550;&#65292;&#20197;&#21033;&#29992;&#35789;&#20803;&#32423;&#21035;&#30340;&#30417;&#30563;&#65292;&#35774;&#35745;&#20102;&#19977;&#20010;&#31934;&#32454;&#20219;&#21153;&#65292;&#21363;&#31934;&#32454;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#65288;RITC&#65289;&#12289;&#31934;&#32454;&#30340;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#65288;RITM&#65289;&#21644;&#26367;&#25442;&#35821;&#35328;&#24314;&#27169;&#65288;RLM&#65289;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#31934;&#32454;&#21270;&#30417;&#30563;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained supervision based on object annotations has been widely used for vision and language pre-training (VLP). However, in real-world application scenarios, aligned multi-modal data is usually in the image-caption format, which only provides coarse-grained supervision. It is not only cost-expensive but also compute-expensive to collect object annotations and build object annotation pre-extractor for different scenarios. In this paper, we propose a fine-grained VLP scheme without object annotations from the linguistic perspective. First, we propose a homonym sentence rewriting (HSR) algorithm to provide token-level supervision. The algorithm replaces a verb/noun/adjective/quantifier word of the caption with its homonyms from WordNet. Correspondingly, we propose refined vision-language modeling (RVLM) framework to exploit the token-level supervision. Three refined tasks, i.e., refined image-text contrastive (RITC), refined image-text matching (RITM), and replace language modeling 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SemEval&#20219;&#21153;10&#65306;&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#65292;&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#30340;&#23618;&#27425;&#20998;&#31867;&#27861;&#65292;&#20197;&#21450;&#26032;&#30340;&#26631;&#31614;&#32454;&#31890;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#24110;&#21161;&#35299;&#20915;&#20108;&#20998;&#31867;&#26816;&#27979;&#24573;&#30053;&#20102;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.04222</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;10&#65306;&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SemEval-2023 Task 10: Explainable Detection of Online Sexism. (arXiv:2303.04222v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SemEval&#20219;&#21153;10&#65306;&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#65292;&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#30340;&#23618;&#27425;&#20998;&#31867;&#27861;&#65292;&#20197;&#21450;&#26032;&#30340;&#26631;&#31614;&#32454;&#31890;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#24110;&#21161;&#35299;&#20915;&#20108;&#20998;&#31867;&#26816;&#27979;&#24573;&#30053;&#20102;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#26159;&#19968;&#31181;&#24191;&#27867;&#19988;&#26377;&#23475;&#30340;&#29616;&#35937;&#12290;&#33258;&#21160;&#21270;&#24037;&#20855;&#21487;&#20197;&#21327;&#21161;&#22823;&#35268;&#27169;&#26816;&#27979;&#24615;&#21035;&#27495;&#35270;&#12290;&#28982;&#32780;&#65292;&#20108;&#20998;&#31867;&#26816;&#27979;&#24573;&#30053;&#20102;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#26080;&#27861;&#28165;&#26970;&#22320;&#35299;&#37322;&#20026;&#20160;&#20040;&#26576;&#20123;&#20869;&#23481;&#26159;&#24615;&#21035;&#27495;&#35270;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SemEval&#20219;&#21153;10&#65306;&#23545;&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#65288;EDOS&#65289;&#36827;&#34892;&#20102;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;i&#65289;&#19968;&#31181;&#26032;&#30340;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#30340;&#23618;&#27425;&#20998;&#31867;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#24615;&#21035;&#27495;&#35270;&#21521;&#37327;&#20197;&#24110;&#21161;&#35299;&#37322;&#65307; ii&#65289;&#19968;&#20010;&#26032;&#30340;&#21253;&#21547;&#32454;&#31890;&#24230;&#26631;&#31614;&#30340;2&#19975;&#20010;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#29992;&#20110;&#27169;&#22411;&#36866;&#24212;&#30340;&#26356;&#22823;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#65307;&#21644; iii&#65289;&#22522;&#32447;&#27169;&#22411;&#20197;&#21450;&#23545;&#21442;&#19982;&#32773;&#25552;&#20132;&#30340;&#20219;&#21153;&#30340;&#26041;&#27861;&#12289;&#32467;&#26524;&#21644;&#38169;&#35823;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online sexism is a widespread and harmful phenomenon. Automated tools can assist the detection of sexism at scale. Binary detection, however, disregards the diversity of sexist content, and fails to provide clear explanations for why something is sexist. To address this issue, we introduce SemEval Task 10 on the Explainable Detection of Online Sexism (EDOS). We make three main contributions: i) a novel hierarchical taxonomy of sexist content, which includes granular vectors of sexism to aid explainability; ii) a new dataset of 20,000 social media comments with fine-grained labels, along with larger unlabelled datasets for model adaptation; and iii) baseline models as well as an analysis of the methods, results and errors for participant submissions to our task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#26412;&#20307;&#23376;&#31867;&#25512;&#26029;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#28041;&#21450;&#21407;&#23376;&#27010;&#24565;&#21644;&#22797;&#21512;&#27010;&#24565;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#23545;&#23376;&#31867;&#25512;&#26029;&#32972;&#26223;&#30693;&#35782;&#30340;&#35760;&#24518;&#30456;&#23545;&#36739;&#23569;&#65292;&#20294;&#22312;&#32473;&#23450;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#21487;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.06761</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#26412;&#20307;&#23376;&#31867;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Language Model Analysis for Ontology Subsumption Inference. (arXiv:2302.06761v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#26412;&#20307;&#23376;&#31867;&#25512;&#26029;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#28041;&#21450;&#21407;&#23376;&#27010;&#24565;&#21644;&#22797;&#21512;&#27010;&#24565;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#23545;&#23376;&#31867;&#25512;&#26029;&#32972;&#26223;&#30693;&#35782;&#30340;&#35760;&#24518;&#30456;&#23545;&#36739;&#23569;&#65292;&#20294;&#22312;&#32473;&#23450;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#21487;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#31350;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#26367;&#20195;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#37117;&#20851;&#27880;&#20110;&#31616;&#21333;&#30340;&#19977;&#20803;&#32452;&#20851;&#31995;&#22411;&#30693;&#35782;&#24211;&#65292;&#24573;&#30053;&#20102;&#26356;&#20026;&#22797;&#26434;&#12289;&#36923;&#36753;&#20026;&#22522;&#30784;&#12289;&#27010;&#24565;&#21270;&#30340; OWL &#26412;&#20307;&#31561;&#30693;&#35782;&#24211;&#12290;&#20026;&#20102;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26412;&#20307;&#30340;&#20102;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986; OntoLAMA&#65292;&#23427;&#21253;&#21547;&#22522;&#20110;&#25512;&#29702;&#30340;&#19968;&#31995;&#21015;&#27979;&#35797;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#20174;&#28041;&#21450;&#21407;&#23376;&#27010;&#24565;&#21644;&#22797;&#21512;&#27010;&#24565;&#30340;&#23376;&#31867;&#25512;&#26029;&#20844;&#29702;&#20986;&#21457;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#39046;&#22495;&#21644;&#35268;&#27169;&#30340;&#26412;&#20307;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#23376;&#31867;&#25512;&#26029;&#30340;&#32972;&#26223;&#30693;&#35782;&#35760;&#24518;&#30456;&#23545;&#36739;&#23569;&#65292;&#20294;&#26159;&#22312;&#32473;&#23450;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23376;&#31867;&#25512;&#26029;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#28304;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.04391</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#26032;&#26631;&#31614;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23384;&#22312;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#24320;&#21457;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;90&#20998;&#20197;&#19978;&#30340;&#25104;&#32489;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25214;&#20986;&#22122;&#22768;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#27169;&#22411;&#39044;&#27979;&#20316;&#20026;&#20154;&#31867;&#26631;&#35760;&#30340;&#21442;&#32771;&#26469;&#37325;&#26032;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24819;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24207;&#21015;&#26631;&#35760;&#12289;&#29289;&#20307;&#26816;&#27979;&#12289;&#24207;&#21015;&#29983;&#25104;&#12289;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#30340;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26174;&#24335;&#23398;&#20064;&#38899;&#39057;&#21644;&#27468;&#35789;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23383;&#24149;&#65292;&#21462;&#24471;&#20102;&#20004;&#20010;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#39046;&#20808;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2212.10901</link><description>&lt;p&gt;
ALCAP: &#22522;&#20110;&#23545;&#40784;&#30340;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
ALCAP: Alignment-Augmented Music Captioner. (arXiv:2212.10901v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#30340;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26174;&#24335;&#23398;&#20064;&#38899;&#39057;&#21644;&#27468;&#35789;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23383;&#24149;&#65292;&#21462;&#24471;&#20102;&#20004;&#20010;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#39046;&#20808;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38899;&#20048;&#27969;&#23186;&#20307;&#24179;&#21488;&#29992;&#20110;&#38899;&#20048;&#25628;&#32034;&#21644;&#25512;&#33616;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#38656;&#35201;&#26032;&#26041;&#27861;&#26469;&#35299;&#37322;&#38899;&#20048;&#65292;&#21516;&#26102;&#32771;&#34385;&#27468;&#35789;&#21644;&#38899;&#39057;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#31934;&#32454;&#35843;&#25972;&#23558;&#38899;&#20048;&#26144;&#23556;&#21040;&#23383;&#24149;&#35760;&#21495;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#21508;&#20010;&#32452;&#20214;&#65292;&#24573;&#30053;&#20102;&#38899;&#39057;&#21644;&#27468;&#35789;&#20043;&#38388;&#23545;&#24212;&#30340;&#28508;&#22312;&#30410;&#22788;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#26174;&#24335;&#23398;&#20064;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23398;&#20064;&#38899;&#39057;-&#27468;&#35789;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20351;&#27169;&#22411;&#25351;&#23548;&#23398;&#20064;&#26356;&#22909;&#30340;&#36328;&#27169;&#24577;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23383;&#24149;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#32463;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20004;&#20010;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26032;&#30340;&#29366;&#24577;-&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing popularity of streaming media platforms for music search and recommendations has led to a need for novel methods for interpreting music that take into account both lyrics and audio. However, many previous works focus on refining individual components of encoder-decoder architecture that maps music to caption tokens, ignoring the potential benefits of correspondence between audio and lyrics. In this paper, we propose to explicitly learn the multimodal alignment through contrastive learning. By learning audio-lyrics correspondence, the model is guided to learn better cross-modal consistency, thus generating high-quality captions. We provide both theoretical and empirical results demonstrating the advantage of the proposed method, and achieve new state-of-the-art on two music captioning datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#30701;&#26263;&#19977;&#21512;&#19968;&#27979;&#39564;&#19978;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#65292;&#23384;&#22312;&#30456;&#23545;&#36739;&#26263;&#30340;&#20154;&#26684;&#27169;&#24335;&#12290;&#23613;&#31649;&#32463;&#36807;&#25351;&#26631;&#24494;&#35843;&#65292;&#20004;&#31181;&#27169;&#22411;&#20173;&#21576;&#29616;&#38544;&#21547;&#30340;&#40657;&#26263;&#20154;&#26684;&#27169;&#24335;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35266;&#23519;&#21040;GPT-3&#21644;InstructGPT&#30340;&#24184;&#31119;&#24863;&#24471;&#20998;&#25345;&#32493;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2212.10529</link><description>&lt;p&gt;
GPT-3&#26159;&#21542;&#23637;&#31034;&#20986;&#31934;&#31070;&#30149;&#24577;&#65311;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models from a Psychological Perspective. (arXiv:2212.10529v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#30701;&#26263;&#19977;&#21512;&#19968;&#27979;&#39564;&#19978;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#65292;&#23384;&#22312;&#30456;&#23545;&#36739;&#26263;&#30340;&#20154;&#26684;&#27169;&#24335;&#12290;&#23613;&#31649;&#32463;&#36807;&#25351;&#26631;&#24494;&#35843;&#65292;&#20004;&#31181;&#27169;&#22411;&#20173;&#21576;&#29616;&#38544;&#21547;&#30340;&#40657;&#26263;&#20154;&#26684;&#27169;&#24335;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35266;&#23519;&#21040;GPT-3&#21644;InstructGPT&#30340;&#24184;&#31119;&#24863;&#24471;&#20998;&#25345;&#32493;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#30830;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26080;&#20559;&#30340;&#25552;&#31034;&#26469;&#31995;&#32479;&#24615;&#22320;&#35780;&#20272;LLMs&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#20154;&#26684;&#27979;&#35797;&#8212;&#8212;&#30701;&#26263;&#19977;&#21512;&#19968;&#27979;&#39564;&#65288;SD-3&#65289;&#21644;&#22823;&#20116;&#20154;&#26684;&#38382;&#21367;&#65288;BFI&#65289;&#27979;&#35797;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;LLMs&#12290;&#25152;&#26377;&#27169;&#22411;&#22312;SD-3&#19978;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#65292;&#34920;&#26126;&#23384;&#22312;&#30456;&#23545;&#36739;&#26263;&#30340;&#20154;&#26684;&#27169;&#24335;&#12290;&#23613;&#31649;&#32463;&#36807;&#25351;&#26631;&#24494;&#35843;&#20197;&#20943;&#23569;&#27602;&#24615;&#65292;InstructGPT&#21644;FLAN-T5&#20173;&#28982;&#21576;&#29616;&#20986;&#38544;&#21547;&#30340;&#40657;&#26263;&#20154;&#26684;&#27169;&#24335;&#65307;&#22312;SD-3&#30340;&#29595;&#22522;&#38597;&#32500;&#21033;&#20027;&#20041;&#21644;&#33258;&#24651;&#29378;&#29305;&#24449;&#19978;&#65292;&#36825;&#20004;&#31181;&#27169;&#22411;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#33258;&#30417;&#30563;GPT-3&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24184;&#31119;&#24863;&#27979;&#35797;&#35780;&#20272;&#20102;GPT-3&#31995;&#21015;&#20013;&#30340;LLMs&#65292;&#20197;&#30740;&#31350;&#26356;&#22810;&#35757;&#32451;&#25968;&#25454;&#30340;&#24494;&#35843;&#23545;&#20854;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;GPT-3&#21644;InstructGPT&#30340;&#24184;&#31119;&#24863;&#24471;&#20998;&#25345;&#32493;&#22686;&#21152;&#12290;&#37492;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#27491;&#38754;&#22238;&#31572;&#20174;&#32780;&#25351;&#26631;&#24494;&#35843;FLAN-T5&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we determined whether large language models (LLMs) are psychologically safe. We designed unbiased prompts to systematically evaluate LLMs from a psychological perspective. First, we tested three different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored higher than the human average on SD-3, suggesting a relatively darker personality pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT and FLAN-T5 still showed implicit dark personality patterns; both models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT-3 series by using well-being tests to study the impact of fine-tuning with more training data. We observed a continuous increase in the well-being scores of GPT-3 and InstructGPT. Following these observations, we showed that instruction fine-tuning FLAN-T5 with positive answers from 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38382;&#39064;&#9472;&#9472;&#20107;&#20214;&#20010;&#20307;&#21270;&#23545;&#20110;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#26159;&#21542;&#36866;&#29992;&#65292;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2212.09702</link><description>&lt;p&gt;
&#35770;&#25991;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#20107;&#20214;&#20010;&#20307;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Event Individuation for Document-Level Information Extraction. (arXiv:2212.09702v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09702
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38382;&#39064;&#9472;&#9472;&#20107;&#20214;&#20010;&#20307;&#21270;&#23545;&#20110;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#26159;&#21542;&#36866;&#29992;&#65292;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#22312;&#22788;&#29702;&#25972;&#20010;&#25991;&#20214;&#26041;&#38754;&#36234;&#26469;&#36234;&#29087;&#32451;&#65292;&#20256;&#32479;&#30340;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#20316;&#20026;&#25991;&#20214;&#32423;&#20449;&#24687;&#25552;&#21462;&#30340;&#22522;&#20934;&#20219;&#21153;&#20877;&#27425;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#20102;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#22312;&#36825;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#35813;&#20219;&#21153;&#35201;&#27714;&#23545;&#20107;&#20214;&#20010;&#20307;&#21270;&#38382;&#39064;&#25552;&#20379;&#26126;&#30830;&#30340;&#31572;&#26696;&#8212;&#8212;&#21363;&#21306;&#20998;&#19981;&#21516;&#30340;&#20107;&#20214;&#8212;&#8212;&#32780;&#21363;&#20351;&#26159;&#20154;&#31867;&#19987;&#23478;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#20063;&#23384;&#22312;&#20998;&#27495;&#12290;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As information extraction (IE) systems have grown more adept at processing whole documents, the classic task of template filling has seen renewed interest as benchmark for document-level IE. In this position paper, we call into question the suitability of template filling for this purpose. We argue that the task demands definitive answers to thorny questions of event individuation -- the problem of distinguishing distinct events -- about which even human experts disagree. Through an annotation study and error analysis, we show that this raises concerns about the usefulness of template filling metrics, the quality of datasets for the task, and the ability of models to learn it. Finally, we consider possible solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;27&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;NL2Code&#30340;&#24212;&#29992;&#65292;&#24635;&#32467;&#20986;&#36825;&#20123;&#27169;&#22411;&#25104;&#21151;&#30340;&#19977;&#22823;&#20851;&#38190;&#22240;&#32032;&#65306;&#24040;&#22823;&#30340;&#27169;&#22411;&#23610;&#23544;&#12289;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;&#19987;&#23478;&#30340;&#35843;&#25972;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#36861;&#36394;&#26368;&#26032;&#36827;&#23637;&#30340;&#32593;&#31449;&#12290;</title><link>http://arxiv.org/abs/2212.09420</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;NL2Code&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Meet NL2Code: A Survey. (arXiv:2212.09420v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;27&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;NL2Code&#30340;&#24212;&#29992;&#65292;&#24635;&#32467;&#20986;&#36825;&#20123;&#27169;&#22411;&#25104;&#21151;&#30340;&#19977;&#22823;&#20851;&#38190;&#22240;&#32032;&#65306;&#24040;&#22823;&#30340;&#27169;&#22411;&#23610;&#23544;&#12289;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;&#19987;&#23478;&#30340;&#35843;&#25972;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#36861;&#36394;&#26368;&#26032;&#36827;&#23637;&#30340;&#32593;&#31449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#20195;&#30721;&#65292;&#21363;NL2Code&#65292;&#34987;&#35270;&#20026;&#20195;&#30721;&#26234;&#33021;&#20013;&#32039;&#36843;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#28044;&#29616;&#20986;&#20102;&#20026;&#20195;&#30721;&#25552;&#20379;&#25903;&#25345;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;NL2Code&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#20419;&#36827;&#27492;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#26412;&#25991;&#32508;&#36848;&#20102;27&#20010;&#29616;&#26377;&#30340;NL2Code&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22238;&#39038;&#20102;&#22522;&#20934;&#21644;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#22312;HumanEval&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20379;&#20102;&#23545;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#30340;&#30452;&#35266;&#27604;&#36739;&#12290;&#36890;&#36807;&#28145;&#20837;&#35266;&#23519;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#65292;&#24635;&#32467;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;NL2Code&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#8220;&#24040;&#22823;&#23610;&#23544;&#12289;&#39640;&#36136;&#37327;&#25968;&#25454;&#12289;&#19987;&#23478;&#35843;&#25972;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#20043;&#38388;&#24046;&#36317;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#32593;&#31449; https://nl2code.github.io&#65292;&#36890;&#36807;&#20247;&#21253;&#35780;&#20272;&#26469;&#36861;&#36394;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are "Large Size, Premium Data, Expert Tuning". In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#26816;&#32034;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#25512;&#21368;&#36131;&#20219;&#30340;&#38382;&#39064;&#65292;&#19988;&#26816;&#32034;&#22120;&#36873;&#25321;&#30340;&#21477;&#23376;&#21644;&#35821;&#35328;&#27169;&#22411;&#19981;&#32771;&#34385;&#21477;&#23376;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#37117;&#20250;&#24433;&#21709;&#25512;&#29702;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;ReForMask&#65292;&#37319;&#29992;&#25513;&#30721;&#26816;&#32034;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#35821;&#21477;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2212.09146</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#22791;&#25512;&#29702;&#33021;&#21147;&#65311;&#26816;&#32034;&#27169;&#22359;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#20105;
&lt;/p&gt;
&lt;p&gt;
Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model. (arXiv:2212.09146v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#26816;&#32034;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#25512;&#21368;&#36131;&#20219;&#30340;&#38382;&#39064;&#65292;&#19988;&#26816;&#32034;&#22120;&#36873;&#25321;&#30340;&#21477;&#23376;&#21644;&#35821;&#35328;&#27169;&#22411;&#19981;&#32771;&#34385;&#21477;&#23376;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#37117;&#20250;&#24433;&#21709;&#25512;&#29702;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;ReForMask&#65292;&#37319;&#29992;&#25513;&#30721;&#26816;&#32034;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#35821;&#21477;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#37319;&#29992;&#26816;&#32034;&#22120;&#26469;&#36873;&#25321;&#25903;&#25345;&#25991;&#26723;&#65292;&#22312;&#35299;&#20915;&#24120;&#35265;&#30340;NLP&#38382;&#39064;&#65288;&#21253;&#25324;&#35821;&#35328;&#24314;&#27169;&#21644;&#38382;&#31572;&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;REALM&#65292;kNN-LM&#65292;FiD&#21644;DPR&#65292;ATLAS&#21644;Flan-T5&#21644;Contriever&#32806;&#21512;&#65289;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#25512;&#29702;&#26816;&#32034;&#35821;&#21477;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26816;&#32034;-&#38405;&#35835;&#27169;&#22411;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#26082;&#26469;&#33258;&#26816;&#32034;&#27169;&#22359;&#65292;&#20063;&#26469;&#33258;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26816;&#32034;&#22120;&#20351;&#29992;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#36890;&#24120;&#19981;&#36275;&#20197;&#29992;&#20110;&#25512;&#29702;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#32771;&#34385;&#35821;&#21477;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#23548;&#33268;&#21363;&#20351;&#20351;&#29992;&#36739;&#22823;&#30340;&#27169;&#22411;&#65292;&#25512;&#29702;&#24615;&#33021;&#20063;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#26816;&#32034;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20013;&#38754;&#20020;&#30528;&#8220;&#36131;&#24618;&#28216;&#25103;&#8221;&#30340;&#38382;&#39064;&#65306;&#24403;&#26816;&#32034;&#22120;&#36873;&#25321;&#27491;&#30830;&#30340;&#35821;&#21477;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#25512;&#29702;&#65307;&#24403;&#26816;&#32034;&#22120;&#36873;&#25321;&#38169;&#35823;&#30340;&#35821;&#21477;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#33391;&#22909;&#30340;&#25512;&#29702;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;ReForMask&#65292;&#37319;&#29992;&#25513;&#30721;&#26816;&#32034;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#35821;&#21477;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReForMask&#22312;&#22810;&#31181;&#24120;&#35265;&#30340;NLP&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting pretrained language models with retrievers to select the supporting documents has shown promise in effectively solving common NLP problems, including language modeling and question answering, in an interpretable way. In this paper, we first study the strengths and weaknesses of different retriever-augmented language models (REALM, $k$NN-LM, FiD coupled with DPR, and ATLAS and Flan-T5 coupled with Contriever) in reasoning over the retrieved statements in different tasks. We show how the retrieve-then-read models' limitations in reasoning are rooted both in the retriever module as well as the language model. Our experimental results demonstrate that the similarity metric used by the retrievers is generally insufficient for reasoning tasks. Additionally, we show that the language models in retriever-augmented models do not take the complicated relations between the statements into account, which leads to poor reasoning performance even when using the larger models. Moreover, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26041;&#27861;&#26469;&#20174;&#20219;&#21153;&#35828;&#26126;&#20013;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;&#35828;&#26126;&#30340;&#21464;&#21270;&#24182;&#25552;&#39640;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.03813</link><description>&lt;p&gt;
&#20174;&#20219;&#21153;&#35828;&#26126;&#20070;&#20013;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustness of Learning from Task Instructions. (arXiv:2212.03813v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26041;&#27861;&#26469;&#20174;&#20219;&#21153;&#35828;&#26126;&#20013;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;&#35828;&#26126;&#30340;&#21464;&#21270;&#24182;&#25552;&#39640;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#22823;&#22810;&#22312;&#20010;&#21035;&#20219;&#21153;&#19978;&#36827;&#34892;&#65292;&#24182;&#38656;&#35201;&#22312;&#22823;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#19978;&#35757;&#32451;&#12290;&#36825;&#31181;&#33539;&#24335;&#20005;&#37325;&#38459;&#30861;&#20102;&#20219;&#21153;&#27010;&#25324;&#30340;&#21457;&#23637;&#65292;&#22240;&#20026;&#20934;&#22791;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#38598;&#26159;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#24555;&#36895;&#36731;&#26494;&#22320;&#25512;&#24191;&#21040;&#26032;&#20219;&#21153;&#30340;&#31995;&#32479;&#65292;&#26368;&#36817;&#37319;&#29992;&#20102;&#20219;&#21153;&#35828;&#26126;&#20316;&#20026;&#30417;&#30563;&#30340;&#26032;&#20852;&#36235;&#21183;&#12290;&#36825;&#20123;&#35828;&#26126;&#32473;&#27169;&#22411;&#23450;&#20041;&#20102;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#27169;&#22411;&#26681;&#25454;&#35828;&#26126;&#21644;&#36755;&#20837;&#36755;&#20986;&#36866;&#24403;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#20219;&#21153;&#35828;&#26126;&#36890;&#24120;&#20197;&#19981;&#21516;&#24418;&#24335;&#34920;&#36798;&#65292;&#21487;&#20197;&#20174;&#20004;&#20010;&#32447;&#32034;&#20013;&#35299;&#37322;&#65306;&#39318;&#20808;&#65292;&#19968;&#20123;&#35828;&#26126;&#26159;&#30701;&#21477;&#65292;&#24182;&#19988;&#26159;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#23548;&#21521;&#65292;&#20363;&#22914;&#25552;&#31034;&#65292;&#32780;&#20854;&#20182;&#35828;&#26126;&#26159;&#27573;&#33853;&#65292;&#24182;&#19988;&#26159;&#20154;&#20026;&#23548;&#21521;&#30340;&#65292;&#20363;&#22914;&#20122;&#39532;&#36874;&#30340;MTurk; &#20854;&#27425;&#65292;&#19981;&#21516;&#30340;&#26368;&#32456;&#29992;&#25143;&#24456;&#21487;&#33021;&#29992;&#19981;&#21516;&#30340;&#25991;&#26412;&#34920;&#36798;&#26041;&#24335;&#35299;&#37322;&#30456;&#21516;&#30340;&#20219;&#21153;&#12290;&#38656;&#35201;&#19968;&#31181;&#40065;&#26834;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#20219;&#21153;&#35828;&#26126;&#30340;&#21487;&#21464;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26041;&#27861;&#26469;&#20174;&#20219;&#21153;&#35828;&#26126;&#20013;&#23398;&#20064;&#65292;&#21487;&#20197;&#22788;&#29702;&#35828;&#26126;&#30340;&#21464;&#21270;&#24182;&#25913;&#21892;&#23545;&#26032;&#20219;&#21153;&#30340;&#27010;&#25324;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional supervised learning mostly works on individual tasks and requires training on a large set of task-specific examples. This paradigm seriously hinders the development of task generalization since preparing a task-specific example set is costly. To build a system that can quickly and easily generalize to new tasks, task instructions have been adopted as an emerging trend of supervision recently. These instructions give the model the definition of the task and allow the model to output the appropriate answer based on the instructions and inputs. However, task instructions are often expressed in different forms, which can be interpreted from two threads: first, some instructions are short sentences and are pretrained language model (PLM) oriented, such as prompts, while other instructions are paragraphs and are human-oriented, such as those in Amazon MTurk; second, different end-users very likely explain the same task with instructions of different textual expressions. A robust 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29992;&#25143;&#21382;&#21490;&#35821;&#35328;&#24314;&#27169;&#21487;&#20197;&#22312;&#19981;&#21516;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#24322;&#32467;&#26524;&#65292;&#24182;&#19988;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#30340;&#29992;&#25143;&#21382;&#21490;&#36824;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.03760</link><description>&lt;p&gt;
&#35821;&#35328;&#24314;&#27169;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65306;&#20016;&#23500;&#20219;&#21153;&#29305;&#23450;&#21644;&#20219;&#21153;&#26080;&#20851;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning. (arXiv:2212.03760v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29992;&#25143;&#21382;&#21490;&#35821;&#35328;&#24314;&#27169;&#21487;&#20197;&#22312;&#19981;&#21516;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#24322;&#32467;&#26524;&#65292;&#24182;&#19988;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#30340;&#29992;&#25143;&#21382;&#21490;&#36824;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#32479;&#19968;&#29992;&#25143;&#24314;&#27169;&#26694;&#26550;&#12290;&#20854;&#20013;&#35768;&#22810;&#21463;&#30410;&#20110;&#23558;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#20316;&#20026;&#32431;&#25991;&#26412;&#20351;&#29992;&#65292;&#20195;&#34920;&#30528;&#20219;&#20309;&#39046;&#22495;&#25110;&#31995;&#32479;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#32780;&#19981;&#22833;&#36890;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#38382;&#39064;&#20135;&#29983;&#20102;&#65306;&#29992;&#25143;&#21382;&#21490;&#35821;&#35328;&#24314;&#27169;&#33021;&#21542;&#24110;&#21161;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#65311;&#34429;&#28982;&#35821;&#35328;&#24314;&#27169;&#30340;&#22810;&#21151;&#33021;&#24615;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20854;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#20173;&#26410;&#28145;&#20837;&#25506;&#35752;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30452;&#25509;&#24212;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#21382;&#21490;&#30340;&#35821;&#35328;&#24314;&#27169;&#22312;&#19981;&#21516;&#30340;&#25512;&#33616;&#20219;&#21153;&#19978;&#21487;&#20197;&#21462;&#24471;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#30340;&#29992;&#25143;&#21382;&#21490;&#36824;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20026;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#26377;&#21069;&#36884;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#26410;&#30693;&#22495;&#21644;&#26381;&#21153;&#19978;&#20063;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have proposed unified user modeling frameworks that leverage user behavior data from various applications. Many of them benefit from utilizing users' behavior sequences as plain texts, representing rich information in any domain or system without losing generality. Hence, a question arises: Can language modeling for user history corpus help improve recommender systems? While its versatile usability has been widely investigated in many domains, its applications to recommender systems still remain underexplored. We show that language modeling applied directly to task-specific user histories achieves excellent results on diverse recommendation tasks. Also, leveraging additional task-agnostic user histories delivers significant performance benefits. We further demonstrate that our approach can provide promising transfer learning capabilities for a broad spectrum of real-world recommender systems, even on unseen domains and services.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08794</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#20302;&#36164;&#28304;&#24494;&#35843;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21442;&#25968;&#30340;&#24040;&#22823;&#25968;&#37327;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24494;&#35843;&#23481;&#26131;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#20986;&#29616;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20197;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#23618;&#20043;&#38388;&#25554;&#20837;&#38543;&#26426;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#26469;&#33258;&#21069;&#19968;&#23618;&#30340;&#28608;&#27963;&#36716;&#25442;&#20026;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#36865;&#21040;&#19978;&#23618;&#12290;&#24494;&#35843;&#32467;&#26463;&#21518;&#65292;&#33258;&#32534;&#30721;&#22120;&#20250;&#34987;&#31227;&#38500;&#25481;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#21442;&#25968;&#25110;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#24207;&#21015;&#21644;&#26631;&#35760;&#32423;&#21035;&#30340;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the huge amount of parameters, fine-tuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into a multi-view compressed representation before feeding it into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level low-resource NLP tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#25506;&#31350;&#22768;&#35843;&#21644;&#35821;&#35328;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#25991;&#26412;&#26391;&#35835;&#33258;&#28982;&#24230;&#39044;&#27979;&#65292;&#36890;&#36807;&#21152;&#20837;&#36825;&#20123;&#29305;&#24449;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#39057;&#35889;&#29305;&#24449;&#30340;&#22522;&#32447;&#27169;&#22411; MOSNet&#65292;&#22312; MOS &#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#24179;&#22343;12%&#26080;&#20844;&#23475;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2211.00342</link><description>&lt;p&gt;
&#25506;&#31350;&#22522;&#20110;&#22768;&#35843;&#21644;&#35821;&#35328;&#29305;&#24449;&#30340;&#31070;&#32463;&#25991;&#26412;&#26391;&#35835;&#33258;&#28982;&#24230;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Investigating Content-Aware Neural Text-To-Speech MOS Prediction Using Prosodic and Linguistic Features. (arXiv:2211.00342v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#25506;&#31350;&#22768;&#35843;&#21644;&#35821;&#35328;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#25991;&#26412;&#26391;&#35835;&#33258;&#28982;&#24230;&#39044;&#27979;&#65292;&#36890;&#36807;&#21152;&#20837;&#36825;&#20123;&#29305;&#24449;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#39057;&#35889;&#29305;&#24449;&#30340;&#22522;&#32447;&#27169;&#22411; MOSNet&#65292;&#22312; MOS &#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#24179;&#22343;12%&#26080;&#20844;&#23475;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#21512;&#25104;&#38899;&#22768;&#30340;&#26368;&#26032;&#35780;&#20272;&#26041;&#27861;&#26159;&#22522;&#20110; MOS &#39044;&#27979;&#31070;&#32463;&#27169;&#22411;&#65292;&#20854;&#20013; MOSNet &#21644; LDNet &#20351;&#29992;&#39057;&#35889;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#65292;&#32780; SSL-MOS &#21017;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#30452;&#25509;&#20351;&#29992;&#35821;&#38899;&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#12290;&#22312;&#29616;&#20195;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463; TTS &#31995;&#32479;&#20013;&#65292;&#23601;&#21457;&#38899;&#20869;&#23481;&#32780;&#35328;&#65292;&#22768;&#35843;&#30340;&#36866;&#24403;&#24615;&#26159;&#20915;&#23450;&#24615;&#30340;&#22240;&#32032;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312; MOS &#39044;&#27979;&#31995;&#32479;&#20013;&#21253;&#25324;&#22768;&#35843;&#21644;&#35821;&#35328;&#29305;&#24449;&#20316;&#20026;&#39069;&#22806;&#30340;&#36755;&#20837;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#38899;&#32032;&#32423;&#30340; F0 &#21644;&#25345;&#32493;&#26102;&#38388;&#29305;&#24449;&#20316;&#20026;&#22768;&#35843;&#36755;&#20837;&#65292;&#20197;&#21450; Tacotron &#32534;&#30721;&#22120;&#36755;&#20986;&#12289;POS &#26631;&#35760;&#21644; BERT &#23884;&#20837;&#20316;&#20026;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#35328;&#36755;&#20837;&#12290;&#25152;&#26377; MOS &#39044;&#27979;&#31995;&#32479;&#22343;&#22312; SOMOS &#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#20165;&#26377;&#31070;&#32463;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;&#21644;&#20247;&#21253;&#33258;&#28982;&#24230; MOS &#35780;&#20272;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#39069;&#22806;&#29305;&#24449;&#33021;&#22815;&#26377;&#21033;&#20110;&#25552;&#39640; MOS &#39044;&#27979;&#32467;&#26524;&#65292;&#30456;&#23545;&#20110;&#20165;&#20351;&#29992;&#39057;&#35889;&#29305;&#24449;&#30340;&#22522;&#32447; MOSNet&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;12%&#12290;
&lt;/p&gt;
&lt;p&gt;
Current state-of-the-art methods for automatic synthetic speech evaluation are based on MOS prediction neural models. Such MOS prediction models include MOSNet and LDNet that use spectral features as input, and SSL-MOS that relies on a pretrained self-supervised learning model that directly uses the speech signal as input. In modern high-quality neural TTS systems, prosodic appropriateness with regard to the spoken content is a decisive factor for speech naturalness. For this reason, we propose to include prosodic and linguistic features as additional inputs in MOS prediction systems, and evaluate their impact on the prediction outcome. We consider phoneme level F0 and duration features as prosodic inputs, as well as Tacotron encoder outputs, POS tags and BERT embeddings as higher-level linguistic inputs. All MOS prediction systems are trained on SOMOS, a neural TTS-only dataset with crowdsourced naturalness MOS evaluations. Results show that the proposed additional features are benefi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#27169;&#22359;&#21270;&#25552;&#31034;&#26041;&#27861;&#65288;MP2&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#20013;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#25552;&#31034;&#35843;&#25972;&#30340;&#19981;&#36275;&#12290;MP2&#26159;&#19968;&#32452;&#22312;38&#20010;&#20013;&#25991;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#30340;&#21487;&#32452;&#21512;&#25552;&#31034;&#65292;&#33021;&#22815;&#22312;&#38754;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#26102;&#20855;&#22791;&#24378;&#22823;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#20004;&#31181;&#23398;&#20064;&#33539;&#24335;&#19979;&#30340;&#23454;&#39564;&#20013;&#65292;MP2&#22312;&#23569;&#26679;&#26412;&#24773;&#24418;&#19979;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.07565</link><description>&lt;p&gt;
&#8220;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#27169;&#22359;&#21270;&#25552;&#31034;&#22312;&#20013;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#8221;
&lt;/p&gt;
&lt;p&gt;
Multitask Pre-training of Modular Prompt for Chinese Few-Shot Learning. (arXiv:2210.07565v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#27169;&#22359;&#21270;&#25552;&#31034;&#26041;&#27861;&#65288;MP2&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#20013;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#25552;&#31034;&#35843;&#25972;&#30340;&#19981;&#36275;&#12290;MP2&#26159;&#19968;&#32452;&#22312;38&#20010;&#20013;&#25991;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#30340;&#21487;&#32452;&#21512;&#25552;&#31034;&#65292;&#33021;&#22815;&#22312;&#38754;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#26102;&#20855;&#22791;&#24378;&#22823;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#20004;&#31181;&#23398;&#20064;&#33539;&#24335;&#19979;&#30340;&#23454;&#39564;&#20013;&#65292;MP2&#22312;&#23569;&#26679;&#26412;&#24773;&#24418;&#19979;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#26159;&#19968;&#31181;&#21442;&#25968;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#34429;&#28982;&#22312;&#35757;&#32451;&#25968;&#25454;&#20805;&#36275;&#26102;&#65292;&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#19982;&#23436;&#20840;&#27169;&#22411;&#35843;&#25972;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#20294;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#23427;&#24448;&#24448;&#38590;&#20197;&#32988;&#20219;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#27169;&#22359;&#21270;&#25552;&#31034;&#65288;MP2&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#25552;&#31034;&#35843;&#25972;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#34920;&#29616;&#12290;MP2&#26159;&#19968;&#32452;&#22312;38&#20010;&#20013;&#25991;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#30340;&#21487;&#32452;&#21512;&#25552;&#31034;&#12290;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#39044;&#35757;&#32451;&#30340;&#25552;&#31034;&#21487;&#20197;&#34987;&#36873;&#25321;&#24615;&#22320;&#28608;&#27963;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#22312;&#38754;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#26102;&#20855;&#22791;&#24378;&#22823;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#24357;&#21512;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;&#19978;&#28216;&#21644;&#19979;&#28216;&#20219;&#21153;&#32479;&#19968;&#20026;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#12290;&#22312;&#20004;&#31181;&#23398;&#20064;&#33539;&#24335;&#21363;&#26799;&#24230;&#19979;&#38477;&#21644;&#40657;&#30418;&#35843;&#25972;&#19979;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;MP2&#22312;&#23569;&#26679;&#26412;&#24773;&#24418;&#19979;&#26174;&#33879;&#20248;&#20110;&#25552;&#31034;&#35843;&#25972;&#12289;&#23436;&#20840;&#27169;&#22411;&#35843;&#25972;&#21644;&#20808;&#21069;&#30340;&#25552;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning is a parameter-efficient approach to adapting pre-trained language models to downstream tasks. Although prompt tuning has been shown to match the performance of full model tuning when training data is sufficient, it tends to struggle in few-shot learning settings. In this paper, we present Multi-task Pre-trained Modular Prompt (MP2) to boost prompt tuning for few-shot learning. MP2 is a set of combinable prompts pre-trained on 38 Chinese tasks. On downstream tasks, the pre-trained prompts are selectively activated and combined, leading to strong compositional generalization to unseen tasks. To bridge the gap between pre-training and fine-tuning, we formulate upstream and downstream tasks into a unified machine reading comprehension task. Extensive experiments under two learning paradigms, i.e., gradient descent and black-box tuning, show that MP2 significantly outperforms prompt tuning, full model tuning, and prior prompt pre-training methods in few-shot settings. In addi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#35843;&#26597;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#23545;&#31038;&#20250;&#21644;&#32593;&#32476;&#23433;&#20840;&#25152;&#24102;&#26469;&#30340;&#23041;&#32961;&#65292;&#25552;&#20379;&#20102;&#26368;&#23436;&#25972;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#35780;&#20272;&#65292;&#20026;&#24212;&#23545;&#23041;&#32961;&#27169;&#22411;&#21644;&#35299;&#20915;&#26816;&#27979;&#38382;&#39064;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2210.07321</link><description>&lt;p&gt;
&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65306;&#23041;&#32961;&#27169;&#22411;&#21644;&#26816;&#27979;&#26041;&#27861;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods. (arXiv:2210.07321v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#35843;&#26597;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#23545;&#31038;&#20250;&#21644;&#32593;&#32476;&#23433;&#20840;&#25152;&#24102;&#26469;&#30340;&#23041;&#32961;&#65292;&#25552;&#20379;&#20102;&#26368;&#23436;&#25972;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#35780;&#20272;&#65292;&#20026;&#24212;&#23545;&#23041;&#32961;&#27169;&#22411;&#21644;&#35299;&#20915;&#26816;&#27979;&#38382;&#39064;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36234;&#26469;&#36234;&#38590;&#20197;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#21306;&#20998;&#24320;&#26469;&#12290;&#21151;&#33021;&#24378;&#22823;&#30340;&#24320;&#28304;&#27169;&#22411;&#21487;&#20197;&#20813;&#36153;&#20351;&#29992;&#65292;&#21487;&#27665;&#20027;&#21270;&#35775;&#38382;&#29983;&#25104;&#27169;&#22411;&#30340;&#29992;&#25143;&#21451;&#22909;&#24037;&#20855;&#27491;&#22312;&#36805;&#36895;&#22686;&#22810;&#12290;&#26412;&#27425;&#35843;&#26597;&#30340;&#31532;&#19968;&#29256;&#38754;&#19990;&#21518;&#19981;&#20037;&#65292;&#21457;&#24067;&#20102;ChatGPT&#65292;&#36825;&#19968;&#36235;&#21183;&#34987;&#24432;&#26174;&#20986;&#26469;&#12290;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31995;&#32479;&#30340;&#24040;&#22823;&#28508;&#21147;&#34987;&#21508;&#31181;&#28389;&#29992;&#36884;&#24452;&#25152;&#25233;&#21046;&#12290;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26159;&#20943;&#23569;NLG&#27169;&#22411;&#28389;&#29992;&#30340;&#20027;&#35201;&#23545;&#31574;&#65292;&#20294;&#20063;&#38754;&#20020;&#30528;&#37325;&#22823;&#25216;&#26415;&#25361;&#25112;&#21644;&#20247;&#22810;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#65292;&#21253;&#25324;1&#65289;&#23545;&#24403;&#20195;NLG&#31995;&#32479;&#36896;&#25104;&#23041;&#32961;&#27169;&#22411;&#30340;&#24191;&#27867;&#20998;&#26512;&#21644;2&#65289;&#25130;&#33267;&#30446;&#21069;&#20026;&#27490;&#20851;&#20110;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#30340;&#26368;&#23436;&#25972;&#30340;&#32508;&#36848;&#12290;&#36825;&#20221;&#35843;&#26597;&#23558;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#32622;&#20110;&#20854;&#32593;&#32476;&#23433;&#20840;&#21644;&#31038;&#20250;&#32972;&#26223;&#20043;&#20013;&#65292;&#24182;&#20026;&#26410;&#26469;&#35299;&#20915;&#26368;&#37325;&#35201;&#23041;&#32961;&#27169;&#22411;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#25351;&#23548;&#65292;&#24182;&#27010;&#36848;&#20102;&#26368;&#26377;&#21069;&#36884;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine generated text is increasingly difficult to distinguish from human authored text. Powerful open-source models are freely available, and user-friendly tools that democratize access to generative models are proliferating. ChatGPT, which was released shortly after the first edition of this survey, epitomizes these trends. The great potential of state-of-the-art natural language generation (NLG) systems is tempered by the multitude of avenues for abuse. Detection of machine generated text is a key countermeasure for reducing abuse of NLG models, with significant technical challenges and numerous open problems. We provide a survey that includes both 1) an extensive analysis of threat models posed by contemporary NLG systems, and 2) the most complete review of machine generated text detection methods to date. This survey places machine generated text within its cybersecurity and social context, and provides strong guidance for future work addressing the most critical threat models, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#21516;&#20041;&#35789;&#26367;&#25442;&#25915;&#20987;&#30340;&#29616;&#29366;&#65292;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#26080;&#35299;&#20915;&#30340;&#38556;&#30861;&#65292;&#29983;&#25104;&#30340;&#25932;&#23545;&#26679;&#26412;&#25928;&#26524;&#19981;&#20339;&#65292;&#38656;&#35201;&#22312;&#26410;&#26469;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.02844</link><description>&lt;p&gt;
&#31163;&#30495;&#27491;&#30340;&#21516;&#20041;&#35789;&#26367;&#25442;&#25915;&#20987;&#36824;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We from Real Synonym Substitution Attacks?. (arXiv:2210.02844v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#21516;&#20041;&#35789;&#26367;&#25442;&#25915;&#20987;&#30340;&#29616;&#29366;&#65292;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#26080;&#35299;&#20915;&#30340;&#38556;&#30861;&#65292;&#29983;&#25104;&#30340;&#25932;&#23545;&#26679;&#26412;&#25928;&#26524;&#19981;&#20339;&#65292;&#38656;&#35201;&#22312;&#26410;&#26469;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#36317;&#31163;&#30495;&#27491;&#30340;&#21516;&#20041;&#35789;&#26367;&#25442;&#25915;&#20987;&#26377;&#22810;&#36828;&#65311;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#21516;&#20041;&#35789;&#26367;&#25442;&#25915;&#20987;&#22914;&#20309;&#26367;&#25442;&#21407;&#22987;&#21477;&#23376;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#26174;&#31034;&#24403;&#21069;&#21516;&#20041;&#35789;&#26367;&#25442;&#25915;&#20987;&#20173;&#23384;&#22312;&#26410;&#35299;&#20915;&#30340;&#38556;&#30861;&#20351;&#24471;&#29983;&#25104;&#30340;&#25932;&#23545;&#26679;&#26412;&#26159;&#26080;&#25928;&#30340;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#22235;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21333;&#35789;&#26367;&#25442;&#26041;&#27861;&#29983;&#25104;&#22823;&#37327;&#26080;&#25928;&#26367;&#25442;&#35789;&#65292;&#36825;&#20123;&#35789;&#26159;&#19981;&#21512;&#35821;&#27861;&#30340;&#25110;&#19981;&#31526;&#21512;&#21407;&#22987;&#21477;&#23376;&#30340;&#35821;&#20041;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26816;&#27979;&#26080;&#25928;&#25932;&#23545;&#26679;&#26412;&#26041;&#38754;&#65292;SSA&#25152;&#20351;&#29992;&#30340;&#35821;&#20041;&#21644;&#35821;&#27861;&#32422;&#26463;&#26159;&#39640;&#24230;&#19981;&#36275;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#26410;&#26469;&#26500;&#24314;&#26356;&#22909;&#30340;SSA&#30340;&#37325;&#35201;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the following question: how far are we from real synonym substitution attacks (SSAs). We approach this question by examining how SSAs replace words in the original sentence and show that there are still unresolved obstacles that make current SSAs generate invalid adversarial samples. We reveal that four widely used word substitution methods generate a large fraction of invalid substitution words that are ungrammatical or do not preserve the original sentence's semantics. Next, we show that the semantic and grammatical constraints used in SSAs for detecting invalid word replacements are highly insufficient in detecting invalid adversarial samples. Our work is an important stepping stone to constructing better SSAs in the future.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38899;&#20048;&#25991;&#26412;&#35270;&#35273;&#20132;&#24863;&#38382;&#39064;&#65292;&#25910;&#38598;&#20102;&#23545;&#40784;&#30340;&#25968;&#25454;&#38598;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#29983;&#25104;&#25551;&#36848;&#38899;&#20048;&#24405;&#38899;&#20869;&#23481;&#30340;&#21477;&#23376;&#65292;&#24182;&#35774;&#35745;&#20102;&#32676;&#25299;&#25169;&#20445;&#25345;&#25439;&#22833;&#26469;&#35299;&#20915;&#39640;&#38750;&#21028;&#21035;&#24615;&#30340;&#21476;&#20856;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2210.00434</link><description>&lt;p&gt;
&#38899;&#20048;&#25991;&#26412;&#35270;&#35273;&#20132;&#24863;&#65306;&#20174;&#38899;&#20048;&#24405;&#38899;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Music-to-Text Synaesthesia: Generating Descriptive Text from Music Recordings. (arXiv:2210.00434v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38899;&#20048;&#25991;&#26412;&#35270;&#35273;&#20132;&#24863;&#38382;&#39064;&#65292;&#25910;&#38598;&#20102;&#23545;&#40784;&#30340;&#25968;&#25454;&#38598;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#29983;&#25104;&#25551;&#36848;&#38899;&#20048;&#24405;&#38899;&#20869;&#23481;&#30340;&#21477;&#23376;&#65292;&#24182;&#35774;&#35745;&#20102;&#32676;&#25299;&#25169;&#20445;&#25345;&#25439;&#22833;&#26469;&#35299;&#20915;&#39640;&#38750;&#21028;&#21035;&#24615;&#30340;&#21476;&#20856;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#38899;&#20048;&#25991;&#26412;&#35270;&#35273;&#20132;&#24863;&#12290;&#19981;&#21516;&#20110;&#25226;&#38899;&#20048;&#24405;&#38899;&#20998;&#31867;&#21040;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#30340;&#32463;&#20856;&#38899;&#20048;&#26631;&#35760;&#38382;&#39064;&#65292;&#38899;&#20048;&#25991;&#26412;&#35270;&#35273;&#20132;&#24863;&#26088;&#22312;&#29983;&#25104;&#20855;&#26377;&#30456;&#21516;&#24773;&#24863;&#30340;&#38899;&#20048;&#24405;&#38899;&#30340;&#25551;&#36848;&#24615;&#25991;&#26412;&#65292;&#20197;&#20415;&#36827;&#19968;&#27493;&#29702;&#35299;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#38899;&#20048;&#30456;&#20851;&#25968;&#25454;&#38598;&#19981;&#21253;&#21547;&#38899;&#20048;&#24405;&#38899;&#30340;&#35821;&#20041;&#25551;&#36848;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;1,955&#20010;&#21476;&#20856;&#38899;&#20048;&#24405;&#38899;&#19982;&#25991;&#26412;&#25551;&#36848;&#30340;&#23545;&#40784;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#29983;&#25104;&#21487;&#20197;&#25551;&#36848;&#38899;&#20048;&#24405;&#38899;&#20869;&#23481;&#30340;&#21477;&#23376;&#12290;&#20026;&#20102;&#35299;&#20915;&#39640;&#24230;&#38750;&#21028;&#21035;&#24615;&#30340;&#21476;&#20856;&#38899;&#20048;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32676;&#25299;&#25169;&#20445;&#25345;&#25439;&#22833;&#65292;&#23427;&#32771;&#34385;&#26356;&#22810;&#30340;&#26679;&#26412;&#20316;&#20026;&#32676;&#32452;&#21442;&#32771;&#65292;&#24182;&#20445;&#30041;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#23545;&#25299;&#25169;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#25351;&#26631;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider a novel research problem: music-to-text synaesthesia. Different from the classical music tagging problem that classifies a music recording into pre-defined categories, music-to-text synaesthesia aims to generate descriptive texts from music recordings with the same sentiment for further understanding. As existing music-related datasets do not contain the semantic descriptions on music recordings, we collect a new dataset that contains 1,955 aligned pairs of classical music recordings and text descriptions. Based on this, we build a computational model to generate sentences that can describe the content of the music recording. To tackle the highly non-discriminative classical music, we design a group topology-preservation loss, which considers more samples as a group reference and preserves the relative topology among different samples. Extensive experimental results qualitatively and quantitatively demonstrate the effectiveness of our proposed model over five
&lt;/p&gt;</description></item><item><title>Vega-MT&#26159;JD Explore Academy&#20026;WMT22&#20849;&#20139;&#30340;&#32763;&#35793;&#20219;&#21153;&#24320;&#21457;&#30340;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#25193;&#22823;&#20102;&#35821;&#35328;&#23545;&#21644;&#27169;&#22411;&#22823;&#23567;&#65292;&#23454;&#29616;&#20102;&#8220;&#21452;&#21521;&#8221;&#21040;&#8220;&#22810;&#21521;&#8221;&#30340;&#25299;&#23637;&#24182;&#37319;&#21462;&#20102;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.09444</link><description>&lt;p&gt;
Vega-MT: JD Explore Academy&#30340;WMT22&#32763;&#35793;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Vega-MT: The JD Explore Academy Translation System for WMT22. (arXiv:2209.09444v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09444
&lt;/p&gt;
&lt;p&gt;
Vega-MT&#26159;JD Explore Academy&#20026;WMT22&#20849;&#20139;&#30340;&#32763;&#35793;&#20219;&#21153;&#24320;&#21457;&#30340;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#25193;&#22823;&#20102;&#35821;&#35328;&#23545;&#21644;&#27169;&#22411;&#22823;&#23567;&#65292;&#23454;&#29616;&#20102;&#8220;&#21452;&#21521;&#8221;&#21040;&#8220;&#22810;&#21521;&#8221;&#30340;&#25299;&#23637;&#24182;&#37319;&#21462;&#20102;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;JD Explore Academy&#21442;&#21152;WMT 2022&#20849;&#20139;&#32763;&#35793;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#21442;&#21152;&#20102;&#25152;&#26377;&#30340;&#39640;&#36164;&#28304;&#36712;&#36947;&#21644;&#19968;&#20010;&#20013;&#36164;&#28304;&#36712;&#36947;&#65292;&#21253;&#25324;&#20013;&#33521;&#25991;&#12289;&#24503;&#33521;&#25991;&#12289;&#25463;&#33521;&#25991;&#12289;&#20420;&#33521;&#25991;&#21644;&#26085;&#33521;&#25991;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;&#35821;&#35328;&#23545;&#21644;&#27169;&#22411;&#22823;&#23567;&#36825;&#20004;&#20010;&#20027;&#35201;&#22240;&#32032;&#65292;&#21363;Vega-MT&#31995;&#32479;&#65292;&#26469;&#25512;&#21160;&#25105;&#20204;&#20043;&#21069;&#36827;&#34892;&#30340;&#32763;&#35793;&#30340;&#21452;&#21521;&#35757;&#32451;&#30340;&#26497;&#38480;&#12290;&#23601;&#35821;&#35328;&#23545;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#8220;&#21452;&#21521;&#8221;&#25193;&#23637;&#21040;&#8220;&#22810;&#21521;&#8221;&#35774;&#32622;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#21442;&#19982;&#30340;&#35821;&#35328;&#65292;&#20197;&#21033;&#29992;&#36328;&#35821;&#35328;&#30340;&#20849;&#21516;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#19979;&#28216;&#30340;&#21452;&#35821;&#20219;&#21153;&#20013;&#12290;&#23601;&#27169;&#22411;&#22823;&#23567;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;Transformer-Big&#25193;&#23637;&#21040;&#20960;&#20046;&#25317;&#26377;47&#20159;&#21442;&#25968;&#30340;&#26497;&#22823;&#27169;&#22411;&#65292;&#20197;&#20805;&#20998;&#25552;&#21319;&#25105;&#20204;&#30340;Vega-MT&#30340;&#27169;&#22411;&#23481;&#37327;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37319;&#21462;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#22914;&#21333;&#35821;&#25968;&#25454;&#30340;&#24490;&#29615;&#32763;&#35793;&#21644;&#21452;&#35821;&#33258;&#25105;&#35757;&#32451;&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe the JD Explore Academy's submission of the WMT 2022 shared general translation task. We participated in all high-resource tracks and one medium-resource track, including Chinese-English, German-English, Czech-English, Russian-English, and Japanese-English. We push the limit of our previous work -- bidirectional training for translation by scaling up two main factors, i.e. language pairs and model sizes, namely the \textbf{Vega-MT} system. As for language pairs, we scale the "bidirectional" up to the "multidirectional" settings, covering all participating languages, to exploit the common knowledge across languages, and transfer them to the downstream bilingual tasks. As for model sizes, we scale the Transformer-Big up to the extremely large model that owns nearly 4.7 Billion parameters, to fully enhance the model capacity for our Vega-MT. Also, we adopt the data augmentation strategies, e.g. cycle translation for monolingual data, and bidirectional self-training for bilingua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24555;&#25463;&#23398;&#20064;&#21644;&#40065;&#26834;&#24615;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#27861;&#21644;&#30456;&#20851;&#30740;&#31350;&#65292;&#21253;&#25324;&#35782;&#21035;&#20854;&#24555;&#25463;&#23398;&#20064;&#34892;&#20026;&#12289;&#21407;&#22240;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#35752;&#20102;&#39046;&#22495;&#30340;&#20027;&#35201;&#30740;&#31350;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2208.11857</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#25463;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Shortcut Learning of Large Language Models in Natural Language Understanding. (arXiv:2208.11857v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24555;&#25463;&#23398;&#20064;&#21644;&#40065;&#26834;&#24615;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#27861;&#21644;&#30456;&#20851;&#30740;&#31350;&#65292;&#21253;&#25324;&#35782;&#21035;&#20854;&#24555;&#25463;&#23398;&#20064;&#34892;&#20026;&#12289;&#21407;&#22240;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#35752;&#20102;&#39046;&#22495;&#30340;&#20027;&#35201;&#30740;&#31350;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#21487;&#33021;&#20250;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#21644;&#32570;&#38519;&#20316;&#20026;&#39044;&#27979;&#30340;&#24555;&#25463;&#26041;&#24335;&#12290;&#36825;&#26174;&#33879;&#22320;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#35299;&#20915;LLMs&#24555;&#25463;&#23398;&#20064;&#21644;&#40065;&#26834;&#24615;&#25361;&#25112;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#25463;&#23398;&#20064;&#27010;&#24565;&#12290;&#28982;&#21518;&#20171;&#32461;&#20102;&#35782;&#21035;&#35821;&#35328;&#27169;&#22411;&#24555;&#25463;&#23398;&#20064;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#34920;&#24449;&#24555;&#25463;&#23398;&#20064;&#30340;&#21407;&#22240;&#65292;&#24182;&#20171;&#32461;&#20102;&#32531;&#35299;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;LLMs&#39046;&#22495;&#30340;&#20027;&#35201;&#30740;&#31350;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved state-of-the-art performance on a series of natural language understanding tasks. However, these LLMs might rely on dataset bias and artifacts as shortcuts for prediction. This has significantly affected their generalizability and adversarial robustness. In this paper, we provide a review of recent developments that address the shortcut learning and robustness challenge of LLMs. We first introduce the concepts of shortcut learning of language models. We then introduce methods to identify shortcut learning behavior in language models, characterize the reasons for shortcut learning, as well as introduce mitigation solutions. Finally, we discuss key research challenges and potential research directions in order to advance the field of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#21578;&#22270;&#29255;&#35821;&#26009;&#24211;&#21644;&#35828;&#26381;&#31574;&#30053;&#39044;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#35774;&#35745;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#24191;&#21578;&#20013;&#30340;&#35828;&#26381;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2208.09626</link><description>&lt;p&gt;
&#24191;&#21578;&#20013;&#30340;&#35828;&#26381;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Persuasion Strategies in Advertisements. (arXiv:2208.09626v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#21578;&#22270;&#29255;&#35821;&#26009;&#24211;&#21644;&#35828;&#26381;&#31574;&#30053;&#39044;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#35774;&#35745;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#24191;&#21578;&#20013;&#30340;&#35828;&#26381;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24191;&#21578;&#20013;&#30340;&#35828;&#26381;&#21147;&#23545;&#20110;&#30740;&#31350;&#23459;&#20256;&#12289;&#31038;&#20250;&#24515;&#29702;&#23398;&#21644;&#33829;&#38144;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#20855;&#26377;&#19982;&#24191;&#21578;&#30456;&#20851;&#30340;&#35828;&#26381;&#31574;&#30053;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#35828;&#26381;&#24314;&#27169;&#20173;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#27867;&#30340;&#35828;&#26381;&#31574;&#30053;&#35789;&#27719;&#65292;&#24182;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#27880;&#37322;&#26377;&#35828;&#26381;&#31574;&#30053;&#30340;&#24191;&#21578;&#22270;&#29255;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#35828;&#26381;&#31574;&#30053;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#20854;&#20182;&#24191;&#21578;&#29702;&#35299;&#20219;&#21153;&#26469;&#39044;&#27979;&#35828;&#26381;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling what makes an advertisement persuasive, i.e., eliciting the desired response from consumer, is critical to the study of propaganda, social psychology, and marketing. Despite its importance, computational modeling of persuasion in computer vision is still in its infancy, primarily due to the lack of benchmark datasets that can provide persuasion-strategy labels associated with ads. Motivated by persuasion literature in social psychology and marketing, we introduce an extensive vocabulary of persuasion strategies and build the first ad image corpus annotated with persuasion strategies. We then formulate the task of persuasion strategy prediction with multi-modal learning, where we design a multi-task attention fusion model that can leverage other ad-understanding tasks to predict persuasion strategies. Further, we conduct a real-world case study on 1600 advertising campaigns of 30 Fortune-500 companies where we use our model's predictions to analyze which strategies work with di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#27969;&#31243;&#65292;&#36890;&#36807;&#39046;&#22495;&#36890;&#29992;&#30701;&#35821;&#39044;&#35757;&#32451;&#12289;&#36801;&#31227;&#26631;&#35760;&#21644;&#26377;&#38480;&#30495;&#23454;&#26631;&#27880;&#25968;&#25454;&#24494;&#35843;&#26469;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#20351;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#65288;KPG&#65289;&#27169;&#22411;&#20855;&#22791;&#26356;&#24378;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.09606</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#36866;&#24212;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#30340;&#36890;&#29992;&#21040;&#29305;&#23450;&#36801;&#31227;&#26631;&#35760;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
General-to-Specific Transfer Labeling for Domain Adaptable Keyphrase Generation. (arXiv:2208.09606v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#27969;&#31243;&#65292;&#36890;&#36807;&#39046;&#22495;&#36890;&#29992;&#30701;&#35821;&#39044;&#35757;&#32451;&#12289;&#36801;&#31227;&#26631;&#35760;&#21644;&#26377;&#38480;&#30495;&#23454;&#26631;&#27880;&#25968;&#25454;&#24494;&#35843;&#26469;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#20351;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#65288;KPG&#65289;&#27169;&#22411;&#20855;&#22791;&#26356;&#24378;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#65288;KPG&#65289;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#20250;&#25104;&#20026;&#38480;&#21046;&#20854;&#36890;&#29992;&#24615;&#30340;&#38556;&#30861;&#65292;&#24182;&#19988;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#38480;&#23450;&#22312;&#29305;&#23450;&#30340;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#35777;&#26126;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#22823;&#35268;&#27169;&#20998;&#24067;&#36716;&#21464;&#20250;&#20005;&#37325;&#38459;&#30861;KPG&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#36890;&#36807;&#39640;&#25928;&#21033;&#29992;&#25968;&#25454;&#65292;&#36880;&#27493;&#24341;&#23548;KPG&#27169;&#22411;&#23398;&#20064;&#28966;&#28857;&#20174;&#36890;&#29992;&#30340;&#21477;&#27861;&#29305;&#24449;&#21040;&#19982;&#39046;&#22495;&#30456;&#20851;&#30340;&#35821;&#20041;&#12290;&#25105;&#20204;&#20351;&#29992;&#39046;&#22495;&#36890;&#29992;&#30701;&#35821;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;&#32593;&#32476;&#19978;&#24191;&#27867;&#21487;&#29992;&#30340;&#36890;&#29992;&#30701;&#35821;&#27880;&#37322;&#23545;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#21508;&#31181;&#39046;&#22495;&#29983;&#25104;&#30701;&#35821;&#12290;&#29983;&#25104;&#30340;&#27169;&#22411;&#28982;&#21518;&#22312;&#36801;&#31227;&#26631;&#35760;&#38454;&#27573;&#20013;&#24212;&#29992;&#20110;&#20135;&#29983;&#39046;&#22495;&#29305;&#23450;&#30340;&#20266;&#20851;&#38190;&#30701;&#35821;&#65292;&#36825;&#26377;&#21161;&#20110;&#23558;&#27169;&#22411;&#36866;&#24212;&#21040;&#19968;&#20010;&#26032;&#39046;&#22495;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26377;&#38480;&#30340;&#25968;&#25454;&#23545;&#30495;&#23454;&#26631;&#31614;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20805;&#20998;&#36866;&#24212;&#30446;&#26631;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Training keyphrase generation (KPG) models require a large amount of annotated data, which can be prohibitively expensive and often limited to specific domains. In this study, we first demonstrate that large distribution shifts among different domains severely hinder the transferability of KPG models. We then propose a three-stage pipeline, which gradually guides KPG models' learning focus from general syntactical features to domain-related semantics, in a data-efficient manner. With Domain-general Phrase pre-training, we pre-train Sequence-to-Sequence models with generic phrase annotations that are widely available on the web, which enables the models to generate phrases in a wide range of domains. The resulting model is then applied in the Transfer Labeling stage to produce domain-specific pseudo keyphrases, which help adapt models to a new domain. Finally, we fine-tune the model with limited data with true labels to fully adapt it to the target domain. Our experiment results show th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#24320;&#25918;&#24615;&#65292;&#24182;&#36890;&#36807;&#35789;&#27719;&#25193;&#23637;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#31867;&#20284;&#20110;CLIP&#30340;&#27169;&#22411;&#24182;&#19981;&#30495;&#27491;&#24320;&#25918;&#65292;&#24182;&#19988;&#38543;&#30528;&#35789;&#27719;&#34920;&#30340;&#25193;&#23637;&#20854;&#24615;&#33021;&#20250;&#24694;&#21270;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;CLIP&#34920;&#31034;&#22312;&#19981;&#21464;&#24615;&#21644;&#29305;&#23450;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2206.01986</link><description>&lt;p&gt;
&#25506;&#31350;CLIP&#30340;&#24320;&#25918;&#24615;
&lt;/p&gt;
&lt;p&gt;
Delving into the Openness of CLIP. (arXiv:2206.01986v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#24320;&#25918;&#24615;&#65292;&#24182;&#36890;&#36807;&#35789;&#27719;&#25193;&#23637;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#31867;&#20284;&#20110;CLIP&#30340;&#27169;&#22411;&#24182;&#19981;&#30495;&#27491;&#24320;&#25918;&#65292;&#24182;&#19988;&#38543;&#30528;&#35789;&#27719;&#34920;&#30340;&#25193;&#23637;&#20854;&#24615;&#33021;&#20250;&#24694;&#21270;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;CLIP&#34920;&#31034;&#22312;&#19981;&#21464;&#24615;&#21644;&#29305;&#23450;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#23558;&#22270;&#20687;&#20998;&#31867;&#20316;&#20026;&#19968;&#39033;&#22270;&#20687;&#21040;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#65292;&#21363;&#23558;&#22270;&#20687;&#19982;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36827;&#34892;&#21305;&#37197;&#65292;&#32780;&#19981;&#26159;&#31163;&#25955;&#30340;&#31867;&#21035;ID&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#20197;&#38646;-shot&#26041;&#24335;&#20174;&#24320;&#25918;&#31867;&#38598;&#65288;&#20063;&#31216;&#20026;&#24320;&#25918;&#35789;&#27719;&#34920;&#65289;&#20013;&#35782;&#21035;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#31867;&#20284;&#20110;CLIP&#30340;&#27169;&#22411;&#30340;&#24320;&#25918;&#24615;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#29702;&#35770;&#19978;&#23545;&#20219;&#24847;&#35789;&#27719;&#24320;&#25918;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#20854;&#31934;&#24230;&#26377;&#25152;&#21464;&#21270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36882;&#22686;&#30340;&#35270;&#35282;&#36890;&#36807;&#35789;&#27719;&#25193;&#23637;&#26469;&#35780;&#20272;&#24320;&#25918;&#24615;&#65292;&#24182;&#23450;&#20041;&#20102;&#21487;&#25193;&#23637;&#24615;&#26469;&#34913;&#37327;&#27169;&#22411;&#22788;&#29702;&#26032;&#31867;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#31867;&#20284;&#20110;CLIP&#30340;&#27169;&#22411;&#24182;&#19981;&#30495;&#27491;&#24320;&#25918;&#65292;&#24182;&#19988;&#38543;&#30528;&#35789;&#27719;&#34920;&#30340;&#25193;&#23637;&#20854;&#24615;&#33021;&#20250;&#24694;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20174;&#34920;&#31034;&#23545;&#40784;&#21644;&#32479;&#19968;&#24615;&#30340;&#35282;&#24230;&#21078;&#26512;&#20102;CLIP&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;CLIP&#34920;&#31034;&#22312;&#19981;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#20013;&#22312;&#19981;&#21464;&#24615;&#21644;&#29305;&#23450;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#25552;&#39640;&#20854;&#29305;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) formulates image classification as an image-to-text matching task, i.e., matching images to the corresponding natural language descriptions instead of discrete category IDs. This allows for open-vocabulary visual recognition, where the model can recognize images from an open class set (also known as an open vocabulary) in a zero-shot manner. However, evaluating the openness of CLIP-like models is challenging, as the models are open to arbitrary vocabulary in theory, but their accuracy varies in practice. To address this, we resort to an incremental perspective to assess the openness through vocabulary expansions, and define extensibility to measure a model's ability to handle novel classes. Our evaluation shows that CLIP-like models are not truly open, and their performance deteriorates as the vocabulary expands. We further dissect the feature space of CLIP from the perspectives of representation alignment and uniformity. Our investigation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#24110;&#21161;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#20316;&#32773;&#23450;&#20041;&#20102;&#23545;&#35805;&#31995;&#32479;&#30340;&#24110;&#21161;&#24615;&#65292;&#20351;&#29992;&#20998;&#31867;&#22120;&#33258;&#21160;&#30830;&#23450;&#24110;&#21161;&#24615;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24110;&#21161;&#32423;&#21035;&#26469;&#34913;&#37327;&#23545;&#35805;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#31995;&#32479;&#26356;&#23481;&#26131;&#20026;&#26469;&#33258;&#21457;&#36798;&#22269;&#23478;&#27010;&#24565;&#30340;&#38382;&#39064;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2205.12554</link><description>&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#24110;&#21161;&#24615;&#21644;&#20844;&#24179;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Helpfulness and Fairness of Task-Oriented Dialogue Systems. (arXiv:2205.12554v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#24110;&#21161;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#20316;&#32773;&#23450;&#20041;&#20102;&#23545;&#35805;&#31995;&#32479;&#30340;&#24110;&#21161;&#24615;&#65292;&#20351;&#29992;&#20998;&#31867;&#22120;&#33258;&#21160;&#30830;&#23450;&#24110;&#21161;&#24615;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24110;&#21161;&#32423;&#21035;&#26469;&#34913;&#37327;&#23545;&#35805;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#31995;&#32479;&#26356;&#23481;&#26131;&#20026;&#26469;&#33258;&#21457;&#36798;&#22269;&#23478;&#27010;&#24565;&#30340;&#38382;&#39064;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#23454;&#29616;&#26576;&#20123;&#30446;&#26631;&#65292;&#22240;&#27492;&#20154;&#20204;&#23545;&#20854;&#24110;&#21161;&#24615;&#30340;&#24863;&#30693;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26410;&#23545;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#20154;&#31867;&#24863;&#30693;&#24110;&#21161;&#24615;&#20197;&#21450;&#20854;&#20844;&#24179;&#24615;&#24433;&#21709;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24110;&#21161;&#24615;&#30340;&#35745;&#31639;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#27880;&#37322;&#26500;&#24314;&#20998;&#31867;&#22120;&#65292;&#33258;&#21160;&#30830;&#23450;&#21709;&#24212;&#30340;&#24110;&#21161;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20351;&#29992;&#23545;&#19981;&#21516;&#29992;&#25143;&#26597;&#35810;&#30340;&#24110;&#21161;&#32423;&#21035;&#26469;&#34913;&#37327;&#23545;&#35805;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#19977;&#31181;&#20449;&#24687;&#26597;&#35810;&#22330;&#26223;&#19979;&#26356;&#23481;&#26131;&#20026;&#26469;&#33258;&#21457;&#36798;&#22269;&#23478;&#27010;&#24565;&#30340;&#38382;&#39064;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-oriented dialogue systems aim to help users achieve certain goals. Therefore, how humans perceive their helpfulness is important. However, neither the human-perceived helpfulness of goal-oriented dialogue systems nor its fairness implication has been well studied. In this paper, we study computational measurements of helpfulness. We first formally define a dialogue response as helpful if it is relevant &amp; coherent, useful, and informative to a query. Then, we collect human annotations for the helpfulness of dialogue responses based on our definition and build a classifier to automatically determine the helpfulness of a response. We further propose to use the helpfulness level of a dialogue system towards different user queries to measure the fairness of a dialogue system. Experiments with state-of-the-art dialogue systems under three information-seeking scenarios reveal that existing systems tend to be more helpful for questions regarding concepts from highly-developed countries th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2201.09227</link><description>&lt;p&gt;
&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#20803;&#21270;&#30340;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
A Large and Diverse Arabic Corpus for Language Modeling. (arXiv:2201.09227v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09227
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24341;&#20837;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24314;&#27169;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#21464;&#65292;&#20854;&#20013;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;LM&#24050;&#32463;&#25104;&#20026;&#22823;&#22810;&#25968;NLP&#20219;&#21153;&#19981;&#21487;&#20998;&#21106;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;LM&#36275;&#22815;&#26234;&#33021;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#35821;&#35328;&#30340;&#26377;&#29992;&#21644;&#30456;&#20851;&#34920;&#31034;&#12290;&#36825;&#20123;&#27169;&#22411;&#34987;&#29992;&#20110;&#23545;&#24120;&#35268;NLP&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20855;&#26377;&#26174;&#30528;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#30456;&#21453;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#38656;&#35201;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35821;&#26009;&#24211;&#65292;&#36825;&#20010;&#35821;&#26009;&#24211;&#21487;&#20197;&#24456;&#22909;&#22320;&#20195;&#34920;&#38463;&#25289;&#20271;&#35821;&#12290;&#30001;&#20110;&#33521;&#35821;&#35821;&#26009;&#24211;&#21487;&#33719;&#24471;&#22823;&#37327;&#36164;&#28304;&#65292;&#22240;&#27492;&#33521;&#35821;LM&#36890;&#24120;&#27604;&#20854;&#20182;&#35821;&#35328;LM&#34920;&#29616;&#26356;&#22909;&#12290;&#26412;&#25991;&#35814;&#32454;&#25551;&#36848;&#20102;&#19968;&#20010;&#22823;&#22411;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#12290;&#23427;&#30001;&#36229;&#36807;500GB&#30340;&#24050;&#21152;&#24037;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;&#32452;&#25104;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#21644;&#19979;&#28216;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35813;&#35821;&#26009;&#24211;&#36824;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#38463;&#25289;&#20271;&#35821;LM&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have introduced a major paradigm shift in Natural Language Processing (NLP) modeling where large pre-trained LMs became integral to most of the NLP tasks. The LMs are intelligent enough to find useful and relevant representations of the language without any supervision. Perhaps, these models are used to fine-tune typical NLP tasks with significantly high accuracy as compared to the traditional approaches. Conversely, the training of these models requires a massively large corpus that is a good representation of the language. English LMs generally perform better than their other language counterparts, due to the availability of massive English corpora. This work elaborates on the design and development of a large Arabic corpus. It consists of over 500 GB of Arabic cleaned text targeted at improving cross-domain knowledge and downstream generalization capability of large-scale language models. Moreover, the corpus is utilized in the training of a large Arabic LM. In
&lt;/p&gt;</description></item></channel></rss>