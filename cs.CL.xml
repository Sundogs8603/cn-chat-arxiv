<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>MAGDi&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#25512;&#29702;&#20132;&#20114;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#26469;&#25913;&#21892;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01620</link><description>&lt;p&gt;
MAGDi&#65306;&#32467;&#26500;&#21270;&#33976;&#39311;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#22270;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01620
&lt;/p&gt;
&lt;p&gt;
MAGDi&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#25512;&#29702;&#20132;&#20114;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#26469;&#25913;&#21892;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#37325;&#22823;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#28041;&#21450;&#22810;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#38271;&#26102;&#38388;&#29983;&#25104;&#65292;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#22810;&#26234;&#33021;&#20307;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#19968;&#20010;&#26368;&#32456;&#30340;&#12289;&#21333;&#19968;&#30340;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MAGDi&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#25512;&#29702;&#20132;&#20114;&#32467;&#26500;&#21270;&#33976;&#39311;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#12290;MAGDi&#36890;&#36807;&#23558;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#20351;&#29992;&#22270;&#24418;&#32534;&#30721;&#22120;&#22686;&#24378;&#22522;&#30784;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#30446;&#26631;&#20989;&#25968;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65306;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#12289;&#27491;&#30830;&#21644;&#38169;&#35823;&#25512;&#29702;&#20043;&#38388;&#30340;&#23545;&#27604;&#25439;&#22833;&#20197;&#21450;&#22522;&#20110;&#22270;&#24418;&#30340;&#30446;&#26631;&#26469;&#24314;&#27169;&#20132;&#20114;&#32467;&#26500;&#12290;&#22312;&#19971;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#24120;&#35782;&#21644;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MAGDi&#25913;&#21892;&#20102;&#36739;&#23567;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20248;&#20110;&#20960;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks. However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference. To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs. MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure. Experiments on seven widely-used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods th
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;&#65292;&#33021;&#22815;&#21387;&#32553;&#20449;&#24687;&#24182;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#37325;&#22823;&#20248;&#21183;</title><link>https://arxiv.org/abs/2403.15729</link><description>&lt;p&gt;
&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards a \textbf{RAG}-based Summarization Agent for the Electron-Ion Collider
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15729
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;&#65292;&#33021;&#22815;&#21387;&#32553;&#20449;&#24687;&#24182;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#37325;&#22823;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#24615;&#21644;&#24222;&#22823;&#30340;&#20449;&#24687;&#37327;&#28085;&#30422;&#20102;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#25991;&#20214;&#12289;&#35770;&#25991;&#12289;&#25968;&#25454;&#21644;&#20854;&#20182;&#36164;&#28304;&#65292;&#23548;&#33268;&#23548;&#33322;&#36825;&#20123;&#22810;&#26679;&#24418;&#24335;&#20449;&#24687;&#30340;&#20219;&#21153;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#23545;&#20110;&#26032;&#21512;&#20316;&#32773;&#21644;&#26089;&#26399;&#31185;&#23398;&#23478;&#26469;&#35828;&#23588;&#20026;&#33392;&#24040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;EIC&#25688;&#35201;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65288;RAGS4EIC&#65289;&#12290;&#35813;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#19981;&#20165;&#21387;&#32553;&#20449;&#24687;&#65292;&#36824;&#26377;&#25928;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#20102;&#37325;&#22823;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#37319;&#21462;&#20102;&#20004;&#27493;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#26597;&#35810;&#21253;&#21547;&#25152;&#26377;&#30456;&#20851;&#23454;&#39564;&#20449;&#24687;&#30340;&#32508;&#21512;&#21521;&#37327;&#25968;&#25454;&#24211;&#65307;&#20854;&#27425;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#21644;&#26816;&#32034;&#25968;&#25454;&#29983;&#25104;&#21253;&#21547;&#24341;&#29992;&#30340;&#31616;&#27905;&#25688;&#35201;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20351;&#29992;RAG&#35780;&#20272;&#30340;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15729v1 Announce Type: cross  Abstract: The complexity and sheer volume of information encompassing documents, papers, data, and other resources from large-scale experiments demand significant time and effort to navigate, making the task of accessing and utilizing these varied forms of information daunting, particularly for new collaborators and early-career scientists. To tackle this issue, a Retrieval Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under development. This AI-Agent not only condenses information but also effectively references relevant responses, offering substantial advantages for collaborators. Our project involves a two-step approach: first, querying a comprehensive vector database containing all pertinent experiment information; second, utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data. We describe the evaluation methods that use RAG assessments 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#27700;&#21360;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#26816;&#27979;&#36807;&#31243;&#20013;&#26681;&#25454;&#20196;&#29260;&#30340;&#29109;&#35843;&#25972;&#20854;&#26435;&#37325;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#27700;&#21360;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13485</link><description>&lt;p&gt;
&#22522;&#20110;&#29109;&#30340;&#25991;&#26412;&#27700;&#21360;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Entropy-based Text Watermarking Detection Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13485
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#27700;&#21360;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#26816;&#27979;&#36807;&#31243;&#20013;&#26681;&#25454;&#20196;&#29260;&#30340;&#29109;&#35843;&#25972;&#20854;&#26435;&#37325;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#27700;&#21360;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#33021;&#22815;&#23884;&#20837;&#38544;&#34255;&#29305;&#24449;&#21040;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#20197;&#20415;&#21518;&#32493;&#26816;&#27979;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;LLMs&#34987;&#35823;&#29992;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#39640;&#29109;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20302;&#29109;&#24773;&#20917;&#19979;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#27700;&#21360;&#26816;&#27979;&#36807;&#31243;&#20013;&#24212;&#20840;&#38754;&#32771;&#34385;&#20196;&#29260;&#29109;&#30340;&#24433;&#21709;&#65292;&#21363;&#24212;&#26681;&#25454;&#20854;&#29109;&#35843;&#25972;&#27599;&#20010;&#20196;&#29260;&#30340;&#37325;&#37327;&#65292;&#32780;&#19981;&#26159;&#20687;&#20197;&#21069;&#30340;&#26041;&#27861;&#20013;&#23558;&#25152;&#26377;&#20196;&#29260;&#30340;&#37325;&#37327;&#35774;&#32622;&#20026;&#30456;&#21516;&#20540;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#27700;&#21360;&#26816;&#27979;&#65288;EWD&#65289;&#65292;&#22312;&#27700;&#21360;&#26816;&#27979;&#36807;&#31243;&#20013;&#36171;&#20104;&#39640;&#29109;&#20196;&#29260;&#26356;&#39640;&#30340;&#26435;&#37325;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#27700;&#21360;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26816;&#27979;&#36807;&#31243;&#26080;&#38656;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13485v1 Announce Type: new  Abstract: Currently, text watermarking algorithms for large language models (LLMs) can embed hidden features to texts generated by LLMs to facilitate subsequent detection, thus alleviating the problem of misuse of LLMs. Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved. In this work, we proposed that the influence of token entropy should be fully considered in the watermark detection process, that is, the weight of each token should be adjusted according to its entropy during watermark detection, rather than setting the weight of all tokens to the same value as in previous methods. Specifically, we proposed an Entropy-based Watermark Detection (EWD) that gives higher-entropy tokens higher weights during watermark detection, so as to better reflect the degree of watermarking. Furthermore, the proposed detection process is training-free a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.13213</link><description>&lt;p&gt;
&#20174;&#34920;&#29616;&#24615;&#20260;&#23475;&#21040;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;:&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#23548;&#33268;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#27493;&#20063;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20854;&#23545;&#24050;&#32463;&#36793;&#32536;&#21270;&#20154;&#32676;&#30340;&#19981;&#21033;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20943;&#36731;&#25514;&#26045;&#26469;&#24320;&#21457;&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#65292;&#27604;&#22914;&#30417;&#30563;&#24335;&#30340;&#23433;&#20840;&#23450;&#21521;&#24494;&#35843;&#21644;&#21033;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#20294;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#20869;&#22312;&#20559;&#35265;&#20173;&#23384;&#22312;&#22810;&#37325;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#20026;&#20102;&#23433;&#20840;&#32780;&#20248;&#21270;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#23637;&#31034;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#65292;&#27604;&#22914;&#20986;&#20110;&#39044;&#38450;&#25514;&#26045;&#32780;&#20542;&#21521;&#20110;&#19981;&#22238;&#24212;&#26576;&#20123;&#35831;&#27714;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#20013;&#24050;&#32463;&#35760;&#24405;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26126;&#26174;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23433;&#20840;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13213v1 Announce Type: cross  Abstract: Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluatin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CantonMT&#39033;&#30446;&#65292;&#21033;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#31908;&#35821;&#33267;&#33521;&#35821;NMT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#20197;&#20419;&#36827;&#30740;&#31350;</title><link>https://arxiv.org/abs/2403.11346</link><description>&lt;p&gt;
CantonMT: &#27721;&#33521;NMT&#24179;&#21488;&#65292;&#20351;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11346
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CantonMT&#39033;&#30446;&#65292;&#21033;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#31908;&#35821;&#33267;&#33521;&#35821;NMT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#20197;&#20419;&#36827;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11346v1 &#28040;&#24687;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#20173;&#28982;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#26631;&#20934;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#21453;&#21521;&#32763;&#35793;&#65292;&#24212;&#29992;&#21040;&#20102;&#26032;&#30340;&#35821;&#35328;&#32763;&#35793;&#26041;&#21521;&#31908;&#35821;&#33267;&#33521;&#35821;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;(&#21253;&#25324;OpusMT, NLLB,&#21644;mBART)&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#25351;&#26631;&#21253;&#25324;&#22522;&#20110;&#35789;&#27719;&#21644;&#23884;&#20837;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#36825;&#39033;\textsc{CantonMT}&#30740;&#31350;&#39033;&#30446;&#20013;&#21253;&#21547;&#30340;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#24182;&#25552;&#20379;&#20415;&#21033;&#23454;&#29616;&#31908;&#35821;&#33267;&#33521;&#35821;MT&#30740;&#31350;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#25105;&#20204;&#30340;&#24320;&#28304;\textsc{CantonMT}&#24037;&#20855;&#21253;\url{https://github.com/kenrickkung/CantoneseTranslation}&#21521;&#24179;&#21488;&#28155;&#21152;&#26356;&#22810;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11346v1 Announce Type: cross  Abstract: Neural Machine Translation (NMT) for low-resource languages is still a challenging task in front of NLP researchers. In this work, we deploy a standard data augmentation methodology by back-translation to a new language translation direction Cantonese-to-English. We present the models we fine-tuned using the limited amount of real data and the synthetic data we generated using back-translation including OpusMT, NLLB, and mBART. We carried out automatic evaluation using a range of different metrics including lexical-based and embedding-based. Furthermore. we create a user-friendly interface for the models we included in this\textsc{ CantonMT} research project and make it available to facilitate Cantonese-to-English MT research. Researchers can add more models into this platform via our open-source\textsc{ CantonMT} toolkit \url{https://github.com/kenrickkung/CantoneseTranslation}.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#30465;&#29305;&#23450;&#39046;&#22495;&#65292;&#29983;&#25104;&#39046;&#22495;&#30456;&#20851;&#23646;&#24615;&#24182;&#21019;&#24314;&#23646;&#24615;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#32469;&#36807;&#22797;&#26434;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#30340;&#21019;&#26032;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.11103</link><description>&lt;p&gt;
ProgGen:&#36890;&#36807;&#33258;&#21453;&#22823;&#35821;&#35328;&#27169;&#22411;&#36880;&#27493;&#29983;&#25104;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11103
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#30465;&#29305;&#23450;&#39046;&#22495;&#65292;&#29983;&#25104;&#39046;&#22495;&#30456;&#20851;&#23646;&#24615;&#24182;&#21019;&#24314;&#23646;&#24615;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#32469;&#36807;&#22797;&#26434;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#30340;&#21019;&#26032;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36328;&#39046;&#22495;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#32467;&#26500;&#21270;&#30693;&#35782;&#25552;&#21462;&#20219;&#21153;&#65288;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;NER&#65289;&#26041;&#38754;&#32463;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#20855;&#26377;&#36866;&#24230;NER&#33021;&#21147;&#30340;LLMs&#26469;&#29983;&#25104;&#20248;&#31168;&#30340;NER&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#22522;&#26412;&#30340;&#31867;&#26465;&#20214;&#25552;&#31034;&#65292;&#32780;&#26159;&#25351;&#23548;LLMs&#23545;&#29305;&#23450;&#39046;&#22495;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#65292;&#20174;&#32780;&#29983;&#25104;&#20855;&#26377;&#39046;&#22495;&#30456;&#20851;&#23646;&#24615;&#65288;&#20363;&#22914;&#24433;&#35780;&#30340;&#31867;&#21035;&#21644;&#24773;&#24863;&#65289;&#30340;&#23646;&#24615;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39044;&#20808;&#29983;&#25104;&#23454;&#20307;&#26415;&#35821;&#65292;&#28982;&#21518;&#22260;&#32469;&#36825;&#20123;&#23454;&#20307;&#24320;&#21457;NER&#19978;&#19979;&#25991;&#25968;&#25454;&#65292;&#26377;&#25928;&#35268;&#36991;&#20102;LLMs&#23545;&#22797;&#26434;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#36890;&#29992;&#21644;&#19987;&#19994;&#39046;&#22495;&#23637;&#24320;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11103v1 Announce Type: new  Abstract: Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER). This paper explores an innovative, cost-efficient strategy to harness LLMs with modest NER capabilities for producing superior NER datasets. Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data. Furthermore, we preemptively generate entity terms and then develop NER context data around these entities, effectively bypassing the LLMs' challenges with complex structures. Our experiments across both general and niche domains reveal significant performance enhancements over conventional data generation methods while being mor
&lt;/p&gt;</description></item><item><title>BAGEL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#25805;&#20316;&#23558;&#38543;&#26426;&#25506;&#32034;&#21040;&#30340;&#36712;&#36857;&#25110;&#21512;&#25104;&#25351;&#20196;&#36716;&#25442;&#20026;&#28436;&#31034;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#22312;&#26032;&#29615;&#22659;&#20013;&#33258;&#20027;&#23398;&#20064;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.08140</link><description>&lt;p&gt;
BAGEL: &#36890;&#36807;&#35821;&#35328;&#24341;&#23548;&#25506;&#32034;&#24341;&#23548;&#20195;&#29702;&#33258;&#20027;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BAGEL: Bootstrapping Agents by Guiding Exploration with Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08140
&lt;/p&gt;
&lt;p&gt;
BAGEL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#25805;&#20316;&#23558;&#38543;&#26426;&#25506;&#32034;&#21040;&#30340;&#36712;&#36857;&#25110;&#21512;&#25104;&#25351;&#20196;&#36716;&#25442;&#20026;&#28436;&#31034;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#22312;&#26032;&#29615;&#22659;&#20013;&#33258;&#20027;&#23398;&#20064;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#29615;&#22659;&#65288;&#20363;&#22914;Web&#27983;&#35272;&#22120;&#21644;REST API&#65289;&#20013;&#36890;&#36807;&#25191;&#34892;&#21160;&#20316;&#26469;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#32463;&#24120;&#22312;&#27809;&#26377;&#20154;&#31867;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BAGEL&#65292;&#19968;&#31181;&#26080;&#38656;&#20154;&#31867;&#30417;&#30563;&#21363;&#21487;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;BAGEL&#23558;&#19968;&#32452;&#38543;&#26426;&#25506;&#32034;&#30340;&#36712;&#36857;&#25110;&#21512;&#25104;&#25351;&#20196;&#36716;&#25442;&#25104;&#28436;&#31034;&#65292;&#36890;&#36807;&#20004;&#20010;&#22122;&#22768;&#35821;&#35328;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#24448;&#36820;&#26469;&#23436;&#25104;&#65306;&#19968;&#20010;&#23558;&#36712;&#36857;&#36716;&#25442;&#20026;&#21512;&#25104;&#25351;&#20196;&#30340;&#35821;&#35328;&#27169;&#22411;&#26631;&#35760;&#22120;&#65292;&#20197;&#21450;&#19968;&#20010;&#23558;&#21512;&#25104;&#25351;&#20196;&#26144;&#23556;&#20026;&#32463;&#36807;&#25913;&#36827;&#30340;&#36712;&#36857;&#30340;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#12290;&#36890;&#36807;&#36845;&#20195;&#25191;&#34892;&#36825;&#20123;&#24448;&#36820;&#25805;&#20316;&#65292;BAGEL&#21487;&#20197;&#24555;&#36895;&#23558;&#26368;&#21021;&#30340;&#36712;&#36857;&#20998;&#24067;&#36716;&#21464;&#20026;&#37027;&#20123;&#34987;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#23436;&#21892;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#20351;&#29992;BAGEL&#28436;&#31034;&#26469;&#22312;&#27979;&#35797;&#26102;&#38388;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#19978;&#35843;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08140v1 Announce Type: new  Abstract: Following natural language instructions by executing actions in digital environments (e.g. web-browsers and REST APIs) is a challenging task for language model (LM) agents. Unfortunately, LM agents often fail to generalize to new environments without human demonstrations. This work presents BAGEL, a method for bootstrapping LM agents without human supervision. BAGEL converts a seed set of randomly explored trajectories or synthetic instructions, into demonstrations, via round-trips between two noisy LM components: an LM labeler which converts a trajectory into a synthetic instruction, and a zero-shot LM agent which maps the synthetic instruction into a refined trajectory. By performing these round-trips iteratively, BAGEL quickly converts the initial distribution of trajectories towards those that are well-described by natural language. We use BAGEL demonstrations to adapt a zero shot LM agent at test time via in-context learning over re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.07865</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Safety Generalization Challenges of Large Language Models via Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CodeAttack&#65292;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#20195;&#30721;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#20026;&#27979;&#35797;LLMs&#30340;&#23433;&#20840;&#27867;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#22312;&#20869;&#30340;&#26368;&#26032;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20195;&#30721;&#36755;&#20837;&#23384;&#22312;&#20849;&#21516;&#30340;&#23433;&#20840;&#28431;&#27934;&#65306;CodeAttack&#22312;&#36229;&#36807;80%&#30340;&#26102;&#38388;&#20869;&#22987;&#32456;&#32469;&#36807;&#25152;&#26377;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25512;&#29702;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#21644;&#19982;&#21307;&#29983;&#20915;&#31574;&#36335;&#24452;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.06609</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#31181;&#23376;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06609
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25512;&#29702;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#21644;&#19982;&#21307;&#29983;&#20915;&#31574;&#36335;&#24452;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#25512;&#29702;&#26159;&#21307;&#29983;&#22312;&#35780;&#20272;&#21644;&#31649;&#29702;&#24739;&#32773;&#26102;&#37319;&#29992;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#28041;&#21450;&#24314;&#35758;&#24517;&#35201;&#30340;&#26816;&#26597;&#65292;&#35786;&#26029;&#24739;&#32773;&#30142;&#30149;&#65292;&#24182;&#20915;&#23450;&#36866;&#24403;&#30340;&#27835;&#30103;&#31561;&#12290;&#20934;&#30830;&#30340;&#20020;&#24202;&#25512;&#29702;&#38656;&#35201;&#24191;&#27867;&#30340;&#21307;&#23398;&#30693;&#35782;&#21644;&#20016;&#23500;&#30340;&#20020;&#24202;&#32463;&#39564;&#65292;&#20026;&#21307;&#29983;&#35774;&#32622;&#20102;&#24456;&#39640;&#30340;&#38376;&#27099;&#12290;&#26368;&#36817;&#65292;&#20687;ChatGPT&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#31034;&#20986;&#22312;&#20020;&#24202;&#25512;&#29702;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#38382;&#39064;&#65292;&#32780;LLMs&#30340;&#25512;&#29702;&#36807;&#31243;&#21487;&#33021;&#19982;&#21307;&#29983;&#30340;&#20020;&#24202;&#20915;&#31574;&#36335;&#24452;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06609v1 Announce Type: cross  Abstract: Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patients' diseases, and deciding on appropriate therapies, etc. Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches. Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning. However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians. In this study, we introduce a 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#36827;&#34892;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.06448</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#30340;&#26080;&#30417;&#30563;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#36827;&#34892;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#26159;&#25351;&#20135;&#29983;&#36830;&#36143;&#20294;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#20013;&#24187;&#35273;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MIND&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#20869;&#37096;&#29366;&#24577;&#36827;&#34892;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;HELM&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#20010;LLMs&#24187;&#35273;&#26816;&#27979;&#30340;&#26032;&#22522;&#20934;&#65292;&#22312;LLMs&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;LLM&#36755;&#20986;&#21644;&#20869;&#37096;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06448v1 Announce Type: cross  Abstract: Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM's inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments d
&lt;/p&gt;</description></item><item><title>Pearl&#25968;&#25454;&#38598;&#21033;&#29992;&#20102;&#35282;&#33394;&#21644;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20855;&#20307;&#29992;&#25143;&#20559;&#22909;&#65292;&#39046;&#22495;&#19987;&#19994;&#24615;&#21644;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.04460</link><description>&lt;p&gt;
Pearl: &#19968;&#39033;&#22522;&#20110;&#35780;&#35770;&#39537;&#21160;&#30340;&#35282;&#33394;&#30693;&#35782;&#23545;&#35805;&#24335;&#25512;&#33616;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04460
&lt;/p&gt;
&lt;p&gt;
Pearl&#25968;&#25454;&#38598;&#21033;&#29992;&#20102;&#35282;&#33394;&#21644;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20855;&#20307;&#29992;&#25143;&#20559;&#22909;&#65292;&#39046;&#22495;&#19987;&#19994;&#24615;&#21644;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04460v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#65292;&#20351;&#24471;&#23545;&#35805;&#36755;&#20837;&#30340;&#22810;&#26679;&#21270;&#25512;&#29702;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35813;&#39046;&#22495;&#36824;&#26377;&#35768;&#22810;&#26041;&#38754;&#26377;&#24453;&#25506;&#32034;&#12290;&#30446;&#21069;&#21487;&#29992;&#30340;&#29992;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#32570;&#20047;&#29305;&#23450;&#29992;&#25143;&#20559;&#22909;&#21644;&#23545;&#25512;&#33616;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#22952;&#30861;&#20102;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;PEARL&#65292;&#19982;&#35282;&#33394;&#21644;&#30693;&#35782;&#22686;&#24378;&#30340;LLM&#27169;&#25311;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#20174;&#30495;&#23454;&#35780;&#35770;&#20013;&#33719;&#24471;&#35814;&#32454;&#30340;&#35282;&#33394;&#21644;&#30693;&#35782;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#36229;&#36807;57k&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PEARL&#20013;&#30340;&#35805;&#35821;&#21253;&#25324;&#26356;&#20855;&#20307;&#30340;&#29992;&#25143;&#20559;&#22909;&#65292;&#26174;&#31034;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04460v1 Announce Type: new  Abstract: Conversational recommender system is an emerging area that has garnered an increasing interest in the community, especially with the advancements in large language models (LLMs) that enable diverse reasoning over conversational input. Despite the progress, the field has many aspects left to explore. The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations. To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators. We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues. Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide recommendations more relev
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24605;&#32500;&#38142;&#33976;&#39311;&#20013;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#19982;&#30693;&#35782;&#38598;&#25104;&#19981;&#36275;&#38382;&#39064;&#30340;&#21464;&#20998;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03348</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#36827;&#34892;&#24605;&#32500;&#38142;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Learning to Maximize Mutual Information for Chain-of-Thought Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03348
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24605;&#32500;&#38142;&#33976;&#39311;&#20013;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#19982;&#30693;&#35782;&#38598;&#25104;&#19981;&#36275;&#38382;&#39064;&#30340;&#21464;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#23558;&#22823;&#22411;&#22797;&#26434;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#36739;&#23567;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#26159;&#23454;&#29616;&#39640;&#25928;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#36890;&#36807;&#21033;&#29992;&#24605;&#32500;&#38142; (CoT) &#33976;&#39311;&#30340;&#26032;&#26041;&#27861;&#8212;&#8212;&#36880;&#27493;&#33976;&#39311; (DSS)&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#20026;&#36739;&#23567;&#27169;&#22411;&#36171;&#20104;&#20854;&#36739;&#22823;&#21516;&#34892;&#30340;&#20248;&#36234;&#25512;&#29702;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#22312;DSS&#20013;&#65292;&#33976;&#39311;&#27169;&#22411;&#36890;&#36807;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#21516;&#26102;&#33719;&#24471;&#29983;&#25104;&#29702;&#30001;&#21644;&#39044;&#27979;&#26631;&#31614;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;DSS&#24573;&#30053;&#20102;&#36825;&#20004;&#20010;&#35757;&#32451;&#20219;&#21153;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#65292;&#23548;&#33268;CoT&#30693;&#35782;&#19982;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#30340;&#26377;&#25928;&#25972;&#21512;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#29942;&#39048;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#34920;&#36848;&#20026;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03348v1 Announce Type: cross  Abstract: Knowledge distillation, the technique of transferring knowledge from large, complex models to smaller ones, marks a pivotal step towards efficient AI deployment. Distilling Step-by-Step (DSS), a novel method utilizing chain-of-thought (CoT) distillation, has demonstrated promise by imbuing smaller models with the superior reasoning capabilities of their larger counterparts. In DSS, the distilled model acquires the ability to generate rationales and predict labels concurrently through a multi-task learning framework. However, DSS overlooks the intrinsic relationship between the two training tasks, leading to ineffective integration of CoT knowledge with the task of label prediction. To this end, we investigate the mutual relationship of the two tasks from Information Bottleneck perspective and formulate it as maximizing the mutual information of the representation features of the two tasks. We propose a variational approach to solve thi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#27599;&#20010;&#23618;&#30340;&#26411;&#31471;&#25506;&#27979;&#20854;&#38544;&#34255;&#29366;&#24577;&#65292;&#20351;&#29992;&#19968;&#20010;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#35782;&#21035;&#20219;&#21153;&#65292;&#26412;&#25991;&#20855;&#20307;&#30740;&#31350;&#20102;Llama2&#30340;&#33258;&#19979;&#32780;&#19978;&#35789;&#27719;&#35821;&#20041;&#28436;&#21464;&#65292;&#21457;&#29616;&#36739;&#20302;&#23618;&#30340;&#34920;&#31034;&#32534;&#30721;&#20102;&#35789;&#27719;&#35821;&#20041;&#65292;&#32780;&#36739;&#39640;&#23618;&#36127;&#36131;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.01509</link><description>&lt;p&gt;
&#22855;&#24187;&#35821;&#20041;&#19982;&#23547;&#25214;&#20043;&#22320;&#65306;&#25506;&#35752;&#29983;&#25104;&#22411;LLM&#30340;&#21738;&#20123;&#23618;&#27425;&#21453;&#26144;&#35789;&#27719;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01509
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#27599;&#20010;&#23618;&#30340;&#26411;&#31471;&#25506;&#27979;&#20854;&#38544;&#34255;&#29366;&#24577;&#65292;&#20351;&#29992;&#19968;&#20010;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#35782;&#21035;&#20219;&#21153;&#65292;&#26412;&#25991;&#20855;&#20307;&#30740;&#31350;&#20102;Llama2&#30340;&#33258;&#19979;&#32780;&#19978;&#35789;&#27719;&#35821;&#20041;&#28436;&#21464;&#65292;&#21457;&#29616;&#36739;&#20302;&#23618;&#30340;&#34920;&#31034;&#32534;&#30721;&#20102;&#35789;&#27719;&#35821;&#20041;&#65292;&#32780;&#36739;&#39640;&#23618;&#36127;&#36131;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#36861;&#27714;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#27861;&#23478;&#26063;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35821;&#20041;&#28436;&#21464;&#38543;&#30528;&#28145;&#24230;&#24182;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#19981;&#20687;&#23427;&#20204;&#30340;&#21069;&#36744;&#65292;&#27604;&#22914;BERT&#31867;&#26550;&#26500;&#12290;&#26412;&#25991;&#20855;&#20307;&#30740;&#31350;&#20102;&#19968;&#27454;&#27969;&#34892;LLM&#65292;&#21363;Llama2&#30340;&#33258;&#19979;&#32780;&#19978;&#35789;&#27719;&#35821;&#20041;&#28436;&#21464;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#23618;&#30340;&#26411;&#31471;&#25506;&#27979;&#20854;&#38544;&#34255;&#29366;&#24577;&#65292;&#20351;&#29992;&#19968;&#20010;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#35782;&#21035;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36739;&#20302;&#23618;&#30340;&#34920;&#31034;&#32534;&#30721;&#20102;&#35789;&#27719;&#35821;&#20041;&#65292;&#32780;&#36739;&#39640;&#23618;&#65292;&#20854;&#35821;&#20041;&#24402;&#32435;&#36739;&#24369;&#65292;&#36127;&#36131;&#39044;&#27979;&#12290;&#36825;&#19982;&#20855;&#26377;&#21028;&#21035;&#30446;&#26631;&#30340;&#27169;&#22411;&#24418;&#25104;&#23545;&#27604;&#65292;&#27604;&#22914;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65292;&#22312;&#37027;&#37324;&#36739;&#39640;&#23618;&#33719;&#24471;&#26356;&#22909;&#30340;&#35789;&#27719;&#35821;&#20041;&#12290;&#32467;&#35770;&#36827;&#19968;&#27493;&#24471;&#21040;&#24615;&#33021;&#21333;&#35843;&#22686;&#21152;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01509v1 Announce Type: new  Abstract: Large language models have achieved remarkable success in general language understanding tasks. However, as a family of generative methods with the objective of next token prediction, the semantic evolution with the depth of these models are not fully explored, unlike their predecessors, such as BERT-like architectures. In this paper, we specifically investigate the bottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by probing its hidden states at the end of each layer using a contextualized word identification task. Our experiments show that the representations in lower layers encode lexical semantics, while the higher layers, with weaker semantic induction, are responsible for prediction. This is in contrast to models with discriminative objectives, such as mask language modeling, where the higher layers obtain better lexical semantics. The conclusion is further supported by the monotonic increase in performance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WARDEN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#25991;&#26412;&#20013;&#21152;&#20837;&#22810;&#20010;&#21487;&#33021;&#30340;&#27700;&#21360;&#26041;&#21521;&#65292;&#22686;&#21152;&#20102;&#27700;&#21360;&#28040;&#38500;&#30340;&#38590;&#24230;&#65292;&#20197;&#24212;&#23545;EaaS&#20013;&#32972;&#38376;&#27700;&#21360;&#34987;&#31227;&#38500;&#30340;&#26032;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2403.01472</link><description>&lt;p&gt;
WARDEN&#65306;&#22810;&#26041;&#21521;&#32972;&#38376;&#27700;&#21360;&#29992;&#20110;Embedding-as-a-Service&#29256;&#26435;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WARDEN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#25991;&#26412;&#20013;&#21152;&#20837;&#22810;&#20010;&#21487;&#33021;&#30340;&#27700;&#21360;&#26041;&#21521;&#65292;&#22686;&#21152;&#20102;&#27700;&#21360;&#28040;&#38500;&#30340;&#38590;&#24230;&#65292;&#20197;&#24212;&#23545;EaaS&#20013;&#32972;&#38376;&#27700;&#21360;&#34987;&#31227;&#38500;&#30340;&#26032;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Embedding as a Service&#65288;EaaS&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;EaaS&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#25277;&#21462;&#25915;&#20987;&#30340;&#23041;&#32961;&#65307;&#28982;&#32780;&#65292;&#36890;&#36807;&#21521;&#25991;&#26412;&#23884;&#20837;&#28155;&#21152;&#32972;&#38376;&#27700;&#21360;&#65292;&#24182;&#38543;&#21518;&#39564;&#35777;&#25915;&#20987;&#27169;&#22411;&#30340;&#21457;&#24067;&#21518;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#26368;&#36817;&#29992;&#20110;EaaS&#30340;&#27700;&#21360;&#31574;&#30053;EmbMarker&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CSE&#65288;Cluster&#12289;Selection&#12289;Elimination&#65289;&#25915;&#20987;&#65292;&#23427;&#33021;&#22815;&#31227;&#38500;&#32972;&#38376;&#27700;&#21360;&#21516;&#26102;&#20445;&#25345;&#23884;&#20837;&#30340;&#39640;&#25928;&#24615;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#27700;&#21360;&#26041;&#27861;&#26159;&#21487;&#20197;&#34987;&#31361;&#30772;&#30340;&#12290;&#38024;&#23545;&#36825;&#19968;&#26032;&#23041;&#32961;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#35758;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#21487;&#33021;&#30340;&#27700;&#21360;&#26041;&#21521;&#20351;&#27700;&#21360;&#30340;&#31227;&#38500;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#26041;&#27861;WARDEN&#26174;&#33879;&#22686;&#21152;&#20102;&#27700;&#21360;&#28040;&#38500;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01472v1 Announce Type: cross  Abstract: Embedding as a Service (EaaS) has become a widely adopted solution, which offers feature extraction capabilities for addressing various downstream tasks in Natural Language Processing (NLP). Prior studies have shown that EaaS can be prone to model extraction attacks; nevertheless, this concern could be mitigated by adding backdoor watermarks to the text embeddings and subsequently verifying the attack models post-publication. Through the analysis of the recent watermarking strategy for EaaS, EmbMarker, we design a novel CSE (Clustering, Selection, Elimination) attack that removes the backdoor watermark while maintaining the high utility of embeddings, indicating that the previous watermarking approach can be breached. In response to this new threat, we propose a new protocol to make the removal of watermarks more challenging by incorporating multiple possible watermark directions. Our defense approach, WARDEN, notably increases the ste
&lt;/p&gt;</description></item><item><title>SCRAP&#37319;&#29992;&#33258;&#27965;&#25512;&#29702;&#21644;&#25552;&#21462;-&#20998;&#37197;&#31574;&#30053;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#21644;&#27491;&#30830;&#39044;&#27979;&#22235;&#20803;&#32452;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;ASQP&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00354</link><description>&lt;p&gt;
&#33258;&#27965;&#25512;&#29702;&#22522;&#20110;&#25552;&#21462;-&#20998;&#37197;&#31574;&#30053;&#30340;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with Extract-Then-Assign Strategy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00354
&lt;/p&gt;
&lt;p&gt;
SCRAP&#37319;&#29992;&#33258;&#27965;&#25512;&#29702;&#21644;&#25552;&#21462;-&#20998;&#37197;&#31574;&#30053;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#21644;&#27491;&#30830;&#39044;&#27979;&#22235;&#20803;&#32452;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;ASQP&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;&#65288;ASQP&#65289;&#20219;&#21153;&#20013;&#65292;&#29992;&#20110;&#39044;&#27979;&#24773;&#24863;&#22235;&#20803;&#30340;&#29983;&#25104;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#19981;&#31934;&#30830;&#39044;&#27979;&#21644;&#26377;&#38480;&#21487;&#35299;&#37322;&#24615;&#30340;&#22256;&#25200;&#65292;&#36825;&#26159;&#30001;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#22235;&#20803;&#32452;&#21512;&#25104;&#36807;&#31243;&#24314;&#27169;&#19981;&#36275;&#24341;&#36215;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#27965;&#25512;&#29702;-&#22522;&#20110;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;&#65288;SCRAP&#65289;&#65292;&#20248;&#21270;&#20854;&#27169;&#22411;&#20197;&#25353;&#39034;&#24207;&#29983;&#25104;&#25512;&#29702;&#21644;&#30456;&#24212;&#30340;&#24773;&#24863;&#22235;&#20803;&#12290; SCRAP&#37319;&#29992;&#25552;&#21462;-&#28982;&#21518;-&#20998;&#37197;&#25512;&#29702;&#31574;&#30053;&#65292;&#32039;&#23494;&#27169;&#20223;&#20154;&#31867;&#35748;&#30693;&#12290;&#26368;&#21518;&#65292;SCRAP&#36890;&#36807;&#19968;&#33268;&#24615;&#25237;&#31080;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20197;&#21450;&#36890;&#36807;&#19968;&#33268;&#24615;&#25237;&#31080;&#27491;&#30830;&#39044;&#27979;&#22235;&#20803;&#32452;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;ASQP&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00354v1 Announce Type: new  Abstract: In the task of aspect sentiment quad prediction (ASQP), generative methods for predicting sentiment quads have shown promising results. However, they still suffer from imprecise predictions and limited interpretability, caused by data scarcity and inadequate modeling of the quadruplet composition process. In this paper, we propose Self-Consistent Reasoning-based Aspect-sentiment quadruple Prediction (SCRAP), optimizing its model to generate reasonings and the corresponding sentiment quadruplets in sequence. SCRAP adopts the Extract-Then-Assign reasoning strategy, which closely mimics human cognition. In the end, SCRAP significantly improves the model's ability to handle complex reasoning tasks and correctly predict quadruplets through consistency voting, resulting in enhanced interpretability and accuracy in ASQP.
&lt;/p&gt;</description></item><item><title>Resonance RoPE&#26159;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;RoPE&#29305;&#24449;&#30340;&#25554;&#20540;&#26469;&#32553;&#23567;&#35757;&#32451;&#30701;-&#27979;&#35797;&#38271;&#22330;&#26223;&#19979;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#22312;&#32447;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00071</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#27867;&#21270;&#33021;&#21147;&#65306;&#20849;&#25391; RoPE
&lt;/p&gt;
&lt;p&gt;
Resonance RoPE: Improving Context Length Generalization of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00071
&lt;/p&gt;
&lt;p&gt;
Resonance RoPE&#26159;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;RoPE&#29305;&#24449;&#30340;&#25554;&#20540;&#26469;&#32553;&#23567;&#35757;&#32451;&#30701;-&#27979;&#35797;&#38271;&#22330;&#26223;&#19979;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#22312;&#32447;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#35757;&#32451;&#30701;-&#27979;&#35797;&#38271;&#65288;TSTL&#65289;&#22330;&#26223;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;Rotary Position Embedding&#65288;RoPE&#65289;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#22312;&#36739;&#30701;&#24207;&#21015;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#36739;&#38271;&#24207;&#21015;&#20013;&#36935;&#21040;&#20301;&#32622;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Resonance RoPE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;RoPE&#29305;&#24449;&#30340;&#25554;&#20540;&#26469;&#32553;&#23567;TSTL&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22312;&#32447;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PosGen&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#22522;&#20934;&#65292;&#19987;&#38376;&#38024;&#23545;TSTL&#22330;&#26223;&#20013;&#30340;&#31934;&#32454;&#34892;&#20026;&#20998;&#26512;&#65292;&#26088;&#22312;&#20174;&#38271;&#19978;&#19979;&#25991;&#20013;&#19981;&#26029;&#22686;&#21152;&#30340;&#20196;&#29260;&#29983;&#25104;&#22256;&#38590;&#21644;&#35782;&#21035;&#26032;&#20196;&#29260;&#20301;&#32622;&#30340;&#25361;&#25112;&#20013;&#20998;&#31163;&#20986;&#26469;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24212;&#29992;Resonance RoPE&#21518;&#65292;Transformer&#27169;&#22411;&#21487;&#20197;&#35782;&#21035;OOD&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00071v1 Announce Type: cross  Abstract: This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences. We introduce Resonance RoPE, a novel approach designed to narrow the generalization gap in TSTL scenarios by refining the interpolation of RoPE features for OOD positions, significantly improving the model performance without additional online computational costs. Furthermore, we present PosGen, a new synthetic benchmark specifically designed for fine-grained behavior analysis in TSTL scenarios, aiming to isolate the constantly increasing difficulty of token generation on long contexts from the challenges of recognizing new token positions. Our experiments on synthetic tasks show that after applying Resonance RoPE, Transformers recognize OOD position bet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21830;&#19994;LLMs&#30340;&#25552;&#31034;&#21453;&#31363;&#21462;&#25915;&#20987;&#26694;&#26550;PRSA&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#20851;&#38190;&#29305;&#24449;&#23454;&#29616;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.19200</link><description>&lt;p&gt;
PRSA&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#21453;&#30423;&#31363;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
PRSA: Prompt Reverse Stealing Attacks against Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21830;&#19994;LLMs&#30340;&#25552;&#31034;&#21453;&#31363;&#21462;&#25915;&#20987;&#26694;&#26550;PRSA&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#20851;&#38190;&#29305;&#24449;&#23454;&#29616;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#20316;&#20026;&#37325;&#35201;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#37325;&#35201;&#24615;&#12290;&#38543;&#30528;&#22522;&#20110;&#25552;&#31034;&#30340;&#26381;&#21153;&#30340;&#23835;&#36215;&#65292;&#22914;&#25552;&#31034;&#24066;&#22330;&#21644;LLM&#24212;&#29992;&#31243;&#24207;&#65292;&#25552;&#20379;&#32773;&#32463;&#24120;&#36890;&#36807;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#23637;&#31034;&#25552;&#31034;&#30340;&#33021;&#21147;&#65292;&#20197;&#21560;&#24341;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#23433;&#20840;&#38382;&#39064;&#65306;&#26292;&#38706;&#36755;&#20837;-&#36755;&#20986;&#23545;&#26159;&#21542;&#20250;&#23545;&#28508;&#22312;&#25552;&#31034;&#27844;&#28431;&#26500;&#25104;&#39118;&#38505;&#65292;&#20405;&#29359;&#24320;&#21457;&#32773;&#30340;&#30693;&#35782;&#20135;&#26435;&#65311;&#23601;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#35752;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#25506;&#35752;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21830;&#19994;LLMs&#30340;&#25552;&#31034;&#21453;&#31363;&#21462;&#25915;&#20987;&#26694;&#26550;&#65292;&#21363;PRSA&#12290;PRSA&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#25105;&#20204;&#27169;&#20223;&#24182;g
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19200v1 Announce Type: cross  Abstract: Prompt, recognized as crucial intellectual property, enables large language models (LLMs) to perform specific tasks without the need of fine-tuning, underscoring their escalating importance. With the rise of prompt-based services, such as prompt marketplaces and LLM applications, providers often display prompts' capabilities through input-output examples to attract users. However, this paradigm raises a pivotal security concern: does the exposure of input-output pairs pose the risk of potential prompt leakage, infringing on the intellectual property rights of the developers? To our knowledge, this problem still has not been comprehensively explored yet. To remedy this gap, in this paper, we perform the first in depth exploration and propose a novel attack framework for reverse-stealing prompts against commercial LLMs, namely PRSA. The main idea of PRSA is that by analyzing the critical features of the input-output pairs, we mimic and g
&lt;/p&gt;</description></item><item><title>VerifiNER&#26159;&#19968;&#20010;&#21518;&#32493;&#39564;&#35777;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#35782;&#21035;&#24182;&#20462;&#27491;&#29616;&#26377;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#30340;&#38169;&#35823;&#65292;&#20197;&#23454;&#29616;&#26356;&#24544;&#23454;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.18374</link><description>&lt;p&gt;
VerifiNER: &#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#25512;&#29702;&#22686;&#24378;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18374
&lt;/p&gt;
&lt;p&gt;
VerifiNER&#26159;&#19968;&#20010;&#21518;&#32493;&#39564;&#35777;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#35782;&#21035;&#24182;&#20462;&#27491;&#29616;&#26377;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#30340;&#38169;&#35823;&#65292;&#20197;&#23454;&#29616;&#26356;&#24544;&#23454;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#29305;&#23450;&#39046;&#22495;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20013;&#30340;&#26041;&#27861;&#65292;&#22914;&#29983;&#29289;&#21307;&#23398;NER&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#24544;&#23454;&#24615;&#65292;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#35748;&#20026;&#23454;&#20307;&#30340;&#30693;&#35782;&#21487;&#20197;&#22312;&#39564;&#35777;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;&#23613;&#31649;&#30693;&#35782;&#30340;&#26377;&#29992;&#24615;&#65292;&#20351;&#29992;&#30693;&#35782;&#26469;&#32416;&#27491;&#36825;&#20123;&#38169;&#35823;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#30693;&#35782;&#26412;&#36523;&#24182;&#19981;&#30452;&#25509;&#25351;&#31034;&#20986;&#30495;&#23454;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VerifiNER&#65292;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#20174;&#29616;&#26377;NER&#26041;&#27861;&#20013;&#35782;&#21035;&#38169;&#35823;&#24182;&#23558;&#20854;&#20462;&#27491;&#20026;&#26356;&#24544;&#23454;&#39044;&#27979;&#30340;&#21518;&#32493;&#39564;&#35777;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#39564;&#35777;&#36807;&#31243;&#20013;&#20805;&#20998;&#22522;&#20110;&#30693;&#35782;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;VerifiNER&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;VerifiNER&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18374v1 Announce Type: new  Abstract: Recent approaches in domain-specific named entity recognition (NER), such as biomedical NER, have shown remarkable advances. However, they still lack of faithfulness, producing erroneous predictions. We assume that knowledge of entities can be useful in verifying the correctness of the predictions. Despite the usefulness of knowledge, resolving such errors with knowledge is nontrivial, since the knowledge itself does not directly indicate the ground-truth label. To this end, we propose VerifiNER, a post-hoc verification framework that identifies errors from existing NER methods using knowledge and revises them into more faithful predictions. Our framework leverages the reasoning abilities of large language models to adequately ground on knowledge and the contextual information in the verification process. We validate effectiveness of VerifiNER through extensive experiments on biomedical datasets. The results suggest that VerifiNER can su
&lt;/p&gt;</description></item><item><title>UniVS&#25552;&#20986;&#20102;&#20351;&#29992;&#25552;&#31034;&#20316;&#20026;&#26597;&#35810;&#30340;&#32479;&#19968;&#35270;&#39057;&#20998;&#21106;&#26550;&#26500;&#65292;&#36890;&#36807;&#24179;&#22343;&#21270;&#21069;&#19968;&#24103;&#20013;&#30446;&#26631;&#30340;&#25552;&#31034;&#29305;&#24449;&#26469;&#35299;&#30721;&#25513;&#30721;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#30446;&#26631;&#30340;&#25552;&#31034;&#20132;&#21449;&#27880;&#24847;&#23618;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32479;&#19968;&#35270;&#39057;&#20998;&#21106;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.18115</link><description>&lt;p&gt;
UniVS&#65306;&#20351;&#29992;&#25552;&#31034;&#20316;&#20026;&#26597;&#35810;&#30340;&#32479;&#19968;&#21644;&#36890;&#29992;&#35270;&#39057;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
UniVS: Unified and Universal Video Segmentation with Prompts as Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18115
&lt;/p&gt;
&lt;p&gt;
UniVS&#25552;&#20986;&#20102;&#20351;&#29992;&#25552;&#31034;&#20316;&#20026;&#26597;&#35810;&#30340;&#32479;&#19968;&#35270;&#39057;&#20998;&#21106;&#26550;&#26500;&#65292;&#36890;&#36807;&#24179;&#22343;&#21270;&#21069;&#19968;&#24103;&#20013;&#30446;&#26631;&#30340;&#25552;&#31034;&#29305;&#24449;&#26469;&#35299;&#30721;&#25513;&#30721;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#30446;&#26631;&#30340;&#25552;&#31034;&#20132;&#21449;&#27880;&#24847;&#23618;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32479;&#19968;&#35270;&#39057;&#20998;&#21106;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32479;&#19968;&#22270;&#20687;&#20998;&#21106;&#65288;IS&#65289;&#21462;&#24471;&#20102;&#26368;&#26032;&#36827;&#23637;&#65292;&#20294;&#24320;&#21457;&#32479;&#19968;&#30340;&#35270;&#39057;&#20998;&#21106;&#65288;VS&#65289;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#36890;&#29992;&#31867;&#21035;&#25351;&#23450;&#30340;VS&#20219;&#21153;&#38656;&#35201;&#26816;&#27979;&#25152;&#26377;&#23545;&#35937;&#24182;&#36319;&#36394;&#23427;&#20204;&#36328;&#36830;&#32493;&#24103;&#65292;&#32780;&#30001;&#25552;&#31034;&#24341;&#23548;&#30340;VS&#20219;&#21153;&#38656;&#35201;&#22312;&#25972;&#20010;&#35270;&#39057;&#20013;&#37325;&#26032;&#35782;&#21035;&#30446;&#26631;&#24182;&#20351;&#29992;&#35270;&#35273;/&#25991;&#26412;&#25552;&#31034;&#65292;&#20351;&#24471;&#29992;&#30456;&#21516;&#26550;&#26500;&#22788;&#29702;&#19981;&#21516;&#20219;&#21153;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#35797;&#22270;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;VS&#26550;&#26500;&#65292;&#21363;UniVS&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#20316;&#20026;&#26597;&#35810;&#12290;UniVS&#23558;&#21069;&#19968;&#24103;&#20013;&#30446;&#26631;&#30340;&#25552;&#31034;&#29305;&#24449;&#20316;&#20026;&#20854;&#21021;&#22987;&#26597;&#35810;&#24179;&#22343;&#21270;&#65292;&#20197;&#26126;&#30830;&#35299;&#30721;&#25513;&#30721;&#65292;&#24182;&#22312;&#25513;&#30721;&#35299;&#30721;&#22120;&#20013;&#24341;&#20837;&#22522;&#20110;&#30446;&#26631;&#30340;&#25552;&#31034;&#20132;&#21449;&#27880;&#24847;&#23618;&#65292;&#20197;&#22312;&#20869;&#23384;&#27744;&#20013;&#25972;&#21512;&#25552;&#31034;&#29305;&#24449;&#12290;&#36890;&#36807;&#23558;&#20808;&#21069;&#24103;&#20013;&#30340;&#23454;&#20307;&#30340;&#39044;&#27979;&#25513;&#30721;&#20316;&#20026;&#23427;&#20204;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;UniVS&#23558;&#19981;&#21516;&#30340;VS&#20219;&#21153;&#36716;&#25442;&#25104;&#36890;&#29992;&#30340;VS&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18115v1 Announce Type: cross  Abstract: Despite the recent advances in unified image segmentation (IS), developing a unified video segmentation (VS) model remains a challenge. This is mainly because generic category-specified VS tasks need to detect all objects and track them across consecutive frames, while prompt-guided VS tasks require re-identifying the target with visual/text prompts throughout the entire video, making it hard to handle the different tasks with the same architecture. We make an attempt to address these issues and present a novel unified VS architecture, namely UniVS, by using prompts as queries. UniVS averages the prompt features of the target from previous frames as its initial query to explicitly decode masks, and introduces a target-wise prompt cross-attention layer in the mask decoder to integrate prompt features in the memory pool. By taking the predicted masks of entities from previous frames as their visual prompts, UniVS converts different VS ta
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20851;&#38190;&#25216;&#26415;&#12289;&#25351;&#26631;&#12289;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.17944</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;--&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on Tabular Data -- A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20851;&#38190;&#25216;&#26415;&#12289;&#25351;&#26631;&#12289;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#26041;&#38754;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#12289;&#38382;&#31572;&#21644;&#34920;&#26684;&#29702;&#35299;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;&#27599;&#20010;&#20219;&#21153;&#37117;&#24102;&#26469;&#29420;&#29305;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#23545;&#35813;&#30740;&#31350;&#39046;&#22495;&#20013;&#20851;&#38190;&#25216;&#26415;&#12289;&#25351;&#26631;&#12289;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20248;&#21270;&#26041;&#27861;&#30340;&#20840;&#38754;&#23457;&#26597;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24635;&#32467;&#24182;&#27604;&#36739;&#36825;&#20123;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20379;&#23545;&#25968;&#25454;&#38598;&#12289;&#25351;&#26631;&#21644;&#26041;&#27861;&#35770;&#30340;&#20840;&#38754;&#35843;&#26597;&#21644;&#20998;&#31867;&#12290;&#23427;&#35782;&#21035;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#20248;&#21183;&#12289;&#23616;&#38480;&#24615;&#12289;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#31354;&#30333;&#65292;&#21516;&#26102;&#20026;&#36825;&#19968;&#37325;&#35201;&#19988;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#24341;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17944v1 Announce Type: new  Abstract: Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;QRData&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#26368;&#24378;&#27169;&#22411;GPT-4&#22312;&#35813;&#27979;&#35797;&#20013;&#20934;&#30830;&#29575;&#20026;58&#65285;&#65292;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.17644</link><description>&lt;p&gt;
LLMs&#26159;&#21542;&#20855;&#22791;&#22522;&#20110;&#25968;&#25454;&#30340;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#65311;&#29992;&#25968;&#25454;&#23545;&#20808;&#36827;&#30340;&#23450;&#37327;&#25512;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;QRData&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#26368;&#24378;&#27169;&#22411;GPT-4&#22312;&#35813;&#27979;&#35797;&#20013;&#20934;&#30830;&#29575;&#20026;58&#65285;&#65292;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#25512;&#29702;&#26159;&#20998;&#26512;&#25968;&#25454;&#30340;&#20851;&#38190;&#25216;&#33021;&#65292;&#28982;&#32780;&#23545;&#36825;&#31181;&#33021;&#21147;&#30340;&#35780;&#20272;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Quantitative Reasoning with Data&#65288;QRData&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#19982;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19968;&#20010;&#31934;&#24515;&#26500;&#24314;&#30340;&#21253;&#21547;&#26469;&#33258;&#25945;&#31185;&#20070;&#12289;&#22312;&#32447;&#23398;&#20064;&#26448;&#26009;&#21644;&#23398;&#26415;&#35770;&#25991;&#30340;&#25968;&#25454;&#34920;&#30340;411&#20010;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#27604;&#36739;&#27169;&#22411;&#22312;&#25968;&#25454;&#21644;&#25991;&#26412;&#19978;&#30340;&#23450;&#37327;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#21253;&#21547;290&#20010;&#20165;&#25991;&#26412;&#38382;&#39064;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65292;&#21363;QRText&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#22522;&#20110;&#31243;&#24207;&#25512;&#29702;&#21644;&#20195;&#29702;&#25512;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;Chain-of-Thought&#12289;Program-of-Thoughts&#12289;ReAct&#21644;&#20195;&#30721;&#35299;&#37322;&#22120;&#36741;&#21161;&#31561;&#22312;&#21508;&#31181;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#12290;&#26368;&#24378;&#30340;&#27169;&#22411;GPT-4&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;58&#65285;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17644v1 Announce Type: cross  Abstract: Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement. Among open-source
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Contrastive EEG-Text Masked Autoencoder&#65288;CET-MAE&#65289;&#21644;E2T-PTR&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;EEG&#30340;&#35821;&#35328;&#35299;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.17433</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#23545;&#27604;&#24615;EEG-&#25991;&#26412;&#33945;&#29256;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#36716;&#31227;&#30340;&#34920;&#31034;&#22686;&#24378;EEG&#21040;&#25991;&#26412;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17433
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Contrastive EEG-Text Masked Autoencoder&#65288;CET-MAE&#65289;&#21644;E2T-PTR&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;EEG&#30340;&#35821;&#35328;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26080;&#21019;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#37325;&#24314;&#33258;&#28982;&#35821;&#35328;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20316;&#20026;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#30340;&#35821;&#35328;&#35299;&#30721;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;EEG&#30340;&#35821;&#35328;&#35299;&#30721;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#38754;&#20020;&#35832;&#22810;&#25216;&#26415;&#38382;&#39064;&#65292;&#22914;&#65306;1&#65289;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#36328;&#27169;&#24577;&#65288;EEG&#21644;&#25991;&#26412;&#20043;&#38388;&#65289;&#33258;&#23398;&#20064;&#19982;EEG&#29305;&#24449;&#25110;&#25991;&#26412;&#24207;&#21015;&#30340;&#27169;&#20869;&#33258;&#37325;&#26500;&#30340;&#28151;&#21512;&#31574;&#30053;&#65307;2&#65289;&#26410;&#20805;&#20998;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;EEG&#30340;&#35821;&#35328;&#35299;&#30721;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#24615;EEG-&#25991;&#26412;&#33945;&#29256;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;CET-MAE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#19987;&#29992;&#30340;&#22810;&#27969;&#32534;&#30721;&#22120;&#22312;EEG&#21644;&#25991;&#26412;&#20043;&#38388;&#20197;&#21450;&#20869;&#37096;&#36827;&#34892;&#22797;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;E2T-PTR&#65288;&#20351;&#29992;&#39044;&#35757;&#32451;&#21487;&#36716;&#31227;&#34920;&#31034;&#36827;&#34892;EEG&#21040;&#25991;&#26412;&#35299;&#30721;&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#32452;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17433v1 Announce Type: new  Abstract: Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modul
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#28151;&#20081;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#38388;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#23558;LMs&#23545;&#40784;&#21040;&#26368;&#36817;&#26102;&#38388;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.16797</link><description>&lt;p&gt;
&#35774;&#23450;&#26102;&#38388;&#65306;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Set the Clock: Temporal Alignment of Pretrained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#28151;&#20081;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#38388;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#23558;LMs&#23545;&#40784;&#21040;&#26368;&#36817;&#26102;&#38388;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#26469;&#33258;&#19981;&#21516;&#26102;&#38388;&#28857;&#30340;&#32593;&#32476;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#24120;&#27809;&#26377;&#20219;&#20309;&#26126;&#30830;&#30340;&#26102;&#38388;&#22522;&#30784;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;LMs&#30340;&#26102;&#38388;&#28151;&#20081;&#65292;&#24182;&#25506;&#35752;&#20102;&#23558;&#23427;&#20204;&#30340;&#20869;&#37096;&#30693;&#35782;&#23545;&#40784;&#21040;&#30446;&#26631;&#26102;&#38388;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#26102;&#38388;&#23545;&#40784;&#8221;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#33258;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;20K&#20010;&#26102;&#24577;&#38382;&#39064;&#21450;&#20854;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20174;2000&#24180;&#21040;2023&#24180;&#30340;&#27599;&#19968;&#24180;&#12290;&#26681;&#25454;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;LMs&#65288;&#20363;&#22914;LLaMa2&#65289;&#65292;&#23613;&#31649;&#26377;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#65288;&#20363;&#22914;2022&#24180;&#65289;&#65292;&#22823;&#22810;&#25968;&#20351;&#29992;&#26356;&#26089;&#30340;&#30693;&#35782;&#26469;&#22238;&#31572;&#38382;&#39064;&#65288;&#20363;&#22914;&#22312;2019&#24180;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20174;&#25552;&#31034;&#21040;&#24494;&#35843;&#65292;&#26469;&#23545;&#40784;LMs&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#20351;&#29992;&#26368;&#26032;&#30340;&#30693;&#35782;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#31181;&#23545;&#40784;&#20013;&#30340;&#21508;&#31181;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;LLaMa2&#23545;&#40784;&#21040;2022&#24180;&#21487;&#20197;&#23558;&#20854;&#24615;&#33021;&#25552;&#39640;&#39640;&#36798;62%
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16797v1 Announce Type: new  Abstract: Language models (LMs) are trained on web text originating from many points in time and, in general, without any explicit temporal grounding. This work investigates the temporal chaos of pretrained LMs and explores various methods to align their internal knowledge to a target time, which we call "temporal alignment." To do this, we first automatically construct a dataset containing 20K time-sensitive questions and their answers for each year from 2000 to 2023. Based on this dataset, we empirically show that pretrained LMs (e.g., LLaMa2), despite having a recent pretraining cutoff (e.g., 2022), mostly answer questions using earlier knowledge (e.g., in 2019). We then develop several methods, from prompting to finetuning, to align LMs to use their most recent knowledge when answering questions, and investigate various factors in this alignment. Our experiments show that aligning LLaMa2 to the year 2022 can boost its performance by up to 62% 
&lt;/p&gt;</description></item><item><title>&#23376;&#35789;&#26631;&#35760;&#21270;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20294;&#20854;&#25104;&#21151;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#12289;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#19981;&#26126;&#30830;&#12290;</title><link>https://arxiv.org/abs/2402.15010</link><description>&lt;p&gt;
&#27861;&#35821;&#21307;&#29992;&#21475;&#32617;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26631;&#35760;&#21270;&#26377;&#22810;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Important Is Tokenization in French Medical Masked Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15010
&lt;/p&gt;
&lt;p&gt;
&#23376;&#35789;&#26631;&#35760;&#21270;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20294;&#20854;&#25104;&#21151;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#12289;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#19981;&#26126;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23376;&#35789;&#30340;&#26631;&#35760;&#21270;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#20854;&#25104;&#21151;&#30340;&#30830;&#20999;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#65292;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#28982;&#19981;&#22815;&#28165;&#26970;&#12290;&#36825;&#22312;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#65292;&#20854;&#29305;&#28857;&#26159;&#20855;&#26377;&#31649;&#29702;&#24418;&#24577;&#32032;&#32452;&#21512;&#30340;&#29305;&#23450;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15010v1 Announce Type: cross  Abstract: Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tokenizers, and the role of morphological information in Indo-European languages remain insufficiently explored. This is particularly pertinent for biomedical terminology, characterized by specific rules governing morpheme combinations. Despite the agglutinative nature of biomedical terminology, existing language models do not explicitly incorporate 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14973</link><description>&lt;p&gt;
GenCeption&#65306;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#22810;&#27169;&#24577;LLM
&lt;/p&gt;
&lt;p&gt;
GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;&#24102;&#26631;&#27880;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#38590;&#20197;&#36319;&#19978;MLLM&#35780;&#20272;&#30340;&#24555;&#36895;&#21457;&#23637;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenCeption&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#38656;&#27880;&#37322;&#30340;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#20165;&#38656;&#35201;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#21453;&#26144;&#20986;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#12290;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;DrawCeption&#28216;&#25103;&#65292;GenCeption&#20174;&#19968;&#20010;&#38750;&#25991;&#26412;&#26679;&#26412;&#24320;&#22987;&#65292;&#24182;&#32463;&#21382;&#19968;&#31995;&#21015;&#36845;&#20195;&#30340;&#25551;&#36848;&#21644;&#29983;&#25104;&#27493;&#39588;&#12290;&#36845;&#20195;&#20043;&#38388;&#30340;&#35821;&#20041;&#28418;&#31227;&#20351;&#29992;GC@T&#25351;&#26631;&#36827;&#34892;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#39564;&#35777;&#20102;GenCeption&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;GenCeption&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26222;&#36941;&#23384;&#22312;&#19988;&#20197;&#21069;&#26410;&#35265;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#25193;&#23637;&#65292;&#20197;&#20943;&#36731;&#35757;&#32451;&#25968;&#25454;&#30340;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14973v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.
&lt;/p&gt;</description></item><item><title>&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14860</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#22522;&#20934;&#23454;&#20917;&#30340;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking Large Language Models without Ground Truth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14860
&lt;/p&gt;
&lt;p&gt;
&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#24433;&#21709;&#21147;&#30340;&#22686;&#24378;&#65292;&#35780;&#20272;&#21644;&#25490;&#21517;LLMs&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#33719;&#21462;&#26114;&#36149;&#30340;&#20154;&#31867;&#21709;&#24212;&#65292;&#35201;&#20040;&#20351;&#29992;LLMs&#25104;&#23545;&#22320;&#20114;&#30456;&#35780;&#20272;&#65292;&#36825;&#21487;&#33021;&#19981;&#22815;&#21487;&#38752;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#22312;&#32473;&#23450;&#19968;&#32452;&#25552;&#31034;&#25968;&#25454;&#38598;&#65288;&#27604;&#22914;&#38382;&#39064;&#12289;&#35828;&#26126;&#31561;&#65289;&#21644;&#19968;&#32452;LLMs&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#20219;&#20309;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#24773;&#20917;&#19979;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#21517;&#12290;&#21463;&#21040;&#29616;&#23454;&#29983;&#27963;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#19987;&#23478;&#21644;&#26377;&#30693;&#35782;&#30340;&#20154;&#37117;&#33021;&#35782;&#21035;&#19968;&#20010;&#26032;&#25163;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#35780;&#20272;&#20854;&#20182;&#20004;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#35782;&#21035;&#26368;&#24046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#24182;&#25552;&#20379;&#20102;&#25104;&#21151;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36890;&#36807;&#21453;&#22797;&#24212;&#29992;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;LLMs&#36827;&#34892;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#26377;&#38480;&#20559;&#22909;&#21453;&#39304;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.14760</link><description>&lt;p&gt;
&#27867;&#21270;&#22870;&#21169;&#24314;&#27169;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalizing Reward Modeling for Out-of-Distribution Preference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14760
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#26377;&#38480;&#20559;&#22909;&#21453;&#39304;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#23398;&#20064;(PL)&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26088;&#22312;&#20351;LLMs&#29983;&#25104;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#20197;&#24448;&#26377;&#20851;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#30340;&#30740;&#31350;&#24050;&#22312;&#20998;&#24067;&#20869;&#30340;PL&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#30340;&#38590;&#24230;&#65292;&#20026;&#27599;&#20010;&#36935;&#21040;&#30340;&#20998;&#24067;&#31163;&#25955;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#36229;&#20986;&#20998;&#24067;(OOD) PL&#20013;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#26469;&#22686;&#24378;LLMs&#26377;&#38480;&#20559;&#22909;&#21453;&#39304;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#23454;&#29992;&#30340;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;OOD PL&#38382;&#39064;&#12290;&#22312;&#20803;&#35757;&#32451;&#26399;&#38388;&#65292;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#24341;&#23548;&#31574;&#30053;&#23398;&#20064;&#20197;&#20351;&#20043;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#22312;&#36935;&#21040;&#27979;&#35797;&#20998;&#24067;&#26102;&#65292;&#20803;&#27979;&#35797;&#36807;&#31243;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14760v1 Announce Type: cross  Abstract: Preference learning (PL) with large language models (LLMs) aims to align the LLMs' generations with human preferences. Previous work on reinforcement learning from human feedback (RLHF) has demonstrated promising results in in-distribution PL. However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging. Thus, out-of-distribution (OOD) PL is practically useful for enhancing the generalization ability of LLMs with limited preference feedback. This work addresses OOD PL by optimizing a general reward model through a meta-learning approach. During meta-training, a bilevel optimization algorithm is utilized to learn a reward model capable of guiding policy learning to align with human preferences across various distributions. When encountering a test distribution, the meta-test procedure conducts regularized policy optimization using the learned reward model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13254</link><description>&lt;p&gt;
CounterCurate: &#36890;&#36807;&#23545;&#29031;&#20363;&#23376;&#22686;&#24378;&#29289;&#29702;&#21644;&#35821;&#20041;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;CounterCurate&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#20840;&#38754;&#25552;&#21319;&#23545;&#27604;&#21644;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#20851;&#38190;&#38382;&#39064;&#65306;&#24573;&#35270;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#25512;&#29702;&#65288;&#35745;&#25968;&#21644;&#20301;&#32622;&#29702;&#35299;&#65289;&#65292;&#20197;&#21450;&#21033;&#29992;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21453;&#20107;&#23454;&#24494;&#35843;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21019;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#31354;&#30333;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#31361;&#20986;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;LLaVA&#65289;&#22312;&#22522;&#20110;&#29289;&#29702;&#30340;&#32452;&#21512;&#25512;&#29702;&#20013;&#20960;&#20046;&#26080;&#27861;&#32988;&#20219;&#30340;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;GLIGEN&#29983;&#25104;&#24494;&#35843;&#25968;&#25454;&#65292;&#20351;&#24471;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65306;&#22312;&#25105;&#20204;&#26032;&#30340;&#31574;&#21010;&#30340;Flickr30k-Positions&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;CLIP&#21644;LLaVA&#30340;&#24615;&#33021;&#20998;&#21035;&#25552;&#39640;&#20102;+33%&#21644;+37%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#36866;&#37197;&#22120;&#22312;&#20923;&#32467;&#30340;&#34920;&#31034;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#36866;&#24212;&#36807;&#31243;&#28176;&#36827;&#20998;&#24067;&#22312;&#22810;&#23618;&#20013;&#65292;&#24182;&#19988;&#22823;&#37096;&#20998;&#39044;&#27979;&#20173;&#22312;&#28304;&#35821;&#35328;&#20013;&#36827;&#34892;&#28436;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.13137</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#36866;&#37197;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
The Hidden Space of Transformer Language Adapters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13137
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#36866;&#37197;&#22120;&#22312;&#20923;&#32467;&#30340;&#34920;&#31034;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#36866;&#24212;&#36807;&#31243;&#28176;&#36827;&#20998;&#24067;&#22312;&#22810;&#23618;&#20013;&#65292;&#24182;&#19988;&#22823;&#37096;&#20998;&#39044;&#27979;&#20173;&#22312;&#28304;&#35821;&#35328;&#20013;&#36827;&#34892;&#28436;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;transformer&#35821;&#35328;&#36866;&#37197;&#22120;&#30340;&#25805;&#20316;&#26041;&#24335;&#65292;&#36825;&#20123;&#23567;&#27169;&#22359;&#22312;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#35757;&#32451;&#65292;&#20197;&#23558;&#20854;&#39044;&#27979;&#36866;&#24212;&#21040;&#26032;&#30340;&#30446;&#26631;&#35821;&#35328;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36866;&#24212;&#21518;&#30340;&#39044;&#27979;&#20027;&#35201;&#22312;&#27169;&#22411;&#35757;&#32451;&#30340;&#28304;&#35821;&#35328;&#20013;&#28436;&#21464;&#65292;&#32780;&#30446;&#26631;&#35821;&#35328;&#20165;&#22312;&#27169;&#22411;&#30340;&#26368;&#21518;&#20960;&#23618;&#20013;&#21464;&#24471;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#36866;&#24212;&#36807;&#31243;&#26159;&#28176;&#36827;&#30340;&#65292;&#20998;&#24067;&#22312;&#22810;&#20010;&#23618;&#20013;&#65292;&#21487;&#20197;&#36339;&#36807;&#23569;&#37327;&#36866;&#37197;&#22120;&#32452;&#32780;&#19981;&#38477;&#20302;&#36866;&#24212;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#37197;&#22120;&#22312;&#27169;&#22411;&#30340;&#20923;&#32467;&#34920;&#31034;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#21516;&#26102;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20854;&#32467;&#26500;&#65292;&#32780;&#19981;&#26159;&#22312;&#8220;&#23396;&#31435;&#8221;&#30340;&#23376;&#31354;&#38388;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13137v1 Announce Type: new  Abstract: We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model's frozen representation space while largely preserving its structure, rather than on an 'isolated' subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency.
&lt;/p&gt;</description></item><item><title>&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24230;&#22312;&#32763;&#35793;&#20013;&#34920;&#29616;&#20026;&#27491;&#30456;&#20851;&#30340;&#24726;&#35770;&#65292;&#36825;&#26159;&#36763;&#26222;&#26862;&#24726;&#35770;&#30340;&#19968;&#20010;&#23454;&#20363;&#65292;&#22312;&#35821;&#26009;&#24211;&#32423;&#21035;&#19978;&#20108;&#32773;&#21576;&#27491;&#30456;&#20851;&#65292;&#22312;&#21333;&#20010;&#28304;&#27573;&#32423;&#21035;&#19978;&#23384;&#22312;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.12690</link><description>&lt;p&gt;
&#36763;&#26222;&#26862;&#24726;&#35770;&#19982;&#32763;&#35793;&#20013;&#30340;&#20934;&#30830;&#24615;-&#27969;&#30021;&#24230;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Simpson's Paradox and the Accuracy-Fluency Tradeoff in Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12690
&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24230;&#22312;&#32763;&#35793;&#20013;&#34920;&#29616;&#20026;&#27491;&#30456;&#20851;&#30340;&#24726;&#35770;&#65292;&#36825;&#26159;&#36763;&#26222;&#26862;&#24726;&#35770;&#30340;&#19968;&#20010;&#23454;&#20363;&#65292;&#22312;&#35821;&#26009;&#24211;&#32423;&#21035;&#19978;&#20108;&#32773;&#21576;&#27491;&#30456;&#20851;&#65292;&#22312;&#21333;&#20010;&#28304;&#27573;&#32423;&#21035;&#19978;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#22909;&#30340;&#32763;&#35793;&#24212;&#35813;&#24544;&#23454;&#20110;&#21407;&#25991;&#24182;&#36981;&#23432;&#30446;&#26631;&#35821;&#35328;&#30340;&#35268;&#33539;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20851;&#20110;&#36825;&#20123;&#30446;&#26631;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35770;&#38590;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#36763;&#26222;&#26862;&#24726;&#35770;&#30340;&#19968;&#31181;&#23454;&#20363;&#65292;&#34920;&#26126;&#22312;&#35821;&#26009;&#24211;&#32423;&#21035;&#19978;&#65292;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24230;&#21576;&#27491;&#30456;&#20851;&#20294;&#22312;&#21333;&#20010;&#28304;&#27573;&#30340;&#32423;&#21035;&#19978;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12690v1 Announce Type: new  Abstract: A good translation should be faithful to the source and should respect the norms of the target language. We address a theoretical puzzle about the relationship between these objectives. On one hand, intuition and some prior work suggest that accuracy and fluency should trade off against each other, and that capturing every detail of the source can only be achieved at the cost of fluency. On the other hand, quality assessment researchers often suggest that accuracy and fluency are highly correlated and difficult for human raters to distinguish (Callison-Burch et al. 2007). We show that the tension between these views is an instance of Simpson's paradox, and that accuracy and fluency are positively correlated at the level of the corpus but trade off at the level of individual source segments. We further suggest that the relationship between accuracy and fluency is best evaluated at the segment (or sentence) level, and that the trade off be
&lt;/p&gt;</description></item><item><title>LLMs&#33021;&#22815;&#22312;&#27809;&#26377;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20165;&#20174;&#36873;&#39033;&#20013;&#22238;&#31572;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#36890;&#36807;&#35760;&#24518;&#12289;&#36873;&#25321;&#21160;&#24577;&#21644;&#38382;&#39064;&#25512;&#29702;&#36827;&#34892;&#40657;&#30418;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#36873;&#25321;&#24615;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#19977;&#20010;&#20851;&#38190;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.12483</link><description>&lt;p&gt;
&#25991;&#29289;&#36824;&#26159;&#32465;&#26550;&#65306;LLMs&#22914;&#20309;&#22312;&#27809;&#26377;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#22238;&#31572;&#22810;&#39033;&#36873;&#25321;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12483
&lt;/p&gt;
&lt;p&gt;
LLMs&#33021;&#22815;&#22312;&#27809;&#26377;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20165;&#20174;&#36873;&#39033;&#20013;&#22238;&#31572;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#36890;&#36807;&#35760;&#24518;&#12289;&#36873;&#25321;&#21160;&#24577;&#21644;&#38382;&#39064;&#25512;&#29702;&#36827;&#34892;&#40657;&#30418;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#36873;&#25321;&#24615;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#19977;&#20010;&#20851;&#38190;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#65288;MCQA&#65289;&#36890;&#24120;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;MCQA&#26159;&#21542;&#25353;&#39044;&#26399;&#35780;&#20272;LLMs&#65292;&#25105;&#20204;&#25506;&#31350;LLMs&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#21482;&#26377;&#36873;&#39033;&#30340;&#25552;&#31034;&#26469;&#36827;&#34892;MCQA&#65292;&#20854;&#20013;&#27169;&#22411;&#24517;&#39035;&#20165;&#20174;&#36873;&#39033;&#20013;&#36873;&#25321;&#27491;&#30830;&#31572;&#26696;&#12290;&#22312;&#19977;&#20010;MCQA&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;LLMs&#20013;&#65292;&#36825;&#20010;&#25552;&#31034;&#22312;12&#20010;&#26696;&#20363;&#20013;&#30340;11&#20010;&#20013;&#20248;&#20110;&#22810;&#25968;&#22522;&#32447;&#65292;&#24182;&#21487;&#33719;&#24471;&#39640;&#36798;0.33&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;&#20026;&#20102;&#24110;&#21161;&#35299;&#37322;&#36825;&#31181;&#34892;&#20026;&#65292;&#25105;&#20204;&#23545;&#35760;&#24518;&#12289;&#36873;&#25321;&#21160;&#24577;&#21644;&#38382;&#39064;&#25512;&#29702;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#40657;&#30418;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#26377;&#19977;&#20010;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#21482;&#26377;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#20165;&#28304;&#33258;&#35760;&#24518;&#12290;&#20854;&#27425;&#65292;&#23545;&#21333;&#20010;&#36873;&#25321;&#30340;&#20808;&#39564;&#24182;&#19981;&#33021;&#23436;&#20840;&#35299;&#37322;&#21482;&#26377;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#65292;&#26263;&#31034;LLMs&#20351;&#29992;&#36873;&#25321;&#30340;&#38598;&#20307;&#21160;&#24577;&#12290;&#31532;&#19977;&#65292;LLMs&#26377;&#19968;&#23450;&#33021;&#21147;&#20174;&#36873;&#25321;&#20013;&#25512;&#26029;&#20986;&#30456;&#20851;&#38382;&#39064;&#65292;&#24182;&#19988;&#20196;&#20154;&#24778;&#35766;&#22320;&#26377;&#26102;&#29978;&#33267;&#21487;&#20197;&#21305;&#37197;&#21407;&#22987;&#38382;&#39064;&#12290;&#25105;&#20204;&#24076;&#26395;&#40723;&#21169;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12483v1 Announce Type: new  Abstract: Multiple-choice question answering (MCQA) is often used to evaluate large language models (LLMs). To see if MCQA assesses LLMs as intended, we probe if LLMs can perform MCQA with choices-only prompts, where models must select the correct answer only from the choices. In three MCQA datasets and four LLMs, this prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy gain. To help explain this behavior, we conduct an in-depth, black-box analysis on memorization, choice dynamics, and question inference. Our key findings are threefold. First, we find no evidence that the choices-only accuracy stems from memorization alone. Second, priors over individual choices do not fully explain choices-only accuracy, hinting that LLMs use the group dynamics of choices. Third, LLMs have some ability to infer a relevant question from choices, and surprisingly can sometimes even match the original question. We hope to motivate the use of st
&lt;/p&gt;</description></item><item><title>&#35748;&#30693;&#39537;&#21160;&#30340;&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#20986;&#21487;&#20197;&#35299;&#37322;&#35768;&#22810;&#35789;&#24207;&#26222;&#36941;&#35268;&#24459;&#30340;&#20248;&#21183;</title><link>https://arxiv.org/abs/2402.12363</link><description>&lt;p&gt;
&#20174;&#35748;&#30693;&#39537;&#21160;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#20986;&#30340;&#35789;&#24207;&#26222;&#36941;&#35268;&#24459;
&lt;/p&gt;
&lt;p&gt;
Emergent Word Order Universals from Cognitively-Motivated Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12363
&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#39537;&#21160;&#30340;&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#20986;&#21487;&#20197;&#35299;&#37322;&#35768;&#22810;&#35789;&#24207;&#26222;&#36941;&#35268;&#24459;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#19978;&#30340;&#35821;&#35328;&#34920;&#29616;&#20986;&#26576;&#20123;&#25152;&#35859;&#30340;&#31867;&#22411;&#23398;&#25110;&#34164;&#21547;&#35268;&#24459;&#65307;&#20363;&#22914;&#65292;&#20027;-&#23486;-&#35859;&#65288;SOV&#65289;&#30340;&#35789;&#24207;&#36890;&#24120;&#20351;&#29992;&#21518;&#32622;&#35789;&#12290;&#35299;&#37322;&#36825;&#20123;&#20559;&#22909;&#30340;&#26469;&#28304;&#26159;&#35821;&#35328;&#23398;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#35748;&#30693;&#20559;&#24046;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#35745;&#31639;&#27169;&#25311;&#30740;&#31350;&#35789;&#24207;&#26222;&#36941;&#35268;&#24459;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20855;&#26377;&#31867;&#22411;&#23398;&#20856;&#22411;&#35789;&#24207;&#30340;&#35821;&#35328;&#20542;&#21521;&#20110;&#20855;&#26377;&#30001;&#20855;&#26377;&#35748;&#30693;&#21512;&#29702;&#20559;&#24046;&#30340;LMs&#20272;&#35745;&#30340;&#36739;&#20302;&#22256;&#24785;&#24230;&#65306;&#21477;&#27861;&#20559;&#24046;&#12289;&#29305;&#23450;&#30340;&#35299;&#26512;&#31574;&#30053;&#21644;&#35760;&#24518;&#38480;&#21046;&#12290;&#36825;&#34920;&#26126;&#65292;&#36825;&#20123;&#35748;&#30693;&#20559;&#24046;&#21644;&#21487;&#39044;&#27979;&#24615;&#65288;&#22256;&#24785;&#24230;&#65289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21487;&#20197;&#35299;&#37322;&#35789;&#24207;&#26222;&#36941;&#35268;&#24459;&#30340;&#35768;&#22810;&#26041;&#38754;&#12290;&#36825;&#20063;&#23637;&#31034;&#20102;&#35748;&#30693;&#39537;&#21160;LMs&#30340;&#20248;&#21183;&#65292;&#22312;&#35745;&#31639;&#27169;&#25311;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#26102;&#36890;&#24120;&#29992;&#20110;&#35748;&#30693;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12363v1 Announce Type: new  Abstract: The world's languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) word order typically employs postpositions. Explaining the source of such biases is a key goal in linguistics. We study the word-order universals through a computational simulation with language models (LMs). Our experiments show that typologically typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of these cognitive biases and predictability (perplexity) can explain many aspects of word-order universals. This also showcases the advantage of cognitively-motivated LMs, which are typically employed in cognitive modeling, in the computational simulation of language universals.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;&#20102;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35266;&#23519;&#21040;LLMs&#22312;&#19981;&#21516;&#28216;&#25103;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12348</link><description>&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#35780;&#20272;&#25581;&#31034;LLM&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#30340;GTBench
&lt;/p&gt;
&lt;p&gt;
GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12348
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;&#20102;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35266;&#23519;&#21040;LLMs&#22312;&#19981;&#21516;&#28216;&#25103;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25972;&#21512;&#21040;&#20851;&#38190;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#30340;&#25112;&#30053;&#21644;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#26412;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20363;&#22914;&#65292;&#38656;&#35201;&#32431;&#36923;&#36753;&#21644;&#25112;&#30053;&#25512;&#29702;&#26469;&#19982;&#23545;&#25163;&#31454;&#20105;&#30340;&#26827;&#30424;&#28216;&#25103;&#21644;&#32440;&#29260;&#28216;&#25103;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;GTBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#35821;&#35328;&#39537;&#21160;&#30340;&#29615;&#22659;&#65292;&#21253;&#25324;10&#20010;&#24191;&#27867;&#35748;&#21487;&#30340;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#20840;&#38754;&#30340;&#28216;&#25103;&#20998;&#31867;&#27861;&#65306;&#23436;&#25972;&#20449;&#24687;&#19982;&#19981;&#23436;&#25972;&#20449;&#24687;&#65292;&#21160;&#24577;&#19982;&#38745;&#24577;&#65292;&#20197;&#21450;&#27010;&#29575;&#19982;&#30830;&#23450;&#24615;&#22330;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#34920;&#24449;LLMs&#30340;&#21338;&#24328;&#35770;&#25512;&#29702;&#65307;&#65288;2&#65289;LLM&#23545;&#25239;LLM&#30340;&#27604;&#36187;&#20316;&#20026;&#25512;&#29702;&#35780;&#20272;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65288;1&#65289;LLMs&#22312;&#21508;&#31181;&#28216;&#25103;&#22330;&#26223;&#19979;&#26377;&#19981;&#21516;&#30340;&#34892;&#20026;&#65307;&#20363;&#22914;&#65292;LLMs&#22312;&#23436;&#25972;&#21644;&#30830;&#23450;&#24615;&#28216;&#25103;&#20013;&#22833;&#36133;&#65292;&#20294;&#23427;&#20204;&#22312;&#27010;&#29575;&#28216;&#25103;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12348v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we investigate two key problems: (1) Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#33539;&#24335;"&#27983;&#35272;&#21644;&#38598;&#20013;"&#65292;&#36890;&#36807;&#22312;&#23558;&#29305;&#24449;&#36755;&#20837;LLMs&#20043;&#21069;&#36827;&#34892;&#28145;&#20837;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#20869;&#23481;&#29702;&#35299;&#20013;&#30340; prior-LLM &#27169;&#24577;&#38548;&#31163;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.12195</link><description>&lt;p&gt;
&#36890;&#36807; prior-LLM &#19978;&#19979;&#25991;&#34701;&#21512;&#26469;&#29702;&#35299;&#22810;&#27169;&#24577;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12195
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#33539;&#24335;"&#27983;&#35272;&#21644;&#38598;&#20013;"&#65292;&#36890;&#36807;&#22312;&#23558;&#29305;&#24449;&#36755;&#20837;LLMs&#20043;&#21069;&#36827;&#34892;&#28145;&#20837;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#20869;&#23481;&#29702;&#35299;&#20013;&#30340; prior-LLM &#27169;&#24577;&#38548;&#31163;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#36817;&#26399;&#23558;LLMs&#19982;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29702;&#35299;&#28041;&#21450;&#22810;&#24352;&#22270;&#29255;&#30340;&#19978;&#19979;&#25991;&#26041;&#38754;&#20173;&#26377;&#19981;&#36275;&#12290;&#36825;&#19968;&#32570;&#38519;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#65292;&#22312;&#23558;&#35270;&#35273;&#29305;&#24449;&#36755;&#20837;LLM&#20027;&#24178;&#20043;&#21069;&#65292;&#27599;&#24352;&#22270;&#29255;&#30340;&#35270;&#35273;&#29305;&#24449;&#37117;&#26159;&#30001;&#20923;&#32467;&#30340;&#32534;&#30721;&#22120;&#21333;&#29420;&#32534;&#30721;&#30340;&#65292;&#32570;&#20047;&#23545;&#20854;&#20182;&#22270;&#29255;&#21644;&#22810;&#27169;&#24577;&#25351;&#20196;&#30340;&#24847;&#35782;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#38382;&#39064;&#31216;&#20026; prior-LLM &#27169;&#24577;&#38548;&#31163;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#33539;&#24335;&#65292;&#21363;&#8220;&#27983;&#35272;&#21644;&#38598;&#20013;&#8221;&#65292;&#20197;&#23454;&#29616;&#22312;&#23558;&#29305;&#24449;&#36755;&#20837;LLMs&#20043;&#21069;&#36827;&#34892;&#28145;&#20837;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#34701;&#21512;&#12290;&#36825;&#31181;&#33539;&#24335;&#26368;&#21021;&#8220;&#27983;&#35272;&#8221;&#36755;&#20837;&#20197;&#33719;&#21462;&#20851;&#38190;&#35265;&#35299;&#65292;&#28982;&#21518;&#20877;&#27425;&#22238;&#39038;&#36755;&#20837;&#8220;&#38598;&#20013;&#8221;&#20110;&#20851;&#38190;&#32454;&#33410;&#65292;&#36890;&#36807;&#36825;&#20123;&#35265;&#35299;&#30340;&#25351;&#23548;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#20869;&#23481;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12195v1 Announce Type: new  Abstract: With the bloom of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) that incorporate LLMs with pre-trained vision models have recently demonstrated impressive performance across diverse vision-language tasks. However, they fall short to comprehend context involving multiple images. A primary reason for this shortcoming is that the visual features for each images are encoded individually by frozen encoders before feeding into the LLM backbone, lacking awareness of other images and the multimodal instructions. We term this issue as prior-LLM modality isolation and propose a two phase paradigm, browse-and-concentrate, to enable in-depth multimodal context fusion prior to feeding the features into LLMs. This paradigm initially "browses" through the inputs for essential insights, and then revisits the inputs to "concentrate" on crucial details, guided by these insights, to achieve a more comprehensive understanding of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#30340;&#39640;&#36890;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11804</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#25552;&#31034;&#22120;&#65306;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#30340;&#39640;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#24402;&#32435;&#25512;&#29702;&#26088;&#22312;&#25512;&#26029;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;KG&#20013;&#32570;&#22833;&#30340;&#20107;&#23454;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;KG&#24402;&#32435;&#25512;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22788;&#29702;&#22312;&#25991;&#26412;&#21644;&#32467;&#26500;&#26041;&#38754;&#37117;&#31232;&#32570;&#30340;&#20302;&#36164;&#28304;&#22330;&#26223;&#12290;&#26412;&#25991;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#20174;&#32780;&#20026;KG&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#24102;&#26469;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#26222;&#36866;&#24615;&#12290;&#22312;&#26041;&#27861;&#35770;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;ProLINK&#65292;&#26088;&#22312;&#22312;&#20219;&#24847;KG&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#22312;&#23454;&#36341;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;36&#20010;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11804v1 Announce Type: new  Abstract: Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-res
&lt;/p&gt;</description></item><item><title>MARS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;MARS&#65292;&#32771;&#34385;&#20102;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11756</link><description>&lt;p&gt;
MARS&#65306;&#29992;&#20110;&#29983;&#25104;&#24335;LLMs&#20013;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24847;&#20041;&#24863;&#30693;&#21709;&#24212;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11756
&lt;/p&gt;
&lt;p&gt;
MARS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;MARS&#65292;&#32771;&#34385;&#20102;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#32780;&#34987;&#24191;&#27867;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20135;&#29983;&#19981;&#20934;&#30830;&#25110;&#35823;&#23548;&#24615;&#36755;&#20986;&#30340;&#20542;&#21521;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#12290;&#22240;&#27492;&#65292;&#20272;&#35745;&#29983;&#25104;&#24335;LLM&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#26159;&#22686;&#24378;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;UE&#65289;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#20854;&#20013;SOTA&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#38271;&#24230;&#26631;&#20934;&#21270;&#35780;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24847;&#20041;&#24863;&#30693;&#21709;&#24212;&#35780;&#20998;&#65288;MARS&#65289;&#30340;&#26367;&#20195;&#38271;&#24230;&#26631;&#20934;&#21270;&#35780;&#20998;&#30340;UE&#26041;&#27861;&#12290;MARS&#26159;&#19968;&#31181;&#32771;&#34385;&#22312;&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#30340;&#26032;&#22411;&#35780;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#23558;MARS&#25972;&#21512;&#21040;UE&#26041;&#27861;&#20013;&#20250;&#22312;UE&#24615;&#33021;&#19978;&#24102;&#26469;&#26222;&#36941;&#21644;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#38381;&#21367;&#24335;&#38382;&#31572;&#26469;&#36827;&#34892;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11756v1 Announce Type: new  Abstract: Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book questi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37322;&#25918;&#28085;&#30422;&#35266;&#28857;&#25688;&#35201;&#35780;&#20272;&#30456;&#20851;&#19971;&#20010;&#32500;&#24230;&#30340;&#26032;&#25968;&#25454;&#38598;SUMMEVAL-OP&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102; Op-I-Prompt &#20316;&#20026;&#19968;&#31181;&#29420;&#31435;&#20110;&#32500;&#24230;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#21450; Op-Prompts &#20316;&#20026;&#19968;&#32452;&#20381;&#36182;&#20110;&#32500;&#24230;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#34920;&#26126; Op-I-Prompt &#26159;&#35780;&#20272;&#35266;&#28857;&#25688;&#35201;&#30340;&#19968;&#20010;&#24456;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#24179;&#22343; Spearman &#30456;&#20851;&#24615;&#20026; 0.70&#12290;</title><link>https://arxiv.org/abs/2402.11683</link><description>&lt;p&gt;
&#19968;&#31181;&#25903;&#37197;&#25152;&#26377;&#30340;&#25552;&#31034;&#65306;LLMs &#29992;&#20110;&#35266;&#28857;&#25688;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11683
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37322;&#25918;&#28085;&#30422;&#35266;&#28857;&#25688;&#35201;&#35780;&#20272;&#30456;&#20851;&#19971;&#20010;&#32500;&#24230;&#30340;&#26032;&#25968;&#25454;&#38598;SUMMEVAL-OP&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102; Op-I-Prompt &#20316;&#20026;&#19968;&#31181;&#29420;&#31435;&#20110;&#32500;&#24230;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#21450; Op-Prompts &#20316;&#20026;&#19968;&#32452;&#20381;&#36182;&#20110;&#32500;&#24230;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#34920;&#26126; Op-I-Prompt &#26159;&#35780;&#20272;&#35266;&#28857;&#25688;&#35201;&#30340;&#19968;&#20010;&#24456;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#24179;&#22343; Spearman &#30456;&#20851;&#24615;&#20026; 0.70&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20256;&#32479;&#22522;&#20110;&#21442;&#32771;&#30340;&#24230;&#37327;&#23545;&#35266;&#28857;&#25688;&#35201;&#36827;&#34892;&#35780;&#20272;&#24456;&#23569;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#24182;&#19988;&#24050;&#32463;&#26174;&#31034;&#20986;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#30456;&#23545;&#36739;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#26080;&#21442;&#32771;&#24230;&#37327;&#30340;NLG&#35780;&#20272;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35266;&#28857;&#25688;&#35201;&#35780;&#20272;&#26041;&#38754;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#26377;&#38480;&#30340;&#35266;&#28857;&#25688;&#35201;&#35780;&#20272;&#25968;&#25454;&#38598;&#38459;&#30861;&#20102;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#28085;&#30422;&#19982;&#35266;&#28857;&#25688;&#35201;&#35780;&#20272;&#30456;&#20851;&#30340;7&#20010;&#32500;&#24230;&#30340;SUMMEVAL-OP&#25968;&#25454;&#38598;&#65306;&#27969;&#30021;&#24615;&#12289;&#36830;&#36143;&#24615;&#12289;&#30456;&#20851;&#24615;&#12289;&#24544;&#23454;&#24230;&#12289;&#26041;&#38754;&#35206;&#30422;&#12289;&#24773;&#24863;&#19968;&#33268;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102; Op-I-Prompt&#65292;&#19968;&#20010;&#29420;&#31435;&#20110;&#32500;&#24230;&#30340;&#25552;&#31034;&#65292;&#20197;&#21450; Op-Prompts&#65292;&#19968;&#20010;&#20381;&#36182;&#20110;&#32500;&#24230;&#30340;&#29992;&#20110;&#35266;&#28857;&#25688;&#35201;&#35780;&#20272;&#30340;&#25552;&#31034;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Op-I-Prompt &#26159;&#35780;&#20272;&#35266;&#28857;&#25688;&#35201;&#30340;&#19968;&#20010;&#24456;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#24179;&#22343; Spearman &#30456;&#20851;&#24615;&#20026; 0.70&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11683v1 Announce Type: new  Abstract: Evaluation of opinion summaries using conventional reference-based metrics rarely provides a holistic evaluation and has been shown to have a relatively low correlation with human judgments. Recent studies suggest using Large Language Models (LLMs) as reference-free metrics for NLG evaluation, however, they remain unexplored for opinion summary evaluation. Moreover, limited opinion summary evaluation datasets inhibit progress. To address this, we release the SUMMEVAL-OP dataset covering 7 dimensions related to the evaluation of opinion summaries: fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity. We investigate Op-I-Prompt a dimension-independent prompt, and Op-Prompts, a dimension-dependent set of prompts for opinion summary evaluation. Experiments indicate that Op-I-Prompt emerges as a good alternative for evaluating opinion summaries achieving an average Spearman correlation of 0.70 w
&lt;/p&gt;</description></item><item><title>k-SemStamp&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#27700;&#21360;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;k&#22343;&#20540;&#32858;&#31867;&#20195;&#26367;LSH&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#25277;&#26679;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#65292;&#20026;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.11399</link><description>&lt;p&gt;
k-SemStamp&#65306;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#35821;&#20041;&#27700;&#21360;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11399
&lt;/p&gt;
&lt;p&gt;
k-SemStamp&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#27700;&#21360;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;k&#22343;&#20540;&#32858;&#31867;&#20195;&#26367;LSH&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#25277;&#26679;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#65292;&#20026;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#27700;&#21360;&#29983;&#25104;&#31639;&#27861;&#22312;&#35821;&#35328;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#21487;&#26816;&#27979;&#30340;&#31614;&#21517;&#65292;&#20197;&#20415;&#36827;&#34892;&#20107;&#21518;&#26816;&#27979;&#12290;&#34429;&#28982;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#30340;&#27700;&#21360;&#23481;&#26131;&#21463;&#21040;&#25913;&#20889;&#25915;&#20987;&#65292;&#20294;SemStamp (Hou&#31561;&#20154;&#65292;2023)&#22312;&#21477;&#23376;&#30340;&#35821;&#20041;&#34920;&#31034;&#19978;&#24212;&#29992;&#27700;&#21360;&#65292;&#24182;&#23637;&#31034;&#20986;&#24456;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;SemStamp&#21033;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#26469;&#21033;&#29992;&#20219;&#24847;&#36229;&#24179;&#38754;&#23545;&#35821;&#20041;&#31354;&#38388;&#36827;&#34892;&#20998;&#21306;&#65292;&#23548;&#33268;&#22312;&#40065;&#26834;&#24615;&#21644;&#36895;&#24230;&#20043;&#38388;&#23384;&#22312;&#27425;&#20248;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;k-SemStamp&#65292;&#36825;&#26159;SemStamp&#30340;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22686;&#24378;&#29256;&#65292;&#21033;&#29992;k&#22343;&#20540;&#32858;&#31867;&#20316;&#20026;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20197;&#20102;&#35299;&#20869;&#22312;&#30340;&#35821;&#20041;&#32467;&#26500;&#26469;&#20998;&#21306;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;k-SemStamp&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#25277;&#26679;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#65292;&#25512;&#36827;&#20102;&#26356;&#26377;&#25928;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11399v1 Announce Type: new  Abstract: Recent watermarked generation algorithms inject detectable signatures during language generation to facilitate post-hoc detection. While token-level watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023) applies watermark on the semantic representation of sentences and demonstrates promising robustness. SemStamp employs locality-sensitive hashing (LSH) to partition the semantic space with arbitrary hyperplanes, which results in a suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Experimental results indicate that k-SemStamp saliently improves its robustness and sampling efficiency while preserving the generation quality, advancing a more effective tool for machine-generated text detection.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#36890;&#36807;&#36827;&#21270;&#20195;&#29702;&#30340;&#21151;&#33021;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;</title><link>https://arxiv.org/abs/2402.11359</link><description>&lt;p&gt;
&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Training Language Model Agents without Modifying Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#36890;&#36807;&#36827;&#21270;&#20195;&#29702;&#30340;&#21151;&#33021;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26368;&#36817;&#24050;&#32463;&#23558;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37325;&#26032;&#23450;&#20041;&#20026;&#20195;&#29702;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#21151;&#33021;&#33258;&#21160;&#21270;&#22320;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;LLM&#20195;&#29702;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#20462;&#25913;LLM&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;LLM&#20195;&#29702;&#30340;&#26032;&#33539;&#24335;&#65292;&#24403;LLM&#38590;&#20197;&#25110;&#26080;&#27861;&#36827;&#34892;&#20462;&#25913;&#26102;&#23588;&#20854;&#26377;&#29992;&#12290;&#21463;&#21040;&#20154;&#31867;&#19981;&#26029;&#38203;&#36896;&#24037;&#20855;&#20197;&#36866;&#24212;&#29616;&#23454;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#32780;&#19981;&#26159;&#25913;&#21464;&#25105;&#20204;&#30340;&#29983;&#29289;&#32467;&#26500;&#20197;&#36866;&#24212;&#19968;&#32452;&#38745;&#24577;&#24037;&#20855;&#65292;&#25105;&#20204;&#25552;&#20986;&#36880;&#27493;&#38203;&#36896;&#20195;&#29702;&#30340;&#21151;&#33021;&#65292;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20462;&#25913;LLM&#26435;&#37325;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#21151;&#33021;&#35270;&#20026;&#21487;&#23398;&#20064;&#30340;&#8220;&#20195;&#29702;&#21442;&#25968;&#8221;&#24182;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#35757;&#32451;&#30340;&#22522;&#26412;&#24605;&#24819;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AgentOptimizer&#65292;&#21033;&#29992;LLM&#26356;&#26032;&#20195;&#29702;&#30340;&#21151;&#33021;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20195;&#29702;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11359v1 Announce Type: new  Abstract: Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with tw
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35884;&#35823;&#29702;&#35299;&#22522;&#20934;FLUB&#65292;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#29702;&#35299;&#33021;&#21147;&#19978;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#35774;&#35745;&#29409;&#29502;&#38382;&#39064;&#35780;&#20272;LLMs&#30340;&#35884;&#35823;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11100</link><description>&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#21040;&#29409;&#29502;&#38382;&#39064;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35884;&#35823;&#29702;&#35299;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11100
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35884;&#35823;&#29702;&#35299;&#22522;&#20934;FLUB&#65292;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#29702;&#35299;&#33021;&#21147;&#19978;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#35774;&#35745;&#29409;&#29502;&#38382;&#39064;&#35780;&#20272;LLMs&#30340;&#35884;&#35823;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;FaLlacy Understanding Benchmark (FLUB)&#30340;&#22522;&#20934;&#26469;&#25361;&#25112;LLMs&#30340;&#25512;&#29702;&#21644;&#29702;&#35299;&#33021;&#21147;&#65292;&#20854;&#20013;&#21253;&#21547;&#26131;&#20110;&#20154;&#31867;&#29702;&#35299;&#20294;&#38590;&#20110;&#27169;&#22411;&#25226;&#25569;&#30340;&#29409;&#29502;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;FLUB&#19987;&#27880;&#20110;&#20174;&#30495;&#23454;&#20114;&#32852;&#32593;&#29615;&#22659;&#20013;&#25910;&#38598;&#21040;&#30340;&#26840;&#25163;&#12289;&#24189;&#40664;&#21644;&#35823;&#23548;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#38590;&#24230;&#36882;&#22686;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#35884;&#35823;&#29702;&#35299;&#33021;&#21147;&#12290;&#22522;&#20110;FLUB&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;&#20195;&#34920;&#24615;&#30340;&#21644;&#20808;&#36827;&#30340;LLMs&#30340;&#34920;&#29616;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;FLUB&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24182;&#20540;&#24471;&#26410;&#26469;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11100v1 Announce Type: new  Abstract: Recently, Large Language Models (LLMs) have made remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning questions that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning questions that FLUB focuses on mainly consist of the tricky, humorous, and misleading questions collected from the real internet environment. And we design three tasks with increasing difficulty in the FLUB benchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB, we investigate the performance of multiple representative and advanced LLMs, reflecting our FLUB is challenging and worthy of more future study. Interesting discoveries and valuable insights 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#65292;&#20351;&#20854;&#29983;&#25104;&#21487;&#25511;&#30340;&#25903;&#25345;&#29992;&#25143;&#23450;&#20041;&#35770;&#28857;&#30340;&#22768;&#26126;&#65292;&#25913;&#36827;&#20102;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;DEBATunE&#27969;&#31243;&#12290;&#36890;&#36807;&#20004;&#20010;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25903;&#25345;&#29983;&#25104;&#26377;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.10614</link><description>&lt;p&gt;
&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#20197;&#29983;&#25104;&#21487;&#25511;&#30340;&#20855;&#26377;&#20105;&#35758;&#24615;&#30340;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#65292;&#20351;&#20854;&#29983;&#25104;&#21487;&#25511;&#30340;&#25903;&#25345;&#29992;&#25143;&#23450;&#20041;&#35770;&#28857;&#30340;&#22768;&#26126;&#65292;&#25913;&#36827;&#20102;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;DEBATunE&#27969;&#31243;&#12290;&#36890;&#36807;&#20004;&#20010;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25903;&#25345;&#29983;&#25104;&#26377;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#20195;&#34920;&#19981;&#21516;&#30340;&#20154;&#32676;&#65292;&#23588;&#20854;&#26159;&#23569;&#25968;&#32676;&#20307;&#65292;&#24182;&#20135;&#29983;&#25903;&#25345;&#20854;&#22810;&#26679;&#21270;&#29978;&#33267;&#26377;&#20105;&#35758;&#35266;&#28857;&#30340;&#22768;&#26126;&#23545;&#20110;&#21019;&#36896;&#19968;&#20010;&#21253;&#23481;&#30340;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#32570;&#20047;&#36275;&#22815;&#30340;&#25511;&#21046;&#24615;&#26469;&#25903;&#25345;&#29983;&#25104;&#20869;&#23481;&#30340;&#31435;&#22330;&#65292;&#20854;&#20013;&#24448;&#24448;&#21253;&#21547;&#19981;&#19968;&#33268;&#12289;&#20013;&#31435;&#25110;&#26377;&#20559;&#35265;&#30340;&#22768;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;LLMs&#22312;&#29983;&#25104;&#25903;&#25345;&#29992;&#25143;&#22312;&#25552;&#31034;&#20013;&#23450;&#20041;&#30340;&#35770;&#28857;&#30340;&#22768;&#26126;&#26102;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20004;&#20010;&#25345;&#26377;&#30456;&#21453;&#31435;&#22330;&#30340;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#20135;&#29983;&#20102;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#65292;&#36825;&#20123;&#22768;&#26126;&#23545;&#20110;&#25913;&#21892;LLMs&#30340;&#21487;&#25511;&#24615;&#26159;&#37325;&#35201;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Debate &amp; Tuning&#65288;&#8220;DEBATunE&#8221;&#65289;&#27969;&#31243;&#65292;&#36890;&#36807;&#24494;&#35843;LLMs&#29983;&#25104;&#36890;&#36807;&#36777;&#35770;&#33719;&#24471;&#30340;&#22768;&#26126;&#12290;&#20026;&#20102;&#26816;&#39564;DEBATunE&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#36804;&#20170;&#20026;&#27490;&#28085;&#30422;710&#20010;&#20105;&#35758;&#24615;&#20027;&#39064;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10614v1 Announce Type: cross  Abstract: Making LLMs speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment. However, existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements. In this paper, we improve the controllability of LLMs in generating statements supporting an argument the user defined in the prompt. We find that multi-round debates between two LLMs with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of LLMs. Motivated by this, we develop a novel debate &amp; tuning ("DEBATunE") pipeline finetuning LLMs to generate the statements obtained via debate. To examine DEBATunE, we curate the largest dataset of debate topics so far, which covers 710 contro
&lt;/p&gt;</description></item><item><title>BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.10373</link><description>&lt;p&gt;
BioMistral&#65306;&#38754;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10373
&lt;/p&gt;
&lt;p&gt;
BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#21644;&#21307;&#23398;&#31561;&#19987;&#19994;&#39046;&#22495;&#25552;&#20379;&#28508;&#22312;&#24212;&#29992;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#38024;&#23545;&#20581;&#24247;&#39046;&#22495;&#23450;&#21046;&#30340;&#24320;&#28304;LLMs&#21487;&#29992;&#65292;&#20294;&#23558;&#36890;&#29992;LLMs&#35843;&#25972;&#21040;&#21307;&#23398;&#39046;&#22495;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BioMistral&#65292;&#19968;&#31181;&#19987;&#20026;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#28304;LLM&#65292;&#37319;&#29992;Mistral&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;PubMed Central&#19978;&#36827;&#19968;&#27493;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;10&#20010;&#24050;&#24314;&#31435;&#30340;&#33521;&#25991;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#19978;&#23545;BioMistral&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#36890;&#36807;&#37327;&#21270;&#21644;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#33719;&#24471;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BioMistral&#30456;&#36739;&#20110;&#29616;&#26377;&#24320;&#28304;&#21307;&#23398;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#19982;&#19987;&#26377;&#23545;&#25163;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10373v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#19982;&#23398;&#29983;LLM&#30340;&#25968;&#25454;&#36873;&#25321;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;</title><link>https://arxiv.org/abs/2402.10110</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#65306;LLM&#25351;&#20196;&#35843;&#33410;&#30340;&#23398;&#29983;&#36873;&#25321;&#25968;&#25454;&#22238;&#25910;
&lt;/p&gt;
&lt;p&gt;
Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#19982;&#23398;&#29983;LLM&#30340;&#25968;&#25454;&#36873;&#25321;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#33410;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35828;&#38750;&#24120;&#20851;&#38190;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25351;&#20196;&#36319;&#36394;&#21644;&#20219;&#21153;&#36866;&#24212;&#33021;&#21147;&#65292;&#20294;&#20854;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#26041;&#27861;&#37117;&#33268;&#21147;&#20110;&#25913;&#36827;&#25968;&#25454;&#36136;&#37327;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#25968;&#25454;&#19982;&#27491;&#22312;&#24494;&#35843;&#30340;&#23398;&#29983;&#27169;&#22411;&#30340;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#8212;&#8212;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#65292;&#36890;&#36807;&#32467;&#21512;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#65292;&#20197;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#12290;&#36825;&#31181;&#24072;&#29983;&#21512;&#20316;&#20135;&#29983;&#20102;&#39640;&#36136;&#37327;&#19988;&#19982;&#23398;&#29983;LLM&#20860;&#23481;&#30340;&#25351;&#20196;&#21709;&#24212;&#23545;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#21644;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#24120;&#33021;&#25913;&#21892;LLM&#24494;&#35843;&#21644;&#33258;&#25105;&#20248;&#21270;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10110v1 Announce Type: cross  Abstract: Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#35299;&#27602;&#21270;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#26377;&#27602;&#25991;&#26412;&#33258;&#21160;&#36716;&#21270;&#20026;&#26080;&#27602;&#25991;&#26412;&#12290;&#36890;&#36807;&#30693;&#35782;&#36716;&#31227;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#21024;&#38500;&#37325;&#24314;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21033;&#29992;Dementieva&#31561;&#20154;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23567;&#22411;&#30340;&#21360;&#22320;&#35821;&#24179;&#34892;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.07767</link><description>&lt;p&gt;
&#25991;&#26412;&#35299;&#27602;&#20316;&#20026;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#20013;&#30340;&#39118;&#26684;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Text Detoxification as Style Transfer in English and Hindi
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#35299;&#27602;&#21270;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#26377;&#27602;&#25991;&#26412;&#33258;&#21160;&#36716;&#21270;&#20026;&#26080;&#27602;&#25991;&#26412;&#12290;&#36890;&#36807;&#30693;&#35782;&#36716;&#31227;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#21024;&#38500;&#37325;&#24314;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21033;&#29992;Dementieva&#31561;&#20154;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23567;&#22411;&#30340;&#21360;&#22320;&#35821;&#24179;&#34892;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#25991;&#26412;&#35299;&#27602;&#65292;&#21363;&#33258;&#21160;&#23558;&#26377;&#27602;&#25991;&#26412;&#36716;&#21270;&#20026;&#38750;&#26377;&#27602;&#25991;&#26412;&#12290;&#36825;&#39033;&#20219;&#21153;&#26377;&#21161;&#20110;&#26356;&#23433;&#20840;&#12289;&#26356;&#23562;&#37325;&#30340;&#22312;&#32447;&#20132;&#27969;&#65292;&#24182;&#21487;&#34987;&#35270;&#20026;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#65288;TST&#65289;&#20219;&#21153;&#65292;&#22312;&#27492;&#20219;&#21153;&#20013;&#65292;&#25991;&#26412;&#39118;&#26684;&#21457;&#29983;&#21464;&#21270;&#65292;&#20294;&#20869;&#23481;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#65306;&#20174;&#31867;&#20284;&#20219;&#21153;&#20013;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#24207;&#21015;&#21040;&#24207;&#21015;&#24314;&#27169;&#19982;&#21508;&#31181;&#27602;&#24615;&#20998;&#31867;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#21024;&#38500;&#21644;&#37325;&#24314;&#26041;&#27861;&#12290;&#20026;&#25903;&#25345;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;Dementieva&#31561;&#20154;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65288;2021&#24180;&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;&#26377;&#27602;&#25991;&#26412;&#23545;&#24212;&#30340;&#22810;&#20010;&#29256;&#26412;&#30340;&#35299;&#27602;&#25991;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19987;&#23478;&#20154;&#24037;&#27880;&#37322;&#21592;&#36873;&#25321;&#20102;&#26368;&#20339;&#30340;&#21464;&#20307;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#26377;&#27602;&#21477;&#23376;&#19982;&#19968;&#20010;&#36866;&#24403;&#30340;&#35299;&#27602;&#29256;&#26412;&#37197;&#23545;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#23567;&#22411;&#30340;&#21360;&#22320;&#35821;&#24179;&#34892;&#25968;&#25454;&#38598;&#65292;&#19982;&#33521;&#35821;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#23545;&#40784;&#65292;&#36866;&#29992;&#20110;&#35780;&#20272;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on text detoxification, i.e., automatically converting toxic text into non-toxic text. This task contributes to safer and more respectful online communication and can be considered a Text Style Transfer (TST) task, where the text style changes while its content is preserved. We present three approaches: knowledge transfer from a similar task, multi-task learning approach, combining sequence-to-sequence modeling with various toxicity classification tasks, and, delete and reconstruct approach. To support our research, we utilize a dataset provided by Dementieva et al.(2021), which contains multiple versions of detoxified texts corresponding to toxic texts. In our experiments, we selected the best variants through expert human annotators, creating a dataset where each toxic sentence is paired with a single, appropriate detoxified version. Additionally, we introduced a small Hindi parallel dataset, aligning with a part of the English dataset, suitable for evaluation purp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.06255</link><description>&lt;p&gt;
&#36827;&#21462;&#30340;&#40077;&#21187;&#36890;&#36807;&#25552;&#31034;&#23545;&#25239;&#35843;&#25972;&#25269;&#21046;&#36234;&#29425;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#23481;&#26131;&#21463;&#21040;&#29305;&#23450;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#32469;&#36807;&#20869;&#32622;&#30340;&#23433;&#20840;&#25514;&#26045;&#24182;&#25552;&#20379;&#21361;&#38505;&#25110;&#38750;&#27861;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#36234;&#29425;&#34892;&#20026;&#12290;&#20026;&#20102;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38598;&#20013;&#22312;&#20869;&#23481;&#36807;&#28388;&#25110;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning&#65288;PAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#23545;&#25239;&#35757;&#32451;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#25105;&#20204;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20132;&#26367;&#26356;&#26032;&#25915;&#20987;&#21644;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20174;&#25552;&#31034;&#35843;&#25972;&#30340;&#35282;&#24230;&#23454;&#26045;&#38450;&#24481;&#30340;&#20154;&#12290;&#19968;&#26086;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#19981;&#20250;&#24433;&#21709;LLMs&#30340;&#25805;&#20316;&#25928;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25269;&#24481;&#36234;&#29425;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak. To protect LLMs from producing harmful information, various defense strategies are proposed, with most focusing on content filtering or adversarial training of models. In this paper, we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense control mechanism, which is then embedded as a prefix to user prompts to implement our defense strategy. We design a training process similar to adversarial training to achieve our optimized goal, alternating between updating attack and defense controls. To our knowledge, we are the first to implement defense from the perspective of prompt tuning. Once employed, our method will hardly impact the operational efficiency of LLMs. Experiments show that our method i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#26469;&#33258;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#12290;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#21487;&#20197;&#22312;&#22238;&#31572;&#26597;&#35810;&#21069;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#65292;&#24182;&#36890;&#36807;MATRIX-simulated&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#20445;&#25345;&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#36981;&#20174;&#21644;&#25512;&#29702;&#36895;&#24230;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;</title><link>https://arxiv.org/abs/2402.05699</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#22404;&#26029;&#23545;&#35805;&#30340;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#26469;&#33258;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#12290;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#21487;&#20197;&#22312;&#22238;&#31572;&#26597;&#35810;&#21069;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#65292;&#24182;&#36890;&#36807;MATRIX-simulated&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#20445;&#25345;&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#36981;&#20174;&#21644;&#25512;&#29702;&#36895;&#24230;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#65292;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#20511;&#37492;&#31038;&#20250;&#23398;&#30340;&#35265;&#35299;&#65292;&#21363;&#35748;&#35782;&#21040;&#25152;&#26377;&#21508;&#26041;&#30340;&#20851;&#20999;&#26159;&#22609;&#36896;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23545;&#40784;LLMs&#30340;&#26032;&#26041;&#21521;&#65306;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#21019;&#26032;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#22120;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#36755;&#20837;&#26597;&#35810;&#21608;&#22260;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#20351;LLM&#22312;&#22238;&#31572;&#21069;&#33021;&#22815;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#12290;MATRIX&#31867;&#20284;&#20110;&#19968;&#20010;&#8220;&#22404;&#26029;&#23545;&#35805;&#8221;&#19979;&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#22312;&#20854;&#20013;&#25198;&#28436;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#22810;&#20010;&#35282;&#33394;&#24182;&#36827;&#34892;&#33258;&#25105;&#23454;&#36341;&#12290;&#20026;&#20102;&#24341;&#20837;&#36825;&#31181;&#23545;&#40784;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;MATRIX&#27169;&#25311;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#30830;&#20445;&#20854;&#22312;&#19981;&#24433;&#21709;&#25512;&#29702;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#31526;&#21512;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;&#26368;&#21518;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that ou
&lt;/p&gt;</description></item><item><title>AttnLRP&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#26469;&#35299;&#20915;&#20102;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#24402;&#22240;&#38382;&#39064;&#65292;&#20855;&#26377;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05602</link><description>&lt;p&gt;
AttnLRP: &#27880;&#24847;&#21147;&#24863;&#30693;&#30340;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#29992;&#20110;Transformer
&lt;/p&gt;
&lt;p&gt;
AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05602
&lt;/p&gt;
&lt;p&gt;
AttnLRP&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#26469;&#35299;&#20915;&#20102;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#24402;&#22240;&#38382;&#39064;&#65292;&#20855;&#26377;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#20559;&#35265;&#30340;&#39044;&#27979;&#21644;&#24187;&#35937;&#65292;&#36825;&#31361;&#26174;&#20102;&#29702;&#35299;&#20854;&#27169;&#22411;&#20869;&#37096;&#25512;&#29702;&#36807;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#23545;&#25972;&#20010;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#20934;&#30830;&#24402;&#22240;&#24182;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#34429;&#28982;&#23384;&#22312;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#12290;&#36890;&#36807;&#23545;Llama 2&#12289;Flan-T5&#21644;Vision Transformer&#26550;&#26500;&#19978;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#65292;&#20026;&#27010;&#24565;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a singular backward pass. Through extensive evaluations against existing methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#8220;ChatCoach&#8221;&#65292;&#19968;&#20010;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21307;&#29983;&#21512;&#20316;&#30340;&#26694;&#26550;&#65292;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#26102;&#21453;&#39304;&#65292;&#20197;&#24110;&#21161;&#21307;&#23398;&#23398;&#21592;&#25552;&#39640;&#27807;&#36890;&#25216;&#24039;&#12290;</title><link>https://arxiv.org/abs/2402.05547</link><description>&lt;p&gt;
&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65306;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#8220;ChatCoach&#8221;&#65292;&#19968;&#20010;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21307;&#29983;&#21512;&#20316;&#30340;&#26694;&#26550;&#65292;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#26102;&#21453;&#39304;&#65292;&#20197;&#24110;&#21161;&#21307;&#23398;&#23398;&#21592;&#25552;&#39640;&#27807;&#36890;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20256;&#32479;&#24212;&#29992;&#20027;&#35201;&#38598;&#20013;&#22312;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#26381;&#21153;&#19978;&#65292;&#22686;&#24378;&#24739;&#32773;&#20114;&#21160;&#21644;&#25252;&#29702;&#20132;&#20184;&#65292;&#20363;&#22914;&#21307;&#23398;&#23545;&#35805;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;NLP&#22312;&#24110;&#21161;&#32463;&#39564;&#19981;&#20016;&#23500;&#30340;&#21307;&#29983;&#65292;&#29305;&#21035;&#26159;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#31561;&#39046;&#22495;&#30340;&#28508;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;ChatCoach&#8221;&#65292;&#19968;&#20010;&#38598;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#19968;&#20010;&#24739;&#32773;&#20195;&#29702;&#21644;&#19968;&#20010;&#36741;&#23548;&#20195;&#29702;&#20849;&#21516;&#25903;&#25345;&#21307;&#23398;&#23398;&#21592;&#22312;&#20250;&#35786;&#36807;&#31243;&#20013;&#32451;&#20064;&#21307;&#23398;&#27807;&#36890;&#25216;&#24039;&#12290;&#19982;&#20256;&#32479;&#30340;&#23545;&#35805;&#31995;&#32479;&#19981;&#21516;&#65292;ChatCoach&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#29615;&#22659;&#65292;&#21307;&#29983;&#21487;&#20197;&#22312;&#20854;&#20013;&#19982;&#24739;&#32773;&#20195;&#29702;&#36827;&#34892;&#21307;&#23398;&#23545;&#35805;&#12290;&#21516;&#26102;&#65292;&#36741;&#23548;&#20195;&#29702;&#20250;&#25552;&#20379;&#23454;&#26102;&#21453;&#39304;&#32473;&#21307;&#29983;&#12290;&#20026;&#20102;&#26500;&#24314;ChatCoach&#31995;&#32479;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#38598;&#25104;&#20102;ChatGPT&#21644;Llama2&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35780;&#20272;&#23427;&#20204;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical dialogue systems. However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored. We introduce ``ChatCoach,'' an integrated human-AI cooperative framework. Within this framework, both a patient agent and a coaching agent collaboratively support medical learners in practicing their medical communication skills during consultations. Unlike traditional dialogue systems, ChatCoach provides a simulated environment where a human doctor can engage in medical dialogue with a patient agent. Simultaneously, a coaching agent provides real-time feedback to the doctor. To construct the ChatCoach system, we developed a dataset and integrated Large Language Models such as ChatGPT and Llama2, aiming to assess 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.04875</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Provable Length and Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#26356;&#38271;&#24207;&#21015;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;&#32452;&#21512;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#20196;&#29260;&#32452;&#21512;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#26159;&#37325;&#35201;&#30340;&#38750;&#20998;&#24067;&#21270;&#27867;&#21270;&#24418;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#26550;&#26500;&#20013;&#65292;&#26397;&#30528;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#26681;&#25454;&#26550;&#26500;&#30340;&#19981;&#21516;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#30340;&#24517;&#35201;&#24615;&#65292;&#20363;&#22914;&#19982;&#30495;&#23454;&#34920;&#31034;&#20855;&#26377;&#32447;&#24615;&#25110;&#25490;&#21015;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#36890;&#29992;&#12289;&#38750;&#21442;&#25968;&#30340;&#21098;&#26525;&#31639;&#27861;KEN&#65292;&#23427;&#33021;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#22823;&#24133;&#33410;&#30465;&#20869;&#23384;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#26368;&#37325;&#35201;&#30340;&#21442;&#25968;&#23454;&#29616;&#20102;&#23545;transformer&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;KEN&#22312;&#26368;&#23569;&#21442;&#25968;&#20943;&#23569;25%&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#31561;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03142</link><description>&lt;p&gt;
"&#23569;&#21363;&#26159;&#22810;&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#31616;&#21333;&#38750;&#21442;&#25968;&#21098;&#26525;&#31639;&#27861;"
&lt;/p&gt;
&lt;p&gt;
Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#36890;&#29992;&#12289;&#38750;&#21442;&#25968;&#30340;&#21098;&#26525;&#31639;&#27861;KEN&#65292;&#23427;&#33021;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#22823;&#24133;&#33410;&#30465;&#20869;&#23384;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#26368;&#37325;&#35201;&#30340;&#21442;&#25968;&#23454;&#29616;&#20102;&#23545;transformer&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;KEN&#22312;&#26368;&#23569;&#21442;&#25968;&#20943;&#23569;25%&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#31561;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#21098;&#26525;&#31639;&#27861;&#36890;&#24120;&#23384;&#22312;&#26550;&#26500;&#29305;&#24322;&#24615;&#12289;&#36807;&#24230;&#22797;&#26434;&#21644;&#20381;&#36182;&#22797;&#26434;&#35745;&#31639;&#31561;&#38480;&#21046;&#65292;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#30340;&#31616;&#21333;&#12289;&#36890;&#29992;&#12289;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#31639;&#27861;KEN&#12290;KEN&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#26377;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#26368;&#37325;&#35201;&#30340;&#21442;&#25968;&#65292;&#21516;&#26102;&#23558;&#20854;&#20182;&#21442;&#25968;&#24674;&#22797;&#21040;&#39044;&#35757;&#32451;&#29366;&#24577;&#65292;&#20174;&#32780;&#26500;&#24314;&#20248;&#21270;&#21518;&#30340;transformer&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#21482;&#23384;&#20648;&#20248;&#21270;&#21518;&#30340;&#23376;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20869;&#23384;&#33410;&#30465;&#12290;&#23545;&#19971;&#20010;transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;KEN&#22312;&#26368;&#23569;&#21442;&#25968;&#20943;&#23569;25%&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#31561;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning has become increasingly crucial due to the complexity of neural network models and their widespread use in various fields. Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on complex calculations, rendering them impractical for real-world applications. In this paper, we propose KEN: a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE). KEN aims to construct optimized transformer models by selectively preserving the most significant parameters while restoring others to their pre-training state. This approach maintains model performance while allowing storage of only the optimized subnetwork, leading to significant memory savings. Extensive evaluations on seven transformer models demonstrate that KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%. In-depth comparisons against other 
&lt;/p&gt;</description></item><item><title>BRAIn&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#22870;&#21169;&#26465;&#20214;&#21270;&#32553;&#20943;&#25512;&#26029;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#39304;&#26469;&#25913;&#36827;RLHF&#65292;&#22312;LLM&#23545;&#40784;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02479</link><description>&lt;p&gt;
BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback
&lt;/p&gt;
&lt;p&gt;
BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02479
&lt;/p&gt;
&lt;p&gt;
BRAIn&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#22870;&#21169;&#26465;&#20214;&#21270;&#32553;&#20943;&#25512;&#26029;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#39304;&#26469;&#25913;&#36827;RLHF&#65292;&#22312;LLM&#23545;&#40784;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#32487;Proximal Policy Optimization (PPO)&#21462;&#24471;&#25104;&#21151;&#20043;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22914;Sequence Likelihood Calibration (SLiC)&#21644;Direct Policy Optimization (DPO)&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#31163;&#32447;&#30340;&#65292;&#24182;&#19988;&#20197;&#38388;&#25509;&#30340;&#26041;&#24335;&#20351;&#29992;&#22870;&#21169;&#12290;&#36825;&#20123;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;DPO&#65292;&#30001;&#20110;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#65292;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;LLM&#23545;&#40784;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36951;&#28431;&#20102;PPO&#26041;&#27861;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#35832;&#22914;SLiC&#25110;RRHF&#30340;&#26041;&#27861;&#20165;&#21033;&#29992;&#22870;&#21169;&#27169;&#22411;(RM)&#36827;&#34892;&#25490;&#24207;/&#20559;&#22909;&#65292;&#20002;&#22833;&#20102;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#24573;&#30053;&#20102;RM&#30340;&#21442;&#25968;&#24418;&#24335;(&#20363;&#22914;Bradley-Terry&#12289;Plackett-Luce)&#65307;&#32780;&#35832;&#22914;DPO&#30340;&#26041;&#27861;&#29978;&#33267;&#19981;&#20351;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;BRAIn&#65292;&#23427;&#23558;RM&#20316;&#20026;&#20998;&#24067;&#21305;&#37197;&#26041;&#27861;&#30340;&#19968;&#37096;&#20998;&#37325;&#26032;&#24341;&#20837;&#12290;BRAIn&#32771;&#34385;&#21040;&#20102;LLM&#20998;&#24067;&#22312;&#20551;&#35774;&#36755;&#20986;&#36136;&#37327;&#33391;&#22909;&#30340;&#26465;&#20214;&#19979;&#65292;&#24182;&#24212;&#29992;B...
&lt;/p&gt;
&lt;p&gt;
Following the success of Proximal Policy Optimization (PPO) for Reinforcement Learning from Human Feedback (RLHF), new techniques such as Sequence Likelihood Calibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that are offline in nature and use rewards in an indirect manner. These techniques, in particular DPO, have recently become the tools of choice for LLM alignment due to their scalability and performance. However, they leave behind important features of the PPO approach. Methods such as SLiC or RRHF make use of the Reward Model (RM) only for ranking/preference, losing fine-grained information and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce), while methods such as DPO do not use even a separate reward model. In this work, we propose a novel approach, named BRAIn, that re-introduces the RM as part of a distribution matching approach.BRAIn considers the LLM distribution conditioned on the assumption of output goodness and applies B
&lt;/p&gt;</description></item><item><title>DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02392</link><description>&lt;p&gt;
DeLLMa:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#20915;&#31574;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02392
&lt;/p&gt;
&lt;p&gt;
DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#12289;&#24037;&#31243;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#39046;&#22495;&#24448;&#24448;&#38754;&#20020;&#20915;&#31574;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#20915;&#31574;&#38382;&#39064;&#19978;&#30452;&#25509;&#20351;&#29992;LLMs&#24448;&#24448;&#25928;&#26524;&#36739;&#24046;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#39064;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeLLMa&#65288;Decision-making Large Language Model assistant&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#31934;&#24230;&#12290;DeLLMa&#21253;&#25324;&#19968;&#20010;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20102;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#20248;&#30340;&#12289;&#21487;&#23457;&#35745;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#30495;&#23454;&#20892;&#19994;&#21644;&#37329;&#34701;&#25968;&#25454;&#30340;&#20915;&#31574;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;DeLLMa&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLMs&#30340;&#20915;&#31574;&#24615;&#33021;&#65292;&#20934;&#30830;&#24615;&#21487;&#25552;&#39640;&#39640;&#36798;40%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over co
&lt;/p&gt;</description></item><item><title>OLMo&#26159;&#19968;&#31181;&#30495;&#27491;&#24320;&#25918;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#32473;&#30740;&#31350;&#31038;&#21306;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20197;&#20419;&#36827;&#23545;&#35821;&#35328;&#27169;&#22411;&#31185;&#23398;&#30340;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.00838</link><description>&lt;p&gt;
OLMo: &#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
OLMo: Accelerating the Science of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00838
&lt;/p&gt;
&lt;p&gt;
OLMo&#26159;&#19968;&#31181;&#30495;&#27491;&#24320;&#25918;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#32473;&#30740;&#31350;&#31038;&#21306;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20197;&#20419;&#36827;&#23545;&#35821;&#35328;&#27169;&#22411;&#31185;&#23398;&#30340;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#21644;&#21830;&#19994;&#20135;&#21697;&#12290;&#38543;&#30528;&#21830;&#19994;&#37325;&#35201;&#24615;&#30340;&#22686;&#21152;&#65292;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#24050;&#32463;&#23553;&#38381;&#36215;&#26469;&#65292;&#21482;&#33021;&#36890;&#36807;&#19987;&#26377;&#25509;&#21475;&#35775;&#38382;&#65292;&#20854;&#35757;&#32451;&#25968;&#25454;&#12289;&#26550;&#26500;&#21644;&#24320;&#21457;&#32454;&#33410;&#27809;&#26377;&#36879;&#38706;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#32454;&#33410;&#23545;&#20110;&#31185;&#23398;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#21253;&#25324;&#20854;&#20559;&#35265;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#25105;&#20204;&#35748;&#20026;&#30740;&#31350;&#31038;&#21306;&#26377;&#26435;&#35775;&#38382;&#24378;&#22823;&#32780;&#30495;&#27491;&#24320;&#25918;&#30340;LM&#12290;&#20026;&#27492;&#65292;&#26412;&#25216;&#26415;&#25253;&#21578;&#35814;&#32454;&#20171;&#32461;&#20102;OLMo&#30340;&#39318;&#20010;&#29256;&#26412;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#12289;&#30495;&#27491;&#24320;&#25918;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#26500;&#24314;&#21644;&#30740;&#31350;&#35821;&#35328;&#24314;&#27169;&#31185;&#23398;&#30340;&#26694;&#26550;&#12290;&#19982;&#20043;&#21069;&#21482;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#21644;&#25512;&#29702;&#20195;&#30721;&#30340;&#21162;&#21147;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#24067;OLMo&#21644;&#25972;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#30721;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#21457;&#24067;&#33021;&#22686;&#24378;&#24320;&#25918;&#30740;&#31350;&#31038;&#21306;&#30340;&#33021;&#21147;&#65292;&#24182;&#28608;&#21457;&#26356;&#22810;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36229;&#32423;&#36807;&#28388;&#8221;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#36739;&#23567;&#21644;&#36739;&#24369;&#30340;&#27169;&#22411;&#23545;&#29992;&#20110;&#35757;&#32451;&#36739;&#22823;&#21644;&#36739;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#25968;&#25454;&#36827;&#34892;&#36807;&#28388;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#36807;&#28388;&#25104;&#26412;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00530</link><description>&lt;p&gt;
&#36229;&#32423;&#36807;&#28388;&#65306;&#29992;&#20110;&#24555;&#36895;&#25351;&#20196;&#35843;&#25972;&#30340;&#24369;&#21040;&#24378;&#25968;&#25454;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00530
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36229;&#32423;&#36807;&#28388;&#8221;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#36739;&#23567;&#21644;&#36739;&#24369;&#30340;&#27169;&#22411;&#23545;&#29992;&#20110;&#35757;&#32451;&#36739;&#22823;&#21644;&#36739;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#25968;&#25454;&#36827;&#34892;&#36807;&#28388;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#36807;&#28388;&#25104;&#26412;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#23545;&#20110;&#25913;&#36827;LLM&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#24120;&#20250;&#36935;&#21040;&#20302;&#36136;&#37327;&#21644;&#20887;&#20313;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25351;&#20196;&#35843;&#25972;&#30340;&#25968;&#25454;&#36807;&#28388;&#22312;&#25552;&#39640;&#35843;&#25972;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#24456;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;LLMs&#22312;&#35813;&#36807;&#31243;&#20013;&#30340;&#21442;&#19982;&#65292;&#36825;&#20063;&#23548;&#33268;&#20102;&#39069;&#22806;&#30340;&#25104;&#26412;&#21644;&#35745;&#31639;&#12290;&#20026;&#20102;&#20943;&#23569;&#36807;&#28388;&#25104;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36229;&#32423;&#36807;&#28388;&#65306;&#21487;&#20197;&#20351;&#29992;&#36739;&#23567;&#19988;&#36739;&#24369;&#30340;&#27169;&#22411;&#26469;&#36873;&#25321;&#35201;&#35843;&#25972;&#26356;&#22823;&#21644;&#26356;&#24378;&#27169;&#22411;&#30340;&#25968;&#25454;&#21527;&#65311;&#23613;&#31649;&#24369;&#21644;&#24378;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#39640;&#24230;&#19968;&#33268;&#30340;&#33021;&#21147;&#21487;&#20197;&#24863;&#30693;&#25351;&#20196;&#30340;&#38590;&#24230;&#21644;&#25968;&#25454;&#36873;&#25321;&#32467;&#26524;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#26356;&#23567;&#26356;&#39640;&#25928;&#30340;&#27169;&#22411;&#26469;&#36807;&#28388;&#29992;&#20110;&#35757;&#32451;&#26356;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#25968;&#25454;&#12290;&#23427;&#19981;&#20165;&#22823;&#22823;&#21152;&#24555;&#20102;&#25968;&#25454;&#36807;&#28388;&#30340;&#36895;&#24230;&#65292;&#32780;&#19988;&#32463;&#36807;&#36807;&#28388;&#25968;&#25454;&#24494;&#35843;&#30340;LLM&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data. Data filtering for instruction tuning has proved important in improving both the efficiency and performance of the tuning process. But it also leads to extra cost and computation due to the involvement of LLMs in this process. To reduce the filtering cost, we study Superfiltering: Can we use a smaller and weaker model to select data for finetuning a larger and stronger model? Despite the performance gap between weak and strong language models, we find their highly consistent capability to perceive instruction difficulty and data selection results. This enables us to use a much smaller and more efficient model to filter the instruction data used to train a larger language model. Not only does it largely speed up the data filtering, but the filtered-data-finetuned LLM achieves even better performance on standard benchmarks. Extensive experiments validate the efficacy and efficiency of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;BPDec&#65288;BERT&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65289;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#22686;&#24378;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#35774;&#35745;&#21450;&#30740;&#31350;&#22312;BERT&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.15861</link><description>&lt;p&gt;
BPDec: &#25581;&#31034;BERT&#39044;&#35757;&#32451;&#20013;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
BPDec: Unveiling the Potential of Masked Language Modeling Decoder in BERT pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;BPDec&#65288;BERT&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65289;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#22686;&#24378;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#35774;&#35745;&#21450;&#30740;&#31350;&#22312;BERT&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BERT&#65288;&#26469;&#33258;Transformer&#30340;&#21452;&#21521;&#32534;&#30721;&#34920;&#31034;&#65289;&#36890;&#36807;&#20854;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#20986;&#33394;&#30340;&#24615;&#33021;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#38598;&#20013;&#22312;&#19982;&#27169;&#22411;&#32467;&#26500;&#30456;&#20851;&#30340;&#22686;&#24378;&#65292;&#20363;&#22914;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#21644;&#26356;&#26377;&#25928;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#36824;&#26377;&#19968;&#20123;&#20154;&#28145;&#20837;&#30740;&#31350;&#20102;&#19982;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#25216;&#24039;&#65292;&#21253;&#25324;&#25972;&#35789;&#25513;&#30721;&#12290;DeBERTa&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;BERT&#32534;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22686;&#24378;&#35299;&#30721;&#22120;&#65292;&#35777;&#26126;&#25928;&#26524;&#38750;&#24120;&#26174;&#33879;&#12290;&#25105;&#20204;&#35748;&#20026;&#22260;&#32469;&#22686;&#24378;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#30340;&#35774;&#35745;&#21644;&#30740;&#31350;&#24182;&#26410;&#24471;&#21040;&#24212;&#26377;&#30340;&#37325;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#35774;&#35745;&#65292;&#24182;&#20171;&#32461;&#20102;BPDec&#65288;BERT&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#24120;&#65292;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20250;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15861v2 Announce Type: replace-cross  Abstract: BERT (Bidirectional Encoder Representations from Transformers) has revolutionized the field of natural language processing through its exceptional performance on numerous tasks. Yet, the majority of researchers have mainly concentrated on enhancements related to the model structure, such as relative position embedding and more efficient attention mechanisms. Others have delved into pretraining tricks associated with Masked Language Modeling, including whole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's encoder model for pretraining, proving to be highly effective. We argue that the design and research around enhanced masked language modeling decoders have been underappreciated. In this paper, we propose several designs of enhanced decoders and introduce BPDec (BERT Pretraining Decoder), a novel method for modeling training. Typically, a pretrained BERT model is fine-tuned for specific Natural Language 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.08189</link><description>&lt;p&gt;
PRewrite: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
PRewrite: Prompt Rewriting with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#20197;&#8220;&#35797;&#38169;&#8221;&#30340;&#26041;&#24335;&#25163;&#21160;&#23436;&#25104;&#12290;&#36825;&#31181;&#25163;&#21160;&#31243;&#24207;&#21487;&#33021;&#32791;&#26102;&#65292;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#29983;&#25104;&#30340;&#25552;&#31034;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#21363;&#20351;&#23545;&#37027;&#20123;&#30475;&#20284;&#36816;&#20316;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#22987;&#32456;&#23384;&#22312;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36827;&#19968;&#27493;&#20462;&#25913;&#20351;&#25552;&#31034;&#21464;&#24471;&#26356;&#22909;&#21602;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#31034;&#24037;&#31243;&#33258;&#21160;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#20351;&#29992;&#24773;&#26223;&#65292;&#21363;&#24320;&#21457;&#32773;/&#29992;&#25143;&#24050;&#32463;&#36215;&#33609;&#20102;&#21021;&#22987;&#25552;&#31034;&#65292;&#20294;&#32570;&#20047;&#26102;&#38388;/&#19987;&#19994;&#30693;&#35782;&#26469;&#20248;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRewrite&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#21487;&#37325;&#20889;&#36825;&#20123;&#33609;&#26696;&#65292;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#12290;PRewrite&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#31471;&#21040;&#31471;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#20801;&#35768;RL&#25628;&#32034;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 Announce Type: replace  Abstract: Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated to
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.07453</link><description>&lt;p&gt;
&#35268;&#27169;&#21270;&#27169;&#22411;&#32534;&#36753;&#20250;&#23548;&#33268;&#28176;&#36827;&#24615;&#21644;&#31361;&#21457;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Model Editing at Scale leads to Gradual and Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07453
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#36753;&#30693;&#35782;&#26159;&#19968;&#31181;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#33021;&#21147;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#32416;&#27491;&#38169;&#35823;&#23398;&#20064;&#30340;&#20107;&#23454;&#65292;&#21516;&#26102;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#30340;&#26032;&#20107;&#23454;&#21015;&#34920;&#26356;&#26032;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#20351;&#27169;&#22411;&#32534;&#36753;&#20855;&#26377;&#23454;&#38469;&#25928;&#29992;&#65292;&#25105;&#20204;&#24517;&#39035;&#33021;&#22815;&#23545;&#21516;&#19968;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#32534;&#36753;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65306;ROME &#21644; MEMIT&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#19981;&#26029;&#22320;&#36951;&#24536;&#20808;&#21069;&#32534;&#36753;&#36807;&#30340;&#20107;&#23454;&#20197;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#36951;&#24536;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;--&#21021;&#22987;&#30340;&#28176;&#36827;&#24615;&#36951;&#24536;&#38454;&#27573;&#65292;&#38543;&#21518;&#26159;&#31361;&#28982;&#25110;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07453v2 Announce Type: replace-cross  Abstract: Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate the current model editing methods at scale, focusing on two state of the art methods: ROME and MEMIT. We find that as the model is edited sequentially with multiple facts, it continually forgets previously edited facts and the ability to perform downstream tasks. This forgetting happens in two phases -- an initial gradual but progressive forgetting phase followed by abrupt or catastrophic forgettin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#29702;&#35299;&#25968;&#23383;&#65292;&#21487;&#20197;&#36890;&#36807;&#21387;&#32553;&#21644;&#32534;&#30721;&#30340;&#26041;&#24335;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2401.03735</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#29702;&#35299;&#25968;&#23383;
&lt;/p&gt;
&lt;p&gt;
Language Models Understand Numbers, at Least Partially
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#29702;&#35299;&#25968;&#23383;&#65292;&#21487;&#20197;&#36890;&#36807;&#21387;&#32553;&#21644;&#32534;&#30721;&#30340;&#26041;&#24335;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#19981;&#36879;&#26126;&#30340;&#20869;&#37096;&#26426;&#21046;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#25968;&#23383;&#65292;&#25968;&#23398;&#20013;&#30340;&#22522;&#26412;&#20803;&#32032;&#12290;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;LLMs&#24212;&#35813;&#33021;&#22815;&#22312;&#20854;&#38544;&#34255;&#29366;&#24577;&#20013;&#21387;&#32553;&#25968;&#23383;&#20197;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#21152;&#27861;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#32447;&#24615;&#25506;&#27979;&#22120;&#20174;&#38544;&#34255;&#29366;&#24577;&#20013;&#35835;&#21462;&#36755;&#20837;&#25968;&#23383;&#12290;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;LLMs&#20013;&#23384;&#22312;&#21387;&#32553;&#30340;&#25968;&#23383;&#12290;&#28982;&#32780;&#65292;&#31934;&#30830;&#37325;&#24314;&#21407;&#22987;&#25968;&#23383;&#26159;&#22256;&#38590;&#30340;&#65292;&#34920;&#26126;&#21387;&#32553;&#36807;&#31243;&#21487;&#33021;&#19981;&#26159;&#26080;&#25439;&#30340;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#21033;&#29992;&#32534;&#30721;&#30340;&#25968;&#23383;&#26469;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#65292;&#24182;&#19988;&#35745;&#31639;&#33021;&#21147;&#38543;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#25968;&#23383;&#19978;&#23637;&#29616;&#20986;&#37096;&#20998;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited impressive competence in various tasks, but their opaque internal mechanisms hinder their use in mathematical problems. In this paper, we study a fundamental question: whether language models understand numbers, a basic element in math. Based on an assumption that LLMs should be capable of compressing numbers in their hidden states to solve mathematical problems, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states. Experimental results support the existence of compressed numbers in LLMs. However, it is difficult to precisely reconstruct the original numbers, indicating that the compression process may not be lossless. Further experiments show that LLMs can utilize encoded numbers to perform arithmetic computations, and the computational ability scales up with the model size. Our preliminary research suggests that LLMs exhibit a partial understanding of number
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26032;VQA&#25968;&#25454;&#38598;BloomVQA&#65292;&#22522;&#20110;Bloom&#30340;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#23618;&#27425;&#22270;&#34920;&#31034;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#19968;&#33268;&#24615;&#35780;&#20272;&#65292;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#32423;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2312.12716</link><description>&lt;p&gt;
BloomVQA&#65306;&#35780;&#20272;&#20998;&#23618;&#22810;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
BloomVQA: Assessing Hierarchical Multi-modal Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12716
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26032;VQA&#25968;&#25454;&#38598;BloomVQA&#65292;&#22522;&#20110;Bloom&#30340;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#23618;&#27425;&#22270;&#34920;&#31034;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#19968;&#33268;&#24615;&#35780;&#20272;&#65292;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#32423;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;VQA&#25968;&#25454;&#38598;BloomVQA&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#19982;&#24403;&#21069;&#30340;&#22522;&#20934;&#19981;&#21516;&#65292;&#23427;&#20204;&#36890;&#24120;&#20391;&#37325;&#20110;&#22522;&#20110;&#20107;&#23454;&#30340;&#35760;&#24518;&#21644;&#27809;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#31616;&#21333;&#25512;&#29702;&#20219;&#21153;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#22522;&#20110;&#22270;&#29255;&#25925;&#20107;&#30340;&#22810;&#39033;&#36873;&#25321;&#26679;&#26412;&#65292;&#21453;&#26144;&#20102;&#19981;&#21516;&#23618;&#27425;&#30340;&#29702;&#35299;&#65292;&#27491;&#22914;&#24067;&#40065;&#22982;&#30340;&#20998;&#31867;&#27861;&#25152;&#23637;&#31034;&#30340;&#65292;&#22312;&#25945;&#32946;&#30740;&#31350;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#32463;&#20856;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#26144;&#23556;&#21040;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#22270;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#21644;&#34920;&#24449;&#27169;&#22411;&#19968;&#33268;&#24615;&#30340;&#26032;&#25514;&#26045;&#12290;&#25105;&#20204;&#23545;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#32423;&#35780;&#20272;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#12290;&#19982;&#20302;&#32423;&#20219;&#21153;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#38656;&#35201;&#39640;&#32423;&#29702;&#35299;&#21644;&#35748;&#30693;&#33021;&#21147;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19979;&#38477;&#65292;VQA&#20934;&#30830;&#24615;&#19979;&#38477;&#20102;&#39640;&#36798;38.0%&#12290;&#19982;&#26089;&#26399;&#27169;&#22411;&#30456;&#27604;&#65292;GPT-4V&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12716v2 Announce Type: replace-cross  Abstract: We propose a novel VQA dataset, BloomVQA, to facilitate comprehensive evaluation of large vision-language models on comprehension tasks. Unlike current benchmarks that often focus on fact-based memorization and simple reasoning tasks without theoretical grounding, we collect multiple-choice samples based on picture stories that reflect different levels of comprehension, as laid out in Bloom's Taxonomy, a classic framework for learning assessment widely adopted in education research. Our data maps to a novel hierarchical graph representation which enables automatic data augmentation and novel measures characterizing model consistency. We perform graded evaluation and reliability analysis on recent multi-modal models. In comparison to low-level tasks, we observe decreased performance on tasks requiring advanced comprehension and cognitive skills with up to 38.0% drop in VQA accuracy. In comparison to earlier models, GPT-4V demons
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25490;&#38500;&#25512;&#29702;&#36807;&#31243;&#20013;&#36935;&#21040;&#22256;&#38590;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#38500;&#25512;&#29702;&#26041;&#27861;PoE&#19982;COT&#65292;&#21457;&#29616;&#27492;&#26041;&#27861;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#19981;&#22914;&#36873;&#25321;&#27491;&#30830;&#31572;&#26696;&#65292;&#24182;&#25351;&#20986;&#20102;&#30740;&#31350;&#20013;&#21457;&#29616;&#30340;&#19968;&#33268;&#24615;&#21644;&#38169;&#35823;&#20998;&#26512;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.07532</link><description>&lt;p&gt;
&#38169;&#35823;&#24182;&#19981;&#23481;&#26131;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25490;&#38500;&#25512;&#29702;&#36807;&#31243;&#20013;&#36935;&#21040;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07532
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25490;&#38500;&#25512;&#29702;&#36807;&#31243;&#20013;&#36935;&#21040;&#22256;&#38590;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#38500;&#25512;&#29702;&#26041;&#27861;PoE&#19982;COT&#65292;&#21457;&#29616;&#27492;&#26041;&#27861;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#19981;&#22914;&#36873;&#25321;&#27491;&#30830;&#31572;&#26696;&#65292;&#24182;&#25351;&#20986;&#20102;&#30740;&#31350;&#20013;&#21457;&#29616;&#30340;&#19968;&#33268;&#24615;&#21644;&#38169;&#35823;&#20998;&#26512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32500;&#65288;COT&#65289;&#25552;&#31034;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26397;&#30528;&#27491;&#30830;&#31572;&#26696;&#36827;&#34892;&#25512;&#29702;&#65292;&#20294;&#20854;&#22312;&#26397;&#30528;&#38169;&#35823;&#31572;&#26696;&#36827;&#34892;&#25512;&#29702;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#34987;&#25506;&#31350;&#12290;&#24403;&#19982;COT&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;&#36825;&#31181;&#25490;&#38500;&#25512;&#29702;&#65288;PoE&#65289;&#21487;&#20197;&#22686;&#24378;&#33258;&#25105;&#19968;&#33268;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#35832;&#22914;&#25490;&#38500;&#24615;&#21307;&#23398;&#35786;&#26029;&#31561;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#20013;&#36827;&#34892;PoE&#19982;COT&#30340;&#26041;&#27861;&#65292;LLMs&#24517;&#39035;&#26397;&#30528;&#19981;&#27491;&#30830;&#30340;&#36873;&#39033;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-3.5&#12289;LLaMA-2&#21644;Falcon&#22312;&#24635;&#20849;&#22235;&#20010;&#24120;&#35782;&#21644;&#31185;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#24102;&#26377;COT&#30340;PoE&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;PoE&#31574;&#30053;&#24635;&#26159;&#34920;&#29616;&#19981;&#22914;&#36873;&#25321;&#27491;&#30830;&#31572;&#26696;&#30340;&#31574;&#30053;&#12290;&#36825;&#20004;&#31181;&#31574;&#30053;&#30340;&#19968;&#33268;&#24615;&#20063;&#20302;&#20110;&#27599;&#31181;&#31574;&#30053;&#30340;&#33258;&#25105;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#38169;&#35823;&#20998;&#26512;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07532v2 Announce Type: replace  Abstract: Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE), when used with COT, can enhance self-consistency, interpretability, and tasks such as medical diagnoses of exclusion. Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct error analyses and give suggestions for future work.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#23383;&#27700;&#21360;&#31574;&#30053;&#65292;&#36890;&#36807;&#36741;&#21161;&#27169;&#22411;&#27979;&#37327;&#39640;&#29109;&#20196;&#29260;&#20998;&#24067;&#65292;&#23558;&#27700;&#21360;&#33258;&#36866;&#24212;&#22320;&#28155;&#21152;&#21040;&#20855;&#26377;&#39640;&#29109;&#30340;&#20196;&#29260;&#20998;&#24067;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#29109;&#20196;&#29260;&#20998;&#24067;&#19981;&#21464;&#65292;&#20197;&#25552;&#39640;&#25991;&#26412;&#36136;&#37327;&#21644;&#27700;&#21360;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#36866;&#24212;&#25991;&#23383;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Adaptive Text Watermark for Large Language Models. (arXiv:2401.13927v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13927
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#23383;&#27700;&#21360;&#31574;&#30053;&#65292;&#36890;&#36807;&#36741;&#21161;&#27169;&#22411;&#27979;&#37327;&#39640;&#29109;&#20196;&#29260;&#20998;&#24067;&#65292;&#23558;&#27700;&#21360;&#33258;&#36866;&#24212;&#22320;&#28155;&#21152;&#21040;&#20855;&#26377;&#39640;&#29109;&#30340;&#20196;&#29260;&#20998;&#24067;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#29109;&#20196;&#29260;&#20998;&#24067;&#19981;&#21464;&#65292;&#20197;&#25552;&#39640;&#25991;&#26412;&#36136;&#37327;&#21644;&#27700;&#21360;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#28389;&#29992;&#30340;&#25285;&#24551;&#65292;&#32780;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#23383;&#27700;&#21360;&#25104;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#20445;&#25345;&#27700;&#21360;&#24378;&#24230;&#12289;&#31283;&#20581;&#24615;&#21644;&#26080;&#38656;&#39044;&#20808;&#30693;&#36947;&#25552;&#31034;&#25110;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#27700;&#21360;&#30340;&#21516;&#26102;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24102;&#27700;&#21360;&#25991;&#26412;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#27700;&#21360;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#25991;&#26412;&#36136;&#37327;&#21644;&#20445;&#25345;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#26681;&#25454;&#36741;&#21161;&#27169;&#22411;&#27979;&#37327;&#30340;&#39640;&#29109;&#20196;&#29260;&#20998;&#24067;&#33258;&#36866;&#24212;&#22320;&#28155;&#21152;&#27700;&#21360;&#65292;&#32780;&#20445;&#25345;&#20302;&#29109;&#20196;&#29260;&#20998;&#24067;&#19981;&#21464;&#12290;&#20026;&#20102;&#20445;&#35777;&#23433;&#20840;&#24615;&#24182;&#36827;&#19968;&#27493;&#20943;&#23567;&#27700;&#21360;&#23545;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#19981;&#20877;&#20351;&#29992;&#20174;&#38543;&#26426;&#31192;&#38053;&#29983;&#25104;&#30340;&#22266;&#23450;&#32418;/&#32511;&#21517;&#21333;&#65292;&#32780;&#26159;&#26681;&#25454;&#21069;&#19968;&#20010;&#35821;&#20041;&#23884;&#20837;&#23558;&#36755;&#20986;&#23545;&#25968;&#27604;&#20363;&#36866;&#24212;&#24615;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Large Language Models (LLMs) has led to increasing concerns about the misuse of AI-generated text, and watermarking for LLM-generated text has emerged as a potential solution. However, it is challenging to generate high-quality watermarked text while maintaining strong security, robustness, and the ability to detect watermarks without prior knowledge of the prompt or model. This paper proposes an adaptive watermarking strategy to address this problem. To improve the text quality and maintain robustness, we adaptively add watermarking to token distributions with high entropy measured using an auxiliary model and keep the low entropy token distributions untouched. For the sake of security and to further minimize the watermark's impact on text quality, instead of using a fixed green/red list generated from a random secret key, which can be vulnerable to decryption and forgery, we adaptively scale up the output logits in proportion based on the semantic embedding of prev
&lt;/p&gt;</description></item><item><title>MLLMReID&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#24182;&#23558;&#20854;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;&#20027;&#24178;&#36827;&#34892;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;MLLM&#22312;ReID&#20219;&#21153;&#20013;&#30340;&#35774;&#35745;&#25351;&#20196;&#21644;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13201</link><description>&lt;p&gt;
MLLMReID: &#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MLLMReID: Multimodal Large Language Model-based Person Re-identification. (arXiv:2401.13201v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13201
&lt;/p&gt;
&lt;p&gt;
MLLMReID&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#24182;&#23558;&#20854;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;&#20027;&#24178;&#36827;&#34892;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;MLLM&#22312;ReID&#20219;&#21153;&#20013;&#30340;&#35774;&#35745;&#25351;&#20196;&#21644;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20154;&#29289;&#20877;&#35782;&#21035;&#65288;ReID&#65289;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#23558;&#30740;&#31350;&#22914;&#20309;&#23558;&#23427;&#20204;&#36866;&#24212;&#20110;ReID&#20219;&#21153;&#12290;&#19968;&#31181;&#30452;&#35266;&#30340;&#24819;&#27861;&#26159;&#20351;&#29992;ReID&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#23545;MLLM&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;ReID&#30340;&#20027;&#24178;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#20004;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65306;&#65288;1&#65289;&#20026;ReID&#35774;&#35745;&#25351;&#20196;&#26102;&#65292;MLLM&#21487;&#33021;&#36807;&#24230;&#25311;&#21512;&#29305;&#23450;&#25351;&#20196;&#65292;&#32780;&#35774;&#35745;&#21508;&#31181;&#25351;&#20196;&#23558;&#23548;&#33268;&#26356;&#39640;&#30340;&#25104;&#26412;&#12290;&#65288;2&#65289;LLM&#30340;&#28508;&#22312;&#22270;&#20687;&#29305;&#24449;&#21521;&#37327;&#27809;&#26377;&#21442;&#19982;&#25439;&#22833;&#35745;&#31639;&#12290;&#25351;&#20196;&#23398;&#20064;&#65292;&#23545;&#40784;&#22270;&#20687;-&#25991;&#26412;&#29305;&#24449;&#65292;&#23548;&#33268;&#38388;&#25509;&#20248;&#21270;&#21644;&#23398;&#20064;&#30446;&#26631;&#19981;&#20805;&#20998;&#21033;&#29992;&#29305;&#24449;&#65292;&#38480;&#21046;&#20102;&#20154;&#29289;&#29305;&#24449;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MLLMReID&#65306;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;ReID&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20844;&#20849;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models (MLLM) have achieved satisfactory results in many tasks. However, their performance in the task of person re-identification (ReID) has not been explored to date. This paper will investigate how to adapt them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and then use their visual encoder as a backbone for ReID. However, there still exist two apparent issues: (1) Designing instructions for ReID, MLLMs may overfit specific instructions, and designing a variety of instructions will lead to higher costs. (2) Latent image feature vectors from LLMs are not involved in loss computation. Instructional learning, aligning image-text features, results in indirect optimization and a learning objective that inadequately utilizes features, limiting effectiveness in person feature learning. To address these problems, this paper proposes MLLMReID: Multimodal Large Language Model-based ReID. Firstly, we proposed Common Instru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#29983;&#25104;&#29305;&#24449;&#30340;&#28176;&#36827;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#65292;&#36890;&#36807;&#39044;&#33976;&#39311;&#21644;&#21387;&#32553;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#24341;&#20837;&#36974;&#34109;&#29983;&#25104;&#30340;&#25945;&#24072;-&#23398;&#29983;&#29305;&#24449;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#24046;&#36317;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12997</link><description>&lt;p&gt;
&#22522;&#20110;&#36974;&#34109;&#29983;&#25104;&#29305;&#24449;&#26041;&#27861;&#30340;&#28176;&#36827;&#33976;&#39311;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Progressive Distillation Based on Masked Generation Feature Method for Knowledge Graph Completion. (arXiv:2401.12997v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#29983;&#25104;&#29305;&#24449;&#30340;&#28176;&#36827;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#65292;&#36890;&#36807;&#39044;&#33976;&#39311;&#21644;&#21387;&#32553;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#24341;&#20837;&#36974;&#34109;&#29983;&#25104;&#30340;&#25945;&#24072;-&#23398;&#29983;&#29305;&#24449;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#24046;&#36317;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (PLM) &#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840; (KGC) &#27169;&#22411;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;PLM &#27169;&#22411;&#30340;&#22823;&#37327;&#21442;&#25968;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#23545;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#29983;&#25104;&#29305;&#24449;&#30340;&#28176;&#36827;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110; KGC &#20219;&#21153;&#65292;&#26088;&#22312;&#26174;&#33879;&#38477;&#20302;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545; PLM &#36827;&#34892;&#39044;&#33976;&#39311;&#65292;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#21387;&#32553; PLM &#32593;&#32476;&#24471;&#21040;&#22810;&#31561;&#32423;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#29305;&#24449;&#33976;&#39311;&#22312;&#25945;&#24072;&#27169;&#22411;&#20013;&#21482;&#26377;&#21333;&#19968;&#20449;&#24687;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25945;&#24072;-&#23398;&#29983;&#29305;&#24449;&#30340;&#36974;&#34109;&#29983;&#25104;&#65292;&#20854;&#20013;&#21253;&#21547;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#34920;&#31034;&#33021;&#21147;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#30340;&#33976;&#39311;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, knowledge graph completion (KGC) models based on pre-trained language model (PLM) have shown promising results. However, the large number of parameters and high computational cost of PLM models pose challenges for their application in downstream tasks. This paper proposes a progressive distillation method based on masked generation features for KGC task, aiming to significantly reduce the complexity of pre-trained models. Specifically, we perform pre-distillation on PLM to obtain high-quality teacher models, and compress the PLM network to obtain multi-grade student models. However, traditional feature distillation suffers from the limitation of having a single representation of information in teacher models. To solve this problem, we propose masked generation of teacher-student features, which contain richer representation information. Furthermore, there is a significant gap in representation ability between teacher and student. Therefore, we design a progressive dist
&lt;/p&gt;</description></item><item><title>DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.10471</link><description>&lt;p&gt;
DeepEdit: &#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#24335;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10471
&lt;/p&gt;
&lt;p&gt;
DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#35270;&#20026;&#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeepEdit&#65288;&#22522;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#28176;&#36827;&#24335;&#35299;&#30721;&#30693;&#35782;&#32534;&#36753;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#12289;&#38382;&#39064;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#26469;&#25913;&#36827;&#30693;&#35782;&#32534;&#36753;&#12290;DeepEdit&#21487;&#28789;&#27963;&#24212;&#29992;&#20110;&#25152;&#26377;&#40657;&#30418;LLMs&#65306;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#34920;&#31034;&#25110;&#36755;&#20986;&#35789;&#27719;&#20998;&#24067;&#12290;DeepEdit&#36880;&#27493;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#23427;&#21033;&#29992;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#26469;&#20462;&#25913;LLMs&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#36755;&#20986;&#23545;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#12290;&#22312;&#30693;&#35782;&#32534;&#36753;&#26041;&#38754;&#65292;DeepEdit&#22312;&#25511;&#21046;LLMs&#20135;&#29983;&#26356;&#31616;&#27905;&#30340;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;MQuaKE&#19978;&#65292;DeepEdit&#22312;&#23450;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#29616;&#20195;&#24230;&#37327;&#26631;&#20934;&#30340;&#8220;&#21160;&#24577;&#33539;&#22260;&#8221;&#65292;&#20197;&#25552;&#20379;&#23545;&#24230;&#37327;&#26631;&#20934;&#20043;&#38388;&#21644;&#20869;&#37096;&#24471;&#20998;&#24046;&#24322;&#30340;&#20849;&#21516;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#24863;&#30693;&#27700;&#24179;&#30340;&#31995;&#32479;&#24046;&#24322;&#36827;&#34892;&#34913;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.06760</link><description>&lt;p&gt;
&#35299;&#20915;&#24230;&#37327;&#25968;&#20540;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#36855;&#23467;&#65306;&#23548;&#33322;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies. (arXiv:2401.06760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#29616;&#20195;&#24230;&#37327;&#26631;&#20934;&#30340;&#8220;&#21160;&#24577;&#33539;&#22260;&#8221;&#65292;&#20197;&#25552;&#20379;&#23545;&#24230;&#37327;&#26631;&#20934;&#20043;&#38388;&#21644;&#20869;&#37096;&#24471;&#20998;&#24046;&#24322;&#30340;&#20849;&#21516;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#24863;&#30693;&#27700;&#24179;&#30340;&#31995;&#32479;&#24046;&#24322;&#36827;&#34892;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21313;&#24180;&#21069;&#65292;&#26426;&#22120;&#32763;&#35793;&#30740;&#31350;&#20013;&#26377;&#19968;&#20010;&#21333;&#19968;&#30340;&#24230;&#37327;&#26631;&#20934;BLEU&#12290;&#22914;&#20170;&#65292;&#27809;&#26377;&#36825;&#26679;&#30340;&#20849;&#35782;&#65292;&#22240;&#27492;&#30740;&#31350;&#20154;&#21592;&#24456;&#38590;&#21457;&#23637;&#21644;&#20445;&#25345;&#20043;&#21069;&#25512;&#21160;&#30740;&#31350;&#21644;&#37096;&#32626;&#20915;&#31574;&#30340;&#37027;&#31181;&#21551;&#21457;&#24615;&#30452;&#35273;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#29616;&#20195;&#24230;&#37327;&#26631;&#20934;&#30340;&#8220;&#21160;&#24577;&#33539;&#22260;&#8221;&#65292;&#20197;&#25552;&#20379;&#23545;&#24230;&#37327;&#26631;&#20934;&#20043;&#38388;&#21644;&#20869;&#37096;&#24471;&#20998;&#24046;&#24322;&#30340;&#20849;&#21516;&#29702;&#35299;&#65307;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#35201;&#38382;&#22312;&#24230;&#37327;&#26631;&#20934;Y&#20013;&#65292;&#20004;&#20010;&#31995;&#32479;&#38656;&#35201;&#26377;&#22810;&#22823;&#30340;&#24471;&#20998;&#24046;&#24322;X&#65292;&#20154;&#31867;&#25165;&#33021;&#27880;&#24847;&#21040;&#65311;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;ToShip23&#36827;&#34892;&#35780;&#20272;&#65292;&#20351;&#29992;&#23427;&#21457;&#29616;&#24230;&#37327;&#26631;&#20934;&#36798;&#21040;&#20154;&#31867;&#24863;&#30693;&#27700;&#24179;&#30340;&#31995;&#32479;&#24046;&#24322;&#65292;&#25105;&#20204;&#36890;&#36807;&#25104;&#23545;&#31995;&#32479;&#20934;&#30830;&#24615;&#26469;&#34913;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#19982;&#26631;&#20934;&#30340;&#32479;&#35745;p&#20540;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#31283;&#23450;&#24615;&#30456;&#27604;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#24314;&#31435;&#24046;&#24322;&#20934;&#30830;&#24615;&#26356;&#20026;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ten years ago a single metric, BLEU, governed progress in machine translation research. For better or worse, there is no such consensus today, and consequently it is difficult for researchers to develop and retain the kinds of heuristic intuitions about metric deltas that drove earlier research and deployment decisions. This paper investigates the "dynamic range" of a number of modern metrics in an effort to provide a collective understanding of the meaning of differences in scores both within and among metrics; in other words, we ask what point difference X in metric Y is required between two systems for humans to notice? We conduct our evaluation on a new large dataset, ToShip23, using it to discover deltas at which metrics achieve system-level differences that are meaningful to humans, which we measure by pairwise system accuracy. We additionally show that this method of establishing delta-accuracy is more stable than the standard use of statistical p-values in regards to testset si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36807;&#20102;GPT-4&#30340;&#32763;&#35793;&#24615;&#33021;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#31163;&#26631;&#32763;&#35793;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2401.06468</link><description>&lt;p&gt;
&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adapting Large Language Models for Document-Level Machine Translation. (arXiv:2401.06468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36807;&#20102;GPT-4&#30340;&#32763;&#35793;&#24615;&#33021;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#31163;&#26631;&#32763;&#35793;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#20043;&#21518;&#65292;&#20013;&#31561;&#35268;&#27169;&#30340;LLMs&#24448;&#24448;&#32988;&#36807;&#20854;&#26356;&#22823;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#23558;LLMs&#35843;&#25972;&#20026;&#29305;&#23450;&#35821;&#35328;&#23545;&#30340;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#65288;DocMT&#65289;&#30340;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25552;&#31034;&#31574;&#30053;&#23545;&#19979;&#28216;&#32763;&#35793;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#24494;&#35843;&#26041;&#27861;&#12289;&#19977;&#31181;LLM&#20027;&#24178;&#21644;18&#20010;&#28041;&#21450;&#20061;&#31181;&#35821;&#35328;&#23545;&#30340;&#32763;&#35793;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#19987;&#38376;&#30340;&#27169;&#22411;&#29978;&#33267;&#22312;&#32763;&#35793;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;GPT-4&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#23427;&#20204;&#19987;&#38376;&#22312;&#21452;&#35821;&#24179;&#34892;&#25991;&#26723;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20173;&#28982;&#26126;&#26174;&#23384;&#22312;&#31163;&#26631;&#32763;&#35793;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#38024;&#23545;DocMT&#37327;&#36523;&#23450;&#21046;&#30340;LLMs&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22914;&#32763;&#35793;&#20934;&#30830;&#24230;&#25913;&#21892;&#12289;&#22810;&#28304;&#20449;&#24687;&#25972;&#21512;&#31561;&#21508;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant strides in various natural language processing (NLP) tasks. Recent research shows that the moderately-sized LLMs often outperform their larger counterparts after task-specific fine-tuning. In this work, we delve into the process of adapting LLMs to specialize in document-level machine translation (DocMT) for a specific language pair. Firstly, we explore how prompt strategies affect downstream translation performance. Then, we conduct extensive experiments with two fine-tuning methods, three LLM backbones, and 18 translation tasks across nine language pairs. Our findings indicate that in some cases, these specialized models even surpass GPT-4 in translation performance, while they still significantly suffer from the off-target translation issue in others, even if they are exclusively fine-tuned on bilingual parallel documents. Furthermore, we provide an in-depth analysis of these LLMs tailored for DocMT, exploring aspects such as transl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.06118</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#36890;&#36807;&#21152;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Extreme Compression of Large Language Models via Additive Quantization. (arXiv:2401.06118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#25216;&#26415;&#30340;&#31454;&#36187;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#22312;&#26368;&#32456;&#29992;&#25143;&#35774;&#22791;&#19978;&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22810;&#30721;&#26412;&#37327;&#21270;(MCQ)&#30340;&#32463;&#20856;&#26041;&#27861;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;&#8220;&#26497;&#31471;&#8221;LLM&#21387;&#32553;&#30340;&#38382;&#39064;&#65292;&#21363;&#38024;&#23545;&#38750;&#24120;&#20302;&#30340;&#20301;&#25968;&#65292;&#20363;&#22914;&#27599;&#20010;&#21442;&#25968;2&#21040;3&#20301;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#21152;&#24615;&#37327;&#21270;&#36825;&#19968;&#32463;&#20856;&#31639;&#27861;&#20043;&#19978;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#31639;&#27861;&#22312;LLM&#21387;&#32553;&#26041;&#38754;&#25512;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#65292;&#20197;&#32473;&#23450;&#21387;&#32553;&#39044;&#31639;&#30340;&#20934;&#30830;&#24615;&#32780;&#35328;&#65292;&#20248;&#20110;&#25152;&#26377;&#26368;&#36817;&#25552;&#20986;&#30340;&#25216;&#26415;&#12290;&#20363;&#22914;&#65292;&#24403;&#23558;Llama 2&#27169;&#22411;&#21387;&#32553;&#21040;&#27599;&#20010;&#21442;&#25968;2&#20301;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;7B&#27169;&#22411;&#37327;&#21270;&#20026;6.93&#22256;&#24785;&#24230;(&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20339;&#24037;&#20316;&#25913;&#36827;1.29&#65292;&#30456;&#23545;&#20110;FP16&#25913;&#36827;1.81)&#65292;13B&#27169;&#22411;&#37327;&#21270;&#20026;5.70&#22256;&#24785;&#24230;(&#25913;&#36827;0.36)&#65292;70B&#27169;&#22411;&#37327;&#21270;&#20026;3.94&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices. In this paper, we revisit the problem of "extreme" LLM compression--defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter, from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our work builds on top of Additive Quantization, a classic algorithm from the MCQ family, and adapts it to the quantization of language models. The resulting algorithm advances the state-of-the-art in LLM compression, outperforming all recently-proposed techniques in terms of accuracy at a given compression budget. For instance, when compressing Llama 2 models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93 perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B model to 3.94 
&lt;/p&gt;</description></item><item><title>LinguAlchemy&#26159;&#19968;&#31181;&#23558;&#35821;&#35328;&#31867;&#22411;&#23398;&#21644;&#22320;&#29702;&#20803;&#32032;&#34701;&#21512;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#26410;&#35265;&#35821;&#35328;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06034</link><description>&lt;p&gt;
LinguAlchemy: &#23558;&#35821;&#35328;&#31867;&#22411;&#23398;&#21644;&#22320;&#29702;&#20803;&#32032;&#34701;&#21512;&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization. (arXiv:2401.06034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06034
&lt;/p&gt;
&lt;p&gt;
LinguAlchemy&#26159;&#19968;&#31181;&#23558;&#35821;&#35328;&#31867;&#22411;&#23398;&#21644;&#22320;&#29702;&#20803;&#32032;&#34701;&#21512;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#26410;&#35265;&#35821;&#35328;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35821;&#35328;&#19978;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#65292;PLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#23548;&#33268;&#35821;&#35328;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#65292;&#29978;&#33267;&#29983;&#25104;&#30340;&#22238;&#24212;&#19982;&#38543;&#26426;&#22522;&#20934;&#30456;&#24403;&#33618;&#21776;&#12290;&#36825;&#19968;&#38480;&#21046;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;PLMs&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#30340;&#22810;&#26679;&#24615;&#21644;&#24179;&#31561;&#33719;&#21462;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;LinguAlchemy&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23558;&#35821;&#35328;&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#31867;&#22411;&#23398;&#12289;&#22320;&#29702;&#21644;&#35821;&#31995;&#65289;&#32435;&#20837;PLMs&#30340;&#34920;&#31034;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#24449;&#30456;&#24212;&#30340;&#35821;&#35328;&#32422;&#26463;&#12290;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;LinguAlchemy&#26174;&#33879;&#25552;&#39640;&#20102;mBERT&#21644;XLM-R&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#20934;&#30830;&#24615;&#32489;&#25928;&#65292;&#20998;&#21035;&#25552;&#39640;&#20102;&#32422;18%&#21644;&#32422;2%&#65292;&#23637;&#29616;&#20986;&#36739;&#39640;&#30340;&#26410;&#35265;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) have shown remarkable generalization toward multiple tasks and languages. Nonetheless, the generalization of PLMs towards unseen languages is poor, resulting in significantly worse language performance, or even generating nonsensical responses that are comparable to a random baseline. This limitation has been a longstanding problem of PLMs raising the problem of diversity and equal access to language modeling technology. In this work, we solve this limitation by introducing LinguAlchemy, a regularization technique that incorporates various aspects of languages covering typological, geographical, and phylogenetic constraining the resulting representation of PLMs to better characterize the corresponding linguistics constraints. LinguAlchemy significantly improves the accuracy performance of mBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared to fully finetuned models and displaying a high degree of unseen language generalization. W
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#65292;&#25105;&#20204;&#23450;&#20301;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#25214;&#21040;&#20102;&#23384;&#20648;&#20102;&#26377;&#20851;&#8220;&#27861;&#22269;&#65292;&#39318;&#37117;&#65292;&#24052;&#40654;&#8221;&#30340;&#30693;&#35782;&#30340;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2312.12141</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23450;&#20301;&#20107;&#23454;&#30693;&#35782;&#65306;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space. (arXiv:2312.12141v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12141
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#65292;&#25105;&#20204;&#23450;&#20301;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#25214;&#21040;&#20102;&#23384;&#20648;&#20102;&#26377;&#20851;&#8220;&#27861;&#22269;&#65292;&#39318;&#37117;&#65292;&#24052;&#40654;&#8221;&#30340;&#30693;&#35782;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#26102;&#65292;&#23376;&#20540;&#20855;&#26377;&#21487;&#20154;&#31867;&#35299;&#37322;&#30340;&#27010;&#24565;&#30340;&#21407;&#22240;&#12290;&#23376;&#20540;&#30340;softmax&#20043;&#21069;&#30340;&#20540;&#36890;&#36807;&#19968;&#20010;&#21152;&#27861;&#20989;&#25968;&#30456;&#21152;&#65292;&#22240;&#27492;&#35789;&#27719;&#31354;&#38388;&#20013;&#21069;&#20960;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#20250;&#22686;&#21152;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#23545;&#25968;&#27010;&#29575;&#22686;&#21152;&#26469;&#35745;&#31639;&#23618;&#21644;&#23376;&#20540;&#30340;&#37325;&#35201;&#24615;&#27604;&#27010;&#29575;&#22686;&#21152;&#26356;&#22909;&#65292;&#22240;&#20026;&#23545;&#25968;&#27010;&#29575;&#22686;&#21152;&#30340;&#26354;&#32447;&#21576;&#32447;&#24615;&#21333;&#35843;&#22686;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35745;&#31639;&#20869;&#31215;&#26469;&#35780;&#20272;&#21069;&#39304;&#32593;&#32476;&#65288;FFN&#65289;&#30340;&#23376;&#20540;&#34987;&#21069;&#38754;&#30340;&#23618;&#28608;&#27963;&#30340;&#31243;&#24230;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#20107;&#23454;&#30693;&#35782;&#8220;&#27861;&#22269;&#65292;&#39318;&#37117;&#65292;&#24052;&#40654;&#8221;&#23384;&#20648;&#30340;&#20301;&#32622;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#27880;&#24847;&#21147;&#23618;&#23384;&#20648;&#8220;&#24052;&#40654;&#19982;&#27861;&#22269;&#30456;&#20851;&#8221;&#12290;FFN&#23618;&#23384;&#20648;&#8220;&#24052;&#40654;&#26159;&#19968;&#20010;&#39318;&#37117;/&#22478;&#24066;&#8221;&#65292;&#30001;&#27880;&#24847;&#21147;&#23376;&#20540;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
We find the location of factual knowledge in large language models by exploring the residual stream and analyzing subvalues in vocabulary space. We find the reason why subvalues have human-interpretable concepts when projecting into vocabulary space. The before-softmax values of subvalues are added by an addition function, thus the probability of top tokens in vocabulary space will increase. Based on this, we find using log probability increase to compute the significance of layers and subvalues is better than probability increase, since the curve of log probability increase has a linear monotonically increasing shape. Moreover, we calculate the inner products to evaluate how much a feed-forward network (FFN) subvalue is activated by previous layers. Base on our methods, we find where factual knowledge &lt;France, capital, Paris&gt; is stored. Specifically, attention layers store "Paris is related to France". FFN layers store "Paris is a capital/city", activated by attention subvalues relate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#25915;&#20987;&#24615;&#20869;&#23481;&#25913;&#20889;&#26041;&#38754;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#36755;&#20837;-&#26631;&#31614;&#28436;&#31034;&#23545;&#26469;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#26597;&#35810;&#30340;&#25152;&#38656;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#21487;&#29992;&#24615;&#21644;&#20943;&#23569;&#25915;&#20987;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10707</link><description>&lt;p&gt;
&#28436;&#31034;&#23601;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999;&#65306;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25512;&#36827;&#25915;&#20987;&#24615;&#20869;&#23481;&#25913;&#20889;
&lt;/p&gt;
&lt;p&gt;
Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning. (arXiv:2310.10707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10707
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#25915;&#20987;&#24615;&#20869;&#23481;&#25913;&#20889;&#26041;&#38754;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#36755;&#20837;-&#26631;&#31614;&#28436;&#31034;&#23545;&#26469;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#26597;&#35810;&#30340;&#25152;&#38656;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#21487;&#29992;&#24615;&#21644;&#20943;&#23569;&#25915;&#20987;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#20889;&#25915;&#20987;&#24615;&#20869;&#23481;&#26159;&#19968;&#31181;&#26356;&#22909;&#30340;&#26367;&#20195;&#20869;&#23481;&#21024;&#38500;&#30340;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;&#27807;&#36890;&#29615;&#22659;&#30340;&#25991;&#26126;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#24335;&#30340;&#25913;&#20889;&#22120;&#22312;&#20445;&#30041;&#24847;&#20041;&#21644;&#24847;&#22270;&#30340;&#21516;&#26102;&#65292;&#23545;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#20381;&#36182;&#24615;&#36739;&#39640;&#12290;&#23427;&#20204;&#20063;&#20445;&#30041;&#20102;&#21407;&#22987;&#20869;&#23481;&#30340;&#22823;&#37096;&#20998;&#25915;&#20987;&#24615;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#25972;&#20307;&#21487;&#29992;&#24615;&#30340;&#30097;&#38382;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#20174;&#19994;&#32773;&#24320;&#21457;&#21487;&#29992;&#30340;&#25913;&#20889;&#22120;&#65292;&#21363;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#36755;&#20837;-&#26631;&#31614;&#28436;&#31034;&#23545;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#26597;&#35810;&#30340;&#25152;&#38656;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20851;&#38190;&#22240;&#32032;&#65292;&#22914;&#28436;&#31034;&#30340;&#25968;&#37327;&#21644;&#39034;&#24207;&#65292;&#25490;&#38500;&#25552;&#31034;&#25351;&#20196;&#65292;&#20197;&#21450;&#38477;&#20302;&#27979;&#37327;&#27602;&#24615;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#25105;&#20204;&#25552;&#20986;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#31036;&#35980;&#25913;&#20889;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#21407;&#21017;&#24615;&#35780;&#20272;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#35805;&#24335;&#30340;&#31895;&#40065;&#21457;&#35328;&#12289;&#31036;&#35980;&#25913;&#20889;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. Supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. They also retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. In this paper we aim to assist practitioners in developing usable paraphrasers by exploring In-Context Learning (ICL) with large language models (LLMs), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. Our study focuses on key factors such as -- number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. We perform principled evaluation on three datasets, including our proposed Context-Aware Polite Paraphrase dataset, comprising of dialogue-style rude utterances, polite paraphr
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07177</link><description>&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07177
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#26469;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#22810;&#26679;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#33609;&#31295;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#33021;&#21147;&#24046;&#36317;&#26102;&#65292;&#20854;&#26377;&#25928;&#24615;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#65288;OSD&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#20016;&#23500;&#30340;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#29992;&#25143;&#26597;&#35810;&#25968;&#25454;&#25345;&#32493;&#26356;&#26032;&#65288;&#22810;&#20010;&#65289;&#33609;&#31295;&#27169;&#22411;&#12290;&#30001;&#20110;LLM&#25512;&#29702;&#21463;&#20869;&#23384;&#38480;&#21046;&#65292;&#20856;&#22411;&#30340;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#30340;&#21097;&#20313;&#35745;&#31639;&#33021;&#21147;&#21487;&#20197;&#29992;&#20110;&#22312;&#32447;&#37325;&#26032;&#35757;&#32451;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#25104;&#26412;&#20445;&#25345;&#20013;&#24615;&#12290;&#30001;&#20110;LLM&#26381;&#21153;&#30340;&#26597;&#35810;&#20998;&#24067;&#30456;&#23545;&#31616;&#21333;&#65292;&#26681;&#25454;&#26597;&#35810;&#20998;&#24067;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#20351;&#33609;&#31295;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#25915;&#20987;&#26080;&#20851;&#38450;&#24481;&#31574;&#30053;AdvFooler&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#36755;&#20837;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#22256;&#24785;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#20174;&#32780;&#36855;&#24785;&#25991;&#26412;&#24858;&#24324;&#32773;&#12290;</title><link>http://arxiv.org/abs/2310.01452</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#21270;&#28508;&#22312;&#34920;&#31034;&#26469;&#36855;&#24785;&#25991;&#26412;&#24858;&#24324;&#32773;
&lt;/p&gt;
&lt;p&gt;
Fooling the Textual Fooler via Randomizing Latent Representations. (arXiv:2310.01452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#25915;&#20987;&#26080;&#20851;&#38450;&#24481;&#31574;&#30053;AdvFooler&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#36755;&#20837;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#22256;&#24785;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#20174;&#32780;&#36855;&#24785;&#25991;&#26412;&#24858;&#24324;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#24494;&#23567;&#22320;&#25913;&#21464;&#36755;&#20837;&#20197;&#23548;&#33268;&#27169;&#22411;&#30340;&#38169;&#35823;&#34892;&#20026;&#12290;&#20854;&#20013;&#65292;&#25932;&#23545;&#35789;&#32423;&#25200;&#21160;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#21644;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#36825;&#20123;&#25915;&#20987;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#36215;&#20316;&#29992;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#32467;&#26500;&#25110;&#21442;&#25968;&#65292;&#22240;&#27492;&#21487;&#33021;&#23545;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#34892;&#25915;&#20987;&#65292;&#23545;&#25163;&#22810;&#27425;&#26597;&#35810;&#21463;&#23475;&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#36755;&#20837;&#25991;&#26412;&#20013;&#26368;&#37325;&#35201;&#30340;&#21333;&#35789;&#65292;&#24182;&#29992;&#23427;&#20204;&#23545;&#24212;&#30340;&#21516;&#20041;&#35789;&#26367;&#25442;&#36825;&#20123;&#21333;&#35789;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#21644;&#25915;&#20987;&#26080;&#20851;&#30340;&#38450;&#24481;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#22256;&#24785;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#20013;&#20135;&#29983;&#25932;&#23545;&#31034;&#20363;&#30340;&#36807;&#31243;&#65307;&#21363;&#24858;&#24324;&#25991;&#26412;&#24858;&#24324;&#32773;&#12290;&#36825;&#31181;&#38450;&#24481;&#21517;&#20026;AdvFooler&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#36755;&#20837;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite outstanding performance in a variety of NLP tasks, recent studies have revealed that NLP models are vulnerable to adversarial attacks that slightly perturb the input to cause the models to misbehave. Among these attacks, adversarial word-level perturbations are well-studied and effective attack strategies. Since these attacks work in black-box settings, they do not require access to the model architecture or model parameters and thus can be detrimental to existing NLP applications. To perform an attack, the adversary queries the victim model many times to determine the most important words in an input text and to replace these words with their corresponding synonyms. In this work, we propose a lightweight and attack-agnostic defense whose main goal is to perplex the process of generating an adversarial example in these query-based black-box attacks; that is to fool the textual fooler. This defense, named AdvFooler, works by randomizing the latent representation of the input at 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21487;&#35780;&#20998;&#30340;&#35848;&#21028;&#28216;&#25103;&#20316;&#20026;LLMs&#30340;&#26032;&#35780;&#20272;&#26694;&#26550;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#38646;-shot&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#23637;&#31034;&#20102;&#20195;&#29702;&#20154;&#21487;&#20197;&#25104;&#21151;&#35848;&#21028;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;GPT-4&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.17234</link><description>&lt;p&gt;
LLM-&#36777;&#35770;: &#20351;&#29992;&#20132;&#20114;&#24335;&#22810;&#26234;&#33021;&#20307;&#21327;&#21830;&#28216;&#25103;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. (arXiv:2309.17234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21487;&#35780;&#20998;&#30340;&#35848;&#21028;&#28216;&#25103;&#20316;&#20026;LLMs&#30340;&#26032;&#35780;&#20272;&#26694;&#26550;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#38646;-shot&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#23637;&#31034;&#20102;&#20195;&#29702;&#20154;&#21487;&#20197;&#25104;&#21151;&#35848;&#21028;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;GPT-4&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20195;&#29702;&#20154;&#26469;&#35299;&#20915;&#21487;&#33021;&#38656;&#35201;&#35780;&#20272;&#22797;&#26434;&#24773;&#20917;&#30340;&#29616;&#23454;&#20219;&#21153;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#26377;&#38480;&#30340;&#29702;&#35299;&#65292;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#30001;&#20110;&#35848;&#21028;&#21644;&#22949;&#21327;&#26159;&#25105;&#20204;&#26085;&#24120;&#27807;&#36890;&#21644;&#21512;&#20316;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#35780;&#20998;&#30340;&#35848;&#21028;&#28216;&#25103;&#20316;&#20026;LLMs&#30340;&#26032;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#12289;&#22810;&#26234;&#33021;&#20307;&#30340;&#12289;&#22810;&#38382;&#39064;&#30340;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#35848;&#21028;&#28216;&#25103;&#27979;&#35797;&#24179;&#21488;&#65292;&#38590;&#24230;&#21487;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20195;&#29702;&#20154;&#38656;&#35201;&#20855;&#22791;&#24378;&#22823;&#30340;&#31639;&#26415;&#12289;&#25512;&#29702;&#12289;&#25506;&#32034;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#21516;&#26102;&#26080;&#32541;&#22320;&#25972;&#21512;&#23427;&#20204;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#38646;-shot&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#29702;&#20154;&#21487;&#20197;&#36827;&#34892;&#35848;&#21028;&#24182;&#25345;&#32493;&#36798;&#25104;&#25104;&#21151;&#20132;&#26131;&#12290;&#25105;&#20204;&#29992;&#22810;&#20010;&#25351;&#26631;&#37327;&#21270;&#24615;&#33021;&#65292;&#24182;&#35266;&#23519;&#21040;GPT-4&#19982;&#21407;&#25991;&#20043;&#38388;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs' reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20105;&#35770;&#24615;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#20105;&#35770;&#24615;&#25512;&#29702;&#24615;&#33021;&#19982;&#36755;&#20837;&#21644;&#36755;&#20986;&#34920;&#31034;&#23494;&#20999;&#30456;&#20851;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#8220;&#20856;&#22411;&#25928;&#24212;&#8221;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#19979;&#35299;&#20915;&#30149;&#24577;&#38382;&#39064;&#26102;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.16938</link><description>&lt;p&gt;
&#25105;&#24076;&#26395;&#20105;&#35770;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20105;&#35770;&#24615;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
I Wish to Have an Argument: Argumentative Reasoning in Large Language Models. (arXiv:2309.16938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20105;&#35770;&#24615;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#20105;&#35770;&#24615;&#25512;&#29702;&#24615;&#33021;&#19982;&#36755;&#20837;&#21644;&#36755;&#20986;&#34920;&#31034;&#23494;&#20999;&#30456;&#20851;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#8220;&#20856;&#22411;&#25928;&#24212;&#8221;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#19979;&#35299;&#20915;&#30149;&#24577;&#38382;&#39064;&#26102;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20105;&#35770;&#24615;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#23454;&#39564;&#26694;&#26550;&#23450;&#20026;&#20105;&#35770;&#25366;&#25496;&#65288;AM&#65289;&#21644;&#20105;&#35770;&#23545;&#25552;&#21462;&#65288;APE&#65289;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#34920;&#31034;&#30340;&#25277;&#35937;&#32423;&#21035;&#19978;&#25191;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#65288;&#20363;&#22914;&#65292;&#20219;&#24847;&#26631;&#31614;&#38598;&#65292;&#35821;&#20041;&#22270;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649; LLMs &#22312; AM &#21644; APE &#19978;&#33021;&#22815;&#19982;&#25110;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#20294;&#23427;&#20204;&#30340;&#20105;&#35770;&#24615;&#25512;&#29702;&#24615;&#33021;&#38750;&#24120;&#20381;&#36182;&#20110;&#36755;&#20837;&#21644;&#36755;&#20986;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#20010;&#8220;&#20856;&#22411;&#25928;&#24212;&#8221;&#65292;&#36807;&#22810;&#30340;&#20856;&#22411;&#23454;&#20363;&#20250;&#23545;&#20219;&#21153;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#26368;&#20339;&#25968;&#37327;&#32422;&#20026;4-5&#20010;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#24182;&#19981;&#36866;&#29992;&#20110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#65306;&#25105;&#20204;&#21457;&#29616;&#20856;&#22411;&#25928;&#24212;&#34987;&#25269;&#28040;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#22312;&#30149;&#24577;&#38382;&#39064;&#19979;&#65292;CoT &#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;&#25152;&#25253;&#21578;&#30340;&#24037;&#20316;&#33021;&#22815;&#20419;&#36827;&#20105;&#35770;&#24615;&#25512;&#29702;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluate the ability of contemporary large language models (LLMs) to perform argumentative reasoning. We frame our experiments in terms of the argument mining (AM) and argument pair extraction (APE) tasks, and evaluate their ability to perform reasoning at increasing levels of abstraction in the input and output representations (e.g., arbitrary label sets, semantic graphs). We find that, although LLMs are able to match or surpass the state-of-the-art in AM and APE, their argumentative reasoning performance is very dependent on the input and output representation. We also find an "exemplar effect", where too many exemplars increasingly become detrimental for task performance, and about 4-5 being the optimal amount. Neither result extends to chain-of-thought (CoT) prompting: we find the exemplar effect to be nullified, and our results suggest that CoT allows for better performance under ill-conditioned problems. We hope that the work reported contributes to the improvement of argument
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#35843;&#26597;&#35780;&#20272;&#20102;&#31227;&#21160;&#20005;&#32899;&#28216;&#25103;&#24212;&#29992;&#20013;&#21475;&#35821;&#21270;&#20154;&#24418;&#26426;&#22120;&#20154;&#23545;&#21487;&#29992;&#24615;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#29992;&#25143;&#26356;&#21916;&#27426;&#19982;&#39640;&#20154;&#31867;&#30456;&#20284;&#24230;&#30340;&#26426;&#22120;&#20154;&#36827;&#34892;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2309.07773</link><description>&lt;p&gt;
&#31227;&#21160;&#20005;&#32899;&#28216;&#25103;&#20013;&#21475;&#35821;&#21270;&#20154;&#24418;&#26426;&#22120;&#20154;&#23545;&#21487;&#29992;&#24615;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Usability Evaluation of Spoken Humanoid Embodied Conversational Agents in Mobile Serious Games. (arXiv:2309.07773v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#35843;&#26597;&#35780;&#20272;&#20102;&#31227;&#21160;&#20005;&#32899;&#28216;&#25103;&#24212;&#29992;&#20013;&#21475;&#35821;&#21270;&#20154;&#24418;&#26426;&#22120;&#20154;&#23545;&#21487;&#29992;&#24615;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#29992;&#25143;&#26356;&#21916;&#27426;&#19982;&#39640;&#20154;&#31867;&#30456;&#20284;&#24230;&#30340;&#26426;&#22120;&#20154;&#36827;&#34892;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#31227;&#21160;&#20005;&#32899;&#28216;&#25103;&#24212;&#29992;&#20013;&#21475;&#35821;&#21270;&#20154;&#24418;&#26426;&#22120;&#20154;&#65288;HECA&#65289;&#23545;&#21487;&#29992;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#23454;&#35777;&#35843;&#26597;&#12290;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22810;&#20010;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#20132;&#20114;&#24187;&#35273;&#23545;&#20132;&#20114;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#30740;&#31350;&#20102;&#20004;&#31181;&#26426;&#22120;&#20154;&#21576;&#29616;&#26041;&#24335;&#65306;&#39640;&#20154;&#31867;&#30456;&#20284;&#24230;&#30340;&#26426;&#22120;&#20154;&#65288;HECA&#65289;&#21644;&#20302;&#20154;&#31867;&#30456;&#20284;&#24230;&#30340;&#26426;&#22120;&#20154;&#65288;&#25991;&#26412;&#65289;&#12290;&#23454;&#39564;&#30340;&#30446;&#30340;&#26159;&#35780;&#20272;&#39640;&#20154;&#31867;&#30456;&#20284;&#24230;&#26426;&#22120;&#20154;&#26159;&#21542;&#33021;&#22815;&#24341;&#21457;&#20154;&#31867;&#24187;&#35273;&#24182;&#24433;&#21709;&#21487;&#29992;&#24615;&#12290;&#39640;&#20154;&#31867;&#30456;&#20284;&#24230;&#26426;&#22120;&#20154;&#26159;&#26681;&#25454;ECA&#35774;&#35745;&#27169;&#22411;&#36827;&#34892;&#35774;&#35745;&#30340;&#65292;&#35813;&#27169;&#22411;&#26159;&#19968;&#31181;ECA&#24320;&#21457;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;90&#20301;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#19982;HECA&#36827;&#34892;&#20132;&#20114;&#12290;&#20004;&#20010;&#29256;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#22312;&#32479;&#35745;&#23398;&#19978;&#20855;&#26377;&#26174;&#33879;&#24615;&#65292;&#25928;&#24212;&#22823;&#23567;&#36739;&#22823;&#65288;d=1.01&#65289;&#65292;&#35768;&#22810;&#21442;&#19982;&#32773;&#36890;&#36807;&#35299;&#37322;&#36873;&#25321;&#26469;&#35777;&#26126;&#20182;&#20204;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an empirical investigation of the extent to which spoken Humanoid Embodied Conversational Agents (HECAs) can foster usability in mobile serious game (MSG) applications. The aim of the research is to assess the impact of multiple agents and illusion of humanness on the quality of the interaction. The experiment investigates two styles of agent presentation: an agent of high human-likeness (HECA) and an agent of low human-likeness (text). The purpose of the experiment is to assess whether and how agents of high humanlikeness can evoke the illusion of humanness and affect usability. Agents of high human-likeness were designed by following the ECA design model that is a proposed guide for ECA development. The results of the experiment with 90 participants show that users prefer to interact with the HECAs. The difference between the two versions is statistically significant with a large effect size (d=1.01), with many of the participants justifying their choice by saying
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#20998;&#35299;&#31070;&#32463;&#20256;&#36755;&#22120;&#20013;&#21152;&#20837;&#20102;&#22522;&#20110;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.07648</link><description>&lt;p&gt;
&#22312;&#20998;&#35299;&#31070;&#32463;&#20256;&#36755;&#22120;&#20013;&#24341;&#20837;&#22522;&#20110;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Incorporating Class-based Language Model for Named Entity Recognition in Factorized Neural Transducer. (arXiv:2309.07648v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07648
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#20998;&#35299;&#31070;&#32463;&#20256;&#36755;&#22120;&#20013;&#21152;&#20837;&#20102;&#22522;&#20110;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#20960;&#24180;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#23545;&#35821;&#20041;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#22686;&#24378;E2E&#27169;&#22411;&#20013;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21508;&#31181;&#22522;&#20110;&#35268;&#21017;&#25110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#19978;&#19979;&#25991;&#20559;&#32622;&#31639;&#27861;&#19978;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#23545;&#20559;&#32622;&#26435;&#37325;&#25935;&#24863;&#65292;&#25110;&#32773;&#30001;&#20110;&#23545;&#21629;&#21517;&#23454;&#20307;&#21015;&#34920;&#30340;&#36807;&#24230;&#20851;&#27880;&#32780;&#38477;&#20302;&#65292;&#24182;&#19988;&#23384;&#22312;&#35823;&#35302;&#21457;&#30340;&#39118;&#38505;&#12290;&#21463;&#20256;&#32479;&#28151;&#21512;&#31995;&#32479;&#20013;&#22522;&#20110;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#20197;&#21450;&#22312;&#20998;&#35299;&#31070;&#32463;&#20256;&#36755;&#22120;&#65288;FNT&#65289;&#20013;&#22768;&#23398;&#21644;&#35821;&#35328;&#20449;&#24687;&#30340;&#26377;&#25928;&#35299;&#32806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;E2E&#27169;&#22411;&#26469;&#23558;&#22522;&#20110;&#31867;&#21035;&#30340;LMs&#32435;&#20837;FNT&#20013;&#65292;&#31216;&#20026;C-FNT&#12290;&#22312;C-FNT&#20013;&#65292;&#21629;&#21517;&#23454;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#24471;&#20998;&#21487;&#20197;&#19982;&#20854;&#31867;&#21035;&#20851;&#32852;&#65292;&#32780;&#19981;&#26159;&#19982;&#20854;&#34920;&#38754;&#24418;&#24335;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
In spite of the excellent strides made by end-to-end (E2E) models in speech recognition in recent years, named entity recognition is still challenging but critical for semantic understanding. In order to enhance the ability to recognize named entities in E2E models, previous studies mainly focus on various rule-based or attention-based contextual biasing algorithms. However, their performance might be sensitive to the biasing weight or degraded by excessive attention to the named entity list, along with a risk of false triggering. Inspired by the success of the class-based language model (LM) in named entity recognition in conventional hybrid systems and the effective decoupling of acoustic and linguistic information in the factorized neural Transducer (FNT), we propose a novel E2E model to incorporate class-based LMs into FNT, which is referred as C-FNT. In C-FNT, the language model score of named entities can be associated with the name class instead of its surface form. The experime
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Prompting4Debugging&#65288;P4D&#65289;&#20316;&#20026;&#19968;&#20010;&#35843;&#35797;&#21644;&#32418;&#38431;&#27979;&#35797;&#24037;&#20855;&#65292;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#38382;&#39064;&#25552;&#31034;&#65292;&#20197;&#27979;&#35797;&#37096;&#32626;&#30340;&#23433;&#20840;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06135</link><description>&lt;p&gt;
Prompting4Debugging: &#36890;&#36807;&#21457;&#29616;&#38382;&#39064;&#25552;&#31034;&#26469;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts. (arXiv:2309.06135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06135
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Prompting4Debugging&#65288;P4D&#65289;&#20316;&#20026;&#19968;&#20010;&#35843;&#35797;&#21644;&#32418;&#38431;&#27979;&#35797;&#24037;&#20855;&#65292;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#38382;&#39064;&#25552;&#31034;&#65292;&#20197;&#27979;&#35797;&#37096;&#32626;&#30340;&#23433;&#20840;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20363;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#65292;&#26368;&#36817;&#23637;&#29616;&#20986;&#39640;&#36136;&#37327;&#20869;&#23481;&#29983;&#25104;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#24182;&#25104;&#20026;&#36817;&#26399;&#21464;&#38761;&#24615;&#20154;&#24037;&#26234;&#33021;&#28010;&#28526;&#30340;&#20195;&#34920;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36827;&#27493;&#20063;&#24102;&#26469;&#20102;&#23545;&#35813;&#29983;&#25104;&#25216;&#26415;&#28389;&#29992;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#29983;&#25104;&#21463;&#29256;&#26435;&#20445;&#25252;&#25110;&#19981;&#36866;&#21512;&#22312;&#24037;&#20316;&#29615;&#22659;&#20013;&#26597;&#30475;&#30340;&#22270;&#20687;&#12290;&#34429;&#28982;&#24050;&#32463;&#20570;&#20986;&#20102;&#19968;&#20123;&#21162;&#21147;&#26469;&#36890;&#36807;&#27169;&#22411;&#24494;&#35843;&#26469;&#36807;&#28388;&#19981;&#36866;&#24403;&#30340;&#22270;&#20687;/&#25552;&#31034;&#25110;&#21024;&#38500;&#19981;&#24076;&#26395;&#30340;&#27010;&#24565;/&#39118;&#26684;&#65292;&#20294;&#36825;&#20123;&#23433;&#20840;&#26426;&#21046;&#23545;&#20110;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#25552;&#31034;&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prompting4Debugging&#65288;P4D&#65289;&#20316;&#20026;&#19968;&#20010;&#35843;&#35797;&#21644;&#32418;&#38431;&#27979;&#35797;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#38382;&#39064;&#25552;&#31034;&#65292;&#20197;&#27979;&#35797;&#37096;&#32626;&#30340;&#23433;&#20840;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;P4D&#24037;&#20855;&#22312;&#21457;&#29616;&#20855;&#26377;&#23433;&#20840;&#26426;&#21046;&#30340;SD&#27169;&#22411;&#30340;&#26032;&#28431;&#27934;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown remarkable ability in high-quality content generation, and become one of the representatives for the recent wave of transformative AI. Nevertheless, such advance comes with an intensifying concern about the misuse of this generative technology, especially for producing copyrighted or NSFW (i.e. not safe for work) images. Although efforts have been made to filter inappropriate images/prompts or remove undesirable concepts/styles via model fine-tuning, the reliability of these safety mechanisms against diversified problematic prompts remains largely unexplored. In this work, we propose Prompting4Debugging (P4D) as a debugging and red-teaming tool that automatically finds problematic prompts for diffusion models to test the reliability of a deployed safety mechanism. We demonstrate the efficacy of our P4D tool in uncovering new vulnerabilities of SD models with safety mechanisms. Particularly, our result shows t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#32467;&#26524;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.03564</link><description>&lt;p&gt;
&#35780;&#20272;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media. (arXiv:2309.03564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#32467;&#26524;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#31867;&#20284;&#24555;&#36895;&#21457;&#23637;&#30340;GPT&#31995;&#21015;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24433;&#21709;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#24515;&#29702;&#23398;&#31561;&#21307;&#23398;&#39046;&#22495;&#23545;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#27987;&#21402;&#20852;&#36259;&#65292;&#20294;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#20855;&#20307;&#25506;&#32034;&#20173;&#28982;&#24456;&#23569;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#29992;&#25143;&#36234;&#26469;&#36234;&#22810;&#22320;&#34920;&#36798;&#20010;&#20154;&#24773;&#24863;&#65307;&#22312;&#29305;&#23450;&#30340;&#20027;&#39064;&#19979;&#65292;&#36825;&#20123;&#24773;&#24863;&#36890;&#24120;&#34920;&#29616;&#20026;&#28040;&#26497;&#24773;&#32490;&#65292;&#26377;&#26102;&#20250;&#21319;&#32423;&#20026;&#33258;&#26432;&#20542;&#21521;&#12290;&#21450;&#26102;&#36776;&#35782;&#36825;&#26679;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#23545;&#26377;&#25928;&#24178;&#39044;&#21644;&#28508;&#22312;&#36991;&#20813;&#20005;&#37325;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#36827;&#34892;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#33258;&#26432;&#39118;&#38505;&#21644;&#35748;&#30693;&#20559;&#24046;&#35782;&#21035;&#30340;&#23454;&#39564;&#65292;&#36827;&#20837;&#20102;&#36825;&#20010;&#39046;&#22495;&#12290;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#65306;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#24494;&#35843;&#65292;&#32771;&#23519;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models, particularly those akin to the rapidly progressing GPT series, are gaining traction for their expansive influence. While there is keen interest in their applicability within medical domains such as psychology, tangible explorations on real-world data remain scant. Concurrently, users on social media platforms are increasingly vocalizing personal sentiments; under specific thematic umbrellas, these sentiments often manifest as negative emotions, sometimes escalating to suicidal inclinations. Timely discernment of such cognitive distortions and suicidal risks is crucial to effectively intervene and potentially avert dire circumstances. Our study ventured into this realm by experimenting on two pivotal tasks: suicidal risk and cognitive distortion identification on Chinese social media platforms. Using supervised learning as a baseline, we examined and contrasted the efficacy of large language models via three distinct strategies: zero-shot, few-shot, and fine-tunin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.12114</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks. (arXiv:2307.12114v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12114
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#8212;&#8212;ChatGPT&#12289;Flan-T5 UL2&#12289;Tk-Instruct&#21644;Alpaca&#8212;&#8212;&#22312;13&#20010;&#23454;&#38469;&#19990;&#30028;&#30340;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20363;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#38382;&#31572;&#65288;QA&#65289;&#12289;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#31561;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#30340;LLM&#24320;&#22987;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;QA&#20219;&#21153;&#34920;&#29616;&#24471;&#29305;&#21035;&#22909;&#65292;&#21363;&#20351;&#23427;&#20204;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#36825;&#20123;&#20219;&#21153;&#30340;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#34920;&#29616;&#20302;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#65288;&#22914;PubMedBERT&#65289;&#21487;&#20197;&#36798;&#21040;&#30340;&#27700;&#24179;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#27809;&#26377;&#19968;&#20010;LLM&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#37117;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#20110;&#29305;&#23450;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#35757;&#32451;&#20102;&#19968;&#20010;&#19987;&#38376;&#36866;&#37197;&#20020;&#24202;&#39046;&#22495;&#30340;LLaMA-LoRA&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20854;&#19982;Downstream LLaMA-LoRA&#36866;&#37197;&#22120;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.03042</link><description>&lt;p&gt;
LLaMA&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain. (arXiv:2307.03042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#35757;&#32451;&#20102;&#19968;&#20010;&#19987;&#38376;&#36866;&#37197;&#20020;&#24202;&#39046;&#22495;&#30340;LLaMA-LoRA&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20854;&#19982;Downstream LLaMA-LoRA&#36866;&#37197;&#22120;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22914;&#20020;&#24202;&#24212;&#29992;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25152;&#26377;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#24040;&#22823;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#36341;&#24615;&#36234;&#26469;&#36234;&#34987;&#35777;&#26126;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#24494;&#35843;&#19968;&#20010;&#23567;&#30340;&#38468;&#21152;&#21442;&#25968;&#38598;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#39046;&#22495;&#36866;&#24212;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20020;&#24202;LLaMA-LoRA&#65292;&#36825;&#26159;&#19968;&#20010;&#26500;&#24314;&#22312;&#24320;&#28304;LLaMA&#27169;&#22411;&#19978;&#30340;PEFT&#36866;&#37197;&#22120;&#23618;&#12290;&#20020;&#24202;LLaMA-LoRA&#20351;&#29992;&#20174;MIMIC-IV&#25968;&#25454;&#24211;&#20013;&#33719;&#21462;&#30340;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#20026;&#20020;&#24202;&#39046;&#22495;&#35774;&#35745;&#30340;&#19987;&#29992;&#36866;&#37197;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#23558;&#20020;&#24202;LLaMA-LoRA&#19982;Downstream LLaMA-LoRA&#36827;&#34892;&#34701;&#21512;&#65292;&#21518;&#32773;&#26159;&#21478;&#19968;&#20010;&#19987;&#20026;&#19979;&#28216;&#20219;&#21153;&#35774;&#35745;&#30340;PEFT&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting pretrained language models to novel domains, such as clinical applications, traditionally involves retraining their entire set of parameters. However, this approach is increasingly proven to be impractical owing to the substantial computational requirements associated with training such large language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) techniques offer a viable solution by selectively fine-tuning a small subset of additional parameters, significantly reducing the computational requirements for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is trained using clinical notes obtained from the MIMIC-IV database, thereby creating a specialised adapter designed for the clinical domain. Additionally, we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#20803;&#20998;&#24067;&#26041;&#27861;&#65288;MDM&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#30340;&#23545;&#27604;&#26144;&#23556;&#21040;&#36136;&#37327;&#24230;&#37327;&#19978;&#65292;&#21487;&#20197;&#35270;&#20026;&#20998;&#24067;&#30340;&#20998;&#24067;&#65292;&#20854;&#21487;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.11879</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#20998;&#24067;&#24314;&#27169;&#30340;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Open-Domain Text Evaluation via Meta Distribution Modeling. (arXiv:2306.11879v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#20803;&#20998;&#24067;&#26041;&#27861;&#65288;MDM&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#30340;&#23545;&#27604;&#26144;&#23556;&#21040;&#36136;&#37327;&#24230;&#37327;&#19978;&#65292;&#21487;&#20197;&#35270;&#20026;&#20998;&#24067;&#30340;&#20998;&#24067;&#65292;&#20854;&#21487;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25511;&#21046;&#21644;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#36798;&#21040;&#25152;&#38656;&#23646;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22522;&#20110;&#21442;&#32771;&#25991;&#26412;&#30340;&#24230;&#37327;&#26631;&#20934;&#22914;BLEU&#12289;ROUGE&#21644;METEOR&#23545;&#20110;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#26469;&#35828;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#21516;&#26679;&#22320;&#65292;&#34429;&#28982;&#20855;&#22791;&#35757;&#32451;&#37492;&#21035;&#22120;&#30340;&#24230;&#37327;&#26631;&#20934;&#34920;&#29616;&#20986;&#20102;&#24076;&#26395;&#30340;&#21069;&#26223;&#65292;&#20294;&#26159;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#21017;&#26159;&#19968;&#39033;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#8212;&#8212;&#20803;&#20998;&#24067;&#26041;&#27861;&#65288;MDM&#65289;&#12290;&#36890;&#36807;&#32771;&#34385;LLMs&#21442;&#25968;&#25968;&#37327;&#19978;&#21319;&#21644;&#24615;&#33021;&#25552;&#21319;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;MDM &#21019;&#36896;&#20102;&#19968;&#20010;&#26144;&#23556;&#65292;&#23558;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#30340;&#23545;&#27604;&#65288;&#19968;&#20010;&#24050;&#30693;&#20248;&#20110;&#21478;&#19968;&#20010;&#65289;&#26144;&#23556;&#21040;&#36136;&#37327;&#24230;&#37327;&#19978;&#65292;&#35813;&#24230;&#37327;&#21487;&#20197;&#35270;&#20026;&#20998;&#24067;&#30340;&#20998;&#24067;&#65292;&#21363;&#20803;&#20998;&#24067;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;MDM&#22312;&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in open-domain text generation models powered by large pre-trained language models (LLMs) have achieved remarkable performance. However, evaluating and controlling these models for desired attributes remains a challenge, as traditional reference-based metrics such as BLEU, ROUGE, and METEOR are insufficient for open-ended generation tasks. Similarly, while trainable discriminator-based evaluation metrics show promise, obtaining high-quality training data is a non-trivial task. In this paper, we introduce a novel approach to evaluate open-domain generation - the Meta-Distribution Methods (MDM). Drawing on the correlation between the rising parameter counts and the improving performance of LLMs, MDM creates a mapping from the contrast of two probabilistic distributions -- one known to be superior to the other -to quality measures, which can be viewed as a distribution of distributions i.e. Meta-Distribution. We investigate MDM for open-domain text generation evaluation 
&lt;/p&gt;</description></item><item><title>DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13915</link><description>&lt;p&gt;
DAPR&#65306;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DAPR: A Benchmark on Document-Aware Passage Retrieval. (arXiv:2305.13915v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13915
&lt;/p&gt;
&lt;p&gt;
DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#26816;&#32034;&#20027;&#35201;&#20851;&#27880;&#30701;&#25991;&#26412;&#30340;&#25490;&#21517;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#35780;&#20272;&#25490;&#21517;&#27573;&#33853;&#25110;&#25972;&#20010;&#25991;&#26723;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#24076;&#26395;&#20174;&#24222;&#22823;&#30340;&#35821;&#26009;&#24211;&#20013;&#25214;&#21040;&#38271;&#25991;&#26723;&#20013;&#30340;&#30456;&#20851;&#27573;&#33853;&#65292;&#20363;&#22914;&#27861;&#24459;&#26696;&#20363;&#65292;&#30740;&#31350;&#35770;&#25991;&#31561;&#65292;&#27492;&#26102;&#27573;&#33853;&#24448;&#24448;&#25552;&#20379;&#24456;&#23569;&#30340;&#25991;&#26723;&#19978;&#19979;&#25991;&#65292;&#36825;&#23601;&#25361;&#25112;&#20102;&#24403;&#21069;&#30340;&#26041;&#27861;&#25214;&#21040;&#27491;&#30830;&#30340;&#25991;&#26723;&#24182;&#36820;&#22238;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#21629;&#21517;&#20102;Document-Aware Passage Retrieval&#65288;DAPR&#65289;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;DAPR&#21644;&#25972;&#20010;&#25991;&#26723;&#26816;&#32034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#25991;&#26723;&#25688;&#35201;&#20013;&#28155;&#21152;&#25991;&#26723;&#32423;&#21035;&#30340;&#20869;&#23481;&#65292;&#27719;&#24635;&#27573;&#33853;&#34920;&#31034;&#21644;&#20351;&#29992;BM25&#36827;&#34892;&#28151;&#21512;&#26816;&#32034;&#65292;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#27573;&#33853;&#26816;&#32034;&#22120;&#12290;&#36825;&#20010;&#28151;&#21512;&#26816;&#32034;&#31995;&#32479;&#65292;&#24635;&#20307;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DAPR&#20219;&#21153;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent neural retrieval mainly focuses on ranking short texts and is challenged with long documents. Existing work mainly evaluates either ranking passages or whole documents. However, there are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. legal cases, research papers, etc. In this scenario, the passage often provides little document context and thus challenges the current approaches to finding the correct document and returning accurate results. To fill this gap, we propose and name this task Document-Aware Passage Retrieval (DAPR) and build a benchmark including multiple datasets from various domains, covering both DAPR and whole-document retrieval. In experiments, we extend the state-of-the-art neural passage retrievers with document-level context via different approaches including prepending document summary, pooling over passage representations, and hybrid retrieval with BM25. The hybrid-retrieval systems, the overall b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20551;&#35774;&#25152;&#26377;&#26816;&#32034;&#20449;&#24687;&#37117;&#26159;&#27491;&#30830;&#30340;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#34394;&#20551;&#20449;&#24687;&#23548;&#33268;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#37492;&#21035;&#33021;&#21147;&#24341;&#20986;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36825;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#19979;&#30340;&#25928;&#26524;&#65307;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2305.01579</link><description>&lt;p&gt;
&#21306;&#20998;&#21644;&#22238;&#31572;&#65306;&#36890;&#36807;&#36776;&#21035;&#22120;&#32531;&#35299;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#20013;&#34394;&#20551;&#20449;&#24687;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models with Discriminators. (arXiv:2305.01579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20551;&#35774;&#25152;&#26377;&#26816;&#32034;&#20449;&#24687;&#37117;&#26159;&#27491;&#30830;&#30340;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#34394;&#20551;&#20449;&#24687;&#23548;&#33268;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#37492;&#21035;&#33021;&#21147;&#24341;&#20986;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36825;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#19979;&#30340;&#25928;&#26524;&#65307;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20551;&#23450;&#25152;&#26377;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#37117;&#26159;&#20107;&#23454;&#19978;&#27491;&#30830;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#19968;&#20010;&#26356;&#21152;&#29616;&#23454;&#30340;&#22330;&#26223;&#65292;&#21363;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#21487;&#33021;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#31934;&#35843;&#21644;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#20013;&#23545;&#36825;&#31181;&#20449;&#24687;&#39640;&#24230;&#33030;&#24369;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#22320;&#23545;&#37492;&#21035;&#22120;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#25110;&#25552;&#31034;&#26469;&#24341;&#20986;GPT-3&#30340;&#37492;&#21035;&#33021;&#21147;&#65292;&#20351;&#26816;&#32034;&#22686;&#24378;LM&#23545;&#34394;&#20551;&#20449;&#24687;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#26041;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;LM&#23545;&#30693;&#35782;&#20914;&#31361;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#30340;&#20915;&#31574;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#30340;&#21457;&#29616;&#65292;&#20026;&#21033;&#29992;&#20004;&#32773;&#30340;&#26368;&#20339;&#26041;&#24335;&#38138;&#24179;&#20102;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing retrieval-augmented language models (LMs) for question answering assume all retrieved information is factually correct. In this work, we study a more realistic scenario in which retrieved documents may contain misinformation, causing conflicts among them. We observe that the existing models are highly brittle to such information in both fine-tuning and in-context few-shot learning settings. We propose approaches to make retrieval-augmented LMs robust to misinformation by explicitly fine-tuning a discriminator or prompting to elicit discrimination capability in GPT-3. Our empirical results on open-domain question answering show that these approaches significantly improve LMs' robustness to knowledge conflicts. We also provide our findings on interleaving the fine-tuned model's decision with the in-context learning process, paving a new path to leverage the best of both worlds.
&lt;/p&gt;</description></item></channel></rss>