<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#22833;&#35823;&#65292;&#24402;&#32435;&#24182;&#30830;&#23450;&#20854;&#22833;&#36133;&#30340;&#21407;&#22240;&#31867;&#22411;&#21644;&#20851;&#38190;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#28508;&#22312;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10513</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;ChatGPT&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Does ChatGPT Fall Short in Answering Questions Faithfully?. (arXiv:2304.10513v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10513
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#22833;&#35823;&#65292;&#24402;&#32435;&#24182;&#30830;&#23450;&#20854;&#22833;&#36133;&#30340;&#21407;&#22240;&#31867;&#22411;&#21644;&#20851;&#38190;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#28508;&#22312;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;ChatGPT&#65292;&#23637;&#31034;&#20986;&#23545;&#20154;&#31867;&#29983;&#27963;&#21508;&#26041;&#38754;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;ChatGPT&#22312;&#35802;&#23454;&#24615;&#31561;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#38382;&#31572;&#31995;&#32479;&#20026;&#20195;&#34920;&#24212;&#29992;&#31243;&#24207;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;&#20026;&#20160;&#20040;ChatGPT&#22312;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#26377;&#25152;&#19981;&#36275;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35797;&#22270;&#20998;&#26512;ChatGPT&#22312;&#22797;&#26434;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#22833;&#36133;&#30340;&#21407;&#22240;&#65292;&#24182;&#30830;&#23450;&#19982;&#36825;&#20123;&#22833;&#36133;&#26377;&#20851;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;ChatGPT&#30340;&#22833;&#36133;&#24402;&#20026;&#22235;&#31181;&#31867;&#22411;&#65306;&#29702;&#35299;&#12289;&#20107;&#23454;&#24615;&#12289;&#20855;&#20307;&#24615;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#19982;QA&#22833;&#36133;&#26377;&#20851;&#30340;&#19977;&#20010;&#20851;&#38190;&#33021;&#21147;&#65306;&#30693;&#35782;&#35760;&#24518;&#12289;&#30693;&#35782;&#20851;&#32852;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22260;&#32469;&#36825;&#20123;&#33021;&#21147;&#30340;&#23454;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21521;&#27169;&#22411;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#22806;&#37096;&#30693;&#35782;&#12289;&#32473;&#20104;&#25552;&#31034;&#26469;&#24110;&#21161;&#23427;&#32858;&#28966;&#24182;&#21152;&#24378;&#20851;&#38190;&#33021;&#21147;&#65292;&#36825;&#37117;&#26377;&#21161;&#20110;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models, such as ChatGPT, have demonstrated significant potential to impact various aspects of human life. However, ChatGPT still faces challenges in aspects like faithfulness. Taking question answering as a representative application, we seek to understand why ChatGPT falls short in answering questions faithfully. To address this question, we attempt to analyze the failures of ChatGPT in complex open-domain question answering and identifies the abilities under the failures. Specifically, we categorize ChatGPT's failures into four types: comprehension, factualness, specificity, and inference. We further pinpoint three critical abilities associated with QA failures: knowledge memorization, knowledge association, and knowledge reasoning. Additionally, we conduct experiments centered on these abilities and propose potential approaches to enhance faithfulness. The results indicate that furnishing the model with fine-grained external knowledge, hints for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#30693;&#35782;&#24863;&#30693;BERT&#27169;&#22411;&#65292;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#21457;&#24067;&#30340;&#19982;&#21512;&#25104;&#38463;&#29255;&#31867;&#29289;&#36136;&#30456;&#20851;&#30340;&#24086;&#23376;&#65292;&#20197;&#20102;&#35299;&#29992;&#25143;&#23545;&#19981;&#21516;&#21512;&#25104;&#38463;&#29255;&#31867;&#29289;&#36136;&#30340;&#24773;&#24863;&#21644;&#24773;&#32490;&#65292;&#20174;&#32780;&#24110;&#21161;&#39044;&#27979;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2304.10512</link><description>&lt;p&gt;
&#8220;&#25105;&#20204;&#33021;&#26816;&#27979;&#20986;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#21527;&#65311;&#8221;&#65306;&#26469;&#33258;&#26263;&#32593;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#30693;&#35782;&#21644;&#26102;&#38388;&#24863;&#30693;&#20998;&#31867;&#30740;&#31350;&#12290;(arXiv:2304.10512v1[cs.LG])
&lt;/p&gt;
&lt;p&gt;
"Can We Detect Substance Use Disorder?": Knowledge and Time Aware Classification on Social Media from Darkweb. (arXiv:2304.10512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#30693;&#35782;&#24863;&#30693;BERT&#27169;&#22411;&#65292;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#21457;&#24067;&#30340;&#19982;&#21512;&#25104;&#38463;&#29255;&#31867;&#29289;&#36136;&#30456;&#20851;&#30340;&#24086;&#23376;&#65292;&#20197;&#20102;&#35299;&#29992;&#25143;&#23545;&#19981;&#21516;&#21512;&#25104;&#38463;&#29255;&#31867;&#29289;&#36136;&#30340;&#24773;&#24863;&#21644;&#24773;&#32490;&#65292;&#20174;&#32780;&#24110;&#21161;&#39044;&#27979;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#32654;&#22269;&#30340;&#38463;&#29255;&#31867;&#21644;&#29289;&#36136;&#28389;&#29992;&#38382;&#39064;&#26085;&#30410;&#20005;&#37325;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#38463;&#29255;&#31867;&#21361;&#26426;&#8221;&#12290;&#29289;&#36136;&#20351;&#29992;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#30340;&#20851;&#31995;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20854;&#20013;&#19968;&#31181;&#21487;&#33021;&#30340;&#20851;&#31995;&#26159;&#65306;&#29289;&#36136;&#28389;&#29992;&#23548;&#33268;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#35777;&#25454;&#65292;&#23548;&#33268;&#21512;&#27861;&#36884;&#24452;&#36141;&#20080;&#30340;&#38463;&#29255;&#31867;&#29289;&#36136;&#24456;&#38590;&#33719;&#24471;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#31038;&#20132;&#23186;&#20307;&#19978;&#26377;&#20851;&#29289;&#36136;&#20351;&#29992;&#30340;&#24086;&#23376;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#21152;&#23494;&#24066;&#22330;&#38144;&#21806;&#30340;&#21512;&#25104;&#38463;&#29255;&#31867;&#29289;&#36136;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#33647;&#29289;&#28389;&#29992;&#26412;&#20307;&#35770;&#65292;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#30693;&#35782;&#24863;&#30693;BERT&#27169;&#22411;&#29983;&#25104;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#24773;&#24863;&#21644;&#24773;&#32490;&#65292;&#20197;&#20102;&#35299;&#29992;&#25143;&#23545;&#31038;&#20132;&#23186;&#20307;&#30340;&#30475;&#27861;&#65292;&#30740;&#31350;&#38382;&#39064;&#65292;&#20363;&#22914;&#65306;&#20154;&#20204;&#23545;&#21738;&#31181;&#21512;&#25104;&#38463;&#29255;&#31867;&#29289;&#36136;&#25345;&#20048;&#35266;&#24577;&#24230;&#12289;&#20013;&#31435;&#24577;&#24230;&#25110;&#32773;&#28040;&#26497;&#24577;&#24230;&#65311;&#25110;&#32773;&#21738;&#20123;&#33647;&#29289;&#24341;&#36215;&#20102;&#24656;&#24807;&#21644;&#24754;&#20260;&#65311;&#20154;&#20204;&#21916;&#27426;&#21738;&#20123;&#33647;&#29289;&#65292;&#25110;&#32773;&#23545;&#21738;&#20123;&#33647;&#29289;&#24863;&#28608;&#65311;&#21738;&#20123;&#33647;&#29289;&#20154;&#20204;&#25345;&#28040;&#26497;&#24577;&#24230;&#65311;
&lt;/p&gt;
&lt;p&gt;
Opioid and substance misuse is rampant in the United States today, with the phenomenon known as the "opioid crisis". The relationship between substance use and mental health has been extensively studied, with one possible relationship being: substance misuse causes poor mental health. However, the lack of evidence on the relationship has resulted in opioids being largely inaccessible through legal means. This study analyzes the substance use posts on social media with opioids being sold through crypto market listings. We use the Drug Abuse Ontology, state-of-the-art deep learning, and knowledge-aware BERT-based models to generate sentiment and emotion for the social media posts to understand users' perceptions on social media by investigating questions such as: which synthetic opioids people are optimistic, neutral, or negative about? or what kind of drugs induced fear and sorrow? or what kind of drugs people love or are thankful about? or which drugs people think negatively about? or 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22806;&#35821;&#21040;&#27424;&#21457;&#36798;&#35821;&#35328;&#33719;&#21462;&#26032;&#35789;&#30340;&#21021;&#27493;&#25351;&#21335;&#65292;&#25552;&#20986;&#21487;&#20197;&#24341;&#20837;&#20219;&#20309;&#35821;&#35328;&#30340;&#26032;&#35789;&#65292;&#21482;&#35201;&#36825;&#20123;&#35789;&#35821;&#8220;&#21548;&#36215;&#26469;&#20687;&#8221;&#30446;&#26631;&#35821;&#35328;&#30340;&#26412;&#22320;&#21333;&#35789;&#12290;</title><link>http://arxiv.org/abs/2304.10495</link><description>&lt;p&gt;
&#20174;&#22806;&#35821;&#21040;&#27424;&#21457;&#36798;&#35821;&#35328;&#20013;&#33719;&#21462;&#26032;&#35789;&#30340;&#21021;&#27493;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
A primer on getting neologisms from foreign languages to under-resourced languages. (arXiv:2304.10495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22806;&#35821;&#21040;&#27424;&#21457;&#36798;&#35821;&#35328;&#33719;&#21462;&#26032;&#35789;&#30340;&#21021;&#27493;&#25351;&#21335;&#65292;&#25552;&#20986;&#21487;&#20197;&#24341;&#20837;&#20219;&#20309;&#35821;&#35328;&#30340;&#26032;&#35789;&#65292;&#21482;&#35201;&#36825;&#20123;&#35789;&#35821;&#8220;&#21548;&#36215;&#26469;&#20687;&#8221;&#30446;&#26631;&#35821;&#35328;&#30340;&#26412;&#22320;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#25903;&#25345;&#65292;&#22823;&#22810;&#25968;&#27424;&#21457;&#36798;&#35821;&#35328;&#22312;&#22823;&#22810;&#25968;&#39046;&#22495;&#21644;&#39046;&#22495;&#20013;&#30340;&#35789;&#27719;&#37327;&#37117;&#24456;&#23569;&#65292;&#32780;&#20854;&#20351;&#29992;&#32773;&#38656;&#35201;&#22823;&#37327;&#22686;&#21152;&#35789;&#27719;&#37327;&#12290;&#34429;&#28982;&#26032;&#35789;&#24212;&#35813;&#20174;&#35821;&#35328;&#26412;&#36523;&#20135;&#29983;&#65292;&#20294;&#22806;&#37096;&#26469;&#28304;&#34987;&#24191;&#27867;&#25509;&#21463;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;&#34987;&#24378;&#21152;&#30340;&#23448;&#26041;&#35821;&#35328;&#30340;&#8220;&#24120;&#35782;&#8221;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#36825;&#20123;&#35821;&#35328;&#24456;&#21487;&#33021;&#26159;&#27542;&#27665;&#20027;&#20041;&#30340;&#36951;&#20135;&#65292;&#20316;&#20026;&#21807;&#19968;&#30340;&#26469;&#28304;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24341;&#20837;&#20219;&#20309;&#35821;&#35328;&#30340;&#26032;&#35789;&#65292;&#21482;&#35201;&#36825;&#20123;&#26032;&#35789;&#8220;&#21548;&#36215;&#26469;&#20687;&#8221;&#30446;&#26631;&#35821;&#35328;&#30340;&#26412;&#22320;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mainly due to lack of support, most under-resourced languages have a reduced lexicon in most realms and domains of increasing importance, then their speakers need to significantly augment it. Although neologisms should arise from the languages themselves, external sources are widely accepted. However, we dispute the "common sense" of using the imposed official languages, which are highly probably a legacy of colonialism, as the only source, and we propose to introduce neologisms from any language as long as these neologisms "sound like" native words of the target languages.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#32534;&#31243;&#35821;&#35328;&#24182;&#36890;&#36807;&#23398;&#20064;&#32534;&#31243;&#26041;&#27861;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#24182;&#25351;&#23548;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;&#32534;&#31243;&#20219;&#21153;&#19978;&#27604;&#22522;&#32447;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.10464</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning to Program with Natural Language. (arXiv:2304.10464v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#32534;&#31243;&#35821;&#35328;&#24182;&#36890;&#36807;&#23398;&#20064;&#32534;&#31243;&#26041;&#27861;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#24182;&#25351;&#23548;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;&#32534;&#31243;&#20219;&#21153;&#19978;&#27604;&#22522;&#32447;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#26412;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#36825;&#24341;&#36215;&#20102;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#24076;&#26395;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#65292;&#25105;&#20204;&#38656;&#35201;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32534;&#31243;&#65292;&#28982;&#21518;&#25353;&#29031;&#31243;&#24207;&#29983;&#25104;&#29305;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#35821;&#35328;&#26469;&#25551;&#36848;&#20219;&#21153;&#36807;&#31243;&#65292;&#20351;&#23427;&#20204;&#26131;&#20110;&#20154;&#31867;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#12290;&#34429;&#28982;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#65292;&#20294;&#36825;&#20123;&#31243;&#24207;&#21487;&#33021;&#20173;&#28982;&#23384;&#22312;&#38169;&#35823;&#25110;&#19981;&#23436;&#25972;&#30340;&#27493;&#39588;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#23398;&#20064;&#32534;&#31243;&#65288;LP&#65289;&#30340;&#26041;&#27861;&#65292;&#35201;&#27714;&#22823;&#35821;&#35328;&#27169;&#22411;&#20174;&#22797;&#26434;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#65292;&#28982;&#21518;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#31243;&#24207;&#26469;&#25351;&#23548;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;AMPS&#65288;&#39640;&#20013;&#25968;&#23398;&#65289;&#21644;Math&#65288;&#31454;&#36187;&#25968;&#23398;&#38382;&#39064;&#65289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#27979;&#35797;ChatGP&#35299;&#20915;&#32534;&#31243;&#20219;&#21153;&#26102;&#65292;LP&#33021;&#22815;&#23454;&#29616;80%&#30340;&#25104;&#21151;&#29575;&#65292;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable performance in various basic natural language tasks, which raises hopes for achieving Artificial General Intelligence. To better complete complex tasks, we need LLMs to program for the task and then follow the program to generate a specific solution for the test sample. We propose using natural language as a new programming language to describe task procedures, making them easily understandable to both humans and LLMs. LLMs are capable of directly generating natural language programs, but these programs may still contain factual errors or incomplete steps. Therefore, we further propose the Learning to Program (LP) method to ask LLMs themselves to learn natural language programs from the training dataset of complex tasks and then use the learned program to guide inference. Our experiments on the AMPS (high school math) and Math (competition mathematics problems) datasets demonstrate the effectiveness of our approach. When testing ChatGP
&lt;/p&gt;</description></item><item><title>Phoenix &#26159;&#19968;&#27454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#27665;&#20027;&#21270;&#65292;&#19981;&#20165;&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20063;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#24471; ChatGPT &#22312;&#26356;&#22810;&#30340;&#22269;&#23478;&#21644;&#22320;&#21306;&#21464;&#24471;&#26356;&#21152;&#21487;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.10453</link><description>&lt;p&gt;
Phoenix: &#23454;&#29616; ChatGPT &#30340;&#36328;&#35821;&#35328;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
Phoenix: Democratizing ChatGPT across Languages. (arXiv:2304.10453v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10453
&lt;/p&gt;
&lt;p&gt;
Phoenix &#26159;&#19968;&#27454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#27665;&#20027;&#21270;&#65292;&#19981;&#20165;&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20063;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#24471; ChatGPT &#22312;&#26356;&#22810;&#30340;&#22269;&#23478;&#21644;&#22320;&#21306;&#21464;&#24471;&#26356;&#21152;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#21162;&#21147;&#23454;&#29616; ChatGPT &#36328;&#35821;&#35328;&#27665;&#20027;&#21270;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#27454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8220;Phoenix&#8221;&#65292;&#22312;&#24320;&#28304;&#30340;&#33521;&#25991;&#21644;&#20013;&#25991;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#65288;&#21253;&#25324;&#25289;&#19969;&#21644;&#38750;&#25289;&#19969;&#35821;&#35328;&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21033;&#20110;&#35753; ChatGPT &#22312;&#26356;&#22810;&#30340;&#22269;&#23478;&#21644;&#22320;&#21306;&#21464;&#24471;&#26356;&#21152;&#21487;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#30001;&#20110;OpenAI&#25110;&#24403;&#22320;&#25919;&#24220;&#30340;&#38480;&#21046;&#32780;&#26080;&#27861;&#20351;&#29992;ChatGPT&#30340;&#22269;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our efforts to democratize ChatGPT across language. We release a large language model "Phoenix", achieving competitive performance among open-source English and Chinese models while excelling in languages with limited resources (covering both Latin and non-Latin languages). We believe this work will be beneficial to make ChatGPT more accessible, especially in countries where people cannot use ChatGPT due to restrictions from OpenAI or local goverments. Our data, code, and models are available at https://github.com/FreedomIntelligence/LLMZoo.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#36827;&#34892;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#20197;&#25429;&#25417;&#38271;&#19978;&#19979;&#25991;&#12290;&#20316;&#32773;&#22522;&#20110;XLNet&#21644;Longformer&#35757;&#32451;&#21644;&#21457;&#24067;&#20102;MentalXLNet&#21644;MentalLongformer&#27169;&#22411;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#21644;&#33021;&#21147;&#27979;&#35797;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#26089;&#26399;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#26816;&#27979;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.10447</link><description>&lt;p&gt;
&#38754;&#21521;&#24515;&#29702;&#20581;&#24247;&#30340;&#35821;&#35328;&#27169;&#22411;&#32487;&#32493;&#39044;&#35757;&#32451;&#20197;&#25429;&#33719;&#38271;&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
Domain-specific Continued Pretraining of Language Models for Capturing Long Context in Mental Health. (arXiv:2304.10447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#36827;&#34892;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#20197;&#25429;&#25417;&#38271;&#19978;&#19979;&#25991;&#12290;&#20316;&#32773;&#22522;&#20110;XLNet&#21644;Longformer&#35757;&#32451;&#21644;&#21457;&#24067;&#20102;MentalXLNet&#21644;MentalLongformer&#27169;&#22411;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#21644;&#33021;&#21147;&#27979;&#35797;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#26089;&#26399;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#26816;&#27979;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#12290;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#65292;&#39044;&#35757;&#32451;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#34987;&#37322;&#25918;&#65292;&#26377;&#21161;&#20110;&#26089;&#26399;&#21457;&#29616;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#12290;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#65288;&#20363;&#22914; Reddit&#65289;&#36890;&#24120;&#37117;&#24456;&#38271;&#12290;&#28982;&#32780;&#65292;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#24182;&#27809;&#26377;&#38754;&#21521;&#38271;&#24207;&#21015;&#30340;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#38754;&#21521;&#24515;&#29702;&#20581;&#24247;&#30340;&#32487;&#32493;&#39044;&#35757;&#32451;&#65292;&#20197;&#25429;&#25417;&#38271;&#19978;&#19979;&#25991;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110; XLNet &#21644; Longformer &#35757;&#32451;&#24182;&#21457;&#24067;&#20102; MentalXLNet &#21644; MentalLongformer&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20004;&#20010;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#31867;&#24615;&#33021;&#21644;&#38271;&#36317;&#31163;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#24050;&#22312; HuggingFace &#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models have been used in various natural language processing applications. In the mental health domain, domain-specific language models are pretrained and released, which facilitates the early detection of mental health conditions. Social posts, e.g., on Reddit, are usually long documents. However, there are no domain-specific pretrained models for long-sequence modeling in the mental health domain. This paper conducts domain-specific continued pretraining to capture the long context for mental health. Specifically, we train and release MentalXLNet and MentalLongformer based on XLNet and Longformer. We evaluate the mental health classification performance and the long-range ability of these two domain-specific pretrained models. Our models are released in HuggingFace.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20013;&#22269;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#35780;&#20272;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;8&#31181;&#20856;&#22411;&#23433;&#20840;&#22330;&#26223;&#21644;6&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#25351;&#20196;&#25915;&#20987;&#30340;&#32508;&#21512;&#23433;&#20840;&#24615;&#33021;&#35780;&#20272;&#22522;&#20934;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#35780;&#20272;&#33021;&#21147;&#24320;&#21457;&#24182;&#37327;&#21270;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10436</link><description>&lt;p&gt;
&#20013;&#22269;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Safety Assessment of Chinese Large Language Models. (arXiv:2304.10436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10436
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20013;&#22269;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#35780;&#20272;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;8&#31181;&#20856;&#22411;&#23433;&#20840;&#22330;&#26223;&#21644;6&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#25351;&#20196;&#25915;&#20987;&#30340;&#32508;&#21512;&#23433;&#20840;&#24615;&#33021;&#35780;&#20272;&#22522;&#20934;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#35780;&#20272;&#33021;&#21147;&#24320;&#21457;&#24182;&#37327;&#21270;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35832;&#22914;ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36805;&#36895;&#26222;&#21450;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#23427;&#20204;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#29983;&#25104;&#20398;&#36785;&#24615;&#21644;&#27495;&#35270;&#24615;&#20869;&#23481;&#65292;&#21453;&#26144;&#19981;&#27491;&#30830;&#30340;&#31038;&#20250;&#20215;&#20540;&#35266;&#65292;&#24182;&#21487;&#33021;&#34987;&#29992;&#20110;&#27450;&#35784;&#21644;&#20256;&#25773;&#35823;&#23548;&#20449;&#24687;&#31561;&#24694;&#24847;&#29992;&#36884;&#12290;&#35780;&#20272;&#21644;&#22686;&#24378;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#23545;&#20110;&#24191;&#27867;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23588;&#20026;&#37325;&#35201;&#12290;&#20026;&#36827;&#19968;&#27493;&#20419;&#36827;LLMs&#30340;&#23433;&#20840;&#37096;&#32626;,&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20013;&#22269;LLM&#23433;&#20840;&#35780;&#20272;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#20174;8&#31181;&#20856;&#22411;&#30340;&#23433;&#20840;&#22330;&#26223;&#21644;6&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25351;&#20196;&#25915;&#20987;&#20004;&#20010;&#26041;&#38754;&#25506;&#32034;LLMs&#30340;&#32508;&#21512;&#23433;&#20840;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#26159;&#22522;&#20110;&#19968;&#20010;&#31616;&#21333;&#26126;&#20102;&#30340;&#36807;&#31243;&#65292;&#20854;&#20013;&#23427;&#25552;&#20379;&#27979;&#35797;&#25552;&#31034;&#24182;&#35780;&#20272;&#20174;&#35780;&#20272;&#27169;&#22411;&#20135;&#29983;&#30340;&#21709;&#24212;&#30340;&#23433;&#20840;&#24615;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#35780;&#20272;&#33021;&#21147;&#65292;&#24182;&#23558;&#20854;&#24320;&#21457;&#20026;&#19968;&#31181;&#23433;&#20840;&#35780;&#20272;&#24037;&#20855;&#65292;&#20197;&#37327;&#21270;&#35780;&#20272;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#23545;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#20013;&#25991;LLMs&#30340;&#23433;&#20840;&#24615;&#33021;&#26159;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid popularity of large language models such as ChatGPT and GPT-4, a growing amount of attention is paid to their safety concerns. These models may generate insulting and discriminatory content, reflect incorrect social values, and may be used for malicious purposes such as fraud and dissemination of misleading information. Evaluating and enhancing their safety is particularly essential for the wide application of large language models (LLMs). To further promote the safe deployment of LLMs, we develop a Chinese LLM safety assessment benchmark. Our benchmark explores the comprehensive safety performance of LLMs from two perspectives: 8 kinds of typical safety scenarios and 6 types of more challenging instruction attacks. Our benchmark is based on a straightforward process in which it provides the test prompts and evaluates the safety of the generated responses from the evaluated model. In evaluation, we utilize the LLM's strong evaluation ability and develop it as a safety ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GPT-NER&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65288;NER&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#23558;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#36716;&#21270;&#20026;&#29983;&#25104;&#20219;&#21153;&#65292;&#23558;LLM&#33021;&#22815;&#23481;&#26131;&#22320;&#36866;&#24212;NER&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;LLMs&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#35810;&#38382;&#33258;&#36523;&#26469;&#30830;&#23450;&#25552;&#21462;&#30340;&#23454;&#20307;&#26159;&#21542;&#23646;&#20110;&#23454;&#38469;&#23384;&#22312;&#30340;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2304.10428</link><description>&lt;p&gt;
GPT-NER&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
GPT-NER: Named Entity Recognition via Large Language Models. (arXiv:2304.10428v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GPT-NER&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65288;NER&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#23558;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#36716;&#21270;&#20026;&#29983;&#25104;&#20219;&#21153;&#65292;&#23558;LLM&#33021;&#22815;&#23481;&#26131;&#22320;&#36866;&#24212;NER&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;LLMs&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#35810;&#38382;&#33258;&#36523;&#26469;&#30830;&#23450;&#25552;&#21462;&#30340;&#23454;&#20307;&#26159;&#21542;&#23646;&#20110;&#23454;&#38469;&#23384;&#22312;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;NER&#24615;&#33021;&#20173;&#28982;&#26126;&#26174;&#20302;&#20110;&#30417;&#30563;&#22522;&#32447;&#12290;&#36825;&#26159;&#30001;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#65306;&#21069;&#32773;&#22312;&#26412;&#36136;&#19978;&#26159;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#65292;&#21518;&#32773;&#26159;&#19968;&#31181;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-NER&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290; GPT-NER&#36890;&#36807;&#23558;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#36716;&#25442;&#20026;&#29983;&#25104;&#20219;&#21153;&#26469;&#24357;&#21512;&#24046;&#36317;&#65292;LLMs&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#12290;&#20363;&#22914;&#65292;&#23558;&#22312;&#36755;&#20837;&#25991;&#26412;&#8220;&#21733;&#20262;&#24067;&#26159;&#19968;&#24231;&#22478;&#24066;&#8221;&#20013;&#26597;&#25214;&#20301;&#32622;&#23454;&#20307;&#30340;&#20219;&#21153;&#36716;&#25442;&#20026;&#29983;&#25104;&#25991;&#26412;&#24207;&#21015;&#8220;@@&#21733;&#20262;&#24067;##&#26159;&#19968;&#24231;&#22478;&#24066;&#8221;&#65292;&#20854;&#20013;&#29305;&#27530;&#26631;&#35760;@@##&#26631;&#35760;&#35201;&#25552;&#21462;&#30340;&#23454;&#20307;&#12290;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;LLMs&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#21363;LLMs&#26377;&#24456;&#24378;&#30340;&#20542;&#21521;&#23558;&#31354;&#36755;&#20837;&#36807;&#24230;&#33258;&#20449;&#22320;&#26631;&#35760;&#20026;&#23454;&#20307;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#35810;&#38382;&#33258;&#36523;&#26469;&#30830;&#23450;&#25552;&#21462;&#30340;&#23454;&#20307;&#26159;&#21542;&#23646;&#20110;&#23454;&#38469;&#23384;&#22312;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model.  In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text "Columbus is a city" is transformed to generate the text sequence "@@Columbus## is a city", where special tokens @@## marks the entity to extract. To efficiently address the "hallucination" issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a lab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CKBP v2, &#19968;&#20010;&#20351;&#29992;&#19987;&#23478;&#27880;&#37322;&#32780;&#22218;&#25324;&#23545;&#25239;&#26679;&#26412;&#30340;&#39640;&#36136;&#37327;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#22522;&#20934;&#65292;&#20197;&#35299;&#20915;CKBP v1&#30001;&#20110;&#20247;&#21253;&#27880;&#37322;&#21644;&#38543;&#26426;&#25277;&#26679;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#20219;&#21153;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10392</link><description>&lt;p&gt;
CKBP v2&#65306;&#19968;&#20010;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#30340;&#19987;&#23478;&#27880;&#37322;&#35780;&#20272;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population. (arXiv:2304.10392v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CKBP v2, &#19968;&#20010;&#20351;&#29992;&#19987;&#23478;&#27880;&#37322;&#32780;&#22218;&#25324;&#23545;&#25239;&#26679;&#26412;&#30340;&#39640;&#36136;&#37327;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#22522;&#20934;&#65292;&#20197;&#35299;&#20915;CKBP v1&#30001;&#20110;&#20247;&#21253;&#27880;&#37322;&#21644;&#38543;&#26426;&#25277;&#26679;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#20219;&#21153;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22635;&#20805;&#36890;&#35782;&#30693;&#35782;&#24211;&#26159;NLP&#20013;&#19968;&#20010;&#37325;&#35201;&#20294;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#22788;&#29702;&#22806;&#37096;&#26469;&#28304;&#12289;&#26410;&#35265;&#36807;&#30340;&#20107;&#20214;&#21644;&#23454;&#20307;&#30340;&#30693;&#35782;&#12290; Fang&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#35780;&#20272;&#38598;CKBP v1&#12290;&#20294;&#26159;&#65292;CKBP v1&#37319;&#29992;&#30001;&#20247;&#21253;&#27880;&#37322;&#65292;&#23384;&#22312;&#30456;&#24403;&#22823;&#27604;&#20363;&#30340;&#38169;&#35823;&#31572;&#26696;&#65292;&#24182;&#19988;&#30001;&#20110;&#38543;&#26426;&#25277;&#26679;&#65292;&#35780;&#20272;&#38598;&#19982;&#22806;&#37096;&#30693;&#35782;&#26469;&#28304;&#30340;&#23545;&#40784;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CKBP v2&#65292;&#19968;&#20010;&#26032;&#30340;&#39640;&#36136;&#37327;&#30340;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#32780;&#19981;&#26159;&#20247;&#21253;&#27880;&#37322;&#65292;&#24182;&#28155;&#21152;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#20351;&#35780;&#20272;&#38598;&#26356;&#20855;&#20195;&#34920;&#24615;&#26469;&#35299;&#20915;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#26032;&#30340;&#35780;&#20272;&#38598;&#19978;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#29992;&#20110;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#20197;&#29992;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#27604;&#36739;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22635;&#20805;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Populating Commonsense Knowledge Bases (CSKB) is an important yet hard task in NLP, as it tackles knowledge from external sources with unseen events and entities. Fang et al. (2021a) proposed a CSKB Population benchmark with an evaluation set CKBP v1. However, CKBP v1 adopts crowdsourced annotations that suffer from a substantial fraction of incorrect answers, and the evaluation set is not well-aligned with the external knowledge source as a result of random sampling. In this paper, we introduce CKBP v2, a new high-quality CSKB Population benchmark, which addresses the two mentioned problems by using experts instead of crowd-sourced annotation and by adding diversified adversarial samples to make the evaluation set more representative. We conduct extensive experiments comparing state-of-the-art methods for CSKB Population on the new evaluation set for future research comparisons. Empirical results show that the population task is still challenging, even for large language models (LLM) 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;XRE&#31639;&#27861;Prompt-XRE&#65292;&#22522;&#20110;Prompt&#35843;&#20248;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#22810;&#35821;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.10354</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;Prompt&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt-Learning for Cross-Lingual Relation Extraction. (arXiv:2304.10354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;XRE&#31639;&#27861;Prompt-XRE&#65292;&#22522;&#20110;Prompt&#35843;&#20248;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#22810;&#35821;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;(RE)&#26159;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#28041;&#21450;&#39044;&#27979;&#32473;&#23450;&#21477;&#23376;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#23558;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;RE&#27169;&#22411;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#36827;&#34892;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;(XRE)&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#35843;&#20248;&#30340;&#26032;&#22411;XRE&#31639;&#27861;&#65292;&#31216;&#20026;Prompt-XRE&#12290;&#20026;&#20102;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#20960;&#20010;Prompt&#27169;&#26495;&#65292;&#21253;&#25324;&#30828;Prompt&#12289;&#36719;Prompt&#21644;&#28151;&#21512;Prompt&#65292;&#24182;&#22312;&#31454;&#20105;&#24615;&#22810;&#35821;&#31181;PLM&#65288;&#29305;&#21035;&#26159;mBART&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#27979;&#35797;&#12290;&#25105;&#20204;&#22312;&#20302;&#36164;&#28304;ACE05&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#28085;&#30422;&#22810;&#31181;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction (RE) is a crucial task in Information Extraction, which entails predicting relationships between entities within a given sentence. However, extending pre-trained RE models to other languages is challenging, particularly in real-world scenarios where Cross-Lingual Relation Extraction (XRE) is required. Despite recent advancements in Prompt-Learning, which involves transferring knowledge from Multilingual Pre-trained Language Models (PLMs) to diverse downstream tasks, there is limited research on the effective use of multilingual PLMs with prompts to improve XRE. In this paper, we present a novel XRE algorithm based on Prompt-Tuning, referred to as Prompt-XRE. To evaluate its effectiveness, we design and implement several prompt templates, including hard, soft, and hybrid prompts, and empirically test their performance on competitive multilingual PLMs, specifically mBART. Our extensive experiments, conducted on the low-resource ACE05 benchmark across multiple language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#34920;&#31034;&#32423;&#24178;&#39044;&#36827;&#34892;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#20013;&#38388;&#35821;&#20041;&#29305;&#24449;&#23545;&#20998;&#31867;&#32467;&#26524;&#30340;&#24433;&#21709;&#30740;&#31350;&#65292;&#21253;&#25324;&#20581;&#24536;&#25506;&#27979;&#21450;&#36951;&#24536;-&#35760;&#24518;&#25506;&#27979;&#21464;&#20307;&#65292;&#20026;&#20043;&#21518;&#30340;&#24178;&#39044;&#25506;&#27979;&#25552;&#20379;&#20102;&#37325;&#35201;&#25351;&#23548;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.10346</link><description>&lt;p&gt;
&#39640;&#32500;&#24178;&#39044;&#25506;&#27979;&#65306;&#19968;&#39033;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Interventional Probing in High Dimensions: An NLI Case Study. (arXiv:2304.10346v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#34920;&#31034;&#32423;&#24178;&#39044;&#36827;&#34892;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#20013;&#38388;&#35821;&#20041;&#29305;&#24449;&#23545;&#20998;&#31867;&#32467;&#26524;&#30340;&#24433;&#21709;&#30740;&#31350;&#65292;&#21253;&#25324;&#20581;&#24536;&#25506;&#27979;&#21450;&#36951;&#24536;-&#35760;&#24518;&#25506;&#27979;&#21464;&#20307;&#65292;&#20026;&#20043;&#21518;&#30340;&#24178;&#39044;&#25506;&#27979;&#25552;&#20379;&#20102;&#37325;&#35201;&#25351;&#23548;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#27979;&#31574;&#30053;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21508;&#31181;&#35821;&#35328;&#29305;&#24449;&#30340;&#23384;&#22312;&#65307;&#29305;&#21035;&#26159;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65288;NLI&#65289;&#30340;&#8220;&#33258;&#28982;&#36923;&#36753;&#8221;&#29255;&#27573;&#30340;&#20013;&#38388;&#35821;&#20041;&#29305;&#24449;&#12290;&#22312;&#33258;&#28982;&#36923;&#36753;&#20013;&#65292;&#20013;&#38388;&#29305;&#24449;&#19982;&#34164;&#21547;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#26126;&#30830;&#30693;&#26195;&#30340;&#65306;&#22240;&#27492;&#65292;&#36825;&#20026;&#23545;NLI&#27169;&#22411;&#34920;&#31034;&#36827;&#34892;&#24178;&#39044;&#30740;&#31350;&#25552;&#20379;&#20102;&#25104;&#29087;&#30340;&#35774;&#32622;&#65292;&#20174;&#32780;&#33021;&#22815;&#25552;&#20379;&#26356;&#24378;&#30340;&#22240;&#26524;&#20551;&#35828;&#21644;&#23545;&#24178;&#39044;&#25506;&#27979;&#26041;&#27861;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#26032;&#30340;&#21644;&#29616;&#26377;&#30340;&#34920;&#31034;&#32423;&#24178;&#39044;&#65292;&#20197;&#30740;&#31350;&#36825;&#20123;&#35821;&#20041;&#29305;&#24449;&#23545;NLI&#20998;&#31867;&#30340;&#24433;&#21709;&#65306;&#25105;&#20204;&#25191;&#34892;&#20102;&#20581;&#24536;&#25506;&#27979;&#65288;&#26681;&#25454;&#23398;&#20064;&#30340;&#32447;&#24615;&#25506;&#27979;&#22120;&#21024;&#38500;&#29305;&#24449;&#65289;&#65292;&#24182;&#24341;&#20837;&#20102;&#36951;&#24536;-&#35760;&#24518;&#25506;&#27979;&#21464;&#20307;&#65288;&#38500;&#20102;&#25506;&#27979;&#22120;&#36873;&#25321;&#30340;&#29305;&#24449;&#22806;&#65292;&#36951;&#24536;&#25152;&#26377;&#32500;&#24230;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#27010;&#36848;&#20102;&#19968;&#20123;&#21487;&#33021;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probing strategies have been shown to detect the presence of various linguistic features in large language models; in particular, semantic features intermediate to the "natural logic" fragment of the Natural Language Inference task (NLI). In the case of natural logic, the relation between the intermediate features and the entailment label is explicitly known: as such, this provides a ripe setting for interventional studies on the NLI models' representations, allowing for stronger causal conjectures and a deeper critical analysis of interventional probing methods. In this work, we carry out new and existing representation-level interventions to investigate the effect of these semantic features on NLI classification: we perform amnesic probing (which removes features as directed by learned linear probes) and introduce the mnestic probing variation (which forgets all dimensions except the probe-selected ones). Furthermore, we delve into the limitations of these methods and outline some pi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#21019;&#24314;&#34913;&#37327;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#12290;&#20182;&#20204;&#20351;&#29992;&#20102;&#34892;&#20026;&#35266;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#38382;&#39064;&#20197;&#34913;&#37327;&#19981;&#21516;&#27700;&#24179;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27700;&#24179;&#21644;&#26041;&#27861;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.10327</link><description>&lt;p&gt;
&#21521;&#30528;&#20154;&#31867;&#21644;&#26426;&#22120;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards a Benchmark for Scientific Understanding in Humans and Machines. (arXiv:2304.10327v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10327
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#21019;&#24314;&#34913;&#37327;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#12290;&#20182;&#20204;&#20351;&#29992;&#20102;&#34892;&#20026;&#35266;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#38382;&#39064;&#20197;&#34913;&#37327;&#19981;&#21516;&#27700;&#24179;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27700;&#24179;&#21644;&#26041;&#27861;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#29702;&#35299;&#26159;&#31185;&#23398;&#30340;&#22522;&#26412;&#30446;&#26631;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#35299;&#37322;&#19990;&#30028;&#12290;&#30446;&#21069;&#36824;&#27809;&#26377;&#22909;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#20195;&#29702;&#20154;&#30340;&#31185;&#23398;&#29702;&#35299;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#20154;&#31867;&#36824;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#32570;&#20047;&#28165;&#26224;&#30340;&#22522;&#20934;&#65292;&#38590;&#20197;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27700;&#24179;&#21644;&#26041;&#27861;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#22312;&#27492;&#36335;&#32447;&#22270;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#31185;&#23398;&#21746;&#23398;&#24037;&#20855;&#21019;&#24314;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#37319;&#29992;&#34892;&#20026;&#35266;&#24565;&#65292;&#35748;&#20026;&#30495;&#27491;&#30340;&#29702;&#35299;&#24212;&#35813;&#34987;&#35748;&#20026;&#26159;&#25191;&#34892;&#26576;&#20123;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#32452;&#38382;&#39064;&#26469;&#25193;&#23637;&#36825;&#20010;&#27010;&#24565;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#34913;&#37327;&#19981;&#21516;&#27700;&#24179;&#30340;&#31185;&#23398;&#29702;&#35299;&#65292;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#65292;&#23433;&#25490;&#20449;&#24687;&#20197;&#29983;&#25104;&#35299;&#37322;&#30340;&#33021;&#21147;&#20197;&#21450;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#25512;&#26029;&#20107;&#29289;&#20250;&#26377;&#21738;&#20123;&#19981;&#21516;&#12290;Scientific Understanding Benchmark&#65288;SUB&#65289;&#30001;
&lt;/p&gt;
&lt;p&gt;
Scientific understanding is a fundamental goal of science, allowing us to explain the world. There is currently no good way to measure the scientific understanding of agents, whether these be humans or Artificial Intelligence systems. Without a clear benchmark, it is challenging to evaluate and compare different levels of and approaches to scientific understanding. In this Roadmap, we propose a framework to create a benchmark for scientific understanding, utilizing tools from philosophy of science. We adopt a behavioral notion according to which genuine understanding should be recognized as an ability to perform certain tasks. We extend this notion by considering a set of questions that can gauge different levels of scientific understanding, covering information retrieval, the capability to arrange information to produce an explanation, and the ability to infer how things would be different under different circumstances. The Scientific Understanding Benchmark (SUB), which is formed by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36328;&#27169;&#24577;&#22810;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36328;&#27169;&#24577;&#22810;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26174;&#24335;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#22312;E2E-ST&#21644;MT&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.10309</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#27169;&#24577;&#22810;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Improving Speech Translation by Cross-Modal Multi-Grained Contrastive Learning. (arXiv:2304.10309v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36328;&#27169;&#24577;&#22810;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36328;&#27169;&#24577;&#22810;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26174;&#24335;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#22312;E2E-ST&#21644;MT&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20302;&#24310;&#36831;&#21644;&#35823;&#24046;&#20256;&#25773;&#23569;&#65292;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65288;E2E-ST&#65289;&#27169;&#22411;&#24050;&#25104;&#20026;&#20027;&#27969;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20219;&#21153;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#24182;&#19981;&#23481;&#26131;&#12290;&#30001;&#20110;&#35821;&#38899;&#21644;&#25991;&#26412;MODALITY&#30340;&#24046;&#24322;&#65292;E2E-ST&#27169;&#22411;&#30340;&#24615;&#33021;&#36890;&#24120;&#27604;&#30456;&#24212;&#30340;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#27169;&#22411;&#31245;&#36874;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#26045;&#21152;&#21508;&#31181;&#32422;&#26463;&#26469;&#20351;&#29992;&#20849;&#20139;&#26426;&#21046;&#36827;&#34892;&#38544;&#24335;&#30693;&#35782;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#26368;&#32456;&#30340;&#27169;&#22411;&#22312;MT&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#24448;&#24448;&#27604;&#21333;&#29420;&#35757;&#32451;&#30340;MT&#27169;&#22411;&#36824;&#35201;&#24046;&#65292;&#36825;&#24847;&#21619;&#30528;&#36825;&#31181;&#26041;&#27861;&#30340;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#20063;&#26159;&#26377;&#38480;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;E2E-ST&#30340;FCCL&#65288;Fine- and Coarse- Granularity Contrastive Learning&#65289;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36328;&#27169;&#24577;&#22810;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26174;&#24335;&#30693;&#35782;&#36716;&#31227;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#22312;&#22810;&#20010;&#31890;&#24230;&#32423;&#21035;&#19978;&#23545;&#32534;&#30721;&#22120;&#36755;&#20986;&#21644;&#35299;&#30721;&#22120;&#36755;&#20837;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#35821;&#38899;&#32534;&#30721;&#22120;&#36755;&#20986;&#21644;&#25991;&#26412;&#35299;&#30721;&#22120;&#36755;&#20837;&#36827;&#34892;&#31934;&#32454;&#21644;&#31895;&#31890;&#24230;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25903;&#25345;E2E-ST&#21644;MT&#27169;&#22411;&#21516;&#26102;&#20248;&#21270;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;E2E-ST&#20219;&#21153;&#21644;MT&#20219;&#21153;&#19978;&#22343;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;MuST-C&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The end-to-end speech translation (E2E-ST) model has gradually become a mainstream paradigm due to its low latency and less error propagation. However, it is non-trivial to train such a model well due to the task complexity and data scarcity. The speech-and-text modality differences result in the E2E-ST model performance usually inferior to the corresponding machine translation (MT) model. Based on the above observation, existing methods often use sharingmechanisms to carry out implicit knowledge transfer by imposing various constraints. However, the final model often performs worse on the MT task than the MT model trained alone, which means that the knowledge transfer ability of this method is also limited. To deal with these problems, we propose the FCCL (Fine- and Coarse- Granularity Contrastive Learning) approach for E2E-ST, which makes explicit knowledge transfer through cross-modal multi-grained contrastive learning. A key ingredient of our approach is applying contrastive learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20174;&#25968;&#25454;&#35282;&#24230;&#25552;&#39640;&#20102;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#32780;&#19981;&#38656;&#35201;&#36716;&#24405;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#35299;&#31163;&#20102;&#38750;&#21442;&#25968;&#30693;&#35782;&#33976;&#39311;&#65292;&#36890;&#36807;&#26500;&#24314;&#25945;&#24072;&#20998;&#24067;&#26469;&#36798;&#21040;&#30693;&#35782;&#36716;&#31227;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.10295</link><description>&lt;p&gt;
&#35299;&#31163;&#38750;&#21442;&#25968;&#30693;&#35782;&#33976;&#39311;&#26469;&#36798;&#21040;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Decouple Non-parametric Knowledge Distillation For End-to-end Speech Translation. (arXiv:2304.10295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20174;&#25968;&#25454;&#35282;&#24230;&#25552;&#39640;&#20102;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#32780;&#19981;&#38656;&#35201;&#36716;&#24405;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#35299;&#31163;&#20102;&#38750;&#21442;&#25968;&#30693;&#35782;&#33976;&#39311;&#65292;&#36890;&#36807;&#26500;&#24314;&#25945;&#24072;&#20998;&#24067;&#26469;&#36798;&#21040;&#30693;&#35782;&#36716;&#31227;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#25216;&#26415;&#36890;&#24120;&#23581;&#35797;&#36890;&#36807;&#19968;&#20123;&#22797;&#26434;&#30340;&#25216;&#26415;&#65292;&#23558;&#24378;&#22823;&#30340;&#26426;&#22120;&#32763;&#35793;(MT)&#21521;&#35821;&#38899;&#32763;&#35793;(ST)&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#65292;&#20294;&#36825;&#24448;&#24448;&#38656;&#35201;&#22312;&#35757;&#32451;&#26399;&#38388;&#20316;&#20026;&#39069;&#22806;&#36755;&#20837;&#26469;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#24182;&#19981;&#24635;&#26159;&#26377;&#36716;&#24405;&#25968;&#25454;&#21487;&#29992;&#65292;&#22914;&#20309;&#22312;&#27809;&#26377;&#36716;&#24405;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;ST&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#25968;&#25454;&#25928;&#29575;&#65292;&#22312;&#25991;&#29486;&#20013;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#25454;&#35282;&#24230;&#25552;&#20986;&#20102;&#35299;&#31163;&#38750;&#21442;&#25968;&#30693;&#35782;&#33976;&#39311;(DNKD)&#26469;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36981;&#24490;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#26500;&#24314;&#23427;&#30340;&#25945;&#24072;&#20998;&#24067;&#65292;&#32780;&#19981;&#26159;&#20174;&#22797;&#26434;&#30340;MT&#27169;&#22411;&#20013;&#33719;&#24471;&#23427;&#65292;&#32780;&#26159;&#36890;&#36807;kNN&#26816;&#32034;&#20174;&#38750;&#21442;&#25968;&#25968;&#25454;&#23384;&#20648;&#20013;&#33719;&#24471;&#23427;&#65292;&#36825;&#28040;&#38500;&#20102;&#23545;&#36716;&#24405;&#21644;MT&#27169;&#22411;&#30340;&#20381;&#36182;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#32463;&#20856;&#30340;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#20998;&#20026;&#30446;&#26631;&#21644;&#38750;&#30446;&#26631;&#33976;&#39311;&#65292;&#20197;&#22686;&#24378;&#38750;&#30446;&#26631;logit&#20043;&#38388;&#30340;&#30693;&#35782;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing techniques often attempt to make knowledge transfer from a powerful machine translation (MT) to speech translation (ST) model with some elaborate techniques, which often requires transcription as extra input during training. However, transcriptions are not always available, and how to improve the ST model performance without transcription, i.e., data efficiency, has rarely been studied in the literature. In this paper, we propose Decoupled Non-parametric Knowledge Distillation (DNKD) from data perspective to improve the data efficiency. Our method follows the knowledge distillation paradigm. However, instead of obtaining the teacher distribution from a sophisticated MT model, we construct it from a non-parametric datastore via k-Nearest-Neighbor (kNN) retrieval, which removes the dependence on transcription and MT model. Then we decouple the classic knowledge distillation loss into target and non-target distillation to enhance the effect of the knowledge among non-target logit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#35843;&#25972;&#20998;&#31867;&#22120;&#25130;&#26029;&#28857;&#32780;&#19981;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#31867;&#20284;&#20110;&#36807;&#37319;&#26679;&#25216;&#26415;&#30340;&#32467;&#26524;&#65292;&#20026;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.10283</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#23545;&#19981;&#24179;&#34913;&#25991;&#26412;&#25968;&#25454;&#38598;&#39044;&#27979;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is augmentation effective to improve prediction in imbalanced text datasets?. (arXiv:2304.10283v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#35843;&#25972;&#20998;&#31867;&#22120;&#25130;&#26029;&#28857;&#32780;&#19981;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#31867;&#20284;&#20110;&#36807;&#37319;&#26679;&#25216;&#26415;&#30340;&#32467;&#26524;&#65292;&#20026;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#24448;&#24448;&#23548;&#33268;&#39044;&#27979;&#26377;&#20559;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#24191;&#27867;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20026;&#23569;&#25968;&#31867;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#20102;&#25968;&#25454;&#22686;&#24378;&#24635;&#26159;&#24517;&#35201;&#26469;&#25552;&#39640;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#39044;&#27979;&#30340;&#24120;&#35265;&#20551;&#35774;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#35843;&#25972;&#20998;&#31867;&#22120;&#25130;&#26029;&#28857;&#32780;&#19981;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#20135;&#29983;&#19982;&#36807;&#37319;&#26679;&#25216;&#26415;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#25903;&#25345;&#36825;&#19968;&#20027;&#24352;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#20102;&#35299;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#19981;&#21516;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#20026;&#32473;&#23450;&#20219;&#21153;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imbalanced datasets present a significant challenge for machine learning models, often leading to biased predictions. To address this issue, data augmentation techniques are widely used in natural language processing (NLP) to generate new samples for the minority class. However, in this paper, we challenge the common assumption that data augmentation is always necessary to improve predictions on imbalanced datasets. Instead, we argue that adjusting the classifier cutoffs without data augmentation can produce similar results to oversampling techniques. Our study provides theoretical and empirical evidence to support this claim. Our findings contribute to a better understanding of the strengths and limitations of different approaches to dealing with imbalanced data, and help researchers and practitioners make informed decisions about which methods to use for a given task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#21019;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#25163;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#23558;&#21360;&#24230;&#25163;&#35821;&#36716;&#25442;&#20026;&#25991;&#26412;&#25110;&#35821;&#38899;&#12290;CNN&#27169;&#22411;&#23545;&#20110;&#38745;&#24577;&#25163;&#35821;&#30340;&#35782;&#21035;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#26159;&#36890;&#36807;&#30417;&#25511;&#21160;&#24577;&#25163;&#21183;&#65292;LSTM&#27169;&#22411;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.10256</link><description>&lt;p&gt;
&#20351;&#29992;Mediapipe Holistic&#35782;&#21035;&#21360;&#24230;&#25163;&#35821;
&lt;/p&gt;
&lt;p&gt;
Indian Sign Language Recognition Using Mediapipe Holistic. (arXiv:2304.10256v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#21019;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#25163;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#23558;&#21360;&#24230;&#25163;&#35821;&#36716;&#25442;&#20026;&#25991;&#26412;&#25110;&#35821;&#38899;&#12290;CNN&#27169;&#22411;&#23545;&#20110;&#38745;&#24577;&#25163;&#35821;&#30340;&#35782;&#21035;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#26159;&#36890;&#36807;&#30417;&#25511;&#21160;&#24577;&#25163;&#21183;&#65292;LSTM&#27169;&#22411;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32843;&#21713;&#20154;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#38754;&#20020;&#30528;&#27807;&#36890;&#38556;&#30861;&#12290;&#20182;&#20204;&#21548;&#19981;&#21040;&#22768;&#38899;&#65292;&#19982;&#19981;&#25026;&#25163;&#35821;&#30340;&#20154;&#27807;&#36890;&#22256;&#38590;&#65292;&#20174;&#32780;&#22312;&#25945;&#32946;&#12289;&#32844;&#19994;&#21644;&#31038;&#20132;&#31561;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#25216;&#26415;&#21487;&#20197;&#25552;&#20379;&#26367;&#20195;&#30340;&#27807;&#36890;&#28192;&#36947;&#65292;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#19968;&#31181;&#26377;&#21161;&#20110;&#32843;&#20154;&#19982;&#21548;&#21147;&#27491;&#24120;&#20154;&#36827;&#34892;&#27807;&#36890;&#30340;&#25216;&#26415;&#26159;&#25163;&#35821;&#35782;&#21035;&#12290;&#25105;&#20204;&#23558;&#21019;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#25163;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#23558;&#21360;&#24230;&#25163;&#35821;&#36716;&#25442;&#20026;&#25991;&#26412;&#25110;&#35821;&#38899;&#12290;&#25105;&#20204;&#23558;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#65292;&#24182;&#27604;&#36739;CNN&#21644;LSTM&#27169;&#22411;&#12290;&#30001;&#20110;&#23384;&#22312;&#38745;&#24577;&#21644;&#21160;&#24577;&#25163;&#35821;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#21306;&#20998;&#23427;&#20204;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;CNN&#27169;&#22411;&#27604;LSTM&#27169;&#22411;&#26356;&#33021;&#25429;&#25417;&#23383;&#27597;&#21644;&#23383;&#31526;&#65292;&#26356;&#36866;&#29992;&#20110;&#38745;&#24577;&#25163;&#35821;&#30340;&#35782;&#21035;&#65292;&#20294;&#36890;&#36807;&#30417;&#25511;&#25163;&#21183;&#65292;&#23427;&#21448;&#36229;&#36234;&#20102;CNN&#12290;
&lt;/p&gt;
&lt;p&gt;
Deaf individuals confront significant communication obstacles on a daily basis. Their inability to hear makes it difficult for them to communicate with those who do not understand sign language. Moreover, it presents difficulties in educational, occupational, and social contexts. By providing alternative communication channels, technology can play a crucial role in overcoming these obstacles. One such technology that can facilitate communication between deaf and hearing individuals is sign language recognition. We will create a robust system for sign language recognition in order to convert Indian Sign Language to text or speech. We will evaluate the proposed system and compare CNN and LSTM models. Since there are both static and gesture sign languages, a robust model is required to distinguish between them. In this study, we discovered that a CNN model captures letters and characters for recognition of static sign language better than an LSTM model, but it outperforms CNN by monitorin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;K-center&#23545;&#27604;&#23398;&#20064;&#21644;&#21487;&#35843;&#20915;&#31574;&#36793;&#30028;&#23398;&#20064;(CLAB)&#30340;&#24320;&#25918;&#24847;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#24471;&#21040;&#30693;&#35782;&#36801;&#31227;&#65292;&#24182;&#29992;K-center&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#20986;&#21306;&#20998;&#24615;&#21644;&#24179;&#34913;&#24615;&#30340;&#24847;&#22270;&#29305;&#24449;&#65292;&#21516;&#26102;&#20351;&#29992;&#21487;&#35843;&#36793;&#30028;&#23398;&#20064;&#26041;&#27861;&#30830;&#23450;&#21512;&#36866;&#30340;&#20915;&#31574;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.10220</link><description>&lt;p&gt;
&#22522;&#20110;K-center&#23545;&#27604;&#23398;&#20064;&#21644;&#21487;&#35843;&#20915;&#31574;&#36793;&#30028;&#30340;&#26377;&#25928;&#24320;&#25918;&#24847;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Effective Open Intent Classification with K-center Contrastive Learning and Adjustable Decision Boundary. (arXiv:2304.10220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;K-center&#23545;&#27604;&#23398;&#20064;&#21644;&#21487;&#35843;&#20915;&#31574;&#36793;&#30028;&#23398;&#20064;(CLAB)&#30340;&#24320;&#25918;&#24847;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#24471;&#21040;&#30693;&#35782;&#36801;&#31227;&#65292;&#24182;&#29992;K-center&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#20986;&#21306;&#20998;&#24615;&#21644;&#24179;&#34913;&#24615;&#30340;&#24847;&#22270;&#29305;&#24449;&#65292;&#21516;&#26102;&#20351;&#29992;&#21487;&#35843;&#36793;&#30028;&#23398;&#20064;&#26041;&#27861;&#30830;&#23450;&#21512;&#36866;&#30340;&#20915;&#31574;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24847;&#22270;&#20998;&#31867;&#26088;&#22312;&#23558;&#24050;&#30693;&#24847;&#22270;&#27491;&#30830;&#20998;&#31867;&#21040;&#23427;&#20204;&#30456;&#24212;&#30340;&#31867;&#21035;&#20013;&#65292;&#21516;&#26102;&#35782;&#21035;&#26032;&#30340;&#26410;&#30693;(&#24320;&#25918;)&#24847;&#22270;&#65292;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#26159;&#19968;&#39033;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;K-center&#23545;&#27604;&#23398;&#20064;&#21644;&#21487;&#35843;&#20915;&#31574;&#36793;&#30028;&#23398;&#20064;(CLAB)&#26469;&#25552;&#39640;&#24320;&#25918;&#24847;&#22270;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open intent classification, which aims to correctly classify the known intents into their corresponding classes while identifying the new unknown (open) intents, is an essential but challenging task in dialogue systems. In this paper, we introduce novel K-center contrastive learning and adjustable decision boundary learning (CLAB) to improve the effectiveness of open intent classification. First, we pre-train a feature encoder on the labeled training instances, which transfers knowledge from known intents to unknown intents. Specifically, we devise a K-center contrastive learning algorithm to learn discriminative and balanced intent features, improving the generalization of the model for recognizing open intents. Second, we devise an adjustable decision boundary learning method with expanding and shrinking (ADBES) to determine the suitable decision conditions. Concretely, we learn a decision boundary for each known intent class, which consists of a decision center and the radius of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992; Paracrawl &#24179;&#34892;&#35821;&#26009;&#24211;&#25552;&#21462;&#24179;&#34892;&#27573;&#33853;&#35757;&#32451;&#25991;&#26723;&#32423;&#32763;&#35793;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10216</link><description>&lt;p&gt;
&#25506;&#31350;&#22522;&#20110; Paracrawl &#30340;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Exploring Paracrawl for Document-level Neural Machine Translation. (arXiv:2304.10216v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992; Paracrawl &#24179;&#34892;&#35821;&#26009;&#24211;&#25552;&#21462;&#24179;&#34892;&#27573;&#33853;&#35757;&#32451;&#25991;&#26723;&#32423;&#32763;&#35793;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#24050;&#32463;&#36229;&#36807;&#20102;&#21477;&#23376;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#36890;&#29992;&#39046;&#22495;&#30340;&#25991;&#26723;&#32423;&#35757;&#32451;&#25968;&#25454;&#65292;&#25991;&#26723;&#32423;&#32763;&#35793;&#20173;&#28982;&#27809;&#26377;&#34987;&#24191;&#27867;&#37319;&#29992;&#20110;&#23454;&#38469;&#30340;&#32763;&#35793;&#31995;&#32479;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992; Paracrawl &#26469;&#23398;&#20064;&#25991;&#26723;&#32423;&#32763;&#35793;&#30340;&#26377;&#25928;&#24615;&#12290;Paracrawl &#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#20174;&#20114;&#32852;&#32593;&#19978;&#29228;&#21462;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;&#21508;&#31181;&#39046;&#22495;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#21477;&#23376;&#23545;&#40784;&#20174; Paracrawl &#24179;&#34892;&#32593;&#39029;&#20013;&#25552;&#21462;&#24179;&#34892;&#27573;&#33853;&#65292;&#24182;&#23558;&#25552;&#21462;&#30340;&#24179;&#34892;&#27573;&#33853;&#29992;&#20316;&#35757;&#32451;&#25991;&#26723;&#32423;&#32763;&#35793;&#27169;&#22411;&#30340;&#24179;&#34892;&#25991;&#26723;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;&#26469;&#33258; Paracrawl &#30340;&#24179;&#34892;&#27573;&#33853;&#35757;&#32451;&#30340;&#25991;&#26723;&#32423; NMT &#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level neural machine translation (NMT) has outperformed sentence-level NMT on a number of datasets. However, document-level NMT is still not widely adopted in real-world translation systems mainly due to the lack of large-scale general-domain training data for document-level NMT. We examine the effectiveness of using Paracrawl for learning document-level translation. Paracrawl is a large-scale parallel corpus crawled from the Internet and contains data from various domains. The official Paracrawl corpus was released as parallel sentences (extracted from parallel webpages) and therefore previous works only used Paracrawl for learning sentence-level translation. In this work, we extract parallel paragraphs from Paracrawl parallel webpages using automatic sentence alignments and we use the extracted parallel paragraphs as parallel documents for training document-level translation models. We show that document-level NMT models trained with only parallel paragraphs from Paracrawl c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27573;&#33853;&#26816;&#32034;&#20013;&#37319;&#29992;&#22522;&#20110;&#25991;&#26412;&#19987;&#23478;&#28151;&#21512;&#30340;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#36827;&#23884;&#20837;&#31354;&#38388;&#30340;&#21028;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.10195</link><description>&lt;p&gt;
CoT-MoTE&#65306;&#25506;&#32034;&#22522;&#20110;&#25991;&#26412;&#19987;&#23478;&#28151;&#21512;&#30340;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#22312;&#27573;&#33853;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval. (arXiv:2304.10195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27573;&#33853;&#26816;&#32034;&#20013;&#37319;&#29992;&#22522;&#20110;&#25991;&#26412;&#19987;&#23478;&#28151;&#21512;&#30340;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#36827;&#23884;&#20837;&#31354;&#38388;&#30340;&#21028;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27573;&#33853;&#26816;&#32034;&#26088;&#22312;&#20174;&#22823;&#35268;&#27169;&#24320;&#25918;&#24335;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#27573;&#33853;&#12290;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#22312;&#21333;&#20307;&#21452;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#29942;&#39048;&#39044;&#35757;&#32451;&#20013;&#35777;&#26126;&#26377;&#25928;&#65292;&#24182;&#24120;&#24120;&#34987;&#37319;&#29992;&#20026;&#22522;&#26412;&#30340;&#26816;&#32034;&#26550;&#26500;&#65292;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#20013;&#23558;&#26597;&#35810;&#21644;&#27573;&#33853;&#32534;&#30721;&#20026;&#23427;&#20204;&#30340;&#28508;&#22312;&#23884;&#20837;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#20849;&#20139;&#25110;&#20998;&#31163;&#21452;&#32534;&#30721;&#22120;&#30340;&#21442;&#25968;&#20250;&#23548;&#33268;&#23884;&#20837;&#31354;&#38388;&#30340;&#19981;&#24179;&#34913;&#21028;&#21035;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#20808;&#35757;&#32451;&#20855;&#26377;&#25991;&#26412;&#19987;&#23478;&#28151;&#21512;&#30340;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;CoT-MoTE&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20026;&#26597;&#35810;&#21644;&#27573;&#33853;&#30340;&#19981;&#21516;&#23646;&#24615;&#20998;&#21035;&#32534;&#30721;&#25991;&#26412;&#29305;&#23450;&#30340;&#19987;&#23478;&#12290;&#21516;&#26102;&#65292;&#20173;&#20445;&#30041;&#19968;&#20010;&#20849;&#20139;&#30340;&#33258;&#25105;&#27880;&#24847;&#23618;&#65292;&#29992;&#20110;&#32479;&#19968;&#30340;&#27880;&#24847;&#24314;&#27169;&#12290;&#23545;&#22823;&#35268;&#27169;&#27573;&#33853;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#30340;&#32467;&#26524;&#26174;&#31034;&#31283;&#23450;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passage retrieval aims to retrieve relevant passages from large collections of the open-domain corpus. Contextual Masked Auto-Encoding has been proven effective in representation bottleneck pre-training of a monolithic dual-encoder for passage retrieval. Siamese or fully separated dual-encoders are often adopted as basic retrieval architecture in the pre-training and fine-tuning stages for encoding queries and passages into their latent embedding spaces. However, simply sharing or separating the parameters of the dual-encoder results in an imbalanced discrimination of the embedding spaces. In this work, we propose to pre-train Contextual Masked Auto-Encoder with Mixture-of-Textual-Experts (CoT-MoTE). Specifically, we incorporate textual-specific experts for individually encoding the distinct properties of queries and passages. Meanwhile, a shared self-attention layer is still kept for unified attention modeling. Results on large-scale passage retrieval benchmarks show steady improvemen
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;FOMC&#23448;&#26041;&#22768;&#26126;&#20013;&#20351;&#29992;&#30340;&#35821;&#35328;&#65292;&#37319;&#29992;VADER&#21644;FinBERT&#31561;&#27169;&#22411;&#39044;&#27979;&#36127;&#38754;&#24773;&#32490;&#65292;&#32467;&#26524;&#26174;&#31034;FinBERT&#34920;&#29616;&#30456;&#23545;&#26356;&#22909;&#12290;&#20294;&#26159;&#65292;&#35813;&#30740;&#31350;&#20063;&#24378;&#35843;&#20102;&#20351;&#29992;&#24403;&#21069;NLP&#25216;&#26415;&#20998;&#26512;FOMC&#25991;&#26412;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24314;&#35758;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#24182;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10164</link><description>&lt;p&gt;
&#20998;&#26512;FOMC&#20250;&#35758;&#35760;&#24405;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing FOMC Minutes: Accuracy and Constraints of Language Models. (arXiv:2304.10164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;FOMC&#23448;&#26041;&#22768;&#26126;&#20013;&#20351;&#29992;&#30340;&#35821;&#35328;&#65292;&#37319;&#29992;VADER&#21644;FinBERT&#31561;&#27169;&#22411;&#39044;&#27979;&#36127;&#38754;&#24773;&#32490;&#65292;&#32467;&#26524;&#26174;&#31034;FinBERT&#34920;&#29616;&#30456;&#23545;&#26356;&#22909;&#12290;&#20294;&#26159;&#65292;&#35813;&#30740;&#31350;&#20063;&#24378;&#35843;&#20102;&#20351;&#29992;&#24403;&#21069;NLP&#25216;&#26415;&#20998;&#26512;FOMC&#25991;&#26412;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24314;&#35758;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#24182;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#20998;&#26512;&#20102;&#32852;&#37030;&#20844;&#24320;&#24066;&#22330;&#22996;&#21592;&#20250;&#65288;FOMC&#65289;&#22312;&#20854;&#23450;&#26399;&#20250;&#35758;&#21518;&#21457;&#24067;&#30340;&#23448;&#26041;&#22768;&#26126;&#20013;&#20351;&#29992;&#30340;&#35821;&#35328;&#65292;&#20197;&#33719;&#21462;&#26377;&#20851;FOMC&#23448;&#26041;&#22768;&#26126;&#23545;&#37329;&#34701;&#24066;&#22330;&#21644;&#32463;&#27982;&#39044;&#27979;&#30340;&#24433;&#21709;&#30340;&#35265;&#35299;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;FOMC&#23567;&#24515;&#36991;&#20813;&#22312;&#21477;&#23376;&#20013;&#34920;&#36798;&#24773;&#24863;&#65292;&#24182;&#36981;&#24490;&#19968;&#22871;&#27169;&#26495;&#26469;&#35206;&#30422;&#32463;&#27982;&#24773;&#20917;&#12290;&#35813;&#20998;&#26512;&#37319;&#29992;&#20102;VADER&#21644;FinBERT&#31561;&#20808;&#36827;&#30340;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#65292;&#20197;&#21450;&#20351;&#29992;GPT-4&#30340;&#35797;&#39564;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20934;&#30830;&#39044;&#27979;&#36127;&#38754;&#24773;&#32490;&#26041;&#38754;&#65292;FinBERT&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#36824;&#24378;&#35843;&#20102;&#20351;&#29992;&#24403;&#21069;NLP&#25216;&#26415;&#20998;&#26512;FOMC&#25991;&#26412;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#24314;&#35758;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#24182;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research article analyzes the language used in the official statements released by the Federal Open Market Committee (FOMC) after its scheduled meetings to gain insights into the impact of FOMC official statements on financial markets and economic forecasting. The study reveals that the FOMC is careful to avoid expressing emotion in their sentences and follows a set of templates to cover economic situations. The analysis employs advanced language modeling techniques such as VADER and FinBERT, and a trial test with GPT-4. The results show that FinBERT outperforms other techniques in predicting negative sentiment accurately. However, the study also highlights the challenges and limitations of using current NLP techniques to analyze FOMC texts and suggests the potential for enhancing language models and exploring alternative approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#26631;&#20934;&#21270;&#35821;&#35328;&#19978;&#30340;&#35789;&#24615;&#26631;&#27880;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#36890;&#36807;&#35843;&#25972;&#26631;&#35760;&#21270;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10158</link><description>&lt;p&gt;
&#21069;&#32622;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#35760;&#26041;&#27861;&#33021;&#21542;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#65311;&#19968;&#20010;&#38750;&#26631;&#20934;&#21270;&#35821;&#35328;&#35789;&#24615;&#26631;&#27880;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Does Manipulating Tokenization Aid Cross-Lingual Transfer? A Study on POS Tagging for Non-Standardized Languages. (arXiv:2304.10158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#26631;&#20934;&#21270;&#35821;&#35328;&#19978;&#30340;&#35789;&#24615;&#26631;&#27880;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#36890;&#36807;&#35843;&#25972;&#26631;&#35760;&#21270;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#65292;&#23427;&#20204;&#30340;&#26631;&#35760;&#22120;&#34429;&#28982;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20294;&#22312;&#22788;&#29702;&#20197;&#21069;&#27809;&#26377;&#35265;&#36807;&#30340;&#25968;&#25454;&#21464;&#21270;&#26102;&#23481;&#26131;&#20986;&#38382;&#39064;&#12290;&#24403;&#22312;&#19968;&#20010;&#35821;&#35328;&#19978;&#36827;&#34892;&#24494;&#35843;&#24182;&#22312;&#27809;&#26377;&#26631;&#20934;&#27491;&#23383;&#27861;&#30340;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#21464;&#20307;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#23613;&#31649;&#20004;&#31181;&#35821;&#35328;&#20855;&#26377;&#39640;&#24230;&#30456;&#20284;&#24615;&#65292;&#20294;&#26631;&#35760;&#21270;&#19981;&#20877;&#23545;&#30446;&#26631;&#25968;&#25454;&#30340;&#26377;&#24847;&#20041;&#34920;&#24449;&#30456;&#23545;&#24212;&#65292;&#23548;&#33268;&#37096;&#20998;&#35821;&#38899;&#26631;&#27880;&#31561;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24494;&#35843;&#20102;&#26469;&#33258;&#19977;&#20010;&#19981;&#21516;&#23478;&#26063;&#30340;&#19971;&#31181;&#35821;&#35328;&#30340; PLM&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#23494;&#20999;&#30456;&#20851;&#30340;&#38750;&#26631;&#20934;&#21270;&#35821;&#35328;&#21464;&#20307;&#19978;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#26631;&#35760;&#21270;&#28304;&#21644;&#30446;&#26631;&#25968;&#25454;&#24046;&#24322;&#24230;&#37327;&#26041;&#24335;&#20197;&#21450;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#35843;&#25972;&#23427;&#20204;&#30340;&#26041;&#24335;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#35843;&#25972;&#26631;&#35760;&#21270;&#26041;&#27861;&#21487;&#20197;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#25552;&#39640; PLM &#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges with finetuning pretrained language models (PLMs) is that their tokenizer is optimized for the language(s) it was pretrained on, but brittle when it comes to previously unseen variations in the data. This can for instance be observed when finetuning PLMs on one language and evaluating them on data in a closely related language variety with no standardized orthography. Despite the high linguistic similarity, tokenization no longer corresponds to meaningful representations of the target data, leading to low performance in, e.g., part-of-speech tagging.  In this work, we finetune PLMs on seven languages from three different families and analyze their zero-shot performance on closely related, non-standardized varieties. We consider different measures for the divergence in the tokenization of the source and target data, and the way they can be adjusted by manipulating the tokenization during the finetuning step. Overall, we find that the similarity between the percenta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#32852;&#24819;&#20559;&#24046;&#21644;&#23454;&#35777;&#20844;&#27491;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#29702;&#35770;&#23454;&#39564;&#21644;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#20004;&#32773;&#21487;&#20197;&#26159;&#29420;&#31435;&#30340;&#12290;&#26412;&#25991;&#21628;&#21505;&#37319;&#29992;&#23454;&#35777;&#20844;&#27491;&#30340;&#27169;&#22411;&#24320;&#21457;&#26041;&#27861;&#24182;&#20943;&#23569;&#34920;&#31034;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.10153</link><description>&lt;p&gt;
&#35770;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32852;&#24819;&#20559;&#24046;&#19982;&#23454;&#35777;&#20844;&#27491;&#30340;&#29420;&#31435;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Independence of Association Bias and Empirical Fairness in Language Models. (arXiv:2304.10153v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#32852;&#24819;&#20559;&#24046;&#21644;&#23454;&#35777;&#20844;&#27491;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#29702;&#35770;&#23454;&#39564;&#21644;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#20004;&#32773;&#21487;&#20197;&#26159;&#29420;&#31435;&#30340;&#12290;&#26412;&#25991;&#21628;&#21505;&#37319;&#29992;&#23454;&#35777;&#20844;&#27491;&#30340;&#27169;&#22411;&#24320;&#21457;&#26041;&#27861;&#24182;&#20943;&#23569;&#34920;&#31034;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#24433;&#21709;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#23427;&#20204;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#21463;&#20445;&#25252;&#23646;&#24615;&#21644;&#20215;&#20540;&#36127;&#36733;&#26415;&#35821;&#20043;&#38388;&#30340;&#24378;&#20851;&#32852;&#65292;&#20174;&#34065;&#31216;&#21040;&#20139;&#26377;&#22768;&#26395;&#30340;&#32844;&#20301;&#21517;&#31216;&#31561;&#12290;&#36825;&#26679;&#30340;&#24037;&#20316;&#34987;&#35748;&#20026;&#26159;&#25506;&#32034;&#27169;&#22411;&#30340;&#20559;&#24046;&#25110;&#20844;&#24179;&#24615;&#65292;&#25110;&#32773;&#36825;&#31181;&#25506;&#27979; "&#34920;&#24449;&#20559;&#24046; "&#30340;&#24037;&#20316;&#34987;&#35748;&#20026;&#26159; "&#22522;&#20110;&#20844;&#24179;&#24615;&#30340; "&#8212;&#8212;&#36825;&#34920;&#26126;&#20102;&#20559;&#24046;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#23494;&#20999;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#21306;&#20998;&#32852;&#24819;&#20559;&#24046;&#21644;&#23454;&#35777;&#20844;&#27491;&#26469;&#25552;&#20379;&#27010;&#24565;&#19978;&#30340;&#28165;&#26224;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#20004;&#32773;&#21487;&#20197;&#26159;&#29420;&#31435;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20102;&#20026;&#20160;&#20040;&#36825;&#19981;&#24212;&#35813;&#35753;&#20154;&#24863;&#21040;&#24778;&#35766;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#19968;&#20010;&#24605;&#24819;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#32852;&#24819;&#20559;&#35265;&#21644;&#23454;&#35777;&#20844;&#27491;&#21487;&#20197;&#23436;&#20840;&#29420;&#31435;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#32463;&#39564;&#35777;&#25454;&#65292;&#34920;&#26126;&#22312;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20559;&#24046;&#25351;&#26631;&#21644;&#20844;&#24179;&#25351;&#26631;&#20043;&#38388;&#19981;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#31038;&#20250;&#21644;&#24515;&#29702;&#23398;&#25991;&#29486;&#65292;&#21246;&#21202;&#20986;&#20102;&#32852;&#24819;&#20559;&#24046;&#22312;&#35821;&#35328;&#20351;&#29992;&#20013;&#30340;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#35299;&#20915;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#26368;&#21518;&#25552;&#20986;&#21628;&#21505;&#65292;&#20851;&#27880;&#38477;&#20302;&#34920;&#31034;&#20559;&#24046;&#21644;&#37319;&#29992;&#23454;&#35777;&#20844;&#27491;&#27169;&#22411;&#24320;&#21457;&#30340;&#20570;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The societal impact of pre-trained language models has prompted researchers to probe them for strong associations between protected attributes and value-loaded terms, from slur to prestigious job titles. Such work is said to probe models for bias or fairness-or such probes 'into representational biases' are said to be 'motivated by fairness'-suggesting an intimate connection between bias and fairness. We provide conceptual clarity by distinguishing between association biases (Caliskan et al., 2022) and empirical fairness (Shen et al., 2022) and show the two can be independent. Our main contribution, however, is showing why this should not come as a surprise. To this end, we first provide a thought experiment, showing how association bias and empirical fairness can be completely orthogonal. Next, we provide empirical evidence that there is no correlation between bias metrics and fairness metrics across the most widely used language models. Finally, we survey the sociological and psychol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#27880;&#37322;&#65292;&#32467;&#26524;&#34920;&#26126;ChatGPT&#26377;&#28508;&#21147;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#65292;&#23613;&#31649;&#20173;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.10145</link><description>&lt;p&gt;
ChatGPT&#33021;&#21542;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#65311;&#23545;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks. (arXiv:2304.10145v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#27880;&#37322;&#65292;&#32467;&#26524;&#34920;&#26126;ChatGPT&#26377;&#28508;&#21147;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#65292;&#23613;&#31649;&#20173;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#21457;&#24067;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#21462;&#20195;&#20154;&#31867;&#26234;&#24935;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;ChatGPT&#26159;&#21542;&#26377;&#28508;&#21147;&#22312;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#20013;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#27880;&#37322;&#12290;&#36825;&#26679;&#30340;&#25104;&#23601;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#31038;&#20132;&#35745;&#31639;&#30740;&#31350;&#30340;&#25104;&#26412;&#21644;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#37325;&#26032;&#26631;&#35760;&#20102;&#20116;&#20010;&#20855;&#26377;&#37324;&#31243;&#30865;&#24847;&#20041;&#30340;&#25968;&#25454;&#38598;&#65292;&#28041;&#21450;&#31435;&#22330;&#26816;&#27979;&#65288;2&#20010;&#65289;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#20167;&#24680;&#35328;&#35770;&#21644;&#26426;&#22120;&#20154;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#26377;&#28508;&#21147;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#65292;&#23613;&#31649;&#20173;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;ChatGPT&#33719;&#24471;&#20102;&#24179;&#22343;&#31934;&#24230;0.609&#12290; ChatGPT&#23545;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#26368;&#20339;&#65292;&#27491;&#30830;&#27880;&#37322;&#20102;64.9&#65285;&#30340;&#25512;&#25991;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26174;&#31034;&#24615;&#33021;&#22312;&#19981;&#21516;&#26631;&#31614;&#20043;&#38388;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#39033;&#24037;&#20316;&#21487;&#20197;&#24320;&#36767;&#26032;&#30340;&#20998;&#26512;&#32447;&#36335;&#65292;&#24182;&#20316;&#20026;&#26410;&#26469;&#21033;&#29992;ChatGPT&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of ChatGPT has uncovered a range of possibilities whereby large language models (LLMs) can substitute human intelligence. In this paper, we seek to understand whether ChatGPT has the potential to reproduce human-generated label annotations in social computing tasks. Such an achievement could significantly reduce the cost and complexity of social computing research. As such, we use ChatGPT to re-label five seminal datasets covering stance detection (2x), sentiment analysis, hate speech, and bot detection. Our results highlight that ChatGPT does have the potential to handle these data annotation tasks, although a number of challenges remain. ChatGPT obtains an average precision 0.609. Performance is highest for the sentiment analysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, we show that performance varies substantially across individual labels. We believe this work can open up new lines of analysis and act as a basis for future research into the exploi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#37319;&#35775;&#20197;&#21450;&#23545;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#25991;&#29486;&#30340;&#30740;&#31350;&#65292;&#22686;&#24378;&#20102;&#8220;AdaTest&#8221;&#23457;&#35745;&#24037;&#20855;&#65292;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20154;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#21327;&#21516;&#20248;&#21183;&#65292;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.09991</link><description>&lt;p&gt;
&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#23457;&#35745;LLM&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Supporting Human-AI Collaboration in Auditing LLMs with LLMs. (arXiv:2304.09991v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#37319;&#35775;&#20197;&#21450;&#23545;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#25991;&#29486;&#30340;&#30740;&#31350;&#65292;&#22686;&#24378;&#20102;&#8220;AdaTest&#8221;&#23457;&#35745;&#24037;&#20855;&#65292;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20154;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#21327;&#21516;&#20248;&#21183;&#65292;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#37096;&#32626;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#21644;&#26222;&#21450;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#35770;&#26159;&#29992;&#20110;&#20998;&#31867;&#36824;&#26159;&#29983;&#25104;&#65292;&#37117;&#34920;&#29616;&#20986;&#26377;&#20559;&#24046;&#21644;&#19981;&#36127;&#36131;&#20219;&#30340;&#34892;&#20026;&#65292;&#23545;&#20154;&#31867;&#36896;&#25104;&#20102;&#35268;&#27169;&#24615;&#30340;&#20260;&#23475;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20005;&#26684;&#23457;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#23457;&#35745;&#24037;&#20855;&#21033;&#29992;&#20154;&#21644;&#25110;AI&#26469;&#21457;&#29616;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#30340;&#25991;&#29486;&#65292;&#24182;&#37319;&#35775;&#20102;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#19987;&#23478;&#65292;&#20197;&#22686;&#24378;&#23457;&#35745;&#24037;&#20855;&#8220;AdaTest&#8221;&#65288;Ribeiro&#21644;Lundberg&#65292;2022&#65289;&#65292;&#35813;&#24037;&#20855;&#30001;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#12290;&#36890;&#36807;&#35774;&#35745;&#36807;&#31243;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#24863;&#30693;&#21644;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#22312;&#21327;&#20316;&#23457;&#35745;&#20013;&#21033;&#29992;&#20154;&#19982;&#29983;&#25104;&#27169;&#22411;&#30340;&#20114;&#34917;&#20248;&#21183;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#22686;&#24378;&#24037;&#20855;AdaTest ++&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#65292;&#20351;&#21442;&#19982;&#32773;&#36827;&#34892;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Large language models are becoming increasingly pervasive and ubiquitous in society via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously. Existing auditing tools leverage either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and conduct interviews with research experts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro and Lundberg, 2022), which is powered by a generative large language model (LLM). Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of the augmented tool, AdaTest++, we conduct user studies with participants audit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Radar de Parit\'e&#65292;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;NLP&#31995;&#32479;&#65292;&#26681;&#25454;&#20845;&#20010;&#21152;&#25343;&#22823;&#27861;&#35821;&#23186;&#20307;&#27599;&#22825;&#24341;&#29992;&#30340;&#22899;&#24615;&#21644;&#30007;&#24615;&#27604;&#20363;&#30340;&#25968;&#25454;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#35813;&#31995;&#32479;&#30340;&#26550;&#26500;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#27861;&#35821;&#35821;&#35328;&#20013;&#30340;&#25216;&#26415;&#38590;&#39064;&#65292;&#23637;&#31034;&#20102;&#22312;&#19968;&#24180;&#30340;&#25968;&#25454;&#32479;&#35745;&#20013;&#65292;&#26032;&#38395;&#25253;&#36947;&#20013;&#22899;&#24615;&#30340;&#20195;&#34920;&#24615;&#19981;&#36275;&#65292;&#22240;&#27492;&#35813;&#31995;&#32479;&#20026;&#25105;&#20204;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09982</link><description>&lt;p&gt;
Radar de Parit\'e: &#19968;&#31181;&#34913;&#37327;&#27861;&#35821;&#26032;&#38395;&#25253;&#36947;&#20013;&#24615;&#21035;&#20195;&#34920;&#24615;&#30340;NLP&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Radar de Parit\'e: An NLP system to measure gender representation in French news stories. (arXiv:2304.09982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Radar de Parit\'e&#65292;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;NLP&#31995;&#32479;&#65292;&#26681;&#25454;&#20845;&#20010;&#21152;&#25343;&#22823;&#27861;&#35821;&#23186;&#20307;&#27599;&#22825;&#24341;&#29992;&#30340;&#22899;&#24615;&#21644;&#30007;&#24615;&#27604;&#20363;&#30340;&#25968;&#25454;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#35813;&#31995;&#32479;&#30340;&#26550;&#26500;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#27861;&#35821;&#35821;&#35328;&#20013;&#30340;&#25216;&#26415;&#38590;&#39064;&#65292;&#23637;&#31034;&#20102;&#22312;&#19968;&#24180;&#30340;&#25968;&#25454;&#32479;&#35745;&#20013;&#65292;&#26032;&#38395;&#25253;&#36947;&#20013;&#22899;&#24615;&#30340;&#20195;&#34920;&#24615;&#19981;&#36275;&#65292;&#22240;&#27492;&#35813;&#31995;&#32479;&#20026;&#25105;&#20204;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Radar de Parit\'e&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#65292;&#29992;&#20110;&#34913;&#37327;&#20845;&#20010;&#21152;&#25343;&#22823;&#27861;&#35821;&#23186;&#20307;&#20013;&#27599;&#26085;&#24341;&#29992;&#30340;&#22899;&#24615;&#21644;&#30007;&#24615;&#30340;&#27604;&#20363;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#35813;&#31995;&#32479;&#30340;&#26550;&#26500;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#20811;&#26381;&#30340;&#27861;&#35821;&#29305;&#23450;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;coreference resolution&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#27861;&#35821;NLP&#25991;&#29486;&#30340;&#26032;&#36129;&#29486;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36229;&#36807;&#19968;&#24180;&#30340;&#25968;&#25454;&#32479;&#35745;&#65288;282,512&#31687;&#26032;&#38395;&#25991;&#31456;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#26032;&#38395;&#25253;&#36947;&#20013;&#22899;&#24615;&#30340;&#20195;&#34920;&#24615;&#19981;&#36275;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#36816;&#29992;&#29616;&#20195;NLP&#26041;&#27861;&#26469;&#34913;&#37327;&#24615;&#21035;&#20195;&#34920;&#24615;&#21644;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Radar de Parit\'e, an automated Natural Language Processing (NLP) system that measures the proportion of women and men quoted daily in six Canadian French-language media outlets. We outline the system's architecture and detail the challenges we overcame to address French-specific issues, in particular regarding coreference resolution, a new contribution to the NLP literature on French. We also showcase statistics covering over one year's worth of data (282,512 news articles). Our results highlight the underrepresentation of women in news stories, while also illustrating the application of modern NLP methods to measure gender representation and address societal issues.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;MasakhaNEWS&#65292;&#23427;&#26159;&#19968;&#20010;&#35206;&#30422;16&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;&#26032;&#38395;&#20027;&#39064;&#20998;&#31867;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#35780;&#20272;&#22522;&#32447;&#27169;&#22411;&#22806;&#65292;&#36824;&#25506;&#32034;&#20102;&#36866;&#29992;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20840;&#38754;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.09972</link><description>&lt;p&gt;
MasakhaNEWS&#65306;&#38750;&#27954;&#35821;&#35328;&#26032;&#38395;&#20027;&#39064;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MasakhaNEWS: News Topic Classification for African languages. (arXiv:2304.09972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09972
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;MasakhaNEWS&#65292;&#23427;&#26159;&#19968;&#20010;&#35206;&#30422;16&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;&#26032;&#38395;&#20027;&#39064;&#20998;&#31867;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#35780;&#20272;&#22522;&#32447;&#27169;&#22411;&#22806;&#65292;&#36824;&#25506;&#32034;&#20102;&#36866;&#29992;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20840;&#38754;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#35206;&#30422;&#22810;&#20010;NLP&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#38750;&#27954;&#35821;&#35328;&#22312;NLP&#30740;&#31350;&#20013;&#20005;&#37325;&#21463;&#21040;&#24573;&#35270;&#12290;&#34429;&#28982;&#23384;&#22312;&#19968;&#20123;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;NLP&#20219;&#21153;&#65288;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#26426;&#22120;&#32763;&#35793;&#65289;&#20855;&#26377;&#35206;&#30422;&#22810;&#20010;&#22320;&#29702;&#21644;&#20998;&#31867;&#22810;&#26679;&#30340;&#38750;&#27954;&#35821;&#35328;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;MasakhaNEWS - &#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#28085;&#30422;&#38750;&#27954;&#24191;&#27867;&#20351;&#29992;&#30340;16&#31181;&#35821;&#35328;&#30340;&#26032;&#38395;&#20027;&#39064;&#20998;&#31867;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#24494;&#35843;&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#26469;&#35780;&#20272;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19968;&#20123;&#36866;&#29992;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#24494;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20363;&#22914;&#36328;&#35821;&#35328;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;&#22914;MAD-X&#65289;&#12289;&#27169;&#24335;&#21033;&#29992;&#35757;&#32451;&#65288;PET&#65289;&#12289;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#21644;&#26080;&#25552;&#31034;&#21477;&#23376;&#35757;&#32451;&#65288;ELECTRA&#65289;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
African languages are severely under-represented in NLP research due to lack of datasets covering several NLP tasks. While there are individual language specific datasets that are being expanded to different tasks, only a handful of NLP tasks (e.g. named entity recognition and machine translation) have standardized benchmark datasets covering several geographical and typologically-diverse African languages. In this paper, we develop MasakhaNEWS -- a new benchmark dataset for news topic classification covering 16 languages widely spoken in Africa. We provide an evaluation of baseline models by training classical machine learning models and fine-tuning several language models. Furthermore, we explore several alternatives to full fine-tuning of language models that are better suited for zero-shot and few-shot learning such as cross-lingual parameter-efficient fine-tuning (like MAD-X), pattern exploiting training (PET), prompting language models (like ChatGPT), and prompt-free sentence tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.09960</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#28508;&#22312;&#31354;&#38388;&#29702;&#35770;&#23545;&#24212;&#26032;&#20852;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24182;&#19981;&#26159;&#38543;&#26426;&#29983;&#25104;&#65292;&#32780;&#26159;&#20026;&#20102;&#20256;&#36882;&#20449;&#24687;&#12290;&#35821;&#35328;&#19982;&#20854;&#24213;&#23618;&#21547;&#20041;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#20851;&#32852;&#65292;&#22312;&#20854;&#30456;&#20851;&#24615;&#26041;&#38754;&#26377;&#30528;&#20005;&#37325;&#20559;&#24046;&#30340;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#31232;&#30095;&#24615;&#65292;&#36825;&#20123;&#39640;&#23792;&#20540;&#24688;&#22909;&#19982;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#21305;&#37197;&#12290;&#38543;&#30528;&#22823;&#25968;&#25454;&#21644;&#22823;&#27169;&#22411;&#19978;&#35757;&#32451;&#30340;LLMs&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#29616;&#22312;&#21487;&#20197;&#31934;&#30830;&#35780;&#20272;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#25506;&#32034;&#32852;&#21512;&#20998;&#24067;&#31232;&#30095;&#32467;&#26500;&#23454;&#29616;&#26377;&#25928;&#25512;&#29702;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#20998;&#31867;&#20026;&#26126;&#30830;&#19982;{\epsilon}-&#27169;&#31946;&#65292;&#24182;&#25552;&#20986;&#23450;&#37327;&#32467;&#26524;&#65292;&#20197;&#34920;&#26126;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#65288;&#20363;&#22914;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#65289;&#37117;&#21487;&#20197;&#24402;&#22240;&#20110;&#23545;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21452;&#35821;&#26041;&#35328;&#35789;&#20856;&#24402;&#32435;&#30340;&#26041;&#27861;&#65292;&#23545;&#24503;&#35821;&#21450;&#20854;&#24052;&#20240;&#21033;&#20122;&#21644;&#38463;&#21202;&#26364;&#23612;&#20122;&#26041;&#35328;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#24212;&#23545;&#36164;&#28304;&#31232;&#32570;&#12289;&#35821;&#35328;&#30456;&#20851;&#24615;&#21644;&#25340;&#20889;&#26631;&#20934;&#21270;&#32570;&#22833;&#31561;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36755;&#20986;&#32467;&#26524;&#36890;&#36807;&#21333;&#35789;&#39057;&#29575;&#21644;&#25104;&#23545;&#32534;&#36753;&#36317;&#31163;&#36827;&#34892;&#35780;&#20272;&#65292;&#21516;&#26102;&#21457;&#24067;&#20102;&#21253;&#25324;1500&#20010;&#21452;&#35821;&#21477;&#23376;&#23545;&#21644;1000&#20010;&#21452;&#35821;&#21333;&#35789;&#23545;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.09957</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20302;&#36164;&#28304;&#21452;&#35821;&#26041;&#35328;&#35789;&#20856;&#24402;&#32435;
&lt;/p&gt;
&lt;p&gt;
Low-resource Bilingual Dialect Lexicon Induction with Large Language Models. (arXiv:2304.09957v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21452;&#35821;&#26041;&#35328;&#35789;&#20856;&#24402;&#32435;&#30340;&#26041;&#27861;&#65292;&#23545;&#24503;&#35821;&#21450;&#20854;&#24052;&#20240;&#21033;&#20122;&#21644;&#38463;&#21202;&#26364;&#23612;&#20122;&#26041;&#35328;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#24212;&#23545;&#36164;&#28304;&#31232;&#32570;&#12289;&#35821;&#35328;&#30456;&#20851;&#24615;&#21644;&#25340;&#20889;&#26631;&#20934;&#21270;&#32570;&#22833;&#31561;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36755;&#20986;&#32467;&#26524;&#36890;&#36807;&#21333;&#35789;&#39057;&#29575;&#21644;&#25104;&#23545;&#32534;&#36753;&#36317;&#31163;&#36827;&#34892;&#35780;&#20272;&#65292;&#21516;&#26102;&#21457;&#24067;&#20102;&#21253;&#25324;1500&#20010;&#21452;&#35821;&#21477;&#23376;&#23545;&#21644;1000&#20010;&#21452;&#35821;&#21333;&#35789;&#23545;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#35821;&#21333;&#35789;&#35789;&#20856;&#26159;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#21161;&#20110;&#23558;&#19968;&#31181;&#35821;&#35328;&#20013;&#30340;&#21333;&#35789;&#26144;&#23556;&#21040;&#21478;&#19968;&#31181;&#35821;&#35328;&#20013;&#30340;&#21516;&#20041;&#35789;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#24503;&#35821;&#21450;&#20854;&#20004;&#31181;&#26041;&#35328;&#65288;&#24052;&#20240;&#21033;&#20122;&#21644;&#38463;&#21202;&#26364;&#23612;&#20122;&#65289;&#30340;&#21452;&#35821;&#35789;&#20856;&#24402;&#32435;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#23384;&#22312;&#36164;&#28304;&#31232;&#32570;&#12289;&#35821;&#35328;&#30456;&#20851;&#24615;&#20197;&#21450;&#26041;&#35328;&#25340;&#20889;&#26631;&#20934;&#21270;&#32570;&#22833;&#31561;&#29420;&#29305;&#25361;&#25112;&#12290;&#20026;&#20102;&#35780;&#20272;&#36755;&#20986;&#32467;&#26524;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21333;&#35789;&#39057;&#29575;&#21644;&#25104;&#23545;&#32534;&#36753;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;1500&#20010;&#21452;&#35821;&#21477;&#23376;&#23545;&#21644;1000&#20010;&#21452;&#35821;&#21333;&#35789;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilingual word lexicons are crucial tools for multilingual natural language understanding and machine translation tasks, as they facilitate the mapping of words in one language to their synonyms in another language. To achieve this, numerous papers have explored bilingual lexicon induction (BLI) in high-resource scenarios, using a typical pipeline consisting of two unsupervised steps: bitext mining and word alignment, both of which rely on pre-trained large language models~(LLMs).  In this paper, we present an analysis of the BLI pipeline for German and two of its dialects, Bavarian and Alemannic. This setup poses several unique challenges, including the scarcity of resources, the relatedness of the languages, and the lack of standardization in the orthography of dialects. To evaluate the BLI outputs, we analyze them with respect to word frequency and pairwise edit distance. Additionally, we release two evaluation datasets comprising 1,500 bilingual sentence pairs and 1,000 bilingual w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;GPT-3&#27169;&#22411;&#65292;&#27604;&#20197;&#24448;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#24615;&#33021;&#26356;&#20986;&#33394;&#12290;&#21516;&#26102;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#34394;&#20551;&#35780;&#35770;&#19982;&#30495;&#23454;&#35780;&#35770;&#22312;&#35821;&#35328;&#34920;&#36798;&#19978;&#23384;&#22312;&#30340;&#24046;&#24322;&#65292;&#20998;&#26512;&#20102;&#34394;&#20551;&#35780;&#35770;&#30340;&#27169;&#24335;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.09948</link><description>&lt;p&gt;
&#25235;&#20303;&#20320;&#30340;&#35805;&#35821;&#65306;&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#35782;&#21035;&#34394;&#20551;&#21307;&#24072;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Catch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers. (arXiv:2304.09948v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;GPT-3&#27169;&#22411;&#65292;&#27604;&#20197;&#24448;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#24615;&#33021;&#26356;&#20986;&#33394;&#12290;&#21516;&#26102;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#34394;&#20551;&#35780;&#35770;&#19982;&#30495;&#23454;&#35780;&#35770;&#22312;&#35821;&#35328;&#34920;&#36798;&#19978;&#23384;&#22312;&#30340;&#24046;&#24322;&#65292;&#20998;&#26512;&#20102;&#34394;&#20551;&#35780;&#35770;&#30340;&#27169;&#24335;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#29983;&#34394;&#20551;&#35780;&#35770;&#30340;&#27867;&#28389;&#21487;&#33021;&#20250;&#23545;&#24739;&#32773;&#31119;&#21033;&#20135;&#29983;&#28508;&#22312;&#21361;&#23475;&#65292;&#24182;&#24341;&#36215;&#28040;&#36153;&#32773;&#20445;&#25252;&#32452;&#32455;&#21644;&#30417;&#31649;&#26426;&#26500;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24050;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#34394;&#20551;&#35780;&#35770;&#19982;&#30495;&#23454;&#35780;&#35770;&#20043;&#38388;&#30340;&#24046;&#24322;&#29305;&#24449;&#36824;&#26159;&#29702;&#35299;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#35760;&#22909;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;38048&#20010;&#21307;&#24072;&#35780;&#35770;&#65292;&#20197;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#35780;&#35770;&#20998;&#31867;&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#19982;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;GPT4&#65292;GPT&#31995;&#21015;&#20013;&#26368;&#26032;&#30340;&#27169;&#22411;&#65292;&#26469;&#25581;&#31034;&#34394;&#20551;&#21644;&#30495;&#23454;&#21307;&#29983;&#35780;&#35770;&#30340;&#20851;&#38190;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;GPT-3&#22312;&#36825;&#20010;&#39046;&#22495;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#34394;&#20551;&#35780;&#35770;&#19982;&#30495;&#23454;&#35780;&#35770;&#22312;&#35821;&#35328;&#34920;&#36798;&#19978;&#30340;&#24046;&#24322;&#65292; &#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#34394;&#20551;&#35780;&#35770;&#30340;&#27169;&#24335;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of fake reviews of doctors has potentially detrimental consequences for patient well-being and has prompted concern among consumer protection groups and regulatory bodies. Yet despite significant advancements in the fields of machine learning and natural language processing, there remains limited comprehension of the characteristics differentiating fraudulent from authentic reviews. This study utilizes a novel pre-labeled dataset of 38048 physician reviews to establish the effectiveness of large language models in classifying reviews. Specifically, we compare the performance of traditional ML models, such as logistic regression and support vector machines, to generative pre-trained transformer models. Furthermore, we use GPT4, the newest model in the GPT family, to uncover the key dimensions along which fake and genuine physician reviews differ. Our findings reveal significantly superior performance of GPT-3 over traditional ML models in this context. Additionally, ou
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;eBible&#30340;&#22307;&#32463;&#32763;&#35793;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;1009&#20010;&#22307;&#32463;&#37096;&#20998;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;833&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#65292;&#20998;&#24067;&#22312;75&#20010;&#35821;&#35328;&#23478;&#26063;&#20013;&#12290;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;NLLB&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#22307;&#32463;&#32763;&#35793;&#39046;&#22495;&#20013;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.09919</link><description>&lt;p&gt;
The eBible&#35821;&#26009;&#24211;&#65306;&#29992;&#20110;&#38754;&#21521;&#20302;&#36164;&#28304;&#35821;&#35328;&#22280;&#30340;&#22307;&#32463;&#32763;&#35793;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
The eBible Corpus: Data and Model Benchmarks for Bible Translation for Low-Resource Languages. (arXiv:2304.09919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09919
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;eBible&#30340;&#22307;&#32463;&#32763;&#35793;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;1009&#20010;&#22307;&#32463;&#37096;&#20998;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;833&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#65292;&#20998;&#24067;&#22312;75&#20010;&#35821;&#35328;&#23478;&#26063;&#20013;&#12290;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;NLLB&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#22307;&#32463;&#32763;&#35793;&#39046;&#22495;&#20013;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#35770;&#37319;&#29992;&#25163;&#21160;&#12289;&#33258;&#21160;&#25110;&#20004;&#32773;&#32467;&#21512;&#30340;&#31574;&#30053;&#65292;&#39640;&#25928;&#20934;&#30830;&#22320;&#23558;&#35821;&#26009;&#24211;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#35768;&#22810;&#22522;&#30563;&#25945;&#32452;&#32455;&#33268;&#21147;&#20110;&#23558;&#22307;&#32463;&#32763;&#35793;&#25104;&#32570;&#20047;&#29616;&#20195;&#32763;&#35793;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;eBible&#35821;&#26009;&#24211;&#65306;&#19968;&#20010;&#21253;&#21547;1009&#20010;&#22307;&#32463;&#37096;&#20998;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;833&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#65292;&#20998;&#24067;&#22312;75&#20010;&#35821;&#35328;&#23478;&#26063;&#20013;&#12290;&#38500;&#20102;&#22307;&#32463;&#32763;&#35793;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#22522;&#20110;No Language Left Behind&#65288;NLLB&#65289;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#30340;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22307;&#32463;&#32763;&#35793;&#39046;&#22495;&#29305;&#26377;&#30340;&#33509;&#24178;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#24050;&#24314;&#31435;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#22522;&#20934;&#22914;&#20309;&#29992;&#20110;&#26410;&#26469;&#30340;&#32763;&#35793;&#24037;&#20316;&#12290;&#23545;&#20110;&#20351;&#29992;NLLB&#36827;&#34892;&#35757;&#32451;&#30340;BT&#20219;&#21153;&#65292;&#21335;&#23707;&#21644;&#26032;&#20960;&#20869;&#20122;&#20256;&#36755;&#35821;&#31995;&#30340;&#35821;&#35328;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#21360;&#27431;&#35821;&#31995;&#21644;&#38750;&#27954;&#20122;&#27954;&#35821;&#31995;&#30340;&#35821;&#35328;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently and accurately translating a corpus into a low-resource language remains a challenge, regardless of the strategies employed, whether manual, automated, or a combination of the two. Many Christian organizations are dedicated to the task of translating the Holy Bible into languages that lack a modern translation. Bible translation (BT) work is currently underway for over 3000 extremely low resource languages. We introduce the eBible corpus: a dataset containing 1009 translations of portions of the Bible with data in 833 different languages across 75 language families. In addition to a BT benchmarking dataset, we introduce model performance benchmarks built on the No Language Left Behind (NLLB) neural machine translation (NMT) models. Finally, we describe several problems specific to the domain of BT and consider how the established data and model benchmarks might be used for future translation efforts. For a BT task trained with NLLB, Austronesian and Trans-New Guinea languag
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35745;&#31639;&#30340;&#35789;&#35821;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24357;&#34917;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#65292;&#36890;&#36807;&#25552;&#20379;&#22823;&#37327;&#25511;&#21046;&#20102;&#22312;&#35789;&#27719;&#22788;&#29702;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#21464;&#37327;&#30340;&#21517;&#35789;&#23545;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#37327;&#21270;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;&#24052;&#26031;&#20811;&#35821;&#21644;&#27431;&#27954;&#35199;&#29677;&#29273;&#35821;&#30340;&#21517;&#35789;&#23545;&#20449;&#24687;&#65292;&#20294;&#36827;&#19968;&#27493;&#30340;&#24037;&#20316;&#24847;&#22270;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#22810;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2304.09616</link><description>&lt;p&gt;
&#36830;&#25509;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;&#24515;&#29702;&#35821;&#35328;&#23398;&#65306;&#38754;&#21521;&#24052;&#26031;&#20811;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#22522;&#20110;&#35821;&#26009;&#24211;&#21644;&#30693;&#35782;&#24211;&#30340;&#35745;&#31639;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Bridging Natural Language Processing and Psycholinguistics: computationally grounded semantic similarity and relatedness datasets for Basque and Spanish. (arXiv:2304.09616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09616
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35745;&#31639;&#30340;&#35789;&#35821;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24357;&#34917;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#65292;&#36890;&#36807;&#25552;&#20379;&#22823;&#37327;&#25511;&#21046;&#20102;&#22312;&#35789;&#27719;&#22788;&#29702;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#21464;&#37327;&#30340;&#21517;&#35789;&#23545;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#37327;&#21270;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;&#24052;&#26031;&#20811;&#35821;&#21644;&#27431;&#27954;&#35199;&#29677;&#29273;&#35821;&#30340;&#21517;&#35789;&#23545;&#20449;&#24687;&#65292;&#20294;&#36827;&#19968;&#27493;&#30340;&#24037;&#20316;&#24847;&#22270;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#22810;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22522;&#20110;&#20004;&#20010;&#33879;&#21517;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#65306;&#25991;&#26412;&#35821;&#26009;&#24211;&#21644;&#30693;&#35782;&#24211;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35745;&#31639;&#30340;&#35789;&#35821;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#24357;&#34917;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#65292;&#36890;&#36807;&#25552;&#20379;&#22823;&#37327;&#25511;&#21046;&#20102;&#22312;&#35789;&#27719;&#22788;&#29702;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#21464;&#37327;&#30340;&#21517;&#35789;&#23545;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#37327;&#21270;&#12290;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;1&#65289;&#20026;&#27599;&#20010;&#21517;&#35789;&#35745;&#31639;&#22235;&#20010;&#20851;&#38190;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24449;&#65306;&#20855;&#20307;&#24615;&#12289;&#39057;&#29575;&#12289;&#35821;&#20041;&#21644;&#38899;&#20301;&#37051;&#36817;&#23494;&#24230;&#65307;2&#65289;&#22312;&#36825;&#20123;&#22235;&#20010;&#21464;&#37327;&#19979;&#23545;&#21517;&#35789;&#36827;&#34892;&#37197;&#23545;&#65307;3&#65289;&#23545;&#20110;&#27599;&#20010;&#21517;&#35789;&#23545;&#65292;&#20998;&#37197;&#19977;&#31181;&#31867;&#22411;&#30340;&#21333;&#35789;&#30456;&#20284;&#24230;&#27979;&#37327;&#65292;&#35745;&#31639;&#20986;&#25991;&#26412;&#12289;Wordnet&#21644;&#28151;&#21512;&#23884;&#20837;&#12290;&#30446;&#21069;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#24052;&#26031;&#20811;&#35821;&#21644;&#27431;&#27954;&#35199;&#29677;&#29273;&#35821;&#30340;&#21517;&#35789;&#23545;&#20449;&#24687;&#65292;&#20294;&#36827;&#19968;&#27493;&#30340;&#24037;&#20316;&#24847;&#22270;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#22810;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a computationally-grounded word similarity dataset based on two well-known Natural Language Processing resources; text corpora and knowledge bases. This dataset aims to fulfil a gap in psycholinguistic research by providing a variety of quantifications of semantic similarity in an extensive set of noun pairs controlled by variables that play a significant role in lexical processing. The dataset creation has consisted in three steps, 1) computing four key psycholinguistic features for each noun; concreteness, frequency, semantic and phonological neighbourhood density; 2) pairing nouns across these four variables; 3) for each noun pair, assigning three types of word similarity measurements, computed out of text, Wordnet and hybrid embeddings. The present dataset includes noun pairs' information in Basque and European Spanish, but further work intends to extend it to more languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#30340;&#35757;&#32451;&#21644;&#23450;&#21046;&#19979;&#23454;&#29616;&#29983;&#25104;&#21487;&#26597;&#35810;&#34920;&#26684;&#30340;&#31616;&#21333;&#31995;&#32479;&#65292;&#24182;&#32473;&#20986;&#20102;&#20004;&#31181;&#23454;&#29616;&#31574;&#30053;&#65292;&#22312;&#19981;&#21516;&#36136;&#37327;&#21644;&#25104;&#26412;&#20013;&#24179;&#34913;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31995;&#32479;&#23545;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#29983;&#25104;&#30340;&#34920;&#26684;&#22343;&#39640;&#36136;&#37327;&#65292;&#19988;&#26080;&#38656;&#25991;&#26723;&#29305;&#23450;&#30340;&#23450;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.09433</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24322;&#26500;&#25968;&#25454;&#28246;&#32467;&#26500;&#21270;&#35270;&#22270;&#29983;&#25104;&#30340;&#31616;&#21333;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes. (arXiv:2304.09433v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#30340;&#35757;&#32451;&#21644;&#23450;&#21046;&#19979;&#23454;&#29616;&#29983;&#25104;&#21487;&#26597;&#35810;&#34920;&#26684;&#30340;&#31616;&#21333;&#31995;&#32479;&#65292;&#24182;&#32473;&#20986;&#20102;&#20004;&#31181;&#23454;&#29616;&#31574;&#30053;&#65292;&#22312;&#19981;&#21516;&#36136;&#37327;&#21644;&#25104;&#26412;&#20013;&#24179;&#34913;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31995;&#32479;&#23545;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#29983;&#25104;&#30340;&#34920;&#26684;&#22343;&#39640;&#36136;&#37327;&#65292;&#19988;&#26080;&#38656;&#25991;&#26723;&#29305;&#23450;&#30340;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31649;&#29702;&#30028;&#38271;&#26399;&#20197;&#26469;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#20986;&#36890;&#29992;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20154;&#21147;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#23450;&#21046;&#24773;&#20917;&#19979;&#25668;&#21462;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#24182;&#36755;&#20986;&#21487;&#26597;&#35810;&#30340;&#34920;&#26684;&#12290;&#37492;&#20110;&#28508;&#22312;&#25991;&#26723;&#30340;&#22810;&#26679;&#24615;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#36827;&#34892;&#31616;&#21270;&#30340;&#20551;&#35774;&#24182;&#20351;&#29992;&#29305;&#23450;&#39046;&#22495;&#30340;&#35757;&#32451;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35810;&#38382;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20445;&#25345;&#24191;&#27867;&#24615;&#12290;&#22312;&#24191;&#27867;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;LLMs&#21487;&#20165;&#38480;&#20110;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#25191;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#30001;LLMs&#39537;&#21160;&#30340;&#31616;&#21333;&#21407;&#22411;&#31995;&#32479;EVAPORATE&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#23454;&#29616;&#35813;&#31995;&#32479;&#30340;&#20004;&#31181;&#22522;&#26412;&#19981;&#21516;&#31574;&#30053;&#65306;&#25552;&#31034;LLM&#30452;&#25509;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#20540;&#25110;&#25552;&#31034;LLM&#21512;&#25104;&#25191;&#34892;&#25552;&#21462;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#25104;&#26412;-&#36136;&#37327;&#26435;&#34913;&#12290;&#20195;&#30721;&#21512;&#25104;&#20415;&#23452;&#65292;&#20294;&#27604;&#30452;&#25509;&#25277;&#21462;&#36828;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#23545;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EVAPORATE&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#25991;&#26723;&#29305;&#23450;&#30340;&#23450;&#21046;&#24773;&#20917;&#19979;&#20026;&#21508;&#31181;&#25991;&#26723;&#31867;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34920;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
A long standing goal of the data management community is to develop general, automated systems that ingest semi-structured documents and output queryable tables without human effort or domain specific customization. Given the sheer variety of potential documents, state-of-the art systems make simplifying assumptions and use domain specific training. In this work, we ask whether we can maintain generality by using large language models (LLMs). LLMs, which are pretrained on broad data, can perform diverse downstream tasks simply conditioned on natural language task descriptions.  We propose and evaluate EVAPORATE, a simple, prototype system powered by LLMs. We identify two fundamentally different strategies for implementing this system: prompt the LLM to directly extract values from documents or prompt the LLM to synthesize code that performs the extraction. Our evaluations show a cost-quality tradeoff between these two approaches. Code synthesis is cheap, but far less accurate than dire
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;-LLM&#20132;&#20114;&#26694;&#26550;&#65292;&#20302;&#20195;&#30721;LLM&#65292;&#35813;&#26694;&#26550;&#21487;&#36890;&#36807;&#20845;&#31181;&#31867;&#22411;&#30340;&#31616;&#21333;&#20302;&#20195;&#30721;&#21487;&#35270;&#21270;&#32534;&#31243;&#20132;&#20114;&#23454;&#29616;&#26356;&#21487;&#25511;&#21644;&#31283;&#23450;&#30340;&#21709;&#24212;&#65292;&#20855;&#26377;&#21487;&#25511;&#24615;&#24378;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;&#20132;&#20114;&#26041;&#24335;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.08103</link><description>&lt;p&gt;
&#20302;&#20195;&#30721;LLM&#65306;LLM&#19978;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Low-code LLM: Visual Programming over LLMs. (arXiv:2304.08103v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;-LLM&#20132;&#20114;&#26694;&#26550;&#65292;&#20302;&#20195;&#30721;LLM&#65292;&#35813;&#26694;&#26550;&#21487;&#36890;&#36807;&#20845;&#31181;&#31867;&#22411;&#30340;&#31616;&#21333;&#20302;&#20195;&#30721;&#21487;&#35270;&#21270;&#32534;&#31243;&#20132;&#20114;&#23454;&#29616;&#26356;&#21487;&#25511;&#21644;&#31283;&#23450;&#30340;&#21709;&#24212;&#65292;&#20855;&#26377;&#21487;&#25511;&#24615;&#24378;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;&#20132;&#20114;&#26041;&#24335;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#21033;&#29992;LLM&#26469;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#32791;&#26102;&#32780;&#38590;&#20197;&#25484;&#25511;&#30340;&#25552;&#31034;&#24037;&#31243;&#22788;&#29702;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;-LLM&#20132;&#20114;&#26694;&#26550;&#65292;&#21363;&#20302;&#20195;&#30721;LLM&#12290;&#23427;&#21253;&#25324;&#20845;&#31181;&#31867;&#22411;&#30340;&#31616;&#21333;&#20302;&#20195;&#30721;&#21487;&#35270;&#21270;&#32534;&#31243;&#20132;&#20114;&#65292;&#20840;&#37096;&#25903;&#25345;&#28857;&#20987;&#12289;&#25302;&#25918;&#25110;&#25991;&#26412;&#32534;&#36753;&#65292;&#20197;&#23454;&#29616;&#26356;&#21487;&#25511;&#21644;&#31283;&#23450;&#30340;&#21709;&#24212;&#12290;&#36890;&#36807;&#19982;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#30340;&#35270;&#35273;&#20132;&#20114;&#65292;&#29992;&#25143;&#21487;&#20197;&#23558;&#20854;&#24819;&#27861;&#32435;&#20837;&#24037;&#20316;&#27969;&#31243;&#65292;&#32780;&#19981;&#24517;&#32534;&#20889;&#29712;&#30862;&#30340;&#25552;&#31034;&#12290;&#25552;&#20986;&#30340;&#20302;&#20195;&#30721;LLM&#26694;&#26550;&#30001;&#35268;&#21010;LLM&#21644;&#25191;&#34892;LLM&#20004;&#37096;&#20998;&#32452;&#25104;&#65292;&#35268;&#21010;LLM&#20026;&#22797;&#26434;&#20219;&#21153;&#35774;&#35745;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#35268;&#21010;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#20302;&#20195;&#30721;&#21487;&#35270;&#21270;&#32534;&#31243;&#25805;&#20316;&#30456;&#24212;&#22320;&#36827;&#34892;&#32534;&#36753;&#21644;&#30830;&#35748;&#65292;&#32780;&#25191;&#34892;LLM&#21017;&#25353;&#29031;&#29992;&#25143;&#30830;&#35748;&#30340;&#24037;&#20316;&#27969;&#31243;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#24378;&#35843;&#20302;&#20195;&#30721;LLM&#30340;&#19977;&#20010;&#20248;&#28857;&#65306;&#21487;&#25511;&#30340;&#29983;&#25104;&#32467;&#26524;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;&#20154;-LLM&#20132;&#20114;&#20197;&#21450;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effectively utilizing LLMs for complex tasks is challenging, often involving a time-consuming and uncontrollable prompt engineering process. This paper introduces a novel human-LLM interaction framework, Low-code LLM. It incorporates six types of simple low-code visual programming interactions, all supported by clicking, dragging, or text editing, to achieve more controllable and stable responses. Through visual interaction with a graphical user interface, users can incorporate their ideas into the workflow without writing trivial prompts. The proposed Low-code LLM framework consists of a Planning LLM that designs a structured planning workflow for complex tasks, which can be correspondingly edited and confirmed by users through low-code visual programming operations, and an Executing LLM that generates responses following the user-confirmed workflow. We highlight three advantages of the low-code LLM: controllable generation results, user-friendly human-LLM interaction, and broadly app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29983;&#25104;&#31526;&#21495;&#65292;&#23454;&#29616;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#12290;&#36825;&#20123;&#31526;&#21495;&#21487;&#20197;&#25429;&#25417;&#21040;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21576;&#29616;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.06377</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#31526;&#21495;&#30340;&#20986;&#29616;&#19982;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29983;&#25104;&#31526;&#21495;&#65292;&#23454;&#29616;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#12290;&#36825;&#20123;&#31526;&#21495;&#21487;&#20197;&#25429;&#25417;&#21040;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21576;&#29616;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#21019;&#36896;&#26377;&#24847;&#20041;&#30340;&#31526;&#21495;&#65292;&#24182;&#29087;&#32451;&#22320;&#23558;&#23427;&#20204;&#29992;&#20110;&#26356;&#39640;&#30340;&#35748;&#30693;&#21151;&#33021;&#65292;&#22914;&#20132;&#27969;&#12289;&#25512;&#29702;&#12289;&#35268;&#21010;&#31561;&#65292;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#37325;&#35201;&#21644;&#29420;&#29305;&#20043;&#22788;&#12290; &#30446;&#21069;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20173;&#36828;&#36828;&#33853;&#21518;&#20110;&#20154;&#31867;&#21019;&#36896;&#31526;&#21495;&#36827;&#34892;&#36825;&#20123;&#39640;&#32423;&#35748;&#30693;&#21151;&#33021;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#31526;&#21495;&#21019;&#36896;&#12289;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#33021;&#21147;&#12290;SEA-net&#29983;&#25104;&#21160;&#24577;&#37197;&#32622;&#32593;&#32476;&#20197;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#31526;&#21495;&#12290;&#36825;&#20123;&#31526;&#21495;&#25429;&#25417;&#20102;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#36890;&#36807;&#32431;&#31526;&#21495;&#25805;&#20316;&#25110;&#20132;&#27969;&#33719;&#24471;&#26032;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#33258;&#21160;&#29983;&#25104;&#30340;&#31526;&#21495;&#21576;&#29616;&#20986;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#34920;&#26126;&#22312;&#20154;&#31867;&#22823;&#33041;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#29983;&#25104;&#21644;&#29702;&#35299;&#31526;&#21495;&#30340;&#20849;&#21516;&#26694;&#26550;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#23558;&#25104;&#20026;&#23558;&#26469;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#30340;&#21161;&#25512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to create meaningful symbols and proficiently use them for higher cognitive functions such as communication, reasoning, planning, etc., is essential and unique for human intelligence. Current deep neural networks are still far behind human's ability to create symbols for such higher cognitive functions. Here we propose a solution, named SEA-net, to endow neural networks with ability of symbol creation, semantic understanding and communication. SEA-net generates symbols that dynamically configure the network to perform specific tasks. These symbols capture compositional semantic information that enables the system to acquire new functions purely by symbolic manipulation or communication. In addition, we found that these self-generated symbols exhibit an intrinsic structure resembling that of natural language, suggesting a common framework underlying the generation and understanding of symbols in both human brains and artificial neural networks. We hope that it will be instrum
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26377;&#25928;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20551;&#26032;&#38395;&#26816;&#27979;&#12290;&#25152;&#35774;&#35745;&#30340;&#30456;&#20284;&#24230;&#24863;&#30693;&#34701;&#21512;&#27169;&#22359;&#21487;&#20197;&#37327;&#21270;&#22320;&#26681;&#25454;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26435;&#34913;&#27599;&#20010;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#22122;&#22768;&#24178;&#25200;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#20063;&#35777;&#26126;&#25552;&#20986;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#24378;&#22823;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04187</link><description>&lt;p&gt;
&#30456;&#20284;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Similarity-Aware Multimodal Prompt Learning for Fake News Detection. (arXiv:2304.04187v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26377;&#25928;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20551;&#26032;&#38395;&#26816;&#27979;&#12290;&#25152;&#35774;&#35745;&#30340;&#30456;&#20284;&#24230;&#24863;&#30693;&#34701;&#21512;&#27169;&#22359;&#21487;&#20197;&#37327;&#21270;&#22320;&#26681;&#25454;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26435;&#34913;&#27599;&#20010;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#22122;&#22768;&#24178;&#25200;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#20063;&#35777;&#26126;&#25552;&#20986;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#24378;&#22823;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#26631;&#20934;&#33539;&#24335;&#20027;&#35201;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#26469;&#24314;&#31435;&#26032;&#38395;&#30340;&#30495;&#23454;&#24615;&#65292;&#28982;&#32780;&#65292;&#32593;&#19978;&#20551;&#26032;&#38395;&#30340;&#35805;&#35821;&#36890;&#24120;&#27604;&#36739;&#24494;&#22937;&#65292;&#38656;&#35201;&#19987;&#23478;&#30693;&#35782;&#25165;&#33021;&#20351;&#29992;&#25991;&#26412;&#20449;&#24687;&#25581;&#38706;&#20551;&#26032;&#38395;&#12290;&#26368;&#36817;&#65292;&#20851;&#27880;&#20110;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#30740;&#31350;&#24050;&#32463;&#36229;&#36234;&#20102;&#20165;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#21462;&#21333;&#27169;&#24577;&#29305;&#24449;&#65292;&#25110;&#30452;&#25509;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36825;&#25104;&#20026;&#26816;&#27979;&#20551;&#26032;&#38395;&#30340;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#23454;&#20363;&#65292;&#35201;&#20040;&#38656;&#35201;&#26356;&#26032;&#25972;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#38598;&#65292;&#19981;&#23454;&#38469;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#23558;&#36328;&#27169;&#24577;&#29305;&#24449;&#30452;&#25509;&#34701;&#21512;&#65292;&#32780;&#19981;&#32771;&#34385;&#19981;&#30456;&#20851;&#30340;&#35821;&#20041;&#34920;&#31034;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#21040;&#22810;&#27169;&#24577;&#29305;&#24449;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#23398;&#20064;&#65288;SAMPLE&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#22312;&#30456;&#20851;&#24615;&#26469;&#26377;&#25928;&#22320;&#34701;&#21512;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#30456;&#20284;&#24230;&#24863;&#30693;&#34701;&#21512;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#23398;&#20064;&#26681;&#25454;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#26435;&#34913;&#19981;&#21516;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22823;&#24133;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard paradigm for fake news detection mainly utilizes text information to model the truthfulness of news. However, the discourse of online fake news is typically subtle and it requires expert knowledge to use textual information to debunk fake news. Recently, studies focusing on multimodal fake news detection have outperformed text-only methods. Recent approaches utilizing the pre-trained model to extract unimodal features, or fine-tuning the pre-trained model directly, have become a new paradigm for detecting fake news. Again, this paradigm either requires a large number of training instances, or updates the entire set of pre-trained model parameters, making real-world fake news detection impractical. Furthermore, traditional multimodal methods fuse the cross-modal features directly without considering that the uncorrelated semantic representation might inject noise into the multimodal features. This paper proposes a Similarity-Aware Multimodal Prompt Learning (SAMPLE) framewo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#65292;&#32780;GPT-4&#30340;&#34920;&#29616;&#21017;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.03439</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#21644;GPT-4&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. (arXiv:2304.03439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#65292;&#32780;GPT-4&#30340;&#34920;&#29616;&#21017;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;4&#65288;GPT-4&#65289;&#30340;&#21457;&#24067;&#65292;&#25105;&#20204;&#28212;&#26395;&#20102;&#35299;GPT-4&#22312;&#21508;&#31181;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;LogiQA&#21644;ReClor&#31561;&#24120;&#29992;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#21450;&#20687;AR-LSAT&#36825;&#26679;&#30340;&#26032;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#38656;&#35201;&#36923;&#36753;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#27979;&#35797;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;ChatGPT&#21644;GPT-4&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;ChatGPT&#21644;GPT-4&#20043;&#38388;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#30340;&#34920;&#29616;&#36828;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#12290;GPT-4&#22312;&#25105;&#20204;&#30340;&#25163;&#21160;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#39640;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#21644;GPT-4&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#20026;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing logical reasoning ability is a comprehensive natural language understanding endeavor. With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks. This report analyses multiple logical reasoning datasets, with popular benchmarks like LogiQA and ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice reading comprehension and natural language inference tasks with benchmarks requiring logical reasoning. We further construct a logical reasoning out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4. We also make a performance comparison between ChatGPT and GPT-4. Experiment results show that ChatGPT performs significantly better than the RoBERTa fine-tuning method on most logical reasoning benchmarks. GPT-4 shows even higher performance on our manual tests. Among benchmarks, ChatGPT and GPT-4 do relatively w
&lt;/p&gt;</description></item><item><title>ParroT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24320;&#28304;LLM&#21644;&#20154;&#24037;&#32534;&#20889;&#30340;&#32763;&#35793;&#35780;&#20272;&#25968;&#25454;&#30340;&#32842;&#22825;&#32763;&#35793;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#32763;&#35793;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25191;&#34892;&#26679;&#24335;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#35201;&#27714;&#26469;&#35268;&#33539;&#32763;&#35793;&#36807;&#31243;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; ParroT &#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.02426</link><description>&lt;p&gt;
ParroT: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32842;&#22825;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
ParroT: Translating During Chat Using Large Language Models. (arXiv:2304.02426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02426
&lt;/p&gt;
&lt;p&gt;
ParroT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24320;&#28304;LLM&#21644;&#20154;&#24037;&#32534;&#20889;&#30340;&#32763;&#35793;&#35780;&#20272;&#25968;&#25454;&#30340;&#32842;&#22825;&#32763;&#35793;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#32763;&#35793;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25191;&#34892;&#26679;&#24335;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#35201;&#27714;&#26469;&#35268;&#33539;&#32763;&#35793;&#36807;&#31243;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; ParroT &#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914; ChatGPT &#21644; GPT-4 &#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#22312;&#32842;&#22825;&#36807;&#31243;&#20013;&#23436;&#25104;&#21508;&#31181;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#21482;&#33021;&#36890;&#36807;&#21463;&#38480;&#30340;API&#35775;&#38382;&#65292;&#36825;&#20026;&#26032;&#30340;&#30740;&#31350;&#21644;&#39046;&#22495;&#36827;&#23637;&#24102;&#26469;&#20102;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ParroT &#26694;&#26550;&#65292;&#22522;&#20110;&#24320;&#28304;LLM&#65288;&#22914;LLaMA-7b&#65289;&#21644;&#20154;&#24037;&#32534;&#20889;&#30340;&#32763;&#35793;&#35780;&#20272;&#25968;&#25454;&#26469;&#22686;&#24378;&#21644;&#35268;&#33539;&#32842;&#22825;&#32763;&#35793;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ParroT&#23558;&#32763;&#35793;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25191;&#34892;&#30340;&#26679;&#24335;&#65292;&#24182;&#24341;&#20837; "Hint " &#23383;&#27573;&#20197;&#21152;&#20837;&#39069;&#22806;&#35201;&#27714;&#26469;&#35268;&#33539;&#32763;&#35793;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#25351;&#20196;&#31867;&#22411;&#26469;&#24494;&#35843; ParroT &#27169;&#22411;&#65292;&#21253;&#25324;&#32763;&#35793;&#25351;&#20196;&#12289;&#23545;&#27604;&#25351;&#20196;&#21644;&#35823;&#24046;&#24341;&#23548;&#25351;&#20196;&#12290;&#22312;&#20004;&#20010; Flores &#23376;&#38598;&#21644; WMT22 &#27979;&#35797;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992; ParroT &#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#19988;&#38656;&#35201;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable abilities on a wide range of natural language processing (NLP) tasks, including various machine translation abilities accomplished during chat. However, these models are only accessible through restricted APIs, which creates barriers to new research and advancements in the field. Therefore, we propose the $\mathbf{ParroT}$ framework to enhance and regulate the translation abilities during chat based on open-sourced LLMs (i.e., LLaMA-7b) and human written translation and evaluation data. Specifically, ParroT reformulates translation data into the instruction-following style, and introduces a "Hint" field for incorporating extra requirements to regulate the translation process. Accordingly, we propose three instruction types for finetuning ParroT models, including translation instruction, contrastive instruction, and error-guided instruction. Experiments on two Flores subsets and WMT22 test sets suggest that tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#20061;&#26001;&#29436;&#30340;&#21367;&#31215;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20132;&#38169;&#38544;&#24335;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#21644;&#25968;&#25454;&#25511;&#21046;&#38376;&#26500;&#36896;&#12290;&#22312;&#35760;&#24518;&#20219;&#21153;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#24207;&#21015;&#26377;&#20960;&#21315;&#21040;&#20960;&#21313;&#19975;&#20010;&#26631;&#35760;&#65292;&#20061;&#26001;&#29436;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#31639;&#23376;&#26356;&#20026;&#31934;&#30830;&#30340;&#34920;&#29616;&#65292;&#36798;&#21040;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#30340;&#27700;&#24179;&#65292;&#24182;&#21462;&#24471;&#20102;&#23494;&#38598;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26368;&#26032;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2302.10866</link><description>&lt;p&gt;
&#20960;&#21315;&#21040;&#20960;&#21313;&#19975;&#20010;&#26631;&#35760;&#30340;&#24207;&#21015;&#25512;&#29702;&#21644;&#35760;&#24518;&#20219;&#21153;&#20013;&#30340;&#21367;&#31215;&#35821;&#35328;&#27169;&#22411; - &#20061;&#26001;&#29436;&#31561;&#32423;: &#36808;&#21521;&#26356;&#22823;&#30340;&#21367;&#31215;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hyena Hierarchy: Towards Larger Convolutional Language Models. (arXiv:2302.10866v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#20061;&#26001;&#29436;&#30340;&#21367;&#31215;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20132;&#38169;&#38544;&#24335;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#21644;&#25968;&#25454;&#25511;&#21046;&#38376;&#26500;&#36896;&#12290;&#22312;&#35760;&#24518;&#20219;&#21153;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#24207;&#21015;&#26377;&#20960;&#21315;&#21040;&#20960;&#21313;&#19975;&#20010;&#26631;&#35760;&#65292;&#20061;&#26001;&#29436;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#31639;&#23376;&#26356;&#20026;&#31934;&#30830;&#30340;&#34920;&#29616;&#65292;&#36798;&#21040;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#30340;&#27700;&#24179;&#65292;&#24182;&#21462;&#24471;&#20102;&#23494;&#38598;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26368;&#26032;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22823;&#22411;Transformer&#30340;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#22312;&#20219;&#24847;&#35268;&#27169;&#19978;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;Transformers&#30340;&#26680;&#24515;&#26500;&#20214;&#8212;&#8212;&#27880;&#24847;&#21147;&#25805;&#20316;&#31526;&#8212;&#8212;&#22312;&#38271;&#24230;&#26041;&#38754;&#21576;&#29616;&#20986;&#20108;&#27425;&#30340;&#25104;&#26412;&#65292;&#38480;&#21046;&#20102;&#21487;&#20197;&#35775;&#38382;&#30340;&#19978;&#19979;&#25991;&#37327;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#20302;&#31209;&#21644;&#31232;&#30095;&#36924;&#36817;&#30340;&#20122;&#20108;&#27425;&#26041;&#27861;&#38656;&#35201;&#19982;&#23494;&#38598;&#30340;&#27880;&#24847;&#21147;&#23618;&#32467;&#21512;&#20351;&#29992;&#26469;&#21305;&#37197;Transformers&#65292;&#34920;&#26126;&#23384;&#22312;&#33021;&#21147;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20061;&#26001;&#29436;&#65292;&#19968;&#31181;&#20122;&#20108;&#27425;&#30340;&#27880;&#24847;&#21147;&#26367;&#20195;&#21697;&#65292;&#36890;&#36807;&#20132;&#38169;&#38544;&#24335;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#21644;&#25968;&#25454;&#25511;&#21046;&#38376;&#26500;&#36896;&#12290;&#22312;&#35760;&#24518;&#20219;&#21153;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#24207;&#21015;&#26377;&#20960;&#21315;&#21040;&#20960;&#21313;&#19975;&#20010;&#26631;&#35760;&#65292;&#20061;&#26001;&#29436;&#30340;&#20934;&#30830;&#24230;&#27604;&#20381;&#36182;&#20110;&#29366;&#24577;&#31354;&#38388;&#21644;&#20854;&#20182;&#38544;&#24335;&#21644;&#26174;&#24335;&#26041;&#27861;&#30340;&#31639;&#23376;&#25552;&#39640;&#20102;50&#20197;&#19978;&#65292;&#36798;&#21040;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#30340;&#27700;&#24179;&#12290;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#20061;&#26001;&#29436;&#24182;&#19981;&#38656;&#35201;&#23494;&#38598;&#27880;&#24847;&#21147;&#30340;&#32467;&#26500;&#65292;&#23601;&#24050;&#32463;&#21462;&#24471;&#20102;&#23494;&#38598;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26368;&#26032;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard dat
&lt;/p&gt;</description></item><item><title>InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08624</link><description>&lt;p&gt;
InstructABSA: &#22522;&#20110;&#25351;&#20196;&#23398;&#20064;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08624
&lt;/p&gt;
&lt;p&gt;
InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;InstructABSA&#65292;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;Aspect Based Sentiment Analysis (ABSA) &#25152;&#26377;&#23376;&#20219;&#21153;&#65288;Aspect Term Extraction (ATE)&#65292;Aspect Term Sentiment Classification (ATSC)&#65292;&#20197;&#21450;Joint Task modeling&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#24341;&#20837;&#20102;&#27491;&#38754;&#12289;&#36127;&#38754;&#12289;&#21644;&#20013;&#24615;&#30340;&#20363;&#23376;&#65292;&#24182;&#20351;&#29992;&#25351;&#20196;&#26469;&#35843;&#25972;&#27599;&#20010;ABSA&#23376;&#20219;&#21153;&#30340;&#27169;&#22411;&#65288;Tk-Instruct&#65289;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#22312;Sem Eval 2014&#12289;2015&#21644;2016&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#19977;&#20010;ABSA&#23376;&#20219;&#21153;&#65288;ATE&#12289;ATSC&#21644;Joint Task&#65289;&#19978;&#65292;InstructABSA&#22312;&#24615;&#33021;&#19978;&#37117;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;SOTA&#65289;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#34920;&#29616;&#36229;&#36807;&#20102;7&#20493;&#22823;&#30340;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;Rest14 ATE&#23376;&#20219;&#21153;&#19978;&#65292;InstructABSA&#36229;&#36807;&#20102;SOTA 7.31%&#30340;&#24471;&#20998;&#65292;Rest15 ATSC&#23376;&#20219;&#21153;&#19978;&#20063;&#26377;&#25552;&#21319;&#65292;&#24182;&#19988;&#22312;Lapt14 Joint Task&#19978;&#30340;&#34920;&#29616;&#25552;&#21319;&#20102;8.63%&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#23545;&#20110;&#25152;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;InstructABSA&#20855;&#26377;&#24378;&#22823;&#30340;&#26032;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present InstructABSA, Aspect Based Sentiment Analysis (ABSA) using the instruction learning paradigm for all ABSA subtasks: Aspect Term Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint Task modeling. Our method introduces positive, negative, and neutral examples to each training sample, and instruction tunes the model (Tk-Instruct) for each ABSA subtask, yielding significant performance improvements. Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on all three ABSA subtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE subtask by 7.31% points, Rest15 ATSC subtask by and on the Lapt14 Joint Task by 8.63% points. Our results also suggest a strong generalization ability to new domains across all three subtasks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.07267</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24515;&#29702;&#23398;&#20013;&#30340;&#8220;&#27491;&#30830;&#31572;&#26696;&#8221;
&lt;/p&gt;
&lt;p&gt;
"Correct answers" from the psychology of artificial intelligence. (arXiv:2302.07267v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper replicates 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, and successfully replicates the results of 8 studies. However, for the remaining 6 studies, GPT3.5 answered survey questions in an extremely predetermined way, making it impossible to analyze these studies.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24050;&#32463;&#22823;&#22823;&#22686;&#24378;&#12290;&#36825;&#31181;AI&#31995;&#32479;&#30340;&#19968;&#20010;&#25552;&#20986;&#30340;&#24212;&#29992;&#26159;&#25903;&#25345;&#31038;&#20250;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#30446;&#21069;&#23436;&#32654;&#30340;&#23454;&#39564;&#25511;&#21046;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#32780;&#22823;&#35268;&#27169;&#12289;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#65288;&#20439;&#31216;GPT3.5&#65289;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27599;&#39033;&#30740;&#31350;&#30340;&#35843;&#26597;&#20316;&#20026;&#25991;&#26412;&#36755;&#20837;&#65292;&#20174;GPT3.5&#30340;&#40664;&#35748;&#35774;&#32622;&#20013;&#25910;&#38598;&#20102;&#21709;&#24212;&#12290;&#22312;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#30340;&#20843;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;GPT&#26679;&#26412;&#22797;&#21046;&#20102;&#21407;&#22987;&#32467;&#26524;&#30340;37.5%&#20197;&#21450;Many Labs 2&#32467;&#26524;&#30340;37.5%&#12290;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#25105;&#20204;&#26080;&#27861;&#20687;&#39044;&#20808;&#27880;&#20876;&#30340;&#35745;&#21010;&#37027;&#26679;&#20998;&#26512;&#21097;&#19979;&#30340;&#20845;&#39033;&#30740;&#31350;&#12290;&#36825;&#26159;&#22240;&#20026;&#23545;&#20110;&#36825;&#20845;&#39033;&#30740;&#31350;&#20013;&#30340;&#27599;&#19968;&#39033;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65288;&#26080;&#35770;&#26159;&#22240;&#21464;&#37327;&#36824;&#26159;&#26465;&#20214;&#21464;&#37327;&#65289;&#65306;&#19968;&#20010;&#26410;&#30693;&#30340;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have vastly grown in capabilities. One proposed application of such AI systems is to support data collection in the social and cognitive sciences, where perfect experimental control is currently unfeasible and the collection of large, representative datasets is generally expensive. In this paper, we re-replicate 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We collected responses from the default setting of GPT3.5 by inputting each study's survey as text. Among the eight studies we could analyse, our GPT sample replicated 37.5% of the original results as well as 37.5% of the Many Labs 2 results. Unexpectedly, we could not analyse the remaining six studies as we had planned in our pre-registration. This was because for each of these six studies, GPT3.5 answered at least one of the survey questions (either a dependent variable or a condition variable) in an extremely predetermined way: an unex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;&#30340;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#39044;&#27979;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#31995;&#25968;&#20197;&#36827;&#34892;&#23454;&#20307;&#32423;&#29305;&#24449;&#32858;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;&#30828;&#23454;&#20307;&#37325;&#25773;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#31946;&#23454;&#20307;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#35757;&#32451;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;MMEA&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.14454</link><description>&lt;p&gt;
MEAformer: &#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid. (arXiv:2212.14454v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14454
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;&#30340;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#39044;&#27979;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#31995;&#25968;&#20197;&#36827;&#34892;&#23454;&#20307;&#32423;&#29305;&#24449;&#32858;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;&#30828;&#23454;&#20307;&#37325;&#25773;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#31946;&#23454;&#20307;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#35757;&#32451;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;MMEA&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#30340;&#19968;&#20010;&#37325;&#35201;&#21464;&#20307;&#65292;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#26088;&#22312;&#21457;&#29616;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#20855;&#26377;&#30456;&#20851;&#22270;&#20687;&#30340;&#30456;&#21516;&#23454;&#20307;&#12290; &#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#24403;&#21069;&#30340;MMEA&#31639;&#27861;&#37117;&#20840;&#23616;&#37319;&#29992;KG&#32423;&#27169;&#24577;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#22810;&#27169;&#24335;&#23454;&#20307;&#34920;&#31034;&#65292;&#20294;&#24573;&#30053;&#20102;&#20010;&#20307;&#23454;&#20307;&#30340;&#27169;&#24577;&#20559;&#22909;&#21464;&#21270;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;&#23545;&#27169;&#24577;&#65288;&#20363;&#22914;&#27169;&#31946;&#22270;&#20687;&#21644;&#20851;&#31995;&#65289;&#20013;&#28508;&#22312;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MEAformer&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;&#30340;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21160;&#24577;&#39044;&#27979;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#31995;&#25968;&#20197;&#36827;&#34892;&#23454;&#20307;&#32423;&#29305;&#24449;&#32858;&#21512;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;&#30828;&#23454;&#20307;&#37325;&#25773;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#31946;&#23454;&#20307;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#22810;&#20010;&#35757;&#32451;&#22330;&#26223;&#65288;&#21253;&#25324;&#26377;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#12289;&#36845;&#20195;&#21644;&#20302;&#36164;&#28304;&#35774;&#32622;&#65289;&#20013;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#65292;&#32780;&#19988;&#36890;&#36807;&#21033;&#29992;&#27169;&#24577;&#20559;&#22909;&#21464;&#21270;&#26377;&#25928;&#25552;&#39640;&#20102;MMEA&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an important variant of entity alignment (EA), multi-modal entity alignment (MMEA) aims to discover identical entities across different knowledge graphs (KGs) with relevant images attached. We noticed that current MMEA algorithms all globally adopt the KG-level modality fusion strategies for multi-modal entity representation but ignore the variation in modality preferences for individual entities, hurting the robustness to potential noise involved in modalities (e.g., blurry images and relations). In this paper, we present MEAformer, a multi-modal entity alignment transformer approach for meta modality hybrid, which dynamically predicts the mutual correlation coefficients among modalities for entity-level feature aggregation. A modal-aware hard entity replay strategy is further proposed for addressing vague entity details. Experimental results show that our model not only achieves SOTA performance on multiple training scenarios including supervised, unsupervised, iterative, and low 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BERT-CTC&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20844;&#24335;&#65292;&#23427;&#21033;&#29992;BERT&#19978;&#19979;&#25991;&#23884;&#20837;&#33719;&#21462;&#26174;&#24335;&#36755;&#20986;&#20381;&#36182;&#24615;&#26469;&#25972;&#21512;&#35821;&#35328;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#20851;&#27880;&#24207;&#21015;&#30340;&#20840;&#37096;&#19978;&#19979;&#25991;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35821;&#20041;&#34920;&#31034;&#23545;&#19979;&#28216;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#26377;&#30410;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.16663</link><description>&lt;p&gt;
BERT&#36935;&#35265;CTC&#65306;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model. (arXiv:2210.16663v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BERT-CTC&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20844;&#24335;&#65292;&#23427;&#21033;&#29992;BERT&#19978;&#19979;&#25991;&#23884;&#20837;&#33719;&#21462;&#26174;&#24335;&#36755;&#20986;&#20381;&#36182;&#24615;&#26469;&#25972;&#21512;&#35821;&#35328;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#20851;&#27880;&#24207;&#21015;&#30340;&#20840;&#37096;&#19978;&#19979;&#25991;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35821;&#20041;&#34920;&#31034;&#23545;&#19979;&#28216;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#26377;&#30410;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BERT-CTC&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20844;&#24335;&#65292;&#35813;&#20844;&#24335;&#36890;&#36807;&#23545;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#20351;&#29992;BERT&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#20844;&#24335;&#25918;&#23485;&#20102;&#24120;&#35268;CTC&#20013;&#20351;&#29992;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;BERT&#19978;&#19979;&#25991;&#23884;&#20837;&#33719;&#21462;&#26174;&#24335;&#36755;&#20986;&#20381;&#36182;&#24615;&#26469;&#25972;&#21512;&#35821;&#35328;&#30693;&#35782;&#12290;BERT-CTC&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#20851;&#27880;&#36755;&#20837;&#21644;&#20551;&#35774;&#30340;&#36755;&#20986;&#24207;&#21015;&#30340;&#20840;&#37096;&#19978;&#19979;&#25991;&#12290;&#35813;&#26426;&#21046;&#40723;&#21169;&#27169;&#22411;&#23398;&#20064;&#38899;&#39057;&#21644;&#26631;&#35760;&#34920;&#31034;&#20043;&#38388;&#30340;&#20869;&#22312;/&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#20445;&#25345;CTC&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;BERT-CTC&#23558;&#36974;&#34109;&#39044;&#27979;&#31639;&#27861;&#19982;CTC&#35299;&#30721;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#36845;&#20195;&#32454;&#21270;&#36755;&#20986;&#24207;&#21015;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BERT-CTC&#22312;&#19981;&#21516;&#30340;&#35328;&#35821;&#39118;&#26684;&#21644;&#35821;&#35328;&#21464;&#21270;&#26041;&#38754;&#37117;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;BERT-CTC&#20013;&#30340;&#35821;&#20041;&#34920;&#31034;&#23545;&#19979;&#28216;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#30340;&#26377;&#30410;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents BERT-CTC, a novel formulation of end-to-end speech recognition that adapts BERT for connectionist temporal classification (CTC). Our formulation relaxes the conditional independence assumptions used in conventional CTC and incorporates linguistic knowledge through the explicit output dependency obtained by BERT contextual embedding. BERT-CTC attends to the full contexts of the input and hypothesized output sequences via the self-attention mechanism. This mechanism encourages a model to learn inner/inter-dependencies between the audio and token representations while maintaining CTC's training efficiency. During inference, BERT-CTC combines a mask-predict algorithm with CTC decoding, which iteratively refines an output sequence. The experimental results reveal that BERT-CTC improves over conventional approaches across variations in speaking styles and languages. Finally, we show that the semantic representations in BERT-CTC are beneficial towards downstream spoken lan
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#35889;&#23646;&#24615;&#21435;&#38500;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31070;&#32463;&#34920;&#31034;&#20013;&#21435;&#38500;&#31169;&#26377;&#25110;&#20445;&#25252;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#26356;&#22909;&#30340;&#20027;&#35201;&#20219;&#21153;&#24615;&#33021;&#65292;&#21516;&#26102;&#21482;&#38656;&#23569;&#37327;&#30340;&#34987;&#20445;&#25252;&#23646;&#24615;&#25968;&#25454;&#21363;&#21487;&#21435;&#38500;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2203.07893</link><description>&lt;p&gt;
&#40644;&#37329;&#24182;&#38750;&#19968;&#20999;&#65306;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20445;&#25252;&#23646;&#24615;&#20449;&#24687;&#30340;&#20809;&#35889;&#21435;&#38500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information. (arXiv:2203.07893v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#35889;&#23646;&#24615;&#21435;&#38500;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31070;&#32463;&#34920;&#31034;&#20013;&#21435;&#38500;&#31169;&#26377;&#25110;&#20445;&#25252;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#26356;&#22909;&#30340;&#20027;&#35201;&#20219;&#21153;&#24615;&#33021;&#65292;&#21516;&#26102;&#21482;&#38656;&#23569;&#37327;&#30340;&#34987;&#20445;&#25252;&#23646;&#24615;&#25968;&#25454;&#21363;&#21487;&#21435;&#38500;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;&#20809;&#35889;&#23646;&#24615;&#21435;&#38500;&#26041;&#27861;; SAL&#65289;&#65292;&#29992;&#20110;&#20174;&#31070;&#32463;&#34920;&#31034;&#20013;&#21435;&#38500;&#31169;&#26377;&#25110;&#20445;&#25252;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#30697;&#38453;&#20998;&#35299;&#23558;&#36755;&#20837;&#34920;&#31034;&#25237;&#24433;&#21040;&#19982;&#20445;&#25252;&#20449;&#24687;&#21327;&#26041;&#24046;&#36739;&#23567;&#30340;&#26041;&#21521;&#19978;&#65292;&#32780;&#19981;&#26159;&#20687;&#22240;&#24335;&#20998;&#35299;&#26041;&#27861;&#37027;&#26679;&#26368;&#22823;&#21270;&#21327;&#26041;&#24046;&#12290;&#25105;&#20204;&#20174;&#32447;&#24615;&#20449;&#24687;&#21024;&#38500;&#24320;&#22987;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20869;&#26680;&#23558;&#31639;&#27861;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#20449;&#24687;&#21024;&#38500;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21024;&#38500;&#20445;&#25252;&#20449;&#24687;&#21518;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20445;&#30041;&#20102;&#26356;&#22909;&#30340;&#20027;&#35201;&#20219;&#21153;&#24615;&#33021;&#65292;&#32780;&#19981;&#20687;&#20197;&#21069;&#30340;&#24037;&#20316;&#37027;&#26679;&#21024;&#38500;&#35813;&#20449;&#24687;&#21066;&#24369;&#20102;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#21482;&#38656;&#23569;&#37327;&#30340;&#34987;&#20445;&#25252;&#23646;&#24615;&#25968;&#25454;&#21363;&#21487;&#21435;&#38500;&#26377;&#20851;&#36825;&#20123;&#23646;&#24615;&#30340;&#20449;&#24687;&#65292;&#36825;&#38477;&#20302;&#20102;&#25935;&#24863;&#25968;&#25454;&#30340;&#26292;&#38706;&#65292;&#24182;&#26356;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#12290;&#21487;&#20197;&#22312;https://github.com/jasonshaoshun/SAL&#19978;&#25214;&#21040;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a simple and effective method (Spectral Attribute removaL; SAL) to remove private or guarded information from neural representations. Our method uses matrix decomposition to project the input representations into directions with reduced covariance with the guarded information rather than maximal covariance as factorization methods normally use. We begin with linear information removal and proceed to generalize our algorithm to the case of nonlinear information removal using kernels. Our experiments demonstrate that our algorithm retains better main task performance after removing the guarded information compared to previous work. In addition, our experiments demonstrate that we need a relatively small amount of guarded attribute data to remove information about these attributes, which lowers the exposure to sensitive data and is more suitable for low-resource scenarios. Code is available at https://github.com/jasonshaoshun/SAL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22269;&#23478;&#26377;&#32447;&#30005;&#35270;&#26032;&#38395;&#23545;&#32654;&#22269;&#26412;&#22303;&#25253;&#32440;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24403;&#22320;&#25253;&#32440;&#30340;&#20869;&#23481;&#20250;&#22240;&#20026;&#24403;&#22320; FNC &#35266;&#20247;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#36235;&#21521;&#20110; FNC &#30340;&#20542;&#21521;&#65292;&#24182;&#19988;&#26377;&#32447;&#30005;&#35270;&#20542;&#21521;&#20250;&#26497;&#21270;&#22320;&#26041;&#26032;&#38395;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2202.07269</link><description>&lt;p&gt;
&#23186;&#20307;&#20542;&#21521;&#26159;&#20855;&#26377;&#20256;&#26579;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Media Slant is Contagious. (arXiv:2202.07269v2 [econ.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22269;&#23478;&#26377;&#32447;&#30005;&#35270;&#26032;&#38395;&#23545;&#32654;&#22269;&#26412;&#22303;&#25253;&#32440;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24403;&#22320;&#25253;&#32440;&#30340;&#20869;&#23481;&#20250;&#22240;&#20026;&#24403;&#22320; FNC &#35266;&#20247;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#36235;&#21521;&#20110; FNC &#30340;&#20542;&#21521;&#65292;&#24182;&#19988;&#26377;&#32447;&#30005;&#35270;&#20542;&#21521;&#20250;&#26497;&#21270;&#22320;&#26041;&#26032;&#38395;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#23186;&#20307;&#20542;&#21521;&#30340;&#20256;&#25773;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#22269;&#23478;&#26377;&#32447;&#30005;&#35270;&#26032;&#38395;&#23545;&#32654;&#22269;&#26412;&#22303;&#25253;&#32440;&#65288;2005-2008&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110; Fox News Channel&#65288;FNC&#65289;&#12289;CNN &#21644; MSNBC &#20869;&#23481;&#30340;&#26377;&#32447;&#30005;&#35270;&#20542;&#21521;&#25991;&#26412;&#24230;&#37327;&#26041;&#27861;&#65292;&#20998;&#26512;&#22320;&#26041;&#25253;&#32440;&#22914;&#20309;&#37319;&#29992; FNC &#30340;&#20542;&#21521;&#32780;&#19981;&#26159; CNN/MSNBC &#30340;&#20542;&#21521;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22320;&#26041;&#26032;&#38395;&#38543;&#30528;&#24403;&#22320; FNC &#35266;&#20247;&#20154;&#25968;&#30340;&#22806;&#37096;&#22686;&#38271;&#32780;&#21464;&#24471;&#26356;&#21152;&#31867;&#20284;&#20110; FNC &#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#36716;&#21464;&#19981;&#20165;&#38480;&#20110;&#20174;&#26377;&#32447;&#30005;&#35270;&#20511;&#37492;&#65292;&#32780;&#26159;&#22320;&#26041;&#25253;&#32440;&#33258;&#36523;&#20869;&#23481;&#30340;&#25913;&#21464;&#12290;&#27492;&#22806;&#65292;&#26377;&#32447;&#30005;&#35270;&#20542;&#21521;&#26497;&#21270;&#20102;&#22320;&#26041;&#26032;&#38395;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the diffusion of media slant, specifically how partisan content from national cable news affects local newspapers in the U.S., 2005-2008. We use a text-based measure of cable news slant trained on content from Fox News Channel (FNC), CNN, and MSNBC to analyze how local newspapers adopt FNC's slant over CNN/MSNBC's. Our findings show that local news becomes more similar to FNC content in response to an exogenous increase in local FNC viewership. This shift is not limited to borrowing from cable news, but rather, local newspapers' own content changes. Further, cable TV slant polarizes local news content.
&lt;/p&gt;</description></item></channel></rss>