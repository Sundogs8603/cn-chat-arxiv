<rss version="2.0"><channel><title>Chinese Chat Arxiv CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#31995;&#32479;&#26469;&#35757;&#32451;&#20248;&#31168;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#25968;&#25454;&#21435;&#31579;&#36873;&#36755;&#20986;&#26469;&#25552;&#39640;&#20445;&#30041;&#29575;&#65292;A/B&#27979;&#35797;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#25552;&#39640;68%&#30340;&#20445;&#30041;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.06135</link><description>&#29992;&#20110;&#30334;&#19975;&#29992;&#25143;&#30495;&#23454;&#19990;&#30028;&#20114;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22870;&#21169;&#31995;&#32479;

        Rewarding Chatbots for Real-World Engagement with Millions of Users. (arXiv:2303.06135v1 [cs.CL])

        [http://arxiv.org/abs/2303.06135](http://arxiv.org/abs/2303.06135)

        &#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#30340;&#31038;&#20132;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#37096;&#32626;&#65292;&#29992;&#20110;&#38386;&#32842;&#12290;&#34429;&#28982;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#35821;&#35328;&#33021;&#21147;&#21644;&#27969;&#21033;&#24230;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#19968;&#23450;&#24341;&#20154;&#20837;&#32988;&#65292;&#26377;&#26102;&#20505;&#24456;&#38590;&#21560;&#24341;&#29992;&#25143;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#24320;&#21457;&#27880;&#37325;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#31038;&#20132;&#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#22686;&#24378;&#20445;&#30041;&#29575;&#65292;&#29305;&#21035;&#26159;&#32771;&#23519;&#20102;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#39640;&#25928;&#24320;&#21457;&#26497;&#20855;&#21560;&#24341;&#21147;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20174;&#29992;&#25143;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#33258;&#21160;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#20010;&#22870;&#21169;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#26102;&#21487;&#20197;&#29992;&#26469;&#25298;&#32477;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#20135;&#29983;&#30340;&#20302;&#20998;&#26679;&#26412;&#21709;&#24212;&#12290;&#24341;&#20837;&#20102;&#30452;&#35266;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230; (MCL)&#65292;&#20316;&#20026;&#34913;&#37327;&#37096;&#32626;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21442;&#19982;&#27700;&#24179;&#30340;&#20195;&#29702;&#12290;&#22312;Chai Research&#24179;&#21488;&#19978;&#65292;&#23545;&#27599;&#26085;&#26032;&#30340;10,000&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#29992;&#25143;&#32452;&#36827;&#34892;&#30340;A/B&#27979;&#35797;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;MCL&#25552;&#39640;&#20102;&#26368;&#22810;70&#65285;&#65292;&#36825;&#30456;&#24403;&#20110;&#23558;&#20445;&#30041;&#29575;&#20174;40&#65285;&#22686;&#21152;&#21040;68&#65285;&#12290;

        The emergence of pretrained large language models has led to the deployment of a range of social chatbots for chitchat. Although these chatbots demonstrate language ability and fluency, they are not guaranteed to be engaging and can struggle to retain users. This work investigates the development of social chatbots that prioritize user engagement to enhance retention, specifically examining the use of human feedback to efficiently develop highly engaging chatbots. The proposed approach uses automatic pseudo-labels collected from user interactions to train a reward model that can be used to reject low-scoring sample responses generated by the chatbot model at inference time. Intuitive evaluation metrics, such as mean conversation length (MCL), are introduced as proxies to measure the level of engagement of deployed chatbots. A/B testing on groups of 10,000 new daily chatbot users on the Chai Research platform shows that this approach increases the MCL by up to 70%, which translates to a
        </description></item><item><title>&#20004;&#20010;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#25509;&#21463;&#26377;&#24433;&#21709;&#21147;&#20449;&#24687;&#21518;&#30340;&#24515;&#29702;&#21464;&#21270;&#12290; &#36825;&#20123;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#26089;&#26399;&#25509;&#35302;&#35821;&#21477;&#21487;&#33021;&#20250;&#22686;&#24378;&#21518;&#26469;&#30340;&#30495;&#23454;&#24615;&#27979;&#35797;&#35780;&#20998;&#65292;&#32780;&#19988;&#20351;&#29992;&#30495;&#30456;&#20197;&#22806;&#30340;&#23646;&#24615;&#21644;&#20351;&#29992;&#30456;&#21516;&#23646;&#24615;&#36827;&#34892;&#26292;&#38706;&#21644;&#27979;&#35797;&#26102;&#34920;&#29616;&#20986;&#32570;&#20047;&#25928;&#24212;</title><link>http://arxiv.org/abs/2303.06074</link><description>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#26131;&#24863;&#24615;

        Susceptibility to Influence of Large Language Models. (arXiv:2303.06074v1 [cs.CL])

        [http://arxiv.org/abs/2303.06074](http://arxiv.org/abs/2303.06074)

        &#20004;&#20010;&#30740;&#31350;&#27979;&#35797;&#20102;&#19968;&#20010;&#20551;&#35774;&#65306;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#24314;&#27169;&#25509;&#21463;&#26377;&#24433;&#21709;&#21147;&#20449;&#24687;&#21518;&#30340;&#24515;&#29702;&#21464;&#21270;&#12290;&#31532;&#19968;&#39033;&#30740;&#31350;&#27979;&#35797;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24433;&#21709;&#27169;&#24335;-&#34394;&#20551;&#30340;&#30495;&#30456;&#25928;&#24212;&#65288;ITE&#65289;&#65292;&#20854;&#20013;&#26089;&#26399;&#25509;&#35302;&#35821;&#21477;&#65288;&#20363;&#22914;&#36890;&#36807;&#35780;&#20215;&#20854;&#20852;&#36259;&#65289;&#20250;&#22686;&#24378;&#21518;&#26469;&#30340;&#30495;&#23454;&#24615;&#27979;&#35797;&#35780;&#20998;&#12290;&#20351;&#29992;&#22312;&#32447;&#23454;&#39564;&#20174;1000&#21517;&#20154;&#31867;&#21442;&#19982;&#32773;&#21644;1000&#20010;&#27169;&#25311;&#21442;&#19982;&#32773;&#25910;&#38598;&#20102;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#24037;&#31243;&#25552;&#31034;&#21644;LLM&#23436;&#25104;&#20102;&#25910;&#38598;&#12290;&#27599;&#20010;&#21442;&#19982;&#32773;&#25910;&#38598;&#20102;64&#20010;&#35780;&#20998;&#65292;&#20351;&#29992;&#20197;&#19979;&#25152;&#26377;&#26292;&#38706;-&#27979;&#35797;&#23646;&#24615;&#30340;&#32452;&#21512;&#65306;&#30495;&#30456;&#65292;&#20852;&#36259;&#65292;&#24773;&#24863;&#21644;&#37325;&#35201;&#24615;&#12290;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#32467;&#26524;&#37325;&#26032;&#30830;&#35748;&#20102;ITE&#65292;&#24182;&#22312;&#30495;&#30456;&#20197;&#22806;&#30340;&#23646;&#24615;&#21644;&#20351;&#29992;&#30456;&#21516;&#23646;&#24615;&#36827;&#34892;&#26292;&#38706;&#21644;&#27979;&#35797;&#26102;&#34920;&#29616;&#20986;&#32570;&#20047;&#25928;&#24212;&#12290;LLM&#27169;&#25311;&#21442;&#19982;&#32773;&#30340;&#32467;&#26524;&#20063;&#21457;&#29616;&#20102;&#30456;&#21516;&#30340;&#25928;&#24212;&#27169;&#24335;&#12290;&#31532;&#20108;&#39033;&#30740;&#31350;&#28041;&#21450;&#19968;&#31181;&#29305;&#23450;&#30340;&#24433;&#21709;&#27169;&#24335;-&#36890;&#20439;&#30340;&#26032;&#38395;&#26694;&#26550;&#26469;&#28608;&#21457;

        Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement (through, for example, rating its interest) boosts a later truthfulness test rating. Data was collected from 1000 human participants using an online experiment, and 1000 simulated participants using engineered prompts and LLM completion. 64 ratings per participant were collected, using all exposure-test combinations of the attributes: truth, interest, sentiment and importance. The results for human participants reconfirmed the ITE, and demonstrated an absence of effect for attributes other than truth, and when the same attribute is used for exposure and test. The same pattern of effects was found for LLM-simulated participants. The second study concerns a specific mode of influence - populist framing of news to inc
        </description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;wav2vec2&#30340;ASR&#27169;&#22411;&#22312;&#24503;&#35821;&#25991;&#21270;&#36951;&#20135;&#32034;&#24341;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#19982;&#21830;&#19994;&#20113;&#21644;&#19987;&#26377;&#26381;&#21153;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30446;&#21069;&#21487;&#20197;&#23454;&#29616;90&#65285;&#20197;&#19978;&#30340;&#35782;&#21035;&#29575;&#65292;&#20294;&#36825;&#20123;&#25968;&#23383;&#24456;&#24555;&#23601;&#20250;&#38477;&#20302;&#65292;&#19968;&#26086;&#24405;&#38899;&#20855;&#26377;&#21463;&#38480;&#30340;&#38899;&#39057;&#36136;&#37327;&#25110;&#20351;&#29992;&#38750;&#26085;&#24120;&#25110;&#36807;&#26102;&#30340;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2303.06026</link><description>wav2vec&#21450;&#20854;&#22312;&#24503;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#28508;&#21147;&#65306;&#25991;&#21270;&#36951;&#20135;&#22330;&#26223;&#19979;&#21487;&#29992;ASR&#25216;&#26415;&#30340;&#27604;&#36739;&#35780;&#20272;

        wav2vec and its current potential to Automatic Speech Recognition in German for the usage in Digital History: A comparative assessment of available ASR-technologies for the use in cultural heritage contexts. (arXiv:2303.06026v1 [eess.AS])

        [http://arxiv.org/abs/2303.06026](http://arxiv.org/abs/2303.06026)

        &#22312;&#26412;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#30340;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#65292;&#20197;&#35780;&#20272;&#35813;&#25216;&#26415;&#22312;&#25968;&#23383;&#20154;&#25991;&#21644;&#25991;&#21270;&#36951;&#20135;&#32034;&#24341;&#21270;&#30340;&#26356;&#22823;&#32972;&#26223;&#19979;&#30340;&#24403;&#21069;&#28508;&#21147;&#12290;&#38500;&#20102;&#26412;&#25991;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#22522;&#20110;wav2vec2&#30340;&#35821;&#38899;&#36716;&#25991;&#23383;&#27169;&#22411;&#65292;&#21516;&#26102;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19982;&#21830;&#29992;&#20113;&#21644;&#19987;&#26377;&#26381;&#21153;&#36827;&#34892;&#27604;&#36739;&#30340;&#21382;&#21490;&#24405;&#38899;&#35821;&#26009;&#24211;&#19978;&#30340;&#34920;&#29616;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#27169;&#22411;&#21462;&#24471;&#20102;&#20013;&#31561;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#19987;&#26377;&#20113;&#26381;&#21153;&#34920;&#29616;&#26356;&#22909;&#12290;&#27491;&#22914;&#25105;&#20204;&#30340;&#32467;&#26524;&#25152;&#26174;&#31034;&#30340;&#65292;&#30446;&#21069;&#21487;&#20197;&#23454;&#29616;90&#65285;&#20197;&#19978;&#30340;&#35782;&#21035;&#29575;&#65292;&#20294;&#36825;&#20123;&#25968;&#23383;&#24456;&#24555;&#23601;&#20250;&#38477;&#20302;&#65292;&#19968;&#26086;&#24405;&#38899;&#20855;&#26377;&#21463;&#38480;&#30340;&#38899;&#39057;&#36136;&#37327;&#25110;&#20351;&#29992;&#38750;&#26085;&#24120;&#25110;&#36807;&#26102;&#30340;&#35821;&#35328;&#12290;&#24503;&#35821;&#20013;&#19981;&#21516;&#26041;&#35328;&#21644;&#21475;&#38899;&#30340;&#31181;&#31867;&#32321;&#22810;&#26159;&#19968;&#20010;&#22823;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#30446;&#21069;&#21487;&#29992;&#30340;&#35782;&#21035;&#36136;&#37327;&#26159;

        In this case study we trained and published a state-of-the-art open-source model for Automatic Speech Recognition (ASR) for German to evaluate the current potential of this technology for the use in the larger context of Digital Humanities and cultural heritage indexation. Along with this paper we publish our wav2vec2 based speech to text model while we evaluate its performance on a corpus of historical recordings we assembled compared against commercial cloud-based and proprietary services. While our model achieves moderate results, we see that proprietary cloud services fare significantly better. As our results show, recognition rates over 90 percent can currently be achieved, however, these numbers drop quickly once the recordings feature limited audio quality or use of non-every day or outworn language. A big issue is the high variety of different dialects and accents in the German language. Nevertheless, this paper highlights that the currently available quality of recognition is 
        </description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31995;&#32479;&#20013;&#20303;&#38498;&#20803;&#20449;&#24687;&#23545;&#20110;&#20986;&#38498;&#25688;&#35201;&#29983;&#25104;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#32534;&#30721;&#20803;&#20449;&#24687;&#30340;&#27169;&#22411;&#19982;&#22522;&#26412;&#27169;&#22411;&#30456;&#27604;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25688;&#35201;&#29983;&#25104;&#30340;ROUGE-1&#21644;BERTScore&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2303.06002</link><description>&#20303;&#38498;&#20803;&#20449;&#24687;&#23545;&#25277;&#35937;&#20986;&#38498;&#25688;&#35201;&#29983;&#25104;&#26159;&#21542;&#26377;&#29992;&#65311;

        Is In-hospital Meta-information Useful for Abstractive Discharge Summary Generation?. (arXiv:2303.06002v1 [cs.CL])

        [http://arxiv.org/abs/2303.06002](http://arxiv.org/abs/2303.06002)

        &#22312;&#30149;&#20154;&#20303;&#38498;&#26399;&#38388;&#65292;&#21307;&#29983;&#24517;&#39035;&#35760;&#24405;&#30149;&#20154;&#30340;&#26085;&#24120;&#35266;&#23519;&#32467;&#26524;&#65292;&#24182;&#22312;&#30149;&#20154;&#20986;&#38498;&#26102;&#23558;&#23427;&#20204;&#24635;&#32467;&#20026;&#19968;&#20221;&#31616;&#30701;&#30340;&#25991;&#26723;&#65292;&#31216;&#20026;&#8220;&#20986;&#38498;&#25688;&#35201;&#8221;&#12290;&#33258;&#21160;&#21270;&#29983;&#25104;&#20986;&#38498;&#25688;&#35201;&#21487;&#20197;&#22823;&#22823;&#20943;&#36731;&#21307;&#29983;&#30340;&#36127;&#25285;&#65292;&#24182;&#26368;&#36817;&#24471;&#21040;&#30740;&#31350;&#30028;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#20197;&#24207;&#21015;&#21040;&#24207;&#21015;&#26550;&#26500;&#20026;&#22522;&#30784;&#30340;&#20986;&#38498;&#25688;&#35201;&#29983;&#25104;&#30740;&#31350;&#21482;&#20851;&#27880;&#20303;&#38498;&#35760;&#24405;&#20316;&#20026;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#36824;&#20855;&#26377;&#20016;&#23500;&#30340;&#32467;&#26500;&#21270;&#20803;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#21307;&#38498;&#12289;&#21307;&#29983;&#12289;&#30142;&#30149;&#12289;&#20303;&#38498;&#26102;&#38388;&#31561;&#65289;&#65292;&#21487;&#33021;&#20250;&#26377;&#25152;&#24110;&#21161;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21307;&#30103;&#20803;&#20449;&#24687;&#23545;&#24635;&#32467;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20174;EHR&#31995;&#32479;&#33719;&#21462;&#20102;&#22235;&#31181;&#31867;&#22411;&#30340;&#20803;&#20449;&#24687;&#65292;&#24182;&#23558;&#27599;&#20010;&#20803;&#20449;&#24687;&#32534;&#30721;&#21040;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#12290;&#20351;&#29992;&#26085;&#26412;EHR&#65292;&#32534;&#30721;&#20803;&#20449;&#24687;&#30340;&#27169;&#22411;&#23558;ROUGE-1&#25552;&#39640;&#20102;&#26368;&#22810;4.45&#20010;&#28857;&#65292;BERTScore&#25552;&#39640;&#20102;3.77&#20010;&#28857;&#65292;&#36229;&#36807;&#20102;&#22522;&#26412;&#27169;&#22411;&#12290;

        During the patient's hospitalization, the physician must record daily observations of the patient and summarize them into a brief document called "discharge summary" when the patient is discharged. Automated generation of discharge summary can greatly relieve the physicians' burden, and has been addressed recently in the research community. Most previous studies of discharge summary generation using the sequence-to-sequence architecture focus on only inpatient notes for input. However, electric health records (EHR) also have rich structured metadata (e.g., hospital, physician, disease, length of stay, etc.) that might be useful. This paper investigates the effectiveness of medical meta-information for summarization tasks. We obtain four types of meta-information from the EHR systems and encode each meta-information into a sequence-to-sequence model. Using Japanese EHRs, meta-information encoded models increased ROUGE-1 by up to 4.45 points and BERTScore by 3.77 points over the vanilla 
        </description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#23558;&#29992;&#25143;&#24086;&#23376;&#20998;&#27573;&#21040;&#26377;&#24847;&#20041;&#30340;&#26102;&#38388;&#36724;&#20013;&#65292;&#20197;&#25552;&#39640;&#25163;&#21160;&#27880;&#37322;&#30340;&#36136;&#37327;&#21644;&#25104;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#35780;&#20272;&#26102;&#38388;&#36724;&#12290;</title><link>http://arxiv.org/abs/2303.05891</link><description>&#32437;&#21521;&#29992;&#25143;&#24086;&#23376;&#26102;&#38388;&#36724;&#30340;&#21019;&#24314;&#21644;&#35780;&#20272;

        Creation and evaluation of timelines for longitudinal user posts. (arXiv:2303.05891v1 [cs.CL])

        [http://arxiv.org/abs/2303.05891](http://arxiv.org/abs/2303.05891)

        &#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#24863;&#20852;&#36259;&#65292;&#29305;&#21035;&#26159;&#38543;&#26102;&#38388;&#25512;&#31227;&#30340;&#25991;&#26412;&#24086;&#23376;&#12290;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#31181;&#19968;&#33268;&#30340;&#26041;&#24335;&#23558;&#29992;&#25143;&#24086;&#23376;&#20998;&#27573;&#21040;&#26377;&#24847;&#20041;&#30340;&#26102;&#38388;&#36724;&#20013;&#65292;&#20197;&#25552;&#39640;&#25163;&#21160;&#27880;&#37322;&#30340;&#36136;&#37327;&#21644;&#25104;&#26412;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#32437;&#21521;&#29992;&#25143;&#24086;&#23376;&#20998;&#27573;&#25104;&#21487;&#33021;&#21253;&#21547;&#29992;&#25143;&#34892;&#20026;&#21464;&#21270;&#26377;&#36259;&#26102;&#21051;&#30340;&#26102;&#38388;&#36724;&#65292;&#22522;&#20110;&#20182;&#20204;&#30340;&#22312;&#32447;&#21457;&#24067;&#27963;&#21160;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#35780;&#20272;&#26102;&#38388;&#36724;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20004;&#20010;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#32972;&#26223;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#25490;&#21517;&#38752;&#21069;&#30340;&#26102;&#38388;&#36724;&#30340;&#35821;&#35328;&#20869;&#23481;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;

        There is increasing interest to work with user generated content in social media, especially textual posts over time. Currently there is no consistent way of segmenting user posts into timelines in a meaningful way that improves the quality and cost of manual annotation. Here we propose a set of methods for segmenting longitudinal user posts into timelines likely to contain interesting moments of change in a user's behaviour, based on their online posting activity. We also propose a novel framework for evaluating timelines and show its applicability in the context of two different social media datasets. Finally, we present a discussion of the linguistic content of highly ranked timelines.
        </description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#20116;&#20010;&#26041;&#38754;&#20171;&#32461;&#20102;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23637;&#26395;&#20102;&#39044;&#35757;&#32451;&#26102;&#20195;&#35821;&#35328;&#24314;&#27169;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.05759</link><description>&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;&#65306;&#26368;&#26032;&#21457;&#23637;&#21644;&#23637;&#26395;

        An Overview on Language Models: Recent Developments and Outlook. (arXiv:2303.05759v1 [cs.CL])

        [http://arxiv.org/abs/2303.05759](http://arxiv.org/abs/2303.05759)

        &#35821;&#35328;&#24314;&#27169;&#30740;&#31350;&#25991;&#26412;&#20018;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#12289;&#35821;&#38899;&#35782;&#21035;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#12290;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#65288;CLM&#65289;&#26088;&#22312;&#20197;&#22240;&#26524;&#26041;&#24335;&#39044;&#27979;&#35821;&#35328;&#24207;&#21015;&#30340;&#27010;&#29575;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#27010;&#24565;&#65292;&#24182;&#21487;&#29992;&#20110;&#22240;&#26524;&#24207;&#21015;&#24314;&#27169;&#21644;&#19979;&#28216;&#24212;&#29992;&#30340;&#24494;&#35843;&#12290;PLMs&#20855;&#26377;&#33258;&#24049;&#30340;&#35757;&#32451;&#33539;&#24335;&#65288;&#36890;&#24120;&#26159;&#33258;&#25105;&#30417;&#30563;&#65289;&#24182;&#22312;&#29616;&#20195;NLP&#31995;&#32479;&#20013;&#20805;&#24403;&#22522;&#30784;&#27169;&#22411;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#20174;&#35821;&#35328;&#21333;&#20803;&#12289;&#32467;&#26500;&#12289;&#35757;&#32451;&#26041;&#27861;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#24212;&#29992;&#20116;&#20010;&#26041;&#38754;&#20171;&#32461;&#20102;CLMs&#21644;PLMs&#65292;&#35752;&#35770;&#20102;CLMs&#21644;PLMs&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23637;&#26395;&#20102;&#39044;&#35757;&#32451;&#26102;&#20195;&#35821;&#35328;&#24314;&#27169;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;

        Language modeling studies the probability distributions over strings of texts. It is one of the most fundamental tasks in natural language processing (NLP). It has been widely used in text generation, speech recognition, machine translation, etc. Conventional language models (CLMs) aim to predict the probability of linguistic sequences in a causal manner. In contrast, pre-trained language models (PLMs) cover broader concepts and can be used in both causal sequential modeling and fine-tuning for downstream applications. PLMs have their own training paradigms (usually self-supervised) and serve as foundation models in modern NLP systems. This overview paper provides an introduction to both CLMs and PLMs from five aspects, i.e., linguistic units, structures, training methods, evaluation methods, and applications. Furthermore, we discuss the relationship between CLMs and PLMs and shed light on the future directions of language modeling in the pre-trained era.
        </description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#31934;&#24230;&#19988;&#20869;&#23384;&#39640;&#25928;&#30340;&#35270;&#39057;&#21644;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;MuLTI&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#36739;&#38271;&#30340;&#24207;&#21015;&#65292;&#24182;&#19988;&#20869;&#23384;&#21344;&#29992;&#36739;&#23569;&#12290;MuLTI&#36890;&#36807;&#29305;&#24449;&#37319;&#26679;&#21644;&#27880;&#24847;&#21147;&#27169;&#22359;&#23454;&#29616;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#29305;&#24449;&#34701;&#21512;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36866;&#37197;&#22120;&#21644;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05707</link><description>MuLTI: &#20351;&#29992;&#22810;&#36335;&#24452;&#21462;&#26679;&#22120;&#21644;&#22810;&#39033;&#36873;&#25321;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#35270;&#39057;&#21644;&#35821;&#35328;&#29702;&#35299;

        MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler and Multiple Choice Modeling. (arXiv:2303.05707v1 [cs.CV])

        [http://arxiv.org/abs/2303.05707](http://arxiv.org/abs/2303.05707)

        &#35270;&#39057;&#19982;&#35821;&#35328;&#30340;&#29702;&#35299;&#22312;&#24037;&#19994;&#20013;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#35270;&#39057;&#38382;&#31572;&#12289;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#29616;&#26377;&#30340;&#35270;&#39057;&#19982;&#35821;&#35328;&#29702;&#35299;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#37325;&#37327;&#32423;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#23427;&#20204;&#28040;&#32791;&#22823;&#37327;&#30340;GPU&#20869;&#23384;&#12290;&#29305;&#21035;&#26159;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#22788;&#29702;&#23494;&#38598;&#30340;&#35270;&#39057;&#24103;&#25110;&#38271;&#25991;&#26412;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#31934;&#24230;&#19988;&#20869;&#23384;&#39640;&#25928;&#30340;&#35270;&#39057;&#21644;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;MuLTI&#65292;&#36890;&#36807;&#29305;&#24449;&#37319;&#26679;&#21644;&#27880;&#24847;&#21147;&#27169;&#22359;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#29305;&#24449;&#34701;&#21512;&#12290;&#22240;&#27492;&#65292;MuLTI&#21487;&#20197;&#22788;&#29702;&#36739;&#38271;&#30340;&#24207;&#21015;&#65292;&#24182;&#19988;&#20869;&#23384;&#21344;&#29992;&#36739;&#23569;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36866;&#37197;&#22120;&#21040;&#32534;&#30721;&#22120;&#20013;&#65292;&#36890;&#36807;&#24494;&#35843;&#27973;&#23618;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#28040;&#32791;&#36739;&#23569;&#30340;GPU&#20869;&#23384;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;

        Video-and-language understanding has a variety of applications in the industry, such as video question answering, text-video retrieval and multi-label classification. Existing video-and-language understanding methods generally adopt heavy multi-modal encoders and feature fusion modules, which consume large amounts of GPU memory. Especially, they have difficulty dealing with dense video frames or long text that are prevalent in industrial applications. In this paper, we propose MuLTI, a highly accurate and memory-efficient video-and-language understanding model that achieves efficient and effective feature fusion through feature sampling and attention modules. Therefore, MuLTI can handle longer sequences with limited GPU memory. Then, we introduce an attention-based adapter to the encoders, which finetunes the shallow features to improve the model's performance with low GPU memory consumption. Finally, to further improve the model's performance, we introduce a new pretraining task named
        </description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#20013;&#23384;&#22312;&#30340;&#24102;&#26377;&#38472;&#35268;&#38475;&#20064;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#24182;&#27604;&#36739;&#20102;&#19982;&#20043;&#23545;&#24212;&#30340;&#25991;&#26412;&#34164;&#28085;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37319;&#29992;&#25991;&#26412;&#34164;&#28085;&#30340;&#26126;&#30830;&#36923;&#36753;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#31038;&#20132;&#31038;&#21306;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#21435;&#20559;&#35265;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.05670</link><description>&#36923;&#36753;&#25171;&#30772;&#20559;&#35265;&#65306;&#25991;&#26412;&#34164;&#28085;&#32531;&#35299;&#20102;&#24102;&#26377;&#38472;&#35268;&#38475;&#20064;&#30340;&#21477;&#23376;&#25512;&#29702;&#12290;

        Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning. (arXiv:2303.05670v1 [cs.CL])

        [http://arxiv.org/abs/2303.05670](http://arxiv.org/abs/2303.05670)

        &#30001;&#20110;&#39044;&#35757;&#32451;&#21477;&#23376;&#32534;&#30721;&#22120;&#30340;&#30456;&#20284;&#24230;&#23398;&#20064;&#30446;&#26631;&#65292;&#23427;&#20204;&#32463;&#24120;&#20869;&#21270;&#21453;&#26144;&#20854;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#30340;&#38472;&#35268;&#38475;&#20064;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#23384;&#22312;&#20110;&#27969;&#34892;&#21477;&#23376;&#34920;&#31034;&#27169;&#22411;&#20013;&#30340;&#20960;&#31181;&#20851;&#20110;&#19981;&#21516;&#31038;&#21306;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#27169;&#22411;&#19982;&#23398;&#20064;&#21508;&#31181;&#19979;&#28216;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#25991;&#26412;&#34164;&#28085;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#25991;&#26412;&#30456;&#20284;&#24230;&#30340;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25991;&#26412;&#34164;&#28085;&#23398;&#20064;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#25991;&#26412;&#34164;&#28085;&#30340;&#26126;&#30830;&#36923;&#36753;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#31038;&#20132;&#31038;&#21306;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#21435;&#20559;&#35265;&#30340;&#36807;&#31243;&#12290;

        Due to their similarity-based learning objectives, pretrained sentence encoders often internalize stereotypical assumptions that reflect the social biases that exist within their training corpora. In this paper, we describe several kinds of stereotypes concerning different communities that are present in popular sentence representation models, including pretrained next sentence prediction and contrastive sentence representation models. We compare such models to textual entailment models that learn language logic for a variety of downstream language understanding tasks. By comparing strong pretrained models based on text similarity with textual entailment learning, we conclude that the explicit logic learning with textual entailment can significantly reduce bias and improve the recognition of social communities, without an explicit de-biasing process
        </description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#36127;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#22521;&#35757;&#38454;&#27573;&#29983;&#25104;&#26377;&#25928;&#30340;&#21512;&#25104;&#24320;&#25918;&#31867;&#21035;&#26679;&#26412;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#19990;&#30028;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36741;&#21161;&#30340;&#19968;&#23545;&#22810;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.05581</link><description>&#33258;&#36866;&#24212;&#36127;&#26679;&#26412;&#30340;&#24320;&#25918;&#19990;&#30028;&#20998;&#31867;

        Open World Classification with Adaptive Negative Samples. (arXiv:2303.05581v1 [cs.CL])

        [http://arxiv.org/abs/2303.05581](http://arxiv.org/abs/2303.05581)

        &#24320;&#25918;&#19990;&#30028;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20855;&#26377;&#20851;&#38190;&#23454;&#38469;&#24847;&#20041;&#21644;&#24433;&#21709;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#24320;&#25918;&#25110;&#26410;&#30693;&#31867;&#21035;&#25968;&#25454;&#20165;&#22312;&#25512;&#26029;&#38454;&#27573;&#26174;&#31034;&#65292;&#22240;&#27492;&#23547;&#25214;&#20855;&#26377;&#36866;&#24403;&#20915;&#31574;&#36793;&#30028;&#20197;&#23481;&#32435;&#24050;&#30693;&#31867;&#21035;&#30340;&#35782;&#21035;&#21644;&#24320;&#25918;&#31867;&#21035;&#30340;&#21306;&#20998;&#30340;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#21463;&#21040;&#35757;&#32451;&#38454;&#27573;&#32570;&#20047;&#26377;&#25928;&#30340;&#24320;&#25918;&#31867;&#21035;&#25968;&#25454;&#25110;&#32570;&#20047;&#23398;&#20064;&#36866;&#24403;&#20915;&#31574;&#36793;&#30028;&#30340;&#33391;&#22909;&#26426;&#21046;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#36127;&#26679;&#26412;&#65288;ANS&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#22521;&#35757;&#38454;&#27573;&#29983;&#25104;&#26377;&#25928;&#30340;&#21512;&#25104;&#24320;&#25918;&#31867;&#21035;&#26679;&#26412;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#21069;&#30340;&#30693;&#35782;&#25110;&#22806;&#37096;&#25968;&#25454;&#38598;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36741;&#21161;&#30340;&#19968;&#23545;&#22810;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#29983;&#25104;&#30340;&#36127;&#26679;&#26412;&#65292;&#24182;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#38408;&#20540;&#25628;&#32034;&#38454;&#27573;&#12290;

        Open world classification is a task in natural language processing with key practical relevance and impact. Since the open or {\em unknown} category data only manifests in the inference phase, finding a model with a suitable decision boundary accommodating for the identification of known classes and discrimination of the open category is challenging. The performance of existing models is limited by the lack of effective open category data during the training stage or the lack of a good mechanism to learn appropriate decision boundaries. We propose an approach based on \underline{a}daptive \underline{n}egative \underline{s}amples (ANS) designed to generate effective synthetic open category samples in the training stage and without requiring any prior knowledge or external datasets. Empirically, we find a significant advantage in using auxiliary one-versus-rest binary classifiers, which effectively utilize the generated negative samples and avoid the complex threshold-seeking stage in pr
        </description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoSyn&#30340;&#26426;&#21046;&#65292;&#21487;&#20197;&#26816;&#27979;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#12290;&#35813;&#26426;&#21046;&#39318;&#20808;&#23545;&#29992;&#25143;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#65292;&#28982;&#21518;&#20351;&#29992;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#20132;&#20114;&#26426;&#21046;&#32852;&#21512;&#24314;&#27169;&#29992;&#25143;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.03387</link><description>CoSyn&#65306;&#20351;&#29992;&#19978;&#19979;&#25991;&#21327;&#21516;&#20316;&#29992;&#30340;&#21452;&#26354;&#32593;&#32476;&#26816;&#27979;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;

        CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network. (arXiv:2303.03387v2 [cs.LG] UPDATED)

        [http://arxiv.org/abs/2303.03387](http://arxiv.org/abs/2303.03387)

        &#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#36827;&#34892;&#22312;&#32447;&#23545;&#35805;&#30340;&#24040;&#22823;&#22686;&#38271;&#20063;&#23548;&#33268;&#20102;&#20167;&#24680;&#35328;&#35770;&#30340;&#26174;&#33879;&#22686;&#38271;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#38598;&#20013;&#20110;&#26816;&#27979;&#26126;&#30830;&#30340;&#20167;&#24680;&#35328;&#35770;&#65292;&#36825;&#26159;&#20844;&#24320;&#19988;&#21033;&#29992;&#20196;&#20154;&#35752;&#21388;&#30340;&#30701;&#35821;&#65292;&#20294;&#24456;&#23569;&#26377;&#20851;&#27880;&#26816;&#27979;&#36890;&#36807;&#38388;&#25509;&#25110;&#32534;&#30721;&#35821;&#35328;&#34920;&#31034;&#20167;&#24680;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoSyn&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#25143;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#21327;&#21516;&#20316;&#29992;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#23545;&#35805;&#26641;&#20013;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#12290;CoSyn&#39318;&#20808;&#20351;&#29992;&#26032;&#39062;&#30340;&#21452;&#26354;&#20613;&#37324;&#21494;&#27880;&#24847;&#26426;&#21046;&#21644;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;&#29992;&#25143;&#30340;&#20010;&#20154;&#21382;&#21490;&#21644;&#31038;&#20132;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#26032;&#30340;&#19978;&#19979;&#25991;&#20132;&#20114;&#26426;&#21046;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#32852;&#21512;&#24314;&#27169;&#29992;&#25143;&#30340;&#20010;&#20154;&#19978;&#19979;&#25991;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#35813;&#26426;&#21046;&#28165;&#26224;&#22320;&#25429;&#25417;&#20004;&#32773;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23545;&#20174;&#20004;&#20010;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#30340;&#20449;&#24687;&#37327;&#36827;&#34892;&#29420;&#31435;&#35780;&#20272;&#12290;CoSyn&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;

        The tremendous growth of social media users interacting in online conversations has also led to significant growth in hate speech. Most of the prior works focus on detecting explicit hate speech, which is overt and leverages hateful phrases, with very little work focusing on detecting hate speech that is implicit or denotes hatred through indirect or coded language. In this paper, we present CoSyn, a user- and conversational-context synergized network for detecting implicit hate speech in online conversation trees. CoSyn first models the user's personal historical and social context using a novel hyperbolic Fourier attention mechanism and hyperbolic graph convolution network. Next, we jointly model the user's personal context and the conversational context using a novel context interaction mechanism in the hyperbolic space that clearly captures the interplay between the two and makes independent assessments on the amounts of information to be retrieved from both contexts. CoSyn perform
        </description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35843;&#26597;&#22312;&#21475;&#35821;&#29702;&#35299;&#20013;&#30340;&#21344;&#20301;&#31526;&#65288;Fillers&#65289;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#21644;&#35745;&#31639;&#35282;&#24230;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#35270;&#35282;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#26041;&#24335;&#21576;&#29616;&#32473;&#21475;&#35821;&#29702;&#35299;&#21644;&#23545;&#35805;AI&#31038;&#21306;&#65292;&#24182;&#35752;&#35770;&#27599;&#20010;&#39046;&#22495;&#30340;&#36235;&#21183;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2301.10761</link><description>&#21475;&#35821;&#29702;&#35299;&#20013;&#30340;&#21344;&#20301;&#31526;&#65306;&#35745;&#31639;&#21644;&#24515;&#29702;&#35821;&#35328;&#23398;&#35282;&#24230;

        Fillers in Spoken Language Understanding: Computational and Psycholinguistic Perspectives. (arXiv:2301.10761v3 [cs.CL] UPDATED)

        [http://arxiv.org/abs/2301.10761](http://arxiv.org/abs/2301.10761)

        &#21475;&#35821;&#20013;&#30340;&#19981;&#27969;&#30021;&#65288;&#21363;&#35821;&#35328;&#27969;&#30340;&#25171;&#26029;&#65289;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#22635;&#20805;&#35821;&#65288;&#8220;&#21999;&#8221;&#8220;&#21834;&#8221;&#65289;&#26159;&#30456;&#23545;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#19981;&#27969;&#30021;&#20986;&#29616;&#26368;&#39057;&#32321;&#30340;&#12290;&#28982;&#32780;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27809;&#26377;&#19968;&#20010;&#36164;&#28304;&#23558;&#24433;&#21709;&#21475;&#35821;&#29702;&#35299;&#30340;&#30740;&#31350;&#35270;&#35282;&#27719;&#38598;&#36215;&#26469;&#65292;&#29992;&#20110;&#36825;&#20123;&#35821;&#38899;&#20107;&#20214;&#30340;&#22788;&#29702;&#12290;&#26412;&#25991;&#26088;&#22312;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35843;&#26597;&#21508;&#31181;&#35282;&#24230;&#65307;&#20174;&#32771;&#34385;&#22522;&#30784;&#65288;&#24515;&#29702;&#65289;&#35821;&#35328;&#23398;&#29702;&#35770;&#65292;&#21040;&#20182;&#20204;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#21475;&#35821;&#29702;&#35299;&#31995;&#32479;&#20013;&#30340;&#27880;&#37322;&#21644;&#32771;&#34385;&#65292;&#26368;&#21518;&#20877;&#20174;&#29983;&#25104;&#35282;&#24230;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#26041;&#24335;&#21521;&#21475;&#35821;&#29702;&#35299;&#21644;&#23545;&#35805;AI&#31038;&#21306;&#20171;&#32461;&#36825;&#20123;&#35270;&#35282;&#65292;&#24182;&#35752;&#35770;&#21069;&#36827;&#30340;&#36235;&#21183;&#21644;&#27599;&#20010;&#39046;&#22495;&#30340;&#25361;&#25112;&#12290;

        Disfluencies (i.e. interruptions in the regular flow of speech), are ubiquitous to spoken discourse. Fillers ("uh", "um") are disfluencies that occur the most frequently compared to other kinds of disfluencies. Yet, to the best of our knowledge, there isn't a resource that brings together the research perspectives influencing Spoken Language Understanding (SLU) on these speech events. This aim of this article is to survey a breadth of perspectives in a holistic way; i.e. from considering underlying (psycho)linguistic theory, to their annotation and consideration in Automatic Speech Recognition (ASR) and SLU systems, to lastly, their study from a generation standpoint. This article aims to present the perspectives in an approachable way to the SLU and Conversational AI community, and discuss moving forward, what we believe are the trends and challenges in each area.
        </description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#29983;&#20135;&#25945;&#32946;&#20869;&#23481;&#30340;&#25928;&#29575;&#65292;&#24182;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26469;&#35299;&#37322;&#20219;&#21153;&#65292;&#20197;&#22521;&#20859;&#20799;&#31461;&#30340;&#22909;&#22855;&#24515;&#25552;&#38382;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#35813;&#20869;&#23481;&#30830;&#23454;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2211.14228</link><description>&#21033;&#29992;GPT-3&#39537;&#21160;&#30340;&#25945;&#23398;&#20195;&#29702;&#22521;&#20859;&#20799;&#31461;&#30340;&#22909;&#22855;&#24515;&#25552;&#38382;&#33021;&#21147;

        GPT-3-driven pedagogical agents for training children's curious question-asking skills. (arXiv:2211.14228v4 [cs.CL] UPDATED)

        [http://arxiv.org/abs/2211.14228](http://arxiv.org/abs/2211.14228)

        &#20026;&#20102;&#22521;&#20859;&#20799;&#31461;&#25552;&#38382;&#22909;&#22855;&#24515;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#35774;&#35745;&#29305;&#23450;&#30340;&#32451;&#20064;&#65292;&#20381;&#36182;&#20110;&#25552;&#20379;&#35821;&#20041;&#21644;&#35821;&#35328;&#32447;&#32034;&#26469;&#24110;&#21161;&#21046;&#23450;&#36825;&#26679;&#30340;&#38382;&#39064;&#12290;&#20294;&#23613;&#31649;&#26174;&#31034;&#20986;&#25945;&#23398;&#25928;&#29575;&#65292;&#36825;&#31181;&#26041;&#27861;&#20173;&#28982;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#25163;&#24037;&#29983;&#25104;&#25152;&#36848;&#25552;&#31034;&#65292;&#36825;&#21487;&#33021;&#26159;&#19968;&#20010;&#38750;&#24120;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;(NLP)&#30340;&#36827;&#23637;&#65292;&#35843;&#26597;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#33258;&#21160;&#29983;&#20135;&#22909;&#22855;&#24515;&#25552;&#38382;(QA)&#22521;&#35757;&#30340;&#25945;&#23398;&#20869;&#23481;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#8220;&#22522;&#20110;&#25552;&#31034;&#8221;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#25152;&#36848;&#20869;&#23481;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#29992;&#33258;&#28982;&#25991;&#26412;&#21521;LLM&#35299;&#37322;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#19987;&#23478;&#27880;&#37322;&#21644;&#19982;&#25163;&#24037;&#29983;&#25104;&#30340;&#20869;&#23481;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#36755;&#20986;&#12290;&#32467;&#26524;&#30830;&#23454;&#34920;&#26126;&#20102;&#36825;&#31181;&#20869;&#23481;&#30340;&#30456;&#20851;&#24615;&#21644;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#22312;&#23567;&#23398;&#36827;&#34892;&#20102;&#19968;&#39033;&#29616;&#22330;&#30740;&#31350;(75 ch

        In order to train children's ability to ask curiosity-driven questions, previous research has explored designing specific exercises relying on providing semantic and linguistic cues to help formulate such questions. But despite showing pedagogical efficiency, this method is still limited as it relies on generating the said cues by hand, which can be a very costly process. In this context, we propose to leverage advances in the natural language processing field (NLP) and investigate the efficiency of using a large language model (LLM) for automating the production of the pedagogical content of a curious question-asking (QA) training. We study generating the said content using the "prompt-based" method that consists of explaining the task to the LLM in natural text. We evaluate the output using human experts annotations and comparisons with hand-generated content. Results suggested indeed the relevance and usefulness of this content. We also conduct a field study in primary school (75 ch
        </description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT-Deep CNN&#30340;&#26032;&#20896;&#32954;&#28814;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;&#30340;&#26368;&#26032;&#30740;&#31350;&#26041;&#27861;&#65292;&#26088;&#22312;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#19978;&#20154;&#20204;&#23545;&#30123;&#24773;&#30340;&#24773;&#24863;&#65292;&#23545;&#20110;&#20445;&#38556;&#31038;&#20250;&#20813;&#21463;&#30123;&#24773;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2211.09733</link><description>BERT-Deep CNN:&#26032;&#20896;&#32954;&#28814;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;&#30340;&#26368;&#26032;&#30740;&#31350;

        BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19 Tweets. (arXiv:2211.09733v2 [cs.CL] UPDATED)

        [http://arxiv.org/abs/2211.09733](http://arxiv.org/abs/2211.09733)

        &#31038;&#20132;&#23186;&#20307;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#21152;&#36895;&#20102;&#20449;&#24687;&#30340;&#33258;&#30001;&#27969;&#21160;&#12290;&#30001;&#20110;&#26032;&#20896;&#30149;&#27602;&#30142;&#30149;(COVID-19)&#30340;&#29190;&#21457;&#65292;&#23545;&#20154;&#21475;&#30340;&#31038;&#20250;&#21644;&#24515;&#29702;&#24433;&#21709;&#26174;&#33879;&#12290;&#26032;&#20896;&#32954;&#28814;&#22823;&#27969;&#34892;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#27491;&#22312;&#35752;&#35770;&#30340;&#24403;&#21069;&#20107;&#20214;&#20043;&#19968;&#12290;&#20026;&#20102;&#20445;&#38556;&#31038;&#20250;&#20813;&#21463;&#36825;&#31181;&#22823;&#27969;&#34892;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#19978;&#20154;&#20204;&#30340;&#24773;&#24863;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#20854;&#29305;&#27530;&#30340;&#29305;&#24615;&#65292;&#23545;&#25512;&#25991;&#31561;&#25991;&#26412;&#30340;&#24773;&#24863;&#20998;&#26512;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24773;&#24863;&#20998;&#26512;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25991;&#26412;&#20998;&#26512;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#24847;&#35265;&#21644;&#24773;&#32490;&#12290;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#23545;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#25991;&#26412;&#36827;&#34892;&#26816;&#26597;&#65292;&#21253;&#25324;&#30005;&#23376;&#37038;&#20214;&#12289;&#35843;&#26597;&#12289;&#35780;&#35770;&#12289;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21644;&#32593;&#32476;&#25991;&#31456;&#65292;&#20174;&#20013;&#25552;&#21462;&#21547;&#20041;&#12290;&#20026;&#20102;&#35780;&#20272;&#24773;&#24863;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32473;&#23454;&#20307;&#20998;&#37197;&#26435;&#37325;&#12290;

        The free flow of information has been accelerated by the rapid development of social media technology. There has been a significant social and psychological impact on the population due to the outbreak of Coronavirus disease (COVID-19). The COVID-19 pandemic is one of the current events being discussed on social media platforms. In order to safeguard societies from this pandemic, studying people's emotions on social media is crucial. As a result of their particular characteristics, sentiment analysis of texts like tweets remains challenging. Sentiment analysis is a powerful text analysis tool. It automatically detects and analyzes opinions and emotions from unstructured data. Texts from a wide range of sources are examined by a sentiment analysis tool, which extracts meaning from them, including emails, surveys, reviews, social media posts, and web articles. To evaluate sentiments, natural language processing (NLP) and machine learning techniques are used, which assign weights to entit
        </description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24773;&#24863;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21508;&#31181;&#26102;&#38388;&#23610;&#24230;&#23398;&#20064;&#22810;&#23610;&#24230;&#30340;&#24773;&#24863;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#36866;&#24212;&#24773;&#24863;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2211.08233</link><description>&#26102;&#38388;&#24314;&#27169;&#30340;&#37325;&#35201;&#24615;&#65306;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#26032;&#22411;&#26102;&#38388;&#24773;&#24863;&#24314;&#27169;&#26041;&#27861;

        Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition. (arXiv:2211.08233v2 [cs.SD] UPDATED)

        [http://arxiv.org/abs/2211.08233](http://arxiv.org/abs/2211.08233)

        &#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#22312;&#36890;&#36807;&#35821;&#38899;&#20449;&#21495;&#25512;&#26029;&#20154;&#31867;&#24773;&#24863;&#21644;&#24773;&#24863;&#29366;&#24577;&#20197;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20174;&#25163;&#24037;&#29305;&#24449;&#20013;&#25366;&#25496;&#26102;&#31354;&#20449;&#24687;&#65292;&#25105;&#20204;&#25506;&#32034;&#22914;&#20309;&#20174;&#21160;&#24577;&#26102;&#38388;&#23610;&#24230;&#27169;&#22411;&#35821;&#38899;&#24773;&#24863;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#26102;&#38388;&#24773;&#24863;&#24314;&#27169;&#26041;&#27861;&#65292;&#31216;&#20026;&#26102;&#38388;&#24863;&#30693;&#30340;&#21452;&#21521;&#22810;&#23610;&#24230;&#32593;&#32476;&#65288;TIM-Net&#65289;&#65292;&#23427;&#20174;&#21508;&#31181;&#26102;&#38388;&#23610;&#24230;&#23398;&#20064;&#22810;&#23610;&#24230;&#30340;&#24773;&#24863;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TIM-Net&#39318;&#20808;&#37319;&#29992;&#26102;&#38388;&#24863;&#30693;&#22359;&#23398;&#20064;&#26102;&#38388;&#24773;&#24863;&#34920;&#31034;&#65292;&#28982;&#21518;&#20174;&#36807;&#21435;&#21644;&#26410;&#26469;&#38598;&#25104;&#34917;&#20805;&#20449;&#24687;&#20197;&#20016;&#23500;&#24773;&#22659;&#34920;&#31034;&#65292;&#26368;&#21518;&#65292;&#34701;&#21512;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#29305;&#24449;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#24773;&#24863;&#21464;&#21270;&#12290;&#22312;&#20845;&#20010;&#22522;&#20934;SER&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;

        Speech emotion recognition (SER) plays a vital role in improving the interactions between humans and machines by inferring human emotion and affective states from speech signals. Whereas recent works primarily focus on mining spatiotemporal information from hand-crafted features, we explore how to model the temporal patterns of speech emotions from dynamic temporal scales. Towards that goal, we introduce a novel temporal emotional modeling approach for SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net), which learns multi-scale contextual affective representations from various time scales. Specifically, TIM-Net first employs temporal-aware blocks to learn temporal affective representation, then integrates complementary information from the past and the future to enrich contextual representations, and finally, fuses multiple time scale features for better adaptation to the emotional variation. Extensive experimental results on six benchmark SER datasets demonstrate th
        </description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#33258;&#21160;&#25552;&#31034;&#24037;&#31243;&#24072;(APE)&#26469;&#33258;&#21160;&#29983;&#25104;&#21644;&#36873;&#25321;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20219;&#21153;&#34920;&#29616;&#65292;&#22312;24&#20010;NLP&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#33258;&#21160;&#29983;&#25104;&#30340;&#35828;&#26126;&#26126;&#26174;&#20248;&#20110;&#20808;&#21069;&#30340;LLM&#22522;&#32447;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#25110;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;</title><link>http://arxiv.org/abs/2211.01910</link><description>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#19982;&#20154;&#31867;&#19968;&#26679;&#30340;&#25552;&#31034;&#24037;&#31243;&#24072;

        Large Language Models Are Human-Level Prompt Engineers. (arXiv:2211.01910v2 [cs.LG] UPDATED)

        [http://arxiv.org/abs/2211.01910](http://arxiv.org/abs/2211.01910)

        &#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20316;&#20026;&#36890;&#29992;&#35745;&#31639;&#26426;&#30340;&#24778;&#20154;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20219;&#21153;&#34920;&#29616;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#29992;&#20110;&#24341;&#23548;&#27169;&#22411;&#30340;&#25552;&#31034;&#30340;&#36136;&#37327;&#65292;&#22823;&#22810;&#25968;&#26377;&#25928;&#30340;&#25552;&#31034;&#26159;&#30001;&#20154;&#31867;&#25163;&#24037;&#21046;&#20316;&#30340;&#12290;&#21463;&#32463;&#20856;&#31243;&#24207;&#32508;&#21512;&#21644;&#20154;&#31867;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#33258;&#21160;&#25351;&#20196;&#29983;&#25104;&#21644;&#36873;&#25321;&#30340;&#33258;&#21160;&#25552;&#31034;&#24037;&#31243;&#24072;(APE)&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#25351;&#20196;&#35270;&#20026;&#8220;&#31243;&#24207;&#8221;&#65292;&#36890;&#36807;&#22312;&#25628;&#32034;&#30001;LLM&#25552;&#20986;&#30340;&#25351;&#20196;&#20505;&#36873;&#27744;&#19978;&#36827;&#34892;&#20248;&#21270;&#26469;&#26368;&#22823;&#21270;&#36873;&#25321;&#30340;&#24471;&#20998;&#20989;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#36873;&#25351;&#20196;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21478;&#19968;&#20010;LLM&#22312;&#25353;&#29031;&#25152;&#36873;&#25351;&#20196;&#36827;&#34892;&#38646;&#23556;&#20987;&#26102;&#30340;&#24615;&#33021;&#12290;&#22312;24&#20010;NLP&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#33258;&#21160;&#29983;&#25104;&#30340;&#35828;&#26126;&#26126;&#26174;&#20248;&#20110;&#20808;&#21069;&#30340;LLM&#22522;&#32447;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#25110;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;

        By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the 
        </description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26816;&#32034;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#26469;&#23398;&#20064;&#26410;&#34987;&#20805;&#20998;&#23398;&#20064;&#30340;&#23454;&#20307;&#30340;&#29992;&#27861;&#30340;&#33258;&#36866;&#24212;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;CrossNER&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#27604;&#24378;&#22522;&#32447;&#34920;&#29616;&#39640;2.35&#20010;&#30334;&#20998;&#28857;</title><link>http://arxiv.org/abs/2210.07523</link><description>&#36890;&#36807;&#26816;&#32034;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#33258;&#36866;&#24212;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;

        Self-Adaptive Named Entity Recognition by Retrieving Unstructured Knowledge. (arXiv:2210.07523v2 [cs.CL] UPDATED)

        [http://arxiv.org/abs/2210.07523](http://arxiv.org/abs/2210.07523)

        &#23613;&#31649;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#39046;&#22495;&#29305;&#23450;&#30340;&#23454;&#20307;&#65288;&#20363;&#22914;&#65292;&#38899;&#20048;&#39046;&#22495;&#20013;&#30340;&#33402;&#26415;&#23478;&#65289;&#65292;&#20294;&#26159;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#21019;&#24314;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#32467;&#26500;&#21270;&#30693;&#35782;&#24211;&#20197;&#25191;&#34892;&#20934;&#30830;&#30340;NER&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;NER&#65292;&#23427;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#26469;&#23398;&#20064;&#26410;&#34987;&#20805;&#20998;&#23398;&#20064;&#30340;&#23454;&#20307;&#30340;&#29992;&#27861;&#12290;&#20026;&#20102;&#26816;&#32034;NER&#30340;&#26377;&#29992;&#30693;&#35782;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#20351;&#29992;&#19981;&#30830;&#23450;&#30340;&#23454;&#20307;&#20316;&#20026;&#26597;&#35810;&#26469;&#26816;&#32034;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#39044;&#27979;&#36755;&#20837;&#20013;&#30340;&#23454;&#20307;&#65292;&#28982;&#21518;&#25214;&#21040;&#39044;&#27979;&#19981;&#30830;&#23450;&#30340;&#23454;&#20307;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#19981;&#30830;&#23450;&#30340;&#23454;&#20307;&#20316;&#20026;&#26597;&#35810;&#26469;&#26816;&#32034;&#30693;&#35782;&#65292;&#24182;&#23558;&#26816;&#32034;&#21040;&#30340;&#25991;&#26412;&#36830;&#25509;&#21040;&#21407;&#22987;&#36755;&#20837;&#26469;&#20462;&#27491;&#39044;&#27979;&#12290;&#22312;CrossNER&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;F1&#24230;&#37327;&#20013;&#27604;&#24378;&#22522;&#32447;&#34920;&#29616;&#39640;2.35&#20010;&#30334;&#20998;&#28857;&#12290;

        Although named entity recognition (NER) helps us to extract domain-specific entities from text (e.g., artists in the music domain), it is costly to create a large amount of training data or a structured knowledge base to perform accurate NER in the target domain. Here, we propose self-adaptive NER, which retrieves external knowledge from unstructured text to learn the usages of entities that have not been learned well. To retrieve useful knowledge for NER, we design an effective two-stage model that retrieves unstructured knowledge using uncertain entities as queries. Our model predicts the entities in the input and then finds those of which the prediction is not confident. Then, it retrieves knowledge by using these uncertain entities as queries and concatenates the retrieved text to the original input to revise the prediction. Experiments on CrossNER datasets demonstrated that our model outperforms strong baselines by 2.35 points in F1 metric.
        </description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#20197;&#20132;&#26367;&#26041;&#24335;&#29983;&#25104;&#25512;&#29702;&#36319;&#36394;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#20004;&#32773;&#20043;&#38388;&#30340;&#26356;&#22823;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;ReAct&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;&#21644;&#20915;&#31574;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#20197;&#21450;&#20248;&#20110;&#27809;&#26377;&#25512;&#29702;&#25110;&#34892;&#21160;&#32452;&#20214;&#30340;&#26041;&#27861;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2210.03629</link><description>ReAct: &#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21327;&#21516;&#25512;&#29702;&#21644;&#34892;&#21160;

        ReAct: Synergizing Reasoning and Acting in Language Models. (arXiv:2210.03629v3 [cs.CL] UPDATED)

        [http://arxiv.org/abs/2210.03629](http://arxiv.org/abs/2210.03629)

        &#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#20132;&#20114;&#24335;&#20915;&#31574;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#25512;&#29702;&#21644;&#34892;&#21160;&#65288;&#20363;&#22914;&#34892;&#21160;&#35745;&#21010;&#29983;&#25104;&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#20027;&#35201;&#20316;&#20026;&#21333;&#29420;&#30340;&#20027;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#20197;&#20132;&#26367;&#26041;&#24335;&#29983;&#25104;&#25512;&#29702;&#36319;&#36394;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#20004;&#32773;&#20043;&#38388;&#30340;&#26356;&#22823;&#21327;&#21516;&#20316;&#29992;&#65306;&#25512;&#29702;&#36319;&#36394;&#26377;&#21161;&#20110;&#27169;&#22411;&#35825;&#23548;&#12289;&#36319;&#36394;&#21644;&#26356;&#26032;&#34892;&#21160;&#35745;&#21010;&#65292;&#24182;&#22788;&#29702;&#24322;&#24120;&#65292;&#32780;&#34892;&#21160;&#20801;&#35768;&#23427;&#19982;&#22806;&#37096;&#26469;&#28304;&#36827;&#34892;&#20132;&#20114;&#65292;&#20363;&#22914;&#30693;&#35782;&#24211;&#25110;&#29615;&#22659;&#65292;&#20197;&#25910;&#38598;&#39069;&#22806;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;ReAct&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;&#21644;&#20915;&#31574;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#20197;&#21450;&#20248;&#20110;&#27809;&#26377;&#25512;&#29702;&#25110;&#34892;&#21160;&#32452;&#20214;&#30340;&#26041;&#27861;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#25552;&#39640;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;

        While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on 
        </description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#27169;&#25311;&#35789;&#27719;&#21644;&#35821;&#27861;&#20307;&#21521;&#30340;&#35745;&#31639;&#26041;&#27861;&#20197;&#21450;&#24517;&#35201;&#30340;&#35821;&#35328;&#27010;&#24565;&#21644;&#26415;&#35821;&#30340;&#30452;&#35266;&#35299;&#37322;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#29366;&#24577;&#12289;&#30446;&#30340;&#24615;&#12289;&#20064;&#24815;&#24615;&#12289;&#23436;&#25104;&#24335;&#21644;&#26410;&#23436;&#25104;&#24335;&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#20107;&#20214;&#21644;&#24773;&#20917;&#31867;&#22411;&#30340;&#37325;&#35201;&#28165;&#21333;&#12290;</title><link>http://arxiv.org/abs/2208.09012</link><description>&#35789;&#27719;&#21644;&#35821;&#27861;&#20307;&#21521;&#30340;&#21451;&#21892;&#20171;&#32461;&#20197;&#21450;&#35745;&#31639;&#26041;&#27861;&#32508;&#36848;

        A Kind Introduction to Lexical and Grammatical Aspect, with a Survey of Computational Approaches. (arXiv:2208.09012v2 [cs.CL] UPDATED)

        [http://arxiv.org/abs/2208.09012](http://arxiv.org/abs/2208.09012)

        &#20307;&#21521;&#24847;&#21619;&#30528;&#20107;&#24773;&#30340;&#20869;&#37096;&#26102;&#38388;&#32467;&#26500;&#26159;&#22914;&#20309;&#21576;&#29616;&#30340;&#12290;&#36825;&#21253;&#25324;&#24773;&#20917;&#26159;&#34987;&#25551;&#36848;&#20026;&#19968;&#31181;&#29366;&#24577;&#36824;&#26159;&#20107;&#20214;&#65292;&#24773;&#20917;&#26159;&#24050;&#32467;&#26463;&#36824;&#26159;&#27491;&#22312;&#36827;&#34892;&#65292;&#20197;&#21450;&#24773;&#20917;&#26159;&#25972;&#20307;&#36824;&#26159;&#20851;&#27880;&#20110;&#29305;&#23450;&#38454;&#27573;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#27169;&#25311;&#35789;&#27719;&#21644;&#35821;&#27861;&#20307;&#21521;&#30340;&#35745;&#31639;&#26041;&#27861;&#20197;&#21450;&#24517;&#35201;&#30340;&#35821;&#35328;&#27010;&#24565;&#21644;&#26415;&#35821;&#30340;&#30452;&#35266;&#35299;&#37322;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#29366;&#24577;&#12289;&#30446;&#30340;&#24615;&#12289;&#20064;&#24815;&#24615;&#12289;&#23436;&#25104;&#24335;&#21644;&#26410;&#23436;&#25104;&#24335;&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#20107;&#20214;&#21644;&#24773;&#20917;&#31867;&#22411;&#30340;&#37325;&#35201;&#28165;&#21333;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#30001;&#20110;&#20307;&#21521;&#26159;&#35821;&#20041;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#29305;&#21035;&#26159;&#22312;&#31934;&#30830;&#25253;&#21578;&#24773;&#20917;&#30340;&#26102;&#38388;&#32467;&#26500;&#26102;&#65292;&#26410;&#26469;&#30340;NLP&#26041;&#27861;&#38656;&#35201;&#33021;&#22815;&#31995;&#32479;&#22320;&#22788;&#29702;&#21644;&#35780;&#20272;&#23427;&#65292;&#20197;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#35821;&#35328;&#29702;&#35299;&#12290;

        Aspectual meaning refers to how the internal temporal structure of situations is presented. This includes whether a situation is described as a state or as an event, whether the situation is finished or ongoing, and whether it is viewed as a whole or with a focus on a particular phase. This survey gives an overview of computational approaches to modeling lexical and grammatical aspect along with intuitive explanations of the necessary linguistic concepts and terminology. In particular, we describe the concepts of stativity, telicity, habituality, perfective and imperfective, as well as influential inventories of eventuality and situation types. We argue that because aspect is a crucial component of semantics, especially when it comes to reporting the temporal structure of situations in a precise way, future NLP approaches need to be able to handle and evaluate it systematically in order to achieve human-level language understanding.
        </description></item><item><title>&#26412;&#25991;&#20351;&#29992;BERT&#36827;&#34892;&#38463;&#25289;&#20271;&#35821;&#24577;&#24230;&#24773;&#24863;&#26497;&#24615;&#20998;&#31867;&#65292;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;BERT&#26550;&#26500;&#19982;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#23618;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#21462;&#24471;&#20102;89.51%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2107.13290</link><description>&#20351;&#29992;BERT&#36827;&#34892;&#38463;&#25289;&#20271;&#35821;&#24577;&#24230;&#24773;&#24863;&#26497;&#24615;&#20998;&#31867;

        Arabic aspect sentiment polarity classification using BERT. (arXiv:2107.13290v4 [cs.CL] UPDATED)

        [http://arxiv.org/abs/2107.13290](http://arxiv.org/abs/2107.13290)

        &#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;(ABSA)&#26159;&#19968;&#31181;&#25991;&#26412;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#23450;&#20041;&#20102;&#19982;&#29305;&#23450;&#30446;&#26631;&#30456;&#20851;&#30340;&#26576;&#20123;&#26041;&#38754;&#19978;&#30340;&#24847;&#35265;&#26497;&#24615;&#12290;&#22823;&#22810;&#25968;ABSA&#30740;&#31350;&#37117;&#26159;&#29992;&#33521;&#35821;&#36827;&#34892;&#30340;&#65292;&#21482;&#26377;&#23569;&#37327;&#30340;&#38463;&#25289;&#20271;&#35821;&#30740;&#31350;&#21487;&#29992;&#12290;&#22823;&#37096;&#20998;&#20197;&#21069;&#30340;&#38463;&#25289;&#20271;&#35821;&#30740;&#31350;&#20381;&#36182;&#20110;&#20027;&#35201;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#26080;&#20851;&#35789;&#21521;&#37327;(&#20363;&#22914;word2vec)&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#35789;&#37117;&#26377;&#19968;&#20010;&#29420;&#31435;&#20110;&#20854;&#19978;&#19979;&#25991;&#30340;&#22266;&#23450;&#34920;&#31034;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(&#22914;BERT)&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#21477;&#23376;&#23545;&#36755;&#20837;&#22788;&#29702;&#38463;&#25289;&#20271;&#35821;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#26497;&#24615;&#20998;&#31867;&#20219;&#21153;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20110;BERT&#30340;&#31070;&#32463;&#22522;&#32447;&#26469;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#12290;&#26681;&#25454;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;BERT&#26550;&#26500;&#19982;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#23618;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#26032;&#24037;&#20316;&#12290;&#22312;&#38463;&#25289;&#20271;&#22269;&#23478;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;89.51%&#30340;&#20934;&#30830;&#29575;&#12290;

        Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that defines the polarity of opinions on certain aspects related to specific targets. The majority of research on ABSA is in English, with a small amount of work available in Arabic. Most previous Arabic research has relied on deep learning models that depend primarily on context-independent word embeddings (e.g.word2vec), where each word has a fixed representation independent of its context. This article explores the modeling capabilities of contextual embeddings from pre-trained language models, such as BERT, and making use of sentence pair input on Arabic aspect sentiment polarity classification task. In particular, we develop a simple but effective BERT-based neural baseline to handle this task. Our BERT architecture with a simple linear classification layer surpassed the state-of-the-art works, according to the experimental results on three different Arabic datasets. Achieving an accuracy of 89.51% on the Arab
        </description></item></channel></rss>