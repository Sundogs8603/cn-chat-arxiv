<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2310.17940</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550;&#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unified Segment-to-Segment Framework for Simultaneous Sequence Generation. (arXiv:2310.17940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17940
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#26159;&#23454;&#26102;&#22330;&#26223;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#27604;&#22914;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#12289;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#21644;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#65292;&#20854;&#20013;&#30446;&#26631;&#24207;&#21015;&#22312;&#25509;&#25910;&#28304;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#12290;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#30340;&#20851;&#38190;&#22312;&#20110;&#30830;&#23450;&#29983;&#25104;&#30340;&#26368;&#20339;&#26102;&#26426;&#65292;&#36890;&#36807;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#23545;&#28304;-&#30446;&#26631;&#26144;&#23556;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#33021;&#21147;&#65292;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#21508;&#31181;&#21516;&#26102;&#20219;&#21153;&#20013;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#20197;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#26144;&#23556;&#12290;&#22312;&#21516;&#26102;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#22312;&#31561;&#24453;&#28304;&#29255;&#27573;&#21644;&#29983;&#25104;&#30446;&#26631;&#29255;&#27573;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous sequence generation is a pivotal task for real-time scenarios, such as streaming speech recognition, simultaneous machine translation and simultaneous speech translation, where the target sequence is generated while receiving the source sequence. The crux of achieving high-quality generation with low latency lies in identifying the optimal moments for generating, accomplished by learning a mapping between the source and target sequences. However, existing methods often rely on task-specific heuristics for different sequence types, limiting the model's capacity to adaptively learn the source-target mapping and hindering the exploration of multi-task learning for various simultaneous tasks. In this paper, we propose a unified segment-to-segment framework (Seg2Seg) for simultaneous sequence generation, which learns the mapping in an adaptive and unified manner. During the process of simultaneous generation, the model alternates between waiting for a source segment and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#32771;&#23519;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#22238;&#31572;&#24739;&#32773;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#32534;&#36753;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#21487;&#25509;&#21463;&#30340;&#33609;&#31295;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24037;&#20316;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.17703</link><description>&lt;p&gt;
&#20351;&#29992;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#22797;&#24739;&#32773;&#20449;&#24687;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The impact of using an AI chatbot to respond to patient messages. (arXiv:2310.17703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#32771;&#23519;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#22238;&#31572;&#24739;&#32773;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#32534;&#36753;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#21487;&#25509;&#21463;&#30340;&#33609;&#31295;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24037;&#20316;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#20214;&#36127;&#25285;&#26159;&#23548;&#33268;&#20020;&#24202;&#21307;&#29983;&#20518;&#24608;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#36825;&#31181;&#24773;&#20917;&#22312;&#20840;&#22269;&#33539;&#22260;&#20869;&#26085;&#30410;&#22686;&#21152;&#65292;&#23545;&#25105;&#20204;&#29031;&#39038;&#24739;&#32773;&#30340;&#33021;&#21147;&#26500;&#25104;&#20102;&#32039;&#36843;&#23041;&#32961;&#12290;&#20687;ChatGPT&#36825;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#21327;&#21161;&#25991;&#26723;&#22788;&#29702;&#26469;&#20943;&#36731;&#20020;&#24202;&#21307;&#29983;&#30340;&#36127;&#25285;&#12290;&#34429;&#28982;&#35768;&#22810;&#21307;&#38498;&#27491;&#22312;&#31215;&#26497;&#23558;&#36825;&#20123;&#31995;&#32479;&#25972;&#21512;&#21040;&#30005;&#23376;&#30149;&#21382;&#31995;&#32479;&#20013;&#65292;&#20294;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#24433;&#21709;&#23578;&#26410;&#38024;&#23545;&#27492;&#39044;&#26399;&#29992;&#36884;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#36215;&#33609;&#22238;&#31572;&#24739;&#32773;&#38382;&#39064;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#30340;&#30740;&#31350;&#32773;&#12290;&#22312;&#25105;&#20204;&#30340;&#20004;&#38454;&#27573;&#27178;&#26029;&#38754;&#30740;&#31350;&#20013;&#65292;6&#21517;&#32959;&#30244;&#23398;&#23478;&#23545;100&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;&#30284;&#30151;&#24739;&#32773;&#22330;&#26223;&#21644;&#21453;&#26144;&#24120;&#35265;&#21307;&#30103;&#24773;&#20917;&#30340;&#38376;&#25143;&#20449;&#24687;&#36827;&#34892;&#20102;&#22238;&#22797;&#65292;&#39318;&#20808;&#26159;&#25163;&#21160;&#22238;&#22797;&#65292;&#28982;&#21518;&#26159;&#36890;&#36807;AI&#21327;&#21161;&#22238;&#22797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;AI&#21327;&#21161;&#30340;&#22238;&#31572;&#26356;&#38271;&#12289;&#21487;&#35835;&#24615;&#36739;&#24046;&#65292;&#20294;&#22312;58%&#30340;&#26102;&#38388;&#37324;&#25552;&#20379;&#20102;&#21487;&#25509;&#21463;&#30340;&#33609;&#31295;&#32780;&#26080;&#38656;&#32534;&#36753;&#12290;AI&#21327;&#21161;&#25552;&#39640;&#20102;77%&#30340;&#25928;&#29575;&#65292;&#19982;&#20256;&#32479;&#25163;&#21160;&#22238;&#31572;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Documentation burden is a major contributor to clinician burnout, which is rising nationally and is an urgent threat to our ability to care for patients. Artificial intelligence (AI) chatbots, such as ChatGPT, could reduce clinician burden by assisting with documentation. Although many hospitals are actively integrating such systems into electronic medical record systems, AI chatbots utility and impact on clinical decision-making have not been studied for this intended use. We are the first to examine the utility of large language models in assisting clinicians draft responses to patient questions. In our two-stage cross-sectional study, 6 oncologists responded to 100 realistic synthetic cancer patient scenarios and portal messages developed to reflect common medical situations, first manually, then with AI assistance.  We find AI-assisted responses were longer, less readable, but provided acceptable drafts without edits 58% of time. AI assistance improved efficiency 77% of time, with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#25991;&#26723;&#29983;&#25104;&#26426;&#21046;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#38454;&#25552;&#31034;&#23545;&#25239;&#20316;&#32773;&#21435;&#21311;&#21517;&#25915;&#20987;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#19979;&#28216;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26426;&#21046;&#22312;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#21516;&#26102;&#33021;&#22815;&#23436;&#20840;&#24674;&#22797;&#28165;&#27905;&#30340;&#24773;&#24863;&#20998;&#25968;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.16111</link><description>&lt;p&gt;
&#20351;&#29992;&#38646;&#38454;&#25552;&#31034;&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#25991;&#26723;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Locally Differentially Private Document Generation Using Zero Shot Prompting. (arXiv:2310.16111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#25991;&#26723;&#29983;&#25104;&#26426;&#21046;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#38454;&#25552;&#31034;&#23545;&#25239;&#20316;&#32773;&#21435;&#21311;&#21517;&#25915;&#20987;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#19979;&#28216;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26426;&#21046;&#22312;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#21516;&#26102;&#33021;&#22815;&#23436;&#20840;&#24674;&#22797;&#28165;&#27905;&#30340;&#24773;&#24863;&#20998;&#25968;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30740;&#31350;&#24050;&#32463;&#24378;&#35843;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20026;&#38544;&#31169;&#20445;&#25252;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DP-Prompt&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#38454;&#25552;&#31034;&#26469;&#23545;&#25239;&#20316;&#32773;&#21435;&#21311;&#21517;&#25915;&#20987;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#19979;&#28216;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#24403;DP-Prompt&#19982;&#20687;ChatGPT&#65288;gpt-3.5&#65289;&#36825;&#26679;&#30340;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21435;&#21311;&#21517;&#25915;&#20987;&#25104;&#21151;&#29575;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#19988;&#23613;&#31649;&#20854;&#35774;&#35745;&#26356;&#31616;&#21333;&#65292;&#20294;&#23427;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24456;&#22823;&#31243;&#24230;&#12290;&#20363;&#22914;&#65292;&#22312;IMDB&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;DP-Prompt&#65288;&#20351;&#29992;ChatGPT&#65289;&#23436;&#20840;&#24674;&#22797;&#20102;&#28165;&#27905;&#30340;&#24773;&#24863;F1&#20998;&#25968;&#65292;&#24182;&#22312;&#38745;&#24577;&#25915;&#20987;&#32773;&#30340;&#20316;&#32773;&#35782;&#21035;F1&#20998;&#25968;&#19978;&#23454;&#29616;&#20102;46&#65285;&#30340;&#38477;&#20302;&#21644;26&#65285;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\% reduction in author identification F1 score against static attackers and a 26\% reduc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#20294;&#23427;&#20204;&#23637;&#31034;&#20102;&#20851;&#38190;&#32780;&#19968;&#33268;&#30340;&#20219;&#21153;&#24615;&#33021;&#25913;&#36827;&#65292;&#36825;&#19968;&#25913;&#36827;&#26080;&#27861;&#36890;&#36807;&#20256;&#32479;&#30340;&#35780;&#20272;&#31574;&#30053;&#26469;&#25429;&#25417;&#65292;&#22240;&#20026;&#35780;&#20272;&#30340;&#31934;&#24230;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2310.03262</link><description>&lt;p&gt;
&#35299;&#38145;&#20174;&#26032;&#20852;&#33021;&#21147;&#20013;&#21487;&#39044;&#27979;&#30340;&#25193;&#23637;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlock Predictable Scaling from Emergent Abilities. (arXiv:2310.03262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#20294;&#23427;&#20204;&#23637;&#31034;&#20102;&#20851;&#38190;&#32780;&#19968;&#33268;&#30340;&#20219;&#21153;&#24615;&#33021;&#25913;&#36827;&#65292;&#36825;&#19968;&#25913;&#36827;&#26080;&#27861;&#36890;&#36807;&#20256;&#32479;&#30340;&#35780;&#20272;&#31574;&#30053;&#26469;&#25429;&#25417;&#65292;&#22240;&#20026;&#35780;&#20272;&#30340;&#31934;&#24230;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31185;&#23398;&#25193;&#23637;&#65292;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#23427;&#20204;&#30340;&#25193;&#23637;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20851;&#20110;&#25193;&#23637;&#29305;&#24615;&#30340;&#30740;&#31350;&#21482;&#33021;&#24471;&#20986;&#19968;&#20010;&#19981;&#23436;&#25972;&#30340;&#31572;&#26696;&#65306;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#20248;&#21270;&#25439;&#22833;&#21487;&#39044;&#27979;&#22320;&#20943;&#23569;&#65292;&#31526;&#21512;&#24050;&#24314;&#31435;&#30340;&#32553;&#25918;&#23450;&#24459;&#65307;&#28982;&#32780;&#65292;&#20219;&#21153;&#30340;&#32553;&#25918;&#23450;&#24459;&#23578;&#26410;&#24314;&#31435;&#65292;&#20219;&#21153;&#34920;&#29616;&#22312;&#25193;&#23637;&#36807;&#31243;&#20013;&#36828;&#38750;&#21487;&#39044;&#27979;&#12290;&#20219;&#21153;&#34920;&#29616;&#36890;&#24120;&#22312;&#23567;&#27169;&#22411;&#19978;&#26174;&#31034;&#20986;&#36731;&#24494;&#22686;&#30410;&#65292;&#30452;&#21040;&#27169;&#22411;&#36229;&#36807;&#26576;&#20010;&#22823;&#23567;&#38408;&#20540;&#21518;&#25165;&#20986;&#29616;&#26174;&#33879;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#8220;&#26032;&#20852;&#33021;&#21147;&#8221;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;&#23567;&#27169;&#22411;&#34920;&#29616;&#20986;&#36731;&#24494;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#23637;&#29616;&#20102;&#20851;&#38190;&#32780;&#19968;&#33268;&#30340;&#20219;&#21153;&#24615;&#33021;&#25913;&#36827;&#65292;&#36825;&#20123;&#25913;&#36827;&#26080;&#27861;&#34987;&#20256;&#32479;&#35780;&#20272;&#31574;&#30053;&#25429;&#25417;&#21040;&#65292;&#22240;&#20026;&#27979;&#37327;&#20998;&#36776;&#29575;&#19981;&#36275;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#25913;&#36827;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PassUntil&#65292;&#22312;&#35299;&#30721;&#38454;&#27573;&#36890;&#36807;&#22823;&#35268;&#27169;&#25277;&#26679;&#36827;&#34892;&#35780;&#20272;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scientific scale-up of large language models (LLMs) necessitates a comprehensive understanding of their scaling properties. However, the existing literature on the scaling properties only yields an incomplete answer: optimization loss decreases predictably as the model size increases, in line with established scaling law; yet no scaling law for task has been established and the task performances are far from predictable during scaling. Task performances typically show minor gains on small models until they improve dramatically once models exceed a size threshold, exemplifying the ``emergent abilities''. In this study, we discover that small models, although they exhibit minor performance, demonstrate critical and consistent task performance improvements that are not captured by conventional evaluation strategies due to insufficient measurement resolution. To measure such improvements, we introduce PassUntil, an evaluation strategy through massive sampling in the decoding phase. We 
&lt;/p&gt;</description></item><item><title>SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00533</link><description>&lt;p&gt;
SELF&#65306;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00533
&lt;/p&gt;
&lt;p&gt;
SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#21331;&#36234;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23398;&#20064;&#21644;&#25512;&#21160;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#8212;&#8212;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;&#30340;&#36335;&#24452;&#20173;&#28982;&#26410;&#30693;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;"SELF"&#65288;&#24102;&#26377;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#20027;&#36827;&#21270;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLM&#33021;&#22815;&#19981;&#26029;&#22320;&#33258;&#25105;&#36827;&#21270;&#12290;&#27492;&#22806;&#65292;SELF&#21033;&#29992;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#20840;&#38754;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#31934;&#30830;&#23450;&#20301;&#21709;&#24212;&#25913;&#36827;&#30340;&#39046;&#22495;&#65292;&#24182;&#25552;&#39640;&#33258;&#20027;&#36827;&#21270;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;SELF&#39318;&#20808;&#36827;&#34892;&#20803;&#25216;&#33021;&#23398;&#20064;&#65292;&#19987;&#27880;&#20110;&#33258;&#25105;&#21453;&#39304;&#21644;&#33258;&#25105;&#31934;&#28860;&#12290;&#36825;&#20123;&#20803;&#25216;&#33021;&#26159;&#20851;&#38190;&#65292;&#24341;&#23548;&#27169;&#22411;&#22312;&#33258;&#21046;&#25968;&#25454;&#30340;&#25345;&#32493;&#35757;&#32451;&#21608;&#26399;&#20013;&#36827;&#34892;&#21518;&#32493;&#30340;&#33258;&#25105;&#36827;&#21270;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#20869;&#22312;&#33021;&#21147;&#12290;&#22312;&#32473;&#23450;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;SELF&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#65288;LMaaS&#65289;&#20316;&#20026;&#19987;&#26377;&#31995;&#32479;&#65292;&#38480;&#21046;&#20102;&#20854;&#21487;&#35775;&#38382;&#24615;&#21644;&#35780;&#20272;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;ARRT&#65288;&#21487;&#35775;&#38382;&#24615;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#65289;&#25361;&#25112;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#36825;&#20123;&#25361;&#25112;&#20197;&#21450;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.16573</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#30340;ARRT: &#26032;&#33539;&#24335;&#21450;&#20854;&#25361;&#25112;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges. (arXiv:2309.16573v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16573
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#65288;LMaaS&#65289;&#20316;&#20026;&#19987;&#26377;&#31995;&#32479;&#65292;&#38480;&#21046;&#20102;&#20854;&#21487;&#35775;&#38382;&#24615;&#21644;&#35780;&#20272;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;ARRT&#65288;&#21487;&#35775;&#38382;&#24615;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#65289;&#25361;&#25112;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#36825;&#20123;&#25361;&#25112;&#20197;&#21450;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#19968;&#20123;&#26368;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#37117;&#26159;&#19987;&#26377;&#31995;&#32479;&#65292;&#21482;&#33021;&#36890;&#36807;&#65288;&#36890;&#24120;&#26159;&#38480;&#21046;&#24615;&#30340;&#65289;&#32593;&#32476;&#25110;&#36719;&#20214;&#32534;&#31243;&#25509;&#21475;&#35775;&#38382;&#12290;&#36825;&#23601;&#26159;&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#65288;LMaaS&#65289;&#30340;&#33539;&#24335;&#12290;&#19982;&#21487;&#20197;&#23436;&#20840;&#35775;&#38382;&#27169;&#22411;&#30340;&#24773;&#20917;&#30456;&#21453;&#65292;&#22914;&#24320;&#25918;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#23553;&#38381;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#35780;&#20272;&#12289;&#22522;&#20934;&#27979;&#35797;&#21644;&#27979;&#35797;&#36896;&#25104;&#20102;&#29305;&#23450;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#30340;&#20004;&#20010;&#30446;&#26631;&#26159;&#65306;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30028;&#23450;&#21069;&#36848;&#25361;&#25112;&#22914;&#20309;&#20316;&#20026;&#23545;LMaaS&#30340;&#21487;&#35775;&#38382;&#24615;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#65288;ARRT&#65289;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#19982;&#27599;&#20010;&#36825;&#22235;&#20010;&#26041;&#38754;&#30340;&#35821;&#35328;&#27169;&#22411;&#20449;&#24687;&#19981;&#36275;&#26377;&#20851;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#24314;&#35758;&#65292;&#24182;&#24378;&#35843;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26412;&#25991;&#26159;&#24403;&#21069;&#20027;&#35201;LMaaS&#29616;&#26377;&#30693;&#35782;&#30340;&#19968;&#31449;&#24335;&#38598;&#38182;&#65292;&#25552;&#20379;&#20102;&#32508;&#21512;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some of the most powerful language models currently are proprietary systems, accessible only via (typically restrictive) web or software programming interfaces. This is the Language-Models-as-a-Service (LMaaS) paradigm. Contrasting with scenarios where full model access is available, as in the case of open-source models, such closed-off language models create specific challenges for evaluating, benchmarking, and testing them. This paper has two goals: on the one hand, we delineate how the aforementioned challenges act as impediments to the accessibility, replicability, reliability, and trustworthiness (ARRT) of LMaaS. We systematically examine the issues that arise from a lack of information about language models for each of these four aspects. We shed light on current solutions, provide some recommendations, and highlight the directions for future advancements. On the other hand, it serves as a one-stop-shop for the extant knowledge about current, major LMaaS, offering a synthesized o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05950</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;VLMs &#30340;&#24494;&#35843;&#26041;&#27861;&#20027;&#35201;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#25805;&#20316;&#65292;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810; VLMs &#20381;&#36182;&#20110;&#19987;&#26377;&#25968;&#25454;&#19988;&#19981;&#24320;&#28304;&#65292;&#38480;&#21046;&#20102;&#20351;&#29992;&#30333;&#30418;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#12290;&#37492;&#20110;&#20687; ChatGPT &#36825;&#26679;&#30340;&#21463;&#27426;&#36814;&#31169;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20173;&#28982;&#25552;&#20379;&#22522;&#20110;&#35821;&#35328;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340; VLMs &#24494;&#35843;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#29305;&#24449;&#23884;&#20837;&#25110;&#36755;&#20986; logits &#30340;&#38656;&#35201;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#32842;&#22825;&#30340; LLMs &#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#20197;&#22312;&#20351;&#29992; CLIP &#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#31034;&#20363;&#20219;&#21153;&#20013;&#23547;&#25214;&#26368;&#20339;&#25991;&#26412;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#21160;"&#29228;&#23665;"&#31243;&#24207;&#65292;&#23427;&#33021;&#25910;&#25947;&#21040;&#26377;&#25928;&#30340;&#25552;&#31034;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prom
&lt;/p&gt;</description></item><item><title>Focused Transformer&#36890;&#36807;&#21453;&#24046;&#35757;&#32451;&#20248;&#21270;&#20102;&#19978;&#19979;&#25991;&#32553;&#25918;&#38382;&#39064;&#65292;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.03170</link><description>&lt;p&gt;
Focused Transformer: &#21453;&#24046;&#35757;&#32451;&#23545;&#19978;&#19979;&#25991;&#32553;&#25918;&#36827;&#34892;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Focused Transformer: Contrastive Training for Context Scaling. (arXiv:2307.03170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03170
&lt;/p&gt;
&lt;p&gt;
Focused Transformer&#36890;&#36807;&#21453;&#24046;&#35757;&#32451;&#20248;&#21270;&#20102;&#19978;&#19979;&#25991;&#32553;&#25918;&#38382;&#39064;&#65292;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#19978;&#19979;&#25991;&#21270;&#30340;&#26041;&#24335;&#21560;&#32435;&#26032;&#30340;&#20449;&#24687;&#65292;&#20294;&#30001;&#20110;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20026;&#27880;&#24847;&#21147;&#23618;&#25552;&#20379;&#35775;&#38382;&#22806;&#37096;&#23384;&#20648;&#22120;&#30340;&#33021;&#21147;&#65292;&#35813;&#23384;&#20648;&#22120;&#30001;&#65288;&#38190;&#65292;&#20540;&#65289;&#23545;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25991;&#26723;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30456;&#20851;&#38190;&#19982;&#26080;&#20851;&#38190;&#30340;&#27604;&#20363;&#20943;&#23569;&#65292;&#20351;&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#26080;&#20851;&#38190;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;&#20998;&#24515;&#38382;&#39064;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#19982;&#19981;&#21516;&#35821;&#20041;&#20540;&#30456;&#20851;&#32852;&#30340;&#38190;&#21487;&#33021;&#37325;&#21472;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Focused Transformer&#65288;FoT&#65289;&#65292;&#19968;&#31181;&#21463;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#65288;&#38190;&#65292;&#20540;&#65289;&#31354;&#38388;&#30340;&#32467;&#26500;&#65292;&#20351;&#19978;&#19979;&#25991;&#38271;&#24230;&#24471;&#20197;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#23545;&#29616;&#26377;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have an exceptional capability to incorporate new information in a contextual manner. However, the full potential of such an approach is often restrained due to a limitation in the effective context length. One solution to this issue is to endow an attention layer with access to an external memory, which comprises of (key, value) pairs. Yet, as the number of documents increases, the proportion of relevant keys to irrelevant ones decreases, leading the model to focus more on the irrelevant keys. We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish. To tackle this problem, we introduce the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning. This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length. Our method allows for fine-tuning pre-existing, large-s
&lt;/p&gt;</description></item><item><title>MagicBrush&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#12290;&#23427;&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;InstructPix2Pix&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.10012</link><description>&lt;p&gt;
MagicBrush: &#20154;&#24037;&#26631;&#27880;&#30340;&#29992;&#20110;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing. (arXiv:2306.10012v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10012
&lt;/p&gt;
&lt;p&gt;
MagicBrush&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#12290;&#23427;&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;InstructPix2Pix&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#32534;&#36753;&#20174;&#20010;&#20154;&#20351;&#29992;&#21040;&#19987;&#19994;&#24212;&#29992;&#65288;&#22914;Photoshop&#65289;&#24191;&#27867;&#38656;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26159;&#38646;&#26679;&#26412;&#65292;&#35201;&#20040;&#26159;&#22312;&#33258;&#21160;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21547;&#26377;&#22823;&#37327;&#30340;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#25163;&#21160;&#35843;&#25972;&#25165;&#33021;&#20135;&#29983;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MagicBrush&#65292;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#65292;&#21253;&#25324;&#21333;&#20010;&#25805;&#20316;&#12289;&#22810;&#20010;&#25805;&#20316;&#12289;&#25552;&#20379;&#25513;&#30721;&#21644;&#19981;&#25552;&#20379;&#25513;&#30721;&#31561;&#19981;&#21516;&#22330;&#26223;&#12290;MagicBrush&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65288;&#28304;&#22270;&#20687;&#65292;&#25351;&#20196;&#65292;&#30446;&#26631;&#22270;&#20687;&#65289;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;MagicBrush&#19978;&#24494;&#35843;InstructPix2Pix&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20351;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided image editing is widely needed in daily life, ranging from personal use to professional applications such as Photoshop. However, existing methods are either zero-shot or trained on an automatically synthesized dataset, which contains a high volume of noise. Thus, they still require lots of manual tuning to produce desirable outcomes in practice. To address this issue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/), the first large-scale, manually annotated dataset for instruction-guided real image editing that covers diverse scenarios: single-turn, multi-turn, mask-provided, and mask-free editing. MagicBrush comprises over 10K manually annotated triples (source image, instruction, target image), which supports trainining large-scale text-guided image editing models. We fine-tune InstructPix2Pix on MagicBrush and show that the new model can produce much better images according to human evaluation. We further conduct extensive experiments to evaluate cu
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#37327;&#21270;&#29702;&#35299;&#30340;&#25506;&#31350;&#65292;&#24182;&#36136;&#30097;&#20043;&#21069;&#30740;&#31350;&#20013;&#20851;&#20110;LLMs&#29702;&#35299;&#26497;&#23569;&#25968;&#31867;&#22411;&#30340;&#37327;&#35789;&#33021;&#21147;&#21576;&#29616;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#35828;&#27861;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#23637;&#31034;&#20854;&#19982;&#20197;&#21069;&#30740;&#31350;&#25152;&#23637;&#31034;&#30340;&#34892;&#20026;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2306.07384</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#37327;&#21270;&#29702;&#35299;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Probing Quantifier Comprehension in Large Language Models. (arXiv:2306.07384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#37327;&#21270;&#29702;&#35299;&#30340;&#25506;&#31350;&#65292;&#24182;&#36136;&#30097;&#20043;&#21069;&#30740;&#31350;&#20013;&#20851;&#20110;LLMs&#29702;&#35299;&#26497;&#23569;&#25968;&#31867;&#22411;&#30340;&#37327;&#35789;&#33021;&#21147;&#21576;&#29616;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#35828;&#27861;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#23637;&#31034;&#20854;&#19982;&#20197;&#21069;&#30740;&#31350;&#25152;&#23637;&#31034;&#30340;&#34892;&#20026;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23427;&#20204;&#30340;&#35268;&#27169;&#22686;&#22823;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36234;&#26469;&#36234;&#22909;&#12290;&#20294;&#21363;&#20351;&#22312;&#20855;&#20307;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#65292;LLMs &#22312;&#21542;&#23450;&#25110;&#37327;&#21270;&#29702;&#35299;&#31561;&#31616;&#21333;&#35821;&#35328;&#27979;&#35797;&#20013;&#20173;&#28982;&#22833;&#36133;&#12290;&#20197;&#21069;&#27979;&#35797; LLMs &#23545;&#20110;&#29702;&#35299;&#37327;&#35789;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#30528;&#27169;&#22411;&#30340;&#19981;&#26029;&#22686;&#22823;&#65292;&#23427;&#20204;&#22312;&#29702;&#35299;&#22823;&#22810;&#25968;&#31867;&#22411;&#30340;&#37327;&#35789;&#26102;&#21464;&#24471;&#26356;&#22909;&#65292;&#20294;&#22312;&#29702;&#35299;&#26497;&#23569;&#25968;&#31867;&#22411;&#30340;&#37327;&#35789;&#26102;&#21464;&#24471;&#36234;&#26469;&#36234;&#24046;&#65292;&#20174;&#32780;&#21576;&#29616;&#20986;&#21453;&#27604;&#20363;&#32553;&#25918;&#27861;&#21017;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#36136;&#30097;&#20102;&#22312; LLMs &#20013;&#21453;&#27604;&#20363;&#32553;&#25918;&#26497;&#23569;&#25968;&#31867;&#22411;&#37327;&#35789;&#29702;&#35299;&#33021;&#21147;&#30340;&#35828;&#27861;&#65292;&#24182;&#34920;&#26126;&#36825;&#26159;&#19981;&#21512;&#36866;&#30340;&#27979;&#35797;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26469;&#27979;&#37327; LLMs &#30340;&#37327;&#21270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#38543;&#30528;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#22823;&#65292;&#36825;&#20123;&#34892;&#20026;&#19982;&#20197;&#21069;&#30340;&#30740;&#31350;&#25152;&#23637;&#31034;&#30340;&#19981;&#21516;&#12290;LLMs &#33021;&#22815;&#19981;&#26029;&#29702;&#35299;&#21547;&#20041;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
With their increasing size, Large language models (LLMs) are becoming increasingly good at language understanding tasks. But even with high performance on specific downstream task, LLMs fail at simple linguistic tests for negation or quantifier understanding. Previous work on testing capability of LLMs on understanding quantifiers suggest that as the size of the models increase, they get better at understanding most-type quantifiers but get increasingly worse at understanding few-type quantifiers, thus presenting a case of an inverse-scaling law. In this paper, we question the claims of inverse scaling of few-type quantifier understanding in LLMs and show that it is a result of inappropriate testing methodology. We also present alternate methods to measure quantifier comprehension in LLMs and show that as the size of the models increase, these behaviours are different from what is shown in previous research. LLMs are consistently able to understand the difference between the meaning of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28201;&#24230;&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;KL-&#25955;&#24230;&#24341;&#23548;&#21160;&#24577;&#35843;&#25972;&#28201;&#24230;&#65292;&#20174;&#32780;&#32531;&#35299;&#22810;&#26679;&#24615;&#21644;&#21487;&#24402;&#22240;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#23545;&#35805;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.01286</link><description>&lt;p&gt;
KL-Divergence&#24341;&#23548;&#19979;&#30340;&#28201;&#24230;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
KL-Divergence Guided Temperature Sampling. (arXiv:2306.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28201;&#24230;&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;KL-&#25955;&#24230;&#24341;&#23548;&#21160;&#24577;&#35843;&#25972;&#28201;&#24230;&#65292;&#20174;&#32780;&#32531;&#35299;&#22810;&#26679;&#24615;&#21644;&#21487;&#24402;&#22240;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#23545;&#35805;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28201;&#24230;&#37319;&#26679;&#26159;&#19968;&#31181;&#24120;&#35268;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#22810;&#26679;&#21270;&#12290;&#38543;&#30528;&#28201;&#24230;&#30340;&#21319;&#39640;&#65292;&#39044;&#27979;&#21464;&#24471;&#26356;&#21152;&#22810;&#26679;&#21270;&#65292;&#20294;&#20063;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#8212;&#8212;&#29983;&#25104;&#30475;&#20284;&#21512;&#29702;&#20294;&#19981;&#27491;&#30830;&#30340;&#20196;&#29260;&#12290;&#32531;&#35299;&#24187;&#35273;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#25552;&#20379;&#28304;/&#22522;&#30784;&#25991;&#26723;&#65292;&#24182;&#20351;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#19982;&#25552;&#20379;&#30340;&#26469;&#28304;&#30456;&#20851;&#19988;&#21487;&#24402;&#22240;&#30340;&#39044;&#27979;&#12290;&#30475;&#26469;&#23384;&#22312;&#22810;&#26679;&#24615;&#21644;&#21487;&#24402;&#22240;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26494;&#24347;&#22266;&#23450;&#28201;&#24230;&#21644;&#36890;&#36807;KL-&#25955;&#24230;&#26681;&#25454;&#20854;&#19982;&#28304;&#30340;&#30456;&#20851;&#24615;&#24341;&#23548;&#21160;&#24577;&#28201;&#24230;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#26435;&#34913;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#22312;&#23545;&#35805;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#20248;&#20110;&#24120;&#35268;&#30340;top-k&#21644;top-p&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temperature sampling is a conventional approach to diversify large language model predictions. As temperature increases, the prediction becomes diverse but also vulnerable to hallucinations -- generating tokens that are sensible but not factual. One common approach to mitigate hallucinations is to provide source/grounding documents and the model is trained to produce predictions that bind to and are attributable to the provided source. It appears that there is a trade-off between diversity and attribution. To mitigate any such trade-off, we propose to relax the constraint of having a fixed temperature over decoding steps, and a mechanism to guide the dynamic temperature according to its relevance to the source through KL-divergence. Our experiments justifies the trade-off, and shows that our sampling algorithm outperforms the conventional top-k and top-p algorithms in conversational question-answering and summarization tasks.
&lt;/p&gt;</description></item><item><title>ANPL&#26159;&#19968;&#20010;&#32534;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.18498</link><description>&lt;p&gt;
ANPL&#65306;&#20351;&#29992;&#20132;&#20114;&#24335;&#20998;&#35299;&#32534;&#35793;&#33258;&#28982;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
ANPL: Compiling Natural Programs with Interactive Decomposition. (arXiv:2305.18498v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18498
&lt;/p&gt;
&lt;p&gt;
ANPL&#26159;&#19968;&#20010;&#32534;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#22312;&#36890;&#36807;&#33258;&#28982;&#20132;&#20114;&#22686;&#24378;&#32534;&#31243;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#23558;&#24120;&#35265;&#30340;&#20351;&#29992;&#27169;&#24335;&#32534;&#35793;&#20026;&#32534;&#31243;&#35821;&#35328;&#65292;&#20363;&#22914;Python&#65292;&#20294;&#22914;&#20309;&#32534;&#36753;&#21644;&#35843;&#35797;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31243;&#24207;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ANPL&#65292;&#19968;&#31181;&#32534;&#31243;&#31995;&#32479;&#65292;&#20801;&#35768;&#29992;&#25143;&#20998;&#35299;&#29305;&#23450;&#20110;&#29992;&#25143;&#30340;&#20219;&#21153;&#12290;&#22312;ANPL&#31243;&#24207;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#35813;&#33609;&#22270;&#25351;&#23450;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#25968;&#25454;&#27969;&#12290;&#29992;&#25143;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#23558;&#29983;&#25104;&#21151;&#33021;&#30340;&#26114;&#36149;&#20219;&#21153;&#21368;&#36733;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#32473;&#23450;&#19968;&#20010;ANPL&#31243;&#24207;&#65292;ANPL&#32534;&#35793;&#22120;&#20250;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23454;&#29616;&#23380;&#20013;&#30340;&#21151;&#33021;&#65292;&#24182;&#36981;&#23432;&#33609;&#22270;&#20013;&#25351;&#23450;&#30340;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#23558;ANPL&#37096;&#32626;&#22312;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#65292;&#23427;&#26159;&#19968;&#32452;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29420;&#29305;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advents of Large Language Models (LLMs) have shown promise in augmenting programming using natural interactions. However, while LLMs are proficient in compiling common usage patterns into a programming language, e.g., Python, it remains a challenge how to edit and debug an LLM-generated program. We introduce ANPL, a programming system that allows users to decompose user-specific tasks. In an ANPL program, a user can directly manipulate sketch, which specifies the data flow of the generated program. The user annotates the modules, or hole with natural language descriptions offloading the expensive task of generating functionalities to the LLM. Given an ANPL program, the ANPL compiler generates a cohesive Python program that implements the functionalities in hole, while respecting the dataflows specified in sketch. We deploy ANPL on the Abstraction and Reasoning Corpus (ARC), a set of unique tasks that are challenging for state-of-the-art AI systems, showing it outperforms baseline p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13172</link><description>&lt;p&gt;
&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33021;&#22815;&#35757;&#32451;&#20986;&#34920;&#29616;&#20248;&#31168;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20294;&#20854;&#20445;&#25345;&#30456;&#20851;&#24615;&#21644;&#32416;&#27491;&#38169;&#35823;&#30340;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#30830;&#23450;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#20960;&#24180;&#20986;&#29616;&#20102;&#35768;&#22810;&#32534;&#36753;LLMs&#30340;&#25216;&#26415;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#39640;&#25928;&#22320;&#25913;&#21464;LLMs&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#19981;&#23545;&#20854;&#20182;&#36755;&#20837;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;LLMs&#27169;&#22411;&#32534;&#36753;&#30456;&#20851;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#32534;&#36753;&#20219;&#21153;&#23450;&#20041;&#21644;&#30456;&#20851;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20197;&#21450;&#23545;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#28145;&#20837;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#26356;&#24378;&#22823;&#30340;&#35780;&#20272;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#25216;&#26415;&#22266;&#26377;&#30340;&#25345;&#20037;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#27599;&#31181;&#32534;&#36753;&#25216;&#26415;&#30340;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#24110;&#21161;&#31038;&#21306;&#22312;LLMs&#30340;&#31649;&#29702;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in ma
&lt;/p&gt;</description></item><item><title>SeViLA&#26159;&#19968;&#20010;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#22312;&#35270;&#39057;&#23450;&#20301;&#21644;&#38382;&#31572;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#35757;&#32451;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#27169;&#22359;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#30340;&#20851;&#38190;&#24103;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06988</link><description>&lt;p&gt;
&#33258;&#25105;&#38142;&#24335;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#35270;&#39057;&#23450;&#20301;&#19982;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Self-Chained Image-Language Model for Video Localization and Question Answering. (arXiv:2305.06988v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06988
&lt;/p&gt;
&lt;p&gt;
SeViLA&#26159;&#19968;&#20010;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#22312;&#35270;&#39057;&#23450;&#20301;&#21644;&#38382;&#31572;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#35757;&#32451;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#27169;&#22359;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#30340;&#20851;&#38190;&#24103;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35270;&#39057;&#38382;&#31572;&#33021;&#22815;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#36825;&#20123;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#21551;&#21160;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#23558;&#22343;&#21248;&#37319;&#26679;&#30340;&#35270;&#39057;&#24103;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#36827;&#34892;&#20018;&#25509;&#65292;&#32780;&#26410;&#36827;&#34892;&#26174;&#24335;&#30340;&#35821;&#35328;&#24863;&#30693;&#21644;&#26102;&#38388;&#24314;&#27169;&#12290;&#24403;&#35270;&#39057;&#36755;&#20837;&#20013;&#21482;&#26377;&#19968;&#37096;&#20998;&#19982;&#35821;&#35328;&#26597;&#35810;&#30456;&#20851;&#26102;&#65292;&#36825;&#31181;&#22343;&#21248;&#24103;&#37319;&#26679;&#36890;&#24120;&#20250;&#23548;&#33268;&#37325;&#35201;&#30340;&#35270;&#35273;&#32447;&#32034;&#20002;&#22833;&#12290;&#23613;&#31649;&#20154;&#31867;&#36890;&#24120;&#20250;&#25214;&#21040;&#35270;&#39057;&#20013;&#35201;&#20851;&#27880;&#30340;&#29255;&#27573;&#24182;&#20498;&#24102;&#29255;&#21051;&#26469;&#22238;&#31572;&#38382;&#39064;&#65292;&#20294;&#35757;&#32451;&#19968;&#20010;&#26126;&#30830;&#30340;&#35270;&#39057;&#29255;&#27573;&#23616;&#37096;&#21270;&#22120;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#27880;&#37322;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SeViLA&#26694;&#26550;&#65292;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#65288;BLIP-2&#65289;&#26469;&#22788;&#29702;&#35270;&#39057;&#30340;&#26102;&#38388;&#20851;&#38190;&#24103;&#23450;&#20301;&#21644;&#38382;&#31572;&#12290;SeViLA&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#65292;&#20004;&#32773;&#20849;&#20139;&#30456;&#21516;&#30340;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#24103;&#20197;&#22238;&#31572;&#32473;&#23450;&#30340;&#38382;&#39064;&#12290;&#22312;TVQA&#12289;TVR&#21644;How2QA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SeViLA&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#27880;&#37322;&#23601;&#33021;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown promising results on utilizing pre-trained image-language models for video question answering. While these image-language models can efficiently bootstrap the representation learning of video-language models, they typically concatenate uniformly sampled video frames as visual inputs without explicit language-aware, temporal modeling. When only a portion of a video input is relevant to the language query, such uniform frame sampling can often lead to missing important visual cues. Although humans often find a video moment to focus on and rewind the moment to answer questions, training a query-aware video moment localizer often requires expensive annotations and high computational costs. To address this issue, we propose Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos. SeViLA framework consists of two modules: Localizer and A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#33258;&#28982;&#35821;&#35328;&#21160;&#20316;&#31354;&#38388;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102; &#949;-&#21487;&#25509;&#21463;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#35821;&#35328;&#27169;&#22411;&#25110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25991;&#26412;&#35282;&#33394;-&#35780;&#35770;&#65288;TAC&#65289;&#20195;&#29702;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.04082</link><description>&lt;p&gt;
&#19968;&#31181;&#25991;&#26412;&#28216;&#25103;&#20013;&#33258;&#28982;&#35821;&#35328;&#21160;&#20316;&#31354;&#38388;&#30340;&#31616;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Minimal Approach for Natural Language Action Space in Text-based Games. (arXiv:2305.04082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#33258;&#28982;&#35821;&#35328;&#21160;&#20316;&#31354;&#38388;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102; &#949;-&#21487;&#25509;&#21463;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#35821;&#35328;&#27169;&#22411;&#25110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25991;&#26412;&#35282;&#33394;-&#35780;&#35770;&#65288;TAC&#65289;&#20195;&#29702;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#28216;&#25103;&#26159;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#24378;&#21270;&#23398;&#20064;&#20132;&#20114;&#29615;&#22659;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#36890;&#24120;&#34987;&#29992;&#20110;&#22788;&#29702;&#25991;&#26412;&#28216;&#25103;&#20013;&#30340;&#22823;&#37327;&#21160;&#20316;&#31354;&#38388;&#65292;&#20294;&#30446;&#21069;&#20173;&#19981;&#30830;&#23450;&#36825;&#20123;&#25216;&#26415;&#26159;&#21542;&#24517;&#35201;&#25110;&#34987;&#36807;&#24230;&#20351;&#29992;&#12290;&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#25506;&#32034;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102; &#949;-&#21487;&#25509;&#21463;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#21487;&#25509;&#21463;&#30340;&#21160;&#20316;&#30340;&#26368;&#23567;&#21270;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#35282;&#33394;-&#35780;&#35770;&#65288;TAC&#65289;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#20165;&#20174;&#28216;&#25103;&#35266;&#23519;&#20013;&#29983;&#25104;&#25991;&#26412;&#21629;&#20196;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#25110;&#30693;&#35782;&#22270;&#35889;&#12290;&#22312; Jericho &#30340; 10 &#31181;&#28216;&#25103;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#20248;&#20110;&#24378;&#22522;&#32447;&#21644;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20984;&#26174;&#20986;&#65292;&#23545;&#20110;&#26377;&#25928;&#22320;&#25506;&#32034;&#25351;&#25968;&#32423;&#21160;&#20316;&#31354;&#38388;&#65292;&#26356;&#36731;&#37327;&#21270;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#21033;&#29992;&#29615;&#22659;&#20449;&#24687;&#30340;&#26032;&#35270;&#35282;&#26159;&#36275;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based games (TGs) are language-based interactive environments for reinforcement learning. While language models (LMs) and knowledge graphs (KGs) are commonly used for handling large action space in TGs, it is unclear whether these techniques are necessary or overused. In this paper, we revisit the challenge of exploring the action space in TGs and propose $ \epsilon$-admissible exploration, a minimal approach of utilizing admissible actions, for training phase. Additionally, we present a text-based actor-critic (TAC) agent that produces textual commands for game, solely from game observations, without requiring any KG or LM. Our method, on average across 10 games from Jericho, outperforms strong baselines and state-of-the-art agents that use LM and KG. Our approach highlights that a much lighter model design, with a fresh perspective on utilizing the information within the environments, suffices for an effective exploration of exponentially large action spaces.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#37319;&#35775;&#20197;&#21450;&#23545;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#25991;&#29486;&#30340;&#30740;&#31350;&#65292;&#22686;&#24378;&#20102;&#8220;AdaTest&#8221;&#23457;&#35745;&#24037;&#20855;&#65292;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20154;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#21327;&#21516;&#20248;&#21183;&#65292;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.09991</link><description>&lt;p&gt;
&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#23457;&#35745;LLM&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Supporting Human-AI Collaboration in Auditing LLMs with LLMs. (arXiv:2304.09991v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#37319;&#35775;&#20197;&#21450;&#23545;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#25991;&#29486;&#30340;&#30740;&#31350;&#65292;&#22686;&#24378;&#20102;&#8220;AdaTest&#8221;&#23457;&#35745;&#24037;&#20855;&#65292;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20154;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#21327;&#21516;&#20248;&#21183;&#65292;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#37096;&#32626;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#21644;&#26222;&#21450;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#35770;&#26159;&#29992;&#20110;&#20998;&#31867;&#36824;&#26159;&#29983;&#25104;&#65292;&#37117;&#34920;&#29616;&#20986;&#26377;&#20559;&#24046;&#21644;&#19981;&#36127;&#36131;&#20219;&#30340;&#34892;&#20026;&#65292;&#23545;&#20154;&#31867;&#36896;&#25104;&#20102;&#35268;&#27169;&#24615;&#30340;&#20260;&#23475;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20005;&#26684;&#23457;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#23457;&#35745;&#24037;&#20855;&#21033;&#29992;&#20154;&#21644;&#25110;AI&#26469;&#21457;&#29616;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#30340;&#25991;&#29486;&#65292;&#24182;&#37319;&#35775;&#20102;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#19987;&#23478;&#65292;&#20197;&#22686;&#24378;&#23457;&#35745;&#24037;&#20855;&#8220;AdaTest&#8221;&#65288;Ribeiro&#21644;Lundberg&#65292;2022&#65289;&#65292;&#35813;&#24037;&#20855;&#30001;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#12290;&#36890;&#36807;&#35774;&#35745;&#36807;&#31243;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#24863;&#30693;&#21644;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#22312;&#21327;&#20316;&#23457;&#35745;&#20013;&#21033;&#29992;&#20154;&#19982;&#29983;&#25104;&#27169;&#22411;&#30340;&#20114;&#34917;&#20248;&#21183;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#22686;&#24378;&#24037;&#20855;AdaTest ++&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#65292;&#20351;&#21442;&#19982;&#32773;&#36827;&#34892;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Large language models are becoming increasingly pervasive and ubiquitous in society via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously. Existing auditing tools leverage either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and conduct interviews with research experts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro and Lundberg, 2022), which is powered by a generative large language model (LLM). Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of the augmented tool, AdaTest++, we conduct user studies with participants audit
&lt;/p&gt;</description></item></channel></rss>