<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.20331</link><description>&lt;p&gt;
&#19981;&#21487;&#35299;&#38382;&#39064;&#26816;&#27979;&#65306;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;UPD&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#35774;&#32622;&#65306;&#32570;&#22833;&#31572;&#26696;&#26816;&#27979;&#65288;AAD&#65289;&#12289;&#19981;&#20860;&#23481;&#31572;&#26696;&#38598;&#26816;&#27979;&#65288;IASD&#65289;&#21644;&#19981;&#20860;&#23481;&#35270;&#35273;&#38382;&#39064;&#26816;&#27979;&#65288;IVQD&#65289;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#28145;&#20837;&#30740;&#31350;UPD&#38382;&#39064;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;VLMs&#65292;&#21253;&#25324;GPT-4V&#21644;LLaVA-Next-34B&#65292;&#22312;&#21508;&#31181;&#31243;&#24230;&#19978;&#37117;&#24456;&#38590;&#24212;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#37325;&#35201;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;UPD&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#38656;&#35757;&#32451;&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#25552;&#35758;&#30340;UPD&#35774;&#32622;&#20869;&#30340;&#26410;&#26469;&#21162;&#21147;&#65292;&#23558;&#22686;&#24378;&#23545;VLMs&#30340;&#26356;&#24191;&#27867;&#29702;&#35299;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20331v1 Announce Type: cross  Abstract: This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#28041;&#21450;&#21040;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#31561;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.20329</link><description>&lt;p&gt;
ReALM: &#21442;&#32771;&#35299;&#26512;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
ReALM: Reference Resolution As Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#28041;&#21450;&#21040;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#31561;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#32771;&#35299;&#26512;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#29702;&#35299;&#21644;&#25104;&#21151;&#22788;&#29702;&#21508;&#31181;&#19978;&#19979;&#25991;&#33267;&#20851;&#37325;&#35201;&#12290; &#36825;&#31181;&#19978;&#19979;&#25991;&#26082;&#21253;&#25324;&#20808;&#21069;&#30340;&#23545;&#35805;&#65292;&#20063;&#21253;&#25324;&#19982;&#38750;&#23545;&#35805;&#23454;&#20307;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#29992;&#25143;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#25110;&#21518;&#21488;&#36816;&#34892;&#30340;&#23454;&#20307;&#12290; &#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#20102;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#23427;&#20204;&#22312;&#21442;&#32771;&#35299;&#26512;&#20013;&#30340;&#36816;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#23545;&#35805;&#23454;&#20307;&#65292;&#20173;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290; &#26412;&#25991;&#23637;&#31034;&#20102;LLMs&#22914;&#20309;&#34987;&#29992;&#26469;&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#20854;&#20013;&#28041;&#21450;&#23631;&#24149;&#19978;&#30340;&#36825;&#31181;&#23454;&#20307;&#31561;&#20256;&#32479;&#19978;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#21442;&#32771;&#35299;&#26512;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#31867;&#20284;&#21151;&#33021;&#30340;&#31995;&#32479;&#30340;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20329v1 Announce Type: cross  Abstract: Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of re
&lt;/p&gt;</description></item><item><title>Gecko&#26159;&#19968;&#31181;&#32039;&#20945;&#32780;&#22810;&#21151;&#33021;&#30340;&#25991;&#26412;&#23884;&#20837;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#28860;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20026;&#26816;&#32034;&#22120;&#65292;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;Massive Text Embedding Benchmark&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25152;&#26377;&#26465;&#30446;&#12290;</title><link>https://arxiv.org/abs/2403.20327</link><description>&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#28860;&#20986;&#30340;&#22810;&#21151;&#33021;&#25991;&#26412;&#23884;&#20837;&#24335;&#27169;&#22411;Gecko
&lt;/p&gt;
&lt;p&gt;
Gecko: Versatile Text Embeddings Distilled from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20327
&lt;/p&gt;
&lt;p&gt;
Gecko&#26159;&#19968;&#31181;&#32039;&#20945;&#32780;&#22810;&#21151;&#33021;&#30340;&#25991;&#26412;&#23884;&#20837;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#28860;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20026;&#26816;&#32034;&#22120;&#65292;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;Massive Text Embedding Benchmark&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25152;&#26377;&#26465;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Gecko&#65292;&#19968;&#31181;&#32039;&#20945;&#32780;&#22810;&#21151;&#33021;&#30340;&#25991;&#26412;&#23884;&#20837;&#24335;&#27169;&#22411;&#12290;Gecko&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#20851;&#38190;&#24605;&#24819;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#26816;&#32034;&#24615;&#33021;&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#30340;&#30693;&#35782;&#25552;&#28860;&#20026;&#26816;&#32034;&#22120;&#12290;&#25105;&#20204;&#30340;&#20004;&#27493;&#25552;&#28860;&#36807;&#31243;&#39318;&#20808;&#28041;&#21450;&#20351;&#29992;LLM&#29983;&#25104;&#22810;&#26679;&#21270;&#21512;&#25104;&#30340;&#37197;&#23545;&#25968;&#25454;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;&#27599;&#20010;&#26597;&#35810;&#26816;&#32034;&#19968;&#32452;&#20505;&#36873;&#27573;&#33853;&#65292;&#24182;&#20351;&#29992;&#30456;&#21516;&#30340;LLM&#37325;&#26032;&#26631;&#35760;&#27491;&#38754;&#21644;&#22256;&#38590;&#30340;&#36127;&#38754;&#27573;&#33853;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;Gecko&#27169;&#22411;&#30340;&#32039;&#20945;&#24615;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#24335;&#22522;&#20934;&#27979;&#35797;(MTEB)&#19978;&#65292;&#20855;&#26377;256&#23884;&#20837;&#32500;&#24230;&#30340;Gecko&#30340;&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#30340;768&#23884;&#20837;&#23610;&#23544;&#30340;&#26465;&#30446;&#12290;&#20855;&#26377;768&#23884;&#20837;&#32500;&#24230;&#30340;Gecko&#23454;&#29616;&#20102;&#24179;&#22343;&#24471;&#20998;66.31&#65292;&#19982;7&#20493;&#22823;&#30340;&#27169;&#22411;&#21644;5&#20493;&#39640;&#32500;&#23884;&#20837;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20327v1 Announce Type: new  Abstract: We present Gecko, a compact and versatile text embedding model. Gecko achieves strong retrieval performance by leveraging a key idea: distilling knowledge from large language models (LLMs) into a retriever. Our two-step distillation process begins with generating diverse, synthetic paired data using an LLM. Next, we further refine the data quality by retrieving a set of candidate passages for each query, and relabeling the positive and hard negative passages using the same LLM. The effectiveness of our approach is demonstrated by the compactness of the Gecko. On the Massive Text Embedding Benchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing entries with 768 embedding size. Gecko with 768 embedding dimensions achieves an average score of 66.31, competing with 7x larger models and 5x higher dimensional embeddings.
&lt;/p&gt;</description></item><item><title>&#20513;&#23548;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#29702;&#24615;&#21270;&#35299;&#37322;&#65292;&#38024;&#23545;&#33258;&#21160;&#20107;&#23454;&#39564;&#35777;&#20219;&#21153;&#20013;&#19981;&#21516;&#32467;&#26500;&#30340;&#35299;&#37322;&#25552;&#20379;&#20102;&#19968;&#22871;&#35780;&#20272;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.20322</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#20107;&#23454;&#39564;&#35777;&#20013;&#35780;&#20272;&#35299;&#37322;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards a Framework for Evaluating Explanations in Automated Fact Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20322
&lt;/p&gt;
&lt;p&gt;
&#20513;&#23548;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#29702;&#24615;&#21270;&#35299;&#37322;&#65292;&#38024;&#23545;&#33258;&#21160;&#20107;&#23454;&#39564;&#35777;&#20219;&#21153;&#20013;&#19981;&#21516;&#32467;&#26500;&#30340;&#35299;&#37322;&#25552;&#20379;&#20102;&#19968;&#22871;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#20063;&#22240;&#27492;&#21464;&#24471;&#19981;&#36879;&#26126;&#65292;&#35299;&#37322;&#23427;&#20204;&#30340;&#24517;&#35201;&#24615;&#21464;&#24471;&#26356;&#20026;&#37325;&#35201;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#29702;&#24615;&#21270;&#35299;&#37322;&#20135;&#29983;&#20102;&#20852;&#36259;&#65292;&#20197;&#25552;&#20379;&#23545;&#39044;&#27979;&#30340;&#31616;&#30701;&#21644;&#36830;&#36143;&#30340;&#29702;&#30001;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20513;&#23548;&#19968;&#20010;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20851;&#20110;&#29702;&#24615;&#21270;&#35299;&#37322;&#30340;&#20851;&#38190;&#27010;&#24565;&#21644;&#23646;&#24615;&#65292;&#20197;&#25903;&#25345;&#31995;&#32479;&#22320;&#35780;&#20272;&#23427;&#20204;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#19968;&#20010;&#36825;&#26679;&#30340;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#23450;&#21046;&#20110;&#36234;&#26469;&#36234;&#22797;&#26434;&#32467;&#26500;&#30340;&#29702;&#24615;&#21270;&#35299;&#37322;&#65292;&#20174;&#33258;&#30001;&#24418;&#24335;&#30340;&#35299;&#37322;&#21040;&#28436;&#32462;&#24615;&#35299;&#37322;&#65292;&#20877;&#21040;&#20855;&#26377;&#26368;&#20016;&#23500;&#32467;&#26500;&#30340;&#35770;&#35777;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#33258;&#21160;&#20107;&#23454;&#39564;&#35777;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#24418;&#24335;&#21270;&#35780;&#20272;&#35299;&#37322;&#30340;&#31034;&#20363;&#21450;&#20854;&#23454;&#29992;&#24615;&#65292;&#38024;&#23545;&#19981;&#21516;&#32467;&#26500;&#20570;&#20986;&#20102;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20322v1 Announce Type: new  Abstract: As deep neural models in NLP become more complex, and as a consequence opaque, the necessity to interpret them becomes greater. A burgeoning interest has emerged in rationalizing explanations to provide short and coherent justifications for predictions. In this position paper, we advocate for a formal framework for key concepts and properties about rationalizing explanations to support their evaluation systematically. We also outline one such formal framework, tailored to rationalizing explanations of increasingly complex structures, from free-form explanations to deductive explanations, to argumentative explanations (with the richest structure). Focusing on the automated fact verification task, we provide illustrations of the use and usefulness of our formalization for evaluating explanations, tailored to their varying structures.
&lt;/p&gt;</description></item><item><title>ChainNet&#26159;&#19968;&#20010;&#35789;&#20856;&#36164;&#28304;&#65292;&#39318;&#27425;&#26126;&#30830;&#22320;&#35782;&#21035;&#20102;WordNet&#20013;&#35789;&#20041;&#30340;&#32467;&#26500;&#21270;&#38544;&#21947;&#21644;&#36716;&#21947;&#20851;&#31995;&#65292;&#25104;&#20026;&#31532;&#19968;&#20010;&#20855;&#26377;&#22522;&#30784;&#38544;&#21947;&#21644;&#36716;&#21947;&#25968;&#25454;&#38598;&#30340;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.20308</link><description>&lt;p&gt;
ChainNet: &#22312;WordNet&#20013;&#30340;&#32467;&#26500;&#21270;&#38544;&#21947;&#21644;&#36716;&#21947;
&lt;/p&gt;
&lt;p&gt;
ChainNet: Structured Metaphor and Metonymy in WordNet
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20308
&lt;/p&gt;
&lt;p&gt;
ChainNet&#26159;&#19968;&#20010;&#35789;&#20856;&#36164;&#28304;&#65292;&#39318;&#27425;&#26126;&#30830;&#22320;&#35782;&#21035;&#20102;WordNet&#20013;&#35789;&#20041;&#30340;&#32467;&#26500;&#21270;&#38544;&#21947;&#21644;&#36716;&#21947;&#20851;&#31995;&#65292;&#25104;&#20026;&#31532;&#19968;&#20010;&#20855;&#26377;&#22522;&#30784;&#38544;&#21947;&#21644;&#36716;&#21947;&#25968;&#25454;&#38598;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#35789;&#30340;&#24847;&#20041;&#23637;&#29616;&#20986;&#20016;&#23500;&#30340;&#20869;&#37096;&#32467;&#26500;&#12290;&#22312;&#20856;&#22411;&#30340;&#35789;&#20856;&#20013;&#65292;&#36825;&#31181;&#32467;&#26500;&#24448;&#24448;&#34987;&#24573;&#35270;&#65306;&#19968;&#20010;&#35789;&#30340;&#24847;&#20041;&#34987;&#32534;&#30721;&#20026;&#19968;&#20010;&#27809;&#26377;&#24847;&#20041;&#38388;&#20851;&#31995;&#30340;&#21015;&#34920;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ChainNet&#65292;&#36825;&#26159;&#19968;&#20010;&#35789;&#20856;&#36164;&#28304;&#65292;&#39318;&#27425;&#26126;&#30830;&#22320;&#35782;&#21035;&#20102;&#36825;&#20123;&#32467;&#26500;&#12290;ChainNet&#34920;&#36798;&#20102;&#22312;Open English Wordnet&#20013;&#35789;&#20041;&#22914;&#20309;&#20174;&#24444;&#27492;&#34893;&#29983;&#20986;&#26469;&#65306;&#19968;&#20010;&#35789;&#30340;&#27599;&#20010;&#21517;&#35789;&#24847;&#20041;&#35201;&#20040;&#36890;&#36807;&#38544;&#21947;&#25110;&#36716;&#21947;&#19982;&#21478;&#19968;&#20010;&#24847;&#20041;&#30456;&#36830;&#65292;&#35201;&#20040;&#22312;&#21516;&#20041;&#35789;&#30340;&#24773;&#20917;&#19979;&#26159;&#26029;&#24320;&#30340;&#12290;&#30001;&#20110;WordNet&#30340;&#21547;&#20041;&#19982;&#25429;&#25417;&#20854;&#21547;&#20041;&#20449;&#24687;&#30340;&#36164;&#28304;&#30456;&#36830;&#65292;ChainNet&#20195;&#34920;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#22522;&#30784;&#38544;&#21947;&#21644;&#36716;&#21947;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20308v1 Announce Type: cross  Abstract: The senses of a word exhibit rich internal structure. In a typical lexicon, this structure is overlooked: a word's senses are encoded as a list without inter-sense relations. We present ChainNet, a lexical resource which for the first time explicitly identifies these structures. ChainNet expresses how senses in the Open English Wordnet are derived from one another: every nominal sense of a word is either connected to another sense by metaphor or metonymy, or is disconnected in the case of homonymy. Because WordNet senses are linked to resources which capture information about their meaning, ChainNet represents the first dataset of grounded metaphor and metonymy.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#38170;&#23450;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20250;&#35805;&#20013;&#35782;&#21035;&#24773;&#24863;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#26131;&#21306;&#20998;&#30340;&#35805;&#35821;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#30456;&#20284;&#24773;&#24863;&#30340;&#21306;&#20998;&#24230;&#21644;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20289</link><description>&lt;p&gt;
&#24773;&#24863;&#38170;&#23450;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#20250;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Emotion-Anchored Contrastive Learning Framework for Emotion Recognition in Conversation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20289
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#38170;&#23450;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20250;&#35805;&#20013;&#35782;&#21035;&#24773;&#24863;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#26131;&#21306;&#20998;&#30340;&#35805;&#35821;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#30456;&#20284;&#24773;&#24863;&#30340;&#21306;&#20998;&#24230;&#21644;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20289v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#20250;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#28041;&#21450;&#22312;&#23545;&#35805;&#20013;&#26816;&#27979;&#27599;&#20010;&#35805;&#35821;&#32972;&#21518;&#30340;&#24773;&#24863;&#12290;&#26377;&#25928;&#29983;&#25104;&#35805;&#35821;&#34920;&#31034;&#20173;&#28982;&#26159;&#36825;&#19968;&#20219;&#21153;&#20013;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#22312;&#21306;&#20998;&#31867;&#20284;&#24773;&#24863;&#65288;&#22914;&#20852;&#22859;&#21644;&#24184;&#31119;&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#38170;&#23450;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;EACL&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#30456;&#20284;&#24773;&#24863;&#29983;&#25104;&#26356;&#26131;&#21306;&#20998;&#30340;&#35805;&#35821;&#34920;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#26631;&#31614;&#32534;&#30721;&#20316;&#20026;&#38170;&#28857;&#26469;&#25351;&#23548;&#35805;&#35821;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#36741;&#21161;&#25439;&#22833;&#26469;&#30830;&#20445;&#30456;&#20284;&#24773;&#24863;&#30340;&#38170;&#28857;&#26377;&#25928;&#20998;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#36866;&#24212;&#36807;&#31243;&#65292;&#29992;&#20110;&#35843;&#25972;&#38170;&#28857;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#20998;&#31867;&#22120;&#65292;&#20197;&#25913;&#21892;&#20998;&#31867;&#24615;&#33021;&#12290;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20289v1 Announce Type: new  Abstract: Emotion Recognition in Conversation (ERC) involves detecting the underlying emotion behind each utterance within a conversation. Effectively generating representations for utterances remains a significant challenge in this task. Recent works propose various models to address this issue, but they still struggle with differentiating similar emotions such as excitement and happiness. To alleviate this problem, We propose an Emotion-Anchored Contrastive Learning (EACL) framework that can generate more distinguishable utterance representations for similar emotions. To achieve this, we utilize label encodings as anchors to guide the learning of utterance representations and design an auxiliary loss to ensure the effective separation of anchors for similar emotions. Moreover, an additional adaptation process is proposed to adapt anchors to serve as effective classifiers to improve classification performance. Across extensive experiments, our pr
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#21307;&#23398;&#20915;&#31574;&#20013;&#25552;&#20379;&#37325;&#35201;&#21453;&#39304;&#65292;&#21487;&#20197;&#25361;&#25112;&#19981;&#27491;&#30830;&#30340;&#35786;&#26029;&#65292;&#20419;&#36827;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.20288</link><description>&lt;p&gt;
LLM&#33021;&#22815;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#32416;&#27491;&#21307;&#29983;&#21527;&#65311;&#30740;&#31350;&#26377;&#25928;&#30340;&#20132;&#20114;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20288
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#21307;&#23398;&#20915;&#31574;&#20013;&#25552;&#20379;&#37325;&#35201;&#21453;&#39304;&#65292;&#21487;&#20197;&#25361;&#25112;&#19981;&#27491;&#30830;&#30340;&#35786;&#26029;&#65292;&#20419;&#36827;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21327;&#21161;&#24182;&#21487;&#33021;&#32416;&#27491;&#21307;&#29983;&#36827;&#34892;&#21307;&#30103;&#20915;&#31574;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;LLMs&#65292;&#21253;&#25324;Meditron&#12289;Llama2&#21644;Mistral&#65292;&#20998;&#26512;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#19982;&#21307;&#29983;&#26377;&#25928;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;PubMedQA&#30340;&#38382;&#39064;&#21644;&#20960;&#39033;&#20219;&#21153;&#65292;&#20174;&#20108;&#20803;&#65288;&#26159;/&#21542;&#65289;&#22238;&#31572;&#21040;&#38271;&#31572;&#26696;&#29983;&#25104;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#31572;&#26696;&#26159;&#22312;&#19982;&#21307;&#29983;&#20132;&#20114;&#21518;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#31034;&#35774;&#35745;&#26174;&#33879;&#24433;&#21709;&#20102;LLMs&#30340;&#19979;&#28216;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;LLMs&#21487;&#20197;&#20026;&#21307;&#29983;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#21453;&#39304;&#65292;&#25361;&#25112;&#19981;&#27491;&#30830;&#30340;&#35786;&#26029;&#65292;&#20419;&#36827;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#20363;&#22914;&#65292;&#24403;&#21307;&#29983;&#20934;&#30830;&#29575;&#20026;38%&#26102;&#65292;Mistral&#21487;&#20197;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#26681;&#25454;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#65292;&#23558;&#20934;&#30830;&#24615;&#25552;&#39640;&#21040;74%&#65292;&#32780;Llama2&#21644;Meditron&#27169;&#22411;&#20063;&#33021;&#25552;&#20379;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20288v1 Announce Type: cross  Abstract: We explore the potential of Large Language Models (LLMs) to assist and potentially correct physicians in medical decision-making tasks. We evaluate several LLMs, including Meditron, Llama2, and Mistral, to analyze the ability of these models to interact effectively with physicians across different scenarios. We consider questions from PubMedQA and several tasks, ranging from binary (yes/no) responses to long answer generation, where the answer of the model is produced after an interaction with a physician. Our findings suggest that prompt design significantly influences the downstream accuracy of LLMs and that LLMs can provide valuable feedback to physicians, challenging incorrect diagnoses and contributing to more accurate decision-making. For example, when the physician is accurate 38% of the time, Mistral can produce the correct answer, improving accuracy up to 74% depending on the prompt being used, while Llama2 and Meditron models
&lt;/p&gt;</description></item><item><title>LayerNorm&#22312;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#65292;&#24494;&#35843;output LayerNorm&#32780;&#20445;&#25345;&#20854;&#20182;&#37096;&#20998;&#19981;&#21464;&#36275;&#20197;&#22312;&#22810;&#20010;GLUE&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#21147;</title><link>https://arxiv.org/abs/2403.20284</link><description>&lt;p&gt;
LayerNorm&#65306;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;
&lt;/p&gt;
&lt;p&gt;
LayerNorm: A key component in parameter-efficient fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20284
&lt;/p&gt;
&lt;p&gt;
LayerNorm&#22312;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#65292;&#24494;&#35843;output LayerNorm&#32780;&#20445;&#25345;&#20854;&#20182;&#37096;&#20998;&#19981;&#21464;&#36275;&#20197;&#22312;&#22810;&#20010;GLUE&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22914;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#36716;&#25442;&#22120;&#65288;BERT&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#65288;&#21253;&#25324;BERT&#65289;&#20013;&#30340;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#65292;&#24494;&#35843;&#36807;&#31243;&#32791;&#36153;&#20102;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#21560;&#24341;&#20154;&#26041;&#27861;&#26159;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#21363;&#20165;&#20462;&#25913;&#27169;&#22411;&#30340;&#26368;&#23567;&#37096;&#20998;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20313;&#37096;&#20998;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;BERT&#27169;&#22411;&#30340;&#21738;&#20010;&#37096;&#20998;&#23545;&#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;BERT&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#32452;&#20214;&#65292;&#20197;&#26597;&#26126;&#22312;&#24494;&#35843;&#21518;&#21738;&#20123;&#32452;&#20214;&#21457;&#29983;&#20102;&#26368;&#26174;&#33879;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#38024;&#23545;&#19981;&#21516;General Language Understanding Evaluation&#65288;GLUE&#65289;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#36755;&#20986;&#30340;LayerNorm&#21457;&#29983;&#30340;&#21464;&#21270;&#27604;&#20854;&#20182;&#32452;&#20214;&#37117;&#35201;&#22823;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20165;&#24494;&#35843;output LayerNorm&#32780;&#20445;&#25345;&#20854;&#20182;&#37096;&#20998;&#19981;&#21464;&#23601;&#36275;&#20197;&#22312;&#22810;&#20010;GLUE&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20284v1 Announce Type: new  Abstract: Fine-tuning a pre-trained model, such as Bidirectional Encoder Representations from Transformers (BERT), has been proven to be an effective method for solving many natural language processing (NLP) tasks. However, due to the large number of parameters in many state-of-the-art NLP models, including BERT, the process of fine-tuning is computationally expensive. One attractive solution to this issue is parameter-efficient fine-tuning, which involves modifying only a minimal segment of the model while keeping the remainder unchanged. Yet, it remains unclear which segment of the BERT model is crucial for fine-tuning. In this paper, we first analyze different components in the BERT model to pinpoint which one undergoes the most significant changes after fine-tuning. We find that output LayerNorm changes more than any other components when fine-tuned for different General Language Understanding Evaluation (GLUE) tasks. Then we show that only fi
&lt;/p&gt;</description></item><item><title>LUQ&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38271;&#25991;&#26412;&#35774;&#35745;&#30340;&#26032;&#22411;&#37319;&#26679;UQ&#26041;&#27861;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#22312;&#19982;&#27169;&#22411;&#30340;&#20107;&#23454;&#24471;&#20998;&#30456;&#20851;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2403.20279</link><description>&lt;p&gt;
LUQ&#65306;LLM&#27169;&#22411;&#30340;&#38271;&#25991;&#26412;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
LUQ: Long-text Uncertainty Quantification for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20279
&lt;/p&gt;
&lt;p&gt;
LUQ&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38271;&#25991;&#26412;&#35774;&#35745;&#30340;&#26032;&#22411;&#37319;&#26679;UQ&#26041;&#27861;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#22312;&#19982;&#27169;&#22411;&#30340;&#20107;&#23454;&#24471;&#20998;&#30456;&#20851;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#23427;&#20204;&#26377;&#25928;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20542;&#21521;&#20110;&#29983;&#25104;&#38750;&#20107;&#23454;&#20869;&#23481;&#12290;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#23545;&#20110;&#22686;&#24378;&#25105;&#20204;&#23545;&#27169;&#22411;&#22312;&#29983;&#25104;&#20869;&#23481;&#19978;&#30340;&#20449;&#24515;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#20943;&#36731;&#38750;&#20107;&#23454;&#36755;&#20986;&#12290;&#29616;&#26377;&#30340;UQ&#30740;&#31350;&#20027;&#35201;&#38024;&#23545;&#30701;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#24120;&#20135;&#29983;&#31616;&#30701;&#30340;&#12289;&#21463;&#35789;&#38480;&#21046;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#38656;&#35201;&#26356;&#38271;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#24378;&#35843;&#20102;&#24403;&#21069;UQ&#26041;&#27861;&#22312;&#22788;&#29702;&#38271;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc{Luq}&#30340;&#26032;&#22411;&#22522;&#20110;&#25277;&#26679;&#30340;UQ&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#38271;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;\textsc{Luq}&#22312;&#19982;&#27169;&#22411;&#30340;&#20107;&#23454;&#24471;&#20998;&#30456;&#20851;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65288;Gemini Pro&#35266;&#23519;&#21040;-0.85&#30340;&#36127;&#30456;&#20851;&#31995;&#25968;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20279v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable capability in a variety of NLP tasks. Despite their effectiveness, these models are prone to generate nonfactual content. Uncertainty Quantification (UQ) is pivotal in enhancing our understanding of a model's confidence in its generated content, thereby aiding in the mitigation of nonfactual outputs. Existing research on UQ predominantly targets short text generation, typically yielding brief, word-limited responses. However, real-world applications frequently necessitate much longer responses. Our study first highlights the limitations of current UQ methods in handling long text generation. We then introduce \textsc{Luq}, a novel sampling-based UQ approach specifically designed for long text. Our findings reveal that \textsc{Luq} outperforms existing baseline methods in correlating with the model's factuality scores (negative coefficient of -0.85 observed for Gemini Pro). With \t
&lt;/p&gt;</description></item><item><title>Latxa&#26159;&#19968;&#31181;&#29992;&#20110;&#24052;&#26031;&#20811;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29087;&#32451;&#24230;&#21644;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#25152;&#26377;&#20197;&#21069;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24052;&#26031;&#20811;&#35821;&#39640;&#36136;&#37327;&#22522;&#20934;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.20266</link><description>&lt;p&gt;
Latxa: &#19968;&#31181;&#29992;&#20110;&#24052;&#26031;&#20811;&#35821;&#30340;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#21644;&#35780;&#20272;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
Latxa: An Open Language Model and Evaluation Suite for Basque
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20266
&lt;/p&gt;
&lt;p&gt;
Latxa&#26159;&#19968;&#31181;&#29992;&#20110;&#24052;&#26031;&#20811;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29087;&#32451;&#24230;&#21644;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#25152;&#26377;&#20197;&#21069;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24052;&#26031;&#20811;&#35821;&#39640;&#36136;&#37327;&#22522;&#20934;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Latxa&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Llama 2&#30340;&#22823;&#22411;&#24052;&#26031;&#20811;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;7&#21040;700&#20159;&#12290;Latxa&#22522;&#20110;&#26032;&#30340;&#24052;&#26031;&#20811;&#35821;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;430&#19975;&#20010;&#25991;&#26723;&#21644;42&#20159;&#20010;&#26631;&#35760;&#12290;&#38024;&#23545;&#24052;&#26031;&#20811;&#35821;&#39640;&#36136;&#37327;&#22522;&#20934;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;4&#20010;&#22810;&#39033;&#36873;&#25321;&#35780;&#20272;&#25968;&#25454;&#38598;&#65306;EusProficiency&#65292;&#21253;&#25324;&#26469;&#33258;&#23448;&#26041;&#35821;&#35328;&#33021;&#21147;&#32771;&#35797;&#30340;5169&#20010;&#38382;&#39064;&#65307;EusReading&#65292;&#21253;&#25324;352&#20010;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#65307;EusTrivia&#65292;&#21253;&#25324;&#26469;&#33258;5&#20010;&#30693;&#35782;&#39046;&#22495;&#30340;1715&#20010;&#29712;&#20107;&#38382;&#39064;&#65307;&#20197;&#21450;EusExams&#65292;&#21253;&#25324;&#26469;&#33258;&#20844;&#20849;&#32771;&#35797;&#30340;16774&#20010;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#20013;&#65292;Latxa&#22312;&#19982;&#25105;&#20204;&#27604;&#36739;&#30340;&#25152;&#26377;&#20808;&#21069;&#24320;&#25918;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;&#38405;&#35835;&#29702;&#35299;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26041;&#38754;&#33853;&#21518;&#65292;&#20294;&#22312;&#35821;&#35328;&#29087;&#32451;&#24230;&#21644;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#65292;&#23427;&#19982;GPT-4 Turbo&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;Latxa&#27169;&#22411;&#31995;&#21015;&#65292;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20266v1 Announce Type: cross  Abstract: We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934; ELITR-Bench&#65292;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377; ELITR &#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#20013;&#28155;&#21152;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.20262</link><description>&lt;p&gt;
ELITR-Bench: &#38754;&#21521;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#20250;&#35758;&#21161;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20262
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934; ELITR-Bench&#65292;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377; ELITR &#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#20013;&#28155;&#21152;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20027;&#35201;&#33268;&#21147;&#20110;&#25193;&#23637;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#38271;&#25991;&#26723;&#20869;&#37096;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#38271;&#36317;&#31163;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#20294;&#29616;&#26377;&#30340;&#21162;&#21147;&#20027;&#35201;&#32771;&#34385;&#30340;&#26159;&#19981;&#19968;&#23450;&#19982;&#29616;&#23454;&#24212;&#29992;&#30456;&#20851;&#30340;&#36890;&#29992;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#30340;&#26032;&#22522;&#20934;&#12290;&#22312;&#36825;&#31181;&#24773;&#26223;&#19979;&#65292;&#38271;&#19978;&#19979;&#25991;&#30001;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#33719;&#24471;&#30340;&#36716;&#24405;&#32452;&#25104;&#65292;&#30001;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#22266;&#26377;&#22024;&#26434;&#24615;&#21644;&#21475;&#35821;&#29305;&#24615;&#65292;&#36825;&#20026;LLMs&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;ELITR-Bench&#65292;&#36890;&#36807;271&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21450;&#20854;&#30495;&#23454;&#31572;&#26696;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;ELITR&#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#12290;&#25105;&#20204;&#22312;ELITR-Bench&#19978;&#23545;&#26368;&#26032;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#36827;&#34892;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20262v1 Announce Type: cross  Abstract: Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications. In contrast, our work proposes a new benchmark for long-context LLMs focused on a practical meeting assistant scenario. In this scenario, the long contexts consist of transcripts obtained by automatic speech recognition, presenting unique challenges for LLMs due to the inherent noisiness and oral nature of such data. Our benchmark, named ELITR-Bench, augments the existing ELITR corpus' transcripts with 271 manually crafted questions and their ground-truth answers. Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, e
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLMs&#24314;&#27169;&#30446;&#26631;&#20154;&#32676;&#30340;&#20449;&#24565;&#21644;&#20559;&#22909;&#65292;&#26088;&#22312;&#23454;&#29616;&#21508;&#31181;&#24212;&#29992;&#65292;&#35780;&#20272;&#19981;&#21516;&#24494;&#35843;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#26816;&#39564;&#20854;&#22312;&#21305;&#37197;&#30495;&#23454;&#20154;&#31867;&#21463;&#35775;&#32773;&#20559;&#22909;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.20252</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#24314;&#27169;&#30446;&#26631;&#20154;&#32676;&#30340;&#20449;&#24565;&#21644;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Using LLMs to Model the Beliefs and Preferences of Targeted Populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20252
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLMs&#24314;&#27169;&#30446;&#26631;&#20154;&#32676;&#30340;&#20449;&#24565;&#21644;&#20559;&#22909;&#65292;&#26088;&#22312;&#23454;&#29616;&#21508;&#31181;&#24212;&#29992;&#65292;&#35780;&#20272;&#19981;&#21516;&#24494;&#35843;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#26816;&#39564;&#20854;&#22312;&#21305;&#37197;&#30495;&#23454;&#20154;&#31867;&#21463;&#35775;&#32773;&#20559;&#22909;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#19982;&#20154;&#32676;&#30340;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#24314;&#27169;&#29305;&#23450;&#20154;&#32676;&#30340;&#20449;&#24565;&#12289;&#20559;&#22909;&#21644;&#34892;&#20026;&#23545;&#20110;&#21508;&#31181;&#19981;&#21516;&#24212;&#29992;&#21487;&#33021;&#24456;&#26377;&#29992;&#65292;&#27604;&#22914;&#20026;&#26032;&#20135;&#21697;&#24320;&#23637;&#27169;&#25311;&#28966;&#28857;&#23567;&#32452;&#12289;&#36827;&#34892;&#34394;&#25311;&#35843;&#26597;&#20197;&#21450;&#27979;&#35797;&#34892;&#20026;&#24178;&#39044;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26114;&#36149;&#12289;&#19981;&#20999;&#23454;&#38469;&#25110;&#19981;&#36947;&#24503;&#30340;&#24178;&#39044;&#12290;&#29616;&#26377;&#30740;&#31350;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#20351;&#29992;LLMs&#20934;&#30830;&#24314;&#27169;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#65292;&#35780;&#20272;&#24471;&#21040;&#30340;&#20154;&#32676;&#22312;&#21305;&#37197;&#23545;&#30005;&#27744;&#30005;&#21160;&#27773;&#36710;(BEVs)&#20559;&#22909;&#35843;&#26597;&#20013;&#30495;&#23454;&#20154;&#31867;&#21463;&#35775;&#32773;&#20559;&#22909;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#19982;&#25972;&#20307;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20197;&#21450;&#20010;&#20307;&#22238;&#24212;&#30456;&#21305;&#37197;&#65292;&#24182;&#30740;&#31350;LLMs&#22312;&#24314;&#27169;&#20154;&#32676;&#20449;&#24565;&#21644;&#20559;&#22909;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20252v1 Announce Type: cross  Abstract: We consider the problem of aligning a large language model (LLM) to model the preferences of a human population. Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical. Existing work has had mixed success using LLMs to accurately model human behavior in different contexts. We benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs). We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role
&lt;/p&gt;</description></item><item><title>&#24352;&#28023;&#27915;&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#20302;&#24310;&#36831;&#35774;&#32622;&#19979;&#65292;&#36739;&#24369;&#30340;&#27973;&#23618;Transformer&#27169;&#22411;&#22312;&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#23436;&#25972;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#33021;&#21463;&#30410;&#20110;&#24191;&#20041;&#20108;&#20803;&#20132;&#21449;&#29109;&#65288;gBCE&#65289;&#35757;&#32451;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.20222</link><description>&lt;p&gt;
&#29992;&#20110;&#20302;&#24310;&#36831;&#26816;&#32034;&#30340;&#27973;&#23618;&#20132;&#21449;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Shallow Cross-Encoders for Low-Latency Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20222
&lt;/p&gt;
&lt;p&gt;
&#24352;&#28023;&#27915;&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#20302;&#24310;&#36831;&#35774;&#32622;&#19979;&#65292;&#36739;&#24369;&#30340;&#27973;&#23618;Transformer&#27169;&#22411;&#22312;&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#23436;&#25972;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#33021;&#21463;&#30410;&#20110;&#24191;&#20041;&#20108;&#20803;&#20132;&#21449;&#29109;&#65288;gBCE&#65289;&#35757;&#32451;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#20132;&#21449;&#32534;&#30721;&#22120;&#22312;&#25991;&#26412;&#26816;&#32034;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#22823;&#22411;Transformer&#27169;&#22411;&#65288;&#22914;BERT&#25110;T5&#65289;&#30340;&#20132;&#21449;&#32534;&#30721;&#22120;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#19988;&#21482;&#20801;&#35768;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#24310;&#36831;&#26102;&#38388;&#31383;&#21475;&#20869;&#35780;&#20998;&#23569;&#37327;&#25991;&#26723;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#29992;&#20110;&#36825;&#20123;&#23454;&#38469;&#20302;&#24310;&#36831;&#35774;&#32622;&#30340;&#36739;&#24369;&#30340;&#27973;&#23618;Transformer&#27169;&#22411;&#65288;&#21363;&#20855;&#26377;&#26377;&#38480;&#23618;&#25968;&#30340;Transformer&#65289;&#23454;&#38469;&#19978;&#27604;&#23436;&#25972;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#22312;&#21516;&#26679;&#30340;&#26102;&#38388;&#39044;&#31639;&#20869;&#20272;&#31639;&#20986;&#26356;&#22810;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#27973;&#23618;Transformer&#21487;&#33021;&#20250;&#21463;&#30410;&#20110;&#26368;&#36817;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#23637;&#31034;&#25104;&#21151;&#30340;&#24191;&#20041;&#20108;&#20803;&#20132;&#21449;&#29109;&#65288;gBCE&#65289;&#35757;&#32451;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;TREC&#28145;&#24230;&#23398;&#20064;&#27573;&#33853;&#25490;&#24207;&#26597;&#35810;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20222v1 Announce Type: cross  Abstract: Transformer-based Cross-Encoders achieve state-of-the-art effectiveness in text retrieval. However, Cross-Encoders based on large transformer models (such as BERT or T5) are computationally expensive and allow for scoring only a small number of documents within a reasonably small latency window. However, keeping search latencies low is important for user satisfaction and energy usage. In this paper, we show that weaker shallow transformer models (i.e., transformers with a limited number of layers) actually perform better than full-scale models when constrained to these practical low-latency settings since they can estimate the relevance of more documents in the same time budget. We further show that shallow transformers may benefit from the generalized Binary Cross-Entropy (gBCE) training scheme, which has recently demonstrated success for recommendation tasks. Our experiments with TREC Deep Learning passage ranking query sets demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#38463;&#25289;&#20271;&#35821;&#65292;&#36890;&#36807;&#23545;&#38463;&#25289;&#20271;&#35821;WordNet&#36827;&#34892;&#37325;&#22823;&#20462;&#35746;&#65292;&#25552;&#39640;&#20869;&#23481;&#36136;&#37327;&#65292;&#26356;&#26032;&#20102;&#36229;&#36807;58%&#30340;&#21516;&#20041;&#35789;&#38598;&#65292;&#24182;&#35299;&#20915;&#20102;&#35789;&#24418;&#38169;&#35823;&#21644;&#32570;&#22833;&#20449;&#24687;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.20215</link><description>&lt;p&gt;
&#25512;&#36827;&#38463;&#25289;&#20271;&#35821;WordNet&#65306;&#25552;&#21319;&#20869;&#23481;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Advancing the Arabic WordNet: Elevating Content Quality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#38463;&#25289;&#20271;&#35821;&#65292;&#36890;&#36807;&#23545;&#38463;&#25289;&#20271;&#35821;WordNet&#36827;&#34892;&#37325;&#22823;&#20462;&#35746;&#65292;&#25552;&#39640;&#20869;&#23481;&#36136;&#37327;&#65292;&#26356;&#26032;&#20102;&#36229;&#36807;58%&#30340;&#21516;&#20041;&#35789;&#38598;&#65292;&#24182;&#35299;&#20915;&#20102;&#35789;&#24418;&#38169;&#35823;&#21644;&#32570;&#22833;&#20449;&#24687;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;WordNet&#23545;&#20110;&#20381;&#36182;&#27492;&#31867;&#36164;&#28304;&#30340;NLP&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#39640;&#36136;&#37327;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35821;&#35328;&#30340;WordNet&#23384;&#22312;&#20005;&#37325;&#30340;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#38382;&#39064;&#65292;&#21253;&#25324;&#38169;&#35823;&#30340;&#35789;&#24418;&#12289;&#32570;&#22833;&#30340;&#27880;&#37322;&#21644;&#20363;&#21477;&#65292;&#20197;&#21450;&#23545;&#35821;&#35328;&#30340;&#24418;&#24577;&#21644;&#35821;&#20041;&#30340;&#19981;&#36275;&#65292;&#20307;&#29616;&#20102;&#35199;&#26041;&#20013;&#24515;&#30340;&#34920;&#24449;&#12290;&#20808;&#21069;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#21152;&#35789;&#27719;&#35206;&#30422;&#29575;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#36136;&#37327;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#38463;&#25289;&#20271;&#35821;&#24182;&#20171;&#32461;&#20102;&#38463;&#25289;&#20271;&#35821;WordNet&#30340;&#37325;&#22823;&#20462;&#35746;&#65292;&#28085;&#30422;&#20102;&#35789;&#27719;&#35821;&#20041;&#36164;&#28304;&#36136;&#37327;&#30340;&#22810;&#20010;&#32500;&#24230;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#32570;&#22833;&#20449;&#24687;&#21644;&#32416;&#27491;&#38169;&#35823;&#26356;&#26032;&#20102;&#29616;&#26377;&#38463;&#25289;&#20271;&#35821;WordNet&#36229;&#36807;58%&#30340;&#21516;&#20041;&#35789;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#35821;&#35328;&#22810;&#26679;&#24615;&#21644;&#26080;&#27861;&#32763;&#35793;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20215v1 Announce Type: new  Abstract: High-quality WordNets are crucial for achieving high-quality results in NLP applications that rely on such resources. However, the wordnets of most languages suffer from serious issues of correctness and completeness with respect to the words and word meanings they define, such as incorrect lemmas, missing glosses and example sentences, or an inadequate, Western-centric representation of the morphology and the semantics of the language. Previous efforts have largely focused on increasing lexical coverage while ignoring other qualitative aspects. In this paper, we focus on the Arabic language and introduce a major revision of the Arabic WordNet that addresses multiple dimensions of lexico-semantic resource quality. As a result, we updated more than 58% of the synsets of the existing Arabic WordNet by adding missing information and correcting errors. In order to address issues of language diversity and untranslatability, we also extended t
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#19981;&#21516;&#35805;&#35821;&#26631;&#27880;&#26694;&#26550;&#38388;&#30340;&#35805;&#35821;&#20851;&#31995;&#28165;&#21333;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#21322;&#33258;&#21160;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.20196</link><description>&lt;p&gt;
&#19981;&#21516;&#35805;&#35821;&#26631;&#27880;&#26694;&#26550;&#30340;&#35805;&#35821;&#20851;&#31995;&#33258;&#21160;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Automatic Alignment of Discourse Relations of Different Discourse Annotation Frameworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20196
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#19981;&#21516;&#35805;&#35821;&#26631;&#27880;&#26694;&#26550;&#38388;&#30340;&#35805;&#35821;&#20851;&#31995;&#28165;&#21333;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#21322;&#33258;&#21160;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35805;&#35821;&#35821;&#26009;&#24211;&#22522;&#20110;&#19981;&#21516;&#30340;&#26694;&#26550;&#36827;&#34892;&#26631;&#27880;&#65292;&#22312;&#21442;&#25968;&#21644;&#20851;&#31995;&#30340;&#23450;&#20041;&#20197;&#21450;&#32467;&#26500;&#32422;&#26463;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#23613;&#31649;&#34920;&#38754;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#20123;&#26694;&#26550;&#20998;&#20139;&#35805;&#35821;&#20851;&#31995;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#36825;&#20123;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#19968;&#30452;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#19981;&#21516;&#26694;&#26550;&#20013;&#20351;&#29992;&#30340;&#20851;&#31995;&#28165;&#21333;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20010;&#38382;&#39064;&#26377;&#21161;&#20110;&#25972;&#21512;&#35805;&#35821;&#29702;&#35770;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#26694;&#26550;&#19979;&#26631;&#27880;&#30340;&#35805;&#35821;&#35821;&#26009;&#24211;&#30340;&#20114;&#25805;&#20316;&#24615;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#25506;&#32034;&#35805;&#35821;&#20851;&#31995;&#28165;&#21333;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21463;&#21040;&#20102;&#35805;&#35821;&#21010;&#20998;&#26631;&#20934;&#30340;&#19981;&#21516;&#38480;&#21046;&#65292;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#30693;&#35782;&#21644;&#25163;&#21160;&#26816;&#26597;&#12290;&#19968;&#20123;&#21322;&#33258;&#21160;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#21516;&#26102;&#22312;&#22810;&#20010;&#26694;&#26550;&#20013;&#36827;&#34892;&#26631;&#27880;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20196v1 Announce Type: new  Abstract: Existing discourse corpora are annotated based on different frameworks, which show significant dissimilarities in definitions of arguments and relations and structural constraints. Despite surface differences, these frameworks share basic understandings of discourse relations. The relationship between these frameworks has been an open research question, especially the correlation between relation inventories utilized in different frameworks. Better understanding of this question is helpful for integrating discourse theories and enabling interoperability of discourse corpora annotated under different frameworks. However, studies that explore correlations between discourse relation inventories are hindered by different criteria of discourse segmentation, and expert knowledge and manual examination are typically needed. Some semi-automatic methods have been proposed, but they rely on corpora annotated in multiple frameworks in parallel. In 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#31995;&#32479;&#22312;&#25972;&#20307;&#38899;&#39057;&#32423;&#21035;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#21033;&#29992;ASR&#39537;&#21160;&#30340;Wav2Vec2&#26550;&#26500;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#22312;&#35821;&#38899;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.20184</link><description>&lt;p&gt;
&#21033;&#29992;ASR&#39537;&#21160;&#30340;Wav2Vec2&#22312;&#25968;&#25454;&#31232;&#32570;&#29615;&#22659;&#20013;&#25506;&#31350;&#30149;&#24577;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Exploring Pathological Speech Quality Assessment with ASR-Powered Wav2Vec2 in Data-Scarce Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#31995;&#32479;&#22312;&#25972;&#20307;&#38899;&#39057;&#32423;&#21035;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#21033;&#29992;ASR&#39537;&#21160;&#30340;Wav2Vec2&#26550;&#26500;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#22312;&#35821;&#38899;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;&#20316;&#20026;&#20256;&#32479;&#24863;&#30693;&#20020;&#24202;&#35780;&#20272;&#30340;&#26367;&#20195;&#25110;&#25903;&#25345;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#22823;&#37096;&#20998;&#30740;&#31350;&#20165;&#22312;&#31616;&#21333;&#20219;&#21153;&#65288;&#22914;&#20108;&#20803;&#20998;&#31867;&#65289;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#25968;&#25454;&#31232;&#32570;&#36896;&#25104;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20542;&#21521;&#20110;&#23558;&#24739;&#32773;&#30340;&#38899;&#39057;&#25991;&#20214;&#20998;&#21106;&#25104;&#22810;&#20010;&#26679;&#26412;&#20197;&#22686;&#21152;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#23558;&#25972;&#20307;&#38899;&#39057;&#24471;&#20998;&#38388;&#25509;&#22320;&#19982;&#20010;&#21035;&#27573;&#30456;&#20851;&#32852;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#31995;&#32479;&#22312;&#25972;&#20010;&#38899;&#39057;&#32423;&#21035;&#23398;&#20064;&#65292;&#32780;&#19981;&#26159;&#22312;&#29255;&#27573;&#32423;&#21035;&#23398;&#20064;&#65292;&#23613;&#31649;&#23384;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#12290;&#26412;&#25991;&#24314;&#35758;&#22312;&#35821;&#38899;&#35780;&#20272;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;Wav2Vec2&#26550;&#26500;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;ASR&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#22312;HNC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;ASR&#39537;&#21160;&#26041;&#27861;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#24314;&#31435;&#20102;&#26032;&#30340;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#24179;&#22343;$MSE=0.73$&#21644;$MSE=1.15$&#26469;&#39044;&#27979;&#26234;&#21830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20184v1 Announce Type: cross  Abstract: Automatic speech quality assessment has raised more attention as an alternative or support to traditional perceptual clinical evaluation. However, most research so far only gains good results on simple tasks such as binary classification, largely due to data scarcity. To deal with this challenge, current works tend to segment patients' audio files into many samples to augment the datasets. Nevertheless, this approach has limitations, as it indirectly relates overall audio scores to individual segments. This paper introduces a novel approach where the system learns at the audio level instead of segments despite data scarcity. This paper proposes to use the pre-trained Wav2Vec2 architecture for both SSL, and ASR as feature extractor in speech assessment. Carried out on the HNC dataset, our ASR-driven approach established a new baseline compared with other approaches, obtaining average $MSE=0.73$ and $MSE=1.15$ for the prediction of intel
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#22312;&#35780;&#20272;&#27721;&#35821;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#22871;&#20214;TMLU&#65292;&#35206;&#30422;37&#20010;&#31185;&#30446;&#65292;&#36890;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#23569;&#26679;&#26412;&#35299;&#37322;&#20419;&#36827;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#24182;&#23545;24&#20010;&#39640;&#32423;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#20013;&#22269;&#24320;&#25918;&#26435;&#37325;&#27169;&#22411;&#34920;&#29616;&#36739;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.20180</link><description>&lt;p&gt;
&#27979;&#37327;&#21488;&#28286;&#26222;&#36890;&#35805;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Measuring Taiwanese Mandarin Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20180
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#22312;&#35780;&#20272;&#27721;&#35821;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#22871;&#20214;TMLU&#65292;&#35206;&#30422;37&#20010;&#31185;&#30446;&#65292;&#36890;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#23569;&#26679;&#26412;&#35299;&#37322;&#20419;&#36827;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#24182;&#23545;24&#20010;&#39640;&#32423;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#20013;&#22269;&#24320;&#25918;&#26435;&#37325;&#27169;&#22411;&#34920;&#29616;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#24341;&#36215;&#20102;&#39046;&#22495;&#20869;&#30340;&#37325;&#35270;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#22312;&#27721;&#35821;&#29615;&#22659;&#19979;&#35780;&#20272;LLMs&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#38024;&#23545;&#20256;&#32479;&#20013;&#25991;&#65292;&#22312;&#29616;&#26377;&#22522;&#20934;&#20013;&#19968;&#30452;&#23384;&#22312;&#30528;&#36739;&#22823;&#30340;&#27424;&#34920;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TMLU&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#35780;&#20272;LLMs&#30340;&#39640;&#32423;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#37327;&#36523;&#23450;&#21046;&#30340;&#32508;&#21512;&#35780;&#20272;&#22871;&#20214;&#65292;&#36866;&#29992;&#20110;&#21488;&#28286;&#26222;&#36890;&#35805;&#29615;&#22659;&#12290;TMLU&#30001;37&#20010;&#31185;&#30446;&#32452;&#25104;&#65292;&#28085;&#30422;&#31038;&#20250;&#31185;&#23398;&#12289;STEM&#12289;&#20154;&#25991;&#23398;&#31185;&#12289;&#21488;&#28286;&#29305;&#23450;&#20869;&#23481;&#31561;&#65292;&#28041;&#21450;&#21021;&#20013;&#21040;&#19987;&#19994;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#31185;&#30446;&#31934;&#24515;&#31574;&#21010;&#20102;&#19968;&#20123;&#31867;&#20284;&#20110;&#24605;&#32500;&#38142;&#30340;&#23569;&#26679;&#26412;&#35299;&#37322;&#65292;&#20197;&#20419;&#36827;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#20026;&#24314;&#31435;&#20840;&#38754;&#30340;&#22522;&#20934;&#32447;&#65292;&#25105;&#20204;&#23545;24&#20010;&#39640;&#32423;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#22810;&#39033;&#24335;&#24320;&#37325;&#37327;&#27169;&#22411;&#65292;&#20013;&#22269;&#24320;&#25918;&#26435;&#37325;&#27169;&#22411;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20180v1 Announce Type: new  Abstract: The evaluation of large language models (LLMs) has drawn substantial attention in the field recently. This work focuses on evaluating LLMs in a Chinese context, specifically, for Traditional Chinese which has been largely underrepresented in existing benchmarks. We present TMLU, a holistic evaluation suit tailored for assessing the advanced knowledge and reasoning capability in LLMs, under the context of Taiwanese Mandarin. TMLU consists of an array of 37 subjects across social science, STEM, humanities, Taiwan-specific content, and others, ranging from middle school to professional levels. In addition, we curate chain-of-thought-like few-shot explanations for each subject to facilitate the evaluation of complex reasoning skills. To establish a comprehensive baseline, we conduct extensive experiments and analysis on 24 advanced LLMs. The results suggest that Chinese open-weight models demonstrate inferior performance comparing to multili
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#19982;Fine-tuned&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#23186;&#20307;&#20559;&#35265;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT&#22312;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#21644;&#25991;&#26412;&#32423;&#19978;&#19979;&#25991;&#20559;&#35265;&#26041;&#38754;&#34920;&#29616;&#19968;&#33268;&#65292;&#20294;&#22312;&#26816;&#27979;&#34394;&#20551;&#26032;&#38395;&#12289;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#35748;&#30693;&#20559;&#35265;&#26041;&#38754;&#21017;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.20158</link><description>&lt;p&gt;
ChatGPT&#19982;&#23186;&#20307;&#20559;&#35265;&#65306;GPT-3.5&#21644;Fine-tuned&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20158
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#19982;Fine-tuned&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#23186;&#20307;&#20559;&#35265;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT&#22312;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#21644;&#25991;&#26412;&#32423;&#19978;&#19979;&#25991;&#20559;&#35265;&#26041;&#38754;&#34920;&#29616;&#19968;&#33268;&#65292;&#20294;&#22312;&#26816;&#27979;&#34394;&#20551;&#26032;&#38395;&#12289;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#35748;&#30693;&#20559;&#35265;&#26041;&#38754;&#21017;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#24555;&#36895;&#21457;&#23637;&#30340;&#25968;&#23383;&#39046;&#22495;&#20013;&#65292;&#36776;&#21035;&#23186;&#20307;&#20559;&#35265;&#30340;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22609;&#36896;&#20844;&#20247;&#24773;&#32490;&#24182;&#24433;&#21709;&#20851;&#38190;&#20915;&#31574;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#23454;&#29992;&#24615;&#65292;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#22312;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#20013;&#26377;&#25928;&#24615;&#30340;&#25506;&#31350;&#12290;ChatGPT&#33021;&#21542;&#26816;&#27979;&#23186;&#20307;&#20559;&#35265;&#65311;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#23186;&#20307;&#20559;&#35265;&#35782;&#21035;&#22522;&#20934;&#65288;MBIB&#65289;&#26469;&#35780;&#20272;ChatGPT&#22312;&#21306;&#20998;&#20845;&#31181;&#23186;&#20307;&#20559;&#35265;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#19982;Fine-tuned&#27169;&#22411;&#65288;&#22914;BART&#12289;ConvBERT&#21644;GPT-2&#65289;&#36827;&#34892;&#23545;&#27604;&#12290;&#30740;&#31350;&#32467;&#26524;&#21576;&#29616;&#20102;&#19968;&#31181;&#20108;&#20998;&#27861;&#65306;ChatGPT&#22312;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#21644;&#25991;&#26412;&#32423;&#19978;&#19979;&#25991;&#20559;&#35265;&#26041;&#38754;&#19982;Fine-tuned&#27169;&#22411;&#34920;&#29616;&#19968;&#33268;&#65292;&#20294;&#22312;&#20854;&#20182;&#20559;&#35265;&#26816;&#27979;&#30340;&#26356;&#24494;&#22937;&#35201;&#32032;&#19978;&#65292;&#21363;&#34394;&#20551;&#26032;&#38395;&#12289;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#35748;&#30693;&#20559;&#35265;&#26041;&#38754;&#21017;&#38754;&#20020;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20158v1 Announce Type: cross  Abstract: In our rapidly evolving digital sphere, the ability to discern media bias becomes crucial as it can shape public sentiment and influence pivotal decisions. The advent of large language models (LLMs), such as ChatGPT, noted for their broad utility in various natural language processing (NLP) tasks, invites exploration of their efficacy in media bias detection. Can ChatGPT detect media bias? This study seeks to answer this question by leveraging the Media Bias Identification Benchmark (MBIB) to assess ChatGPT's competency in distinguishing six categories of media bias, juxtaposed against fine-tuned models such as BART, ConvBERT, and GPT-2. The findings present a dichotomy: ChatGPT performs at par with fine-tuned models in detecting hate speech and text-level context bias, yet faces difficulties with subtler elements of other bias detections, namely, fake news, racial, gender, and cognitive biases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#31181;&#32763;&#35793;&#20013;&#23376;&#35789;&#20998;&#21106;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#23376;&#35789;&#27491;&#21017;&#21270;&#25552;&#39640;&#20102;&#22810;&#35821;&#31181;&#24314;&#27169;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#32780;BPE&#21017;&#26356;&#26377;&#25928;&#22320;&#20419;&#36827;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.20157</link><description>&lt;p&gt;
&#22810;&#35821;&#31181;&#32763;&#35793;&#20013;&#30340;&#23376;&#35789;&#21644;&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#31995;&#32479;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Systematic Analysis of Subwords and Cross-Lingual Transfer in Multilingual Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#31181;&#32763;&#35793;&#20013;&#23376;&#35789;&#20998;&#21106;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#23376;&#35789;&#27491;&#21017;&#21270;&#25552;&#39640;&#20102;&#22810;&#35821;&#31181;&#24314;&#27169;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#32780;BPE&#21017;&#26356;&#26377;&#25928;&#22320;&#20419;&#36827;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#31181;&#24314;&#27169;&#21487;&#20197;&#36890;&#36807;&#20849;&#20139;&#23376;&#35789;&#34920;&#31034;&#37096;&#20998;&#25913;&#21892;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26426;&#22120;&#32763;&#35793;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23376;&#35789;&#20998;&#21106;&#22312;&#36328;&#35821;&#35328;&#36801;&#31227;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#31995;&#32479;&#27604;&#36739;&#20102;&#20960;&#31181;&#23376;&#35789;&#26041;&#27861;&#22312;&#19981;&#21516;&#35821;&#35328;&#31867;&#22411;&#20043;&#38388;&#20419;&#36827;&#21327;&#21516;&#20316;&#29992;&#21644;&#38450;&#27490;&#24178;&#25200;&#30340;&#21151;&#25928;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23376;&#35789;&#27491;&#21017;&#21270;&#25552;&#39640;&#20102;&#22810;&#35821;&#31181;&#24314;&#27169;&#20013;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#32780;BPE&#22312;&#36328;&#35821;&#35328;&#24494;&#35843;&#20013;&#26356;&#26377;&#25928;&#22320;&#20419;&#36827;&#20102;&#36801;&#31227;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27491;&#23383;&#27861;&#21333;&#35789;&#36793;&#30028;&#32422;&#23450;&#30340;&#24046;&#24322;&#65288;&#20070;&#38754;&#35789;&#27719;&#24418;&#24577;&#30340;&#31890;&#24230;&#65289;&#21487;&#33021;&#27604;&#35821;&#35328;&#26080;&#20851;&#24615;&#26356;&#26174;&#33879;&#22320;&#38459;&#30861;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#23454;&#65292;&#22260;&#32469;&#23376;&#35789;&#24314;&#27169;&#30340;&#20915;&#31574;&#21487;&#33021;&#26159;&#20248;&#21270;&#22810;&#35821;&#31181;&#24314;&#27169;&#25928;&#30410;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20157v1 Announce Type: new  Abstract: Multilingual modelling can improve machine translation for low-resource languages, partly through shared subword representations. This paper studies the role of subword segmentation in cross-lingual transfer. We systematically compare the efficacy of several subword methods in promoting synergy and preventing interference across different linguistic typologies. Our findings show that subword regularisation boosts synergy in multilingual modelling, whereas BPE more effectively facilitates transfer during cross-lingual fine-tuning. Notably, our results suggest that differences in orthographic word boundary conventions (the morphological granularity of written words) may impede cross-lingual transfer more significantly than linguistic unrelatedness. Our study confirms that decisions around subword modelling can be key to optimising the benefits of multilingual modelling.
&lt;/p&gt;</description></item><item><title>IndiBias&#26159;&#19968;&#20010;&#20026;&#35780;&#20272;&#21360;&#24230;&#35821;&#22659;&#20013;&#31038;&#20250;&#20559;&#35265;&#32780;&#35774;&#35745;&#30340;&#32508;&#21512;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#36807;&#28388;&#21644;&#32763;&#35793;&#29616;&#26377;&#25968;&#25454;&#38598;&#20197;&#21450;&#21033;&#29992;&#19981;&#21516;LLMs&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#21360;&#24230;&#20013;&#27969;&#34892;&#30340;&#21508;&#31181;&#31038;&#20250;&#20559;&#35265;&#32500;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.20147</link><description>&lt;p&gt;
IndiBias&#65306;&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;&#21360;&#24230;&#35821;&#22659;&#19979;&#35821;&#35328;&#27169;&#22411;&#31038;&#20250;&#20559;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20147
&lt;/p&gt;
&lt;p&gt;
IndiBias&#26159;&#19968;&#20010;&#20026;&#35780;&#20272;&#21360;&#24230;&#35821;&#22659;&#20013;&#31038;&#20250;&#20559;&#35265;&#32780;&#35774;&#35745;&#30340;&#32508;&#21512;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#36807;&#28388;&#21644;&#32763;&#35793;&#29616;&#26377;&#25968;&#25454;&#38598;&#20197;&#21450;&#21033;&#29992;&#19981;&#21516;LLMs&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#21360;&#24230;&#20013;&#27969;&#34892;&#30340;&#21508;&#31181;&#31038;&#20250;&#20559;&#35265;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#25968;&#25454;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#24433;&#21709;&#24341;&#21457;&#20102;&#23545;&#25429;&#25417;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#36825;&#20123;&#20559;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#21644;&#35199;&#26041;&#32972;&#26223;&#65292;&#32570;&#20047;&#19968;&#20010;&#21487;&#38752;&#30340;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#20307;&#29616;&#21360;&#24230;&#29420;&#29305;&#30340;&#31038;&#20250;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;IndiBias&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#21360;&#24230;&#35821;&#22659;&#20013;&#31038;&#20250;&#20559;&#35265;&#30340;&#20840;&#38754;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36807;&#28388;&#21644;&#32763;&#35793;&#29616;&#26377;&#30340;CrowS-Pairs&#25968;&#25454;&#38598;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#36866;&#21512;&#21360;&#24230;&#35821;&#22659;&#20013;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#21360;&#22320;&#35821;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#21253;&#25324;ChatGPT&#21644;InstructGPT&#22312;&#20869;&#30340;LLMs&#65292;&#20197;&#21360;&#24230;&#27969;&#34892;&#30340;&#21508;&#31181;&#31038;&#20250;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#22686;&#24378;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#21253;&#21547;&#30340;&#20559;&#35265;&#32500;&#24230;&#28085;&#30422;&#24615;&#21035;&#12289;&#23447;&#25945;&#12289;&#31181;&#22995;&#12289;&#24180;&#40836;&#12289;&#22320;&#21306;&#12289;&#22806;&#35980;&#21644;&#32844;&#19994;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#36164;&#28304;&#26469;&#35299;&#20915;&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20147v1 Announce Type: new  Abstract: The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs). Existing efforts predominantly focus on English language and the Western context, leaving a void for a reliable dataset that encapsulates India's unique socio-cultural nuances. To bridge this gap, we introduce IndiBias, a comprehensive benchmarking dataset designed specifically for evaluating social biases in the Indian context. We filter and translate the existing CrowS-Pairs dataset to create a benchmark dataset suited to the Indian context in Hindi language. Additionally, we leverage LLMs including ChatGPT and InstructGPT to augment our dataset with diverse societal biases and stereotypes prevalent in India. The included bias dimensions encompass gender, religion, caste, age, region, physical appearance, and occupation. We also build a resource to address intersec
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#30340;&#24494;&#35843;&#21644;&#26410;&#24494;&#35843;&#65292;&#21457;&#29616;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#32988;&#36807;&#29616;&#26377;&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.20145</link><description>&lt;p&gt;
&#20026;&#33258;&#21160;&#35786;&#26029;&#31579;&#26597;&#24635;&#32467;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models for Automated Diagnostic Screening Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20145
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#30340;&#24494;&#35843;&#21644;&#26410;&#24494;&#35843;&#65292;&#21457;&#29616;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#32988;&#36807;&#29616;&#26377;&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21457;&#23637;&#20013;&#22269;&#23478;&#25913;&#21892;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#38656;&#27714;&#12290;&#19968;&#31181;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24320;&#21457;&#21487;&#25193;&#23637;&#30340;&#33258;&#21160;&#31995;&#32479;&#36827;&#34892;&#35786;&#26029;&#31579;&#26597;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#36731;&#24515;&#29702;&#20581;&#24247;&#19987;&#19994;&#20154;&#21592;&#30340;&#36127;&#25285;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#21644;&#26410;&#24494;&#35843;&#65292;&#29992;&#20110;&#20174;&#24515;&#29702;&#29366;&#24577;&#26816;&#26597;&#20013;&#29983;&#25104;&#31616;&#26126;&#25688;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;ROUGE&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#32773;&#30340;&#36755;&#20837;&#65292;&#23545;&#22235;&#31181;&#19981;&#21516;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#32988;&#36807;&#29616;&#26377;&#27169;&#22411;&#65292;&#20998;&#21035;&#23454;&#29616;&#20102;0.810&#21644;0.764&#30340;ROUGE-1&#21644;ROUGE-L&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#24494;&#35843;&#27169;&#22411;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;D4&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65292;&#34920;&#26126;&#20854;&#28508;&#22312;&#36866;&#29992;&#24615;&#36229;&#20986;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20145v1 Announce Type: new  Abstract: Improving mental health support in developing countries is a pressing need. One potential solution is the development of scalable, automated systems to conduct diagnostic screenings, which could help alleviate the burden on mental health professionals. In this work, we evaluate several state-of-the-art Large Language Models (LLMs), with and without fine-tuning, on our custom dataset for generating concise summaries from mental state examinations. We rigorously evaluate four different models for summary generation using established ROUGE metrics and input from human evaluators. The results highlight that our top-performing fine-tuned model outperforms existing models, achieving ROUGE-1 and ROUGE-L values of 0.810 and 0.764, respectively. Furthermore, we assessed the fine-tuned model's generalizability on a publicly available D4 dataset, and the outcomes were promising, indicating its potential applicability beyond our custom dataset.
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#29992;&#25143;&#22312;&#20219;&#21153;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#35835;&#29992;&#25143;&#36164;&#26009;&#20197;&#23454;&#29616;&#26356;&#20010;&#24615;&#21270;&#29992;&#25143;&#25351;&#23548;&#30340;&#33021;&#21147;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.20134</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;AI&#21161;&#25163;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#24314;&#27169;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
User Modeling Challenges in Interactive AI Assistant Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20134
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#29992;&#25143;&#22312;&#20219;&#21153;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#35835;&#29992;&#25143;&#36164;&#26009;&#20197;&#23454;&#29616;&#26356;&#20010;&#24615;&#21270;&#29992;&#25143;&#25351;&#23548;&#30340;&#33021;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21161;&#25163;&#31995;&#32479;&#26088;&#22312;&#25552;&#20379;&#21450;&#26102;&#30340;&#25351;&#23548;&#65292;&#24110;&#21161;&#29992;&#25143;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;&#20854;&#20013;&#20173;&#28982;&#23384;&#22312;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#22312;&#20219;&#21153;&#25191;&#34892;&#36807;&#31243;&#20013;&#20102;&#35299;&#29992;&#25143;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#26356;&#21152;&#20010;&#24615;&#21270;&#30340;&#25351;&#23548;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#29992;&#25143;&#22312;&#20219;&#21153;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#24182;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#35835;&#29992;&#25143;&#36164;&#26009;&#20197;&#23454;&#29616;&#26356;&#20010;&#24615;&#21270;&#29992;&#25143;&#25351;&#23548;&#30340;&#33021;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20134v1 Announce Type: new  Abstract: Interactive Artificial Intelligent(AI) assistant systems are designed to offer timely guidance to help human users to complete a variety tasks. One of the remaining challenges is to understand user's mental states during the task for more personalized guidance. In this work, we analyze users' mental states during task executions and investigate the capabilities and challenges for large language models to interpret user profiles for more personalized user guidance.
&lt;/p&gt;</description></item><item><title>&#21453;&#35328;&#35770;&#20316;&#20026;&#19968;&#31181;&#38750;&#21319;&#32423;&#22238;&#24212;&#31574;&#30053;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22312;&#32447;&#21644;&#31163;&#32447;&#26292;&#21147;&#65292;&#26368;&#36817;NLP&#31038;&#21306;&#23545;&#20110;&#20998;&#26512;&#12289;&#25910;&#38598;&#12289;&#20998;&#31867;&#21644;&#33258;&#21160;&#29983;&#25104;&#21453;&#35328;&#35770;&#30340;&#25361;&#25112;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;</title><link>https://arxiv.org/abs/2403.20103</link><description>&lt;p&gt;
NLP&#29992;&#20110;&#23545;&#25239;&#20167;&#24680;&#30340;&#35328;&#35770;&#65306;&#35843;&#26597;&#19982;&#23454;&#29992;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
NLP for Counterspeech against Hate: A Survey and How-To Guide
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20103
&lt;/p&gt;
&lt;p&gt;
&#21453;&#35328;&#35770;&#20316;&#20026;&#19968;&#31181;&#38750;&#21319;&#32423;&#22238;&#24212;&#31574;&#30053;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22312;&#32447;&#21644;&#31163;&#32447;&#26292;&#21147;&#65292;&#26368;&#36817;NLP&#31038;&#21306;&#23545;&#20110;&#20998;&#26512;&#12289;&#25910;&#38598;&#12289;&#20998;&#31867;&#21644;&#33258;&#21160;&#29983;&#25104;&#21453;&#35328;&#35770;&#30340;&#25361;&#25112;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21453;&#35328;&#35770;&#24050;&#32463;&#25104;&#20026;&#25171;&#20987;&#32593;&#32476;&#20167;&#24680;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#20043;&#19968;&#12290;&#36825;&#20123;&#38750;&#21319;&#32423;&#22238;&#24212;&#22788;&#29702;&#22312;&#32447;&#34384;&#24453;&#34892;&#20026;&#30340;&#21516;&#26102;&#20445;&#30041;&#29992;&#25143;&#30340;&#35328;&#35770;&#33258;&#30001;&#65292;&#24182;&#19988;&#22312;&#20943;&#23569;&#22312;&#32447;&#21644;&#31163;&#32447;&#26292;&#21147;&#26041;&#38754;&#21487;&#20197;&#20135;&#29983;&#20999;&#23454;&#24433;&#21709;&#12290;&#26368;&#36817;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#23545;&#20998;&#26512;&#12289;&#25910;&#38598;&#12289;&#20998;&#31867;&#21644;&#33258;&#21160;&#29983;&#25104;&#21453;&#35328;&#35770;&#30340;&#25361;&#25112;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#27987;&#21402;&#30340;&#20852;&#36259;&#65292;&#20197;&#20943;&#36731;&#25163;&#21160;&#29983;&#20135;&#30340;&#24040;&#22823;&#36127;&#25285;&#12290;&#29305;&#21035;&#26159;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#37319;&#21462;&#20102;&#19981;&#21516;&#30340;&#26041;&#21521;&#65292;&#22240;&#27492;&#25552;&#20379;&#20102;&#21508;&#31181;&#30456;&#20851;&#20219;&#21153;&#21644;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#31034;&#20363;&#25551;&#36848;&#20102;&#30740;&#31350;&#21453;&#35328;&#35770;&#30340;&#27493;&#39588;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#20197;&#20174;&#36825;&#20010;&#20027;&#39064;&#30340;NLP&#30740;&#31350;&#20013;&#23398;&#21040;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#20026;&#30740;&#31350;&#21453;&#35328;&#35770;&#25552;&#20379;&#19968;&#20010;&#25351;&#21335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23384;&#22312;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20103v1 Announce Type: new  Abstract: In recent years, counterspeech has emerged as one of the most promising strategies to fight online hate. These non-escalatory responses tackle online abuse while preserving the freedom of speech of the users, and can have a tangible impact in reducing online and offline violence. Recently, there has been growing interest from the Natural Language Processing (NLP) community in addressing the challenges of analysing, collecting, classifying, and automatically generating counterspeech, to reduce the huge burden of manually producing it. In particular, researchers have taken different directions in addressing these challenges, thus providing a variety of related tasks and resources. In this paper, we provide a guide for doing research on counterspeech, by describing - with detailed examples - the steps to undertake, and providing best practices that can be learnt from the NLP studies on this topic. Finally, we discuss open challenges and fut
&lt;/p&gt;</description></item><item><title>RealKIE&#25552;&#20379;&#20102;&#20116;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20225;&#19994;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#20026;&#25237;&#36164;&#20998;&#26512;&#21644;&#27861;&#24459;&#25968;&#25454;&#22788;&#29702;&#31561;&#20219;&#21153;&#25552;&#20379;&#20102;&#29616;&#23454;&#30340;&#27979;&#35797;&#22522;&#22320;&#65292;&#24182;&#20026;NLP&#27169;&#22411;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.20101</link><description>&lt;p&gt;
RealKIE: &#20116;&#20010;&#26032;&#39062;&#30340;&#20225;&#19994;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RealKIE: Five Novel Datasets for Enterprise Key Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20101
&lt;/p&gt;
&lt;p&gt;
RealKIE&#25552;&#20379;&#20102;&#20116;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20225;&#19994;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#20026;&#25237;&#36164;&#20998;&#26512;&#21644;&#27861;&#24459;&#25968;&#25454;&#22788;&#29702;&#31561;&#20219;&#21153;&#25552;&#20379;&#20102;&#29616;&#23454;&#30340;&#27979;&#35797;&#22522;&#22320;&#65292;&#24182;&#20026;NLP&#27169;&#22411;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;RealKIE&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25512;&#21160;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#26041;&#27861;&#21457;&#23637;&#30340;&#20116;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#22522;&#20934;&#65292;&#37325;&#28857;&#26159;&#20225;&#19994;&#24212;&#29992;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#25324;&#32654;&#22269;SEC S1&#25991;&#20214;&#12289;&#32654;&#22269;&#20445;&#23494;&#21327;&#35758;&#12289;&#33521;&#22269;&#24904;&#21892;&#25253;&#21578;&#12289;FCC&#21457;&#31080;&#21644;&#36164;&#28304;&#21512;&#21516;&#31561;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26723;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#25991;&#26412;&#24207;&#21015;&#21270;&#19981;&#20339;&#12289;&#38271;&#25991;&#26723;&#20013;&#31232;&#30095;&#30340;&#27880;&#37322;&#21644;&#22797;&#26434;&#30340;&#34920;&#26684;&#24067;&#23616;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#20026;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65288;&#22914;&#25237;&#36164;&#20998;&#26512;&#21644;&#27861;&#24459;&#25968;&#25454;&#22788;&#29702;&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#29616;&#23454;&#30340;&#27979;&#35797;&#22522;&#22320;&#12290;&#38500;&#20102;&#20171;&#32461;&#36825;&#20123;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#27880;&#37322;&#36807;&#31243;&#12289;&#25991;&#26723;&#22788;&#29702;&#25216;&#26415;&#21644;&#22522;&#32447;&#24314;&#27169;&#26041;&#27861;&#30340;&#28145;&#20837;&#25551;&#36848;&#12290;&#36825;&#19968;&#36129;&#29486;&#20419;&#36827;&#20102;&#33021;&#22815;&#22788;&#29702;&#23454;&#38469;&#25361;&#25112;&#30340;NLP&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#24182;&#25903;&#25345;&#36827;&#19968;&#27493;&#30740;&#31350;&#21487;&#24212;&#29992;&#20110;&#24037;&#19994;&#30340;&#20449;&#24687;&#25552;&#21462;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20101v1 Announce Type: new  Abstract: We introduce RealKIE, a benchmark of five challenging datasets aimed at advancing key information extraction methods, with an emphasis on enterprise applications. The datasets include a diverse range of documents including SEC S1 Filings, US Non-disclosure Agreements, UK Charity Reports, FCC Invoices, and Resource Contracts. Each presents unique challenges: poor text serialization, sparse annotations in long documents, and complex tabular layouts. These datasets provide a realistic testing ground for key information extraction tasks like investment analysis and legal data processing.   In addition to presenting these datasets, we offer an in-depth description of the annotation process, document processing techniques, and baseline modeling approaches. This contribution facilitates the development of NLP models capable of handling practical challenges and supports further research into information extraction technologies applicable to indu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#26469;&#30740;&#31350;&#38646;&#32763;&#35793;&#35821;&#35328;&#27169;&#22411;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#30340;&#36801;&#31227;&#35821;&#35328;&#24433;&#21709;&#65292;&#21457;&#29616;&#19968;&#20123;&#35821;&#35328;&#23545;&#20854;&#20182;&#35821;&#35328;&#24433;&#21709;&#19981;&#22823;&#65292;&#32780;&#19968;&#20123;&#35821;&#35328;&#23545;&#19981;&#21516;&#30446;&#26631;&#35821;&#35328;&#21487;&#33021;&#26497;&#20026;&#26377;&#21033;&#25110;&#26377;&#23475;&#65292;&#21516;&#26102;&#20063;&#35266;&#23519;&#21040;&#20197;&#21069;&#26410;&#34987;MLMs&#30475;&#21040;&#30340;&#35821;&#35328;&#22987;&#32456;&#21463;&#30410;&#20110;&#26469;&#33258;&#20960;&#20046;&#25152;&#26377;&#36801;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.20088</link><description>&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30740;&#31350;&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Approach for Studying Cross-Lingual Transfer in Multilingual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20088
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#26469;&#30740;&#31350;&#38646;&#32763;&#35793;&#35821;&#35328;&#27169;&#22411;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#30340;&#36801;&#31227;&#35821;&#35328;&#24433;&#21709;&#65292;&#21457;&#29616;&#19968;&#20123;&#35821;&#35328;&#23545;&#20854;&#20182;&#35821;&#35328;&#24433;&#21709;&#19981;&#22823;&#65292;&#32780;&#19968;&#20123;&#35821;&#35328;&#23545;&#19981;&#21516;&#30446;&#26631;&#35821;&#35328;&#21487;&#33021;&#26497;&#20026;&#26377;&#21033;&#25110;&#26377;&#23475;&#65292;&#21516;&#26102;&#20063;&#35266;&#23519;&#21040;&#20197;&#21069;&#26410;&#34987;MLMs&#30475;&#21040;&#30340;&#35821;&#35328;&#22987;&#32456;&#21463;&#30410;&#20110;&#26469;&#33258;&#20960;&#20046;&#25152;&#26377;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20088v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#20247;&#25152;&#21608;&#30693;&#65292;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#22312;&#38646;&#32763;&#35793;&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#23481;&#37327;&#21644;&#25928;&#26524;&#24050;&#32463;&#24471;&#21040;&#30830;&#35748;&#12290;&#28982;&#32780;&#65292;&#27491;&#21521;&#25110;&#36127;&#21521;&#36801;&#31227;&#30340;&#29616;&#35937;&#20197;&#21450;&#35821;&#35328;&#36873;&#25321;&#30340;&#24433;&#21709;&#20173;&#38656;&#35201;&#24471;&#21040;&#20805;&#20998;&#29702;&#35299;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;LMs&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;\textit {&#39640;&#25928;}&#26041;&#27861;&#26469;&#30740;&#31350;&#38646;&#32763;&#35793;&#24615;&#33021;&#19978;&#23545;&#30446;&#26631;&#35821;&#35328;&#30340;&#36801;&#31227;&#35821;&#35328;&#24433;&#21709;&#12290;&#19982;&#20197;&#24448;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19979;&#28216;&#20219;&#21153;&#19982;&#35821;&#35328;&#20998;&#31163;&#65292;&#20351;&#29992;&#19987;&#29992;&#30340;&#36866;&#37197;&#22120;&#21333;&#20803;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#35821;&#35328;&#23545;&#20854;&#20182;&#35821;&#35328;&#24433;&#21709;&#19981;&#22823;&#65292;&#32780;&#19968;&#20123;&#35821;&#35328;&#65292;&#23588;&#20854;&#26159;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#65292;&#23545;&#19981;&#21516;&#30340;&#30446;&#26631;&#35821;&#35328;&#21487;&#33021;&#26497;&#20026;&#26377;&#21033;&#25110;&#26377;&#23475;&#12290;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#36801;&#31227;&#35821;&#35328;&#23545;&#25152;&#26377;&#30446;&#26631;&#35821;&#35328;&#37117;&#26377;&#30410;&#12290;&#25105;&#20204;&#22855;&#24618;&#22320;&#35266;&#23519;&#21040;&#65292;&#27492;&#21069;&#26410;&#34987;MLMs&#30475;&#21040;&#30340;&#35821;&#35328;&#19968;&#30452;&#21463;&#30410;&#20110;&#26469;&#33258;&#20960;&#20046;&#25152;&#26377;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20088v1 Announce Type: new  Abstract: The capacity and effectiveness of pre-trained multilingual models (MLMs) for zero-shot cross-lingual transfer is well established. However, phenomena of positive or negative transfer, and the effect of language choice still need to be fully understood, especially in the complex setting of massively multilingual LMs. We propose an \textit{efficient} method to study transfer language influence in zero-shot performance on another target language. Unlike previous work, our approach disentangles downstream tasks from language, using dedicated adapter units. Our findings suggest that some languages do not largely affect others, while some languages, especially ones unseen during pre-training, can be extremely beneficial or detrimental for different target languages. We find that no transfer language is beneficial for all target languages. We do, curiously, observe languages previously unseen by MLMs consistently benefit from transfer from almo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#23545;&#23391;&#21152;&#25289;&#25991;&#26412;&#30340;&#22269;&#38469;&#38899;&#26631;&#36716;&#20889;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#22522;&#20110;DL&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#26032;&#39062;IPA&#36716;&#20889;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.20084</link><description>&lt;p&gt;
&#23391;&#21152;&#25289;&#25991;&#26412;&#30340;&#22269;&#38469;&#38899;&#26631;&#36716;&#20889;
&lt;/p&gt;
&lt;p&gt;
IPA Transcription of Bengali Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20084
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23545;&#23391;&#21152;&#25289;&#25991;&#26412;&#30340;&#22269;&#38469;&#38899;&#26631;&#36716;&#20889;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#22522;&#20110;DL&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#26032;&#39062;IPA&#36716;&#20889;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#29992;&#20110;&#31995;&#32479;&#21270;&#35821;&#35328;&#20013;&#30340;&#38899;&#32032;&#65292;&#23454;&#29616;&#23545;&#21457;&#38899;&#30340;&#31934;&#30830;&#25991;&#26412;&#34920;&#31034;&#12290;&#22312;&#23391;&#21152;&#25289;&#35821;&#38899;&#38901;&#23398;&#20013;&#65292;&#23398;&#26415;&#30028;&#19968;&#30452;&#22312;&#25506;&#35752;IPA&#26631;&#20934;&#21644;&#26680;&#24515;&#23391;&#21152;&#25289;&#38899;&#32032;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26816;&#35270;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#35782;&#21035;&#20102;&#24403;&#21069;&#21644;&#28508;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23391;&#21152;&#25289;IPA&#26631;&#20934;&#30340;&#26694;&#26550;&#65292;&#20419;&#36827;&#20102;&#35821;&#35328;&#20998;&#26512;&#21644;NLP&#36164;&#28304;&#21019;&#24314;&#20197;&#21450;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20084v1 Announce Type: new  Abstract: The International Phonetic Alphabet (IPA) serves to systematize phonemes in language, enabling precise textual representation of pronunciation. In Bengali phonology and phonetics, ongoing scholarly deliberations persist concerning the IPA standard and core Bengali phonemes. This work examines prior research, identifies current and potential issues, and suggests a framework for a Bengali IPA standard, facilitating linguistic analysis and NLP resource creation and downstream technology development. In this work, we present a comprehensive study of Bengali IPA transcription and introduce a novel IPA transcription framework incorporating a novel dataset with DL-based benchmarks.
&lt;/p&gt;</description></item><item><title>&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#36328;&#35821;&#35328;&#20256;&#36755;&#33021;&#21147;&#65292;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#40065;&#26834;&#24615;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#65292;&#30740;&#31350;&#21457;&#29616;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#36328;&#35821;&#35328;&#20256;&#36755;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23454;&#20307;&#30340;&#37325;&#21472;</title><link>https://arxiv.org/abs/2403.20056</link><description>&lt;p&gt;
&#22312;&#23545;&#25239;&#25968;&#25454;&#38598;&#19978;&#36328;&#35821;&#35328;&#20256;&#36755;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20056
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#36328;&#35821;&#35328;&#20256;&#36755;&#33021;&#21147;&#65292;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#40065;&#26834;&#24615;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#65292;&#30740;&#31350;&#21457;&#29616;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#36328;&#35821;&#35328;&#20256;&#36755;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23454;&#20307;&#30340;&#37325;&#21472;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#36328;&#35821;&#35328;&#20256;&#36755;&#33021;&#21147;&#65292;&#21363;&#21033;&#29992;&#22312;&#28304;&#35821;&#35328;&#20013;&#33719;&#21462;&#30340;&#20449;&#24687;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30446;&#26631;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#22312;&#24050;&#24314;&#31435;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#25214;&#21040;&#23454;&#38469;&#24212;&#29992;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#23558;&#28304;&#35821;&#35328;&#24212;&#29992;&#20110;&#30446;&#26631;&#35821;&#35328;&#26102;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25200;&#21160;&#36755;&#20837;&#27979;&#35797;&#38598;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;13&#23545;&#35821;&#35328;&#65292;&#27599;&#23545;&#35821;&#35328;&#21253;&#25324;&#19968;&#20010;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;HRL&#65289;&#21644;&#19968;&#20010;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;LRL&#65289;&#65292;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#22320;&#29702;&#12289;&#36951;&#20256;&#25110;&#20511;&#29992;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#35821;&#35328;&#23545;&#19978;&#35780;&#20272;&#20102;&#20004;&#20010;&#30693;&#21517;&#30340;MLLMs--MBERT&#21644;XLM-R&#65292;&#22312;&#26412;&#26426;LRL&#21644;&#36328;&#35821;&#35328;&#20256;&#36755;&#35774;&#32622;&#20013;&#65292;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#22312;&#19968;&#32452;&#19981;&#21516;&#30340;&#25200;&#21160;&#19979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;NER&#36328;&#35821;&#35328;&#20256;&#36755;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23454;&#20307;&#30340;&#37325;&#21472;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20056v1 Announce Type: new  Abstract: Multilingual Language Models (MLLMs) exhibit robust cross-lingual transfer capabilities, or the ability to leverage information acquired in a source language and apply it to a target language. These capabilities find practical applications in well-established Natural Language Processing (NLP) tasks such as Named Entity Recognition (NER). This study aims to investigate the effectiveness of a source language when applied to a target language, particularly in the context of perturbing the input test set. We evaluate on 13 pairs of languages, each including one high-resource language (HRL) and one low-resource language (LRL) with a geographic, genetic, or borrowing relationship. We evaluate two well-known MLLMs--MBERT and XLM-R--on these pairs, in native LRL and cross-lingual transfer settings, in two tasks, under a set of different perturbations. Our findings indicate that NER cross-lingual transfer depends largely on the overlap of entity 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;\textsc{CoTErrorSet}&#65292;&#30740;&#31350;&#20102;LLMs&#26159;&#21542;&#33021;&#22815;&#20174;&#20197;&#24448;&#30340;&#38169;&#35823;&#20013;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.20046</link><description>&lt;p&gt;
LLM&#33021;&#20174;&#20197;&#21069;&#30340;&#38169;&#35823;&#20013;&#23398;&#20064;&#21527;&#65311;&#35843;&#26597;LLMs'&#38169;&#35823;&#20197;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20046
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;\textsc{CoTErrorSet}&#65292;&#30740;&#31350;&#20102;LLMs&#26159;&#21542;&#33021;&#22815;&#20174;&#20197;&#24448;&#30340;&#38169;&#35823;&#20013;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#20174;&#24494;&#35843;&#40644;&#37329;&#26631;&#20934;&#30340;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#35299;&#37322;&#25110;&#23558;&#20854;&#29992;&#20316;&#23569;&#37327;&#25552;&#31034;&#20013;&#30340;&#27491;&#30830;&#31034;&#20363;&#20013;&#21463;&#30410;&#12290;&#23613;&#31649;&#20154;&#31867;&#30830;&#23454;&#21487;&#20197;&#27169;&#20223;&#27491;&#30830;&#30340;&#20363;&#23376;&#65292;&#20294;&#20174;&#25105;&#20204;&#30340;&#38169;&#35823;&#20013;&#23398;&#20064;&#26159;&#20154;&#31867;&#35748;&#30693;&#30340;&#21478;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#38382;&#39064;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65306;LLMs&#33021;&#21542;&#23398;&#20064;&#24182;&#21463;&#30410;&#20110;&#20182;&#20204;&#30340;&#38169;&#35823;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20182;&#20204;&#30340;&#25512;&#29702;&#65311;&#26412;&#30740;&#31350;&#20174;&#25552;&#31034;&#21644;&#27169;&#22411;&#35843;&#25972;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;\textsc{CoTErrorSet}&#65292;&#20854;&#20013;&#21253;&#21547;609,432&#20010;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#35774;&#35745;&#26377;&#27491;&#30830;&#21644;&#38169;&#35823;&#30340;&#21442;&#32771;&#25991;&#29486;&#65292;&#24182;&#23637;&#31034;&#20102;&#21046;&#36896;&#36825;&#20123;&#38169;&#35823;&#30340;&#31867;&#22411;&#21644;&#21407;&#22240;&#12290;&#20026;&#20102;&#25506;&#35752;&#36825;&#20123;&#38169;&#35823;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#65288;1&#65289;\textbf{&#33258;&#25105;&#21453;&#24605;}&#25552;&#31034;&#25351;&#23548;LLMs&#37325;&#26032;&#32771;&#34385;&#20182;&#20204;&#26159;&#21542;&#26366;&#32463;&#29359;&#36807;&#31867;&#20284;&#30340;&#38169;&#35823;&#65307;&#21644;&#65288;2&#65289;\textbf{&#38169;&#35823;&#35843;&#25972;}&#21253;&#25324;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20046v1 Announce Type: new  Abstract: Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting. While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: \textit{can LLMs learn and benefit from their mistakes, especially for their reasoning? } This study investigates this problem from both the prompting and model-tuning perspectives. We begin by introducing \textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) \textbf{Self-rethinking} prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) \textbf{Mistake tuning} involves finetuning mo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22235;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#22312;&#25163;&#26426;GPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#22312;&#31227;&#21160;&#25512;&#26029;&#24341;&#25806;Transformer-Lite&#20013;&#65292;&#25552;&#39640;&#20102;&#25512;&#26029;&#36895;&#24230;&#21644;&#38477;&#20302;&#20102;&#25163;&#26426;&#28382;&#21518;&#65292;&#20174;&#32780;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.20041</link><description>&lt;p&gt;
Transformer-Lite: &#22312;&#25163;&#26426;GPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22235;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#22312;&#25163;&#26426;GPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#22312;&#31227;&#21160;&#25512;&#26029;&#24341;&#25806;Transformer-Lite&#20013;&#65292;&#25552;&#39640;&#20102;&#25512;&#26029;&#36895;&#24230;&#21644;&#38477;&#20302;&#20102;&#25163;&#26426;&#28382;&#21518;&#65292;&#20174;&#32780;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26234;&#33021;&#21161;&#25163;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#32763;&#35793;&#21644;&#25163;&#26426;&#19978;&#30340;&#22810;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35774;&#22791;&#19978;LLM&#37096;&#32626;&#26041;&#27861;&#36895;&#24230;&#36739;&#24930;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19981;&#20339;&#12290;&#20026;&#20102;&#23454;&#29616;&#22312;&#35774;&#22791;GPU&#19978;&#39640;&#25928;&#37096;&#32626;LLM&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#20248;&#21270;&#25216;&#26415;&#65306;&#65288;a&#65289;&#22522;&#20110;&#31526;&#21495;&#34920;&#36798;&#30340;&#26041;&#27861;&#25903;&#25345;&#21160;&#24577;&#24418;&#29366;&#27169;&#22411;&#25512;&#26029;&#65307;&#65288;b&#65289;&#25805;&#20316;&#20248;&#21270;&#21644;&#25191;&#34892;&#20248;&#20808;&#32423;&#35774;&#32622;&#20197;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#24182;&#20943;&#23569;&#25163;&#26426;&#28382;&#21518;&#65307;&#65288;c&#65289;&#19968;&#31181;&#21517;&#20026;M0E4&#30340;FP4&#37327;&#21270;&#26041;&#27861;&#20197;&#20943;&#23569;&#21435;&#37327;&#21270;&#24320;&#38144;&#65307;&#65288;d&#65289;&#19968;&#31181;&#22522;&#20110;&#23376;&#24352;&#37327;&#30340;&#25216;&#26415;&#26469;&#22312;LLM&#25512;&#26029;&#21518;&#28040;&#38500;&#22797;&#21046;KV&#32531;&#23384;&#30340;&#38656;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#31227;&#21160;&#25512;&#26029;&#24341;&#25806;Transformer-Lite&#20013;&#23454;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#35813;&#24341;&#25806;&#19982;&#39640;&#36890;&#21644;MTK&#22788;&#29702;&#22120;&#20860;&#23481;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;Transformer-Lite&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20041v1 Announce Type: new  Abstract: The Large Language Model (LLM) is widely employed for tasks such as intelligent assistants, text summarization, translation, and multi-modality on mobile phones. However, the current methods for on-device LLM deployment maintain slow inference speed, which causes poor user experience. To facilitate high-efficiency LLM deployment on device GPUs, we propose four optimization techniques: (a) a symbolic expression-based approach to support dynamic shape model inference; (b) operator optimizations and execution priority setting to enhance inference speed and reduce phone lagging; (c) an FP4 quantization method termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based technique to eliminate the need for copying KV cache after LLM inference. Furthermore, we implement these methods in our mobile inference engine, Transformer-Lite, which is compatible with both Qualcomm and MTK processors. We evaluated Transformer-Lite's performance u
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#20132;&#25442;&#22810;&#27169;&#24577;&#25512;&#29702;&#65288;FSMR&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#29305;&#24449;&#20132;&#25442;&#22686;&#24378;&#22810;&#27169;&#24577;&#25512;&#29702;&#65292;&#21253;&#25324;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#29420;&#29305;&#30340;&#29305;&#24449;&#20132;&#25442;&#27169;&#22359;&#65292;&#20197;&#21450;&#34701;&#21512;&#22810;&#27169;&#24577;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#26469;&#20419;&#36827;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#30340;&#32852;&#21512;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2403.20026</link><description>&lt;p&gt;
FSMR&#65306;&#20855;&#26377;&#32852;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#32447;&#32034;&#30340;&#29305;&#24449;&#20132;&#25442;&#22810;&#27169;&#24577;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FSMR: A Feature Swapping Multi-modal Reasoning Approach with Joint Textual and Visual Clues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20026
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#20132;&#25442;&#22810;&#27169;&#24577;&#25512;&#29702;&#65288;FSMR&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#29305;&#24449;&#20132;&#25442;&#22686;&#24378;&#22810;&#27169;&#24577;&#25512;&#29702;&#65292;&#21253;&#25324;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#29420;&#29305;&#30340;&#29305;&#24449;&#20132;&#25442;&#27169;&#22359;&#65292;&#20197;&#21450;&#34701;&#21512;&#22810;&#27169;&#24577;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#26469;&#20419;&#36827;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#30340;&#32852;&#21512;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25512;&#29702;&#22312;&#24357;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#23545;&#19978;&#19979;&#25991;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#20132;&#25442;&#22810;&#27169;&#24577;&#25512;&#29702;&#65288;FSMR&#65289;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#29305;&#24449;&#20132;&#25442;&#22686;&#24378;&#22810;&#27169;&#24577;&#25512;&#29702;&#12290;FSMR&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#25509;&#32435;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#65292;&#26377;&#25928;&#22320;&#34920;&#24449;&#20004;&#31181;&#27169;&#24577;&#30340;&#29305;&#24449;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#29305;&#24449;&#20132;&#25442;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#20013;&#35782;&#21035;&#30340;&#23545;&#35937;&#21644;&#25991;&#26412;&#20013;&#23545;&#24212;&#35789;&#27719;&#20043;&#38388;&#29305;&#24449;&#30340;&#20132;&#25442;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21152;&#24378;&#20854;&#22810;&#27169;&#24577;&#23545;&#40784;&#33021;&#21147;&#65292;FSMR&#34701;&#21512;&#20102;&#22810;&#27169;&#24577;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#65292;&#20419;&#36827;&#20102;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32852;&#21512;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20026v1 Announce Type: cross  Abstract: Multi-modal reasoning plays a vital role in bridging the gap between textual and visual information, enabling a deeper understanding of the context. This paper presents the Feature Swapping Multi-modal Reasoning (FSMR) model, designed to enhance multi-modal reasoning through feature swapping. FSMR leverages a pre-trained visual-language model as an encoder, accommodating both text and image inputs for effective feature representation from both modalities. It introduces a unique feature swapping module, enabling the exchange of features between identified objects in images and corresponding vocabulary words in text, thereby enhancing the model's comprehension of the interplay between images and text. To further bolster its multi-modal alignment capabilities, FSMR incorporates a multi-modal cross-attention mechanism, facilitating the joint modeling of textual and visual information. During training, we employ image-text matching and cros
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31616;&#21333;&#30340;&#21103;&#35789;&#21024;&#38500;&#31574;&#30053;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22686;&#24378;&#25991;&#26412;&#25968;&#25454;&#65292;&#20445;&#30041;&#35821;&#20041;&#65292;&#36866;&#29992;&#20110;&#21333;&#19968;&#25991;&#26412;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.20015</link><description>&lt;p&gt;
&#21103;&#35789;&#26159;&#20851;&#38190;&#65306;&#20351;&#29992;&#21103;&#35789;&#21024;&#38500;&#36827;&#34892;&#31616;&#21333;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20015
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31616;&#21333;&#30340;&#21103;&#35789;&#21024;&#38500;&#31574;&#30053;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22686;&#24378;&#25991;&#26412;&#25968;&#25454;&#65292;&#20445;&#30041;&#35821;&#20041;&#65292;&#36866;&#29992;&#20110;&#21333;&#19968;&#25991;&#26412;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#39046;&#22495;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#34987;&#24191;&#27867;&#37319;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#33021;&#20002;&#22833;&#32473;&#23450;&#25991;&#26412;&#30340;&#21407;&#22987;&#35821;&#20041;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#21024;&#38500;&#22312;&#21477;&#23376;&#20013;&#36215;&#36741;&#21161;&#20316;&#29992;&#30340;&#21103;&#35789;&#26469;&#36991;&#20813;&#36825;&#31181;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;&#21333;&#19968;&#25991;&#26412;&#20998;&#31867;&#65292;&#36824;&#36866;&#29992;&#20110;&#38656;&#35201;&#35821;&#20041;&#20445;&#30041;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#29992;&#20110;&#21487;&#37325;&#29616;&#24615;&#30340;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20015v1 Announce Type: cross  Abstract: In the field of text data augmentation, rule-based methods are widely adopted for real-world applications owing to their cost-efficiency. However, conventional rule-based approaches suffer from the possibility of losing the original semantics of the given text. We propose a novel text data augmentation strategy that avoids such phenomena through a straightforward deletion of adverbs, which play a subsidiary role in the sentence. Our comprehensive experiments demonstrate the efficiency and effectiveness of our proposed approach for not just single text classification, but also natural language inference that requires semantic preservation. We publicly released our source code for reproducibility.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PURPLE&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#28436;&#31034;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;SQL&#29983;&#25104;&#20013;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.20014</link><description>&lt;p&gt;
PURPLE: &#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;SQL&#32534;&#20889;&#22120;
&lt;/p&gt;
&lt;p&gt;
PURPLE: Making a Large Language Model a Better SQL Writer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20014
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PURPLE&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#28436;&#31034;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;SQL&#29983;&#25104;&#20013;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25216;&#26415;&#22312;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#65288;NL2SQL&#65289;&#32763;&#35793;&#20013;&#36215;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;LLMs&#20855;&#26377;&#24378;&#22823;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#22522;&#26412;SQL&#29983;&#25104;&#33021;&#21147;&#65292;&#26080;&#38656;&#38024;&#23545;NL2SQL&#20219;&#21153;&#36827;&#34892;&#39069;&#22806;&#35843;&#25972;&#12290;&#29616;&#26377;&#22522;&#20110;LLMs&#30340;NL2SQL&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#24378;&#35843;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#26469;&#25552;&#39640;&#32763;&#35793;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#22312;&#32452;&#32455;&#22797;&#26434;&#30340;&#36923;&#36753;&#36816;&#31639;&#31526;&#32452;&#21512;&#26041;&#38754;&#30340;&#30693;&#35782;&#65292;LLMs&#26377;&#26102;&#20250;&#29983;&#25104;&#19981;&#21512;&#36866;&#30340;SQL&#12290;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#21521;LLMs&#36755;&#20837;&#28436;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#25968;&#25454;&#24211;&#30340;&#24050;&#30693;NL2SQL&#32763;&#35793;&#12290;LLMs&#21487;&#20197;&#20174;&#36755;&#20837;&#28436;&#31034;&#20013;&#23398;&#20064;&#22914;&#20309;&#20026;&#32473;&#23450;&#20219;&#21153;&#32452;&#32455;&#36816;&#31639;&#31526;&#32452;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PURPLE&#65288;Pre-trained models Utilized to Retrieve Prompts for Logical Enhancement&#65289;&#65292;&#36890;&#36807;&#26816;&#32034;&#28436;&#31034;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20014v1 Announce Type: cross  Abstract: Large Language Model (LLM) techniques play an increasingly important role in Natural Language to SQL (NL2SQL) translation. LLMs trained by extensive corpora have strong natural language understanding and basic SQL generation abilities without additional tuning specific to NL2SQL tasks. Existing LLMs-based NL2SQL approaches try to improve the translation by enhancing the LLMs with an emphasis on user intention understanding. However, LLMs sometimes fail to generate appropriate SQL due to their lack of knowledge in organizing complex logical operator composition. A promising method is to input the LLMs with demonstrations, which include known NL2SQL translations from various databases. LLMs can learn to organize operator compositions from the input demonstrations for the given task. In this paper, we propose PURPLE (Pre-trained models Utilized to Retrieve Prompts for Logical Enhancement), which improves accuracy by retrieving demonstrati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#29702;&#21160;&#24577;&#30340;&#35282;&#24230;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#23545;&#20107;&#23454;&#24615;&#38382;&#39064;&#21644;&#36755;&#20986; token &#27010;&#29575;&#21160;&#24577;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#21457;&#29983;&#30340;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.20009</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#30340;&#24187;&#35273;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
On Large Language Models' Hallucination with Regard to Known Facts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20009
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#21160;&#24577;&#30340;&#35282;&#24230;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#23545;&#20107;&#23454;&#24615;&#38382;&#39064;&#21644;&#36755;&#20986; token &#27010;&#29575;&#21160;&#24577;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#21457;&#29983;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#20107;&#23454;&#31867;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#20063;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#29702;&#21160;&#24577;&#30340;&#35282;&#24230;&#30740;&#31350;LLMs&#20855;&#26377;&#27491;&#30830;&#31572;&#26696;&#30693;&#35782;&#21364;&#20173;&#28982;&#20135;&#29983;&#24187;&#35273;&#30340;&#29616;&#35937;&#65292;&#36825;&#26159;&#20197;&#24448;&#20851;&#20110;&#24187;&#35273;&#30740;&#31350;&#23578;&#26410;&#28085;&#30422;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#24605;&#36335;&#36827;&#34892;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26597;&#35810;&#30456;&#21516;&#19977;&#20803;&#30693;&#35782;&#20294;&#23548;&#33268;&#19981;&#21516;&#31572;&#26696;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#12290;&#27169;&#22411;&#22312;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#36755;&#20986;&#19978;&#30340;&#34892;&#20026;&#24046;&#24322;&#22240;&#27492;&#26263;&#31034;&#20102;&#24187;&#35273;&#21457;&#29983;&#30340;&#27169;&#24335;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#34913;&#37327;&#36825;&#31181;&#27169;&#24335;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#21097;&#20313;&#27969;&#21040;&#35789;&#27719;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#36755;&#20986;&#20196;&#29260;&#27010;&#29575;&#22312;&#27491;&#30830;&#21644;&#24187;&#35273;&#24773;&#20917;&#19979;&#22312;&#23618;&#28145;&#24230;&#19978;&#30340;&#19981;&#21516;&#21160;&#24577;&#12290;&#22312;&#24187;&#35273;&#26696;&#20363;&#20013;&#65292;&#36755;&#20986;&#20196;&#29260;&#30340;&#20449;&#24687;&#24456;&#23569;&#34920;&#29616;&#20986;&#31361;&#22686;&#21644;&#25345;&#32493;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20009v1 Announce Type: new  Abstract: Large language models are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen. Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space. We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output token's information rarely demonstrates abrupt increases and consistent
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#22659;&#23545;&#35805;&#27169;&#22411;&#65292;&#26088;&#22312;&#24110;&#21161;&#35821;&#35328;&#23398;&#20064;&#32773;&#36827;&#34892;&#26377;&#25928;&#30340;&#20250;&#35805;&#32451;&#20064;&#65292;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#25903;&#25345;&#22810;&#26679;&#30340;&#20250;&#35805;&#35805;&#39064;&#65292;&#20026;&#35299;&#20915;&#23398;&#29983;&#32570;&#20047;&#21512;&#26684;&#25945;&#24072;&#25110;&#27597;&#35821;&#32773;&#32451;&#20064;&#20250;&#35805;&#25216;&#33021;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.20005</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#22659;&#23545;&#35805;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Language Model based Situational Dialogues for Second Language Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20005
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#22659;&#23545;&#35805;&#27169;&#22411;&#65292;&#26088;&#22312;&#24110;&#21161;&#35821;&#35328;&#23398;&#20064;&#32773;&#36827;&#34892;&#26377;&#25928;&#30340;&#20250;&#35805;&#32451;&#20064;&#65292;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#25903;&#25345;&#22810;&#26679;&#30340;&#20250;&#35805;&#35805;&#39064;&#65292;&#20026;&#35299;&#20915;&#23398;&#29983;&#32570;&#20047;&#21512;&#26684;&#25945;&#24072;&#25110;&#27597;&#35821;&#32773;&#32451;&#20064;&#20250;&#35805;&#25216;&#33021;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#20013;&#65292;&#22522;&#20110;&#24773;&#22659;&#30340;&#23545;&#35805;&#32451;&#20064;&#23545;&#20110;&#35821;&#35328;&#23398;&#20064;&#32773;&#23454;&#29616;&#21475;&#35821;&#27969;&#21033;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23398;&#29983;&#24120;&#24120;&#32570;&#20047;&#19982;&#21512;&#26684;&#25945;&#24072;&#25110;&#27597;&#35821;&#32773;&#32451;&#20064;&#20250;&#35805;&#25216;&#33021;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24773;&#22659;&#23545;&#35805;&#27169;&#22411;&#65292;&#20379;&#23398;&#29983;&#21442;&#19982;&#20250;&#35805;&#32451;&#20064;&#12290;&#25105;&#20204;&#30340;&#24773;&#22659;&#23545;&#35805;&#27169;&#22411;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#65292;&#26088;&#22312;&#32467;&#21512;&#24320;&#25918;&#24335;&#23545;&#35805;&#30340;&#21560;&#24341;&#21147;&#21644;&#22522;&#20110;&#24773;&#22659;&#20219;&#21153;&#30340;&#19987;&#27880;&#32451;&#20064;&#12290;&#21033;&#29992;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#24773;&#22659;&#23545;&#35805;&#27169;&#22411;&#19981;&#20165;&#22312;&#35757;&#32451;&#35805;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#22312;&#35757;&#32451;&#20013;&#26410;&#36935;&#21040;&#30340;&#35805;&#39064;&#19978;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20026;&#25903;&#25345;&#24191;&#27867;&#30340;&#20250;&#35805;&#35805;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#19981;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#22312;&#23545;&#35805;&#39046;&#22495;&#30340;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20005v1 Announce Type: new  Abstract: In second language learning, scenario-based conversation practice is important for language learners to achieve fluency in speaking, but students often lack sufficient opportunities to practice their conversational skills with qualified instructors or native speakers. To bridge this gap, we propose situational dialogue models for students to engage in conversational practice. Our situational dialogue models are fine-tuned on large language models (LLMs), with the aim of combining the engaging nature of an open-ended conversation with the focused practice of scenario-based tasks. Leveraging the generalization capabilities of LLMs, we demonstrate that our situational dialogue models perform effectively not only on training topics but also on topics not encountered during training. This offers a promising solution to support a wide range of conversational topics without extensive manual work. Additionally, research in the field of dialogue 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#35270;&#35273;&#12289;&#26412;&#20307;&#24863;&#30693;&#21644;&#35821;&#35328;&#30340;&#22823;&#33041;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#21644;&#20027;&#21160;&#25512;&#26029;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#33258;&#30001;&#33021;&#21407;&#29702;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#32452;&#21512;&#24615;&#21644;&#24863;&#35273;&#36816;&#21160;&#25216;&#33021;&#30340;&#32852;&#21512;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.19995</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#23398;&#20064;&#35821;&#35328;&#21644;&#26426;&#22120;&#20154;&#21160;&#20316;&#23454;&#29616;&#32452;&#21512;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Development of Compositionality and Generalization through Interactive Learning of Language and Action of Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19995
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#35270;&#35273;&#12289;&#26412;&#20307;&#24863;&#30693;&#21644;&#35821;&#35328;&#30340;&#22823;&#33041;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#21644;&#20027;&#21160;&#25512;&#26029;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#33258;&#30001;&#33021;&#21407;&#29702;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#32452;&#21512;&#24615;&#21644;&#24863;&#35273;&#36816;&#21160;&#25216;&#33021;&#30340;&#32852;&#21512;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25797;&#38271;&#23558;&#23398;&#21040;&#30340;&#34892;&#20026;&#24212;&#29992;&#20110;&#26410;&#23398;&#20064;&#36807;&#30340;&#24773;&#22659;&#12290;&#36825;&#31181;&#27867;&#21270;&#34892;&#20026;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#25105;&#20204;&#33021;&#22815;&#23558;&#25972;&#20307;&#20998;&#35299;&#25104;&#21487;&#37325;&#22797;&#21033;&#29992;&#30340;&#37096;&#20998;&#30340;&#33021;&#21147;&#65292;&#21363;&#32452;&#21512;&#24615;&#12290;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#28041;&#21450;&#36825;&#31181;&#29305;&#24449;&#12290;&#8220;&#22312;&#20010;&#20307;&#21482;&#23398;&#20064;&#37096;&#20998;&#35821;&#35328;&#32452;&#21512;&#21450;&#20854;&#30456;&#24212;&#30340;&#24863;&#35273;&#36816;&#21160;&#27169;&#24335;&#26102;&#65292;&#22914;&#20309;&#36890;&#36807;&#32852;&#24819;&#23398;&#20064;&#21516;&#26102;&#21457;&#23637;&#35821;&#35328;&#30340;&#32452;&#21512;&#24615;&#21644;&#24863;&#35273;&#36816;&#21160;&#25216;&#33021;&#65311;&#8221;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#35270;&#35273;&#12289;&#26412;&#20307;&#24863;&#30693;&#21644;&#35821;&#35328;&#30340;&#22823;&#33041;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23558;&#20854;&#32435;&#20837;&#22522;&#20110;&#33258;&#30001;&#33021;&#21407;&#29702;&#30340;&#39044;&#27979;&#32534;&#30721;&#21644;&#20027;&#21160;&#25512;&#26029;&#26694;&#26550;&#20013;&#12290;&#36890;&#36807;&#19982;&#26426;&#22120;&#20154;&#25163;&#33218;&#36827;&#34892;&#30340;&#21508;&#31181;&#27169;&#25311;&#23454;&#39564;&#35780;&#20272;&#20102;&#36825;&#20010;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23398;&#20064;&#20013;&#23545;&#20110;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19995v1 Announce Type: new  Abstract: Humans excel at applying learned behavior to unlearned situations. A crucial component of this generalization behavior is our ability to compose/decompose a whole into reusable parts, an attribute known as compositionality. One of the fundamental questions in robotics concerns this characteristic. "How can linguistic compositionality be developed concomitantly with sensorimotor skills through associative learning, particularly when individuals only learn partial linguistic compositions and their corresponding sensorimotor patterns?" To address this question, we propose a brain-inspired neural network model that integrates vision, proprioception, and language into a framework of predictive coding and active inference, based on the free-energy principle. The effectiveness and capabilities of this model were assessed through various simulation experiments conducted with a robot arm. Our results show that generalization in learning to unlear
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#29305;&#23450;&#20110;&#20195;&#29702;&#30340;&#25968;&#25454;&#24182;&#32454;&#35843;&#27169;&#22411;&#20197;&#21450;&#35774;&#35745;&#33021;&#22815;&#26377;&#25928;&#28608;&#27963;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#22686;&#24378;&#20302;&#21442;&#25968;LLMs&#30340;&#36890;&#29992;&#20195;&#29702;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19962</link><description>&lt;p&gt;
&#36890;&#36807;&#35843;&#25972;&#21644;&#22810;&#25903;&#36335;&#25512;&#29702;&#22686;&#24378;&#20302;&#21442;&#25968;LLMs&#30340;&#36890;&#29992;&#20195;&#29702;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19962
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#29305;&#23450;&#20110;&#20195;&#29702;&#30340;&#25968;&#25454;&#24182;&#32454;&#35843;&#27169;&#22411;&#20197;&#21450;&#35774;&#35745;&#33021;&#22815;&#26377;&#25928;&#28608;&#27963;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#22686;&#24378;&#20302;&#21442;&#25968;LLMs&#30340;&#36890;&#29992;&#20195;&#29702;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19962v1 &#22768;&#26126;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#24320;&#28304;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#23427;&#20204;&#29992;&#20316;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#38382;&#39064;&#30340;&#20195;&#29702;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#36828;&#36828;&#19981;&#21450;ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#21830;&#29992;&#27169;&#22411;&#12290;&#20316;&#20026;&#26234;&#33021;&#20195;&#29702;&#65292;LLMs&#38656;&#35201;&#20855;&#22791;&#20219;&#21153;&#35268;&#21010;&#12289;&#38271;&#26399;&#35760;&#24518;&#20197;&#21450;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#21508;&#31181;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#12290;&#19968;&#26041;&#38754;&#65292;&#26377;&#20123;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#29305;&#23450;&#20110;&#20195;&#29702;&#30340;&#25968;&#25454;&#21644;&#24494;&#35843;&#27169;&#22411;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19968;&#20123;&#26041;&#27861;&#38598;&#20013;&#20110;&#35774;&#35745;&#33021;&#26377;&#25928;&#28608;&#27963;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;7B&#21644;13B&#27169;&#22411;&#19978;&#21516;&#26102;&#25506;&#35752;&#20102;&#36825;&#20004;&#31181;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-4&#26500;&#24314;&#29305;&#23450;&#20110;&#20195;&#29702;&#25968;&#25454;&#30340;&#20840;&#38754;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19962v1 Announce Type: cross  Abstract: Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. T
&lt;/p&gt;</description></item><item><title>SLFNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#20381;&#36182;&#21477;&#27861;&#20449;&#24687;&#21644;&#35821;&#20041;&#27010;&#29575;&#22270;&#26469;&#20174;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35821;&#20041;&#36923;&#36753;&#24418;&#24335;&#65292;&#20197;&#35299;&#20915;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#8220;&#39034;&#24207;&#37325;&#35201;&#24615;&#8221;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19936</link><description>&lt;p&gt;
SLFNet: &#20351;&#29992;&#35821;&#20041;&#27010;&#29575;&#22270;&#20174;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35821;&#20041;&#36923;&#36753;&#24418;&#24335;
&lt;/p&gt;
&lt;p&gt;
SLFNet: Generating Semantic Logic Forms from Natural Language Using Semantic Probability Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19936
&lt;/p&gt;
&lt;p&gt;
SLFNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#20381;&#36182;&#21477;&#27861;&#20449;&#24687;&#21644;&#35821;&#20041;&#27010;&#29575;&#22270;&#26469;&#20174;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35821;&#20041;&#36923;&#36753;&#24418;&#24335;&#65292;&#20197;&#35299;&#20915;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#8220;&#39034;&#24207;&#37325;&#35201;&#24615;&#8221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#36890;&#24120;&#20351;&#29992;&#35821;&#20041;&#35299;&#26512;&#22120;&#26469;&#35299;&#26512;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#30340;&#35821;&#20041;&#36923;&#36753;&#24418;&#24335;(SLFs)&#12290; &#20027;&#27969;&#26041;&#27861;&#26159;&#37319;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#65292;&#36825;&#35201;&#27714;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#21644;SLFs&#24517;&#39035;&#25353;&#39034;&#24207;&#34920;&#31034;&#12290; &#30001;&#20110;&#21333;&#20010;&#33258;&#28982;&#35821;&#35328;&#21487;&#33021;&#20855;&#26377;&#22810;&#20010;SLF&#25110;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#21487;&#33021;&#20855;&#26377;&#30456;&#21516;&#30340;SLF&#65292;&#22240;&#27492;&#35757;&#32451;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#23545;&#23427;&#20204;&#20043;&#38388;&#30340;&#36873;&#25321;&#25935;&#24863;&#65292;&#36825;&#19968;&#29616;&#35937;&#34987;&#35760;&#24405;&#20026;&#8220;&#39034;&#24207;&#37325;&#35201;&#24615;&#8221;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;SLFNet&#65292;&#39318;&#20808;&#23558;&#20381;&#36182;&#21477;&#27861;&#20449;&#24687;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#24182;&#33021;&#22815;&#25429;&#33719;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#21333;&#35789;&#20043;&#38388;&#30340;&#38271;&#36317;&#31163;&#20132;&#20114;&#20316;&#29992;&#12290; &#20854;&#27425;&#26500;&#24314;&#35821;&#20041;&#27010;&#29575;&#22270;&#20197;&#33719;&#24471;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#23616;&#37096;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19936v1 Announce Type: new  Abstract: Building natural language interfaces typically uses a semantic parser to parse the user's natural language and convert it into structured \textbf{S}emantic \textbf{L}ogic \textbf{F}orms (SLFs). The mainstream approach is to adopt a sequence-to-sequence framework, which requires that natural language commands and SLFs must be represented serially. Since a single natural language may have multiple SLFs or multiple natural language commands may have the same SLF, training a sequence-to-sequence model is sensitive to the choice among them, a phenomenon recorded as "order matters". To solve this problem, we propose a novel neural network, SLFNet, which firstly incorporates dependent syntactic information as prior knowledge and can capture the long-range interactions between contextual information and words. Secondly construct semantic probability graphs to obtain local dependencies between predictor variables. Finally we propose the Multi-Hea
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#30417;&#30563;&#35774;&#32622;&#20013;&#20026;&#20013;&#25991;&#30701;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#24494;&#35843;LLMs&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#20219;&#21153;&#24314;&#27169;&#26041;&#27861;&#12289;&#25552;&#31034;&#26684;&#24335;&#21644;&#36755;&#20986;&#26684;&#24335;&#31561;&#22240;&#32032;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.19930</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#26159;&#24494;&#35843;&#30340;&#26377;&#25928;&#39592;&#24178;&#65311;&#23545;&#20013;&#25991;&#30701;&#25991;&#26412;&#21305;&#37197;&#30340;&#30417;&#30563;LLMs&#30340;&#23454;&#39564;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19930
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#30417;&#30563;&#35774;&#32622;&#20013;&#20026;&#20013;&#25991;&#30701;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#24494;&#35843;LLMs&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#20219;&#21153;&#24314;&#27169;&#26041;&#27861;&#12289;&#25552;&#31034;&#26684;&#24335;&#21644;&#36755;&#20986;&#26684;&#24335;&#31561;&#22240;&#32032;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#20808;&#21069;&#23545;LLMs&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#25110;&#21033;&#29992;&#23427;&#20204;&#22312;&#38646;-shot&#21644;&#23569;-shot&#35774;&#32622;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#21463;&#30417;&#30563;&#30340;&#35774;&#32622;&#20013;&#65292;&#23545;LLMs&#36827;&#34892;&#26377;&#25928;&#24494;&#35843;&#20197;&#29992;&#20110;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#35843;&#26597;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#20013;&#25991;&#30701;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#36827;&#34892;&#20102;&#23454;&#39564;&#20998;&#26512;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#24433;&#21709;&#22312;&#24494;&#35843;LLMs&#26102;&#24615;&#33021;&#30340;&#21508;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#20219;&#21153;&#24314;&#27169;&#26041;&#27861;&#65292;&#25552;&#31034;&#26684;&#24335;&#21644;&#36755;&#20986;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19930v1 Announce Type: new  Abstract: The recent success of Large Language Models (LLMs) has garnered significant attention in both academia and industry. Prior research on LLMs has primarily focused on enhancing or leveraging their generalization capabilities in zero- and few-shot settings. However, there has been limited investigation into effectively fine-tuning LLMs for a specific natural language understanding task in supervised settings. In this study, we conduct an experimental analysis by fine-tuning LLMs for the task of Chinese short text matching. We explore various factors that influence performance when fine-tuning LLMs, including task modeling methods, prompt formats, and output formats.
&lt;/p&gt;</description></item><item><title>DiJiang&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#22495;&#26680;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#30340;&#22522;&#26412;Transformer&#27169;&#22411;&#36716;&#21270;&#20026;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#65292;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#36924;&#36817;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.19928</link><description>&lt;p&gt;
DiJiang&#65306;&#36890;&#36807;&#32039;&#20945;&#30340;&#26680;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiJiang: Efficient Large Language Models through Compact Kernelization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19928
&lt;/p&gt;
&lt;p&gt;
DiJiang&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#22495;&#26680;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#30340;&#22522;&#26412;Transformer&#27169;&#22411;&#36716;&#21270;&#20026;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#65292;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#36924;&#36817;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#23569;Transformers&#30340;&#35745;&#31639;&#36127;&#33655;&#65292;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#27880;&#24847;&#26426;&#21046;&#30340;&#25913;&#36827;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#32463;&#36807;&#22823;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#22312;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DiJiang&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#22495;&#26680;&#26041;&#27861;&#65292;&#21487;&#23558;&#39044;&#35757;&#32451;&#30340;&#22522;&#26412;Transformer&#36716;&#21270;&#20026;&#20855;&#26377;&#36739;&#23567;&#35757;&#32451;&#25104;&#26412;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#27169;&#22411;&#12290;&#36890;&#36807;&#37319;&#29992;&#21152;&#26435;&#25311;&#38543;&#26426;&#37319;&#26679;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#36924;&#36817;&#25928;&#29575;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#38477;&#20302;&#35757;&#32451;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#30340;&#26680;&#26041;&#27861;&#22522;&#20110;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#65288;DCT&#65289;&#25805;&#20316;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#19982;&#21407;&#22987;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#35757;&#32451;&#26102;&#38388;&#22823;&#22823;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19928v1 Announce Type: new  Abstract: In an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced trainin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;MANGO&#22522;&#20934;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#22238;&#31572;&#28041;&#21450;&#26144;&#23556;&#21644;&#23548;&#33322;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2403.19913</link><description>&lt;p&gt;
MANGO&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19913
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;MANGO&#22522;&#20934;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#22238;&#31572;&#28041;&#21450;&#26144;&#23556;&#21644;&#23548;&#33322;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#36817;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MANGO&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#23427;&#20204;&#25191;&#34892;&#22522;&#20110;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;&#19968;&#22871;&#25991;&#26412;&#28216;&#25103;&#30340;53&#20010;&#36855;&#23467;&#65306;&#27599;&#20010;&#36855;&#23467;&#37117;&#19982;&#19968;&#20010;&#28216;&#35272;&#35828;&#26126;&#37197;&#23545;&#65292;&#20854;&#20013;&#21253;&#21547;&#27599;&#20010;&#20301;&#32622;&#30340;&#35775;&#38382;&#20294;&#19981;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#30340;&#36335;&#24452;&#12290;&#20219;&#21153;&#26159;&#38382;&#31572;&#65306;&#23545;&#20110;&#27599;&#20010;&#36855;&#23467;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35835;&#21462;&#28216;&#35272;&#35828;&#26126;&#24182;&#22238;&#31572;&#25968;&#30334;&#20010;&#26144;&#23556;&#21644;&#23548;&#33322;&#38382;&#39064;&#65292;&#20363;&#22914;&#8220;&#20320;&#24212;&#35813;&#20174;&#25151;&#23376;&#35199;&#37096;&#22914;&#20309;&#21435;&#38401;&#27004;&#65311;&#8221;&#21644;&#8220;&#22914;&#26524;&#25105;&#20204;&#20174;&#22320;&#19979;&#23460;&#21521;&#21271;&#21644;&#19996;&#36208;&#65292;&#25105;&#20204;&#20250;&#22312;&#21738;&#37324;&#65311;&#8221;&#12290;&#23613;&#31649;&#36825;&#20123;&#38382;&#39064;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#65292;&#20294;&#20107;&#23454;&#35777;&#26126;&#65292;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#29978;&#33267;&#22312;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#23558;&#26377;&#21033;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19913v1 Announce Type: cross  Abstract: Large language models such as ChatGPT and GPT-4 have recently achieved astonishing performance on a variety of natural language processing tasks. In this paper, we propose MANGO, a benchmark to evaluate their capabilities to perform text-based mapping and navigation. Our benchmark includes 53 mazes taken from a suite of textgames: each maze is paired with a walkthrough that visits every location but does not cover all possible paths. The task is question-answering: for each maze, a large language model reads the walkthrough and answers hundreds of mapping and navigation questions such as "How should you go to Attic from West of House?" and "Where are we if we go north and east from Cellar?". Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them. Further, our experiments suggest that a strong mapping and navigation ability would benefit large languag
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;-&#22522;&#30784;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#20581;&#22766;&#24615;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#20010;&#20840;&#38754;&#30340;&#31995;&#32479;&#26469;&#22686;&#24378;&#27169;&#22411;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#30340;&#20581;&#22766;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19889</link><description>&lt;p&gt;
&#26397;&#21521;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#25688;&#35201;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards a Robust Retrieval-Based Summarization System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;-&#22522;&#30784;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#20581;&#22766;&#24615;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#20010;&#20840;&#38754;&#30340;&#31995;&#32479;&#26469;&#22686;&#24378;&#27169;&#22411;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#30340;&#20581;&#22766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;-&#22522;&#30784;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#20581;&#22766;&#24615;&#36827;&#34892;&#30340;&#35843;&#26597;&#12290;&#34429;&#28982;LLMs&#25552;&#20379;&#20102;&#25688;&#35201;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;LogicSumm&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#29616;&#23454;&#22330;&#26223;&#65292;&#29992;&#26469;&#35780;&#20272;LLMs&#22312;RAG&#22522;&#30784;&#25688;&#35201;&#36807;&#31243;&#20013;&#30340;&#20581;&#22766;&#24615;&#12290;&#26681;&#25454;LogicSumm&#35782;&#21035;&#20986;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SummRAG&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#21019;&#24314;&#35757;&#32451;&#23545;&#35805;&#24182;&#24494;&#35843;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#22312;LogicSumm&#22330;&#26223;&#20013;&#30340;&#20581;&#22766;&#24615;&#12290;SummRAG&#26159;&#25105;&#20204;&#23450;&#20041;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#27979;&#35797;LLM&#33021;&#21147;&#30340;&#30446;&#26631;&#30340;&#19968;&#20010;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#19968;&#21171;&#27704;&#36920;&#22320;&#35299;&#20915;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;SummRAG&#30340;&#24378;&#22823;&#65292;&#23637;&#31034;&#20102;&#36923;&#36753;&#36830;&#36143;&#24615;&#21644;&#25688;&#35201;&#36136;&#37327;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19889v1 Announce Type: cross  Abstract: This paper describes an investigation of the robustness of large language models (LLMs) for retrieval augmented generation (RAG)-based summarization tasks. While LLMs provide summarization capabilities, their performance in complex, real-world scenarios remains under-explored. Our first contribution is LogicSumm, an innovative evaluation framework incorporating realistic scenarios to assess LLM robustness during RAG-based summarization. Based on limitations identified by LogiSumm, we then developed SummRAG, a comprehensive system to create training dialogues and fine-tune a model to enhance robustness within LogicSumm's scenarios. SummRAG is an example of our goal of defining structured methods to test the capabilities of an LLM, rather than addressing issues in a one-off fashion. Experimental results confirm the power of SummRAG, showcasing improved logical coherence and summarization quality. Data, corresponding model weights, and Py
&lt;/p&gt;</description></item><item><title>Jamba&#26159;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;Transformer-Mamba&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21333;&#20010;80GB GPU&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#23545;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#21644;&#38271;&#19978;&#19979;&#25991;&#35780;&#20272;&#20855;&#26377;state-of-the-art&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.19887</link><description>&lt;p&gt;
Jamba: &#19968;&#20010;&#28151;&#21512;Transformer-Mamba&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jamba: A Hybrid Transformer-Mamba Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19887
&lt;/p&gt;
&lt;p&gt;
Jamba&#26159;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;Transformer-Mamba&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21333;&#20010;80GB GPU&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#23545;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#21644;&#38271;&#19978;&#19979;&#25991;&#35780;&#20272;&#20855;&#26377;state-of-the-art&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Jamba&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26032;&#39062;&#30340;&#28151;&#21512;Transformer-Mamba&#28151;&#21512;&#19987;&#23478;(MoE)&#26550;&#26500;&#30340;&#26032;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Jamba&#20132;&#38169;&#20351;&#29992;Transformer&#21644;Mamba&#23618;&#65292;&#20174;&#20004;&#31181;&#27169;&#22411;&#23478;&#26063;&#20013;&#33719;&#30410;&#12290;MoE&#34987;&#28155;&#21152;&#22312;&#20854;&#20013;&#19968;&#20123;&#23618;&#20013;&#65292;&#20197;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27963;&#36291;&#21442;&#25968;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#26550;&#26500;&#20801;&#35768;&#29305;&#23450;&#36164;&#28304;&#21644;&#30446;&#26631;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19887v1 Announce Type: new  Abstract: We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transfor
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27573;&#33853;&#35760;&#24518;&#30340;&#26799;&#24230;&#20855;&#26377;&#21487;&#21306;&#20998;&#30340;&#31354;&#38388;&#27169;&#24335;&#65292;&#36890;&#36807;&#24494;&#35843;&#39640;&#26799;&#24230;&#26435;&#37325;&#21487;&#20197;&#21462;&#28040;&#23398;&#20064;&#65292;&#23450;&#20301;&#20102;&#29305;&#21035;&#21442;&#19982;&#27573;&#33853;&#35760;&#24518;&#30340;&#20302;&#23618;&#27880;&#24847;&#22836;&#65292;&#24182;&#30740;&#31350;&#20102;&#35760;&#24518;&#22312;&#21069;&#32512;&#20013;&#30340;&#26412;&#22320;&#21270;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.19851</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27573;&#33853;&#35760;&#24518;&#26412;&#22320;&#21270;
&lt;/p&gt;
&lt;p&gt;
Localizing Paragraph Memorization in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19851
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27573;&#33853;&#35760;&#24518;&#30340;&#26799;&#24230;&#20855;&#26377;&#21487;&#21306;&#20998;&#30340;&#31354;&#38388;&#27169;&#24335;&#65292;&#36890;&#36807;&#24494;&#35843;&#39640;&#26799;&#24230;&#26435;&#37325;&#21487;&#20197;&#21462;&#28040;&#23398;&#20064;&#65292;&#23450;&#20301;&#20102;&#29305;&#21035;&#21442;&#19982;&#27573;&#33853;&#35760;&#24518;&#30340;&#20302;&#23618;&#27880;&#24847;&#22836;&#65292;&#24182;&#30740;&#31350;&#20102;&#35760;&#24518;&#22312;&#21069;&#32512;&#20013;&#30340;&#26412;&#22320;&#21270;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#35760;&#24518;&#21644;&#32972;&#35829;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#27573;&#30340;&#26435;&#37325;&#21644;&#26426;&#21046;&#26412;&#22320;&#21270;&#65311;&#26412;&#25991;&#34920;&#26126;&#65292;&#34429;&#28982;&#35760;&#24518;&#20998;&#24067;&#22312;&#22810;&#20010;&#23618;&#27425;&#21644;&#27169;&#22411;&#32452;&#20214;&#20013;&#65292;&#20294;&#35760;&#24518;&#27573;&#33853;&#30340;&#26799;&#24230;&#20855;&#26377;&#21487;&#21306;&#20998;&#30340;&#31354;&#38388;&#27169;&#24335;&#65292;&#36739;&#20302;&#27169;&#22411;&#23618;&#27425;&#20013;&#30340;&#26799;&#24230;&#27604;&#38750;&#35760;&#24518;&#31034;&#20363;&#30340;&#26799;&#24230;&#26356;&#22823;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#35760;&#24518;&#31034;&#20363;&#21487;&#20197;&#36890;&#36807;&#20165;&#24494;&#35843;&#39640;&#26799;&#24230;&#26435;&#37325;&#26469;&#21462;&#28040;&#23398;&#20064;&#12290;&#25105;&#20204;&#23450;&#20301;&#20102;&#19968;&#20010;&#20284;&#20046;&#29305;&#21035;&#21442;&#19982;&#27573;&#33853;&#35760;&#24518;&#30340;&#20302;&#23618;&#27880;&#24847;&#22836;&#12290;&#36825;&#20010;&#22836;&#37096;&#20027;&#35201;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#22312;&#35821;&#26009;&#24211;&#32423;&#21035;&#30340;&#21333;&#35821;&#20998;&#24067;&#20013;&#26368;&#19981;&#39057;&#32321;&#30340;&#29420;&#29305;&#12289;&#32597;&#35265;&#30340;&#20196;&#29260;&#19978;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#25200;&#21160;&#20196;&#29260;&#24182;&#27979;&#37327;&#23545;&#35299;&#30721;&#36896;&#25104;&#30340;&#25913;&#21464;&#26469;&#30740;&#31350;&#35760;&#24518;&#22312;&#21069;&#32512;&#20013;&#30340;&#26412;&#22320;&#21270;&#31243;&#24230;&#12290;&#21069;&#32512;&#20013;&#30340;&#19968;&#20123;&#29420;&#29305;&#20196;&#29260;&#32463;&#24120;&#20250;&#20351;&#25972;&#20010;&#20869;&#23481;&#21463;&#25439;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19851v1 Announce Type: new  Abstract: Can we localize the weights and mechanisms used by a language model to memorize and recite entire paragraphs of its training data? In this paper, we show that while memorization is spread across multiple layers and model components, gradients of memorized paragraphs have a distinguishable spatial pattern, being larger in lower model layers than gradients of non-memorized examples. Moreover, the memorized examples can be unlearned by fine-tuning only the high-gradient weights. We localize a low-layer attention head that appears to be especially involved in paragraph memorization. This head is predominantly focusing its attention on distinctive, rare tokens that are least frequent in a corpus-level unigram distribution. Next, we study how localized memorization is across the tokens in the prefix by perturbing tokens and measuring the caused change in the decoding. A few distinctive tokens early in a prefix can often corrupt the entire cont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26356;&#20808;&#36827;&#30340;&#26234;&#33021;&#20316;&#29289;&#31649;&#29702;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26469;&#20248;&#21270;&#20316;&#29289;&#31649;&#29702;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2403.19839</link><description>&lt;p&gt;
&#26032;&#30340;&#20892;&#23398;&#23478;&#65306;&#35821;&#35328;&#27169;&#22411;&#26159;&#20316;&#29289;&#31649;&#29702;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
The New Agronomists: Language Models are Experts in Crop Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26356;&#20808;&#36827;&#30340;&#26234;&#33021;&#20316;&#29289;&#31649;&#29702;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26469;&#20248;&#21270;&#20316;&#29289;&#31649;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#29289;&#31649;&#29702;&#22312;&#20915;&#23450;&#20316;&#29289;&#20135;&#37327;&#12289;&#32463;&#27982;&#30408;&#21033;&#21644;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#22312;&#20197;&#24448;&#30740;&#31350;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#20808;&#36827;&#30340;&#26234;&#33021;&#20316;&#29289;&#31649;&#29702;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#29420;&#29305;&#22320;&#23558;&#24378;&#21270;&#23398;&#20064;&#12289;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21644;&#30001;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20026;&#20892;&#19994;&#25216;&#26415;&#36716;&#31227;&#65288;DSSAT&#65289;&#23454;&#29616;&#30340;&#20316;&#29289;&#27169;&#25311;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;Q&#32593;&#32476;&#65292;&#26469;&#35757;&#32451;&#22788;&#29702;&#27169;&#25311;&#22120;&#20013;&#20247;&#22810;&#29366;&#24577;&#21464;&#37327;&#20316;&#20026;&#35266;&#27979;&#30340;&#31649;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19839v1 Announce Type: cross  Abstract: Crop management plays a crucial role in determining crop yield, economic profitability, and environmental sustainability. Despite the availability of management guidelines, optimizing these practices remains a complex and multifaceted challenge. In response, previous studies have explored using reinforcement learning with crop simulators, typically employing simple neural-network-based reinforcement learning (RL) agents. Building on this foundation, this paper introduces a more advanced intelligent crop management system. This system uniquely combines RL, a language model (LM), and crop simulations facilitated by the Decision Support System for Agrotechnology Transfer (DSSAT). We utilize deep RL, specifically a deep Q-network, to train management policies that process numerous state variables from the simulator as observations. A novel aspect of our approach is the conversion of these state variables into more informative language, fac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.19837</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#27010;&#24565;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Concept-based Analysis of Neural Networks via Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24418;&#24335;&#21270;&#20998;&#26512;&#38750;&#24120;&#21487;&#21462;&#65292;&#20294;&#30001;&#20110;&#38590;&#20197;&#34920;&#36798;&#35270;&#35273;&#20219;&#21153;&#30340;&#24418;&#24335;&#21270;&#35268;&#33539;&#20197;&#21450;&#32570;&#20047;&#39640;&#25928;&#30340;&#39564;&#35777;&#31243;&#24207;&#65292;&#36825;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#12289;&#35270;&#35273;&#35821;&#35328;&#12289;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#36890;&#36807;&#20854;&#21487;&#20197;&#25512;&#29702;&#35270;&#35273;&#27169;&#22411;&#30340;&#36879;&#38236;&#12290;VLMs&#24050;&#32463;&#22312;&#22823;&#37327;&#22270;&#20687;&#21450;&#20854;&#25991;&#26412;&#25551;&#36848;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#22240;&#27492;&#38544;&#24335;&#22320;&#20102;&#35299;&#25551;&#36848;&#36825;&#20123;&#22270;&#20687;&#30340;&#39640;&#23618;&#27425;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{Con}_{\texttt{spec}}$&#30340;&#36923;&#36753;&#35268;&#33539;&#35821;&#35328;&#65292;&#26088;&#22312;&#20415;&#20110;&#25353;&#29031;&#36825;&#20123;&#27010;&#24565;&#32534;&#20889;&#35268;&#33539;&#12290;&#20026;&#20102;&#23450;&#20041;&#21644;&#24418;&#24335;&#21270;&#26816;&#26597;$\texttt{Con}_{\texttt{spec}}$&#35268;&#33539;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;VLM&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#32534;&#30721;&#21644;&#39640;&#25928;&#26816;&#26597;&#35270;&#35273;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#23646;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;te
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19837v1 Announce Type: cross  Abstract: Formal analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\texttt{Con}_{\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\texttt{Con}_{\texttt{spec}}$ specifications, we leverage a VLM, which provides a means to encode and efficiently check natural-language properties of vision models. We demonstrate our te
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20391;&#37325;&#20110;&#36776;&#35782;&#20167;&#24680;&#35328;&#35770;&#30340;&#26263;&#31034;&#30446;&#26631;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#30446;&#26631;&#36328;&#24230;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#65292;&#30446;&#30340;&#26159;&#35782;&#21035;&#26356;&#21152;&#24494;&#22937;&#30340;&#20167;&#24680;&#35328;&#35770;&#24182;&#22686;&#24378;&#23545;&#25968;&#23383;&#24179;&#21488;&#19978;&#26377;&#23475;&#20869;&#23481;&#30340;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.19836</link><description>&lt;p&gt;
&#38544;&#24335;&#26377;&#23475;&#20869;&#23481;&#30340;&#30446;&#26631;&#36328;&#24230;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Target Span Detection for Implicit Harmful Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19836
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20391;&#37325;&#20110;&#36776;&#35782;&#20167;&#24680;&#35328;&#35770;&#30340;&#26263;&#31034;&#30446;&#26631;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#30446;&#26631;&#36328;&#24230;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#65292;&#30446;&#30340;&#26159;&#35782;&#21035;&#26356;&#21152;&#24494;&#22937;&#30340;&#20167;&#24680;&#35328;&#35770;&#24182;&#22686;&#24378;&#23545;&#25968;&#23383;&#24179;&#21488;&#19978;&#26377;&#23475;&#20869;&#23481;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36776;&#35782;&#20167;&#24680;&#35328;&#35770;&#30340;&#30446;&#26631;&#26159;&#29702;&#35299;&#27492;&#31867;&#35328;&#35770;&#24615;&#36136;&#30340;&#20851;&#38190;&#19968;&#27493;&#65292;&#26368;&#32456;&#26377;&#21161;&#20110;&#25913;&#36827;&#22312;&#32447;&#35770;&#22363;&#19978;&#20882;&#29359;&#24615;&#24086;&#23376;&#30340;&#26816;&#27979;&#12290;&#22312;&#32447;&#24179;&#21488;&#19978;&#35768;&#22810;&#26377;&#23475;&#20869;&#23481;&#20351;&#29992;&#38544;&#21547;&#35821;&#35328;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#33030;&#24369;&#21644;&#21463;&#20445;&#25252;&#32676;&#20307;&#65292;&#20363;&#22914;&#20351;&#29992;&#21051;&#26495;&#30340;&#29305;&#24449;&#32780;&#38750;&#26126;&#31034;&#30340;&#30446;&#26631;&#21517;&#31216;&#65292;&#36825;&#20351;&#24471;&#26816;&#27979;&#21644;&#20943;&#36731;&#20854;&#35821;&#35328;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#36776;&#35782;&#20167;&#24680;&#35328;&#35770;&#30340;&#26263;&#31034;&#30446;&#26631;&#65292;&#36825;&#23545;&#35782;&#21035;&#26356;&#21152;&#24494;&#22937;&#30340;&#20167;&#24680;&#35328;&#35770;&#21450;&#22686;&#24378;&#25968;&#23383;&#24179;&#21488;&#19978;&#26377;&#23475;&#20869;&#23481;&#30340;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26088;&#22312;&#35782;&#21035;&#21363;&#20351;&#26410;&#26126;&#31034;&#30340;&#30446;&#26631;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#33879;&#21517;&#30340;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65288;SBIC&#12289;DynaHate&#21644;IHC&#65289;&#20013;&#25910;&#38598;&#24182;&#26631;&#27880;&#30446;&#26631;&#36328;&#24230;&#12290;&#25105;&#20204;&#23558;&#24471;&#21040;&#30340;&#21512;&#24182;&#38598;&#21512;&#21629;&#21517;&#20026;&#38544;&#21547;-&#30446;&#26631;-&#36328;&#24230;&#12290;&#36825;&#19968;&#38598;&#21512;&#26159;&#36890;&#36807;&#19968;&#20010;&#21019;&#26032;&#30340;&#26041;&#27861;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19836v1 Announce Type: new  Abstract: Identifying the targets of hate speech is a crucial step in grasping the nature of such speech and, ultimately, in improving the detection of offensive posts on online forums. Much harmful content on online platforms uses implicit language especially when targeting vulnerable and protected groups such as using stereotypical characteristics instead of explicit target names, making it harder to detect and mitigate the language. In this study, we focus on identifying implied targets of hate speech, essential for recognizing subtler hate speech and enhancing the detection of harmful content on digital platforms. We define a new task aimed at identifying the targets even when they are not explicitly stated. To address that task, we collect and annotate target spans in three prominent implicit hate speech datasets: SBIC, DynaHate, and IHC. We call the resulting merged collection Implicit-Target-Span. The collection is achieved using an innovat
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20174;&#30456;&#20851;&#32467;&#26500;&#65288;&#20363;&#22914;&#8220;a few days&#8221;&#65289;&#36827;&#34892;&#27867;&#21270;&#23398;&#20064;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;AANN&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.19827</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20174;&#19981;&#24120;&#35265;&#30340;&#29616;&#35937;&#20013;&#23398;&#20064;&#65306;&#32570;&#22833;AANN&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19827
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20174;&#30456;&#20851;&#32467;&#26500;&#65288;&#20363;&#22914;&#8220;a few days&#8221;&#65289;&#36827;&#34892;&#27867;&#21270;&#23398;&#20064;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;AANN&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#32597;&#35265;&#30340;&#21477;&#27861;&#29616;&#35937;&#65292;&#20294;&#26377;&#20154;&#35748;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#27515;&#35760;&#30828;&#32972;&#65292;&#32780;&#19981;&#26159;&#35821;&#27861;&#27010;&#25324;&#12290;&#25105;&#20204;&#22312;&#35268;&#27169;&#20026;&#20154;&#31867;&#35268;&#27169;&#30340;&#35821;&#26009;&#24211;&#65288;1&#20159;&#23383;&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36845;&#20195;&#35757;&#32451;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#35780;&#20272;&#23427;&#20204;&#23545;&#29305;&#23450;&#32597;&#35265;&#35821;&#27861;&#29616;&#35937;&#30340;&#23398;&#20064;&#65306;&#33521;&#35821;&#30340;&#20896;&#35789;+&#24418;&#23481;&#35789;+&#25968;&#23383;+&#21517;&#35789;&#65288;AANN&#65289;&#32467;&#26500;&#65288;&#8220;a beautiful five days&#8221;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19827v1 Announce Type: new  Abstract: Language models learn rare syntactic phenomena, but it has been argued that they rely on rote memorization, as opposed to grammatical generalization. Training on a corpus of human-scale in size (100M words), we iteratively trained transformer language models on systematically manipulated corpora and then evaluated their learning of a particular rare grammatical phenomenon: the English Article+Adjective+Numeral+Noun (AANN) construction (``a beautiful five days''). We first compared how well this construction was learned on the default corpus relative to a counterfactual corpus in which the AANN sentences were removed. AANNs were still learned better than systematically perturbed variants of the construction. Using additional counterfactual corpora, we suggest that this learning occurs through generalization from related constructions (e.g., ``a few days''). An additional experiment showed that this learning is enhanced when there is more 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#20197;&#21450;&#22522;&#20110;&#32763;&#35793;&#30340;&#30417;&#30563;&#20013;&#38388;&#35757;&#32451;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;Librispeech&#21644;SUPERB&#19978;&#30456;&#23545;&#25552;&#39640;&#39640;&#36798;38.45%&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.19822</link><description>&lt;p&gt;
&#22810;&#38454;&#27573;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19822
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#20197;&#21450;&#22522;&#20110;&#32763;&#35793;&#30340;&#30417;&#30563;&#20013;&#38388;&#35757;&#32451;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;Librispeech&#21644;SUPERB&#19978;&#30456;&#23545;&#25552;&#39640;&#39640;&#36798;38.45%&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#19982;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#30456;&#27604;&#65292;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#24615;&#33021;&#65292;&#21363;&#20351;&#27169;&#22411;&#22312;&#21333;&#27169;&#24577;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;ASR&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#38454;&#27573;&#39044;&#35757;&#32451;&#65292;&#21363;&#20351;&#29992;&#21333;&#20010;&#26080;&#30417;&#30563;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#19982;&#22522;&#20110;&#32763;&#35793;&#30340;&#30417;&#30563;&#20013;&#38388;&#35757;&#32451;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#65292;&#36825;&#31181;&#22810;&#38454;&#27573;&#26041;&#27861;&#22312;Librispeech&#21644;SUPERB&#19978;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#30340;&#25913;&#36827;&#26368;&#39640;&#36798;38.45&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#36873;&#25321;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#19968;&#20123;&#37325;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19822v1 Announce Type: cross  Abstract: Recent advances in machine learning have demonstrated that multi-modal pre-training can improve automatic speech recognition (ASR) performance compared to randomly initialized models, even when models are fine-tuned on uni-modal tasks. Existing multi-modal pre-training methods for the ASR task have primarily focused on single-stage pre-training where a single unsupervised task is used for pre-training followed by fine-tuning on the downstream task. In this work, we introduce a novel method combining multi-modal and multi-task unsupervised pre-training with a translation-based supervised mid-training approach. We empirically demonstrate that such a multi-stage approach leads to relative word error rate (WER) improvements of up to 38.45% over baselines on both Librispeech and SUPERB. Additionally, we share several important findings for choosing pre-training methods and datasets.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#25928;&#21033;&#29992;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#12290;</title><link>https://arxiv.org/abs/2403.19802</link><description>&lt;p&gt;
&#24320;&#21457;&#21307;&#30103;&#20445;&#20581;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Developing Healthcare Language Model Embedding Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19802
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#25928;&#21033;&#29992;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#22312;&#35832;&#22914;&#19987;&#27880;&#20110;&#21307;&#30103;&#20445;&#20581;&#25991;&#26412;&#20043;&#31867;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#32463;&#24120;&#38754;&#20020;&#22256;&#38590;&#12290;&#25105;&#20204;&#25506;&#32034;&#19987;&#38376;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#35843;&#25972;&#36739;&#23567;&#30340;LLMs&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#38598;&#12290;&#35780;&#20272;&#20102;&#19977;&#31181;&#26041;&#27861;&#65306;&#20256;&#32479;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#12289;&#29992;&#20110;&#26080;&#30417;&#30563;&#25991;&#26412;&#34920;&#31034;&#30340;&#28145;&#24230;&#23545;&#27604;&#23398;&#20064; (DeCLUTR) &#21644;&#19968;&#31181;&#21033;&#29992;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#30340;&#20803;&#25968;&#25454;&#31867;&#21035;&#30340;&#26032;&#39062;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#19979;&#28216;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#20102;&#39069;&#22806;&#20998;&#26512;&#12290;&#23545;&#27604;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#24378;&#22823;&#24615;&#33021;&#65292;&#24182;&#19988;&#38656;&#35201;&#36739;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#12290;&#34429;&#28982;&#22522;&#20110;&#20803;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#24182;&#26410;&#36827;&#19968;&#27493;&#25552;&#39640;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#20294;&#23427;&#30830;&#23454;&#20135;&#29983;&#20102;&#26377;&#36259;&#30340;&#23884;&#20837;&#32858;&#31867;&#21487;&#20998;&#24615;&#12290;&#25152;&#26377;&#39046;&#22495;&#30340;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19802v1 Announce Type: cross  Abstract: Pre-trained Large Language Models (LLMs) often struggle on out-of-domain datasets like healthcare focused text. We explore specialized pre-training to adapt smaller LLMs to different healthcare datasets. Three methods are assessed: traditional masked language modeling, Deep Contrastive Learning for Unsupervised Textual Representations (DeCLUTR), and a novel pre-training objective utilizing metadata categories from the healthcare settings. These schemes are evaluated on downstream document classification tasks for each dataset, with additional analysis of the resultant embedding spaces. Contrastively trained models outperform other approaches on the classification tasks, delivering strong performance from limited labeled data and with fewer model parameter updates required. While metadata-based pre-training does not further improve classifications across the datasets, it yields interesting embedding cluster separability. All domain adap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;2024&#24180;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#30340;&#37327;&#23376;&#35745;&#31639;&#24212;&#29992;&#65292;&#22312;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#20102;&#35832;&#22914;&#35789;&#23884;&#20837;&#12289;&#24207;&#21015;&#27169;&#22411;&#12289;&#27880;&#24847;&#21147;&#21644;&#35821;&#27861;&#20998;&#26512;&#31561;NLP&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#35774;&#35745;&#26469;&#22788;&#29702;&#25991;&#26412;&#32534;&#30721;&#65292;&#24182;&#25506;&#35752;&#20102;&#37327;&#23376;&#29702;&#35770;&#23545;&#8220;&#19981;&#30830;&#23450;&#24615;&#26159;&#20160;&#20040;&#65311;&#8221;&#21644;&#8220;&#26234;&#33021;&#26159;&#20160;&#20040;&#65311;&#8221;&#31561;&#26680;&#24515;&#38382;&#39064;&#30340;&#20851;&#38190;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.19758</link><description>&lt;p&gt;
2024&#24180;&#30340;&#33258;&#28982;&#35821;&#35328;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#37327;&#23376;&#35745;&#31639;&#65306;QNLP&#20013;&#30340;&#30740;&#31350;&#35201;&#28857;&#21644;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Natural Language, AI, and Quantum Computing in 2024: Research Ingredients and Directions in QNLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19758
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;2024&#24180;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#30340;&#37327;&#23376;&#35745;&#31639;&#24212;&#29992;&#65292;&#22312;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#20102;&#35832;&#22914;&#35789;&#23884;&#20837;&#12289;&#24207;&#21015;&#27169;&#22411;&#12289;&#27880;&#24847;&#21147;&#21644;&#35821;&#27861;&#20998;&#26512;&#31561;NLP&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#35774;&#35745;&#26469;&#22788;&#29702;&#25991;&#26412;&#32534;&#30721;&#65292;&#24182;&#25506;&#35752;&#20102;&#37327;&#23376;&#29702;&#35770;&#23545;&#8220;&#19981;&#30830;&#23450;&#24615;&#26159;&#20160;&#20040;&#65311;&#8221;&#21644;&#8220;&#26234;&#33021;&#26159;&#20160;&#20040;&#65311;&#8221;&#31561;&#26680;&#24515;&#38382;&#39064;&#30340;&#20851;&#38190;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19758v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25277;&#35937;&#65306;&#35821;&#35328;&#22788;&#29702;&#26159;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#20851;&#38190;&#65292;&#21516;&#26102;&#37327;&#23376;&#35745;&#31639;&#20063;&#24320;&#22987;&#24212;&#29992;&#12290;&#36825;&#24341;&#36215;&#20102;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#20986;&#29616;&#20102;&#20960;&#20010;&#26089;&#26399;&#25552;&#26696;&#21644;&#23454;&#39564;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;NLP&#30456;&#20851;&#25216;&#26415;&#65292;&#21253;&#25324;&#35789;&#23884;&#20837;&#12289;&#24207;&#21015;&#27169;&#22411;&#12289;&#27880;&#24847;&#21147;&#21644;&#35821;&#27861;&#20998;&#26512;&#26159;&#22914;&#20309;&#24212;&#29992;&#20110;&#37327;&#23376;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#35774;&#35745;&#29992;&#20110;&#25991;&#26412;&#32534;&#30721;&#30340;&#22522;&#26412;&#20219;&#21153;&#65288;&#22312;&#20869;&#23384;&#20013;&#34920;&#31034;&#19968;&#20010;&#23383;&#31526;&#20018;&#65289;&#65292;&#36825;&#22312;&#20197;&#21069;&#27809;&#26377;&#35814;&#32454;&#35752;&#35770;&#36807;&#12290;&#38500;&#20102;&#25512;&#21160;&#26032;&#25216;&#26415;&#65292;&#37327;&#23376;&#29702;&#35770;&#36824;&#23545;&#8220;&#19981;&#30830;&#23450;&#24615;&#26159;&#20160;&#20040;&#65311;&#8221;&#21644;&#8220;&#26234;&#33021;&#26159;&#20160;&#20040;&#65311;&#8221;&#31561;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20570;&#20986;&#20102;&#20851;&#38190;&#36129;&#29486;&#12290;&#38543;&#30528;&#36825;&#20123;&#38382;&#39064;&#22312;&#20154;&#24037;&#31995;&#32479;&#20013;&#21464;&#24471;&#24840;&#21457;&#32039;&#36843;&#65292;&#26412;&#25991;&#36824;&#32771;&#34385;&#20102;&#19968;&#20123;&#20107;&#23454;&#27010;&#24565;&#21270;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19758v1 Announce Type: cross  Abstract: Language processing is at the heart of current developments in artificial intelligence, and quantum computers are becoming available at the same time. This has led to great interest in quantum natural language processing, and several early proposals and experiments. This paper surveys the state of this area, showing how NLP-related techniques including word embeddings, sequential models, attention, and grammatical parsing have been used in quantum language processing. We introduce a new quantum design for the basic task of text encoding (representing a string of characters in memory), which has not been addressed in detail before.   As well as motivating new technologies, quantum theory has made key contributions to the challenging questions of 'What is uncertainty?' and 'What is intelligence?' As these questions are taking on fresh urgency with artificial systems, the paper also considers some of the ways facts are conceptualized and 
&lt;/p&gt;</description></item><item><title>GOLD&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#26080;&#20851;&#30340;&#25968;&#25454;&#29983;&#25104;&#21644;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#36229;&#20986;&#20998;&#24067;&#24341;&#23548;&#30340;&#21453;&#39304;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#22788;&#29702;&#22024;&#26434;&#29983;&#25104;&#25968;&#25454;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;OOD&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19754</link><description>&lt;p&gt;
GOLD: &#36890;&#36807;&#36229;&#20986;&#20998;&#24067;&#24341;&#23548;&#30340;&#35821;&#35328;&#25968;&#25454;&#29983;&#25104;&#23454;&#29616;&#36890;&#29992;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19754
&lt;/p&gt;
&lt;p&gt;
GOLD&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#26080;&#20851;&#30340;&#25968;&#25454;&#29983;&#25104;&#21644;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#36229;&#20986;&#20998;&#24067;&#24341;&#23548;&#30340;&#21453;&#39304;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#22788;&#29702;&#22024;&#26434;&#29983;&#25104;&#25968;&#25454;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;OOD&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19754v1 &#36890;&#21578;&#31867;&#22411;&#65306;&#26032; &#25277;&#35937;&#65306;&#20174;LLMs&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#23545;&#20110;&#26377;&#25928;&#37096;&#32626;&#35821;&#35328;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;LLMs&#29983;&#25104;&#25968;&#25454;&#26469;&#20934;&#22791;&#33976;&#39311;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#20351;&#29992;LLMs&#29983;&#25104;&#25968;&#25454;&#23481;&#26131;&#20174;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#20013;&#24515;&#36827;&#34892;&#25277;&#26679;&#12290;&#36825;&#31181;&#23616;&#38480;&#24615;&#38459;&#30861;&#20102;&#33976;&#39311;&#27169;&#22411;&#23398;&#20064;&#30495;&#23454;&#30340;&#28508;&#22312;&#25968;&#25454;&#20998;&#24067;&#24182;&#19988;&#36951;&#24536;&#20998;&#24067;&#30340;&#23614;&#37096;&#65288;&#20855;&#26377;&#36739;&#20302;&#27010;&#29575;&#30340;&#26679;&#26412;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GOLD&#65292;&#19968;&#20010;&#20219;&#21153;&#26080;&#20851;&#30340;&#25968;&#25454;&#29983;&#25104;&#21644;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#36845;&#20195;&#30340;&#36229;&#20986;&#20998;&#24067;&#24341;&#23548;&#21453;&#39304;&#26426;&#21046;&#29992;&#20110;LLM&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#25552;&#39640;&#20102;&#33976;&#39311;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;OOD&#35780;&#20272;&#26041;&#27861;&#26469;&#22788;&#29702;&#22024;&#26434;&#30340;&#29983;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;NLP&#30340;10&#20010;&#19981;&#21516;&#20998;&#31867;&#21644;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;GOLD respe
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19754v1 Announce Type: new  Abstract: Knowledge distillation from LLMs is essential for the efficient deployment of language models. Prior works have proposed data generation using LLMs for preparing distilled models. We argue that generating data with LLMs is prone to sampling mainly from the center of original content distribution. This limitation hinders the distilled model from learning the true underlying data distribution and to forget the tails of the distributions (samples with lower probability). To this end, we propose GOLD, a task-agnostic data generation and knowledge distillation framework, which employs an iterative out-of-distribution-guided feedback mechanism for the LLM. As a result, the generated data improves the generalizability of distilled models. An energy-based OOD evaluation approach is also introduced to deal with noisy generated data. Our extensive experiments on 10 different classification and sequence-to-sequence tasks in NLP show that GOLD respe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#32599;&#39532;&#21270;&#20711;&#20285;&#32599;&#35821;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#24335;&#12289;&#24773;&#32490;&#21644;&#34892;&#20026;&#32447;&#32034;&#21487;&#39640;&#20934;&#30830;&#29575;&#33258;&#21160;&#31579;&#26597;&#25233;&#37057;&#30151;&#30151;&#29366;&#65292;&#26174;&#33879;&#36229;&#36234;&#20102;&#24403;&#21069;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#30830;&#23450;&#38656;&#31215;&#26497;&#24178;&#39044;&#21644;&#25903;&#25345;&#30340;&#20010;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.19728</link><description>&lt;p&gt;
EmoScan&#65306;&#32599;&#39532;&#21270;&#20711;&#20285;&#32599;&#35821;&#25512;&#25991;&#20013;&#25233;&#37057;&#30151;&#30151;&#29366;&#30340;&#33258;&#21160;&#31579;&#26597;
&lt;/p&gt;
&lt;p&gt;
EmoScan: Automatic Screening of Depression Symptoms in Romanized Sinhala Tweets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19728
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#32599;&#39532;&#21270;&#20711;&#20285;&#32599;&#35821;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#24335;&#12289;&#24773;&#32490;&#21644;&#34892;&#20026;&#32447;&#32034;&#21487;&#39640;&#20934;&#30830;&#29575;&#33258;&#21160;&#31579;&#26597;&#25233;&#37057;&#30151;&#30151;&#29366;&#65292;&#26174;&#33879;&#36229;&#36234;&#20102;&#24403;&#21069;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#30830;&#23450;&#38656;&#31215;&#26497;&#24178;&#39044;&#21644;&#25903;&#25345;&#30340;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#32599;&#39532;&#21270;&#20711;&#20285;&#32599;&#35821;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#35782;&#21035;&#23384;&#22312;&#25233;&#37057;&#39118;&#38505;&#30340;&#20010;&#20307;&#12290;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#30340;&#35821;&#35328;&#27169;&#24335;&#12289;&#24773;&#32490;&#21644;&#34892;&#20026;&#32447;&#32034;&#65292;&#22312;&#19968;&#22871;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#31579;&#26597;&#25233;&#37057;&#30151;&#30151;&#29366;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;&#31070;&#32463;&#32593;&#32476;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36866;&#29992;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#27880;&#24847;&#21147;&#23618;&#65292;&#33021;&#22788;&#29702;&#38271;&#24207;&#21015;&#25968;&#25454;&#65292;&#22312;&#26816;&#27979;&#25233;&#37057;&#30151;&#29366;&#26041;&#38754;&#36798;&#21040;&#20102;93.25%&#30340;&#26174;&#33879;&#20934;&#30830;&#29575;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#35813;&#26041;&#27861;&#22312;&#30830;&#23450;&#38656;&#35201;&#31215;&#26497;&#24178;&#39044;&#21644;&#25903;&#25345;&#30340;&#20010;&#20307;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#24515;&#29702;&#20581;&#24247;&#19987;&#19994;&#20154;&#22763;&#12289;&#20915;&#31574;&#32773;&#21644;&#31038;&#20132;&#23186;&#20307;&#20844;&#21496;&#21487;&#20197;&#33719;&#24471;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19728v1 Announce Type: new  Abstract: This work explores the utilization of Romanized Sinhala social media data to identify individuals at risk of depression. A machine learning-based framework is presented for the automatic screening of depression symptoms by analyzing language patterns, sentiment, and behavioural cues within a comprehensive dataset of social media posts. The research has been carried out to compare the suitability of Neural Networks over the classical machine learning techniques. The proposed Neural Network with an attention layer which is capable of handling long sequence data, attains a remarkable accuracy of 93.25% in detecting depression symptoms, surpassing current state-of-the-art methods. These findings underscore the efficacy of this approach in pinpointing individuals in need of proactive interventions and support. Mental health professionals, policymakers, and social media companies can gain valuable insights through the proposed model. Leveragin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#35821;&#21475;&#35821;&#29702;&#35299;&#30340;&#26032;&#35821;&#20041;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;MEDIA&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#26631;&#27880;&#24847;&#22270;&#26469;&#25193;&#23637;&#20854;&#29992;&#36884;&#21644;&#20351;&#29992;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.19727</link><description>&lt;p&gt;
&#29992;&#20110;&#27861;&#35821;&#21475;&#35821;&#29702;&#35299;&#30340;&#26032;&#35821;&#20041;&#20219;&#21153;MEDIA&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
New Semantic Task for the French Spoken Language Understanding MEDIA Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19727
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#35821;&#21475;&#35821;&#29702;&#35299;&#30340;&#26032;&#35821;&#20041;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;MEDIA&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#26631;&#27880;&#24847;&#22270;&#26469;&#25193;&#23637;&#20854;&#29992;&#36884;&#21644;&#20351;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#20998;&#31867;&#21644;&#27133;&#22635;&#20805;&#26159;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#22312;&#22823;&#22810;&#25968;SLU&#31995;&#32479;&#20013;&#65292;&#36825;&#20123;&#20219;&#21153;&#30001;&#29420;&#31435;&#27169;&#22359;&#23454;&#29616;&#12290;&#36817;&#21313;&#20116;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#21516;&#26102;&#23454;&#29616;&#36825;&#20004;&#20010;&#20219;&#21153;&#24182;&#21033;&#29992;&#23427;&#20204;&#30456;&#20114;&#22686;&#24378;&#30340;&#27169;&#22411;&#12290;&#19968;&#20010;&#20351;&#29992;&#32852;&#21512;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#27169;&#22359;&#34987;&#35774;&#24819;&#29992;&#20110;&#20026;&#19968;&#20010;&#27431;&#27954;&#39033;&#30446;HumanE-AI-Net&#21019;&#24314;&#19968;&#20010;&#26053;&#28216;&#23545;&#35805;&#31995;&#32479;&#12290;&#24314;&#35758;&#32467;&#21512;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;MEDIA&#25968;&#25454;&#38598;&#65292;&#26469;&#35757;&#32451;&#36825;&#20010;&#32852;&#21512;&#27169;&#22411;&#12290;MEDIA SLU&#25968;&#25454;&#38598;&#26159;&#30001;ELRA&#20174;2005&#24180;&#24320;&#22987;&#20998;&#21457;&#30340;&#27861;&#35821;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#34987;&#27861;&#22269;&#30740;&#31350;&#30028;&#20351;&#29992;&#65292;&#33258;2020&#24180;&#36215;&#29992;&#20110;&#23398;&#26415;&#30740;&#31350;&#20813;&#36153;&#20351;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#21482;&#22312;&#27133;&#19978;&#26631;&#27880;&#32780;&#19981;&#26631;&#27880;&#24847;&#22270;&#12290;&#24050;&#26500;&#24314;&#20102;&#19968;&#20010;&#24102;&#26377;&#24847;&#22270;&#26631;&#27880;&#30340;&#22686;&#24378;&#29256;&#26412;&#30340;MEDIA&#65292;&#20197;&#25193;&#23637;&#20854;&#29992;&#36884;&#21040;&#26356;&#22810;&#20219;&#21153;&#21644;&#29992;&#20363;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#33719;&#24471;&#27492;&#22686;&#24378;&#29256;&#26412;&#30340;&#21322;&#33258;&#21160;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19727v1 Announce Type: cross  Abstract: Intent classification and slot-filling are essential tasks of Spoken Language Understanding (SLU). In most SLUsystems, those tasks are realized by independent modules. For about fifteen years, models achieving both of themjointly and exploiting their mutual enhancement have been proposed. A multilingual module using a joint modelwas envisioned to create a touristic dialogue system for a European project, HumanE-AI-Net. A combination ofmultiple datasets, including the MEDIA dataset, was suggested for training this joint model. The MEDIA SLU datasetis a French dataset distributed since 2005 by ELRA, mainly used by the French research community and free foracademic research since 2020. Unfortunately, it is annotated only in slots but not intents. An enhanced version ofMEDIA annotated with intents has been built to extend its use to more tasks and use cases. This paper presents thesemi-automatic methodology used to obtain this enhanced ver
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#29983;&#29289;&#21307;&#23398;&#27861;&#35821;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23545;&#20960;&#31181;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2403.19726</link><description>&lt;p&gt;
&#27861;&#35821;&#20020;&#24202;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Evaluation of Clinical Named Entity Recognition in French
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#29983;&#29289;&#21307;&#23398;&#27861;&#35821;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23545;&#20960;&#31181;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#21560;&#24341;&#20102;&#25345;&#32493;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#22312;&#29305;&#23450;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#25110;&#24494;&#35843;&#26469;&#36866;&#24212;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#23376;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#27604;&#29616;&#20195;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26356;&#36731;&#12290;&#26368;&#36817;&#65292;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21457;&#24067;&#20102;&#20960;&#20010;MLMs&#65292;&#24182;&#19988;&#23454;&#39564;&#34920;&#26126;&#23427;&#20204;&#20248;&#20110;&#26631;&#20934;&#27861;&#35821;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#25552;&#20379;&#23545;&#21516;&#19968;&#35821;&#26009;&#24211;&#19978;&#25152;&#26377;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#30340;&#31995;&#32479;&#35780;&#20272;&#12290;&#30446;&#26631;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#27861;&#35821;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#26041;&#27861;&#21644;&#26448;&#26009;&#65306;&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;CamemBERT-bio&#21644;DrBERT&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#26631;&#20934;&#27861;&#35821;&#27169;&#22411;CamemBERT&#12289;FlauBERT&#21644;FrALBERT&#20197;&#21450;&#22810;&#35821;&#35328;mBERT&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19726v1 Announce Type: cross  Abstract: Background: Transformer-based language models have shown strong performance on many Natural LanguageProcessing (NLP) tasks. Masked Language Models (MLMs) attract sustained interest because they can be adaptedto different languages and sub-domains through training or fine-tuning on specific corpora while remaining lighterthan modern Large Language Models (LLMs). Recently, several MLMs have been released for the biomedicaldomain in French, and experiments suggest that they outperform standard French counterparts. However, nosystematic evaluation comparing all models on the same corpora is available. Objective: This paper presentsan evaluation of masked language models for biomedical French on the task of clinical named entity recognition.Material and methods: We evaluate biomedical models CamemBERT-bio and DrBERT and compare them tostandard French models CamemBERT, FlauBERT and FrALBERT as well as multilingual mBERT using three publicall
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20843;&#31181;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#31867;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21478;&#22806;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#26356;&#30701;&#12289;&#35789;&#27719;&#21464;&#21270;&#26356;&#23569;&#65292;&#32780;&#20154;&#31867;&#29983;&#25104;&#20869;&#23481;&#21017;&#20351;&#29992;&#20102;&#19968;&#20123;&#29305;&#23450;&#39046;&#22495;&#30456;&#20851;&#20851;&#38190;&#35789;&#34987;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24573;&#30053;&#20102;&#12290;</title><link>https://arxiv.org/abs/2403.19725</link><description>&lt;p&gt;
MUGC&#65306;&#26426;&#22120;&#29983;&#25104;&#19982;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MUGC: Machine Generated versus User Generated Content Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20843;&#31181;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#31867;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21478;&#22806;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#26356;&#30701;&#12289;&#35789;&#27719;&#21464;&#21270;&#26356;&#23569;&#65292;&#32780;&#20154;&#31867;&#29983;&#25104;&#20869;&#23481;&#21017;&#20351;&#29992;&#20102;&#19968;&#20123;&#29305;&#23450;&#39046;&#22495;&#30456;&#20851;&#20851;&#38190;&#35789;&#34987;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24573;&#30053;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20808;&#36827;&#30340;&#29616;&#20195;&#31995;&#32479;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#22686;&#24378;&#20854;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#21644;&#36924;&#30495;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#21306;&#20998;&#29992;&#25143;&#29983;&#25104;&#19982;&#26426;&#22120;&#29983;&#25104;&#20869;&#23481;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20843;&#31181;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#35780;&#20272;&#65292;&#20197;&#21306;&#20998;&#36328;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#65288;&#35799;&#27468;&#12289;&#25688;&#35201;&#21644;&#35770;&#25991;&#65289;&#20013;&#30340;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#31867;&#29983;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#21453;&#26144;&#20102;&#20687;RoBERT&#36825;&#26679;&#30340;&#28909;&#38376;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#19982;&#20154;&#31867;&#29983;&#25104;&#20869;&#23481;&#30456;&#27604;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#26356;&#30701;&#65292;&#35789;&#27719;&#21464;&#21270;&#26356;&#23569;&#12290;&#34429;&#28982;&#20154;&#31867;&#24120;&#29992;&#30340;&#29305;&#23450;&#39046;&#22495;&#30456;&#20851;&#20851;&#38190;&#35789;&#34987;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24573;&#30053;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19725v1 Announce Type: cross  Abstract: As advanced modern systems like deep neural networks (DNNs) and generative AI continue to enhance their capabilities in producing convincing and realistic content, the need to distinguish between user-generated and machine generated content is becoming increasingly evident. In this research, we undertake a comparative evaluation of eight traditional machine-learning algorithms to distinguish between machine-generated and human-generated data across three diverse datasets: Poems, Abstracts, and Essays. Our results indicate that traditional methods demonstrate a high level of accuracy in identifying machine-generated data, reflecting the documented effectiveness of popular pre-trained models like RoBERT. We note that machine-generated texts tend to be shorter and exhibit less word variety compared to human-generated content. While specific domain-related keywords commonly utilized by humans, albeit disregarded by current LLMs (Large Lang
&lt;/p&gt;</description></item><item><title>HGT&#26694;&#26550;&#32467;&#21512;&#20102;&#24322;&#36136;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#21644;&#22810;&#31890;&#24230;&#33258;&#30417;&#30563;HG&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19723</link><description>&lt;p&gt;
HGT&#65306;&#21033;&#29992;&#24322;&#36136;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19723
&lt;/p&gt;
&lt;p&gt;
HGT&#26694;&#26550;&#32467;&#21512;&#20102;&#24322;&#36136;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#21644;&#22810;&#31890;&#24230;&#33258;&#30417;&#30563;HG&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#29702;&#35299; (TU) &#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#38754;&#20020;&#25163;&#21160;&#26631;&#35760;&#34920;&#26684;&#30340;&#31232;&#32570;&#24615;&#21644;&#22797;&#26434;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; HGT &#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#24322;&#36136;&#22270; (HG) &#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM)&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412; TU &#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#36719;&#25552;&#31034;&#21644;&#25351;&#23548;&#36716;&#25442;&#23558;&#34920;&#26684;&#35821;&#20041;&#19982;LLM&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#28041;&#21450;&#19977;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#33258;&#30417;&#30563;HG&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26041;&#26696;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36890;&#36807;&#23454;&#35777;&#26041;&#27861;&#23637;&#31034;&#20102;HGT&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#23427;&#22312;&#23569;&#26679;&#26412;&#22797;&#26434;TU&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19723v1 Announce Type: cross  Abstract: Table understanding (TU) has achieved promising advancements, but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table structures.To address these challenges, we propose HGT, a framework with a heterogeneous graph (HG)-enhanced large language model (LLM) to tackle few-shot TU tasks.It leverages the LLM by aligning the table semantics with the LLM's parametric knowledge through soft prompts and instruction turning and deals with complex tables by a multi-task pre-training scheme involving three novel multi-granularity self-supervised HG pre-training objectives.We empirically demonstrate the effectiveness of HGT, showing that it outperforms the SOTA for few-shot complex TU on several benchmarks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#20132;&#20114;&#26085;&#24535;&#30340;&#29992;&#25143;&#37325;&#32452;&#25968;&#25454;&#26469;&#24320;&#21457;&#33258;&#21160;&#25552;&#31034;&#37325;&#32452;&#27169;&#22411;&#65292;CAPR&#26694;&#26550;&#21019;&#26032;&#24615;&#22320;&#23558;&#29992;&#25143;&#33021;&#21147;&#25972;&#21512;&#21040;&#25552;&#31034;&#37325;&#32452;&#36807;&#31243;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.19716</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#33021;&#21147;&#24863;&#30693;&#25552;&#31034;&#37325;&#32452;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Capability-aware Prompt Reformulation Learning for Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19716
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#20132;&#20114;&#26085;&#24535;&#30340;&#29992;&#25143;&#37325;&#32452;&#25968;&#25454;&#26469;&#24320;&#21457;&#33258;&#21160;&#25552;&#31034;&#37325;&#32452;&#27169;&#22411;&#65292;CAPR&#26694;&#26550;&#21019;&#26032;&#24615;&#22320;&#23558;&#29992;&#25143;&#33021;&#21147;&#25972;&#21512;&#21040;&#25552;&#31034;&#37325;&#32452;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#33402;&#26415;&#21019;&#20316;&#39046;&#22495;&#20013;&#30340;&#38761;&#21629;&#24615;&#24037;&#20855;&#65292;&#20026;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#35270;&#35273;&#33402;&#26415;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20415;&#21033;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#25928;&#21147;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#25552;&#31034;&#36136;&#37327;&#23494;&#20999;&#30456;&#20851;&#65292;&#36825;&#24120;&#24120;&#23545;&#19981;&#29087;&#24713;&#25552;&#31034;&#21046;&#20316;&#30340;&#29992;&#25143;&#26500;&#25104;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#20132;&#20114;&#26085;&#24535;&#30340;&#29992;&#25143;&#37325;&#32452;&#25968;&#25454;&#26469;&#24320;&#21457;&#33258;&#21160;&#25552;&#31034;&#37325;&#32452;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#26085;&#24535;&#30340;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#29992;&#25143;&#25552;&#31034;&#30340;&#37325;&#32452;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20010;&#20307;&#29992;&#25143;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#37325;&#32452;&#23545;&#30340;&#36136;&#37327;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20026;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33021;&#21147;&#24863;&#30693;&#25552;&#31034;&#37325;&#32452;&#65288;CAPR&#65289;&#26694;&#26550;&#12290;CAPR&#21019;&#26032;&#24615;&#22320;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#23558;&#29992;&#25143;&#33021;&#21147;&#25972;&#21512;&#21040;&#37325;&#32452;&#36807;&#31243;&#20013;&#65306;&#26377;&#26465;&#20214;&#30340;&#25552;&#31034;&#37325;&#32452;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19716v1 Announce Type: cross  Abstract: Text-to-image generation systems have emerged as revolutionary tools in the realm of artistic creation, offering unprecedented ease in transforming textual prompts into visual art. However, the efficacy of these systems is intricately linked to the quality of user-provided prompts, which often poses a challenge to users unfamiliar with prompt crafting. This paper addresses this challenge by leveraging user reformulation data from interaction logs to develop an automatic prompt reformulation model. Our in-depth analysis of these logs reveals that user prompt reformulation is heavily dependent on the individual user's capability, resulting in significant variance in the quality of reformulation pairs. To effectively use this data for training, we introduce the Capability-aware Prompt Reformulation (CAPR) framework. CAPR innovatively integrates user capability into the reformulation process through two key components: the Conditional Refo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;TRAC-2024&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#26631;&#27880;&#30340;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#21361;&#23475;&#21487;&#33021;&#24615;&#24182;&#35782;&#21035;&#30446;&#26631;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#36187;&#36947;&#20013;&#21462;&#24471;&#31532;&#20108;&#21517;&#30340;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2403.19713</link><description>&lt;p&gt;
NJUST-KMG&#21442;&#21152;TRAC-2024&#20219;&#21153;1&#21644;&#20219;&#21153;2&#65306;&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
NJUST-KMG at TRAC-2024 Tasks 1 and 2: Offline Harm Potential Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;TRAC-2024&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#26631;&#27880;&#30340;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#21361;&#23475;&#21487;&#33021;&#24615;&#24182;&#35782;&#21035;&#30446;&#26631;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#36187;&#36947;&#20013;&#21462;&#24471;&#31532;&#20108;&#21517;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;TRAC-2024&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;&#20013;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#35813;&#27604;&#36187;&#21253;&#21547;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;&#30740;&#31350;&#21033;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#30340;&#20016;&#23500;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#35780;&#20998;&#26631;&#27880;&#65292;&#20197;&#25429;&#25417;&#31163;&#32447;&#29615;&#22659;&#21361;&#23475;&#30340;&#24494;&#22937;&#21547;&#20041;&#12290;&#21442;&#19982;&#32773;&#30340;&#20219;&#21153;&#26159;&#35774;&#35745;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#29305;&#23450;&#24773;&#20917;&#19979;&#21361;&#23475;&#21487;&#33021;&#24615;&#24182;&#35782;&#21035;&#31163;&#32447;&#21361;&#23475;&#26368;&#21487;&#33021;&#30340;&#30446;&#26631;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#36187;&#36947;&#20013;&#25490;&#21517;&#31532;&#20108;&#65292;F1&#20540;&#20998;&#21035;&#20026;0.73&#21644;0.96&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#36873;&#25321;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25972;&#21512;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#26041;&#27861;&#24212;&#29992;&#20110;&#27979;&#35797;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19713v1 Announce Type: new  Abstract: This report provide a detailed description of the method that we proposed in the TRAC-2024 Offline Harm Potential dentification which encloses two sub-tasks. The investigation utilized a rich dataset comprised of social media comments in several Indian languages, annotated with precision by expert judges to capture the nuanced implications for offline context harm. The objective assigned to the participants was to design algorithms capable of accurately assessing the likelihood of harm in given situations and identifying the most likely target(s) of offline harm. Our approach ranked second in two separate tracks, with F1 values of 0.73 and 0.96 respectively. Our method principally involved selecting pretrained models for finetuning, incorporating contrastive learning techniques, and culminating in an ensemble approach for the test set.
&lt;/p&gt;</description></item><item><title>STRUM-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#23646;&#24615;&#21270;&#12289;&#32467;&#26500;&#21270;&#21644;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#24182;&#31361;&#20986;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#25110;&#22266;&#23450;&#23646;&#24615;&#21015;&#34920;&#65292;&#20855;&#26377;&#39640;&#21534;&#21520;&#37327;&#21644;&#23567;&#20307;&#31215;&#12290;</title><link>https://arxiv.org/abs/2403.19710</link><description>&lt;p&gt;
STRUM-LLM: &#23646;&#24615;&#21270;&#21644;&#32467;&#26500;&#21270;&#23545;&#27604;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
STRUM-LLM: Attributed and Structured Contrastive Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19710
&lt;/p&gt;
&lt;p&gt;
STRUM-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#23646;&#24615;&#21270;&#12289;&#32467;&#26500;&#21270;&#21644;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#24182;&#31361;&#20986;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#25110;&#22266;&#23450;&#23646;&#24615;&#21015;&#34920;&#65292;&#20855;&#26377;&#39640;&#21534;&#21520;&#37327;&#21644;&#23567;&#20307;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#32463;&#24120;&#22312;&#20004;&#20010;&#36873;&#39033;&#65288;A vs B&#65289;&#20043;&#38388;&#20570;&#20915;&#31574;&#26102;&#24863;&#21040;&#22256;&#38590;&#65292;&#22240;&#20026;&#36825;&#36890;&#24120;&#38656;&#35201;&#22312;&#22810;&#20010;&#32593;&#39029;&#19978;&#36827;&#34892;&#32791;&#26102;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STRUM-LLM&#65292;&#36890;&#36807;&#29983;&#25104;&#24102;&#23646;&#24615;&#12289;&#32467;&#26500;&#21270;&#21644;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#25688;&#35201;&#65292;&#31361;&#20986;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;STRUM-LLM&#35782;&#21035;&#20102;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#65306;&#20004;&#20010;&#36873;&#39033;&#22312;&#21738;&#20123;&#29305;&#23450;&#23646;&#24615;&#19978;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#20197;&#21450;&#26368;&#26377;&#21487;&#33021;&#24433;&#21709;&#29992;&#25143;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#65292;&#24182;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#25110;&#22266;&#23450;&#23646;&#24615;&#21015;&#34920;&#20316;&#20026;&#30417;&#30563;&#12290;STRUM-LLM&#23558;&#25152;&#26377;&#25552;&#21462;&#30340;&#20869;&#23481;&#23646;&#24615;&#21270;&#65292;&#20197;&#21450;&#25991;&#26412;&#35777;&#25454;&#65292;&#19988;&#19981;&#38480;&#21046;&#20854;&#22788;&#29702;&#30340;&#36755;&#20837;&#26469;&#28304;&#30340;&#38271;&#24230;&#12290;STRUM-LLM Distilled&#30340;&#21534;&#21520;&#37327;&#27604;&#20855;&#26377;&#30456;&#20284;&#24615;&#33021;&#30340;&#27169;&#22411;&#39640;100&#20493;&#65292;&#21516;&#26102;&#20307;&#31215;&#23567;10&#20493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19710v1 Announce Type: cross  Abstract: Users often struggle with decision-making between two options (A vs B), as it usually requires time-consuming research across multiple web pages. We propose STRUM-LLM that addresses this challenge by generating attributed, structured, and helpful contrastive summaries that highlight key differences between the two options. STRUM-LLM identifies helpful contrast: the specific attributes along which the two options differ significantly and which are most likely to influence the user's decision. Our technique is domain-agnostic, and does not require any human-labeled data or fixed attribute list as supervision. STRUM-LLM attributes all extractions back to the input sources along with textual evidence, and it does not have a limit on the length of input sources that it can process. STRUM-LLM Distilled has 100x more throughput than the models with comparable performance while being 10x smaller. In this paper, we provide extensive evaluations
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36882;&#24402;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#36866;&#37197;&#22330;&#26223;&#19979;&#38477;&#20302;&#27599;&#20010;&#20219;&#21153;&#30340;&#21442;&#25968;&#24320;&#38144;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#36866;&#37197;&#22120;&#26041;&#27861;&#21644;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#22522;&#32447;</title><link>https://arxiv.org/abs/2403.19709</link><description>&lt;p&gt;
&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#22823;&#22411;&#35821;&#38899;&#27169;&#22411;&#30340;&#20998;&#23618;&#36882;&#24402;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36882;&#24402;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#36866;&#37197;&#22330;&#26223;&#19979;&#38477;&#20302;&#27599;&#20010;&#20219;&#21153;&#30340;&#21442;&#25968;&#24320;&#38144;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#36866;&#37197;&#22120;&#26041;&#27861;&#21644;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#37197;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35757;&#32451;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#20851;&#38190;&#26426;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#36866;&#37197;&#22330;&#26223;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#36866;&#37197;&#22120;&#22312;&#36866;&#37197;&#22120;&#21442;&#25968;&#20998;&#37197;&#26041;&#38754;&#26159;&#20998;&#23618;&#30340;&#12290;&#36866;&#37197;&#22120;&#30001;&#19968;&#20010;&#20849;&#20139;&#30340;&#25511;&#21046;&#32593;&#32476;&#21644;&#22810;&#20010;&#20219;&#21153;&#32423;&#36866;&#37197;&#22120;&#22836;&#32452;&#25104;&#65292;&#20197;&#20943;&#23569;&#27599;&#20010;&#20219;&#21153;&#30340;&#21442;&#25968;&#24320;&#38144;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#36866;&#37197;&#22120;&#36824;&#26159;&#36882;&#24402;&#30340;&#65292;&#22240;&#27492;&#25972;&#20010;&#36866;&#37197;&#22120;&#21442;&#25968;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19981;&#21516;&#23618;&#20043;&#38388;&#34987;&#37325;&#29992;&#12290;&#25105;&#20204;&#30340;&#20998;&#23618;&#36882;&#24402;&#36866;&#37197;&#22120;&#65288;HRA&#65289;&#22312;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#36866;&#37197;&#35774;&#32622;&#20013;&#37117;&#20248;&#20110;&#20808;&#21069;&#30340;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#20197;&#21450;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19709v1 Announce Type: cross  Abstract: Parameter efficient adaptation methods have become a key mechanism to train large pre-trained models for downstream tasks. However, their per-task parameter overhead is considered still high when the number of downstream tasks to adapt for is large. We introduce an adapter module that has a better efficiency in large scale multi-task adaptation scenario. Our adapter is hierarchical in terms of how the adapter parameters are allocated. The adapter consists of a single shared controller network and multiple task-level adapter heads to reduce the per-task parameter overhead without performance regression on downstream tasks. The adapter is also recurrent so the entire adapter parameters are reused across different layers of the pre-trained model. Our Hierarchical Recurrent Adapter (HRA) outperforms the previous adapter-based approaches as well as full model fine-tuning baseline in both single and multi-task adaptation settings when evalua
&lt;/p&gt;</description></item><item><title>AttentionStore&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23454;&#29616;KV&#32531;&#23384;&#30340;&#22797;&#29992;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#37325;&#22797;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.19708</link><description>&lt;p&gt;
AttentionStore: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#20013;&#23454;&#29616;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#27880;&#24847;&#21147;&#25104;&#26412;&#25928;&#30410;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19708
&lt;/p&gt;
&lt;p&gt;
AttentionStore&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23454;&#29616;KV&#32531;&#23384;&#30340;&#22797;&#29992;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#37325;&#22797;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#36718;&#23545;&#35805;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#20114;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#37325;&#22797;&#35745;&#31639;&#21382;&#21490;&#35760;&#21495;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#65292;&#23548;&#33268;&#29616;&#26377;&#29992;&#20110;&#25191;&#34892;&#22810;&#36718;&#23545;&#35805;&#30340;LLM&#26381;&#21153;&#24341;&#25806;&#25928;&#29575;&#20302;&#19979;&#65292;&#20135;&#29983;&#39640;&#26114;&#30340;&#26381;&#21153;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;AttentionStore&#65292;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#36328;&#22810;&#36718;&#23545;&#35805;&#30340;KV&#32531;&#23384;&#22797;&#29992;&#65288;&#21363; &#27880;&#24847;&#21147;&#22797;&#29992;&#65289;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#37325;&#22797;&#35745;&#31639;&#24320;&#38144;&#12290;AttentionStore&#32500;&#25252;&#20102;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;KV&#32531;&#23384;&#31995;&#32479;&#65292;&#21033;&#29992;&#25104;&#26412;&#25928;&#30410;&#30340;&#20869;&#23384;/&#23384;&#20648;&#20171;&#36136;&#20026;&#25152;&#26377;&#35831;&#27714;&#20445;&#23384;KV&#32531;&#23384;&#12290;&#20026;&#20102;&#20943;&#23569;&#24930;&#36895;&#20171;&#36136;&#30340;KV&#32531;&#23384;&#35775;&#38382;&#24320;&#38144;&#65292;AttentionStore&#37319;&#29992;&#36880;&#23618;&#39044;&#21152;&#36733;&#21644;&#24322;&#27493;&#20445;&#23384;&#26041;&#26696;&#65292;&#23558;KV&#32531;&#23384;&#35775;&#38382;&#19982;GPU&#35745;&#31639;&#37325;&#21472;&#12290;&#20026;&#30830;&#20445;&#35201;&#35775;&#38382;&#30340;KV&#32531;&#23384;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19708v1 Announce Type: new  Abstract: Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines for executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes AttentionStore, a new attention mechanism that enables the reuse of KV caches (i.e., attention reuse) across multi-turn conversations, significantly reducing the repetitive computation overheads. AttentionStore maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, AttentionStore employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are pl
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#21363;&#20351;&#32570;&#20047;&#35270;&#35273;&#36755;&#20837;&#65292;&#21033;&#29992;&#25152;&#26377;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24674;&#22797;&#22823;&#37096;&#20998;VLM&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#35821;&#35328;&#36890;&#36807;&#25552;&#20379;&#23545;&#20808;&#21069;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#35775;&#38382;&#26469;&#23545;&#23398;&#20064;&#26032;&#20219;&#21153;&#26377;&#36129;&#29486;</title><link>https://arxiv.org/abs/2403.19669</link><description>&lt;p&gt;
&#20998;&#26512;&#35821;&#35328;&#21644;&#35270;&#35273;&#22312;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Roles of Language and Vision in Learning from Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19669
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#21363;&#20351;&#32570;&#20047;&#35270;&#35273;&#36755;&#20837;&#65292;&#21033;&#29992;&#25152;&#26377;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24674;&#22797;&#22823;&#37096;&#20998;VLM&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#35821;&#35328;&#36890;&#36807;&#25552;&#20379;&#23545;&#20808;&#21069;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#35775;&#38382;&#26469;&#23545;&#23398;&#20064;&#26032;&#20219;&#21153;&#26377;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19669v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#35821;&#35328;&#26159;&#21542;&#26377;&#21161;&#20110;&#29702;&#35299;&#35270;&#35273;&#19990;&#30028;&#65311;&#23454;&#38469;&#35266;&#23519;&#19990;&#30028;&#38656;&#35201;&#30475;&#21040;&#23454;&#38469;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#29992;&#25991;&#23383;&#25551;&#36848;&#21527;&#65311;&#20851;&#20110;&#26234;&#33021;&#26412;&#36136;&#30340;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#24456;&#38590;&#22238;&#31572;&#65292;&#22240;&#20026;&#25105;&#20204;&#21482;&#26377;&#19968;&#20010;&#26234;&#33021;&#31995;&#32479;&#30340;&#20363;&#23376;&#8212;&#8212;&#20154;&#31867;&#8212;&#8212;&#20197;&#21450;&#26377;&#38480;&#30340;&#29420;&#31435;&#35821;&#35328;&#25110;&#35270;&#35273;&#30340;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#22797;&#26434;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#25506;&#32034;&#35821;&#35328;&#21644;&#35270;&#35273;&#23545;&#20110;&#23398;&#20064;&#19990;&#30028;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#20174;&#36825;&#20123;&#27169;&#22411;&#30340;&#35748;&#30693;&#26550;&#26500;&#20013;&#20999;&#38500;&#32452;&#20214;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21033;&#29992;&#25152;&#26377;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#24674;&#22797;&#20102;&#22823;&#37096;&#20998;VLM&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#23427;&#32570;&#20047;&#35270;&#35273;&#36755;&#20837;&#65292;&#32780;&#35821;&#35328;&#20284;&#20046;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#23545;&#20808;&#21069;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#35775;&#38382;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19669v1 Announce Type: cross  Abstract: Does language help make sense of the visual world? How important is it to actually see the world rather than having it described with words? These basic questions about the nature of intelligence have been difficult to answer because we only had one example of an intelligent system -- humans -- and limited access to cases that isolated language or vision. However, the development of sophisticated Vision-Language Models (VLMs) by artificial intelligence researchers offers us new opportunities to explore the contributions that language and vision make to learning about the world. We ablate components from the cognitive architecture of these models to identify their contributions to learning new tasks from limited data. We find that a language model leveraging all components recovers a majority of a VLM's performance, despite its lack of visual input, and that language seems to allow this by providing access to prior knowledge and reasoni
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#26816;&#27979;&#24182;&#32416;&#27491;&#22269;&#23478;&#26292;&#21147;&#27515;&#20129;&#25253;&#21578;&#31995;&#32479;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#33258;&#26432;&#21361;&#26426;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19432</link><description>&lt;p&gt;
&#36890;&#36807;&#27515;&#22240;&#35843;&#26597;&#31508;&#35760;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#25581;&#31034;&#33258;&#26432;&#21407;&#22240;&#30340;&#35823;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Uncovering Misattributed Suicide Causes through Annotation Inconsistency Detection in Death Investigation Notes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19432
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#26816;&#27979;&#24182;&#32416;&#27491;&#22269;&#23478;&#26292;&#21147;&#27515;&#20129;&#25253;&#21578;&#31995;&#32479;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#33258;&#26432;&#21361;&#26426;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20934;&#30830;&#24615;&#23545;&#31185;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#22269;&#23478;&#26292;&#21147;&#27515;&#20129;&#25253;&#21578;&#31995;&#32479;&#65288;NVDRS&#65289;&#25968;&#25454;&#34987;&#24191;&#27867;&#29992;&#20110;&#21457;&#29616;&#27515;&#20129;&#30340;&#27169;&#24335;&#21644;&#21407;&#22240;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;NVDRS&#20869;&#23384;&#22312;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#38169;&#35823;&#30340;&#33258;&#26432;&#21407;&#22240;&#24402;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#26469;&#26816;&#27979;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#31867;&#20284;&#20132;&#21449;&#39564;&#35777;&#30340;&#33539;&#24335;&#26469;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;2003&#24180;&#33267;2020&#24180;&#38388;&#20174;NVDRS&#20013;&#30340;267,804&#36215;&#33258;&#26432;&#27515;&#20129;&#26696;&#20363;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23558;&#30446;&#26631;&#24030;&#30340;&#25968;&#25454;&#32435;&#20837;&#35757;&#32451;&#33258;&#26432;&#21361;&#26426;&#20998;&#31867;&#22120;&#65292;&#20351;&#24471;&#22312;&#30446;&#26631;&#24030;&#27979;&#35797;&#38598;&#19978;&#30340;F-1&#20998;&#25968;&#22686;&#21152;&#20102;5.4&#65285;&#65292;&#22312;&#20854;&#20182;&#24030;&#27979;&#35797;&#38598;&#19978;&#38477;&#20302;&#20102;1.1&#65285;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NVDRS&#27515;&#22240;&#35843;&#26597;&#31508;&#35760;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19432v1 Announce Type: cross  Abstract: Data accuracy is essential for scientific research and policy development. The National Violent Death Reporting System (NVDRS) data is widely used for discovering the patterns and causes of death. Recent studies suggested the annotation inconsistencies within the NVDRS and the potential impact on erroneous suicide-cause attributions. We present an empirical Natural Language Processing (NLP) approach to detect annotation inconsistencies and adopt a cross-validation-like paradigm to identify problematic instances. We analyzed 267,804 suicide death incidents between 2003 and 2020 from the NVDRS. Our results showed that incorporating the target state's data into training the suicide-crisis classifier brought an increase of 5.4% to the F-1 score on the target state's test set and a decrease of 1.1% on other states' test set. To conclude, we demonstrated the annotation inconsistencies in NVDRS's death investigation notes, identified problema
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#26469;&#33258;&#25105;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#38382;&#32773;&#36890;&#36807;&#35810;&#38382;&#35282;&#33394;&#25198;&#28436;&#32773;&#26469;&#24341;&#20986;&#20559;&#22909;&#65292;&#20174;&#32780;&#36845;&#20195;&#24494;&#35843;&#20197;&#22686;&#21152;&#20219;&#21153;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.19154</link><description>&lt;p&gt;
STaR-GATE: &#25945;&#25480;&#35821;&#35328;&#27169;&#22411;&#35810;&#38382;&#28548;&#28165;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
STaR-GATE: Teaching Language Models to Ask Clarifying Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19154
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#26469;&#33258;&#25105;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#38382;&#32773;&#36890;&#36807;&#35810;&#38382;&#35282;&#33394;&#25198;&#28436;&#32773;&#26469;&#24341;&#20986;&#20559;&#22909;&#65292;&#20174;&#32780;&#36845;&#20195;&#24494;&#35843;&#20197;&#22686;&#21152;&#20219;&#21153;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#20219;&#21153;&#26102;&#65292;&#29992;&#25143;&#36890;&#24120;&#20250;&#36951;&#28431;&#37325;&#35201;&#30340;&#32454;&#33410;&#12290;&#34429;&#28982;&#25552;&#38382;&#21487;&#20197;&#35299;&#20915;&#36825;&#31181;&#27495;&#20041;&#65292;&#20294;&#27169;&#22411;&#24448;&#24448;&#24456;&#38590;&#25552;&#20986;&#22909;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22870;&#21169;&#27169;&#22411;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#26469;&#33258;&#25105;&#25913;&#36827;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;STaR-GATE&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;25,500&#20010;&#29420;&#29305;&#20154;&#29289;-&#20219;&#21153;&#25552;&#31034;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#27169;&#25311;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;--&#25552;&#38382;&#32773;--&#19982;&#19968;&#20010;&#20854;&#20559;&#22909;&#26410;&#30693;&#30340;&#35282;&#33394;&#25198;&#28436;&#32773;&#20043;&#38388;&#30340;&#23545;&#35805;&#12290;&#36890;&#36807;&#25552;&#38382;&#65292;&#25552;&#38382;&#32773;&#20174;&#35282;&#33394;&#25198;&#28436;&#32773;&#37027;&#37324;&#24341;&#20986;&#20559;&#22909;&#12290;&#25552;&#38382;&#32773;&#22312;&#37027;&#20123;&#22686;&#21152;&#39640;&#36136;&#37327;&#21709;&#24212;&#27010;&#29575;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#36845;&#20195;&#24494;&#35843;&#65292;&#36825;&#20123;&#38382;&#39064;&#26159;&#30001;&#20855;&#26377;&#23545;&#35282;&#33394;&#25198;&#28436;&#32773;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35328;&#32773;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19154v1 Announce Type: cross  Abstract: When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity \citep[GATE;][]{li2023eliciting}, models often struggle to ask good questions. We explore a language model's ability to self-improve \citep[STaR;][]{zelikman2022star} by rewarding the model for generating useful questions -- a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model -- the \texttt{Questioner} -- and a \texttt{Roleplayer} whose preferences are unknown to the \texttt{Questioner}. By asking questions, the \texttt{Questioner} elicits preferences from the \texttt{Roleplayer}. The \texttt{Questioner} is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an \texttt{Oracle} with access to the \texttt{Ro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#21464;&#21387;&#22120;&#22359;&#36827;&#34892;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#65292;&#20174;&#32780;&#25552;&#39640;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17343</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#30340;&#20813;&#36153;&#21161;&#25512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models are Free Boosters for Biomedical Imaging Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#21464;&#21387;&#22120;&#22359;&#36827;&#34892;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#65292;&#20174;&#32780;&#25552;&#39640;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#26159;&#20256;&#32479;&#19978;&#32570;&#20047;&#35821;&#35328;&#25110;&#25991;&#26412;&#25968;&#25454;&#30340;&#39046;&#22495;&#12290;&#35813;&#26041;&#27861;&#19981;&#21516;&#20110;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;LLMs&#20013;&#25552;&#21462;&#30340;&#20923;&#32467;&#21464;&#21387;&#22120;&#22359;&#20316;&#20026;&#21019;&#26032;&#30340;&#32534;&#30721;&#22120;&#23618;&#65292;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#12290;&#36825;&#31181;&#31574;&#30053;&#19982;&#36890;&#24120;&#20381;&#36182;&#20110;&#35821;&#35328;&#39537;&#21160;&#25552;&#31034;&#21644;&#36755;&#20837;&#30340;&#26631;&#20934;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#26694;&#26550;&#26377;&#30528;&#26174;&#33879;&#30340;&#19981;&#21516;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;LLMs&#33021;&#22815;&#25552;&#21319;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;2D&#21644;3D&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#65292;&#20805;&#24403;&#21363;&#25554;&#21363;&#29992;&#30340;&#21161;&#25512;&#22120;&#12290;&#26356;&#26377;&#36259;&#30340;&#26159;&#65292;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#22312;M&#30340;&#24191;&#27867;&#12289;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17343v1 Announce Type: cross  Abstract: In this study, we uncover the unexpected efficacy of residual-based large language models (LLMs) as part of encoders for biomedical imaging tasks, a domain traditionally devoid of language or textual data. The approach diverges from established methodologies by utilizing a frozen transformer block, extracted from pre-trained LLMs, as an innovative encoder layer for the direct processing of visual tokens. This strategy represents a significant departure from the standard multi-modal vision-language frameworks, which typically hinge on language-driven prompts and inputs. We found that these LLMs could boost performance across a spectrum of biomedical imaging applications, including both 2D and 3D visual classification tasks, serving as plug-and-play boosters. More interestingly, as a byproduct, we found that the proposed framework achieved superior performance, setting new state-of-the-art results on extensive, standardized datasets in M
&lt;/p&gt;</description></item><item><title>WoLF&#26694;&#26550;&#25552;&#20986;&#20102;&#23545;&#20110;CXR&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#20351;&#29992;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#12289;&#37325;&#26500;&#25253;&#21578;&#20197;&#25552;&#20379;&#26356;&#26377;&#32452;&#32455;&#30340;&#20449;&#24687;&#12289;&#20197;&#21450;&#25913;&#36827;&#29983;&#25104;&#31572;&#26696;&#30340;&#32454;&#33268;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.15456</link><description>&lt;p&gt;
WoLF: &#29992;&#20110;&#33016;&#37096;X&#32447;&#22270;&#29702;&#35299;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
WoLF: Large Language Model Framework for CXR Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15456
&lt;/p&gt;
&lt;p&gt;
WoLF&#26694;&#26550;&#25552;&#20986;&#20102;&#23545;&#20110;CXR&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#20351;&#29992;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#12289;&#37325;&#26500;&#25253;&#21578;&#20197;&#25552;&#20379;&#26356;&#26377;&#32452;&#32455;&#30340;&#20449;&#24687;&#12289;&#20197;&#21450;&#25913;&#36827;&#29983;&#25104;&#31572;&#26696;&#30340;&#32454;&#33268;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#21462;&#24471;&#20102;&#23545;&#33016;&#37096;X&#32447;&#22270;(CXR)&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#30528;&#26041;&#27861;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35270;&#35273;&#38382;&#31572;(VQA)&#21644;CXR&#25253;&#21578;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CXR&#29702;&#35299;&#26694;&#26550;&#20173;&#23384;&#22312;&#20960;&#20010;&#31243;&#24207;&#19978;&#30340;&#32570;&#38519;&#12290;(1)&#20197;&#24448;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;CXR&#25253;&#21578;&#65292;&#36825;&#23545;&#20110;&#20840;&#38754;&#30340;&#35270;&#35273;&#38382;&#31572;(VQA)&#26469;&#35828;&#26159;&#19981;&#22815;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#22914;&#29992;&#33647;&#21382;&#21490;&#21644;&#20808;&#21069;&#30340;&#35786;&#26029;&#26102;&#12290;(2)&#20197;&#24448;&#30340;&#26041;&#27861;&#20351;&#29992;&#26410;&#32463;&#22788;&#29702;&#30340;CXR&#25253;&#21578;&#65292;&#36825;&#20123;&#25253;&#21578;&#24448;&#24448;&#32467;&#26500;&#38543;&#24847;&#12290;&#34429;&#28982;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29702;&#35299;&#21508;&#31181;&#25991;&#26412;&#26684;&#24335;&#65292;&#20294;&#20026;&#20102;&#25552;&#20379;&#26356;&#28165;&#26224;&#12289;&#26377;&#32452;&#32455;&#30340;&#22522;&#20110;&#35299;&#21078;&#23398;&#30340;&#20449;&#24687;&#65292;&#37325;&#26500;&#25253;&#21578;&#21487;&#33021;&#20250;&#22686;&#24378;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;(3)&#30446;&#21069;&#29992;&#20110;CXR-VQA&#30340;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#24378;&#35843;&#35821;&#35328;&#27491;&#30830;&#24615;&#65292;&#32570;&#20047;&#23545;&#29983;&#25104;&#31572;&#26696;&#30340;&#24494;&#22937;&#35780;&#20272;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15456v1 Announce Type: new  Abstract: Significant methodological strides have been made toward Chest X-ray (CXR) understanding via modern vision-language models (VLMs), demonstrating impressive Visual Question Answering (VQA) and CXR report generation abilities. However, existing CXR understanding frameworks still possess several procedural caveats. (1) Previous methods solely use CXR reports, which are insufficient for comprehensive Visual Question Answering (VQA), especially when additional health-related data like medication history and prior diagnoses are needed. (2) Previous methods use raw CXR reports, which are often arbitrarily structured. While modern language models can understand various text formats, restructuring reports for clearer, organized anatomy-based information could enhance their usefulness. (3) Current evaluation methods for CXR-VQA primarily emphasize linguistic correctness, lacking the capability to offer nuanced assessments of the generated answers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25991;&#26412;&#25968;&#25454;&#24773;&#24863;&#20998;&#31867;&#20013;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#25216;&#26415;&#22914;&#21435;&#38500;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#21487;&#33021;&#20250;&#38459;&#30861;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#36825;&#20123;&#20803;&#32032;&#20173;&#28982;&#33021;&#22815;&#20256;&#36798;&#24773;&#24863;&#25110;&#24378;&#35843;&#65292;&#32780;Transformer&#30340;&#20248;&#21183;&#22312;&#20110;&#29702;&#35299;&#25991;&#26412;&#20869;&#30340;&#35821;&#22659;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.15454</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#36827;&#34892;&#24773;&#24863;&#26816;&#27979;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Emotion Detection with Transformers: A Comparative Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25991;&#26412;&#25968;&#25454;&#24773;&#24863;&#20998;&#31867;&#20013;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#25216;&#26415;&#22914;&#21435;&#38500;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#21487;&#33021;&#20250;&#38459;&#30861;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#36825;&#20123;&#20803;&#32032;&#20173;&#28982;&#33021;&#22815;&#20256;&#36798;&#24773;&#24863;&#25110;&#24378;&#35843;&#65292;&#32780;Transformer&#30340;&#20248;&#21183;&#22312;&#20110;&#29702;&#35299;&#25991;&#26412;&#20869;&#30340;&#35821;&#22659;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#25991;&#26412;&#25968;&#25454;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#21464;&#20307;&#30340;Transformer&#23545;Emotion&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#35770;&#25991;&#36824;&#20998;&#26512;&#20102;&#19968;&#20123;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#27604;&#22914;Transformer&#23618;&#30340;&#24494;&#35843;&#12289;&#23618;&#30340;&#21487;&#35757;&#32451;&#24615;&#20197;&#21450;&#25991;&#26412;&#25968;&#25454;&#30340;&#39044;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24120;&#29992;&#25216;&#26415;&#22914;&#21435;&#38500;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#21487;&#33021;&#20250;&#38459;&#30861;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;Transformer&#30340;&#20248;&#21183;&#22312;&#20110;&#29702;&#35299;&#25991;&#26412;&#20869;&#30340;&#35821;&#22659;&#20851;&#31995;&#12290;&#20687;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#36825;&#26679;&#30340;&#20803;&#32032;&#20173;&#28982;&#21487;&#20197;&#20256;&#36798;&#24773;&#24863;&#25110;&#24378;&#35843;&#65292;&#21435;&#38500;&#23427;&#20204;&#21487;&#33021;&#20250;&#30772;&#22351;&#36825;&#31181;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15454v1 Announce Type: new  Abstract: In this study, we explore the application of transformer-based models for emotion classification on text data. We train and evaluate several pre-trained transformer models, on the Emotion dataset using different variants of transformers. The paper also analyzes some factors that in-fluence the performance of the model, such as the fine-tuning of the transformer layer, the trainability of the layer, and the preprocessing of the text data. Our analysis reveals that commonly applied techniques like removing punctuation and stop words can hinder model performance. This might be because transformers strength lies in understanding contextual relationships within text. Elements like punctuation and stop words can still convey sentiment or emphasis and removing them might disrupt this context.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09732</link><description>&lt;p&gt;
PET-SQL&#65306;&#19968;&#20010;&#24102;&#26377;&#20132;&#21449;&#19968;&#33268;&#24615;&#30340;&#22686;&#24378;&#25552;&#31034;&#30340;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;SQL&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25991;&#26412;&#21040;SQL&#65288;Text2SQL&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#24378;&#35843;&#21050;&#28608;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#30340;&#29992;&#25143;&#24847;&#22270;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#34920;&#31034;&#65292;&#31216;&#20026;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#24335;&#20449;&#24687;&#21644;&#20174;&#34920;&#26684;&#38543;&#26426;&#25277;&#26679;&#30340;&#21333;&#20803;&#26684;&#20540;&#65292;&#20197;&#25351;&#23548;LLM&#29983;&#25104;SQL&#26597;&#35810;&#12290;&#28982;&#21518;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#26816;&#32034;&#38382;&#39064;-SQL&#23545;&#20316;&#20026;&#23569;&#37327;&#28436;&#31034;&#65292;&#20419;&#20351;LLM&#29983;&#25104;&#21021;&#27493;SQL&#65288;PreSQL&#65289;&#12290;&#20043;&#21518;&#65292;&#35299;&#26512;PreSQL&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#36827;&#34892;&#27169;&#24335;&#38142;&#25509;&#65292;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#26377;&#29992;&#20449;&#24687;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21033;&#29992;&#38142;&#25509;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09732v1 Announce Type: cross  Abstract: Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26126;&#31034;&#24847;&#35782;&#26657;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09488</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#32416;&#27491;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Rectifying Demonstration Shortcut in In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26126;&#31034;&#24847;&#35782;&#26657;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#20165;&#20973;&#23569;&#37327;&#28436;&#31034;&#20415;&#33021;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;LLMs&#24120;&#24120;&#20381;&#36182;&#20110;&#23427;&#20204;&#23545;&#28436;&#31034;&#30340;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#20041;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#26681;&#25454;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#32487;&#32493;&#36827;&#34892;ICL&#39044;&#27979;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#29616;&#35937;&#31216;&#20026;&#8220;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#8221;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#25913;&#36827;&#39044;&#23450;&#20041;&#20219;&#21153;&#30340;ICL&#39044;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#32416;&#27491;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#20174;&#32780;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#30340;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26126;&#31034;&#24847;&#35782;&#30340;&#26657;&#20934;&#26041;&#27861;&#65306;In-Context Calibration&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#65288;1&#65289;&#20351;&#29992;&#26631;&#20934;&#26631;&#31614;&#31354;&#38388;&#30340;&#21407;&#22987;ICL&#20219;&#21153;&#20197;&#21450;&#65288;2&#65289;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#26631;&#31614;&#31354;&#38388;&#34987;&#35821;&#20041;&#26080;&#20851;&#30340;&#26631;&#35760;&#26367;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09488v1 Announce Type: cross  Abstract: Large language models (LLMs) are able to solve various tasks with only a few demonstrations utilizing their in-context learning (ICL) abilities. However, LLMs often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with ICL prediction. In this work, we term this phenomenon as the `Demonstration Shortcut'. While previous works have primarily focused on improving ICL prediction results for predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM to effectively learn new input-label relationships from demonstrations. To achieve this, we introduce In-Context Calibration, a demonstration-aware calibration method. We evaluate the effectiveness of the proposed method in two settings: (1) the Original ICL Task using the standard label space and (2) the Task Learning setting, where the label space is replaced with semantically unrelated tokens. In 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SHROOM&#20849;&#20139;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#26816;&#27979;&#24187;&#35273;&#65292;&#20197;&#21450;&#21442;&#19982;&#32773;&#20351;&#29992;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.07726</link><description>&lt;p&gt;
SemEval-2024&#20849;&#20139;&#20219;&#21153;6: SHROOM&#65292;&#19968;&#20010;&#20851;&#20110;&#24187;&#35273;&#21450;&#30456;&#20851;&#21487;&#35266;&#23519;&#36807;&#24230;&#29983;&#25104;&#38169;&#35823;&#30340;&#20849;&#20139;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SHROOM&#20849;&#20139;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#26816;&#27979;&#24187;&#35273;&#65292;&#20197;&#21450;&#21442;&#19982;&#32773;&#20351;&#29992;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SHROOM&#30340;&#32467;&#26524;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#26816;&#27979;&#24187;&#35273;&#30340;&#20849;&#20139;&#20219;&#21153;&#65306;&#21363;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31995;&#32479;&#30340;&#36755;&#20986;&#27969;&#30021;&#20294;&#19981;&#20934;&#30830;&#12290;&#36825;&#31181;&#36807;&#24230;&#29983;&#25104;&#30340;&#24773;&#20917;&#21487;&#33021;&#21361;&#21450;&#35768;&#22810;NLG&#24212;&#29992;&#65292;&#20854;&#20013;&#27491;&#30830;&#24615;&#24448;&#24448;&#33267;&#20851;&#37325;&#35201;&#12290;&#20849;&#20139;&#20219;&#21153;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4000&#20010;&#30001;5&#20010;&#26631;&#27880;&#32773;&#26631;&#35760;&#30340;&#27169;&#22411;&#36755;&#20986;&#65292;&#28085;&#30422;&#20102;3&#20010;NLP&#20219;&#21153;&#65306;&#26426;&#22120;&#32763;&#35793;&#12289;&#37322;&#20041;&#29983;&#25104;&#21644;&#23450;&#20041;&#24314;&#27169;&#12290; &#20849;&#20139;&#20219;&#21153;&#30001;58&#20010;&#19981;&#21516;&#29992;&#25143;&#32452;&#25104;&#30340;42&#25903;&#22242;&#38431;&#20849;&#21516;&#35299;&#20915;&#65292;&#20854;&#20013;27&#25903;&#36873;&#25321;&#25776;&#20889;&#31995;&#32479;&#25551;&#36848;&#35770;&#25991;&#65307;&#20182;&#20204;&#20849;&#25552;&#20132;&#20102;&#36229;&#36807;300&#20010;&#39044;&#27979;&#38598;&#22312;&#20849;&#20139;&#20219;&#21153;&#30340;&#20004;&#20010;&#36319;&#36394;&#19978;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#34987;&#22788;&#29702;&#30340;&#19968;&#20123;&#20851;&#38190;&#36235;&#21183;--&#35768;&#22810;&#21442;&#19982;&#32773;&#20381;&#36182;&#23569;&#25968;&#27169;&#22411;&#65292;&#24182;&#32463;&#24120;&#20381;&#36182;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#25110;&#38646;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07726v1 Announce Type: new  Abstract: This paper presents the results of the SHROOM, a shared task focused on detecting hallucinations: outputs from natural language generation (NLG) systems that are fluent, yet inaccurate. Such cases of overgeneration put in jeopardy many NLG applications, where correctness is often mission-critical. The shared task was conducted with a newly constructed dataset of 4000 model outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine translation, paraphrase generation and definition modeling.   The shared task was tackled by a total of 58 different users grouped in 42 teams, out of which 27 elected to write a system description paper; collectively, they submitted over 300 prediction sets on both tracks of the shared task. We observe a number of key trends in how this approach was tackled -- many participants rely on a handful of model, and often rely either on synthetic data for fine-tuning or zero-shot prompting strategies. While 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#26694;&#26550;&#21644;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;HTC&#27169;&#22411;&#30340;HiAdv&#26694;&#26550;&#65292;&#20248;&#21270;&#20102;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#35777;&#26126;&#20102;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#32597;&#35265;&#31867;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.18825</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Utilizing Local Hierarchy with Adversarial Training for Hierarchical Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18825
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#26694;&#26550;&#21644;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;HTC&#27169;&#22411;&#30340;HiAdv&#26694;&#26550;&#65292;&#20248;&#21270;&#20102;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#35777;&#26126;&#20102;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#32597;&#35265;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#65288;HTC&#65289;&#26159;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#20219;&#21153;&#65292;&#22240;&#20026;&#20854;&#22797;&#26434;&#30340;&#20998;&#31867;&#32467;&#26500;&#12290;&#20960;&#20046;&#25152;&#26377;&#26368;&#36817;&#30340;HTC&#20316;&#21697;&#37117;&#20851;&#27880;&#26631;&#31614;&#22914;&#20309;&#32467;&#26500;&#21270;&#65292;&#20294;&#24573;&#30053;&#20102;&#26681;&#25454;&#27599;&#20010;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#30340;&#23376;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#20016;&#23500;&#30340;&#26631;&#31614;&#20849;&#29616;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36825;&#31181;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#21644;&#19968;&#20010;&#23545;&#25239;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HiAdv&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#20960;&#20046;&#25152;&#26377;HTC&#27169;&#22411;&#65292;&#24182;&#23558;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20856;&#22411;&#30340;HTC&#27169;&#22411;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;HiAdv&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#20998;&#31867;&#23618;&#27425;&#32467;&#26500;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#26694;&#26550;&#30340;&#25552;&#21319;&#30830;&#23454;&#26469;&#33258;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#65292;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#26377;&#21033;&#20110;&#37027;&#20123;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#32597;&#35265;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18825v1 Announce Type: new  Abstract: Hierarchical text classification (HTC) is a challenging subtask of multi-label classification due to its complex taxonomic structure. Nearly all recent HTC works focus on how the labels are structured but ignore the sub-structure of ground-truth labels according to each input text which contains fruitful label co-occurrence information. In this work, we introduce this local hierarchy with an adversarial framework. We propose a HiAdv framework that can fit in nearly all HTC models and optimize them with the local hierarchy as auxiliary information. We test on two typical HTC models and find that HiAdv is effective in all scenarios and is adept at dealing with complex taxonomic hierarchies. Further experiments demonstrate that the promotion of our framework indeed comes from the local hierarchy and the local hierarchy is beneficial for rare classes which have insufficient training data.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;ChatGPT&#22312;&#25552;&#20379;&#23450;&#21046;&#21270;&#30340;&#35821;&#22659;&#29305;&#23450;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#21487;&#20197;&#36741;&#21161;&#26415;&#35821;&#23398;&#23478;&#36827;&#34892;&#26415;&#35821;&#32534;&#32386;&#65292;&#23454;&#29616;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.16139</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#23545;&#26415;&#35821;&#23450;&#20041;&#30340;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
What Generative Artificial Intelligence Means for Terminological Definitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16139
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;ChatGPT&#22312;&#25552;&#20379;&#23450;&#21046;&#21270;&#30340;&#35821;&#22659;&#29305;&#23450;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#21487;&#20197;&#36741;&#21161;&#26415;&#35821;&#23398;&#23478;&#36827;&#34892;&#26415;&#35821;&#32534;&#32386;&#65292;&#23454;&#29616;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#23545;&#26415;&#35821;&#23450;&#20041;&#30340;&#21019;&#24314;&#21644;&#28040;&#36153;&#30340;&#24433;&#21709;&#12290;&#20687;ChatGPT&#36825;&#26679;&#30340;GenAI&#24037;&#20855;&#19982;&#20256;&#32479;&#26415;&#35821;&#36164;&#28304;&#30456;&#27604;&#65292;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#30410;&#22788;&#21644;&#25361;&#25112;&#12290;ChatGPT&#22312;&#20197;&#20132;&#20114;&#24335;&#21644;&#23450;&#21046;&#21270;&#30340;&#26041;&#24335;&#25552;&#20379;&#29305;&#23450;&#35821;&#22659;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#35782;&#21035;&#36164;&#28304;&#20013;&#30340;&#26415;&#35821;&#23450;&#20041;&#21487;&#33021;&#20250;&#22240;&#20854;&#21487;&#38752;&#24615;&#32780;&#32487;&#32493;&#23384;&#22312;&#12290;&#20174;&#26415;&#35821;&#23398;&#23478;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35832;&#22914;ChatGPT&#20043;&#31867;&#30340;&#24037;&#20855;&#20351;&#24471;AI&#36741;&#21161;&#30340;&#26415;&#35821;&#32534;&#32386;&#25104;&#20026;&#21487;&#33021;&#65292;&#21253;&#25324;&#21518;&#26399;&#32534;&#36753;&#26415;&#35821;&#32534;&#32386;&#65292;&#23558;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#36895;&#30340;&#23450;&#20041;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16139v1 Announce Type: cross  Abstract: This paper examines the impact of Generative Artificial Intelligence (GenAI) on the creation and consumption of terminological definitions. GenAI tools like ChatGPT present a mix of benefits and drawbacks compared to traditional terminological resources. ChatGPT excels in providing context-specific meanings in an interactive and customized fashion but faces challenges with accuracy. Terminological definitions in recognized resources will likely survive because of their reliability. From the point of view of the terminologist, tools like ChatGPT enable AI-assisted terminography, including post-editing terminography, as an approach blending AI efficiency with human expertise for faster definition creation.
&lt;/p&gt;</description></item><item><title>PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#20026;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#31283;&#20581;&#38450;&#24481;</title><link>https://arxiv.org/abs/2402.12168</link><description>&lt;p&gt;
&#38024;&#23545;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12168
&lt;/p&gt;
&lt;p&gt;
PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#20026;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#31283;&#20581;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#25552;&#20986;&#24182;&#25104;&#21151;&#23454;&#26045;&#20102;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#38754;&#23545;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#26102;&#65292;&#20165;&#26356;&#26032;&#26377;&#38480;&#27169;&#22411;&#21442;&#25968;&#30340;PEFT&#26159;&#21542;&#26500;&#25104;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#39044;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#20173;&#28982;&#26131;&#21463;&#21033;&#29992;&#65292;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#22312;&#24494;&#35843;&#21518;&#20381;&#28982;&#20445;&#25345;&#39640;&#32622;&#20449;&#24230;&#12290;&#21463;&#21040;&#36825;&#19968;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21033;&#29992;PEFT&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#65292;&#25552;&#20379;&#38024;&#23545;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31283;&#20581;&#38450;&#24481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;PEFT&#35757;&#32451;PSIM&#65292;&#24102;&#26377;&#38543;&#26426;&#37325;&#32622;&#26679;&#26412;&#26631;&#31614;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12168v1 Announce Type: cross  Abstract: Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21453;&#21465;&#20107;&#65292;&#36890;&#36807;5&#20010;&#26041;&#38754;&#20174;&#19987;&#38376; NGO &#25351;&#21335;&#20013;&#25552;&#21462;&#23450;&#20041;&#30340;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#20197;&#24448;&#35780;&#20272;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11676</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21453;&#21465;&#20107;&#35780;&#20272;&#30340;&#22810;&#26041;&#38754;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11676
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21453;&#21465;&#20107;&#65292;&#36890;&#36807;5&#20010;&#26041;&#38754;&#20174;&#19987;&#38376; NGO &#25351;&#21335;&#20013;&#25552;&#21462;&#23450;&#20041;&#30340;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#20197;&#24448;&#35780;&#20272;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21465;&#20107;&#26159;&#23545;&#20167;&#24680;&#35328;&#35770;&#32972;&#26223;&#30340;&#30693;&#24773;&#22238;&#24212;&#65292;&#26088;&#22312;&#39539;&#26021;&#20167;&#24680;&#20027;&#24352;&#24182;&#21270;&#35299;&#20914;&#31361;&#65292;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20167;&#24680;&#35328;&#35770;&#24178;&#39044;&#31574;&#30053;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#33258;&#21160;&#29983;&#25104;&#21453;&#21465;&#20107;&#30340;&#26041;&#27861;&#26469;&#36741;&#21161;&#25163;&#21160;&#24178;&#39044;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#35780;&#20272;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21457;&#23637;&#12290;&#20808;&#21069;&#29992;&#20110;&#21453;&#21465;&#20107;&#35780;&#20272;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#32570;&#20047;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#19968;&#33268;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#34920;&#38754;&#21442;&#32771;&#27604;&#36739;&#65292;&#32780;&#19981;&#26159;&#23558;&#21453;&#21465;&#20107;&#36136;&#37327;&#30340;&#20851;&#38190;&#26041;&#38754;&#32435;&#20837;&#35780;&#20272;&#26631;&#20934;&#12290;&#20026;&#35299;&#20915;&#20808;&#21069;&#30340;&#35780;&#20272;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20419;&#20351;LLM&#25552;&#20379;&#29983;&#25104;&#30340;&#21453;&#21465;&#20107;&#20505;&#36873;&#30340;&#24471;&#20998;&#21644;&#21453;&#39304;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#19987;&#38376;NGO&#30340;&#21453;&#21465;&#20107;&#25351;&#21335;&#20013;&#25552;&#21462;&#30340;5&#20010;&#23450;&#20041;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11676v1 Announce Type: cross  Abstract: Counter narratives - informed responses to hate speech contexts designed to refute hateful claims and de-escalate encounters - have emerged as an effective hate speech intervention strategy. While previous work has proposed automatic counter narrative generation methods to aid manual interventions, the evaluation of these approaches remains underdeveloped. Previous automatic metrics for counter narrative evaluation lack alignment with human judgment as they rely on superficial reference comparisons instead of incorporating key aspects of counter narrative quality as evaluation criteria. To address prior evaluation limitations, we propose a novel evaluation framework prompting LLMs to provide scores and feedback for generated counter narrative candidates using 5 defined aspects derived from guidelines from counter narrative specialized NGOs. We found that LLM evaluators achieve strong alignment to human-annotated scores and feedback and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#28436;&#31034;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#22411;&#37197;&#32622;&#30340;&#27169;&#31946;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10189</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;&#21644;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#28436;&#31034;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#22411;&#37197;&#32622;&#30340;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#24615;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#19968;&#20123;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#28436;&#31034;&#26469;&#24443;&#24213;&#25913;&#21464;&#20102;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;LLM&#21709;&#24212;&#20013;&#30340;&#21487;&#20449;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#65292;&#20063;&#34987;&#31215;&#26497;&#35752;&#35770;&#12290;&#29616;&#26377;&#24037;&#20316;&#33268;&#21147;&#20110;&#37327;&#21270;LLM&#21709;&#24212;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;LLM&#30340;&#22797;&#26434;&#24615;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#29420;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#20851;&#30340;LLM&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#26469;&#33258;&#20110;&#25552;&#20379;&#30340;&#28436;&#31034;&#65288;aleatoric&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#19982;&#27169;&#22411;&#37197;&#32622;&#30456;&#20851;&#30340;&#27169;&#31946;&#24615;&#65288;epistemic&#19981;&#30830;&#23450;&#24615;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24335;&#21644;&#30456;&#24212;&#30340;&#20272;&#35745;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#20026;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#37324;&#30340;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10189v1 Announce Type: new  Abstract: In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-cont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22806;&#37096;&#24605;&#32771;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#36890;&#20449;&#21327;&#35758;&#21644;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#28216;&#25103;&#25512;&#29702;&#12289;&#35821;&#38899;&#29983;&#25104;&#21644;&#22312;&#32447;&#35780;&#20272;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02330</link><description>&lt;p&gt;
&#22312;&#29436;&#20154;&#28216;&#25103;&#20013;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhance Reasoning for Large Language Models in the Game Werewolf
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22806;&#37096;&#24605;&#32771;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#36890;&#20449;&#21327;&#35758;&#21644;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#28216;&#25103;&#25512;&#29702;&#12289;&#35821;&#38899;&#29983;&#25104;&#21644;&#22312;&#32447;&#35780;&#20272;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22806;&#37096;&#24605;&#32771;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#19982;&#36890;&#36807;prompt&#24037;&#31243;&#22686;&#21152;LLM&#19981;&#21516;&#65292;&#24605;&#32771;&#32773;&#30452;&#25509;&#21033;&#29992;&#25968;&#25454;&#24211;&#20013;&#30340;&#30693;&#35782;&#65292;&#24182;&#37319;&#29992;&#21508;&#31181;&#20248;&#21270;&#25216;&#26415;&#12290;&#35813;&#26694;&#26550;&#24418;&#25104;&#20102;&#19968;&#20010;&#25512;&#29702;&#23618;&#27425;&#32467;&#26500;&#65292;&#22312;&#20854;&#20013;LLM&#22788;&#29702;&#30452;&#35266;&#30340;&#31995;&#32479;1&#20219;&#21153;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#32780;&#24605;&#32771;&#32773;&#19987;&#27880;&#20110;&#38656;&#35201;&#22797;&#26434;&#30340;&#36923;&#36753;&#20998;&#26512;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#35748;&#30693;&#31995;&#32479;2&#20219;&#21153;&#12290;&#25105;&#20204;&#20197;&#38656;&#35201;&#21452;&#31995;&#32479;&#25512;&#29702;&#30340;9&#20154;&#29436;&#20154;&#28216;&#25103;&#20026;&#20363;&#20171;&#32461;&#20102;&#35813;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LLM&#21644;&#24605;&#32771;&#32773;&#20043;&#38388;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;18800&#20010;&#20154;&#31867;&#20250;&#35805;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#35757;&#32451;&#20102;&#24605;&#32771;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#28436;&#32462;&#25512;&#29702;&#12289;&#35821;&#38899;&#29983;&#25104;&#21644;&#22312;&#32447;&#28216;&#25103;&#35780;&#20272;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;6B LLM&#65292;&#36229;&#36234;&#20102;GPT4&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an innovative framework that integrates Large Language Models (LLMs) with an external Thinker module to enhance the reasoning capabilities of LLM-based agents. Unlike augmenting LLMs with prompt engineering, Thinker directly harnesses knowledge from databases and employs various optimization techniques. The framework forms a reasoning hierarchy where LLMs handle intuitive System-1 tasks such as natural language processing, while the Thinker focuses on cognitive System-2 tasks that require complex logical analysis and domain-specific knowledge. Our framework is presented using a 9-player Werewolf game that demands dual-system reasoning. We introduce a communication protocol between LLMs and the Thinker, and train the Thinker using data from 18800 human sessions and reinforcement learning. Experiments demonstrate the framework's effectiveness in deductive reasoning, speech generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to surpass GPT4 when
&lt;/p&gt;</description></item><item><title>CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;</title><link>https://arxiv.org/abs/2402.00786</link><description>&lt;p&gt;
CroissantLLM: &#19968;&#20010;&#30495;&#27491;&#30340;&#21452;&#35821;&#27861;&#35821;-&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CroissantLLM: A Truly Bilingual French-English Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00786
&lt;/p&gt;
&lt;p&gt;
CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CroissantLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;3T&#20010;&#33521;&#35821;&#21644;&#27861;&#35821;&#26631;&#35760;&#19978;&#39044;&#35757;&#32451;&#30340;13&#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#30740;&#31350;&#21644;&#24037;&#19994;&#31038;&#21306;&#24102;&#26469;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#12289;&#23436;&#20840;&#24320;&#28304;&#30340;&#21452;&#35821;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#28040;&#36153;&#32423;&#26412;&#22320;&#30828;&#20214;&#19978;&#24555;&#36895;&#36816;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#19968;&#31181;&#20869;&#22312;&#21452;&#35821;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#27861;&#35821;&#20998;&#21106;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#25163;&#24037;&#31574;&#21010;&#12289;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#28304;&#12290;&#20026;&#20102;&#35780;&#20272;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; FrenchBench&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#27169;&#22411;&#22312;&#27861;&#35821;&#35821;&#35328;&#20013;&#24615;&#33021;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#25345;&#36879;&#26126;&#24230;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20195;&#30721;&#24211;&#21644;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#20960;&#21313;&#20010;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#37325;&#36848;&#25216;&#26415;&#25913;&#36827;&#23545;&#35805;&#24335;&#38382;&#31572;&#24615;&#33021;&#30340;CornNet&#27169;&#22411;</title><link>https://arxiv.org/abs/2312.17269</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#37325;&#36848;&#36827;&#34892;&#23545;&#35805;&#24335;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Conversational Question Answering with Reformulations over Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17269
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#37325;&#36848;&#25216;&#26415;&#25913;&#36827;&#23545;&#35805;&#24335;&#38382;&#31572;&#24615;&#33021;&#30340;CornNet&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#38382;&#31572;&#65288;convQA&#65289;&#26159;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#30340;&#22810;&#36718;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#22238;&#31572;&#12290;&#30446;&#21069;&#30340;convQA&#26041;&#27861;&#36890;&#24120;&#22312;&#38590;&#20197;&#29702;&#35299;&#30340;&#38382;&#31572;&#37197;&#23545;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#36825;&#20123;&#36755;&#20837;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#22312;&#23545;&#35805;&#21382;&#21490;&#30340;&#22522;&#30784;&#19978;&#24456;&#23481;&#26131;&#29702;&#35299;&#65292;&#20294;&#23545;&#20110;&#26426;&#22120;&#26469;&#35828;&#24456;&#38590;&#35299;&#37322;&#65292;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;convQA&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#27169;&#22411;CornNet&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#38382;&#39064;&#37325;&#36848;&#26469;&#25552;&#39640;convQA&#24615;&#33021;&#12290;CornNet&#37319;&#29992;&#25945;&#24072;-&#23398;&#29983;&#26550;&#26500;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#37325;&#36848;&#26469;&#23398;&#20064;&#38382;&#39064;&#34920;&#31034;&#65292;&#23398;&#29983;&#27169;&#22411;&#36890;&#36807;LLMs&#29983;&#25104;&#30340;&#37325;&#36848;&#26469;&#27169;&#20223;&#25945;&#24072;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;RL&#27169;&#22411;&#20351;&#29992;&#23398;&#21040;&#30340;&#38382;&#39064;&#34920;&#31034;&#26469;&#22312;KG&#20013;&#23450;&#20301;&#27491;&#30830;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17269v2 Announce Type: replace-cross  Abstract: Conversational question answering (convQA) over knowledge graphs (KGs) involves answering multi-turn natural language questions about information contained in a KG. State-of-the-art methods of ConvQA often struggle with inexplicit question-answer pairs. These inputs are easy for human beings to understand given a conversation history, but hard for a machine to interpret, which can degrade ConvQA performance. To address this problem, we propose a reinforcement learning (RL) based model, CornNet, which utilizes question reformulations generated by large language models (LLMs) to improve ConvQA performance. CornNet adopts a teacher-student architecture where a teacher model learns question representations using human writing reformulations, and a student model to mimic the teacher model's output via reformulations generated by LLMs. The learned question representation is then used by an RL model to locate the correct answer in a K
&lt;/p&gt;</description></item><item><title>GlitchBench&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#28216;&#25103;&#36136;&#37327;&#20445;&#35777;&#20219;&#21153;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#25361;&#25112;LMMs&#22312;&#26816;&#27979;&#21644;&#35299;&#37322;&#24322;&#24120;&#20107;&#20214;&#26041;&#38754;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.05291</link><description>&lt;p&gt;
GlitchBench&#65306;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#22815;&#26816;&#27979;&#35270;&#39057;&#28216;&#25103;&#28431;&#27934;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
GlitchBench: Can large multimodal models detect video game glitches?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05291
&lt;/p&gt;
&lt;p&gt;
GlitchBench&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#28216;&#25103;&#36136;&#37327;&#20445;&#35777;&#20219;&#21153;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#25361;&#25112;LMMs&#22312;&#26816;&#27979;&#21644;&#35299;&#37322;&#24322;&#24120;&#20107;&#20214;&#26041;&#38754;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#24050;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21457;&#23637;&#32780;&#26469;&#65292;&#20197;&#25972;&#21512;&#22810;&#31181;&#36755;&#20837;&#27169;&#24577;&#65292;&#22914;&#35270;&#35273;&#36755;&#20837;&#12290;&#36825;&#31181;&#25972;&#21512;&#22686;&#24378;&#20102;LLMs&#22312;&#38656;&#35201;&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23588;&#20854;&#26159;&#22312;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#22686;&#24378;&#30340;&#33021;&#21147;&#30340;&#31243;&#24230;&#21644;&#38480;&#21046;&#23578;&#26410;&#23436;&#20840;&#34987;&#29702;&#35299;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GlitchBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#28304;&#33258;&#20110;&#35270;&#39057;&#28216;&#25103;&#36136;&#37327;&#20445;&#35777;&#20219;&#21153;&#65292;&#26088;&#22312;&#27979;&#35797;&#21644;&#35780;&#20272;LMMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#26159;&#20174;&#21508;&#31181;&#35270;&#39057;&#28216;&#25103;&#20013;&#30340;&#19981;&#23547;&#24120;&#21644;&#20986;&#29616;&#25925;&#38556;&#30340;&#22330;&#26223;&#31934;&#24515;&#31574;&#21010;&#32780;&#25104;&#65292;&#26088;&#22312;&#25361;&#25112;LMMs&#22312;&#26816;&#27979;&#21644;&#35299;&#37322;&#38750;&#21516;&#23547;&#24120;&#20107;&#20214;&#26041;&#38754;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;LMMs&#65292;&#24182;&#23637;&#31034;&#20102;GlitchBench&#20026;&#36825;&#20123;&#27169;&#22411;&#25552;&#20986;&#20102;&#26032;&#25361;&#25112;&#12290; &#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://glitchb
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05291v2 Announce Type: replace-cross  Abstract: Large multimodal models (LMMs) have evolved from large language models (LLMs) to integrate multiple input modalities, such as visual inputs. This integration augments the capacity of LLMs for tasks requiring visual comprehension and reasoning. However, the extent and limitations of their enhanced abilities are not fully understood, especially when it comes to real-world tasks. To address this gap, we introduce GlitchBench, a novel benchmark derived from video game quality assurance tasks, to test and evaluate the reasoning capabilities of LMMs. Our benchmark is curated from a variety of unusual and glitched scenarios from video games and aims to challenge both the visual and linguistic reasoning powers of LMMs in detecting and interpreting out-of-the-ordinary events. We evaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents a new challenge for these models. Code and data are available at: https://glitchb
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#65292;&#20197;&#20811;&#26381;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.17076</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32452;&#21512;&#24335;&#24605;&#32500;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Compositional Chain-of-Thought Prompting for Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#65292;&#20197;&#20811;&#26381;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#30340;&#35270;&#35273;&#39592;&#24178;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#25512;&#29702;&#30340;&#32467;&#21512;&#24050;&#32463;&#23548;&#33268;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMM)&#25104;&#20026;&#24403;&#21069;&#24191;&#27867;&#35270;&#35273;&#21644;&#35821;&#35328;(VL)&#20219;&#21153;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LMM&#20173;&#28982;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#65292;&#27604;&#22914;&#23545;&#35937;&#20043;&#38388;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#22330;&#26223;&#22270;(SGs)&#8212;&#8212;&#23545;&#35937;&#21450;&#20854;&#20851;&#31995;&#21644;&#23646;&#24615;&#30340;&#24418;&#24335;&#21270;&#34920;&#36798;&#65292;&#23427;&#24050;&#34987;&#24191;&#27867;&#29992;&#20316;&#35270;&#35273;&#21644;&#25991;&#26412;&#39046;&#22495;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#28982;&#32780;&#65292;&#22330;&#26223;&#22270;&#25968;&#25454;&#38656;&#35201;&#22330;&#26223;&#22270;&#27880;&#37322;&#65292;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#38590;&#20197;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22330;&#26223;&#22270;&#25968;&#25454;&#24494;&#35843;LMM&#21487;&#33021;&#23548;&#33268;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#21463;&#21040;&#24605;&#32500;&#38142;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17076v2 Announce Type: replace-cross  Abstract: The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-sho
&lt;/p&gt;</description></item><item><title>PEMA&#26159;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20214;&#22806;&#37096;&#20869;&#23384;&#33258;&#36866;&#24212;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#32469;&#36807;&#20102;&#23545;&#25152;&#26377;&#26435;&#37325;&#30340;&#35775;&#38382;&#38656;&#27714;&#65292;&#21516;&#26102;&#21033;&#29992;&#22806;&#37096;&#20869;&#23384;&#21644;&#36866;&#37197;&#22120;&#26435;&#37325;&#30697;&#38453;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.08590</link><description>&lt;p&gt;
PEMA&#65306;&#19968;&#31181;&#21487;&#22312;&#31163;&#32447;&#35843;&#25972;&#30340;&#22806;&#37096;&#25554;&#20214;&#20869;&#23384;&#33258;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08590
&lt;/p&gt;
&lt;p&gt;
PEMA&#26159;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20214;&#22806;&#37096;&#20869;&#23384;&#33258;&#36866;&#24212;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#32469;&#36807;&#20102;&#23545;&#25152;&#26377;&#26435;&#37325;&#30340;&#35775;&#38382;&#38656;&#27714;&#65292;&#21516;&#26102;&#21033;&#29992;&#22806;&#37096;&#20869;&#23384;&#21644;&#36866;&#37197;&#22120;&#26435;&#37325;&#30697;&#38453;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#30001;&#20110;&#36164;&#28304;&#38656;&#27714;&#24040;&#22823;&#65292;&#35768;&#22810;PLM&#26435;&#37325;&#26159;&#26426;&#23494;&#30340;&#65292;&#29992;&#25143;&#34987;&#36843;&#23558;&#20854;&#25968;&#25454;&#19982;&#27169;&#22411;&#25152;&#26377;&#32773;&#20849;&#20139;&#65292;&#20197;&#20415;&#20026;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25554;&#20214;&#22806;&#37096;&#20869;&#23384;&#33258;&#36866;&#24212;&#65288;PEMA&#65289;&#65292;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#38656;&#35201;&#35775;&#38382;&#25152;&#26377;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#23545;PLM&#36827;&#34892;&#24494;&#35843;&#12290;PEMA&#22312;&#25512;&#29702;&#26399;&#38388;&#38598;&#25104;&#20102;&#26469;&#33258;&#27979;&#35797;&#25968;&#25454;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#20197;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#12290;&#23427;&#20351;&#29992;&#22806;&#37096;&#20869;&#23384;&#23384;&#20648;&#30001;PLM&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#19982;&#30446;&#26631;&#26631;&#35760;&#30456;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;PLM&#26368;&#32456;&#23618;&#20013;&#31867;&#20284;LoRA&#30340;&#29942;&#39048;&#36866;&#37197;&#22120;&#30340;&#26435;&#37325;&#30697;&#38453;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08590v2 Announce Type: replace  Abstract: Pre-trained language models (PLMs) show impressive performance in various downstream NLP tasks. However, pre-training large language models demands substantial memory and training compute. Furthermore, due to the substantial resources required, many PLM weights are confidential. Consequently, users are compelled to share their data with model owners for fine-tuning specific tasks. To overcome the limitations, we introduce Plug-in External Memory Adaptation (PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM fine-tuning without requiring access to all the weights. PEMA integrates with context representations from test data during inference to perform downstream tasks. It uses external memory to store PLM-generated context representations mapped with target tokens. Our method utilizes weight matrices of LoRA-like bottlenecked adapter in the PLM's final layer to enhance efficiency. Our approach also includes Gradual Un
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#38169;&#35823;&#32416;&#27491;&#30340;&#25968;&#25454;&#23545;&#26469;&#25913;&#36827;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25552;&#21319;&#20165;&#20351;&#29992;CoT&#36827;&#34892;&#24494;&#35843;&#21518;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.20689</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#38169;&#35823;&#20013;&#20351;LLM&#25104;&#20026;&#26356;&#22909;&#30340;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Learning From Mistakes Makes LLM Better Reasoner
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.20689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#38169;&#35823;&#32416;&#27491;&#30340;&#25968;&#25454;&#23545;&#26469;&#25913;&#36827;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25552;&#21319;&#20165;&#20351;&#29992;CoT&#36827;&#34892;&#24494;&#35843;&#21518;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#26159;&#21542;&#21487;&#20197;&#23398;&#20064;&#20174;&#38169;&#35823;&#20013;&#33719;&#30410;&#65288;LEMA&#65289;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#32771;&#34385;&#19968;&#20010;&#26410;&#33021;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#20154;&#31867;&#23398;&#29983;&#65292;&#20182;&#20250;&#20174;&#33258;&#24049;&#29359;&#30340;&#38169;&#35823;&#20013;&#23398;&#20064;&#65292;&#24182;&#32416;&#27491;&#23427;&#12290;&#27169;&#20223;&#36825;&#31181;&#38169;&#35823;&#39537;&#21160;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;LEMA&#22312;LLM&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#38169;&#35823;&#32416;&#27491;&#30340;&#25968;&#25454;&#23545;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#26469;&#33258;&#21508;&#31181;LLM&#30340;&#38169;&#35823;&#25512;&#29702;&#36335;&#24452;&#65292;&#28982;&#21518;&#20351;&#29992;GPT-4&#20316;&#20026;&#8220;&#32416;&#27491;&#32773;&#8221;&#26469;&#35782;&#21035;&#38169;&#35823;&#27493;&#39588;&#65292;&#35299;&#37322;&#38169;&#35823;&#21407;&#22240;&#65292;&#32416;&#27491;&#38169;&#35823;&#24182;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#32416;&#27491;&#30340;&#36827;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#22320;&#25193;&#23637;&#20102;&#29983;&#25104;&#32416;&#27491;&#25968;&#25454;&#30340;&#38382;&#39064;&#38598;&#12290;&#22312;&#21508;&#31181;LLM&#21644;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LEMA&#22987;&#32456;&#21487;&#20197;&#25552;&#21319;&#20165;&#20351;&#29992;CoT&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a "corrector" to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that \textsc{LeMa} consistently improves CoT-alone fine-tuning. Our fu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#23884;&#20837;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#38656;&#35201;&#31526;&#21495;&#21644;&#25968;&#20540;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;Python&#31243;&#24207;&#36827;&#34892;&#25512;&#29702;&#65292;&#36824;&#21487;&#20197;&#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#25991;&#26412;&#20998;&#31867;&#12289;&#38382;&#39064;&#22238;&#31572;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2309.10814</link><description>&lt;p&gt;
&#29992;&#20110;&#28151;&#21512;&#35821;&#35328;&#31526;&#21495;&#25512;&#29702;&#30340;&#33258;&#28982;&#35821;&#35328;&#23884;&#20837;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.10814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#23884;&#20837;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#38656;&#35201;&#31526;&#21495;&#21644;&#25968;&#20540;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;Python&#31243;&#24207;&#36827;&#34892;&#25512;&#29702;&#65292;&#36824;&#21487;&#20197;&#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#25991;&#26412;&#20998;&#31867;&#12289;&#38382;&#39064;&#22238;&#31572;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2309.10814v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#25105;&#20204;&#22914;&#20309;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#25191;&#34892;&#35745;&#31639;&#20197;&#35299;&#20915;&#38656;&#35201;&#31526;&#21495;&#21644;&#25968;&#20540;&#25512;&#29702;&#30340;&#20219;&#21153;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#23884;&#20837;&#31243;&#24207;&#65288;NLEP&#65289;&#20316;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#23398;/&#31526;&#21495;&#25512;&#29702;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#20351;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#23436;&#25972;&#30340;Python&#31243;&#24207;&#65292;&#23450;&#20041;&#22312;&#21253;&#21547;&#32467;&#26500;&#21270;&#30693;&#35782;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#30340;&#25968;&#25454;&#32467;&#26500;&#19978;&#30340;&#20989;&#25968;&#12290;&#28982;&#21518;Python&#35299;&#37322;&#22120;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#24182;&#25171;&#21360;&#36755;&#20986;&#12290;&#23613;&#31649;&#20351;&#29992;&#20102;&#19968;&#20010;&#20219;&#21153;&#36890;&#29992;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#19968;&#31995;&#21015;&#19981;&#21516;&#20219;&#21153;&#19978;&#21253;&#25324;&#25968;&#23398;&#21644;&#31526;&#21495;&#25512;&#29702;&#12289;&#25991;&#26412;&#20998;&#31867;&#12289;&#38382;&#39064;&#22238;&#31572;&#21644;&#25351;&#20196;&#36319;&#38543;&#20013;&#25913;&#36827;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#29983;&#25104;&#30340;&#31243;&#24207;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#27010;&#36848;&#20102;&#31243;&#24207;&#35299;&#37322;&#22120;&#36981;&#24490;&#30340;&#30830;&#20999;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.10814v2 Announce Type: replace  Abstract: How can we perform computations over natural language representations to solve tasks that require symbolic and numeric reasoning? We propose natural language embedded programs (NLEP) as a unifying framework for addressing math/symbolic reasoning, natural language understanding, and instruction following tasks. Our approach prompts a language model to generate full Python programs that define functions over data structures which contain natural language representations of structured knowledge. A Python interpreter then executes the generated code and prints the output. Despite using a task-general prompt, we find that this approach can improve upon strong baselines across a range of different tasks including math and symbolic reasoning, text classification, question answering, and instruction following. We found that the generated programs are interpretable since they outline the exact reasoning process followed by the program interpr
&lt;/p&gt;</description></item><item><title>S\={a}mayik&#26159;&#19968;&#20010;&#21253;&#21547;&#32422;53,000&#20010;&#24179;&#34892;&#33521;&#26805;&#21477;&#23376;&#30340;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#20102;&#22810;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#20851;&#27880;&#26805;&#25991;&#30340;&#24403;&#20195;&#29992;&#27861;&#65292;&#29992;&#20110;&#22521;&#35757;&#30340;&#32763;&#35793;&#27169;&#22411;&#22312;&#32763;&#35793;&#20986;&#39046;&#22495;&#30340;&#24403;&#20195;&#35821;&#26009;&#26102;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25552;&#21319;</title><link>https://arxiv.org/abs/2305.14004</link><description>&lt;p&gt;
S\={a}mayik: &#19968;&#31181;&#29992;&#20110;&#33521;&#26805;&#35821;&#32763;&#35793;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
S\={a}mayik: A Benchmark and Dataset for English-Sanskrit Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14004
&lt;/p&gt;
&lt;p&gt;
S\={a}mayik&#26159;&#19968;&#20010;&#21253;&#21547;&#32422;53,000&#20010;&#24179;&#34892;&#33521;&#26805;&#21477;&#23376;&#30340;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#20102;&#22810;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#20851;&#27880;&#26805;&#25991;&#30340;&#24403;&#20195;&#29992;&#27861;&#65292;&#29992;&#20110;&#22521;&#35757;&#30340;&#32763;&#35793;&#27169;&#22411;&#22312;&#32763;&#35793;&#20986;&#39046;&#22495;&#30340;&#24403;&#20195;&#35821;&#26009;&#26102;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#24067;&#20102;S\={a}mayik&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#32422;53,000&#20010;&#33521;&#26805;&#24179;&#34892;&#21477;&#23376;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#29616;&#20195;&#25955;&#25991;&#20070;&#20889;&#12290;&#26805;&#25991;&#26159;&#19968;&#31181;&#20173;&#22312;&#20351;&#29992;&#30340;&#21476;&#20856;&#35821;&#35328;&#65292;&#25317;&#26377;&#20016;&#23500;&#30340;&#25991;&#29486;&#20256;&#25215;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#23383;&#21270;&#20869;&#23481;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#26805;&#25991;&#20173;&#28982;&#26159;&#19968;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#29616;&#26377;&#30340;&#26805;&#25991;&#35821;&#26009;&#24211;&#65292;&#26080;&#35770;&#26159;&#21333;&#35821;&#36824;&#26159;&#21452;&#35821;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#35799;&#27468;&#19978;&#65292;&#24182;&#23545;&#24403;&#20195;&#20070;&#38754;&#26448;&#26009;&#30340;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#12290;S\={a}mayik&#20174;&#21508;&#31181;&#39046;&#22495;&#20013;&#31934;&#36873;&#20986;&#26469;&#65292;&#20854;&#20013;&#21253;&#25324;&#35821;&#35328;&#25945;&#23398;&#26448;&#26009;&#12289;&#25991;&#26412;&#25945;&#23398;&#27861;&#21644;&#22312;&#32447;&#25945;&#31243;&#31561;&#12290;&#23427;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#30340;&#36164;&#28304;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#26805;&#25991;&#30340;&#24403;&#20195;&#29992;&#27861;&#65292;&#20027;&#35201;&#24378;&#35843;&#25955;&#25991;&#20889;&#20316;&#12290;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32763;&#35793;&#27169;&#22411;&#22312;&#32763;&#35793;&#22495;&#22806;&#30340;&#24403;&#20195;&#35821;&#26009;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20248;&#20110;&#37027;&#20123;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14004v2 Announce Type: replace  Abstract: We release S\={a}mayik, a dataset of around 53,000 parallel English-Sanskrit sentences, written in contemporary prose. Sanskrit is a classical language still in sustenance and has a rich documented heritage. However, due to the limited availability of digitized content, it still remains a low-resource language. Existing Sanskrit corpora, whether monolingual or bilingual, have predominantly focused on poetry and offer limited coverage of contemporary written materials. S\={a}mayik is curated from a diverse range of domains, including language instruction material, textual teaching pedagogy, and online tutorials, among others. It stands out as a unique resource that specifically caters to the contemporary usage of Sanskrit, with a primary emphasis on prose writing. Translation models trained on our dataset demonstrate statistically significant improvements when translating out-of-domain contemporary corpora, outperforming models traine
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#23545;&#35805;&#36136;&#37327;&#21644;&#22270;&#20687;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#21147;&#20171;&#20837;&#12290;</title><link>https://arxiv.org/abs/2212.04119</link><description>&lt;p&gt;
DialogCC: &#19968;&#20010;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
DialogCC: An Automated Pipeline for Creating High-Quality Multi-Modal Dialogue Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.04119
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#23545;&#35805;&#36136;&#37327;&#21644;&#22270;&#20687;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#21147;&#20171;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#21363;&#26102;&#28040;&#24687;&#20013;&#20998;&#20139;&#22270;&#29255;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#23545;&#23398;&#20064;&#22270;&#20687;&#25991;&#26412;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#36827;&#34892;&#20102;&#31215;&#26497;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#23545;&#35805;&#20013;&#22270;&#20687;&#30340;&#36136;&#37327;&#20302;&#21644;&#22810;&#26679;&#24615;&#26377;&#38480;&#65292;&#35757;&#32451;&#19968;&#20010;&#33391;&#22909;&#27867;&#21270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#20173;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#23545;&#35805;&#36136;&#37327;&#21644;&#22270;&#20687;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#21147;&#20171;&#20837;&#12290;&#22312;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#20013;&#65292;&#20026;&#20102;&#30830;&#20445;&#22270;&#20687;&#19982;&#23545;&#35805;&#20043;&#38388;&#30340;&#36830;&#36143;&#24615;&#65292;&#25105;&#20204;&#24341;&#23548;GPT-4&#25512;&#26029;&#28508;&#22312;&#30340;&#22270;&#20687;&#20998;&#20139;&#26102;&#21051; - &#20855;&#20307;&#22320;&#65292;&#35805;&#35821;&#12289;&#35828;&#35805;&#32773;&#12289;&#29702;&#30001;&#21644;&#22270;&#20687;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;CLIP&#30456;&#20284;&#24230;&#26469;&#20445;&#25345;&#22810;&#20010;&#23545;&#40784;&#22270;&#20687;&#19982;&#35805;&#35821;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#36825;&#20010;&#27969;&#27700;&#32447;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DialogCC&#65292;&#19968;&#20010;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.04119v2 Announce Type: replace-cross  Abstract: As sharing images in an instant message is a crucial factor, there has been active research on learning an image-text multi-modal dialogue models. However, training a well-generalized multi-modal dialogue model remains challenging due to the low quality and limited diversity of images per dialogue in existing multi-modal dialogue datasets. In this paper, we propose an automated pipeline to construct a multi-modal dialogue dataset, ensuring both dialogue quality and image diversity without requiring minimum human effort. In our pipeline, to guarantee the coherence between images and dialogue, we prompt GPT-4 to infer potential image-sharing moments - specifically, the utterance, speaker, rationale, and image description. Furthermore, we leverage CLIP similarity to maintain consistency between aligned multiple images to the utterance. Through this pipeline, we introduce DialogCC, a high-quality and diverse multi-modal dialogue da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; QAGCN &#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38382;&#39064;&#36827;&#34892;&#24863;&#30693;&#26469;&#23454;&#29616;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#65292;&#20174;&#32780;&#22238;&#31572;&#22810;&#20851;&#31995;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#26174;&#24335;&#22810;&#27493;&#25512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26356;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#26131;&#20110;&#37319;&#29992;&#12290;</title><link>https://arxiv.org/abs/2206.01818</link><description>&lt;p&gt;
QAGCN&#65306;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#22238;&#31572;&#22810;&#20851;&#31995;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
QAGCN: Answering Multi-Relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.01818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; QAGCN &#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38382;&#39064;&#36827;&#34892;&#24863;&#30693;&#26469;&#23454;&#29616;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#65292;&#20174;&#32780;&#22238;&#31572;&#22810;&#20851;&#31995;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#26174;&#24335;&#22810;&#27493;&#25512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26356;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#26131;&#20110;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20851;&#31995;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#30001;&#22810;&#20010;&#20851;&#31995;&#32452;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#38271;&#26102;&#38388;&#25512;&#29702;&#38142;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#26126;&#26174;&#20351;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26174;&#24335;&#22810;&#27493;&#25512;&#29702;&#26041;&#27861;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#36880;&#27493;&#26631;&#31614;&#20256;&#25773;&#30340;&#26041;&#27861;&#20197;&#21450;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#27983;&#35272;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#24369;&#28857;&#26159;&#23427;&#20204;&#30340;&#25512;&#29702;&#26426;&#21046;&#36890;&#24120;&#22797;&#26434;&#19988;&#38590;&#20197;&#23454;&#29616;&#25110;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#21487;&#20197;&#36890;&#36807;&#31471;&#21040;&#31471;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#23454;&#29616;&#22810;&#20851;&#31995;QA&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#31616;&#21333;&#12289;&#26356;&#39640;&#25928;&#19988;&#26356;&#26131;&#20110;&#37319;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; QAGCN -- &#19968;&#31181;&#22522;&#20110;&#38382;&#39064;&#24847;&#35782;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#21463;&#25511;&#38382;&#39064;&#30456;&#20851;&#20449;&#24687;&#20256;&#25773;&#30340;GCN&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.01818v3 Announce Type: replace  Abstract: Multi-relation question answering (QA) is a challenging task, where given questions usually require long reasoning chains in KGs that consist of multiple relations. Recently, methods with explicit multi-step reasoning over KGs have been prominently used in this task and have demonstrated promising performance. Examples include methods that perform stepwise label propagation through KG triples and methods that navigate over KG triples based on reinforcement learning. A main weakness of these methods is that their reasoning mechanisms are usually complex and difficult to implement or train. In this paper, we argue that multi-relation QA can be achieved via end-to-end single-step implicit reasoning, which is simpler, more efficient, and easier to adopt. We propose QAGCN -- a Question-Aware Graph Convolutional Network (GCN)-based method that includes a novel GCN architecture with controlled question-dependent message propagation for the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38024;&#23545;&#26041;&#35328;&#30340;&#26041;&#27861;&#21644;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#26041;&#35328;&#23545;&#20110;NLP&#27169;&#22411;&#24615;&#33021;&#21644;&#35821;&#35328;&#25216;&#26415;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#26041;&#35328;&#30456;&#20851;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2401.05632</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#26041;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing for Dialects of a Language: A Survey. (arXiv:2401.05632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05632
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38024;&#23545;&#26041;&#35328;&#30340;&#26041;&#27861;&#21644;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#26041;&#35328;&#23545;&#20110;NLP&#27169;&#22411;&#24615;&#33021;&#21644;&#35821;&#35328;&#25216;&#26415;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#26041;&#35328;&#30456;&#20851;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#22312;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#37325;&#35201;&#23646;&#24615;&#65306;&#35821;&#35328;&#26041;&#35328;&#12290;&#32771;&#34385;&#21040;&#38024;&#23545;&#26041;&#35328;&#25968;&#25454;&#38598;&#30340;NLP&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#21450;&#20854;&#23545;&#35821;&#35328;&#25216;&#26415;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26377;&#20851;&#26041;&#35328;NLP&#30340;&#36807;&#21435;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#20004;&#20010;&#31867;&#21035;&#30340;&#35270;&#35282;&#25551;&#36848;&#20102;&#21508;&#31181;NLP&#20219;&#21153;&#65306;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#65288;&#22914;&#26041;&#35328;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#35299;&#26512;&#21644;NLU&#22522;&#20934;&#27979;&#35797;&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#65288;&#22914;&#25688;&#35201;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#23545;&#35805;&#31995;&#32479;&#65289;&#12290;&#36825;&#39033;&#35843;&#26597;&#36824;&#24191;&#27867;&#28085;&#30422;&#20102;&#33521;&#35821;&#12289;&#38463;&#25289;&#20271;&#35821;&#12289;&#24503;&#35821;&#31561;&#22810;&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#26377;&#20851;&#26041;&#35328;&#30340;&#36807;&#21435;NLP&#24037;&#20316;&#19981;&#27490;&#20110;&#26041;&#35328;&#20998;&#31867;&#65292;&#32780;&#26159;...
&lt;/p&gt;
&lt;p&gt;
State-of-the-art natural language processing (NLP) models are trained on massive training corpora, and report a superlative performance on evaluation datasets. This survey delves into an important attribute of these datasets: the dialect of a language. Motivated by the performance degradation of NLP models for dialectic datasets and its implications for the equity of language technologies, we survey past research in NLP for dialects in terms of datasets, and approaches. We describe a wide range of NLP tasks in terms of two categories: natural language understanding (NLU) (for tasks such as dialect classification, sentiment analysis, parsing, and NLU benchmarks) and natural language generation (NLG) (for summarisation, machine translation, and dialogue systems). The survey is also broad in its coverage of languages which include English, Arabic, German among others. We observe that past work in NLP concerning dialects goes deeper than mere dialect classification, and . This includes ear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DialogBench&#65292;&#19968;&#20010;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;28&#20010;LLMs&#30340;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#23545;&#25552;&#21319;&#24615;&#33021;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2311.01677</link><description>&lt;p&gt;
DialogBench: &#23558;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DialogBench: Evaluating LLMs as Human-like Dialogue Systems. (arXiv:2311.01677v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DialogBench&#65292;&#19968;&#20010;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;28&#20010;LLMs&#30340;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#23545;&#25552;&#21319;&#24615;&#33021;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26032;&#30340;&#23545;&#35805;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#21047;&#26032;&#20102;&#20154;&#20204;&#23545;&#23545;&#35805;&#31995;&#32479;&#30340;&#21360;&#35937;&#12290;&#23545;&#35805;&#31995;&#32479;&#38271;&#26399;&#20197;&#26469;&#30340;&#30446;&#26631;&#26159;&#36275;&#22815;&#20687;&#20154;&#31867;&#65292;&#20197;&#20415;&#36890;&#36807;&#28385;&#36275;&#20132;&#27969;&#12289;&#24773;&#24863;&#21644;&#31038;&#20132;&#24402;&#23646;&#30340;&#38656;&#35201;&#19982;&#29992;&#25143;&#24314;&#31435;&#38271;&#26399;&#32852;&#31995;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DialogBench&#65292;&#19968;&#20010;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;&#65292;&#30446;&#21069;&#21253;&#21547;12&#20010;&#23545;&#35805;&#20219;&#21153;&#65292;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#24212;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#29983;&#25104;&#27599;&#20010;&#20219;&#21153;&#30340;&#35780;&#20272;&#23454;&#20363;&#12290;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#24191;&#27867;&#20351;&#29992;&#30340;&#35774;&#35745;&#21407;&#21017;&#35774;&#35745;&#22522;&#26412;&#25552;&#31034;&#65292;&#24182;&#36827;&#19968;&#27493;&#20943;&#36731;&#29616;&#26377;&#30340;&#20559;&#35265;&#65292;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#35780;&#20272;&#23454;&#20363;&#12290;&#25105;&#20204;&#23545;28&#20010;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#65288;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#25351;&#23548;&#35843;&#20248;&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#25351;&#23548;&#24494;&#35843;&#25928;&#30410;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable breakthroughs in new dialogue capabilities, refreshing human's impressions on dialogue systems. The long-standing goal of dialogue systems is to be human-like enough to establish long-term connections with users by satisfying the need for communication, affection and social belonging. Therefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems. In this paper, we propose DialogBench, a dialogue evaluation benchmark that currently contains $12$ dialogue tasks to assess the capabilities of LLMs as human-like dialogue systems should have. Specifically, we prompt GPT-4 to generate evaluation instances for each task. We first design the basic prompt based on widely-used design principles and further mitigate the existing biases to generate higher-quality evaluation instances. Our extensive test over $28$ LLMs (including pre-trained and supervised instruction-tuning) shows that instruction fine-tuning benefits 
&lt;/p&gt;</description></item><item><title>ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13001</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;ConFIRM&#65289;
&lt;/p&gt;
&lt;p&gt;
Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13001
&lt;/p&gt;
&lt;p&gt;
ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#37329;&#34701;&#31561;&#19987;&#38376;&#39046;&#22495;&#30340;&#26032;&#20852;&#29305;&#24615;&#20855;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#31561;&#21463;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#38656;&#35201;&#20855;&#22791;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ConFIRM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#21644;&#30693;&#35782;&#24211;&#26631;&#35760;&#12290;ConFIRM&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;2&#65289;&#35780;&#20272;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#22810;&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12290;ConFIRM&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#31526;&#21512;&#30417;&#31649;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;ConFIRM&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#21462;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#30340;&#31934;&#30830;&#26597;&#35810;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;PEFT&#26131;&#21463;&#29305;&#27931;&#20234;&#25915;&#20987;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;PETA&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#35302;&#21457;&#22120;&#35774;&#35745;&#20013;&#36827;&#34892;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;PETA&#33021;&#22815;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#26410;&#21463;&#24433;&#21709;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00648</link><description>&lt;p&gt;
&#26356;&#23569;&#23601;&#24847;&#21619;&#30528;&#26356;&#22810;: &#23545;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#29305;&#27931;&#20234;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning. (arXiv:2310.00648v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;PEFT&#26131;&#21463;&#29305;&#27931;&#20234;&#25915;&#20987;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;PETA&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#35302;&#21457;&#22120;&#35774;&#35745;&#20013;&#36827;&#34892;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;PETA&#33021;&#22815;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#26410;&#21463;&#24433;&#21709;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#39640;&#25928;&#22320;&#36866;&#24212;&#21040;&#29305;&#23450;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#20165;&#24494;&#35843;&#19968;&#23567;&#37096;&#20998;&#65288;&#39069;&#22806;&#30340;&#65289;&#21442;&#25968;&#65292;PEFT&#23454;&#29616;&#20102;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;PEFT&#30340;&#23433;&#20840;&#24615;&#24433;&#21709;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;PEFT&#23545;&#29305;&#27931;&#20234;&#25915;&#20987;&#30340;&#29420;&#29305;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PETA&#65292;&#19968;&#31181;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#32771;&#34385;&#19979;&#28216;&#36866;&#24212;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#24335;&#65306;&#19978;&#23618;&#30446;&#26631;&#23558;&#21518;&#38376;&#23884;&#20837;PLM&#20013;&#65292;&#32780;&#19979;&#23618;&#30446;&#26631;&#27169;&#25311;PEFT&#20197;&#20445;&#30041;PLM&#30340;&#20219;&#21153;&#29305;&#23450;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#35302;&#21457;&#22120;&#35774;&#35745;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PETA&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#26410;&#21463;&#24433;&#21709;&#30340;&#24178;&#20928;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#21463;&#23475;&#29992;&#25143;&#22312;&#20351;&#29992;&#32431;&#20928;&#25968;&#25454;&#23545;&#24102;&#26377;&#21518;&#38376;&#30340;PLM&#36827;&#34892;PEFT&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of pre-trained language models (PLMs) to specific tasks. By tuning only a minimal set of (extra) parameters, PEFT achieves performance comparable to full fine-tuning. However, despite its prevalent use, the security implications of PEFT remain largely unexplored. In this paper, we conduct a pilot study revealing that PEFT exhibits unique vulnerability to trojan attacks. Specifically, we present PETA, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM's task-specific performance. With extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. Moreover, we empi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#20107;&#23454;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#19968;&#23450;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#24448;&#24448;&#20063;&#20381;&#36182;&#20110;&#29421;&#31364;&#12289;&#38590;&#20197;&#36716;&#31227;&#30340;&#36807;&#31243;&#65292;&#36825;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#35299;&#37322;&#21644;&#29702;&#35299;&#26377;&#30528;&#37325;&#35201;&#30340;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.02477</link><description>&lt;p&gt;
&#25512;&#29702;&#36824;&#26159;&#32972;&#35829;&#65311;&#36890;&#36807;&#21453;&#20107;&#23454;&#20219;&#21153;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. (arXiv:2307.02477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02477
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#19968;&#23450;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#24448;&#24448;&#20063;&#20381;&#36182;&#20110;&#29421;&#31364;&#12289;&#38590;&#20197;&#36716;&#31227;&#30340;&#36807;&#31243;&#65292;&#36825;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#35299;&#37322;&#21644;&#29702;&#35299;&#26377;&#30528;&#37325;&#35201;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#34920;&#26126;&#23427;&#20204;&#20855;&#22791;&#19968;&#23450;&#31243;&#24230;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#26159;&#36890;&#29992;&#19988;&#21487;&#36716;&#31227;&#30340;&#65292;&#36824;&#26159;&#19987;&#38376;&#38024;&#23545;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#29305;&#23450;&#20219;&#21153;&#65311;&#20026;&#20102;&#20998;&#24320;&#36825;&#20123;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22522;&#20110;&#8220;&#21453;&#20107;&#23454;&#8221;&#20219;&#21153;&#21464;&#31181;&#65292;&#36825;&#20123;&#21464;&#31181;&#19982;&#25903;&#25745;&#26631;&#20934;&#20219;&#21153;&#30340;&#40664;&#35748;&#20551;&#35774;&#26377;&#25152;&#20559;&#31163;&#12290;&#22312;&#19968;&#22871;&#21253;&#21547;11&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21453;&#20107;&#23454;&#21464;&#31181;&#30340;&#38750;&#24179;&#20961;&#24615;&#33021;&#65292;&#20294;&#19982;&#40664;&#35748;&#26465;&#20214;&#30456;&#27604;&#65292;&#24615;&#33021;&#26174;&#33879;&#32780;&#25345;&#32493;&#22320;&#19979;&#38477;&#12290;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20855;&#22791;&#25277;&#35937;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20063;&#20381;&#36182;&#20110;&#29421;&#31364;&#12289;&#38590;&#20197;&#36716;&#31227;&#30340;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#12290;&#36825;&#20123;&#32467;&#26524;&#20419;&#20351;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#26356;&#21152;&#35880;&#24910;&#30340;&#35299;&#37322;&#65292;&#20197;&#21306;&#20998;&#36825;&#20123;&#34892;&#20026;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to a degree, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#36890;&#36807;&#27169;&#22359;&#21270;&#23398;&#20064;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#65292;&#36171;&#20104;LLMs&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#22312;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.14178</link><description>&lt;p&gt;
mPLUG-Owl: &#27169;&#22359;&#21270;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality. (arXiv:2304.14178v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#36890;&#36807;&#27169;&#22359;&#21270;&#23398;&#20064;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#65292;&#36171;&#20104;LLMs&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#22312;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#22312;&#21508;&#31181;&#24320;&#25918;&#24335;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23558;LLMs&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;mPLUG-Owl&#65292;&#36890;&#36807;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#30340;&#27169;&#22359;&#21270;&#23398;&#20064;&#65292;&#20351;LLMs&#20855;&#22791;&#20102;&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#27169;&#24577;&#65292;&#24182;&#36890;&#36807;&#27169;&#24577;&#21327;&#20316;&#20419;&#36827;&#20102;&#22810;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#21253;&#25324;&#29992;&#20110;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#36741;&#21161;&#23398;&#20064;&#35270;&#35273;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#29978;&#33267;&#25913;&#36827;&#20102;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#20351;&#29992;&#20923;&#32467;&#30340;LLM&#27169;&#22359;&#23545;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#25277;&#35937;&#22120;&#27169;&#22359;&#36827;&#34892;&#35757;&#32451;&#20197;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#20351;&#29992;&#20165;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30417;&#30563;&#25968;&#25454;&#38598;&#20849;&#21516;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23545;&#20110;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tu
&lt;/p&gt;</description></item></channel></rss>