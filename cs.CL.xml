<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#26694;&#26550;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#32422;&#26463;&#26631;&#35760;&#30340;&#20851;&#27880;&#31243;&#24230;&#19982;&#20107;&#23454;&#20934;&#30830;&#24615;&#24378;&#27491;&#30456;&#20851;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#32422;&#26463;&#28385;&#36275;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#20801;&#35768;&#26089;&#26399;&#38169;&#35823;&#35782;&#21035;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15098</link><description>&lt;p&gt;
&#28385;&#36275;&#20851;&#27880;&#65306;&#23545;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#38169;&#35823;&#30340;&#32422;&#26463;&#28385;&#36275;&#35270;&#35282;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models. (arXiv:2309.15098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#26694;&#26550;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#32422;&#26463;&#26631;&#35760;&#30340;&#20851;&#27880;&#31243;&#24230;&#19982;&#20107;&#23454;&#20934;&#30830;&#24615;&#24378;&#27491;&#30456;&#20851;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#32422;&#26463;&#28385;&#36275;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#20801;&#35768;&#26089;&#26399;&#38169;&#35823;&#35782;&#21035;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#20107;&#23454;&#19978;&#38169;&#35823;&#30340;&#25991;&#26412;&#26102;&#30340;&#20869;&#37096;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#20107;&#23454;&#26597;&#35810;&#24314;&#27169;&#20026;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#26694;&#26550;&#30740;&#31350;&#27169;&#22411;&#22914;&#20309;&#19982;&#20107;&#23454;&#32422;&#26463;&#36827;&#34892;&#20869;&#37096;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#23545;&#32422;&#26463;&#26631;&#35760;&#30340;&#20851;&#27880;&#31243;&#24230;&#19982;&#20854;&#21709;&#24212;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#23384;&#22312;&#24378;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;11&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#24635;&#35745;&#36229;&#36807;40,000&#20010;&#25552;&#31034;&#30340;&#31934;&#24515;&#31574;&#21010;&#22871;&#35013;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;Llama-2&#31995;&#21015;&#22312;&#25152;&#26377;&#35268;&#27169;&#65288;7B&#65292;13B&#65292;70B&#65289;&#19978;&#39044;&#27979;&#20107;&#23454;&#38169;&#35823;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SAT Probe&#65292;&#19968;&#31181;&#25506;&#26597;&#33258;&#27880;&#24847;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#32422;&#26463;&#28385;&#36275;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#20801;&#35768;&#26089;&#26399;&#38169;&#35823;&#35782;&#21035;&#12290;&#36825;&#19968;&#26041;&#27861;&#21644;&#21457;&#29616;&#34920;&#26126;&#65292;&#21033;&#29992;&#23545;LLM&#20013;&#20107;&#23454;&#24615;&#30340;&#26426;&#26800;&#29702;&#35299;&#21487;&#20197;&#22686;&#24378;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as Constraint Satisfaction Problems and use this framework to investigate how the model interacts internally with factual constraints. Specifically, we discover a strong positive relation between the model's attention to constraint tokens and the factual accuracy of its responses. In our curated suite of 11 datasets with over 40,000 prompts, we study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe, a method probing self-attention patterns, that can predict constraint satisfaction and factual errors, and allows early error identification. The approach and findings demonstrate how using the mechanistic understanding of factuality in LLMs can enhance reliability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#23454;&#29616;&#19968;&#33268;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#26469;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#12290;</title><link>http://arxiv.org/abs/2309.15091</link><description>&lt;p&gt;
VideoDirectorGPT: &#36890;&#36807;LLM&#24341;&#23548;&#30340;&#35268;&#21010;&#23454;&#29616;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning. (arXiv:2309.15091v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#23454;&#29616;&#19968;&#33268;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#26469;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#24037;&#20316;&#38598;&#20013;&#22312;&#29983;&#25104;&#21333;&#20010;&#20107;&#20214;&#21644;&#21333;&#19968;&#32972;&#26223;&#30340;&#30701;&#35270;&#39057;&#29255;&#27573;&#65288;&#21363;&#21333;&#22330;&#26223;&#35270;&#39057;&#65289;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#24067;&#23616;&#21644;&#25511;&#21046;&#19979;&#28216;&#35270;&#35273;&#27169;&#22359;&#65288;&#22914;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65289;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;&#36825;&#20123;LLMs&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#36827;&#34892;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#21333;&#20010;&#25991;&#26412;&#25552;&#31034;&#36755;&#20837;&#25105;&#20204;&#30340;&#35270;&#39057;&#35268;&#21010;&#22120;LLM&#65288;GPT-4&#65289;&#20013;&#65292;&#23558;&#20854;&#25193;&#23637;&#20026;&#8220;&#35270;&#39057;&#35745;&#21010;&#8221;&#65292;&#20854;&#20013;&#21253;&#25324;&#29983;&#25104;&#22330;&#26223;&#25551;&#36848;&#12289;&#23454;&#20307;&#21450;&#20854;&#24067;&#23616;&#12289;&#27599;&#20010;&#22330;&#26223;&#30340;&#32972;&#26223;&#20197;&#21450;&#20445;&#25345;&#19968;&#33268;&#24615;&#31561;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although recent text-to-video (T2V) generation methods have seen significant advancements, most of these works focus on producing short video clips of a single event with a single background (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules such as image generation models. This raises an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which involves generating the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency 
&lt;/p&gt;</description></item><item><title>RankVicuna&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#25191;&#34892;&#39640;&#36136;&#37327;&#21015;&#34920;&#25490;&#24207;&#30340;&#23436;&#20840;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#27604;GPT-3.5&#23567;&#24471;&#22810;&#30340;&#21442;&#25968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19982;&#38646;&#26679;&#26412;&#37325;&#26032;&#25490;&#24207;&#30456;&#24403;&#30340;&#25928;&#26524;&#65292;&#24182;&#20026;&#23558;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2309.15088</link><description>&lt;p&gt;
RankVicuna: &#20351;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#21015;&#34920;&#25490;&#24207;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models. (arXiv:2309.15088v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15088
&lt;/p&gt;
&lt;p&gt;
RankVicuna&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#25191;&#34892;&#39640;&#36136;&#37327;&#21015;&#34920;&#25490;&#24207;&#30340;&#23436;&#20840;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#27604;GPT-3.5&#23567;&#24471;&#22810;&#30340;&#21442;&#25968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19982;&#38646;&#26679;&#26412;&#37325;&#26032;&#25490;&#24207;&#30456;&#24403;&#30340;&#25928;&#26524;&#65292;&#24182;&#20026;&#23558;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25104;&#21151;&#22320;&#23558;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#37325;&#26032;&#25490;&#24207;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#26679;&#30340;&#24037;&#20316;&#22823;&#22810;&#24314;&#31435;&#22312;&#19981;&#36879;&#26126;&#30340;API&#21518;&#38754;&#30340;&#19987;&#26377;&#27169;&#22411;&#19978;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#30340;&#23454;&#39564;&#32467;&#26524;&#19981;&#21487;&#22797;&#29616;&#19988;&#38750;&#30830;&#23450;&#24615;&#65292;&#23041;&#32961;&#21040;&#24314;&#31435;&#22312;&#36825;&#31181;&#19981;&#31283;&#23450;&#22522;&#30784;&#19978;&#30340;&#32467;&#26524;&#30340;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#37325;&#22823;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RankVicuna&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#25191;&#34892;&#39640;&#36136;&#37327;&#21015;&#34920;&#25490;&#24207;&#30340;&#23436;&#20840;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;TREC 2019&#21644;2020&#28145;&#24230;&#23398;&#20064;&#36319;&#36394;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27604;GPT-3.5&#23567;&#24471;&#22810;&#30340;7B&#21442;&#25968;&#27169;&#22411;&#23454;&#29616;&#19982;&#38646;&#26679;&#26412;&#37325;&#26032;&#25490;&#24207;&#30456;&#24403;&#30340;&#25928;&#26524;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#25928;&#26524;&#20173;&#30053;&#36874;&#20110;GPT-4&#37325;&#26032;&#25490;&#24207;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#23558;&#26469;&#20351;&#29992;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#30340;&#30740;&#31350;&#25552;&#20379;&#22522;&#30784;&#12290;&#22797;&#29616;&#25105;&#20204;&#32467;&#26524;&#25152;&#38656;&#30340;&#25152;&#26377;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;h&#38142;&#25509;&#22788;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have successfully applied large language models (LLMs) such as ChatGPT to reranking in an information retrieval context, but to date, such work has mostly been built on proprietary models hidden behind opaque API endpoints. This approach yields experimental results that are not reproducible and non-deterministic, threatening the veracity of outcomes that build on such shaky foundations. To address this significant shortcoming, we present RankVicuna, the first fully open-source LLM capable of performing high-quality listwise reranking in a zero-shot setting. Experimental results on the TREC 2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness comparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter model, although our effectiveness remains slightly behind reranking with GPT-4. We hope our work provides the foundation for future research on reranking with modern LLMs. All the code necessary to reproduce our results is available at h
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#65292;&#36890;&#36807;&#19982;LLMs&#20132;&#20114;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.15074</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#19978;&#19979;&#25991;&#24314;&#27169;&#19982;&#25512;&#29702;&#65306;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Natural Language based Context Modeling and Reasoning with LLMs: A Tutorial. (arXiv:2309.15074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#65292;&#36890;&#36807;&#19982;LLMs&#20132;&#20114;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;2018&#24180;&#20197;&#26469;&#24613;&#21095;&#22686;&#38271;&#65292;&#33258;&#24341;&#20837;&#19978;&#19979;&#25991;&#24863;&#30693;&#35745;&#31639;&#31995;&#32479;20&#24180;&#21518;&#12290;&#19978;&#19979;&#25991;&#24863;&#30693;&#35745;&#31639;&#36890;&#36807;&#32771;&#34385;&#26222;&#36866;&#35774;&#22791;&#12289;&#29992;&#25143;&#21644;&#31038;&#20250;&#30340;&#24773;&#20917;&#65292;&#23454;&#29616;&#20102;&#24191;&#27867;&#30340;&#21019;&#26032;&#24212;&#29992;&#65292;&#22914;&#36741;&#21161;&#29983;&#27963;&#12289;&#22522;&#20110;&#20301;&#32622;&#30340;&#31038;&#20132;&#32593;&#32476;&#26381;&#21153;&#31561;&#12290;&#20026;&#20102;&#35782;&#21035;&#19978;&#19979;&#25991;&#24182;&#30456;&#24212;&#22320;&#20570;&#20986;&#20915;&#31574;&#65292;&#37319;&#29992;&#20102;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65288;&#22914;&#26412;&#20307;&#35770;&#21644;OWL&#65289;&#20316;&#20026;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;LLMs&#30340;&#23835;&#36215;&#21644;&#23427;&#20204;&#25913;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#24314;&#27169;&#19978;&#19979;&#25991;&#24182;&#36890;&#36807;&#19982;ChatGPT&#21644;GPT-4&#31561;LLMs&#20132;&#20114;&#36827;&#34892;&#19978;&#19979;&#25991;&#25512;&#29702;&#21464;&#24471;&#21487;&#34892;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#25991;&#26412;&#12289;&#25552;&#31034;&#21644;&#33258;&#20027;&#20195;&#29702;&#65288;AutoAgents&#65289;&#20351;LLMs&#33021;&#22815;&#25191;&#34892;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have become phenomenally surging, since 2018--two decades after introducing context-awareness into computing systems. Through taking into account the situations of ubiquitous devices, users and the societies, context-aware computing has enabled a wide spectrum of innovative applications, such as assisted living, location-based social network services and so on. To recognize contexts and make decisions for actions accordingly, various artificial intelligence technologies, such as Ontology and OWL, have been adopted as representations for context modeling and reasoning. Recently, with the rise of LLMs and their improved natural language understanding and reasoning capabilities, it has become feasible to model contexts using natural language and perform context reasoning by interacting with LLMs such as ChatGPT and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and autonomous agents (AutoAgents) that enable LLMs to perform context modeling 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15028</link><description>&lt;p&gt;
&#35753;PPO&#21464;&#24471;&#26356;&#22909;&#65306;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26102;&#65292;&#20351;&#29992;&#26368;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;Proximal Policy Optimization (PPO)&#65292;&#22240;&#27492;&#21487;&#20197;&#35748;&#20026;&#25512;&#29702;&#26102;&#38388;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#22914;Monte-Carlo Tree Search (MCTS) &#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;PPO&#30340;&#24615;&#33021;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#35299;&#30721;&#25991;&#26412;&#26102;&#65292;&#19981;&#35201;&#20002;&#24323;&#20540;&#32593;&#32476;&#65292;&#21363;PPO&#35757;&#32451;&#26102;&#29992;&#20110;&#35780;&#20272;&#37096;&#20998;&#36755;&#20986;&#24207;&#21015;&#30340;&#21103;&#20135;&#21697;&#65292;&#32780;&#26159;&#23558;&#20854;&#19982;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PPO-MCTS&#30340;&#26032;&#39062;&#30340;&#20540;&#23548;&#21521;&#35299;&#30721;&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;PPO&#30340;&#20540;&#32593;&#32476;&#19982;&#25512;&#29702;&#26102;&#38388;&#20135;&#29983;&#30340;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#19982;&#22522;&#20110;MCTS&#30340;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#20943;&#23569;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#30340;&#35780;&#20998;&#26426;&#21046;&#30340;&#22522;&#26412;&#19981;&#21305;&#37197;&#12290;&#22312;&#22235;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;PPO-MCTS&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS grea
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#35752;&#65292;&#24182;&#25552;&#20986;&#20102;&#20869;&#37096;&#23545;&#40784;&#21644;&#22806;&#37096;&#23545;&#40784;&#30340;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#28508;&#22312;&#30340;&#23545;&#25239;&#25915;&#20987;&#28431;&#27934;&#12290;&#32771;&#34385;&#21040;&#27169;&#22411;&#21487;&#33021;&#20135;&#29983;&#30340;&#19981;&#20934;&#30830;&#21644;&#35823;&#23548;&#24615;&#25991;&#26412;&#65292;&#23545;&#40784;&#25216;&#26415;&#26174;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.15025</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Alignment: A Survey. (arXiv:2309.15025v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15025
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#35752;&#65292;&#24182;&#25552;&#20986;&#20102;&#20869;&#37096;&#23545;&#40784;&#21644;&#22806;&#37096;&#23545;&#40784;&#30340;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#28508;&#22312;&#30340;&#23545;&#25239;&#25915;&#20987;&#28431;&#27934;&#12290;&#32771;&#34385;&#21040;&#27169;&#22411;&#21487;&#33021;&#20135;&#29983;&#30340;&#19981;&#20934;&#30830;&#21644;&#35823;&#23548;&#24615;&#25991;&#26412;&#65292;&#23545;&#40784;&#25216;&#26415;&#26174;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#20123;&#36827;&#23637;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#21508;&#31181;&#25285;&#24551;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#21147;&#26080;&#30097;&#26159;&#24040;&#22823;&#30340;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#19981;&#20934;&#30830;&#12289;&#35823;&#23548;&#24615;&#29978;&#33267;&#26377;&#23475;&#30340;&#25991;&#26412;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#23545;&#40784;&#25216;&#26415;&#20197;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#19968;&#33268;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#23545;&#38024;&#23545;LLMs&#35774;&#35745;&#30340;&#23545;&#40784;&#26041;&#27861;&#36827;&#34892;&#24191;&#27867;&#25506;&#35752;&#65292;&#24182;&#32467;&#21512;&#35813;&#39046;&#22495;&#20013;&#30340;&#29616;&#26377;&#33021;&#21147;&#30740;&#31350;&#12290;&#37319;&#29992;AI&#23545;&#40784;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#23558;&#29992;&#20110;&#23545;&#40784;LLMs&#30340;&#20027;&#27969;&#26041;&#27861;&#21644;&#26032;&#20852;&#25552;&#35758;&#20998;&#20026;&#22806;&#37096;&#23545;&#40784;&#21644;&#20869;&#37096;&#23545;&#40784;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21644;&#28508;&#22312;&#30340;&#23545;&#25239;&#25915;&#20987;&#28431;&#27934;&#31561;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35780;&#20272;LLM&#30340;&#23545;&#40784;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values.  This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain. Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment. We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. To assess LLM alignment, we present a wide variety of benchmarks and evaluation metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;GPT-4&#36827;&#34892;&#38382;&#31572;&#30340;&#27861;&#24459;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;-&#31572;&#26696;&#23545;&#26469;&#35206;&#30422;&#21442;&#32771;&#25688;&#35201;&#20013;&#30340;&#20027;&#35201;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;GPT-4&#23545;&#21442;&#32771;&#25688;&#35201;&#21644;&#29983;&#25104;&#25688;&#35201;&#30340;&#31572;&#26696;&#36827;&#34892;&#35780;&#20998;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.15016</link><description>&lt;p&gt;
&#29992;&#38382;&#31572;&#26041;&#27861;&#35780;&#20272;&#27861;&#24459;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Question-Answering Approach to Evaluate Legal Summaries. (arXiv:2309.15016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;GPT-4&#36827;&#34892;&#38382;&#31572;&#30340;&#27861;&#24459;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;-&#31572;&#26696;&#23545;&#26469;&#35206;&#30422;&#21442;&#32771;&#25688;&#35201;&#20013;&#30340;&#20027;&#35201;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;GPT-4&#23545;&#21442;&#32771;&#25688;&#35201;&#21644;&#29983;&#25104;&#25688;&#35201;&#30340;&#31572;&#26696;&#36827;&#34892;&#35780;&#20998;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35780;&#20272;&#25351;&#26631;&#22914;ROUGE&#20165;&#27604;&#36739;&#21442;&#32771;&#25688;&#35201;&#21644;&#29983;&#25104;&#25688;&#35201;&#20043;&#38388;&#30340;&#35789;&#27719;&#37325;&#21472;&#65292;&#32780;&#19981;&#32771;&#34385;&#35770;&#28857;&#32467;&#26500;&#65292;&#32780;&#36825;&#23545;&#20110;&#27861;&#24459;&#25688;&#35201;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27861;&#24459;&#25688;&#35201;&#35780;&#20272;&#26694;&#26550;&#65292;&#21033;&#29992;GPT-4&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#28085;&#30422;&#21442;&#32771;&#25688;&#35201;&#20013;&#30340;&#20027;&#35201;&#28857;&#21644;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#29983;&#25104;&#25688;&#35201;&#22238;&#31572;&#21442;&#32771;&#25688;&#35201;&#20013;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;GPT-4&#23545;&#21442;&#32771;&#25688;&#35201;&#21644;&#29983;&#25104;&#25688;&#35201;&#30340;&#31572;&#26696;&#36827;&#34892;&#35780;&#20998;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;GPT-4&#35780;&#20998;&#19982;&#20154;&#24037;&#35780;&#20998;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;GPT-4&#30340;&#38382;&#31572;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional evaluation metrics like ROUGE compare lexical overlap between the reference and generated summaries without taking argumentative structure into account, which is important for legal summaries. In this paper, we propose a novel legal summarization evaluation framework that utilizes GPT-4 to generate a set of question-answer pairs that cover main points and information in the reference summary. GPT-4 is then used to generate answers based on the generated summary for the questions from the reference summary. Finally, GPT-4 grades the answers from the reference summary and the generated summary. We examined the correlation between GPT-4 grading with human grading. The results suggest that this question-answering approach with GPT-4 can be a useful tool for gauging the quality of the summary.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#21457;&#24067;&#20102;&#19977;&#20010;&#26631;&#20934;&#30340;ASR&#35821;&#26009;&#24211;&#65292;&#24182;&#26356;&#26032;&#20102;&#36716;&#24405;&#21644;&#23545;&#40784;&#65292;&#20197;&#29992;&#20110;&#38271;&#31687;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;&#12290;&#23545;&#20110;&#36716;&#24405;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65292;&#25105;&#20204;&#21457;&#29616;AEDs&#23545;&#20110;&#35757;&#32451;-&#27979;&#35797;&#19981;&#21305;&#37197;&#38382;&#39064;&#26356;&#20026;&#25935;&#24863;&#12290;&#22312;&#39046;&#22495;&#36716;&#21464;&#19979;&#65292;&#31616;&#21333;&#30340;&#38271;&#31687;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15013</link><description>&lt;p&gt;
&#38271;&#31687;&#35821;&#38899;&#35782;&#21035;&#30340;&#26356;&#26032;&#35821;&#26009;&#24211;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Updated Corpora and Benchmarks for Long-Form Speech Recognition. (arXiv:2309.15013v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#21457;&#24067;&#20102;&#19977;&#20010;&#26631;&#20934;&#30340;ASR&#35821;&#26009;&#24211;&#65292;&#24182;&#26356;&#26032;&#20102;&#36716;&#24405;&#21644;&#23545;&#40784;&#65292;&#20197;&#29992;&#20110;&#38271;&#31687;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;&#12290;&#23545;&#20110;&#36716;&#24405;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65292;&#25105;&#20204;&#21457;&#29616;AEDs&#23545;&#20110;&#35757;&#32451;-&#27979;&#35797;&#19981;&#21305;&#37197;&#38382;&#39064;&#26356;&#20026;&#25935;&#24863;&#12290;&#22312;&#39046;&#22495;&#36716;&#21464;&#19979;&#65292;&#31616;&#21333;&#30340;&#38271;&#31687;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32477;&#22823;&#22810;&#25968;ASR&#30740;&#31350;&#20351;&#29992;&#39044;&#20808;&#20998;&#21106;&#20026;&#35821;&#21477;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;ASR&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#27979;&#35797;&#38899;&#39057;&#24182;&#27809;&#26377;&#20998;&#21106;&#65292;&#20174;&#32780;&#23548;&#33268;&#25512;&#29702;&#26102;&#26465;&#20214;&#21644;&#35757;&#32451;&#22312;&#20998;&#21106;&#35821;&#21477;&#19978;&#30340;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#37325;&#26032;&#21457;&#24067;&#20102;&#19977;&#20010;&#26631;&#20934;ASR&#35821;&#26009;&#24211;-TED-LIUM 3&#65292;Gigapeech&#21644;VoxPopuli-en&#65292;&#24182;&#26356;&#26032;&#20102;&#36716;&#24405;&#21644;&#23545;&#40784;&#65292;&#20197;&#20415;&#23558;&#20854;&#29992;&#20110;&#38271;&#31687;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#37325;&#32452;&#30340;&#35821;&#26009;&#24211;&#30740;&#31350;&#20102;&#36716;&#24405;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;(AEDs)&#30340;&#35757;&#32451;-&#27979;&#35797;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#30830;&#35748;AEDs&#23545;&#27492;&#38382;&#39064;&#26356;&#20026;&#25935;&#24863;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20026;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#31616;&#21333;&#30340;&#38271;&#31687;&#35757;&#32451;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#27492;&#39046;&#22495;&#36716;&#21464;&#19979;&#30340;&#27169;&#22411;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vast majority of ASR research uses corpora in which both the training and test data have been pre-segmented into utterances. In most real-word ASR use-cases, however, test audio is not segmented, leading to a mismatch between inference-time conditions and models trained on segmented utterances. In this paper, we re-release three standard ASR corpora - TED-LIUM 3, Gigapeech, and VoxPopuli-en - with updated transcription and alignments to enable their use for long-form ASR research. We use these reconstituted corpora to study the train-test mismatch problem for transducers and attention-based encoder-decoders (AEDs), confirming that AEDs are more susceptible to this issue. Finally, we benchmark a simple long-form training for these models, showing its efficacy for model robustness under this domain shift.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#29992;&#20110;&#23398;&#26657;&#24418;&#25104;&#24615;&#21644;&#24635;&#32467;&#24615;&#35780;&#20272;&#30340;&#33258;&#21160;&#21270;&#38382;&#39064;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#23545;&#25945;&#24072;&#30340;&#35843;&#26597;&#65292;&#35777;&#26126;&#20102;&#33258;&#21160;&#21270;&#29983;&#25104;&#38382;&#39064;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#33258;&#21160;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15004</link><description>&lt;p&gt;
&#20174;&#25945;&#32946;&#25991;&#26412;&#20013;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Automating question generation from educational text. (arXiv:2309.15004v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#29992;&#20110;&#23398;&#26657;&#24418;&#25104;&#24615;&#21644;&#24635;&#32467;&#24615;&#35780;&#20272;&#30340;&#33258;&#21160;&#21270;&#38382;&#39064;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#23545;&#25945;&#24072;&#30340;&#35843;&#26597;&#65292;&#35777;&#26126;&#20102;&#33258;&#21160;&#21270;&#29983;&#25104;&#38382;&#39064;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#33258;&#21160;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#24335;&#27963;&#21160;&#65288;QBA&#65289;&#22312;&#25945;&#32946;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20256;&#32479;&#19978;&#26159;&#23398;&#20064;&#21644;&#35780;&#20272;&#36807;&#31243;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#29992;&#20110;&#23398;&#26657;&#24418;&#25104;&#24615;&#21644;&#24635;&#32467;&#24615;&#35780;&#20272;&#30340;&#33258;&#21160;&#21270;&#38382;&#39064;&#29983;&#25104;&#24037;&#20855;&#12290;&#36890;&#36807;&#23545;104&#21517;&#25945;&#24072;&#30340;&#19987;&#23478;&#35843;&#26597;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#21160;&#21270;&#29983;&#25104;QBA&#30340;&#38656;&#27714;&#65292;&#20316;&#20026;&#19968;&#20010;&#33021;&#22815;&#26174;&#33879;&#20943;&#36731;&#25945;&#24072;&#24037;&#20316;&#37327;&#24182;&#20419;&#36827;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#30340;&#24037;&#20855;&#12290;&#21033;&#29992;&#29983;&#25104;&#22411;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#33258;&#21160;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#38382;&#39064;&#29983;&#25104;&#12289;&#27491;&#30830;&#31572;&#26696;&#39044;&#27979;&#21644;&#24178;&#25200;&#39033;&#21046;&#23450;&#30340;&#19981;&#21516;&#27169;&#22359;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35780;&#20272;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of question-based activities (QBAs) is wide-spread in education, traditionally forming an integral part of the learning and assessment process. In this paper, we design and evaluate an automated question generation tool for formative and summative assessment in schools. We present an expert survey of one hundred and four teachers, demonstrating the need for automated generation of QBAs, as a tool that can significantly reduce the workload of teachers and facilitate personalized learning experiences. Leveraging the recent advancements in generative AI, we then present a modular framework employing transformer based language models for automatic generation of multiple-choice questions (MCQs) from textual content. The presented solution, with distinct modules for question generation, correct answer prediction, and distractor formulation, enables us to evaluate different language models and generation techniques. Finally, we perform an extensive quantitative and qualitative evaluat
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#36895;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#24615;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#30495;&#38451;&#24615;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.14974</link><description>&lt;p&gt;
&#22312;&#19968;&#21315;&#24180;&#21069;&#30340;&#25289;&#19969;&#25991;&#26412;&#20013;&#26816;&#27979;&#21477;&#23376;&#32423;&#21035;&#30340;&#24615;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Detecting Sexual Content at the Sentence Level in First Millennium Latin Texts. (arXiv:2309.14974v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#36895;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#24615;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#30495;&#38451;&#24615;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#24555;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#39033;&#20256;&#32479;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#32422;2500&#20010;&#21477;&#23376;&#65292;&#28085;&#30422;&#20102;&#20174;&#20844;&#20803;&#21069;300&#24180;&#21040;&#20844;&#20803;900&#24180;&#30340;&#24615;&#35821;&#20041;&#23398;&#65288;&#21307;&#23398;&#65292;&#24773;&#33394;&#31561;&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#21477;&#23376;&#20998;&#31867;&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#37117;&#27604;&#31616;&#21333;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#25628;&#32034;&#26041;&#27861;&#26356;&#22909;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20010;&#20154;&#35328;&#35821;&#21644;&#31038;&#20250;&#35328;&#35821;&#20803;&#25968;&#25454;&#23884;&#20837;&#65288;&#19990;&#32426;&#65292;&#20316;&#32773;&#65292;&#20889;&#20316;&#31867;&#22411;&#65289;&#30340;&#25972;&#21512;&#65292;&#20294;&#21457;&#29616;&#36825;&#23548;&#33268;&#20102;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;HAN&#20998;&#21035;&#36798;&#21040;&#20102;70.60%&#30340;&#39640;&#31934;&#24230;&#21644;86.33%&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65288;420&#32780;&#19981;&#26159;2013&#65289;&#65292;&#24182;&#26174;&#31034;&#20986;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#27169;&#22411;&#24615;&#33021;&#21487;&#33021;&#31245;&#26377;&#19979;&#38477;&#65292;&#20294;&#24615;&#33021;&#20173;&#28982;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose to evaluate the use of deep learning methods for semantic classification at the sentence level to accelerate the process of corpus building in the field of humanities and linguistics, a traditional and time-consuming task. We introduce a novel corpus comprising around 2500 sentences spanning from 300 BCE to 900 CE including sexual semantics (medical, erotica, etc.). We evaluate various sentence classification approaches and different input embedding layers, and show that all consistently outperform simple token-based searches. We explore the integration of idiolectal and sociolectal metadata embeddings (centuries, author, type of writing), but find that it leads to overfitting. Our results demonstrate the effectiveness of this approach, achieving high precision and true positive rates (TPR) of respectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset size on the model performances (420 instead of 2013), and show that, while our models per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#31038;&#20132;&#23186;&#20307;&#34920;&#31034;&#36136;&#37327;&#65292;&#36890;&#36807;&#20154;&#31867;&#20114;&#21160;&#24110;&#21161;&#33258;&#21160;&#21270;&#31995;&#32479;&#26816;&#27979;&#26032;&#38395;&#26469;&#28304;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#21363;&#20351;&#36827;&#34892;&#20102;&#23569;&#37327;&#30340;&#20154;&#31867;&#20114;&#21160;&#65292;&#20063;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14966</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#23398;&#20064;&#31038;&#20132;&#23186;&#20307;&#34920;&#31034;&#25913;&#21892;&#26032;&#38395;&#26469;&#28304;&#30495;&#23454;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interactively Learning Social Media Representations Improves News Source Factuality Detection. (arXiv:2309.14966v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#31038;&#20132;&#23186;&#20307;&#34920;&#31034;&#36136;&#37327;&#65292;&#36890;&#36807;&#20154;&#31867;&#20114;&#21160;&#24110;&#21161;&#33258;&#21160;&#21270;&#31995;&#32479;&#26816;&#27979;&#26032;&#38395;&#26469;&#28304;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#21363;&#20351;&#36827;&#34892;&#20102;&#23569;&#37327;&#30340;&#20154;&#31867;&#20114;&#21160;&#65292;&#20063;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#20852;&#36215;&#20351;&#24471;&#34394;&#20551;&#26032;&#38395;&#30340;&#24191;&#27867;&#20256;&#25773;&#25104;&#20026;&#21487;&#33021;&#65292;&#34394;&#20551;&#26032;&#38395;&#26159;&#25351;&#20197;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#24433;&#21709;&#20449;&#20208;&#20026;&#30446;&#30340;&#30340;&#25991;&#26412;&#12290;&#21450;&#26102;&#26816;&#27979;&#34394;&#20551;&#26032;&#38395;&#65292;&#29305;&#21035;&#26159;&#22312;&#26032;&#20107;&#20214;&#20986;&#29616;&#26102;&#65292;&#23545;&#20110;&#38450;&#27490;&#35823;&#23548;&#20449;&#24687;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#30417;&#30563;&#23398;&#20064;&#31995;&#32479;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#33258;&#21160;&#24314;&#27169;&#31038;&#20132;&#23186;&#20307;&#20256;&#25773;&#34394;&#20551;&#26032;&#38395;&#30340;&#22797;&#26434;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30456;&#21453;&#65292;&#36890;&#36807;&#20154;&#24037;&#23454;&#26102;&#26816;&#26597;&#25152;&#26377;&#26032;&#38395;&#26159;&#19981;&#21487;&#25193;&#23637;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20154;&#20204;&#21487;&#20197;&#19982;&#33258;&#21160;&#21270;&#31995;&#32479;&#20114;&#21160;&#65292;&#24110;&#21161;&#20854;&#23398;&#20064;&#26356;&#22909;&#30340;&#31038;&#20132;&#23186;&#20307;&#34920;&#31034;&#36136;&#37327;&#12290;&#22312;&#30495;&#23454;&#20107;&#20214;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#36827;&#34892;&#20102;&#23569;&#37327;&#30340;&#20154;&#31867;&#20114;&#21160;&#65292;&#20063;&#33021;&#25552;&#39640;&#26816;&#27979;&#26032;&#38395;&#26469;&#28304;&#30495;&#23454;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of social media has enabled the widespread propagation of fake news, text that is published with an intent to spread misinformation and sway beliefs. Rapidly detecting fake news, especially as new events arise, is important to prevent misinformation.  While prior works have tackled this problem using supervised learning systems, automatedly modeling the complexities of the social media landscape that enables the spread of fake news is challenging. On the contrary, having humans fact check all news is not scalable. Thus, in this paper, we propose to approach this problem interactively, where humans can interact to help an automated system learn a better social media representation quality. On real world events, our experiments show performance improvements in detecting factuality of news sources, even after few human interactions.
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#31532;&#19968;&#35821;&#35328;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35821;&#27861;&#21477;&#27861;&#36830;&#32493;&#36716;&#21464;&#65292;&#24182;&#35777;&#26126;&#35813;&#36716;&#21464;&#23545;&#20110;&#26126;&#30830;&#23545;&#31216;&#24615;&#30340;&#25171;&#30772;&#26159;&#40065;&#26834;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.14913</link><description>&lt;p&gt;
&#38543;&#26426;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustness of the Random Language Model. (arXiv:2309.14913v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14913
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#31532;&#19968;&#35821;&#35328;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35821;&#27861;&#21477;&#27861;&#36830;&#32493;&#36716;&#21464;&#65292;&#24182;&#35777;&#26126;&#35813;&#36716;&#21464;&#23545;&#20110;&#26126;&#30830;&#23545;&#31216;&#24615;&#30340;&#25171;&#30772;&#26159;&#40065;&#26834;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#35821;&#35328;&#27169;&#22411;(De Giuli 2019)&#26159;&#19968;&#32452;&#38543;&#26426;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65292;&#37327;&#21270;&#20154;&#31867;&#21644;&#35745;&#31639;&#26426;&#35821;&#35328;&#30340;&#21477;&#27861;&#12290;&#35813;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31532;&#19968;&#35821;&#35328;&#23398;&#20064;&#22270;&#26223;&#65292;&#21363;&#20316;&#20026;&#28508;&#22312;&#35821;&#35328;&#31354;&#38388;&#20013;&#19968;&#20010;&#36864;&#28779;&#31867;&#22411;&#65292;&#25512;&#26029;&#20102;&#21521;&#35821;&#27861;&#21477;&#27861;&#30340;&#21333;&#19968;&#36830;&#32493;&#36716;&#21464;&#65292;&#20854;&#20013;&#28508;&#22312;&#30340;&#35789;&#27719;&#21644;&#20998;&#31867;&#20043;&#38388;&#30340;&#23545;&#31216;&#24615;&#20250;&#33258;&#21457;&#25171;&#30772;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#23545;&#26126;&#30830;&#23545;&#31216;&#24615;&#25171;&#30772;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#36825;&#19968;&#22270;&#26223;&#36827;&#34892;&#20102;&#20005;&#26684;&#23457;&#35270;&#65292;&#36825;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#23398;&#20064;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22330;&#26223;&#23545;&#20110;&#36825;&#31181;&#23545;&#31216;&#24615;&#30340;&#25171;&#30772;&#26159;&#40065;&#26834;&#30340;&#12290;&#19982;&#35821;&#27861;&#32593;&#32476;&#32858;&#31867;&#31995;&#25968;&#30340;&#20154;&#31867;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#34920;&#26126;&#65292;&#35266;&#23519;&#21040;&#30340;&#36716;&#21464;&#30456;&#24403;&#20110;&#20799;&#31461;24&#20010;&#26376;&#26102;&#36890;&#24120;&#32463;&#21382;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Random Language Model (De Giuli 2019) is an ensemble of stochastic context-free grammars, quantifying the syntax of human and computer languages. The model suggests a simple picture of first language learning as a type of annealing in the vast space of potential languages. In its simplest formulation, it implies a single continuous transition to grammatical syntax, at which the symmetry among potential words and categories is spontaneously broken. Here this picture is scrutinized by considering its robustness against explicit symmetry breaking, an inevitable component of learning in the real world. It is shown that the scenario is robust to such symmetry breaking. Comparison with human data on the clustering coefficient of syntax networks suggests that the observed transition is equivalent to that normally experienced by children at age 24 months.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#21106;&#30340;&#27969;&#24335;&#26426;&#22120;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#24310;&#36831;&#20998;&#21106;&#20915;&#31574;&#26469;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#21644;&#24310;&#36831;&#24179;&#34913;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.14823</link><description>&lt;p&gt;
&#26080;&#38656;&#20998;&#21106;&#30340;&#27969;&#24335;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Segmentation-Free Streaming Machine Translation. (arXiv:2309.14823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#21106;&#30340;&#27969;&#24335;&#26426;&#22120;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#24310;&#36831;&#20998;&#21106;&#20915;&#31574;&#26469;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#21644;&#24310;&#36831;&#24179;&#34913;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24335;&#26426;&#22120;&#32763;&#35793;&#26159;&#23454;&#26102;&#23558;&#26410;&#38480;&#23450;&#36755;&#20837;&#25991;&#26412;&#27969;&#36827;&#34892;&#32763;&#35793;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#32423;&#32852;&#26041;&#27861;&#23558;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#32467;&#21512;&#65292;&#20381;&#36182;&#20110;&#20013;&#38388;&#30340;&#20998;&#21106;&#27493;&#39588;&#65292;&#23558;&#36716;&#24405;&#27969;&#20998;&#21106;&#25104;&#21477;&#23376;&#26679;&#24335;&#30340;&#21333;&#20301;&#12290;&#28982;&#32780;&#65292;&#30828;&#20998;&#21106;&#30340;&#24341;&#20837;&#38480;&#21046;&#20102;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#19988;&#26159;&#38169;&#35823;&#30340;&#26469;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#21106;&#30340;&#26694;&#26550;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#29983;&#25104;&#32763;&#35793;&#20043;&#21069;&#24310;&#36831;&#20998;&#21106;&#20915;&#31574;&#65292;&#32763;&#35793;&#26410;&#20998;&#21106;&#30340;&#28304;&#27969;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26080;&#38656;&#20998;&#21106;&#30340;&#26694;&#26550;&#22312;&#36136;&#37327;&#24310;&#36831;&#24179;&#34913;&#26041;&#38754;&#20248;&#20110;&#20351;&#29992;&#29420;&#31435;&#20998;&#21106;&#27169;&#22411;&#30340;&#31454;&#20105;&#26041;&#27861;&#12290;&#35770;&#25991;&#36890;&#36807;&#21518;&#23558;&#20844;&#24320;&#21457;&#24067;&#36719;&#20214;&#12289;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Streaming Machine Translation (MT) is the task of translating an unbounded input text stream in real-time. The traditional cascade approach, which combines an Automatic Speech Recognition (ASR) and an MT system, relies on an intermediate segmentation step which splits the transcription stream into sentence-like units. However, the incorporation of a hard segmentation constrains the MT system and is a source of errors. This paper proposes a Segmentation-Free framework that enables the model to translate an unsegmented source stream by delaying the segmentation decision until the translation has been generated. Extensive experiments show how the proposed Segmentation-Free framework has better quality-latency trade-off than competing approaches that use an independent segmentation model. Software, data and models will be released upon paper acceptance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25277;&#21462;&#22411;QA&#27169;&#22411;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26723;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#38480;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32454;&#35843;&#24503;&#35821;QA&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#38024;&#23545;&#23450;&#21046;&#21270;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14805</link><description>&lt;p&gt;
&#32454;&#35843;&#21644;&#23545;&#40784;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#20197;&#36827;&#34892;&#22797;&#26434;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning and aligning question answering models for complex information extraction tasks. (arXiv:2309.14805v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25277;&#21462;&#22411;QA&#27169;&#22411;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26723;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#38480;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32454;&#35843;&#24503;&#35821;QA&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#38024;&#23545;&#23450;&#21046;&#21270;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#25552;&#21319;&#20102;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#21487;&#33021;&#24615;&#12290;&#23613;&#31649;&#20687;ChatGPT&#36825;&#26679;&#30340;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#20026;&#19968;&#20123;&#21830;&#19994;&#29992;&#20363;&#24320;&#21551;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#20294;&#20854;&#24403;&#21069;&#20542;&#21521;&#20110;&#20135;&#29983;&#34394;&#20551;&#20869;&#23481;&#30340;&#29305;&#28857;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#22312;&#25991;&#26723;&#20998;&#26512;(&#22914;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#20449;&#24687;)&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#30456;&#21453;&#65292;&#20687;&#38382;&#39064;&#22238;&#31572;(QA)&#25110;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#36825;&#26679;&#30340;&#25277;&#21462;&#22411;&#35821;&#35328;&#27169;&#22411;&#20445;&#35777;&#26597;&#35810;&#32467;&#26524;&#22312;&#30456;&#24212;&#19978;&#19979;&#25991;&#25991;&#26723;&#30340;&#36793;&#30028;&#20869;&#65292;&#20351;&#20854;&#25104;&#20026;&#20844;&#21496;&#29983;&#20135;&#29615;&#22659;&#20013;&#26356;&#21487;&#38752;&#30340;&#20449;&#24687;&#25552;&#21462;&#20505;&#36873;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#21644;&#25972;&#21512;&#25277;&#21462;&#22411;QA&#27169;&#22411;&#26469;&#25913;&#36827;&#23545;&#24503;&#35821;&#21830;&#19994;&#25991;&#26723;(&#22914;&#20445;&#38505;&#25253;&#21578;&#25110;&#33647;&#21697;&#35828;&#26126;&#20070;)&#30340;&#29305;&#24449;&#25552;&#21462;&#65292;&#24418;&#25104;&#19968;&#20010;&#25991;&#26723;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#32454;&#35843;&#29616;&#26377;&#24503;&#35821;QA&#27169;&#22411;&#21487;&#20197;&#25552;&#21319;&#38024;&#23545;&#23450;&#21046;&#21270;&#20449;&#24687;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of Large Language Models (LLMs) has boosted performance and possibilities in various NLP tasks. While the usage of generative AI models like ChatGPT opens up new opportunities for several business use cases, their current tendency to hallucinate fake content strongly limits their applicability to document analysis, such as information retrieval from documents. In contrast, extractive language models like question answering (QA) or passage retrieval models guarantee query results to be found within the boundaries of an according context document, which makes them candidates for more reliable information extraction in productive environments of companies. In this work we propose an approach that uses and integrates extractive QA models for improved feature extraction of German business documents such as insurance reports or medical leaflets into a document analysis solution. We further show that fine-tuning existing German QA models boosts performance for tailored extractio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#19982;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#32467;&#21512;&#24212;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#20998;&#31867;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#38646;&#21806;&#19994;&#30340;&#23458;&#25143;&#21644;&#20195;&#29702;&#20154;&#20132;&#20114;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#19979;&#65292;SLM T5-base&#33021;&#22815;&#23454;&#29616;&#32422;75%&#30340;&#20934;&#30830;&#29575;&#65292;&#23637;&#29616;&#20102;SLMs&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14779</link><description>&lt;p&gt;
&#20351;&#29992;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#25506;&#32034;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#25928;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Small Language Models with Prompt-Learning Paradigm for Efficient Domain-Specific Text Classification. (arXiv:2309.14779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#19982;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#32467;&#21512;&#24212;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#20998;&#31867;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#38646;&#21806;&#19994;&#30340;&#23458;&#25143;&#21644;&#20195;&#29702;&#20154;&#20132;&#20114;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#19979;&#65292;SLM T5-base&#33021;&#22815;&#23454;&#29616;&#32422;75%&#30340;&#20934;&#30830;&#29575;&#65292;&#23637;&#29616;&#20102;SLMs&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#25163;&#21160;&#26631;&#35760;&#30340;&#39640;&#25104;&#26412;&#65292;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#20998;&#31867;&#38754;&#20020;&#31232;&#32570;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#25552;&#31034;&#23398;&#20064;&#20316;&#20026;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#20294;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65292;&#23567;&#20110;10&#20159;&#20010;&#21442;&#25968;&#65289;&#22312;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#23450;&#21046;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#65292;&#31526;&#21512;&#24037;&#19994;&#32422;&#26463;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;SLMs&#19982;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#32467;&#21512;&#24212;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#20998;&#31867;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#38646;&#21806;&#19994;&#30340;&#23458;&#25143;&#21644;&#20195;&#29702;&#20154;&#20132;&#20114;&#20013;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#23569;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#21487;&#20197;&#36827;&#34892;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#24494;&#35843;&#26102;&#65292;&#20855;&#26377;220M&#21442;&#25968;&#30340;&#20856;&#22411;SLM T5-base&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#19978;&#23454;&#29616;&#32422;75%&#30340;&#20934;&#30830;&#29575;&#65288;&#36798;&#21040;&#23436;&#25972;&#25968;&#25454;&#30340;15%&#65289;&#65292;&#26174;&#31034;&#20986;SLMs&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-specific text classification faces the challenge of scarce labeled data due to the high cost of manual labeling. Prompt-learning, known for its efficiency in few-shot scenarios, is proposed as an alternative to traditional fine-tuning methods. And besides, although large language models (LLMs) have gained prominence, small language models (SLMs, with under 1B parameters) offer significant customizability, adaptability, and cost-effectiveness for domain-specific tasks, given industry constraints. In this study, we investigate the potential of SLMs combined with prompt-learning paradigm for domain-specific text classification, specifically within customer-agent interactions in retail. Our evaluations show that, in few-shot settings when prompt-based model fine-tuning is possible, T5-base, a typical SLM with 220M parameters, achieve approximately 75% accuracy with limited labeled data (up to 15% of full data), which shows great potentials of SLMs with prompt-learning. Based on this
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#22270;&#20687;&#26631;&#39064;&#27169;&#22411;&#30340;&#21442;&#25968;&#24182;&#21482;&#35843;&#25972;&#38468;&#21152;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#31227;&#21160;&#35774;&#22791;&#23631;&#24149;&#25130;&#22270;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#20013;&#22823;&#37327;&#21442;&#25968;&#30340;&#24320;&#38144;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14774</link><description>&lt;p&gt;
BLIP-Adapter&#65306;&#31227;&#21160;&#35774;&#22791;&#23631;&#24149;&#25130;&#22270;&#26631;&#39064;&#29983;&#25104;&#30340;&#21442;&#25968;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BLIP-Adapter: Parameter-Efficient Transfer Learning for Mobile Screenshot Captioning. (arXiv:2309.14774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#22270;&#20687;&#26631;&#39064;&#27169;&#22411;&#30340;&#21442;&#25968;&#24182;&#21482;&#35843;&#25972;&#38468;&#21152;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#31227;&#21160;&#35774;&#22791;&#23631;&#24149;&#25130;&#22270;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#20013;&#22823;&#37327;&#21442;&#25968;&#30340;&#24320;&#38144;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#26377;&#25928;&#30340;&#35843;&#25972;&#26041;&#27861;&#26469;&#22788;&#29702;&#23631;&#24149;&#25130;&#22270;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#26631;&#39064;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26159;&#23545;&#20110;&#31227;&#21160;&#35774;&#22791;&#23631;&#24149;&#25130;&#22270;&#30340;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#30446;&#21069;&#30340;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#26696;&#20363;&#20013;&#23545;&#20135;&#21697;&#25130;&#23631;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#30340;&#25551;&#36848;&#30456;&#23545;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23581;&#35797;&#23545;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#35299;&#20915;&#23631;&#24149;&#25130;&#22270;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#28040;&#32791;&#22823;&#37327;&#36164;&#28304;&#65292;&#38656;&#35201;&#32771;&#34385;&#21040;&#22270;&#20687;&#26631;&#39064;&#29983;&#25104;&#27169;&#22411;&#20013;&#22823;&#37327;&#21442;&#25968;&#30340;&#26102;&#38388;&#12289;&#35745;&#31639;&#21147;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#37197;&#22120;&#26041;&#27861;&#30340;&#32452;&#21512;&#65292;&#21482;&#38656;&#35201;&#35843;&#25972;&#27169;&#22411;&#20013;&#30340;&#38468;&#21152;&#27169;&#22359;&#12290;&#36825;&#20123;&#26041;&#27861;&#26368;&#21021;&#26159;&#20026;&#35270;&#35273;&#25110;&#35821;&#35328;&#20219;&#21153;&#35774;&#35745;&#30340;&#65292;&#25105;&#20204;&#30340;&#24847;&#22270;&#26159;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#35299;&#20915;&#23631;&#24149;&#25130;&#22270;&#26631;&#39064;&#29983;&#25104;&#20013;&#30340;&#31867;&#20284;&#25361;&#25112;&#12290;&#36890;&#36807;&#20923;&#32467;&#22270;&#20687;&#26631;&#39064;&#27169;&#22411;&#30340;&#21442;&#25968;&#24182;&#35757;&#32451;&#20854;&#20182;&#27169;&#22359;&#65292;&#21487;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to explore efficient tuning methods for the screenshot captioning task. Recently, image captioning has seen significant advancements, but research in captioning tasks for mobile screens remains relatively scarce. Current datasets and use cases describing user behaviors within product screenshots are notably limited. Consequently, we sought to fine-tune pre-existing models for the screenshot captioning task. However, fine-tuning large pre-trained models can be resource-intensive, requiring considerable time, computational power, and storage due to the vast number of parameters in image captioning models. To tackle this challenge, this study proposes a combination of adapter methods, which necessitates tuning only the additional modules on the model. These methods are originally designed for vision or language tasks, and our intention is to apply them to address similar challenges in screenshot captioning. By freezing the parameters of the image caption models and trainin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20107;&#23454;&#30693;&#35782;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#35843;&#20248;&#26694;&#26550;&#26469;&#25913;&#21892;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14771</link><description>&lt;p&gt;
&#20351;&#29992;&#20107;&#23454;&#30693;&#35782;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Boosting In-Context Learning with Factual Knowledge. (arXiv:2309.14771v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20107;&#23454;&#30693;&#35782;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#35843;&#20248;&#26694;&#26550;&#26469;&#25913;&#21892;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26088;&#22312;&#36890;&#36807;&#20381;&#36182;&#20110;&#23569;&#37327;&#30340;&#35757;&#32451;&#31034;&#20363;&#35299;&#20915;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#28040;&#38500;&#21442;&#25968;&#26356;&#26032;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20107;&#23454;&#30693;&#35782;&#22312;ICL&#30340;&#24615;&#33021;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#21253;&#25324;&#22312;LLM&#20013;&#23398;&#21040;&#30340;&#22266;&#26377;&#30693;&#35782;&#65292;&#20174;&#25152;&#36873;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#24471;&#20986;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20197;&#21450;LLM&#22312;&#36755;&#20986;&#29983;&#25104;&#20013;&#30340;&#30693;&#35782;&#20559;&#24046;&#12290;&#20026;&#20102;&#21457;&#25381;LLM&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#35843;&#20248;&#65288;KICT&#65289;&#26694;&#26550;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;ICL&#30340;&#24615;&#33021;&#65306;1&#65289;&#22312;&#25345;&#32493;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26399;&#38388;&#21521;LLM&#27880;&#20837;&#20107;&#23454;&#30693;&#35782;&#65292;2&#65289;&#35880;&#24910;&#36873;&#25321;&#20855;&#26377;&#39640;&#30693;&#35782;&#30456;&#20851;&#24615;&#30340;&#31034;&#20363;&#65292;3&#65289;&#26681;&#25454;&#20808;&#21069;&#30340;&#30693;&#35782;&#23545;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#26657;&#20934;&#12290;&#25105;&#20204;&#22312;&#33258;&#22238;&#24402;LLM&#65288;&#22914;GPT&#39118;&#26684;&#27169;&#22411;&#65289;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL) over Large language models (LLMs) aims at solving previously unseen tasks by conditioning on a few training examples, eliminating the need for parameter updates and achieving competitive performance. In this paper, we demonstrate that factual knowledge is imperative for the performance of ICL in three core facets, i.e., the inherent knowledge learned in LLMs, the factual knowledge derived from the selected in-context examples, and the knowledge biases in LLMs for output generation. To unleash the power of LLMs in few-shot learning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further improve the performance of ICL: 1) injecting factual knowledge to LLMs during continual self-supervised pre-training, 2) judiciously selecting the examples with high knowledge relevance, and 3) calibrating the prediction results based on prior knowledge. We evaluate the proposed approaches on auto-regressive LLMs (e.g., GPT-style models) over 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#20851;&#31995;&#24314;&#27169;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#29983;&#25104;&#36830;&#36143;&#30340;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#21453;&#21521;&#20851;&#31995;&#21019;&#24314;&#23545;&#31216;&#22270;&#26469;&#25552;&#20379;&#39069;&#22806;&#30340;&#26631;&#31614;&#21644;&#34917;&#20805;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.14770</link><description>&lt;p&gt;
KERMIT: &#24102;&#26377;&#21453;&#36716;&#21464;&#25442;&#30340;&#22686;&#24378;&#20851;&#31995;&#24314;&#27169;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with Inverse Transformation. (arXiv:2309.14770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#20851;&#31995;&#24314;&#27169;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#29983;&#25104;&#36830;&#36143;&#30340;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#21453;&#21521;&#20851;&#31995;&#21019;&#24314;&#23545;&#31216;&#22270;&#26469;&#25552;&#20379;&#39069;&#22806;&#30340;&#26631;&#31614;&#21644;&#34917;&#20805;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26159;&#19968;&#39033;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#21487;&#29992;&#20449;&#24687;&#22635;&#20805;&#32570;&#22833;&#19977;&#20803;&#32452;&#30340;&#20219;&#21153;&#12290;&#22312;&#24403;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#19977;&#20803;&#32452;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24314;&#27169;&#26041;&#27861;&#21487;&#33021;&#36935;&#21040;&#19968;&#20123;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#24403;&#25551;&#36848;&#19981;&#33021;&#20934;&#30830;&#20805;&#20998;&#22320;&#34920;&#36798;&#39044;&#26399;&#21547;&#20041;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20004;&#20010;&#39069;&#22806;&#26426;&#21046;&#26469;&#22686;&#21152;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20316;&#20026;&#22806;&#37096;&#30693;&#35782;&#24211;&#65292;&#29983;&#25104;&#36830;&#36143;&#30340;&#25551;&#36848;&#20197;&#24357;&#34917;&#26597;&#35810;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#21453;&#21521;&#20851;&#31995;&#21019;&#24314;&#23545;&#31216;&#22270;&#65292;&#20174;&#32780;&#20026;&#38142;&#25509;&#39044;&#27979;&#25552;&#20379;&#39069;&#22806;&#30340;&#26631;&#31614;&#21644;&#34917;&#20805;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20851;&#31995;&#23454;&#20307;&#20043;&#38388;&#39069;&#22806;&#30340;&#27934;&#23519;&#21147;&#12290;&#36890;&#36807;&#36825;&#20123;&#21162;&#21147;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion is a task that revolves around filling in missing triples based on the information available in a knowledge graph. Among the current studies, text-based methods complete the task by utilizing textual descriptions of triples. However, this modeling approach may encounter limitations, particularly when the description fails to accurately and adequately express the intended meaning. To overcome these challenges, we propose the augmentation of data through two additional mechanisms. Firstly, we employ ChatGPT as an external knowledge base to generate coherent descriptions to bridge the semantic gap between the queries and answers. Secondly, we leverage inverse relations to create a symmetric graph, thereby creating extra labeling and providing supplementary information for link prediction. This approach offers additional insights into the relationships between entities. Through these efforts, we have observed significant improvements in knowledge graph completion
&lt;/p&gt;</description></item><item><title>ConPET&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PET&#65289;&#21644;&#21160;&#24577;&#22238;&#25918;&#31574;&#30053;&#65292;&#20943;&#23569;&#35843;&#25972;&#25104;&#26412;&#12289;&#32531;&#35299;&#36807;&#24230;&#25311;&#21512;&#21644;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14763</link><description>&lt;p&gt;
ConPET: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
ConPET: Continual Parameter-Efficient Tuning for Large Language Models. (arXiv:2309.14763v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14763
&lt;/p&gt;
&lt;p&gt;
ConPET&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PET&#65289;&#21644;&#21160;&#24577;&#22238;&#25918;&#31574;&#30053;&#65292;&#20943;&#23569;&#35843;&#25972;&#25104;&#26412;&#12289;&#32531;&#35299;&#36807;&#24230;&#25311;&#21512;&#21644;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#25345;&#32493;&#35843;&#25972;&#20197;&#36866;&#24212;&#26032;&#20986;&#29616;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#23545;&#26087;&#20219;&#21153;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#23545;&#20110;&#20855;&#26377;&#20840;&#21442;&#25968;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35828;&#65292;&#36825;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#65292;&#21407;&#22240;&#26159;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#20869;&#23384;&#28040;&#32791;&#22823;&#19988;&#23481;&#26131;&#36951;&#24536;&#12290;&#21463;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PET&#65289;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25345;&#32493;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;ConPET&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;LLMs&#30340;&#20855;&#26377;&#20219;&#21153;&#25968;&#37327;&#26080;&#20851;&#35757;&#32451;&#22797;&#26434;&#24230;&#30340;&#24310;&#32493;&#20219;&#21153;&#36866;&#24212;&#30340;&#36890;&#29992;&#33539;&#24335;&#12290;ConPET&#21253;&#25324;&#20004;&#20010;&#29256;&#26412;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#39318;&#20808;&#65292;&#38745;&#24577;ConPET&#21487;&#20197;&#36890;&#36807;PET&#21644;&#21160;&#24577;&#22238;&#25918;&#31574;&#30053;&#23558;&#21407;&#26412;&#38024;&#23545;&#36739;&#23567;&#27169;&#22411;&#35774;&#35745;&#30340;&#26087;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#21040;LLMs&#20013;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#35843;&#25972;&#25104;&#26412;&#65292;&#32531;&#35299;&#36807;&#24230;&#25311;&#21512;&#21644;&#36951;&#24536;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#25345;&#21487;&#20280;&#32553;&#24615;&#65292;&#21160;&#24577;ConPET&#20026;&#19981;&#21516;&#20219;&#21153;&#37319;&#29992;&#21333;&#29420;&#30340;PET&#27169;&#22359;&#21644;PET&#27169;&#22359;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning necessitates the continual adaptation of models to newly emerging tasks while minimizing the catastrophic forgetting of old ones. This is extremely challenging for large language models (LLMs) with vanilla full-parameter tuning due to high computation costs, memory consumption, and forgetting issue. Inspired by the success of parameter-efficient tuning (PET), we propose Continual Parameter-Efficient Tuning (ConPET), a generalizable paradigm for continual task adaptation of LLMs with task-number-independent training complexity. ConPET includes two versions with different application scenarios. First, Static ConPET can adapt former continual learning methods originally designed for relatively smaller models to LLMs through PET and a dynamic replay strategy, which largely reduces the tuning costs and alleviates the over-fitting and forgetting issue. Furthermore, to maintain scalability, Dynamic ConPET adopts separate PET modules for different tasks and a PET module sele
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;CodeT5&#36827;&#34892;&#26368;&#23567;&#32534;&#36753;&#30340;&#31243;&#24207;&#20462;&#22797;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#38169;&#35823;&#21644;&#27491;&#30830;&#31243;&#24207;&#30340;&#20195;&#30721;&#23545;&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;CodeT5&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#26524;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.14760</link><description>&lt;p&gt;
&#20351;&#29992;CodeT5&#36827;&#34892;&#26368;&#23567;&#32534;&#36753;&#30340;&#31243;&#24207;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Program Repair with Minimal Edits Using CodeT5. (arXiv:2309.14760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;CodeT5&#36827;&#34892;&#26368;&#23567;&#32534;&#36753;&#30340;&#31243;&#24207;&#20462;&#22797;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#38169;&#35823;&#21644;&#27491;&#30830;&#31243;&#24207;&#30340;&#20195;&#30721;&#23545;&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;CodeT5&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#21592;&#24448;&#24448;&#38590;&#20197;&#35782;&#21035;&#21644;&#20462;&#22797;&#31243;&#24207;&#20013;&#30340;&#38169;&#35823;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#32463;&#34987;&#25552;&#20986;&#29992;&#26469;&#20462;&#22797;&#38169;&#35823;&#30340;&#31243;&#24207;&#24182;&#25903;&#25345;&#38169;&#35823;&#24674;&#22797;&#12290;&#28982;&#32780;&#65292;LMs&#24448;&#24448;&#20250;&#29983;&#25104;&#19982;&#21407;&#22987;&#36755;&#20837;&#31243;&#24207;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#30340;&#29702;&#35299;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;CodeT5&#24314;&#35758;&#36827;&#34892;&#26368;&#23567;&#20462;&#22797;&#32534;&#36753;&#30340;&#27491;&#30830;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#38169;&#35823;&#21644;&#27491;&#30830;&#31243;&#24207;&#30340;&#20195;&#30721;&#23545;&#19978;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;CodeT5&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;CodeT5&#30340;&#36890;&#36807;&#29575;&#20026;91.95%&#65292;&#26368;&#30456;&#20284;&#30340;&#27491;&#30830;&#31243;&#24207;&#30340;&#24179;&#22343;&#32534;&#36753;&#36317;&#31163;&#20026;6.84&#65292;&#36825;&#34920;&#26126;&#33267;&#23569;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;100&#20010;&#20505;&#36873;&#31243;&#24207;&#26469;&#24314;&#35758;&#19968;&#20010;&#27491;&#30830;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#21021;&#32423;&#32534;&#31243;&#38382;&#39064;&#26102;&#24314;&#35758;&#20351;&#29992;&#26368;&#23567;&#32534;&#36753;&#30340;&#31243;&#24207;&#20462;&#22797;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Programmers often struggle to identify and fix bugs in their programs. In recent years, many language models (LMs) have been proposed to fix erroneous programs and support error recovery. However, the LMs tend to generate solutions that differ from the original input programs. This leads to potential comprehension difficulties for users. In this paper, we propose an approach to suggest a correct program with minimal repair edits using CodeT5. We fine-tune a pre-trained CodeT5 on code pairs of wrong and correct programs and evaluate its performance with several baseline models. The experimental results show that the fine-tuned CodeT5 achieves a pass@100 of 91.95% and an average edit distance of the most similar correct program of 6.84, which indicates that at least one correct program can be suggested by generating 100 candidate programs. We demonstrate the effectiveness of LMs in suggesting program repair with minimal edits for solving introductory programming problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;AILQA&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#35299;&#26512;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#24182;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.14735</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#21516;&#26816;&#32034;&#21644;&#38382;&#31572;&#27169;&#22411;&#30340;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Artificial Intelligence for Indian Legal Question Answering (AILQA) Using Different Retrieval and QA Models. (arXiv:2309.14735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;AILQA&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#35299;&#26512;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#24182;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#38382;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#26377;&#28508;&#21147;&#25913;&#21464;&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#19982;&#26696;&#20363;&#25991;&#20214;&#30340;&#20114;&#21160;&#26041;&#24335;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#21360;&#24230;&#27861;&#24459;&#20307;&#31995;&#19979;&#22238;&#31572;&#27861;&#24459;&#38382;&#39064;&#30340;&#25928;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#65288;AILQA&#65289;&#24182;&#30740;&#31350;&#20102;&#24403;&#21069;&#21487;&#29992;&#30340;&#19981;&#21516;&#26816;&#32034;&#21644;QA&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#21033;&#29992;OpenAI GPT&#27169;&#22411;&#20316;&#20026;&#22522;&#20934;&#65292;&#32467;&#21512;&#26597;&#35810;&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#29616;&#26377;&#30340;AILQA&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#35299;&#26512;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#24182;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#26412;&#30740;&#31350;&#29305;&#21035;&#20851;&#27880;&#21360;&#24230;&#21009;&#20107;&#21496;&#27861;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35813;&#39046;&#22495;&#30001;&#20110;&#22797;&#26434;&#24615;&#21644;&#36164;&#28304;&#38480;&#21046;&#32780;&#38754;&#20020;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;&#20026;&#20102;&#20005;&#26684;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32463;&#39564;&#35777;&#35780;&#20272;&#19982;&#20174;&#23454;&#36341;&#20013;&#33719;&#24471;&#30340;&#21453;&#39304;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal question-answering (QA) systems have the potential to revolutionize the way legal professionals interact with case law documents. This paper conducts a comparative analysis of existing artificial intelligence models for their utility in answering legal questions within the Indian legal system, specifically focusing on Indian Legal Question Answering (AILQA) and our study investigates the efficacy of different retrieval and QA algorithms currently available. Utilizing the OpenAI GPT model as a benchmark, along with query prompts, our investigation shows that existing AILQA systems can automatically interpret natural language queries from users and generate highly accurate responses. This research is particularly focused on applications within the Indian criminal justice domain, which has its own set of challenges due to its complexity and resource constraints. In order to rigorously assess the performance of these models, empirical evaluations are complemented by feedback from pra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26356;&#36866;&#24212;&#20110;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#35813;&#27169;&#22411;&#20998;&#20026;&#20010;&#20154;&#32423;&#21035;&#12289;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#23567;&#22411;&#21270;&#20197;&#36866;&#24212;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#65292;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.14726</link><description>&lt;p&gt;
PLMM&#65306;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PLMM: Personal Large Models on Mobile Devices. (arXiv:2309.14726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26356;&#36866;&#24212;&#20110;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#35813;&#27169;&#22411;&#20998;&#20026;&#20010;&#20154;&#32423;&#21035;&#12289;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#23567;&#22411;&#21270;&#20197;&#36866;&#24212;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#65292;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26356;&#36866;&#24212;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#22914;&#25945;&#32946;&#32972;&#26223;&#21644;&#29233;&#22909;&#12290;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#20026;&#19977;&#20010;&#32423;&#21035;&#65306;&#20010;&#20154;&#32423;&#21035;&#65292;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#12290;&#20010;&#20154;&#32423;&#21035;&#27169;&#22411;&#36866;&#24212;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#23545;&#29992;&#25143;&#30340;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#24182;&#20445;&#25252;&#20854;&#38544;&#31169;&#12290;&#19987;&#23478;&#32423;&#21035;&#27169;&#22411;&#19987;&#27880;&#20110;&#21512;&#24182;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#22914;&#37329;&#34701;&#12289;IT&#21644;&#33402;&#26415;&#12290;&#20256;&#32479;&#27169;&#22411;&#19987;&#27880;&#20110;&#26222;&#36941;&#30693;&#35782;&#30340;&#21457;&#29616;&#21644;&#25552;&#21319;&#19987;&#23478;&#27169;&#22411;&#12290;&#22312;&#36825;&#26679;&#30340;&#20998;&#31867;&#20013;&#65292;&#20010;&#20154;&#27169;&#22411;&#30452;&#25509;&#19982;&#29992;&#25143;&#20132;&#20114;&#12290;&#23545;&#20110;&#25972;&#20010;&#31995;&#32479;&#26469;&#35828;&#65292;&#20010;&#20154;&#27169;&#22411;&#20855;&#26377;&#29992;&#25143;&#30340;&#65288;&#21152;&#23494;&#30340;&#65289;&#20010;&#20154;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#36275;&#22815;&#23567;&#20197;&#22312;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#12290;&#26368;&#21518;&#65292;&#23427;&#20204;&#36824;&#24517;&#39035;&#23454;&#26102;&#21709;&#24212;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by Federated Learning, in this paper, we propose personal large models that are distilled from traditional large language models but more adaptive to local users' personal information such as education background and hobbies. We classify the large language models into three levels: the personal level, expert level and traditional level. The personal level models are adaptive to users' personal information. They encrypt the users' input and protect their privacy. The expert level models focus on merging specific knowledge such as finance, IT and art. The traditional models focus on the universal knowledge discovery and upgrading the expert models. In such classifications, the personal models directly interact with the user. For the whole system, the personal models have users' (encrypted) personal information. Moreover, such models must be small enough to be performed on personal computers or mobile devices. Finally, they also have to response in real-time for better user exper
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31616;&#21333;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#65292;&#36890;&#36807;&#21516;&#26102;&#32534;&#30721;&#25991;&#26412;&#21644;&#22270;&#20687;&#21040;&#38544;&#34255;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;U-Net&#36827;&#34892;&#22270;&#20687;&#37325;&#24314;&#65292;&#33021;&#22815;&#29983;&#25104;&#26377;&#24076;&#26395;&#30340;&#35270;&#39057;&#12290;</title><link>http://arxiv.org/abs/2309.14683</link><description>&lt;p&gt;
&#36890;&#36807;Transformer&#30340;&#31616;&#21333;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Simple Text to Video Model via Transformer. (arXiv:2309.14683v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14683
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31616;&#21333;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#65292;&#36890;&#36807;&#21516;&#26102;&#32534;&#30721;&#25991;&#26412;&#21644;&#22270;&#20687;&#21040;&#38544;&#34255;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;U-Net&#36827;&#34892;&#22270;&#20687;&#37325;&#24314;&#65292;&#33021;&#22815;&#29983;&#25104;&#26377;&#24076;&#26395;&#30340;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#36890;&#29992;&#31616;&#21333;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#12290;&#30001;&#20110;&#25991;&#26412;&#21644;&#35270;&#39057;&#37117;&#26159;&#24207;&#21015;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#32534;&#30721;&#20026;&#30456;&#21516;&#30340;&#38544;&#34255;&#31354;&#38388;&#65292;&#28982;&#21518;&#23558;&#20854;&#36755;&#20837;&#21040;Transformer&#20013;&#20197;&#25429;&#25417;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#20877;&#36890;&#36807;&#35299;&#30721;&#22120;&#29983;&#25104;&#25991;&#26412;&#25110;&#22270;&#20687;&#12290;&#32771;&#34385;&#21040;&#38271;&#24207;&#21015;&#20013;&#22270;&#20687;&#20449;&#21495;&#21487;&#33021;&#21464;&#24369;&#65292;&#25105;&#20204;&#24341;&#20837;U-Net&#26469;&#20174;&#22270;&#20687;&#30340;&#22122;&#22768;&#29256;&#26412;&#20013;&#37325;&#24314;&#22270;&#20687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#38271;&#24207;&#21015;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#21407;&#22987;&#22270;&#20687;&#30340;&#22122;&#22768;&#32423;&#21035;&#65292;&#28982;&#21518;&#20351;&#29992;U-Net&#20013;&#30340;&#19979;&#37319;&#26679;&#27169;&#22359;&#26469;&#32534;&#30721;&#24102;&#22122;&#22768;&#30340;&#22270;&#20687;&#65292;&#36827;&#32780;&#36755;&#20837;&#21040;Transformer&#20013;&#20197;&#39044;&#27979;&#19979;&#19968;&#20010;&#28165;&#26224;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#28155;&#21152;&#20102;&#19968;&#20010;&#32422;&#26463;&#26465;&#20214;&#65292;&#20197;&#20419;&#36827;&#35270;&#39057;&#20013;&#20219;&#24847;&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#20043;&#38388;&#30340;&#36816;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;GPT2&#65292;&#24182;&#22312;UCF101&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#21487;&#20197;&#29983;&#25104;&#26377;&#24076;&#26395;&#30340;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a general and simple text to video model based on Transformer. Since both text and video are sequential data, we encode both texts and images into the same hidden space, which are further fed into Transformer to capture the temporal consistency and then decoder to generate either text or images. Considering the image signal may become weak in the long sequence, we introduce the U-Net to reconstruct image from its noised version. Specifically, we increase the noise level to the original image in the long sequence, then use the $down$ module from U-Net to encode noised images, which are further input to transformer to predict next clear images. We also add a constraint to promote motion between any generated image pair in the video. We use GPT2 and test our approach on UCF101 dataset and show it can generate promising videos.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;FP8&#26684;&#24335;&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#30456;&#23545;&#20110;INT8&#20855;&#26377;&#26356;&#22909;&#30340;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14592</link><description>&lt;p&gt;
&#20351;&#29992;FP8&#26684;&#24335;&#30340;&#39640;&#25928;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Post-training Quantization with FP8 Formats. (arXiv:2309.14592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;FP8&#26684;&#24335;&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#30456;&#23545;&#20110;INT8&#20855;&#26377;&#26356;&#22909;&#30340;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#65292;&#22914;LLMs&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#23545;&#25913;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#30340;&#38656;&#27714;&#65292;&#20197;&#28385;&#36275;&#36825;&#20123;&#29616;&#20195;&#26550;&#26500;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;FP8&#25968;&#25454;&#26684;&#24335;&#22312;75&#20010;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#20248;&#21183;&#65292;&#36825;&#20123;&#32593;&#32476;&#26550;&#26500;&#28085;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#35328;&#24314;&#27169;&#12289;&#25991;&#26412;&#29983;&#25104;&#12289;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#20998;&#21106;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;FP8&#34920;&#31034;&#65288;E5M2&#12289;E4M3&#21644;E3M4&#65289;&#65292;&#20197;&#30740;&#31350;&#22312;&#21160;&#24577;&#33539;&#22260;&#21644;&#31934;&#24230;&#20043;&#38388;&#19981;&#21516;&#26435;&#34913;&#31243;&#24230;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#24191;&#27867;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#21487;&#20197;&#27010;&#25324;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#26684;&#24335;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;INT8&#65292;&#21253;&#25324;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#65288;92.64&#65285;&#23545;65.87&#65285;&#65289;&#12289;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#25805;&#20316;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning methods such as LLMs and Diffusion models have created a need for improved quantization methods that can meet the computational demands of these modern architectures while maintaining accuracy. Towards this goal, we study the advantages of FP8 data formats for post-training quantization across 75 unique network architectures covering a wide range of tasks, including machine translation, language modeling, text generation, image classification, generation, and segmentation. We examine three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects of varying degrees of trade-off between dynamic range and precision on model accuracy. Based on our extensive study, we developed a quantization workflow that generalizes across different network architectures. Our empirical results show that FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of ope
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#29616;&#20195;&#24076;&#20271;&#26469;&#35821;&#30340;&#22823;&#35268;&#27169;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;DictaLM&#65292;&#36890;&#36807;&#21457;&#24067;&#22522;&#30784;&#27169;&#22411;&#21644;&#35757;&#32451;&#27169;&#22411;&#65292;&#20419;&#36827;&#20102;&#24076;&#20271;&#26469;&#35821;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21021;&#22987;&#30340;&#24076;&#20271;&#26469;&#35821;LLM&#27169;&#22411;&#29992;&#20110;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.14568</link><description>&lt;p&gt;
&#24341;&#20837;DictaLM - &#19968;&#31181;&#29992;&#20110;&#29616;&#20195;&#24076;&#20271;&#26469;&#35821;&#30340;&#22823;&#35268;&#27169;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Introducing DictaLM -- A Large Generative Language Model for Modern Hebrew. (arXiv:2309.14568v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14568
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#29616;&#20195;&#24076;&#20271;&#26469;&#35821;&#30340;&#22823;&#35268;&#27169;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;DictaLM&#65292;&#36890;&#36807;&#21457;&#24067;&#22522;&#30784;&#27169;&#22411;&#21644;&#35757;&#32451;&#27169;&#22411;&#65292;&#20419;&#36827;&#20102;&#24076;&#20271;&#26469;&#35821;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21021;&#22987;&#30340;&#24076;&#20271;&#26469;&#35821;LLM&#27169;&#22411;&#29992;&#20110;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;DictaLM&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#29616;&#20195;&#24076;&#20271;&#26469;&#35821;&#37327;&#36523;&#23450;&#21046;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#25317;&#26377;70&#20159;&#20010;&#21442;&#25968;&#65292;&#20027;&#35201;&#35757;&#32451;&#20110;&#24076;&#20271;&#26469;&#35821;&#30456;&#20851;&#25968;&#25454;&#12290;&#20026;&#20102;&#20419;&#36827;&#24076;&#20271;&#26469;&#35821;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#25105;&#20204;&#20197;Creative Commons&#35768;&#21487;&#35777;&#21457;&#24067;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#35757;&#32451;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;DictaLM-Rab&#65292;&#21478;&#19968;&#20010;&#38024;&#23545;&#25289;&#27604;/&#21382;&#21490;&#24076;&#20271;&#26469;&#35821;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#21487;&#20316;&#20026;&#21508;&#31181;&#24076;&#20271;&#26469;&#35821;&#29305;&#23450;&#20219;&#21153;&#65288;&#22914;&#35828;&#26126;&#12289;&#38382;&#31572;&#12289;&#24773;&#24863;&#20998;&#26512;&#31561;&#65289;&#30340;&#29702;&#24819;&#36215;&#28857;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#21457;&#24067;&#20195;&#34920;&#20102;&#19968;&#20010;&#21021;&#27493;&#27493;&#39588;&#65292;&#20026;&#24076;&#20271;&#26469;&#35821;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#21021;&#22987;&#30340;&#24076;&#20271;&#26469;&#35821;LLM&#27169;&#22411;&#29992;&#20110;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DictaLM, a large-scale language model tailored for Modern Hebrew. Boasting 7B parameters, this model is predominantly trained on Hebrew-centric data. As a commitment to promoting research and development in the Hebrew language, we release both the foundation model and the instruct-tuned model under a Creative Commons license. Concurrently, we introduce DictaLM-Rab, another foundation model geared towards Rabbinic/Historical Hebrew. These foundation models serve as ideal starting points for fine-tuning various Hebrew-specific tasks, such as instruction, Q&amp;A, sentiment analysis, and more. This release represents a preliminary step, offering an initial Hebrew LLM model for the Hebrew NLP community to experiment with.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20889;&#20316;&#21019;&#36896;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#22312;&#21019;&#24847;&#27979;&#35797;&#20013;&#36890;&#36807;&#30340;&#25968;&#37327;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26080;&#27861;&#20195;&#26367;&#19987;&#23478;&#36827;&#34892;TTCW&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.14556</link><description>&lt;p&gt;
&#33402;&#26415;&#36824;&#26159;&#25216;&#24039;&#65311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21019;&#36896;&#21147;&#30340;&#34394;&#20551;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
Art or Artifice? Large Language Models and the False Promise of Creativity. (arXiv:2309.14556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20889;&#20316;&#21019;&#36896;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#22312;&#21019;&#24847;&#27979;&#35797;&#20013;&#36890;&#36807;&#30340;&#25968;&#37327;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26080;&#27861;&#20195;&#26367;&#19987;&#23478;&#36827;&#34892;TTCW&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#35748;&#20026;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#20174;&#21338;&#23458;&#21040;&#25925;&#20107;&#30340;&#39640;&#36136;&#37327;&#20889;&#20316;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23458;&#35266;&#35780;&#20272;&#19968;&#27573;&#25991;&#23383;&#30340;&#21019;&#36896;&#21147;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21463;&#21019;&#36896;&#24615;&#24605;&#32500;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTC)&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#20849;&#35782;&#35780;&#20272;&#25216;&#26415;[3]&#65292;&#25552;&#20986;&#20102;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#21019;&#36896;&#21147;&#20316;&#20026;&#19968;&#20010;&#20135;&#21697;&#12290;TTCW&#30001;&#21253;&#21547;&#22312;&#27969;&#30021;&#24230;&#12289;&#28789;&#27963;&#24615;&#12289;&#29420;&#21019;&#24615;&#21644;&#32454;&#33268;&#24230;&#21407;&#22987;&#32500;&#24230;&#20013;&#30340;14&#20010;&#20108;&#20803;&#27979;&#35797;&#32452;&#25104;&#12290;&#25105;&#20204;&#25307;&#21215;&#20102;10&#20301;&#21019;&#24847;&#20316;&#23478;&#65292;&#24182;&#20351;&#29992;TTCW&#23545;48&#20010;&#30001;&#19987;&#19994;&#20316;&#23478;&#25110;LLMs&#25776;&#20889;&#30340;&#25925;&#20107;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#36890;&#36807;&#30340;TTCW&#27979;&#35797;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#20102;3-10&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#35780;&#20215;&#32773;&#65292;&#20197;&#33258;&#21160;&#21270;TTCW&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#27809;&#26377;&#19968;&#20010;LLM&#19982;&#19987;&#23478;&#35780;&#20272;&#21576;&#27491;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT), which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose the Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Factually Augmented RLHF&#30340;&#26032;&#23545;&#40784;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#20107;&#23454;&#20449;&#24687;&#21152;&#20837;&#22870;&#21169;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#27169;&#22411;&#20043;&#38388;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14525</link><description>&lt;p&gt;
&#29992;&#20107;&#23454;&#22686;&#24378;&#30340;RLHF&#26041;&#27861;&#23545;&#40784;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Multimodal Models with Factually Augmented RLHF. (arXiv:2309.14525v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14525
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Factually Augmented RLHF&#30340;&#26032;&#23545;&#40784;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#20107;&#23454;&#20449;&#24687;&#21152;&#20837;&#22870;&#21169;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#27169;&#22411;&#20043;&#38388;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#26159;&#36328;&#27169;&#24577;&#26500;&#24314;&#30340;&#65292;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#21487;&#33021;&#23548;&#33268;&#8220;&#24187;&#35273;&#8221;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#36755;&#20986;&#27809;&#26377;&#19982;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#30456;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#22810;&#27169;&#24577;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#25991;&#26412;&#39046;&#22495;&#30340;&#24378;&#21270;&#23398;&#20064;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26041;&#27861;&#25913;&#36827;&#20026;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#20219;&#21153;&#65292;&#20854;&#20013;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#32773;&#27604;&#36739;&#20004;&#20010;&#21709;&#24212;&#24182;&#25351;&#20986;&#8220;&#24187;&#35273;&#8221;&#26356;&#20005;&#37325;&#30340;&#19968;&#20010;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21017;&#34987;&#35757;&#32451;&#20197;&#26368;&#22823;&#21270;&#27169;&#25311;&#20154;&#31867;&#22870;&#21169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#40784;&#31639;&#27861;&#31216;&#20026;Factually Augmented RLHF&#65292;&#23427;&#36890;&#36807;&#38468;&#21152;&#22270;&#20687;&#26631;&#39064;&#21644;&#22320;&#38754;&#30495;&#23454;&#22810;&#36873;&#39033;&#31561;&#39069;&#22806;&#20107;&#23454;&#20449;&#24687;&#26469;&#22686;&#24378;&#22870;&#21169;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#22870;&#21169;&#27450;&#39575;&#29616;&#35937;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20043;&#21069;&#21487;&#29992;&#30340;&#20154;&#24037;&#32534;&#20889;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#20102;GPT-4&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#65288;&#29992;&#20110;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in "hallucination", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;&#26631;&#20934;&#21270;&#32771;&#35797;&#20934;&#22791;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;GRE&#23450;&#37327;&#39046;&#22495;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;ChatGPT&#22312;&#22238;&#31572;&#19981;&#21516;&#31867;&#22411;&#30340;GRE&#38382;&#39064;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#24182;&#19988;&#20462;&#25913;&#38382;&#39064;&#25552;&#31034;&#21487;&#20197;&#24433;&#21709;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14519</link><description>&lt;p&gt;
ChatGPT&#22312;&#26631;&#20934;&#21270;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#8212;&#8212;&#23398;&#20064;&#32773;&#30340;&#19968;&#31181;&#25552;&#35758;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Performance on Standardized Testing Exam -- A Proposed Strategy for Learners. (arXiv:2309.14519v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;&#26631;&#20934;&#21270;&#32771;&#35797;&#20934;&#22791;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;GRE&#23450;&#37327;&#39046;&#22495;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;ChatGPT&#22312;&#22238;&#31572;&#19981;&#21516;&#31867;&#22411;&#30340;GRE&#38382;&#39064;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#24182;&#19988;&#20462;&#25913;&#38382;&#39064;&#25552;&#31034;&#21487;&#20197;&#24433;&#21709;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#20197;&#21450;&#20854;&#22312;&#26631;&#20934;&#21270;&#32771;&#35797;&#20934;&#22791;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;GRE&#23450;&#37327;&#32771;&#35797;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#26174;&#31034;&#20102;&#22312;&#21508;&#20010;&#23398;&#31185;&#30340;&#23398;&#20064;&#26041;&#27861;&#38761;&#21629;&#20013;&#65292;&#21033;&#29992;ChatGPT&#36827;&#34892;&#23398;&#26415;&#30446;&#30340;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#22312;GRE&#23450;&#37327;&#39046;&#22495;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#35843;&#25972;&#38382;&#39064;&#25552;&#31034;&#23545;&#20854;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#22320;&#65292;&#26412;&#30740;&#31350;&#22238;&#31572;&#20102;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;&#65306;1. ChatGPT&#22914;&#20309;&#22238;&#31572;GRE&#22522;&#30784;&#23450;&#37327;&#39064;&#30446;&#22312;&#19981;&#21516;&#20869;&#23481;&#39046;&#22495;&#65311;2. &#20462;&#25913;&#38382;&#39064;&#25552;&#31034;&#22914;&#20309;&#24433;&#21709;ChatGPT&#30340;&#20934;&#30830;&#24615;&#65311;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;100&#20010;&#20174;ETS&#23448;&#26041;GRE&#32771;&#35797;&#20934;&#22791;&#25351;&#21335;&#20013;&#38543;&#26426;&#36873;&#25321;&#30340;GRE&#23450;&#37327;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#37327;&#35780;&#20272;&#26469;&#22238;&#31572;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;t&#26816;&#39564;&#26469;&#26816;&#26597;&#32479;&#35745;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the problem solving capabilities of ChatGPT and its prospective applications in standardized test preparation, focusing on the GRE quantitative exam. Prior research has shown great potential for the utilization of ChatGPT for academic purposes in revolutionizing the approach to studying across various disciplines. We investigate how ChatGPT performs across various question types in the GRE quantitative domain, and how modifying question prompts impacts its accuracy. More specifically this study addressed two research questions: 1. How does ChatGPT perform in answering GRE-based quantitative questions across various content areas? 2. How does the accuracy of ChatGPT vary with modifying the question prompts? The dataset consisting of 100 randomly selected GRE quantitative questions was collected from the ETS official guide to GRE test preparation. We used quantitative evaluation to answer our first research question, and t-test to examine the statistical association b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#23481;&#23457;&#26597;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#21644;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23545;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#30340;&#25913;&#36827;&#25928;&#26524;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.14517</link><description>&lt;p&gt;
&#27880;&#24847;&#35328;&#36766;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20869;&#23481;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Watch Your Language: Large Language Models and Content Moderation. (arXiv:2309.14517v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#23481;&#23457;&#26597;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#21644;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23545;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#30340;&#25913;&#36827;&#25928;&#26524;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#22522;&#20110;&#25991;&#26412;&#30340;&#20869;&#23481;&#23457;&#26597;&#26159;&#20854;&#20013;&#19968;&#20010;&#21463;&#21040;&#36817;&#26399;&#28909;&#24773;&#20851;&#27880;&#30340;LLM&#24212;&#29992;&#26696;&#20363;&#65292;&#28982;&#32780;&#65292;&#40092;&#26377;&#30740;&#31350;&#35843;&#26597;LLMs&#22312;&#20869;&#23481;&#23457;&#26597;&#35774;&#32622;&#20013;&#30340;&#34920;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#22871;&#29616;&#20195;&#12289;&#21830;&#19994;&#21270;&#30340;LLMs&#65288;GPT-3&#12289;GPT-3.5&#12289;GPT-4&#65289;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20869;&#23481;&#23457;&#26597;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65306;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#21644;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#12290;&#23545;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;95&#20010;LLM&#23457;&#26597;&#24341;&#25806;&#65292;&#24182;&#20351;&#29992;95&#20010;Reddit&#23376;&#31038;&#21306;&#30340;&#35268;&#21017;&#36827;&#34892;&#25351;&#23548;&#65292;&#21457;&#29616;LLMs&#22312;&#35768;&#22810;&#31038;&#21306;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#23457;&#26597;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#20013;&#20301;&#25968;&#20934;&#30830;&#29575;&#20026;64%&#21644;&#20013;&#20301;&#25968;&#31934;&#30830;&#24230;&#20026;83%&#12290;&#22312;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#21830;&#19994;&#21487;&#29992;&#30340;&#26377;&#23475;&#24615;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#26368;&#36817;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23545;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#20960;&#20046;&#27809;&#26377;&#24102;&#26469;&#26126;&#26174;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exploded in popularity due to their ability to perform a wide array of natural language tasks. Text-based content moderation is one LLM use case that has received recent enthusiasm, however, there is little research investigating how LLMs perform in content moderation settings. In this work, we evaluate a suite of modern, commercial LLMs (GPT-3, GPT-3.5, GPT-4) on two common content moderation tasks: rule-based community moderation and toxic content detection. For rule-based community moderation, we construct 95 LLM moderation-engines prompted with rules from 95 Reddit subcommunities and find that LLMs can be effective at rule-based moderation for many communities, achieving a median accuracy of 64% and a median precision of 83%. For toxicity detection, we find that LLMs significantly outperform existing commercially available toxicity classifiers. However, we also find that recent increases in model size add only marginal benefit to toxicity detection
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DeepSpeed-Ulysses&#65292;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#20855;&#22791;&#26497;&#38271;&#24207;&#21015;&#38271;&#24230;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;LLM&#35757;&#32451;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14509</link><description>&lt;p&gt;
DeepSpeed Ulysses&#65306;&#29992;&#20110;&#35757;&#32451;&#26497;&#38271;&#24207;&#21015;Transformer&#27169;&#22411;&#30340;&#31995;&#32479;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models. (arXiv:2309.14509v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DeepSpeed-Ulysses&#65292;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#20855;&#22791;&#26497;&#38271;&#24207;&#21015;&#38271;&#24230;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;LLM&#35757;&#32451;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35745;&#31639;&#21487;&#20197;&#36890;&#36807;&#25209;&#37327;&#22823;&#23567;&#12289;&#38544;&#34255;&#32500;&#24230;&#12289;&#23618;&#25968;&#21644;&#24207;&#21015;&#38271;&#24230;&#26469;&#25551;&#36848;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#21152;&#36895;LLM&#35757;&#32451;&#30340;&#31995;&#32479;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21069;&#19977;&#20010;&#32500;&#24230;&#19978;&#65306;&#25209;&#37327;&#22823;&#23567;&#30340;&#25968;&#25454;&#24182;&#34892;&#21270;&#12289;&#38544;&#34255;&#23610;&#23544;&#30340;&#24352;&#37327;&#24182;&#34892;&#21270;&#20197;&#21450;&#27169;&#22411;&#28145;&#24230;&#25110;&#23618;&#25968;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#21270;&#12290;&#36825;&#20123;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#24182;&#34892;&#24418;&#24335;&#24182;&#19981;&#38024;&#23545;&#38271;&#24207;&#21015;Transformer&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#37492;&#20110;&#38271;&#24207;&#21015;LLM&#22312;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#19978;&#30340;&#37325;&#35201;&#24615;&#65292;&#24207;&#21015;&#24182;&#34892;&#21270;&#24341;&#36215;&#20102;&#37325;&#26032;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24207;&#21015;&#24182;&#34892;&#21270;&#24037;&#20316;&#21463;&#21040;&#20869;&#23384;&#36890;&#20449;&#25928;&#29575;&#30340;&#38480;&#21046;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38271;&#24207;&#21015;&#22823;&#27169;&#22411;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DeepSpeed-Ulysses&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#20415;&#25658;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20855;&#22791;&#26497;&#38271;&#24207;&#21015;&#38271;&#24230;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;LLM&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input da
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35282;&#24230;&#26126;&#21487;&#22827;&#26031;&#22522;$p$-&#36317;&#31163;&#21487;&#20197;&#33719;&#24471;&#27604;&#20256;&#32479;&#20313;&#24358;&#30456;&#20284;&#24230;&#26356;&#39640;&#30340;&#20998;&#31867;&#24615;&#33021;</title><link>http://arxiv.org/abs/2309.14495</link><description>&lt;p&gt;
&#20351;&#29992;&#35282;&#24230;&#26126;&#21487;&#22827;&#26031;&#22522;$p$-&#36317;&#31163;&#20998;&#31867;&#26631;&#35760;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
Classifying token frequencies using angular Minkowski $p$-distance. (arXiv:2309.14495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14495
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35282;&#24230;&#26126;&#21487;&#22827;&#26031;&#22522;$p$-&#36317;&#31163;&#21487;&#20197;&#33719;&#24471;&#27604;&#20256;&#32479;&#20313;&#24358;&#30456;&#20284;&#24230;&#26356;&#39640;&#30340;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35282;&#24230;&#26126;&#21487;&#22827;&#26031;&#22522;$p$-&#36317;&#31163;&#26159;&#19968;&#31181;&#20351;&#29992;&#26126;&#21487;&#22827;&#26031;&#22522;$p$-&#36317;&#31163;&#26367;&#20195;&#20313;&#24358;&#30456;&#20284;&#24230;&#23450;&#20041;&#30340;&#19981;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#12290;&#20313;&#24358;&#30456;&#20284;&#24230;&#32463;&#24120;&#29992;&#20110;&#21253;&#21547;&#26631;&#35760;&#39057;&#29575;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#32780;&#35282;&#24230;&#26126;&#21487;&#22827;&#26031;&#22522;$p$-&#36317;&#31163;&#21487;&#33021;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#26159;&#26356;&#22909;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;20-newsgroups&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#35780;&#20272;&#20102;&#20256;&#32479;&#21152;&#26435;&#26368;&#36817;&#37051;&#21644;&#27169;&#31946;&#31895;&#31961;&#26368;&#36817;&#37051;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#36229;&#21442;&#25968;$p$&#65292;&#25968;&#25454;&#38598;&#32500;&#25968;$m$&#65292;&#37051;&#23621;&#25968;&#37327;$k$&#65292;&#26435;&#37325;&#36873;&#25321;&#21644;&#20998;&#31867;&#22120;&#36873;&#25321;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#20351;&#29992;&#21512;&#36866;&#30340;$p$&#20540;&#65292;&#20351;&#29992;&#35282;&#24230;&#26126;&#21487;&#22827;&#26031;&#22522;$p$-&#36317;&#31163;&#21487;&#20197;&#33719;&#24471;&#27604;&#20256;&#32479;&#20313;&#24358;&#30456;&#20284;&#24230;&#26356;&#39640;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Angular Minkowski $p$-distance is a dissimilarity measure that is obtained by replacing Euclidean distance in the definition of cosine dissimilarity with other Minkowski $p$-distances. Cosine dissimilarity is frequently used with datasets containing token frequencies, and angular Minkowski $p$-distance may potentially be an even better choice for certain tasks. In a case study based on the 20-newsgroups dataset, we evaluate clasification performance for classical weighted nearest neighbours, as well as fuzzy rough nearest neighbours. In addition, we analyse the relationship between the hyperparameter $p$, the dimensionality $m$ of the dataset, the number of neighbours $k$, the choice of weights and the choice of classifier. We conclude that it is possible to obtain substantially higher classification performance with angular Minkowski $p$-distance with suitable values for $p$ than with classical cosine dissimilarity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20154;&#31867;&#21644;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35780;&#20272;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26102;&#65292;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.14488</link><description>&lt;p&gt;
&#24403;&#33258;&#21160;&#21270;&#35780;&#20272;&#36935;&#19978;&#33258;&#21160;&#21270;&#20869;&#23481;&#29983;&#25104;&#65306;&#22312;GPT&#26102;&#20195;&#23457;&#26597;&#25991;&#26412;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs. (arXiv:2309.14488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20154;&#31867;&#21644;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35780;&#20272;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26102;&#65292;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35780;&#20272;&#21644;&#25171;&#20998;&#25991;&#26412;&#25968;&#25454;&#26041;&#38754;&#30340;&#24212;&#29992;&#24050;&#32463;&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#25628;&#32034;&#21644;&#25512;&#33616;&#20197;&#21450;&#22312;&#32447;&#20869;&#23481;&#21487;&#20449;&#24230;&#35780;&#20272;&#31561;&#21508;&#31181;&#24773;&#22659;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#25991;&#26412;&#20132;&#21449;&#39046;&#22495;&#30340;&#19968;&#27425;&#37325;&#35201;&#21464;&#38761;&#26159;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#31561;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#24615;&#35780;&#20272;&#20154;&#31867;&#21644;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#23545;&#20110;&#22522;&#20110;&#20154;&#31867;&#20869;&#23481;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#25171;&#20998;&#27169;&#22411;&#22914;&#20309;&#35780;&#20272;&#20869;&#23481;&#36136;&#37327;&#30340;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#35770;&#25991;&#35780;&#20998;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12289;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#30340;&#35770;&#25991;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20197;&#31616;&#27905;&#32771;&#34385;&#21040;&#34987;&#35843;&#26597;&#32773;&#31867;&#22411;&#12289;&#25552;&#31034;&#31867;&#22411;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#20016;&#23500;&#30340;&#27979;&#35797;&#26679;&#26412;&#65292;&#28085;&#30422;&#20102;18,460&#31687;&#20154;&#24037;&#29983;&#25104;&#21644;GPT&#29983;&#25104;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#32447;&#24615;&#20998;&#31867;&#22120;&#22312;&#35780;&#20272;GPT&#29983;&#25104;&#30340;&#35770;&#25991;&#26102;&#24615;&#33021;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of machine learning (ML) models to assess and score textual data has become increasingly pervasive in an array of contexts including natural language processing, information retrieval, search and recommendation, and credibility assessment of online content. A significant disruption at the intersection of ML and text are text-generating large-language models such as generative pre-trained transformers (GPTs). We empirically assess the differences in how ML-based scoring models trained on human content assess the quality of content generated by humans versus GPTs. To do so, we propose an analysis framework that encompasses essay scoring ML-models, human and ML-generated essays, and a statistical model that parsimoniously considers the impact of type of respondent, prompt genre, and the ML model used for assessment model. A rich testbed is utilized that encompasses 18,460 human-generated and GPT-based essays. Results of our benchmark analysis reveal that transformer pretrained lan
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25913;&#36827;&#20102;&#32852;&#21512;NLU&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#20854;&#21487;&#35299;&#37322;&#65292;&#21516;&#26102;&#25193;&#23637;&#36866;&#29992;&#20110;&#20854;&#20182;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.14485</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#21644;&#20934;&#30830;&#30340;&#35821;&#38899;&#21161;&#25163;&#21450;&#20854;&#23427;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Explainable and Accurate Natural Language Understanding for Voice Assistants and Beyond. (arXiv:2309.14485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14485
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25913;&#36827;&#20102;&#32852;&#21512;NLU&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#20854;&#21487;&#35299;&#37322;&#65292;&#21516;&#26102;&#25193;&#23637;&#36866;&#29992;&#20110;&#20854;&#20182;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#24847;&#22270;&#26816;&#27979;&#21644;&#27133;&#22635;&#20805;&#65292;&#20063;&#31216;&#20026;&#32852;&#21512;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#65292;&#23545;&#20110;&#26234;&#33021;&#35821;&#38899;&#21161;&#25163;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#21508;&#31181;&#25216;&#26415;&#25552;&#39640;&#20934;&#30830;&#24615;&#19978;&#12290;&#21487;&#35299;&#37322;&#24615;&#26080;&#30097;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#32852;&#21512;NLU&#27169;&#22411;&#12290;&#22914;&#26524;&#27809;&#26377;&#35299;&#37322;&#24615;&#65292;&#23427;&#20204;&#30340;&#20915;&#31574;&#23545;&#22806;&#30028;&#26469;&#35828;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#22240;&#27492;&#23481;&#26131;&#32570;&#20047;&#29992;&#25143;&#30340;&#20449;&#20219;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;&#23436;&#25972;&#30340;&#32852;&#21512;NLU&#27169;&#22411;&#36716;&#21270;&#20026;&#22312;&#32454;&#31890;&#24230;&#19978;&#8220;&#20869;&#22312;&#22320;&#8221;&#21487;&#35299;&#37322;&#30340;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#23436;&#25972;&#30340;&#32852;&#21512;NLU&#27169;&#22411;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25193;&#23637;&#21487;&#20197;&#25104;&#21151;&#24212;&#29992;&#20110;&#20854;&#20182;&#19968;&#33324;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint intent detection and slot filling, which is also termed as joint NLU (Natural Language Understanding) is invaluable for smart voice assistants. Recent advancements in this area have been heavily focusing on improving accuracy using various techniques. Explainability is undoubtedly an important aspect for deep learning-based models including joint NLU models. Without explainability, their decisions are opaque to the outside world and hence, have tendency to lack user trust. Therefore to bridge this gap, we transform the full joint NLU model to be `inherently' explainable at granular levels without compromising on accuracy. Further, as we enable the full joint NLU model explainable, we show that our extension can be successfully used in other general classification tasks. We demonstrate this using sentiment analysis and named entity recognition.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#22312;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#20013;&#38754;&#20020;&#30340;&#31867;&#21035;&#20998;&#24067;&#27874;&#21160;&#21644;&#25968;&#25454;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#35757;&#32451;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#24037;&#20316;&#37327;&#20943;&#23569;5&#20493;&#12290;</title><link>http://arxiv.org/abs/2309.14460</link><description>&lt;p&gt;
&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Active Learning For Sound Event Detection. (arXiv:2309.14460v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14460
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#22312;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#20013;&#38754;&#20020;&#30340;&#31867;&#21035;&#20998;&#24067;&#27874;&#21160;&#21644;&#25968;&#25454;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#35757;&#32451;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#24037;&#20316;&#37327;&#20943;&#23569;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#26159;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#32321;&#29712;&#12289;&#32791;&#26102;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65288;OAL&#65289;&#26159;&#19968;&#31181;&#21516;&#26102;&#20943;&#23569;&#35757;&#32451;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#26631;&#27880;&#37327;&#24182;&#36866;&#24212;&#25968;&#25454;&#21464;&#21270;&#30340;&#33539;&#20363;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27874;&#21160;&#30340;&#31867;&#21035;&#20998;&#24067;&#21644;&#25968;&#25454;&#28418;&#31227;&#20173;&#28982;&#26159;OAL&#30340;&#24120;&#35265;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;OAL&#24212;&#29992;&#20110;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;SONYC&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#35821;&#38899;&#31867;&#22411;&#35782;&#21035;&#65288;VTD&#65289;&#35821;&#26009;&#24211;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OAL&#21487;&#20197;&#23558;&#35757;&#32451;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#24037;&#20316;&#37327;&#20943;&#23569;5&#20493;&#65292;&#24182;&#19988;&#26412;&#25991;&#20171;&#32461;&#30340;&#26032;&#26041;&#27861;&#25104;&#21151;&#35299;&#20915;&#20102;&#29616;&#26377;OAL&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data collection and annotation is a laborious, time-consuming prerequisite for supervised machine learning tasks. Online Active Learning (OAL) is a paradigm that addresses this issue by simultaneously minimizing the amount of annotation required to train a classifier and adapting to changes in the data over the duration of the data collection process. Prior work has indicated that fluctuating class distributions and data drift are still common problems for OAL. This work presents new loss functions that address these challenges when OAL is applied to Sound Event Detection (SED). Experimental results from the SONYC dataset and two Voice-Type Discrimination (VTD) corpora indicate that OAL can reduce the time and effort required to train SED classifiers by a factor of 5 for SONYC, and that the new methods presented here successfully resolve issues present in existing OAL methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#12289;&#27604;&#36739;&#21644;&#36870;&#21521;&#25628;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2309.14402</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;3.2&#37096;&#20998;&#65292;&#30693;&#35782;&#25805;&#25511;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 3.2, Knowledge Manipulation. (arXiv:2309.14402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#12289;&#27604;&#36739;&#21644;&#36870;&#21521;&#25628;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23384;&#20648;&#22823;&#37327;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#22312;&#20351;&#29992;&#36825;&#20123;&#30693;&#35782;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#20854;&#23384;&#20648;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#22235;&#31181;&#25805;&#25511;&#31867;&#22411;&#65306;&#26816;&#32034;&#65288;&#20363;&#22914;&#65292;&#8220;A&#30340;&#23646;&#24615;X&#26159;&#20160;&#20040;&#8221;&#65289;&#12289;&#20998;&#31867;&#65288;&#20363;&#22914;&#65292;&#8220;A&#30340;&#23646;&#24615;X&#26159;&#22855;&#25968;&#36824;&#26159;&#20598;&#25968;&#8221;&#65289;&#12289;&#27604;&#36739;&#65288;&#20363;&#22914;&#65292;&#8220;&#22312;&#23646;&#24615;X&#20013;A&#26159;&#21542;&#22823;&#20110;B&#8221;&#65289;&#21644;&#36870;&#21521;&#25628;&#32034;&#65288;&#20363;&#22914;&#65292;&#8220;&#21738;&#20010;&#20154;&#30340;&#23646;&#24615;X&#31561;&#20110;T&#8221;&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20687;GPT2/3/4&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#25110;&#27604;&#36739;&#20219;&#21153;&#20013;&#24456;&#38590;&#32988;&#20219;&#65292;&#38500;&#38750;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;Chain of Thoughts&#65288;CoTs&#65289;&#12290;&#26080;&#35770;&#25552;&#31034;&#26159;&#20160;&#20040;&#65292;&#23427;&#20204;&#22312;&#36870;&#21521;&#30693;&#35782;&#25628;&#32034;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#20010;&#20026;&#25511;&#21046;&#23454;&#39564;&#32780;&#35774;&#35745;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35777;&#23454;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models can store vast amounts of factual knowledge, but their ability to use this knowledge for logical reasoning remains questionable. This paper explores a language model's ability to manipulate its stored knowledge during inference. We focus on four manipulation types: retrieval (e.g., "What is person A's attribute X"), classification (e.g., "Is A's attribute X even or odd?"), comparison (e.g., "Is A greater than B in attribute X?") and inverse search (e.g., "Which person's attribute X equals T?")  We observe that pre-trained language models like GPT2/3/4 excel in knowledge retrieval but struggle with simple classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. They also perform poorly in inverse knowledge search, irrespective of the prompts. Our primary contribution is a synthetic dataset for a controlled experiment that confirms these inherent weaknesses: a language model cannot efficiently manipulate knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#22312;&#21160;&#26426;&#24615;&#35775;&#35848;&#20013;&#20934;&#30830;&#21306;&#20998;&#20102;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#19977;&#31181;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#22120;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#23545;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#21644;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#25214;&#21040;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2309.14398</link><description>&lt;p&gt;
&#30475;&#35265;&#21644;&#21548;&#21040;&#27809;&#34987;&#35828;&#30340;&#35805;&#65306;&#21487;&#35299;&#37322;&#34701;&#21512;&#30340;&#22810;&#27169;&#24577;&#21160;&#26426;&#24615;&#35775;&#35848;&#23458;&#25143;&#34892;&#20026;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion. (arXiv:2309.14398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#22312;&#21160;&#26426;&#24615;&#35775;&#35848;&#20013;&#20934;&#30830;&#21306;&#20998;&#20102;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#19977;&#31181;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#22120;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#23545;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#21644;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#25214;&#21040;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#24615;&#35775;&#35848;&#65288;MI&#65289;&#26159;&#19968;&#31181;&#24378;&#35843;&#21512;&#20316;&#24182;&#40723;&#21169;&#34892;&#20026;&#25913;&#21464;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;MI&#23545;&#35805;&#30340;&#36136;&#37327;&#65292;&#21487;&#20197;&#21033;&#29992;MISC&#20195;&#30721;&#23558;&#23458;&#25143;&#35805;&#35821;&#20998;&#31867;&#20026;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#25110;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#12290;MI&#23545;&#35805;&#20013;&#21464;&#21270;&#35805;&#35821;&#30340;&#27604;&#20363;&#19982;&#27835;&#30103;&#32467;&#26524;&#21576;&#27491;&#30456;&#20851;&#65292;&#22240;&#27492;&#20934;&#30830;&#20998;&#31867;&#23458;&#25143;&#35805;&#35821;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#20934;&#30830;&#21306;&#20998;&#19977;&#20010;MISC&#31867;&#21035;&#65288;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#65289;&#12290;&#20026;&#20102;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#25910;&#38598;&#20102;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivational Interviewing (MI) is an approach to therapy that emphasizes collaboration and encourages behavioral change. To evaluate the quality of an MI conversation, client utterances can be classified using the MISC code as either change talk, sustain talk, or follow/neutral talk. The proportion of change talk in a MI conversation is positively correlated with therapy outcomes, making accurate classification of client utterances essential. In this paper, we present a classifier that accurately distinguishes between the three MISC classes (change talk, sustain talk, and follow/neutral talk) leveraging multimodal features such as text, prosody, facial expressivity, and body expressivity. To train our model, we perform annotations on the publicly available AnnoMI dataset to collect multimodal information, including text, audio, facial expressivity, and body expressivity. Furthermore, we identify the most important modalities in the decision-making process, providing valuable insights i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#65288;MDD&#65289;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#32423;&#21035;&#26469;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20219;&#24847;&#22495;&#20043;&#38388;&#30340;&#32763;&#35793;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14394</link><description>&lt;p&gt;
&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation. (arXiv:2309.14394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#65288;MDD&#65289;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#32423;&#21035;&#26469;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20219;&#24847;&#22495;&#20043;&#38388;&#30340;&#32763;&#35793;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#38388;&#32763;&#35793;&#28041;&#21450;&#22312;&#32473;&#23450;&#28304;&#22495;&#26465;&#20214;&#19979;&#29983;&#25104;&#30446;&#26631;&#22495;&#26679;&#26412;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#22266;&#23450;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#22495;&#19978;&#65292;&#21363;&#23427;&#20204;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#37197;&#32622;&#65288;&#20363;&#22914;&#23545;&#20110;&#20004;&#20010;&#22495;&#65292;&#35201;&#20040;$D_1\rightarrow{}D_2$&#65292;&#35201;&#20040;$D_2\rightarrow{}D_1$&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Multi-Domain Diffusion&#65288;MDD&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#30340;&#26465;&#20214;&#25193;&#25955;&#26694;&#26550;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MDD&#19981;&#38656;&#35201;&#23450;&#20041;&#36755;&#20837;&#21644;&#36755;&#20986;&#22495;&#65292;&#20801;&#35768;&#22312;&#19968;&#32452;&#22495;&#30340;&#20219;&#20309;&#20998;&#21306;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65288;&#20363;&#22914;$(D_1, D_2)\rightarrow{}D_3$&#65292;$D_2\rightarrow{}(D_1, D_3)$&#65292;$D_3\rightarrow{}D_1$&#31561;&#65289;&#65292;&#32780;&#26080;&#38656;&#20026;&#27599;&#20010;&#22495;&#37197;&#32622;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;MDD&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#24418;&#24335;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#22495;&#24341;&#20837;&#19968;&#20010;&#22122;&#22768;&#32423;&#21035;&#65292;&#20197;&#33258;&#28982;&#30340;&#26041;&#24335;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#23558;&#20256;&#32479;&#30340;&#32763;&#35793;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#36890;&#36807;&#22122;&#22768;&#24314;&#27169;&#26469;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-to-domain translation involves generating a target domain sample given a condition in the source domain. Most existing methods focus on fixed input and output domains, i.e. they only work for specific configurations (i.e. for two domains, either $D_1\rightarrow{}D_2$ or $D_2\rightarrow{}D_1$). This paper proposes Multi-Domain Diffusion (MDD), a conditional diffusion framework for multi-domain translation in a semi-supervised context. Unlike previous methods, MDD does not require defining input and output domains, allowing translation between any partition of domains within a set (such as $(D_1, D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$, etc. for 3 domains), without the need to train separate models for each domain configuration. The key idea behind MDD is to leverage the noise formulation of diffusion models by incorporating one noise level per domain, which allows missing domains to be modeled with noise in a natural way. This transforms the tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#38024;&#23545;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14393</link><description>&lt;p&gt;
LLMCarbon: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models. (arXiv:2309.14393v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#38024;&#23545;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30899;&#36275;&#36857;&#26159;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#25512;&#29702;&#12289;&#23454;&#39564;&#21644;&#23384;&#20648;&#36807;&#31243;&#20013;&#30340;&#25490;&#25918;&#65292;&#21253;&#25324;&#36816;&#33829;&#21644;&#22266;&#23450;&#30899;&#25490;&#25918;&#12290;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#22312;LLMs&#35757;&#32451;&#20043;&#21069;&#20934;&#30830;&#20272;&#35745;&#20854;&#30899;&#24433;&#21709;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;GPU&#30340;&#20351;&#29992;&#12290;&#29616;&#26377;&#30740;&#31350;&#24050;&#25253;&#21578;&#20102;LLMs&#35757;&#32451;&#30340;&#30899;&#36275;&#36857;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#24037;&#20855;mlco2&#33021;&#22815;&#22312;&#23454;&#38469;&#35757;&#32451;&#20043;&#21069;&#39044;&#27979;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#30899;&#36275;&#36857;&#12290;&#28982;&#32780;&#65292;mlco2&#23384;&#22312;&#19968;&#20123;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;&#23427;&#19981;&#33021;&#25193;&#23637;&#20854;&#23545;&#23494;&#38598;&#25110;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;LLMs&#30340;&#20272;&#35745;&#65292;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#26550;&#26500;&#21442;&#25968;&#65292;&#20165;&#20851;&#27880;GPU&#65292;&#24182;&#19981;&#33021;&#24314;&#27169;&#22266;&#21270;&#30340;&#30899;&#36275;&#36857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#20026;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#12290;&#19982;mlco2&#30456;&#27604;&#65292;LLMCarbon&#26174;&#33879;&#22686;&#24378;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \textit{LLMCarbon}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly enhances the ac
&lt;/p&gt;</description></item><item><title>&#19968;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#34987;&#20171;&#32461;&#26469;&#35299;&#37322;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#38754;&#21521;&#26381;&#21153;&#31995;&#32479;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#36890;&#36807;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#24314;&#31435;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2309.14391</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#35299;&#37322;&#38754;&#21521;&#26381;&#21153;&#31995;&#32479;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#30340;AI&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
An AI Chatbot for Explaining Deep Reinforcement Learning Decisions of Service-oriented Systems. (arXiv:2309.14391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14391
&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#34987;&#20171;&#32461;&#26469;&#35299;&#37322;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#38754;&#21521;&#26381;&#21153;&#31995;&#32479;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#36890;&#36807;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#24314;&#31435;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (Deep RL) &#22312;&#38754;&#21521;&#26381;&#21153;&#31995;&#32479;&#20013;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#20197;&#24212;&#23545;&#24320;&#25918;&#19990;&#30028;&#30340;&#20551;&#35774;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21160;&#24577;&#26381;&#21153;&#32452;&#21512;&#12289;&#20316;&#19994;&#35843;&#24230;&#12289;&#21368;&#36733;&#20197;&#21450;&#26381;&#21153;&#36866;&#24212;&#31561;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20854;&#23398;&#21040;&#30340;&#20915;&#31574;&#31574;&#30053;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#40657;&#30418;&#23376;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#36807;&#31243;&#23545;&#20110;&#24110;&#21161;&#26381;&#21153;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#35843;&#35797;&#12289;&#25903;&#25345;&#26381;&#21153;&#25552;&#20379;&#21830;&#36981;&#23432;&#30456;&#20851;&#27861;&#24459;&#26694;&#26550;&#20197;&#21450;&#24110;&#21161;&#26381;&#21153;&#20351;&#29992;&#32773;&#24314;&#31435;&#20449;&#20219;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Chat4XAI&#65292;&#36890;&#36807;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#20419;&#36827;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#12290;&#19982;&#35270;&#35273;&#35299;&#37322;&#30456;&#27604;&#65292;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#25253;&#21578;&#20248;&#28857;&#21253;&#25324;&#38750;&#25216;&#26415;&#29992;&#25143;&#26356;&#22909;&#30340;&#21487;&#29702;&#35299;&#24615;&#12289;&#29992;&#25143;&#30340;&#25509;&#21463;&#24230;&#21644;&#20449;&#20219;&#24230;&#25552;&#39640;&#65292;&#20197;&#21450;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (Deep RL) is increasingly used to cope with the open-world assumption in service-oriented systems. Deep RL was successfully applied to problems such as dynamic service composition, job scheduling, and offloading, as well as service adaptation. While Deep RL offers many benefits, understanding the decision-making of Deep RL is challenging because its learned decision-making policy essentially appears as a black box. Yet, understanding the decision-making of Deep RL is key to help service developers perform debugging, support service providers to comply with relevant legal frameworks, and facilitate service users to build trust. We introduce Chat4XAI to facilitate the understanding of the decision-making of Deep RL by providing natural-language explanations. Compared with visual explanations, the reported benefits of natural-language explanations include better understandability for non-technical users, increased user acceptance and trust, as well as more effi
&lt;/p&gt;</description></item><item><title>"&#21516;&#24847;&#19981;&#21516;&#24847;"&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#33258;&#21160;&#35299;&#26512;&#21644;&#24635;&#32467;&#20351;&#29992;&#25143;&#20415;&#20110;&#29702;&#35299;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#22312;&#25215;&#35834;&#21327;&#35758;&#20043;&#21069;&#32771;&#34385;&#37325;&#35201;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2309.14382</link><description>&lt;p&gt;
&#21516;&#24847;&#19981;&#21516;&#24847;
&lt;/p&gt;
&lt;p&gt;
Agree To Disagree. (arXiv:2309.14382v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14382
&lt;/p&gt;
&lt;p&gt;
"&#21516;&#24847;&#19981;&#21516;&#24847;"&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#33258;&#21160;&#35299;&#26512;&#21644;&#24635;&#32467;&#20351;&#29992;&#25143;&#20415;&#20110;&#29702;&#35299;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#22312;&#25215;&#35834;&#21327;&#35758;&#20043;&#21069;&#32771;&#34385;&#37325;&#35201;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27880;&#20876;&#26381;&#21153;&#12289;&#23433;&#35013;&#36719;&#20214;&#25110;&#35775;&#38382;&#32593;&#31449;&#20043;&#21069;&#65292;&#20010;&#20154;&#26377;&#22810;&#39057;&#32321;&#22320;&#20180;&#32454;&#23457;&#26597;&#26465;&#27454;&#21644;&#26465;&#20214;&#65311;&#22823;&#22810;&#25968;&#20114;&#32852;&#32593;&#29992;&#25143;&#24182;&#19981;&#21442;&#19982;&#36825;&#31181;&#20570;&#27861;&#12290;&#37492;&#20110;&#26465;&#27454;&#21644;&#26465;&#20214;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#22797;&#26434;&#30340;&#27861;&#24459;&#26415;&#35821;&#21644;&#26214;&#28073;&#38590;&#25026;&#30340;&#21477;&#23376;&#65292;&#36825;&#31181;&#36235;&#21183;&#24182;&#19981;&#20196;&#20154;&#24847;&#22806;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20197;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#33258;&#21160;&#35299;&#26512;&#21644;&#24635;&#32467;&#20851;&#38190;&#20449;&#24687;&#12290;&#36825;&#39033;&#25216;&#26415;&#19987;&#27880;&#20110;&#25552;&#21462;&#29992;&#25143;&#22312;&#25215;&#35834;&#21327;&#35758;&#20043;&#21069;&#24212;&#32771;&#34385;&#30340;&#30456;&#20851;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
How frequently do individuals thoroughly review terms and conditions before proceeding to register for a service, install software, or access a website? The majority of internet users do not engage in this practice. This trend is not surprising, given that terms and conditions typically consist of lengthy documents replete with intricate legal terminology and convoluted sentences. In this paper, we introduce a Machine Learning-powered approach designed to automatically parse and summarize critical information in a user-friendly manner. This technology focuses on distilling the pertinent details that users should contemplate before committing to an agreement.
&lt;/p&gt;</description></item><item><title>&#31038;&#20132;&#20559;&#35265;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35843;&#26597;&#65292;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#23545;&#28508;&#22312;&#31038;&#20250;&#20559;&#35265;&#30340;&#29702;&#35299;&#65292;&#20197;&#35299;&#20915;&#36164;&#28304;&#20998;&#37197;&#19981;&#22343;&#21644;&#19981;&#20844;&#24179;&#20195;&#34920;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14381</link><description>&lt;p&gt;
&#31038;&#20132;&#20559;&#35265;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey of Social Bias in Vision-Language Models. (arXiv:2309.14381v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14381
&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#20559;&#35265;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35843;&#26597;&#65292;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#23545;&#28508;&#22312;&#31038;&#20250;&#20559;&#35265;&#30340;&#29702;&#35299;&#65292;&#20197;&#35299;&#20915;&#36164;&#28304;&#20998;&#37197;&#19981;&#22343;&#21644;&#19981;&#20844;&#24179;&#20195;&#34920;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#24555;&#36895;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#25429;&#25417;&#21644;&#24378;&#21270;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#23548;&#33268;&#36164;&#28304;&#20998;&#37197;&#19981;&#22343;&#21644;&#23545;&#29305;&#23450;&#31038;&#20250;&#32676;&#20307;&#30340;&#19981;&#20844;&#24179;&#20195;&#34920;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#35299;&#20915;&#36825;&#20123;&#20559;&#35265;&#24182;&#30830;&#20445;&#20844;&#24179;&#24615;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#20851;&#38190;&#20851;&#20999;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#39046;&#22495;&#20013;&#38656;&#35201;&#20851;&#27880;&#36825;&#20123;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#28508;&#22312;&#31038;&#20250;&#20559;&#35265;&#12290;&#34429;&#28982;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#31038;&#20250;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20294;&#23545;&#20110;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20559;&#35265;&#30456;&#27604;&#65292;&#20154;&#20204;&#23545;&#20854;&#20102;&#35299;&#26377;&#38480;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#32508;&#36848;&#21644;&#36164;&#28304;&#65292;&#20197;&#22686;&#36827;&#23545;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#20559;&#35265;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the rapid advancement of machine learning (ML) models, particularly transformer-based pre-trained models, has revolutionized Natural Language Processing (NLP) and Computer Vision (CV) fields. However, researchers have discovered that these models can inadvertently capture and reinforce social biases present in their training datasets, leading to potential social harms, such as uneven resource allocation and unfair representation of specific social groups. Addressing these biases and ensuring fairness in artificial intelligence (AI) systems has become a critical concern in the ML community.  The recent introduction of pre-trained vision-and-language (VL) models in the emerging multimodal field demands attention to the potential social biases present in these models as well. Although VL models are susceptible to social bias, there is a limited understanding compared to the extensive discussions on bias in NLP and CV. This survey aims to provide researchers with a high-le
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#36741;&#21161;&#30340;&#28151;&#21512;&#26041;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#25991;&#31038;&#31185;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;16&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#28085;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#35328;&#20998;&#26512;&#12289;&#25991;&#26412;&#25366;&#25496;&#12289;&#31038;&#20132;&#32593;&#32476;&#25512;&#26029;&#31561;&#12290;</title><link>http://arxiv.org/abs/2309.14379</link><description>&lt;p&gt;
&#26426;&#22120;&#36741;&#21161;&#30340;&#28151;&#21512;&#26041;&#27861;&#65306;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#20154;&#25991;&#31038;&#31185;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence. (arXiv:2309.14379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#36741;&#21161;&#30340;&#28151;&#21512;&#26041;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#25991;&#31038;&#31185;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;16&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#28085;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#35328;&#20998;&#26512;&#12289;&#25991;&#26412;&#25366;&#25496;&#12289;&#31038;&#20132;&#32593;&#32476;&#25512;&#26029;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19981;&#26029;&#36827;&#21270;&#20026;&#20154;&#25991;&#31038;&#31185;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#33021;&#22815;&#22312;&#20197;&#21069;&#36890;&#24120;&#30001;&#20154;&#21147;&#23436;&#25104;&#30340;&#23450;&#24615;&#20998;&#26512;&#20219;&#21153;&#20013;&#23454;&#29616;&#35268;&#27169;&#21270;&#12289;&#33258;&#21160;&#21270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#28151;&#21512;&#26041;&#27861;&#26694;&#26550;&#65292;&#20197;&#21033;&#29992;&#23450;&#24615;&#20998;&#26512;&#19987;&#19994;&#30693;&#35782;&#12289;&#26426;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20005;&#35880;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#27880;&#37325;&#36879;&#26126;&#24230;&#21644;&#21487;&#22797;&#21046;&#24615;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;16&#20010;&#26426;&#22120;&#36741;&#21161;&#30340;&#26696;&#20363;&#30740;&#31350;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#12290;&#20219;&#21153;&#21253;&#25324;&#35821;&#35328;&#21644;&#35805;&#35821;&#20998;&#26512;&#12289;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#12289;&#37319;&#35775;&#20998;&#26512;&#12289;&#21382;&#21490;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#21644;&#25991;&#26412;&#25366;&#25496;&#12289;&#25919;&#27835;&#31435;&#22330;&#26816;&#27979;&#12289;&#25991;&#26412;&#21644;&#24605;&#24819;&#37325;&#22797;&#20351;&#29992;&#12289;&#25991;&#23398;&#21644;&#30005;&#24433;&#20013;&#30340;&#25991;&#31867;&#26500;&#25104;&#12289;&#31038;&#20132;&#32593;&#32476;&#25512;&#26029;&#12289;&#33258;&#21160;&#35789;&#20856;&#32534;&#32386;&#12289;&#20803;&#25968;&#25454;&#34917;&#20805;&#21644;&#22810;&#27169;&#24577;&#35270;&#35273;&#25991;&#21270;&#20998;&#26512;&#12290;&#19982;&#29616;&#26377;LLM&#24212;&#29992;&#25991;&#29486;&#20013;&#23545;&#33521;&#25991;&#30340;&#20851;&#27880;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#28085;&#30422;&#22810;&#31181;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing capacities of large language models (LLMs) present an unprecedented opportunity to scale up data analytics in the humanities and social sciences, augmenting and automating qualitative analytic tasks previously typically allocated to human labor. This contribution proposes a systematic mixed methods framework to harness qualitative analytic expertise, machine scalability, and rigorous quantification, with attention to transparency and replicability. 16 machine-assisted case studies are showcased as proof of concept. Tasks include linguistic and discourse analysis, lexical semantic change detection, interview analysis, historical event cause inference and text mining, detection of political stance, text and idea reuse, genre composition in literature and film; social network inference, automated lexicography, missing metadata augmentation, and multimodal visual cultural analytics. In contrast to the focus on English in the emerging LLM applicability literature, many exampl
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#35780;&#20272;&#21644;&#22686;&#24378;&#24314;&#31569;&#27861;&#35268;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#12290;&#30740;&#31350;&#32771;&#34385;&#20102;&#26465;&#27454;&#21644;&#25991;&#26723;&#23618;&#38754;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#20960;&#20010;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14374</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#24314;&#31569;&#27861;&#35268;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Text Classification-Based Approach for Evaluating and Enhancing the Machine Interpretability of Building Codes. (arXiv:2309.14374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#35780;&#20272;&#21644;&#22686;&#24378;&#24314;&#31569;&#27861;&#35268;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#12290;&#30740;&#31350;&#32771;&#34385;&#20102;&#26465;&#27454;&#21644;&#25991;&#26723;&#23618;&#38754;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#20960;&#20010;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#30417;&#31649;&#25991;&#20214;&#25110;&#24314;&#31569;&#27861;&#35268;&#35299;&#37322;&#20026;&#21487;&#35745;&#31639;&#26426;&#22788;&#29702;&#30340;&#26684;&#24335;&#23545;&#20110;&#26234;&#33021;&#35774;&#35745;&#21644;&#24314;&#36896;&#24314;&#31569;&#21644;&#22522;&#30784;&#35774;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#33258;&#21160;&#21270;&#35268;&#21017;&#35299;&#37322;&#65288;ARI&#65289;&#26041;&#27861;&#24050;&#32463;&#30740;&#31350;&#22810;&#24180;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#24314;&#31569;&#27861;&#35268;&#20013;&#26089;&#26399;&#21644;&#25163;&#21160;&#31579;&#36873;&#21487;&#35299;&#37322;&#26465;&#27454;&#12290; &#34429;&#28982;&#20854;&#20013;&#23569;&#25968;&#26041;&#27861;&#32771;&#34385;&#20102;&#20174;&#26465;&#27454;&#21644;&#25991;&#26723;&#23618;&#38754;&#19978;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#36825;&#20195;&#34920;&#20102;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#35745;&#31639;&#26426;&#22788;&#29702;&#26684;&#24335;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#35780;&#20272;&#21644;&#22686;&#24378;&#21333;&#20010;&#26465;&#27454;&#21644;&#24314;&#31569;&#27861;&#35268;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#20960;&#20010;&#31867;&#21035;&#65292;&#20197;&#32771;&#34385;&#23545;&#35268;&#21017;&#35299;&#37322;&#30340;&#35201;&#27714;&#23545;&#27599;&#20010;&#24314;&#31569;&#27861;&#35268;&#26465;&#27454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#30340;&#39640;&#25928;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting regulatory documents or building codes into computer-processable formats is essential for the intelligent design and construction of buildings and infrastructures. Although automated rule interpretation (ARI) methods have been investigated for years, most of them highly depend on the early and manual filtering of interpretable clauses from a building code. While few of them considered machine interpretability, which represents the potential to be transformed into a computer-processable format, from both clause- and document-level. Therefore, this research aims to propose a novel approach to automatically evaluate and enhance the machine interpretability of single clause and building codes. First, a few categories are introduced to classify each clause in a building code considering the requirements for rule interpretation, and a dataset is developed for model training. Then, an efficient text classification model is developed based on a pretrained domain-specific language 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#36716;&#24405;&#25968;&#25454;&#65292;&#36890;&#36807;&#22312;&#26631;&#27880;&#38454;&#27573;&#36827;&#34892;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#37325;&#26032;&#22788;&#29702;&#21644;&#22312;&#26631;&#27880;&#21518;&#38454;&#27573;&#36827;&#34892;&#33258;&#21160;&#35789;&#38169;&#35823;&#20462;&#27491;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#36716;&#24405;&#35789;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#24182;&#21457;&#29616;&#20102;&#36716;&#24405;&#38169;&#35823;&#23545;ASR&#27169;&#22411;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14372</link><description>&lt;p&gt;
&#20154;&#31867;&#36716;&#24405;&#36136;&#37327;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Human Transcription Quality Improvement. (arXiv:2309.14372v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#36716;&#24405;&#25968;&#25454;&#65292;&#36890;&#36807;&#22312;&#26631;&#27880;&#38454;&#27573;&#36827;&#34892;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#37325;&#26032;&#22788;&#29702;&#21644;&#22312;&#26631;&#27880;&#21518;&#38454;&#27573;&#36827;&#34892;&#33258;&#21160;&#35789;&#38169;&#35823;&#20462;&#27491;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#36716;&#24405;&#35789;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#24182;&#21457;&#29616;&#20102;&#36716;&#24405;&#38169;&#35823;&#23545;ASR&#27169;&#22411;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#36716;&#24405;&#25968;&#25454;&#23545;&#20110;&#35757;&#32451;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34892;&#19994;&#32423;&#25968;&#25454;&#25910;&#38598;&#31649;&#36947;&#23545;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#25104;&#26412;&#39640;&#26114;&#65292;&#32780;&#20247;&#21253;&#36716;&#24405;&#30340;&#36136;&#37327;&#36739;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#25910;&#38598;&#35821;&#38899;&#36716;&#24405;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26426;&#21046;&#26469;&#25913;&#21892;&#36716;&#24405;&#36136;&#37327;&#65306;&#22312;&#26631;&#27880;&#38454;&#27573;&#22522;&#20110;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#37325;&#26032;&#22788;&#29702;&#21644;&#22312;&#26631;&#27880;&#21518;&#38454;&#27573;&#30340;&#33258;&#21160;&#35789;&#38169;&#35823;&#20462;&#27491;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;LibriCrowd - &#19968;&#20010;&#21253;&#21547;100&#23567;&#26102;&#33521;&#35821;&#35821;&#38899;&#36716;&#24405;&#30340;&#22823;&#35268;&#27169;&#20247;&#21253;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36716;&#24405;&#35789;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#20102;50%&#20197;&#19978;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#36716;&#24405;&#38169;&#35823;&#23545;ASR&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#24378;&#30456;&#20851;&#24615;&#12290;&#36716;&#24405;&#36136;&#37327;&#30340;&#25552;&#21319;&#20351;ASR&#27169;&#22411;&#30340;WER&#30456;&#23545;&#20943;&#23569;&#20102;10%&#20197;&#19978;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#65292;&#20197;&#36896;&#31119;&#30740;&#31350;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
High quality transcription data is crucial for training automatic speech recognition (ASR) systems. However, the existing industry-level data collection pipelines are expensive to researchers, while the quality of crowdsourced transcription is low. In this paper, we propose a reliable method to collect speech transcriptions. We introduce two mechanisms to improve transcription quality: confidence estimation based reprocessing at labeling stage, and automatic word error correction at post-labeling stage. We collect and release LibriCrowd - a large-scale crowdsourced dataset of audio transcriptions on 100 hours of English speech. Experiment shows the Transcription WER is reduced by over 50%. We further investigate the impact of transcription error on ASR model performance and found a strong correlation. The transcription quality improvement provides over 10% relative WER reduction for ASR models. We release the dataset and code to benefit the research community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#20195;&#29702;&#19982;&#20256;&#32479;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#26680;&#24515;&#24046;&#24322;&#21644;&#29305;&#28857;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35760;&#24518;&#20998;&#31867;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#20840;&#26032;&#30340;&#35774;&#35745;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2309.14365</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#28145;&#24230;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An In-depth Survey of Large Language Model-based Artificial Intelligence Agents. (arXiv:2309.14365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#20195;&#29702;&#19982;&#20256;&#32479;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#26680;&#24515;&#24046;&#24322;&#21644;&#29305;&#28857;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35760;&#24518;&#20998;&#31867;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#20840;&#26032;&#30340;&#35774;&#35745;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#26368;&#36817;&#20154;&#20204;&#19968;&#30452;&#22312;&#21162;&#21147;&#23558;&#23427;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLM-based AI&#20195;&#29702;&#19982;&#20256;&#32479;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#26680;&#24515;&#24046;&#24322;&#21644;&#29305;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#31867;&#22411;&#20195;&#29702;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#38416;&#26126;&#20102;LLM-based&#20195;&#29702;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#12289;&#30693;&#35782;&#23384;&#20648;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;AI&#20195;&#29702;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#35268;&#21010;&#12289;&#35760;&#24518;&#21644;&#24037;&#20855;&#20351;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#23588;&#20854;&#26159;&#23545;&#20110;&#20851;&#38190;&#30340;&#35760;&#24518;&#32452;&#20214;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#26041;&#26696;&#65292;&#19981;&#20165;&#36828;&#31163;&#20102;&#20256;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#32780;&#19988;&#20026;AI&#20195;&#29702;&#30340;&#35760;&#24518;&#31995;&#32479;&#35774;&#35745;&#25552;&#20379;&#20102;&#20840;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#22362;&#20449;&#23545;&#36825;&#20123;&#26680;&#24515;&#32452;&#20214;&#36827;&#34892;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Due to the powerful capabilities demonstrated by large language model (LLM), there has been a recent surge in efforts to integrate them with AI agents to enhance their performance. In this paper, we have explored the core differences and characteristics between LLM-based AI agents and traditional AI agents. Specifically, we first compare the fundamental characteristics of these two types of agents, clarifying the significant advantages of LLM-based agents in handling natural language, knowledge storage, and reasoning capabilities. Subsequently, we conducted an in-depth analysis of the key components of AI agents, including planning, memory, and tool use. Particularly, for the crucial component of memory, this paper introduced an innovative classification scheme, not only departing from traditional classification methods but also providing a fresh perspective on the design of an AI agent's memory system. We firmly believe that in-depth research and understanding of these core components
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22810;&#26679;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22806;&#37096;&#33258;&#28982;&#38382;&#39064;&#22312;&#30693;&#35782;&#24211;&#19978;&#36827;&#34892;&#22810;&#26679;&#21270;&#38382;&#39064;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#27169;&#22411;&#26694;&#26550;&#26469;&#35299;&#20915;&#22914;&#20309;&#22686;&#24378;&#22810;&#26679;&#21270;&#38382;&#39064;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.14362</link><description>&lt;p&gt;
&#36890;&#36807;&#22806;&#37096;&#33258;&#28982;&#38382;&#39064;&#22312;&#30693;&#35782;&#24211;&#19978;&#36827;&#34892;&#22810;&#26679;&#21270;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diversifying Question Generation over Knowledge Base via External Natural Questions. (arXiv:2309.14362v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14362
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22810;&#26679;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22806;&#37096;&#33258;&#28982;&#38382;&#39064;&#22312;&#30693;&#35782;&#24211;&#19978;&#36827;&#34892;&#22810;&#26679;&#21270;&#38382;&#39064;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#27169;&#22411;&#26694;&#26550;&#26469;&#35299;&#20915;&#22914;&#20309;&#22686;&#24378;&#22810;&#26679;&#21270;&#38382;&#39064;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#21333;&#20010;&#29983;&#25104;&#38382;&#39064;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20154;&#31867;&#20986;&#33394;&#30340;&#25913;&#20889;&#33021;&#21147;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#20041;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#34920;&#36798;&#26469;&#20256;&#36798;&#12290;&#20197;&#19978;&#35266;&#28857;&#20351;&#24471;&#22810;&#26679;&#21270;&#38382;&#39064;&#29983;&#25104;&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;&#25361;&#25112;&#26159;&#22810;&#26679;&#24615;&#35780;&#20272;&#25351;&#26631;&#12290;&#24403;&#21069;&#30340;&#25351;&#26631;&#19981;&#36275;&#20197;&#35780;&#20272;&#22810;&#26679;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#35745;&#31639;&#29983;&#25104;&#38382;&#39064;&#20013;&#21807;&#19968;n-gram&#30340;&#27604;&#20363;&#65292;&#26356;&#20542;&#21521;&#20110;&#34913;&#37327;&#37325;&#22797;&#32780;&#38750;&#30495;&#27491;&#30340;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26679;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#34913;&#37327;&#27599;&#20010;&#23454;&#20363;&#30340;&#21069;k&#20010;&#29983;&#25104;&#38382;&#39064;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#23427;&#20204;&#19982;&#22522;&#20934;&#38382;&#39064;&#30456;&#20851;&#12290;&#26174;&#28982;&#65292;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#22914;&#20309;&#22686;&#24378;&#22810;&#26679;&#21270;&#38382;&#39064;&#29983;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;&#20004;&#20010;&#36873;&#25321;&#27169;&#22411;&#20132;&#32455;&#32780;&#25104;&#30340;&#21452;&#27169;&#22411;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous methods on knowledge base question generation (KBQG) primarily focus on enhancing the quality of a single generated question. Recognizing the remarkable paraphrasing ability of humans, we contend that diverse texts should convey the same semantics through varied expressions. The above insights make diversifying question generation an intriguing task, where the first challenge is evaluation metrics for diversity. Current metrics inadequately assess the above diversity since they calculate the ratio of unique n-grams in the generated question itself, which leans more towards measuring duplication rather than true diversity. Accordingly, we devise a new diversity evaluation metric, which measures the diversity among top-k generated questions for each instance while ensuring their relevance to the ground truth. Clearly, the second challenge is how to enhance diversifying question generation. To address this challenge, we introduce a dual model framework interwoven by two selection
&lt;/p&gt;</description></item><item><title>COCO-Counterfactuals&#26159;&#19968;&#20010;&#33258;&#21160;&#26500;&#24314;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#21453;&#20107;&#23454;&#20363;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#20363;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;COCO-Counterfactuals&#30340;&#36136;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#25913;&#21892;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14356</link><description>&lt;p&gt;
COCO-Counterfactuals:&#33258;&#21160;&#26500;&#24314;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#21453;&#20107;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs. (arXiv:2309.14356v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14356
&lt;/p&gt;
&lt;p&gt;
COCO-Counterfactuals&#26159;&#19968;&#20010;&#33258;&#21160;&#26500;&#24314;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#21453;&#20107;&#23454;&#20363;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#20363;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;COCO-Counterfactuals&#30340;&#36136;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#25913;&#21892;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#20363;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#20013;&#24050;&#35777;&#26126;&#23545;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#24615;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#23613;&#31649;&#21453;&#20107;&#23454;&#20363;&#22312;NLP&#39046;&#22495;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#29992;&#65292;&#20294;&#30001;&#20110;&#21019;&#24314;&#26368;&#23567;&#21453;&#20107;&#23454;&#21464;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#30340;&#38590;&#24230;&#65292;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#20363;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21453;&#20107;&#23454;&#20363;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#21019;&#24314;COCO-Counterfactuals&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;MS-COCO&#25968;&#25454;&#38598;&#30340;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#25991;&#26412;&#26631;&#39064;&#30340;&#37197;&#23545;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#39564;&#35777;&#20102;COCO-Counterfactuals&#30340;&#36136;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#21453;&#20107;&#23454;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;COCO-Counterfactuals&#22312;&#25913;&#21892;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#12289;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#27665;&#31929;&#20027;&#20041;&#31435;&#22330;&#65292;&#36890;&#36807;&#22312;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#30340;&#28436;&#35762;&#20013;&#36827;&#34892;&#35821;&#35328;&#26631;&#35760;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;PopBERT&#65289;&#26469;&#26816;&#27979;&#21644;&#37327;&#21270;&#27665;&#31929;&#20027;&#20041;&#30340;&#26680;&#24515;&#32500;&#24230;&#65292;&#24182;&#30830;&#23450;&#27665;&#31929;&#20027;&#20041;&#38472;&#36848;&#19982;&#24038;&#32764;&#25110;&#21491;&#32764;&#20027;&#23548;&#24847;&#35782;&#24418;&#24577;&#30340;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2309.14355</link><description>&lt;p&gt;
PopBERT. &#26816;&#27979;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#20013;&#30340;&#27665;&#31929;&#20027;&#20041;&#21450;&#20854;&#20027;&#23548;&#24847;&#35782;&#24418;&#24577;
&lt;/p&gt;
&lt;p&gt;
PopBERT. Detecting populism and its host ideologies in the German Bundestag. (arXiv:2309.14355v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#12289;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#27665;&#31929;&#20027;&#20041;&#31435;&#22330;&#65292;&#36890;&#36807;&#22312;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#30340;&#28436;&#35762;&#20013;&#36827;&#34892;&#35821;&#35328;&#26631;&#35760;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;PopBERT&#65289;&#26469;&#26816;&#27979;&#21644;&#37327;&#21270;&#27665;&#31929;&#20027;&#20041;&#30340;&#26680;&#24515;&#32500;&#24230;&#65292;&#24182;&#30830;&#23450;&#27665;&#31929;&#20027;&#20041;&#38472;&#36848;&#19982;&#24038;&#32764;&#25110;&#21491;&#32764;&#20027;&#23548;&#24847;&#35782;&#24418;&#24577;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27665;&#31929;&#20027;&#20041;&#30340;&#23835;&#36215;&#24341;&#36215;&#20102;&#35768;&#22810;&#25919;&#27835;&#23398;&#23478;&#21644;&#20174;&#19994;&#20154;&#21592;&#30340;&#20851;&#27880;&#65292;&#28982;&#32780;&#23545;&#20854;&#28508;&#22312;&#35821;&#35328;&#30340;&#26816;&#27979;&#20173;&#28982;&#19981;&#23436;&#25972;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#21487;&#38752;&#12289;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#27665;&#31929;&#20027;&#20041;&#31435;&#22330;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#30340;&#35758;&#20250;&#28436;&#35762;(2013&#24180;&#33267;2021&#24180;)&#21019;&#24314;&#20102;&#19968;&#20010;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#12290;&#36981;&#24490;&#27665;&#31929;&#20027;&#20041;&#30340;&#27010;&#24565;&#23450;&#20041;&#65292;&#25105;&#20204;&#23558;&#23545;&#39640;&#23578;&#20154;&#27665;&#25110;&#33104;&#36133;&#31934;&#33521;&#30340;&#36947;&#24503;&#24341;&#29992;&#26631;&#35760;&#20026;&#27665;&#31929;&#20027;&#20041;&#35821;&#35328;&#30340;&#26680;&#24515;&#32500;&#24230;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30830;&#23450;&#27665;&#31929;&#20027;&#20041;&#34180;&#24847;&#35782;&#24418;&#24577;&#30340;&#24418;&#25104;&#26041;&#24335;&#65292;&#25105;&#20204;&#27880;&#37322;&#20102;&#27665;&#31929;&#20027;&#20041;&#38472;&#36848;&#22914;&#20309;&#19982;&#24038;&#32764;&#25110;&#21491;&#32764;&#30340;&#20027;&#23548;&#24847;&#35782;&#24418;&#24577;&#30456;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;PopBERT&#65289;&#20316;&#20026;&#19968;&#20010;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#37327;&#21270;&#27599;&#20010;&#32500;&#24230;&#12290;&#19968;&#31995;&#21015;&#39564;&#35777;&#26816;&#26597;&#26174;&#31034;&#35813;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#24378;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#38754;&#37096;&#26377;&#25928;&#24615;&#65292;&#19982;&#19987;&#23478;&#35843;&#26597;&#30340;&#20826;&#27966;&#25490;&#21517;&#30456;&#21563;&#21512;&#65292;&#24182;&#26816;&#27979;&#20986;-of-sa
&lt;/p&gt;
&lt;p&gt;
The rise of populism concerns many political scientists and practitioners, yet the detection of its underlying language remains fragmentary. This paper aims to provide a reliable, valid, and scalable approach to measure populist stances. For that purpose, we created an annotated dataset based on parliamentary speeches of the German Bundestag (2013 to 2021). Following the ideational definition of populism, we label moralizing references to the virtuous people or the corrupt elite as core dimensions of populist language. To identify, in addition, how the thin ideology of populism is thickened, we annotate how populist statements are attached to left-wing or right-wing host ideologies. We then train a transformer-based model (PopBERT) as a multilabel classifier to detect and quantify each dimension. A battery of validation checks reveals that the model has a strong predictive accuracy, provides high qualitative face validity, matches party rankings of expert surveys, and detects out-of-sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14348</link><description>&lt;p&gt;
&#36890;&#36807;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#25269;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM. (arXiv:2309.14348v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;LLMs&#21487;&#33021;&#34987;&#28389;&#29992;&#26469;&#29983;&#25104;&#26377;&#23475;&#25110;&#24694;&#24847;&#20869;&#23481;&#12290;&#23613;&#31649;&#26377;&#19968;&#31995;&#21015;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#40784;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#24182;&#38450;&#27490;&#23427;&#20204;&#29983;&#25104;&#19981;&#36866;&#24403;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#23545;&#40784;&#36890;&#24120;&#26159;&#33030;&#24369;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#20248;&#21270;&#25110;&#25163;&#24037;&#26500;&#24314;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#20197;&#38450;&#33539;&#28508;&#22312;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#26500;&#24314;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#65292;&#36890;&#36807;&#20855;&#26377;&#31283;&#20581;&#23545;&#40784;&#26816;&#26597;&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#23545;&#21407;&#22987;LLM&#36827;&#34892;&#20219;&#20309;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;RA-LLM&#22312;&#38450;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments
&lt;/p&gt;</description></item><item><title>MiChao-HuaFen 1.0&#26159;&#19968;&#20010;&#19987;&#20026;&#26032;&#38395;&#21644;&#25919;&#24220;&#37096;&#38376;&#23450;&#21046;&#30340;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#22823;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;&#65292;&#23427;&#19981;&#20165;&#33021;&#22815;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#36136;&#37327;&#38656;&#27714;&#65292;&#36824;&#26377;&#21161;&#20110;&#25512;&#21160;&#30456;&#20851;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.13079</link><description>&lt;p&gt;
MiChao-HuaFen 1.0&#65306;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#22823;&#27169;&#22411;&#30340;&#19987;&#29992;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MiChao-HuaFen 1.0: A Specialized Pre-trained Corpus Dataset for Domain-specific Large Models. (arXiv:2309.13079v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13079
&lt;/p&gt;
&lt;p&gt;
MiChao-HuaFen 1.0&#26159;&#19968;&#20010;&#19987;&#20026;&#26032;&#38395;&#21644;&#25919;&#24220;&#37096;&#38376;&#23450;&#21046;&#30340;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#22823;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;&#65292;&#23427;&#19981;&#20165;&#33021;&#22815;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#36136;&#37327;&#38656;&#27714;&#65292;&#36824;&#26377;&#21161;&#20110;&#25512;&#21160;&#30456;&#20851;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#22914;GPT-4&#31561;&#36890;&#29992;&#22823;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#35832;&#22914;&#21307;&#30103;&#12289;&#27861;&#24459;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#23545;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#29305;&#23450;&#36755;&#20986;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#39318;&#20808;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#22823;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#29305;&#27530;&#38656;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;MiChao-HuaFen 1.0&#8221;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#29305;&#21035;&#38024;&#23545;&#26032;&#38395;&#21644;&#25919;&#24220;&#37096;&#38376;&#12290;&#35813;&#25968;&#25454;&#38598;&#26469;&#28304;&#20110;2022&#24180;&#20844;&#24320;&#21487;&#29992;&#30340;&#20114;&#32852;&#32593;&#25968;&#25454;&#65292;&#32463;&#36807;&#22810;&#36718;&#28165;&#27905;&#21644;&#22788;&#29702;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20855;&#22791;&#25345;&#32493;&#21644;&#31283;&#23450;&#30340;&#26356;&#26032;&#26426;&#21046;&#12290;&#35813;&#25968;&#25454;&#38598;&#19981;&#20165;&#25903;&#25345;&#38024;&#23545;&#20013;&#25991;&#22402;&#30452;&#39046;&#22495;&#30340;&#22823;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#36824;&#21161;&#21147;&#20110;&#25512;&#21160;&#30456;&#20851;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advancement of deep learning technologies, general-purpose large models such as GPT-4 have demonstrated exceptional capabilities across various domains. Nevertheless, there remains a demand for high-quality, domain-specific outputs in areas like healthcare, law, and finance. This paper first evaluates the existing large models for specialized domains and discusses their limitations. To cater to the specific needs of certain domains, we introduce the ``MiChao-HuaFen 1.0'' pre-trained corpus dataset, tailored for the news and governmental sectors. The dataset, sourced from publicly available internet data from 2022, underwent multiple rounds of cleansing and processing to ensure high quality and reliable origins, with provisions for consistent and stable updates. This dataset not only supports the pre-training of large models for Chinese vertical domains but also aids in propelling deep learning research and applications in related fields.
&lt;/p&gt;</description></item><item><title>InvestLM&#26159;&#19968;&#20010;&#36890;&#36807;&#23545;&#37329;&#34701;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35299;&#37329;&#34701;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#25237;&#36164;&#30456;&#20851;&#38382;&#39064;&#19978;&#25552;&#20379;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;&#37329;&#34701;&#19987;&#23478;&#35780;&#20215;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#21830;&#19994;&#27169;&#22411;&#21487;&#23218;&#32654;&#65292;&#24182;&#22312;&#37329;&#34701;NLP&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.13064</link><description>&lt;p&gt;
InvestLM&#65306;&#20351;&#29992;&#37329;&#34701;&#39046;&#22495;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning. (arXiv:2309.13064v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13064
&lt;/p&gt;
&lt;p&gt;
InvestLM&#26159;&#19968;&#20010;&#36890;&#36807;&#23545;&#37329;&#34701;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35299;&#37329;&#34701;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#25237;&#36164;&#30456;&#20851;&#38382;&#39064;&#19978;&#25552;&#20379;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;&#37329;&#34701;&#19987;&#23478;&#35780;&#20215;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#21830;&#19994;&#27169;&#22411;&#21487;&#23218;&#32654;&#65292;&#24182;&#22312;&#37329;&#34701;NLP&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#37329;&#34701;&#39046;&#22495;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InvestLM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#19982;&#37329;&#34701;&#25237;&#36164;&#30456;&#20851;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#23545;LLaMA-65B&#36827;&#34892;&#35843;&#20248;&#12290;&#21463;&#21040;&#8220;&#23569;&#21363;&#26159;&#22810;&#8221;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25163;&#21160;&#31574;&#21010;&#20102;&#19968;&#20010;&#26082;&#23567;&#21448;&#22810;&#26679;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20174;&#29305;&#35768;&#37329;&#34701;&#20998;&#26512;&#24072;&#65288;CFA&#65289;&#32771;&#35797;&#38382;&#39064;&#21040;SEC&#25991;&#20214;&#21644;Stackexchange&#37327;&#21270;&#37329;&#34701;&#35752;&#35770;&#30340;&#24191;&#27867;&#37329;&#34701;&#30456;&#20851;&#20027;&#39064;&#12290;InvestLM&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#29702;&#35299;&#37329;&#34701;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#25237;&#36164;&#30456;&#20851;&#38382;&#39064;&#25552;&#20379;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;&#21253;&#25324;&#23545;&#20914;&#22522;&#37329;&#32463;&#29702;&#21644;&#30740;&#31350;&#20998;&#26512;&#24072;&#22312;&#20869;&#30340;&#37329;&#34701;&#19987;&#23478;&#23558;InvestLM&#30340;&#22238;&#31572;&#35780;&#20215;&#20026;&#19982;&#26368;&#20808;&#36827;&#30340;&#21830;&#19994;&#27169;&#22411;&#65288;GPT-3.5&#12289;GPT-4&#21644;Claude-2&#65289;&#21487;&#23218;&#32654;&#12290;&#23545;&#19968;&#32452;&#37329;&#34701;NLP&#22522;&#20934;&#38382;&#39064;&#36827;&#34892;&#38646;&#26679;&#26412;&#35780;&#20272;&#34920;&#26126;&#20102;&#20854;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20174;&#30740;&#31350;&#35282;&#24230;&#26469;&#30475;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#29305;&#23450;LLM&#36827;&#34892;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new financial domain large language model, InvestLM, tuned on LLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset related to financial investment. Inspired by less-is-more-for-alignment (Zhou et al., 2023), we manually curate a small yet diverse instruction dataset, covering a wide range of financial related topics, from Chartered Financial Analyst (CFA) exam questions to SEC filings to Stackexchange quantitative finance discussions. InvestLM shows strong capabilities in understanding financial text and provides helpful responses to investment related questions. Financial experts, including hedge fund managers and research analysts, rate InvestLM's response as comparable to those of state-of-the-art commercial models (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of financial NLP benchmarks demonstrates strong generalizability. From a research perspective, this work suggests that a high-quality domain specific LLM can be tuned usin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#20132;&#20114;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#33021;&#22815;&#20934;&#30830;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.12460</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#31185;&#23398;&#25104;&#20687;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Multimodal Deep Learning for Scientific Imaging Interpretation. (arXiv:2309.12460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#20132;&#20114;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#33021;&#22815;&#20934;&#30830;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#25104;&#20687;&#39046;&#22495;&#65292;&#35299;&#37322;&#35270;&#35273;&#25968;&#25454;&#24120;&#24120;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;&#23545;&#20027;&#39064;&#26448;&#26009;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#27169;&#25311;&#24182;&#35780;&#20272;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;SEM&#65289;&#22270;&#20687;&#30340;&#20154;&#31867;&#20132;&#20114;&#65292;&#29305;&#21035;&#26159;&#29627;&#29827;&#26448;&#26009;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20174;&#21516;&#34892;&#35780;&#35758;&#30340;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#20511;&#21161; GPT-4 &#30340;&#33021;&#21147;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#23613;&#31649;&#23384;&#22312;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#32454;&#24494;&#30340;&#35299;&#37322;&#21644;&#19987;&#19994;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#22312;&#21046;&#23450;&#20934;&#30830;&#30340;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#65292;&#20351;&#24471;&#36827;&#34892;&#32508;&#21512;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of scientific imaging, interpreting visual data often demands an intricate combination of human expertise and deep comprehension of the subject materials. This study presents a novel methodology to linguistically emulate and subsequently evaluate human-like interactions with Scanning Electron Microscopy (SEM) images, specifically of glass materials. Leveraging a multimodal deep learning framework, our approach distills insights from both textual and visual data harvested from peer-reviewed articles, further augmented by the capabilities of GPT-4 for refined data synthesis and evaluation. Despite inherent challenges--such as nuanced interpretations and the limited availability of specialized datasets--our model (GlassLLaVA) excels in crafting accurate interpretations, identifying key features, and detecting defects in previously unseen SEM images. Moreover, we introduce versatile evaluation metrics, suitable for an array of scientific imaging applications, which allows for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MBR&#24494;&#35843;&#21644;QE&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#26102;&#30340;&#36136;&#37327;&#25552;&#21319;&#33976;&#39311;&#21040;&#22522;&#20934;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10966</link><description>&lt;p&gt;
MBR&#21644;QE&#24494;&#35843;&#65306;&#23545;&#26368;&#20339;&#21644;&#26368;&#26114;&#36149;&#30340;&#35299;&#30721;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#26102;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods. (arXiv:2309.10966v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MBR&#24494;&#35843;&#21644;QE&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#26102;&#30340;&#36136;&#37327;&#25552;&#21319;&#33976;&#39311;&#21040;&#22522;&#20934;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#30340;&#35299;&#30721;&#26041;&#27861;&#30740;&#31350;&#20013;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#27874;&#26463;&#25628;&#32034;&#21644;&#36138;&#23146;&#35299;&#30721;&#31639;&#27861;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#27169;&#22411;&#27010;&#29575;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#27169;&#22411;&#22256;&#24785;&#24230;&#19982;&#36136;&#37327;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26356;&#24378;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#21253;&#25324;&#36136;&#37327;&#20272;&#35745;&#65288;QE&#65289;&#37325;&#25490;&#24207;&#21644;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#12290;&#23613;&#31649;&#36825;&#20123;&#35299;&#30721;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MBR&#24494;&#35843;&#21644;QE&#24494;&#35843;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#33976;&#39311;&#20102;&#36825;&#20123;&#35299;&#30721;&#26041;&#27861;&#30340;&#36136;&#37327;&#25552;&#21319;&#65292;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#36825;&#19968;&#20856;&#22411;&#30340;NLG&#20219;&#21153;&#65292;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#36827;&#34892;&#33258;&#35757;&#32451;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#30340;&#24615;&#33021;&#20173;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#22806;&#37096;LLM&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#26102;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#20063;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in decoding methods for Natural Language Generation (NLG) tasks has shown that the traditional beam search and greedy decoding algorithms are not optimal, because model probabilities do not always align with human preferences. Stronger decoding methods, including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR) decoding, have since been proposed to mitigate the model-perplexity-vs-quality mismatch. While these decoding methods achieve state-of-the-art performance, they are prohibitively expensive to compute. In this work, we propose MBR finetuning and QE finetuning which distill the quality gains from these decoding methods at training time, while using an efficient decoding algorithm at inference time. Using the canonical NLG task of Neural Machine Translation (NMT), we show that even with self-training, these finetuning methods significantly outperform the base model. Moreover, when using an external LLM as a teacher model, these finetuning methods outp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.10313</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#36827;&#34892;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;GPT4&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#20391;&#37325;&#20110;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#35270;&#35273;&#27169;&#22411;&#26469;&#24320;&#21457;&#36890;&#29992;&#30340;LLM&#12290;&#28982;&#32780;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24494;&#35843;&#27169;&#22411;&#26080;&#27861;&#20445;&#25345;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#20173;&#28982;&#26159;&#22810;&#27169;&#24577;LLM&#65288;MLLM&#65289;&#20013;&#30340;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EMT&#65306;&#29992;&#20110;&#35780;&#20272;MLLM&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;MLLM&#20316;&#20026;&#19968;&#20010;&#22270;&#20687;&#20998;&#31867;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;EMT&#26469;&#35780;&#20272;&#20960;&#20010;&#24320;&#28304;&#30340;&#24494;&#35843;MLLM&#65292;&#24182;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;MLLM&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#26080;&#27861;&#20445;&#25345;&#19982;&#20182;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32487;&#32493;&#24494;&#35843;LLaVA&#65292;&#19968;&#31181;MLLM&#65292;&#24182;&#21033;&#29992;EMT&#26469;&#35780;&#20272;&#25972;&#20010;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#30340;&#24494;&#35843;&#38454;&#27573;&#26159;&#20851;&#38190;&#30340;&#65292;&#36807;&#26089;&#20572;&#27490;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#20302;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GLORY&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20840;&#23616;&#22270;&#19982;&#26412;&#22320;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26500;&#24314;&#20840;&#23616;&#24863;&#30693;&#21382;&#21490;&#26032;&#38395;&#32534;&#30721;&#22120;&#26469;&#34701;&#21512;&#21382;&#21490;&#26032;&#38395;&#34920;&#31034;&#65292;&#24182;&#32771;&#34385;&#20102;&#29992;&#25143;&#38544;&#34255;&#30340;&#21160;&#26426;&#21644;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.06576</link><description>&lt;p&gt;
&#36229;&#36234;&#26412;&#22320;&#33539;&#22260;&#65306;&#20840;&#29699;&#22270;&#22686;&#24378;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Going Beyond Local: Global Graph-Enhanced Personalized News Recommendations. (arXiv:2307.06576v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GLORY&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20840;&#23616;&#22270;&#19982;&#26412;&#22320;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26500;&#24314;&#20840;&#23616;&#24863;&#30693;&#21382;&#21490;&#26032;&#38395;&#32534;&#30721;&#22120;&#26469;&#34701;&#21512;&#21382;&#21490;&#26032;&#38395;&#34920;&#31034;&#65292;&#24182;&#32771;&#34385;&#20102;&#29992;&#25143;&#38544;&#34255;&#30340;&#21160;&#26426;&#21644;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#21521;&#29992;&#25143;&#25512;&#33616;&#20505;&#36873;&#26032;&#38395;&#25991;&#31456;&#19968;&#30452;&#26159;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#36817;&#26399;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#20016;&#23500;&#30340;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#29992;&#20174;&#26412;&#22320;&#21382;&#21490;&#26032;&#38395;&#27966;&#29983;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#20840;&#23616;&#35270;&#35282;&#65292;&#26410;&#33021;&#32771;&#34385;&#29992;&#25143;&#38544;&#34255;&#30340;&#21160;&#26426;&#21644;&#34892;&#20026;&#65292;&#36229;&#36234;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411; GLORY&#65288;Global-LOcal news Recommendation sYstem&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#20174;&#20854;&#20182;&#29992;&#25143;&#23398;&#21040;&#30340;&#20840;&#23616;&#34920;&#31034;&#21644;&#26412;&#22320;&#34920;&#31034;&#65292;&#26469;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20840;&#23616;&#24863;&#30693;&#21382;&#21490;&#26032;&#38395;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#20840;&#23616;&#26032;&#38395;&#22270;&#65292;&#24182;&#20351;&#29992;&#38376;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20016;&#23500;&#26032;&#38395;&#34920;&#31034;&#65292;&#20174;&#32780;&#36890;&#36807;&#21382;&#21490;&#26032;&#38395;&#32858;&#21512;&#22120;&#34701;&#21512;&#21382;&#21490;&#26032;&#38395;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precisely recommending candidate news articles to users has always been a core challenge for personalized news recommendation systems. Most recent works primarily focus on using advanced natural language processing techniques to extract semantic information from rich textual data, employing content-based methods derived from local historical news. However, this approach lacks a global perspective, failing to account for users' hidden motivations and behaviors beyond semantic information. To address this challenge, we propose a novel model called GLORY (Global-LOcal news Recommendation sYstem), which combines global representations learned from other users with local representations to enhance personalized recommendation systems. We accomplish this by constructing a Global-aware Historical News Encoder, which includes a global news graph and employs gated graph neural networks to enrich news representations, thereby fusing historical news representations by a historical news aggregator.
&lt;/p&gt;</description></item><item><title>MO-VLN&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#36890;&#29992;&#26426;&#22120;&#20154;&#22312;&#22810;&#20219;&#21153;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;5&#24320;&#21457;&#36924;&#30495;&#30340;&#22330;&#26223;&#21644;&#21253;&#21547;&#22810;&#31181;&#19981;&#24120;&#35265;&#29289;&#20307;&#26469;&#27979;&#35797;&#20854;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.10322</link><description>&lt;p&gt;
MO-VLN:&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#38598;&#21512;&#38646;&#26679;&#26412;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934; (arXiv:2306.10322v2 [cs.CV] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
MO-VLN: A Multi-Task Benchmark for Open-set Zero-Shot Vision-and-Language Navigation. (arXiv:2306.10322v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10322
&lt;/p&gt;
&lt;p&gt;
MO-VLN&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#36890;&#29992;&#26426;&#22120;&#20154;&#22312;&#22810;&#20219;&#21153;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;5&#24320;&#21457;&#36924;&#30495;&#30340;&#22330;&#26223;&#21644;&#21253;&#21547;&#22810;&#31181;&#19981;&#24120;&#35265;&#29289;&#20307;&#26469;&#27979;&#35797;&#20854;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#24517;&#39035;&#29702;&#35299;&#25351;&#20196;&#24182;&#26681;&#25454;&#35270;&#35273;&#35266;&#23519;&#25214;&#21040;&#30446;&#26631;&#23545;&#35937;&#25110;&#20301;&#32622;&#65292;&#21363;&#20351;&#22312;&#26410;&#25506;&#32034;&#30340;&#29615;&#22659;&#20013;&#20063;&#33021;&#20570;&#21040;&#12290;&#22823;&#22810;&#25968;&#20195;&#29702;&#20381;&#36182;&#20110;&#22823;&#37327;&#22810;&#26679;&#30340;&#35757;&#32451;&#25968;&#25454;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#65292;&#36825;&#38656;&#35201;&#26114;&#36149;&#30340;&#21171;&#21160;&#21147;&#12290;&#36825;&#20123;&#20195;&#29702;&#36890;&#24120;&#21482;&#20851;&#27880;&#24120;&#35265;&#30340;&#23545;&#35937;&#21644;&#36739;&#23569;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#19981;&#36275;&#20197;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25351;&#20196;&#12290;&#20026;&#20102;&#20419;&#36827;&#24320;&#25918;&#38598;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MO-VLN&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#27979;&#35797;&#20195;&#29702;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;5&#24320;&#21457;&#20102;&#19968;&#20010;3D&#27169;&#25311;&#22120;&#65292;&#28210;&#26579;&#20102;&#36924;&#30495;&#30340;&#22330;&#26223;&#65292;&#21253;&#21547;&#26356;&#30495;&#23454;&#30340;&#20809;&#29031;&#21644;&#32454;&#33410;&#12290;&#27169;&#25311;&#22120;&#21253;&#21547;&#19977;&#20010;&#22330;&#26223;&#65292;&#21363;&#21654;&#21857;&#39302;&#12289;&#39184;&#21381;&#21644;&#20859;&#32769;&#38498;&#65292;&#36825;&#20123;&#22330;&#26223;&#22312;&#24037;&#19994;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#25311;&#22120;&#28041;&#21450;&#22810;&#31181;&#19981;&#24120;&#35265;&#30340;&#29289;&#20307;&#65292;&#22914;&#22806;&#21334;&#26479;&#21644;&#21307;&#29992;&#33014;&#24102;&#65292;&#36825;&#20123;&#29289;&#20307;&#26356;&#21152;&#22797;&#26434;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a natural language, a general robot has to comprehend the instruction and find the target object or location based on visual observations even in unexplored environments. Most agents rely on massive diverse training data to achieve better generalization, which requires expensive labor. These agents often focus on common objects and fewer tasks, thus are not intelligent enough to handle different types of instructions. To facilitate research in open-set vision-and-language navigation, we propose a benchmark named MO-VLN, aiming at testing the effectiveness and generalization of the agent in the multi-task setting. First, we develop a 3D simulator rendered by realistic scenarios using Unreal Engine 5, containing more realistic lights and details. The simulator contains three scenes, i.e., cafe, restaurant, and nursing house, of high value in the industry. Besides, our simulator involves multiple uncommon objects, such as takeaway cup and medical adhesive tape, which are more compli
&lt;/p&gt;</description></item><item><title>Med-UniC&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#36328;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#65292;&#23454;&#29616;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#23545;&#40784;&#35268;&#21017;(CTR)&#65292;&#20197;&#26126;&#30830;&#32479;&#19968;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#30340;&#21307;&#23398;&#25253;&#21578;&#30340;&#36328;&#35821;&#35328;&#35821;&#20041;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.19894</link><description>&lt;p&gt;
Med-UniC&#65306;&#36890;&#36807;&#20943;&#23569;&#20559;&#35265;&#23454;&#29616;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;
&lt;/p&gt;
&lt;p&gt;
Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias. (arXiv:2305.19894v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19894
&lt;/p&gt;
&lt;p&gt;
Med-UniC&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#36328;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#65292;&#23454;&#29616;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#23545;&#40784;&#35268;&#21017;(CTR)&#65292;&#20197;&#26126;&#30830;&#32479;&#19968;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#30340;&#21307;&#23398;&#25253;&#21578;&#30340;&#36328;&#35821;&#35328;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#32570;&#24615;&#23545;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#30340;&#25928;&#26524;&#36896;&#25104;&#20102;&#20005;&#37325;&#38556;&#30861;&#12290;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#22312;&#20110;&#32467;&#21512;&#26469;&#33258;&#21508;&#31181;&#35821;&#35328;&#31038;&#21306;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;&#25361;&#25112;&#26469;&#33258;&#20110;&#25972;&#21512;&#19981;&#21516;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#12289;&#29305;&#23450;&#20110;&#35821;&#35328;&#30340;&#21307;&#23398;&#26415;&#35821;&#20197;&#21450;&#29305;&#23450;&#20110;&#25991;&#21270;&#30340;&#38544;&#24335;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#32771;&#34385;&#22240;&#32032;&#26159;&#30001;&#19981;&#21516;&#35821;&#35328;&#24341;&#36215;&#30340;&#31038;&#21306;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#32479;&#19968;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;(Med-UniC)&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#25972;&#21512;&#26469;&#33258;&#20004;&#31181;&#26368;&#24120;&#35265;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#65292;&#21363;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#23545;&#40784;&#35268;&#21017;(CTR)&#65292;&#26126;&#30830;&#32479;&#19968;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#30340;&#21307;&#23398;&#25253;&#21578;&#30340;&#36328;&#35821;&#35328;&#35821;&#20041;&#34920;&#31034;&#12290;&#36890;&#36807;&#28508;&#22312;&#35821;&#35328;&#35299;&#32544;&#65292;&#20248;&#21270;CTR&#65292;&#20351;&#25105;&#20204;&#30340;&#20248;&#21270;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scarcity of data presents a critical obstacle to the efficacy of medical visionlanguage pre-training (VLP). A potential solution lies in the combination of datasets from various language communities. Nevertheless, the main challenge stems from the complexity of integrating diverse syntax and semantics, language-specific medical terminology, and culture-specific implicit knowledge. Therefore, one crucial aspect to consider is the presence of community bias caused by different languages. This paper presents a novel framework named Unifying Cross-Lingual Medical Vision-Language Pre-Training (Med-UniC), designed to integrate multimodal medical data from the two most prevalent languages, English and Spanish. Specifically, we propose Cross-lingual Text Alignment Regularization (CTR) to explicitly unify cross-lingual semantic representations of medical reports originating from diverse language communities. CTR is optimized through latent language disentanglement, rendering our optimizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#36827;&#34892;&#23398;&#20064;&#65292;&#20854;&#30446;&#26631;&#26159;&#23454;&#29616;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#65292;&#36890;&#36807;&#32852;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.10913</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement. (arXiv:2305.10913v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#36827;&#34892;&#23398;&#20064;&#65292;&#20854;&#30446;&#26631;&#26159;&#23454;&#29616;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#65292;&#36890;&#36807;&#32852;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#20165;&#21033;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#23398;&#20064;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#12290;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#38590;&#24230;&#26356;&#22823;&#65292;&#22240;&#20026;&#26080;&#27861;&#33719;&#24471;&#36793;&#30028;&#26694;&#21644;&#25991;&#26412;&#30701;&#35821;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#27169;&#22411;&#65288;SPRM&#65289;&#65292;&#20854;&#39044;&#27979;&#32467;&#26524;&#26159;&#36890;&#36807;&#32452;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#24471;&#21040;&#30340;&#12290;&#31532;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22359;&#26088;&#22312;&#36820;&#22238;&#25991;&#26412;&#30701;&#35821;&#21644;&#36793;&#30028;&#26694;&#20043;&#38388;&#30340;&#31895;&#30053;&#23545;&#40784;&#12290;&#31532;&#20108;&#20010;&#35757;&#32451;&#36807;&#30340;&#27169;&#22359;&#30001;&#20004;&#20010;&#23376;&#32452;&#20214;&#32452;&#25104;&#65292;&#29992;&#20110;&#32454;&#21270;&#31895;&#30053;&#30340;&#23545;&#40784;&#20197;&#25552;&#39640;&#26368;&#32456;&#30701;&#35821;-&#36793;&#30028;&#26694;&#23545;&#40784;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#35757;&#32451;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#22270;&#20687;&#21644;&#21477;&#23376;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#30456;&#20284;&#24230;&#65292;&#21516;&#26102;&#20351;&#21516;&#19968;&#21477;&#23376;&#21644;&#19968;&#20010;&#26032;&#30340;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#30456;&#20284;&#24230;&#26368;&#23567;&#21270;&#65292;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#35757;&#32451;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using only image-sentence pairs, weakly-supervised visual-textual grounding aims to learn region-phrase correspondences of the respective entity mentions. Compared to the supervised approach, learning is more difficult since bounding boxes and textual phrases correspondences are unavailable. In light of this, we propose the Semantic Prior Refinement Model (SPRM), whose predictions are obtained by combining the output of two main modules. The first untrained module aims to return a rough alignment between textual phrases and bounding boxes. The second trained module is composed of two sub-components that refine the rough alignment to improve the accuracy of the final phrase-bounding box alignments. The model is trained to maximize the multimodal similarity between an image and a sentence, while minimizing the multimodal similarity of the same sentence and a new unrelated image, carefully selected to help the most during training. Our approach shows state-of-the-art results on two popula
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26080;&#30417;&#30563;&#35821;&#38899;&#37325;&#26500;&#26469;&#35299;&#20915;&#20174;&#35821;&#38899;&#20013;&#20998;&#31163;&#24773;&#32490;&#38901;&#24459;&#30340;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#38598;&#25104;&#22810;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#25552;&#21462;&#20986;&#38901;&#24459;&#20449;&#24687;&#12290;&#36825;&#23545;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#35828;&#35805;&#20154;&#39564;&#35777;&#31561;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2212.06972</link><description>&lt;p&gt;
&#29992;&#26080;&#30417;&#30563;&#35821;&#38899;&#37325;&#26500;&#35299;&#24320;&#38901;&#24459;&#34920;&#31034;&#30340;&#32416;&#32544;
&lt;/p&gt;
&lt;p&gt;
Disentangling Prosody Representations with Unsupervised Speech Reconstruction. (arXiv:2212.06972v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26080;&#30417;&#30563;&#35821;&#38899;&#37325;&#26500;&#26469;&#35299;&#20915;&#20174;&#35821;&#38899;&#20013;&#20998;&#31163;&#24773;&#32490;&#38901;&#24459;&#30340;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#38598;&#25104;&#22810;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#25552;&#21462;&#20986;&#38901;&#24459;&#20449;&#24687;&#12290;&#36825;&#23545;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#35828;&#35805;&#20154;&#39564;&#35777;&#31561;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35821;&#38899;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#34920;&#24449;&#65292;&#21253;&#25324;&#35821;&#20041;&#20869;&#23481;&#12289;&#35828;&#35805;&#32773;&#36523;&#20221;&#21644;&#38901;&#24459;&#20449;&#24687;&#12290;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35828;&#35805;&#20154;&#39564;&#35777;&#20219;&#21153;&#20013;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#34920;&#31034;&#20998;&#31163;&#26041;&#38754;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#38901;&#24459;&#20449;&#24687;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#19981;&#21516;&#23646;&#24615;&#65288;&#22914;&#38899;&#33394;&#21644;&#33410;&#22863;&#65289;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#20197;&#21450;&#38656;&#35201;&#26377;&#30417;&#30563;&#35757;&#32451;&#26041;&#26696;&#26469;&#23454;&#29616;&#24378;&#22823;&#30340;&#22823;&#35268;&#27169;&#12289;&#19982;&#35828;&#35805;&#32773;&#26080;&#20851;&#30340;ASR&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#26080;&#30417;&#30563;&#37325;&#26500;&#26469;&#35299;&#20915;&#20174;&#35821;&#38899;&#20013;&#20998;&#31163;&#24773;&#32490;&#38901;&#24459;&#30340;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#25552;&#20986;&#30340;&#35821;&#38899;&#37325;&#26500;&#27169;&#22411;Prosody2Vec&#20013;&#37492;&#21035;&#12289;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#38598;&#25104;&#20102;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;(1) &#19968;&#20010;&#21333;&#20803;&#32534;&#30721;&#22120;&#65292;&#23558;&#35821;&#38899;&#20449;&#21495;&#36716;&#21270;&#20026;&#31163;&#25955;&#21333;&#20803;&#20197;&#34920;&#31034;&#35821;&#20041;&#20869;&#23481;&#65292;(2) &#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35828;&#35805;&#32773;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#20998;&#31163;&#20986;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#34920;&#31034;&#65292;&#20197;&#21450; (3) &#19968;&#20010;&#24773;&#32490;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#20998;&#31163;&#20986;&#24773;&#32490;&#38901;&#24459;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human speech can be characterized by different components, including semantic content, speaker identity and prosodic information. Significant progress has been made in disentangling representations for semantic content and speaker identity in Automatic Speech Recognition (ASR) and speaker verification tasks respectively. However, it is still an open challenging research question to extract prosodic information because of the intrinsic association of different attributes, such as timbre and rhythm, and because of the need for supervised training schemes to achieve robust large-scale and speaker-independent ASR. The aim of this paper is to address the disentanglement of emotional prosody from speech based on unsupervised reconstruction. Specifically, we identify, design, implement and integrate three crucial components in our proposed speech reconstruction model Prosody2Vec: (1) a unit encoder that transforms speech signals into discrete units for semantic content, (2) a pretrained speak
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25490;&#21015;&#19981;&#21464;&#30697;&#38453;&#32479;&#35745;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#29983;&#25104;&#30340;&#30697;&#38453;&#32479;&#35745;&#65292;&#24182;&#25551;&#36848;&#20102;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#22788;&#29702;&#21516;&#20041;&#35789;&#12289;&#21453;&#20041;&#35789;&#12289;&#19978;&#20041;&#35789;&#21644;&#19979;&#20041;&#35789;&#31561;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2202.06829</link><description>&lt;p&gt;
&#25490;&#21015;&#19981;&#21464;&#30697;&#38453;&#32479;&#35745;&#21644;&#35745;&#31639;&#35821;&#35328;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Permutation invariant matrix statistics and computational language tasks. (arXiv:2202.06829v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25490;&#21015;&#19981;&#21464;&#30697;&#38453;&#32479;&#35745;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#29983;&#25104;&#30340;&#30697;&#38453;&#32479;&#35745;&#65292;&#24182;&#25551;&#36848;&#20102;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#22788;&#29702;&#21516;&#20041;&#35789;&#12289;&#21453;&#20041;&#35789;&#12289;&#19978;&#20041;&#35789;&#21644;&#19979;&#20041;&#35789;&#31561;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;Kartsaklis, Ramgoolam&#21644;Sadrzadeh&#24341;&#20837;&#30340;&#35821;&#35328;&#30697;&#38453;&#29702;&#35770;&#26041;&#26696;&#26159;&#19968;&#31181;&#22788;&#29702;&#31867;&#22411;&#39537;&#21160;&#20998;&#24067;&#35821;&#20041;&#20013;&#29983;&#25104;&#30340;&#30697;&#38453;&#32479;&#35745;&#30340;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#34987;&#35270;&#20026;&#32534;&#30721;&#26174;&#33879;&#32479;&#35745;&#20449;&#24687;&#30340;&#20851;&#38190;&#21487;&#35266;&#27979;&#37327;&#30340;&#25490;&#21015;&#19981;&#21464;&#22810;&#39033;&#24335;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#28304;&#33258;&#32452;&#21512;&#20998;&#24067;&#35821;&#20041;&#30340;&#30697;&#38453;&#20998;&#24067;&#30340;&#36817;&#20284;&#39640;&#26031;&#24615;&#36827;&#34892;&#20102;&#25512;&#24191;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#21033;&#29992;&#25490;&#21015;&#19981;&#21464;&#37327;&#30340;&#22270;&#35770;&#22522;&#30784;&#21644;&#19982;&#21333;&#35789;&#30456;&#20851;&#30340;&#30697;&#38453;&#31995;&#21015;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#35266;&#23519;&#21521;&#37327;&#30340;&#20960;&#20309;&#27010;&#24565;&#26469;&#23450;&#20041;&#21333;&#35789;&#30340;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#23558;&#27492;&#32479;&#19968;&#26694;&#26550;&#25104;&#21151;&#24212;&#29992;&#20110;&#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#19968;&#20123;&#20219;&#21153;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#20219;&#21153;&#28041;&#21450;&#21516;&#20041;&#35789;&#12289;&#21453;&#20041;&#35789;&#12289;&#19978;&#20041;&#35789;&#21644;&#19979;&#20041;&#35789;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Linguistic Matrix Theory programme introduced by Kartsaklis, Ramgoolam and Sadrzadeh is an approach to the statistics of matrices that are generated in type-driven distributional semantics, based on permutation invariant polynomial functions which are regarded as the key observables encoding the significant statistics. In this paper we generalize the previous results on the approximate Gaussianity of matrix distributions arising from compositional distributional semantics. We also introduce a geometry of observable vectors for words, defined by exploiting the graph-theoretic basis for the permutation invariants and the statistical characteristics of the ensemble of matrices associated with the words. We describe successful applications of this unified framework to a number of tasks in computational linguistics, associated with the distinctions between synonyms, antonyms, hypernyms and hyponyms.
&lt;/p&gt;</description></item></channel></rss>