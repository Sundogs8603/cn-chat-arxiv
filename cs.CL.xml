<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>PaperWeaver&#36890;&#36807;&#23558;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#19982;&#25512;&#33616;&#35770;&#25991;&#19978;&#19979;&#25991;&#21270;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#20027;&#39064;&#35770;&#25991;&#25552;&#37266;</title><link>https://arxiv.org/abs/2403.02939</link><description>&lt;p&gt;
PaperWeaver&#65306;&#36890;&#36807;&#23558;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#19982;&#25512;&#33616;&#35770;&#25991;&#19978;&#19979;&#25991;&#21270;&#65292;&#20016;&#23500;&#20027;&#39064;&#35770;&#25991;&#25552;&#37266;
&lt;/p&gt;
&lt;p&gt;
PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02939
&lt;/p&gt;
&lt;p&gt;
PaperWeaver&#36890;&#36807;&#23558;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#19982;&#25512;&#33616;&#35770;&#25991;&#19978;&#19979;&#25991;&#21270;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#20027;&#39064;&#35770;&#25991;&#25552;&#37266;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23398;&#26415;&#26723;&#26696;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#30740;&#31350;&#20154;&#21592;&#35746;&#38405;&#8220;&#35770;&#25991;&#25552;&#37266;&#8221;&#31995;&#32479;&#65292;&#23450;&#26399;&#20026;&#20182;&#20204;&#25512;&#33616;&#26368;&#36817;&#21457;&#34920;&#30340;&#19982;&#20043;&#21069;&#25910;&#38598;&#30340;&#35770;&#25991;&#30456;&#20284;&#30340;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#26377;&#26102;&#24456;&#38590;&#29702;&#35299;&#25512;&#33616;&#35770;&#25991;&#19982;&#20182;&#20204;&#33258;&#24049;&#30740;&#31350;&#32972;&#26223;&#20043;&#38388;&#24494;&#22937;&#30340;&#32852;&#31995;&#65292;&#22240;&#20026;&#29616;&#26377;&#31995;&#32479;&#21482;&#21576;&#29616;&#35770;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#36825;&#20123;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PaperWeaver&#65292;&#36825;&#26159;&#19968;&#20010;&#20016;&#23500;&#30340;&#35770;&#25991;&#25552;&#37266;&#31995;&#32479;&#65292;&#26681;&#25454;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#25552;&#20379;&#25512;&#33616;&#35770;&#25991;&#30340;&#19978;&#19979;&#25991;&#21270;&#25991;&#26412;&#25551;&#36848;&#12290;PaperWeaver&#37319;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20174;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#20013;&#25512;&#26029;&#29992;&#25143;&#30340;&#30740;&#31350;&#20852;&#36259;&#65292;&#25552;&#21462;&#35770;&#25991;&#30340;&#29305;&#23450;&#32972;&#26223;&#65292;&#24182;&#22312;&#36825;&#20123;&#32972;&#26223;&#19978;&#27604;&#36739;&#25512;&#33616;&#35770;&#25991;&#21644;&#25910;&#38598;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#65288;N=15&#65289;&#34920;&#26126;&#65292;&#20351;&#29992;PaperWeaver&#30340;&#21442;&#19982;&#32773;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02939v1 Announce Type: cross  Abstract: With the rapid growth of scholarly archives, researchers subscribe to "paper alert" systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users' research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to
&lt;/p&gt;</description></item><item><title>DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.01954</link><description>&lt;p&gt;
DECIDERS&#65306;&#19968;&#31181;&#36890;&#36807;&#27169;&#20223;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#23454;&#29616;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#30340;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DECIDER: A Rule-Controllable Decoding Strategy for Language Generation by Imitating Dual-System Cognitive Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01954
&lt;/p&gt;
&lt;p&gt;
DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#20856;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#26576;&#20123;&#30446;&#26631;&#27010;&#24565;&#25511;&#21046;&#25152;&#29983;&#25104;&#25991;&#26412;&#30340;&#24847;&#20041;&#25110;&#39118;&#26684;&#12290;&#29616;&#26377;&#26041;&#27861;&#36807;&#20110;&#20851;&#27880;&#36825;&#20123;&#30446;&#26631;&#26412;&#36523;&#65292;&#23548;&#33268;&#32570;&#20047;&#20851;&#20110;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#30340;&#39640;&#23618;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#36890;&#24120;&#36890;&#36807;&#36981;&#24490;&#26576;&#20123;&#35268;&#21017;&#26469;&#22788;&#29702;&#20219;&#21153;&#65292;&#36825;&#20123;&#35268;&#21017;&#19981;&#20165;&#20851;&#27880;&#20110;&#30446;&#26631;&#26412;&#36523;&#65292;&#36824;&#20851;&#27880;&#20110;&#24341;&#21457;&#30446;&#26631;&#21457;&#29983;&#30340;&#35821;&#20041;&#30456;&#20851;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DECIDER&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#32422;&#26463;&#35821;&#35328;&#29983;&#25104;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;DECIDER&#20013;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#37197;&#22791;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#20197;&#39640;&#23618;&#35268;&#21017;&#20316;&#20026;&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;DECIDER&#20801;&#35768;&#35268;&#21017;&#20449;&#21495;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#27969;&#20837;PLM&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DECIDER&#33021;&#22815;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#30340;&#35268;&#21017;&#65292;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#36827;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01954v1 Announce Type: cross  Abstract: Lexicon-based constrained decoding approaches aim to control the meaning or style of the generated text through certain target concepts. Existing approaches over-focus the targets themselves, leading to a lack of high-level reasoning about how to achieve them. However, human usually tackles tasks by following certain rules that not only focuses on the targets but also on semantically relevant concepts that induce the occurrence of targets. In this work, we present DECIDER, a rule-controllable decoding strategy for constrained language generation inspired by dual-system cognitive theory. Specifically, in DECIDER, a pre-trained language model (PLM) is equiped with a logic reasoner that takes high-level rules as input. Then, the DECIDER allows rule signals to flow into the PLM at each decoding step. Extensive experimental results demonstrate that DECIDER can effectively follow given rules to guide generation direction toward the targets i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07818</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#36827;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#33539;&#20363;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#26222;&#21450;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#20445;&#25252;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#36798;&#21040;&#28385;&#24847;&#30340;&#26435;&#34913;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;DP-SGD&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#12290;&#23613;&#31649;&#23558;DP-SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#25512;&#21040;&#20102;&#26497;&#38480;&#65292;&#20294;&#22522;&#20110;DP-SGD&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#24184;&#22320;&#21463;&#21040;&#20102;SGD&#22266;&#26377;&#20302;&#25928;&#29575;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DP&#38646;&#38454;&#26041;&#27861;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#26356;&#39640;&#25928;&#30340;&#38646;&#38454;&#26799;&#24230;&#26469;&#36817;&#20284;&#26799;&#24230;&#65292;&#36991;&#20813;&#20102;SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#12290;&#19982;&#23558;&#38646;&#38454;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21106;&#25509;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20197;&#38750;&#24120;&#25509;&#36817;&#30340;&#26041;&#24335;&#27169;&#25311;DP-SGD&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#28982;&#21518;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFTED&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2402.06512</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multimodal Clinical Trial Outcome Prediction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFTED&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#24180;&#26102;&#38388;&#21644;&#22823;&#37327;&#36130;&#21147;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#26088;&#22312;&#25490;&#38500;&#21487;&#33021;&#22833;&#36133;&#30340;&#33647;&#29289;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#25104;&#26412;&#33410;&#32422;&#28508;&#21147;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#39537;&#21160;&#23581;&#35797;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#65292;&#36825;&#38480;&#21046;&#20102;&#36866;&#24212;&#26032;&#27169;&#24577;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#35782;&#21035;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30456;&#20284;&#20449;&#24687;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#65288;LIFTED&#65289;&#26041;&#27861;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LIFTED&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;LIFTED&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#65292;&#20174;&#27169;&#24577;&#29305;&#23450;&#30340;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The clinical trial is a pivotal and costly process, often spanning multiple years and requiring substantial financial resources. Therefore, the development of clinical trial outcome prediction models aims to exclude drugs likely to fail and holds the potential for significant cost savings. Recent data-driven attempts leverage deep learning methods to integrate multimodal data for predicting clinical trial outcomes. However, these approaches rely on manually designed modal-specific encoders, which limits both the extensibility to adapt new modalities and the ability to discern similar information patterns across different modalities. To address these issues, we propose a multimodal mixture-of-experts (LIFTED) approach for clinical trial outcome prediction. Specifically, LIFTED unifies different modality data by transforming them into natural language descriptions. Then, LIFTED constructs unified noise-resilient encoders to extract information from modal-specific language descriptions. S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FAQ-Gen&#65292;&#19968;&#20010;&#21033;&#29992;&#25991;&#26412;&#36716;&#25991;&#26412;&#36716;&#25442;&#27169;&#22411;&#24320;&#21457;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#29983;&#25104;&#29305;&#23450;&#39046;&#22495;&#30340;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#31574;&#21010;&#30340;&#31639;&#27861;&#26469;&#20248;&#21270;&#20449;&#24687;&#34920;&#31034;&#21644;&#38382;&#39064;-&#31572;&#26696;&#25490;&#21517;&#65292;&#25552;&#39640;&#20102;FAQ&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#23450;&#24615;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#29983;&#25104;&#30340;FAQs&#26500;&#36896;&#33391;&#22909;&#19988;&#21487;&#25512;&#24191;&#33267;&#19981;&#21516;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.05812</link><description>&lt;p&gt;
FAQ-Gen&#65306;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#29305;&#23450;&#39046;&#22495;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#36741;&#21161;&#29702;&#35299;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
FAQ-Gen: An automated system to generate domain-specific FAQs to aid content comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FAQ-Gen&#65292;&#19968;&#20010;&#21033;&#29992;&#25991;&#26412;&#36716;&#25991;&#26412;&#36716;&#25442;&#27169;&#22411;&#24320;&#21457;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#29983;&#25104;&#29305;&#23450;&#39046;&#22495;&#30340;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#31574;&#21010;&#30340;&#31639;&#27861;&#26469;&#20248;&#21270;&#20449;&#24687;&#34920;&#31034;&#21644;&#38382;&#39064;-&#31572;&#26696;&#25490;&#21517;&#65292;&#25552;&#39640;&#20102;FAQ&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#23450;&#24615;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#29983;&#25104;&#30340;FAQs&#26500;&#36896;&#33391;&#22909;&#19988;&#21487;&#25512;&#24191;&#33267;&#19981;&#21516;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#65288;FAQ&#65289;&#26159;&#25351;&#20851;&#20110;&#29305;&#23450;&#20869;&#23481;&#30340;&#26368;&#24120;&#35265;&#35810;&#38382;&#12290;&#23427;&#20204;&#36890;&#36807;&#31616;&#21270;&#20027;&#39064;&#21644;&#36890;&#36807;&#31616;&#26126;&#25212;&#35201;&#22320;&#21576;&#29616;&#20449;&#24687;&#26469;&#22686;&#24378;&#29702;&#35299;&#65292;&#20316;&#20026;&#20869;&#23481;&#29702;&#35299;&#30340;&#36741;&#21161;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#21033;&#29992;&#25991;&#26412;&#36716;&#25991;&#26412;&#36716;&#25442;&#27169;&#22411;&#26469;&#35299;&#20915;FAQ&#29983;&#25104;&#20316;&#20026;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#28085;&#30422;&#20256;&#32479;&#38382;&#31572;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24378;&#35843;&#23427;&#20204;&#22312;&#30452;&#25509;&#24212;&#29992;&#20110;FAQ&#29983;&#25104;&#20219;&#21153;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#39046;&#22495;&#30340;&#25991;&#26412;&#20869;&#23481;&#26500;&#24314;FAQs&#65292;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#25105;&#31574;&#21010;&#30340;&#31639;&#27861;&#26469;&#33719;&#24471;&#25552;&#20379;&#32473;&#36755;&#20837;&#30340;&#20449;&#24687;&#30340;&#26368;&#20339;&#34920;&#31034;&#65292;&#24182;&#23545;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#25490;&#24207;&#20197;&#26368;&#22823;&#21270;&#20154;&#31867;&#29702;&#35299;&#12290;&#23450;&#24615;&#20154;&#31867;&#35780;&#20272;&#26174;&#31034;&#29983;&#25104;&#30340;FAQs&#26500;&#36896;&#33391;&#22909;&#19988;&#21487;&#25512;&#24191;&#33267;&#19981;&#21516;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Frequently Asked Questions (FAQs) refer to the most common inquiries about specific content. They serve as content comprehension aids by simplifying topics and enhancing understanding through succinct presentation of information. In this paper, we address FAQ generation as a well-defined Natural Language Processing (NLP) task through the development of an end-to-end system leveraging text-to-text transformation models. We present a literature review covering traditional question-answering systems, highlighting their limitations when applied directly to the FAQ generation task. We propose our system capable of building FAQs from textual content tailored to specific domains, enhancing their accuracy and relevance. We utilise self-curated algorithms for obtaining optimal representation of information to be provided as input and also for ranking the question-answer pairs to maximise human comprehension. Qualitative human evaluation showcases the generated FAQs to be well-constructed and re
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31435;&#38656;&#35201;&#26356;&#22909;&#22320;&#25972;&#21512;&#20154;&#31867;&#32972;&#26223;&#65292;&#24182;&#38754;&#20020;&#30528;&#22914;&#20309;&#25429;&#25417;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#20197;&#21450;&#22914;&#20309;&#24314;&#27169;&#30340;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.07751</link><description>&lt;p&gt;
&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#65306;&#38656;&#27714;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Large Human Language Models: A Need and the Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07751
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31435;&#38656;&#35201;&#26356;&#22909;&#22320;&#25972;&#21512;&#20154;&#31867;&#32972;&#26223;&#65292;&#24182;&#38754;&#20020;&#30528;&#22914;&#20309;&#25429;&#25417;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#20197;&#21450;&#22914;&#20309;&#24314;&#27169;&#30340;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#31867;&#20013;&#24515;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#36827;&#23637;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#23558;&#20154;&#31867;&#21644;&#31038;&#20250;&#22240;&#32032;&#32435;&#20837;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#24050;&#32463;&#20005;&#37325;&#20381;&#36182;&#20110;LLM&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#24182;&#27809;&#26377;&#23545;&#20316;&#32773;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#30495;&#27491;&#29702;&#35299;&#20154;&#31867;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#26356;&#22909;&#22320;&#23558;&#20154;&#31867;&#32972;&#26223;&#25972;&#21512;&#21040;LLM&#20013;&#12290;&#36825;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#35774;&#35745;&#32771;&#34385;&#21644;&#25361;&#25112;&#65292;&#28041;&#21450;&#21040;&#35201;&#25429;&#25417;&#21738;&#20123;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#23427;&#20204;&#20197;&#21450;&#35201;&#37319;&#29992;&#20309;&#31181;&#24314;&#27169;&#31574;&#30053;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20513;&#20174;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#31185;&#23398;&#30340;&#27010;&#24565;&#20986;&#21457;&#65292;&#25903;&#25345;&#19977;&#20010;&#31435;&#22330;&#26469;&#21019;&#24314;&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#65288;LHLMs&#65289;&#65306;&#39318;&#20808;&#65292;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#24212;&#21253;&#25324;&#20154;&#31867;&#32972;&#26223;&#12290;&#20854;&#27425;&#65292;LHLMs&#24212;&#35813;&#24847;&#35782;&#21040;&#20154;&#19981;&#20165;&#20165;&#26159;&#20182;&#20204;&#25152;&#23646;&#30340;&#32676;&#20307;&#12290;&#31532;&#19977;&#65292;LHLMs&#24212;&#35813;&#33021;&#22815;&#32771;&#34385;&#21040;&#20154;&#31867;&#32972;&#26223;&#30340;&#21160;&#24577;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07751v2 Announce Type: replace-cross  Abstract: As research in human-centered NLP advances, there is a growing recognition of the importance of incorporating human and social factors into NLP models. At the same time, our NLP systems have become heavily reliant on LLMs, most of which do not model authors. To build NLP systems that can truly understand human language, we must better integrate human contexts into LLMs. This brings to the fore a range of design considerations and challenges in terms of what human aspects to capture, how to represent them, and what modeling strategies to pursue. To address these, we advocate for three positions toward creating large human language models (LHLMs) using concepts from psychological and behavioral sciences: First, LM training should include the human context. Second, LHLMs should recognize that people are more than their group(s). Third, LHLMs should be able to account for the dynamic and temporally-dependent nature of the human con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SlimPajama&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#19981;&#21516;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20840;&#23616;&#21435;&#37325;&#21644;&#23616;&#37096;&#21435;&#37325;&#30340;&#27604;&#36739;&#21644;&#39640;&#36136;&#37327;&#22810;&#28304;&#25968;&#25454;&#38598;&#30340;&#27604;&#20363;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.10818</link><description>&lt;p&gt;
SlimPajama-DC: &#29702;&#35299;LLM&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
SlimPajama-DC: Understanding Data Combinations for LLM Training. (arXiv:2309.10818v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SlimPajama&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#19981;&#21516;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20840;&#23616;&#21435;&#37325;&#21644;&#23616;&#37096;&#21435;&#37325;&#30340;&#27604;&#36739;&#21644;&#39640;&#36136;&#37327;&#22810;&#28304;&#25968;&#25454;&#38598;&#30340;&#27604;&#20363;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20102;&#35299;&#20351;&#29992;SlimPajama&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#21508;&#31181;&#25968;&#25454;&#32452;&#21512;&#65288;&#22914;&#32593;&#32476;&#25991;&#26412;&#12289;&#32500;&#22522;&#30334;&#31185;&#12289;GitHub&#12289;&#22270;&#20070;&#65289;&#23545;&#20854;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;SlimPajama&#26159;&#19968;&#20010;&#32463;&#36807;&#20005;&#26684;&#21435;&#37325;&#30340;&#22810;&#28304;&#25968;&#25454;&#38598;&#65292;&#20174;Together&#36129;&#29486;&#30340;1.2T&#20010;token&#30340;RedPajama&#25968;&#25454;&#38598;&#20013;&#31934;&#32454;&#32452;&#21512;&#21644;&#21435;&#37325;&#65292;&#24635;&#20849;&#24471;&#21040;&#20102;627B&#20010;tokens&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#30740;&#31350;&#31216;&#20026;SlimPajama-DC&#65292;&#36825;&#26159;&#19968;&#39033;&#26088;&#22312;&#25581;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#20351;&#29992;SlimPajama&#25152;&#28041;&#21450;&#30340;&#22522;&#26412;&#29305;&#24449;&#21644;&#26368;&#20339;&#23454;&#36341;&#30340;&#32463;&#39564;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#20351;&#29992;SlimPajama&#36827;&#34892;&#30740;&#31350;&#30340;&#36807;&#31243;&#20013;&#65292;&#20986;&#29616;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#65306;&#65288;1&#65289;&#20840;&#23616;&#21435;&#37325; vs. &#23616;&#37096;&#21435;&#37325;&#12290;&#25105;&#20204;&#20998;&#26512;&#21644;&#35752;&#35770;&#20102;&#20840;&#23616;&#21435;&#37325;&#65288;&#36328;&#19981;&#21516;&#25968;&#25454;&#38598;&#28304;&#65289;&#21644;&#23616;&#37096;&#21435;&#37325;&#65288;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#28304;&#20869;&#37096;&#65289;&#23545;&#35757;&#32451;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#65288;2&#65289;&#39640;&#36136;&#37327;/&#39640;&#24230;&#21435;&#37325;&#30340;&#22810;&#28304;&#25968;&#25454;&#38598;&#22312;&#32452;&#21512;&#20013;&#30340;&#27604;&#20363;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper aims to understand the impacts of various data combinations (e.g., web text, wikipedia, github, books) on the training of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T tokens RedPajama dataset contributed by Together. We've termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of high-quality/highly-deduplicated multi-source datasets in the combination. To study this, we cons
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.00264</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00264
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#22810;&#20010;&#27169;&#24577;&#36873;&#25321;&#21644;&#34701;&#21512;&#29305;&#24449;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#32452;&#21512;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20197;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#24182;&#19988;&#30740;&#31350;&#20102;&#22810;&#25439;&#22833;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#30830;&#23450;&#20102;&#19982;&#23376;&#32593;&#24615;&#33021;&#30456;&#20851;&#30340;&#26377;&#29992;&#21457;&#29616;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;CMU-MOSI&#12289;CMU-MOSEI&#21644;CH-SIMS&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#22810;&#27169;&#24577;&#29305;&#24449;&#21487;&#20197;&#25913;&#21892;&#21333;&#27169;&#24577;&#27979;&#35797;&#65292;&#24182;&#19988;&#22522;&#20110;&#25968;&#25454;&#38598;&#27880;&#37322;&#27169;&#24335;&#35774;&#35745;&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#24773;&#24863;&#26816;&#27979;&#30340;&#20248;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06945</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06945
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65288;ICAE&#65289;&#12290; ICAE&#26377;&#20004;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#37319;&#29992;LoRA&#26041;&#24335;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#20197;&#21450;&#19968;&#20010;&#22266;&#23450;&#30340;&#35299;&#30721;&#22120;&#65292;&#20316;&#20026;&#30446;&#26631;LLM&#65292;&#21487;&#20197;&#26681;&#25454;&#20869;&#23384;&#27133;&#26469;&#36827;&#34892;&#21508;&#31181;&#30446;&#30340;&#30340;&#26465;&#20214;&#22788;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#33258;&#32534;&#30721;&#21644;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;ICAE&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21644;&#20840;&#38754;&#34920;&#31034;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#20869;&#23384;&#27133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#25351;&#23548;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#30340;ICAE&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#19982;&#21508;&#31181;&#25552;&#31034;&#30340;&#20132;&#20114;&#65292;&#20174;&#32780;&#20135;&#29983;&#29702;&#24819;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#23398;&#20064;&#30340;ICAE&#21487;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;$4\times$&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#20869;&#23384;&#27133;&#65292;&#30446;&#26631;LLM&#21487;&#20197;&#24456;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
&lt;/p&gt;</description></item></channel></rss>