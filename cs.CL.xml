<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>CFGPT&#26159;&#19968;&#20010;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#22269;&#37329;&#34701;&#21161;&#25163;&#65292;&#21253;&#25324;CFData&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#21450;CFLLM&#29992;&#20110;&#22788;&#29702;&#37329;&#34701;&#25991;&#26412;&#65292;CFAPP&#29992;&#20110;&#23454;&#38469;&#37329;&#34701;&#24212;&#29992;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#21508;&#20010;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10654</link><description>&lt;p&gt;
CFGPT: &#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#22269;&#37329;&#34701;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
CFGPT: Chinese Financial Assistant with Large Language Model. (arXiv:2309.10654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10654
&lt;/p&gt;
&lt;p&gt;
CFGPT&#26159;&#19968;&#20010;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#22269;&#37329;&#34701;&#21161;&#25163;&#65292;&#21253;&#25324;CFData&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#21450;CFLLM&#29992;&#20110;&#22788;&#29702;&#37329;&#34701;&#25991;&#26412;&#65292;CFAPP&#29992;&#20110;&#23454;&#38469;&#37329;&#34701;&#24212;&#29992;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#21508;&#20010;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CFGPT&#30340;&#20013;&#22269;&#37329;&#34701;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#38598;&#65288;CFData&#65289;&#65292;&#29992;&#20110;&#29087;&#32451;&#22788;&#29702;&#37329;&#34701;&#25991;&#26412;&#30340;&#37329;&#34701;LLM&#65288;CFLLM&#65289;&#65292;&#20197;&#21450;&#29992;&#20110;&#23454;&#38469;&#37329;&#34701;&#24212;&#29992;&#30340;&#37096;&#32626;&#26694;&#26550;&#65288;CFAPP&#65289;&#12290;CFData&#21253;&#25324;&#19968;&#20010;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#27719;&#38598;&#20102;&#20013;&#22269;&#37329;&#34701;&#25968;&#25454;&#21644;&#20998;&#26512;&#65292;&#20197;&#21450;&#24635;&#20849;584M&#20010;&#25991;&#20214;&#21644;141B&#20010;&#26631;&#35760;&#30340;&#36739;&#23567;&#30340;&#36890;&#29992;&#25991;&#26412;&#23376;&#38598;&#65292;&#24182;&#19988;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;&#38024;&#23545;&#20845;&#20010;&#19981;&#21516;&#30340;&#37329;&#34701;&#20219;&#21153;&#36827;&#34892;&#20102;&#23450;&#21046;&#65292;&#20869;&#23481;&#28085;&#30422;&#20102;&#37329;&#34701;&#20998;&#26512;&#21644;&#20915;&#31574;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;1.5M&#20010;&#25351;&#20196;&#23545;&#21644;&#24635;&#35745;1.5B&#20010;&#26631;&#35760;&#12290;CFLLM&#22522;&#20110;InternLM-7B&#36827;&#34892;&#20102;&#24179;&#34913;&#27169;&#22411;&#33021;&#21147;&#30340;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated great potential in natural language processing tasks within the financial domain. In this work, we present a Chinese Financial Generative Pre-trained Transformer framework, named CFGPT, which includes a dataset~(CFData) for pre-training and supervised fine-tuning, a financial LLM~(CFLLM) to adeptly manage financial texts, and a deployment framework~(CFAPP) designed to navigate real-world financial applications. The CFData comprising both a pre-training dataset and a supervised fine-tuning dataset, where the pre-training dataset collates Chinese financial data and analytics, alongside a smaller subset of general-purpose text with 584M documents and 141B tokens in total, and the supervised fine-tuning dataset is tailored for six distinct financial tasks, embodying various facets of financial analysis and decision-making with 1.5M instruction pairs and 1.5B tokens in total. The CFLLM, which is based on InternLM-7B to balance the model capabil
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.10621</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10621
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24615;&#26631;&#31614;&#26159;&#35780;&#20272;&#21644;&#20248;&#21270;&#25628;&#32034;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#33719;&#21462;&#22823;&#37327;&#30456;&#20851;&#24615;&#26631;&#31614;&#36890;&#24120;&#38656;&#35201;&#31532;&#19977;&#26041;&#26631;&#27880;&#20154;&#21592;&#65292;&#20294;&#23384;&#22312;&#20302;&#36136;&#37327;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#26631;&#31614;&#36136;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#24471;&#20180;&#32454;&#21453;&#39304;&#26469;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#26500;&#24314;&#22270;&#24418;&#34920;&#31034;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#25991;&#26412;&#35821;&#20041;&#21644;&#21307;&#23398;&#30693;&#35782;&#30340;&#29702;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10608</link><description>&lt;p&gt;
&#25913;&#36827;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#30340;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Dialogue Generation with Abstract Meaning Representations. (arXiv:2309.10608v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10608
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#26500;&#24314;&#22270;&#24418;&#34920;&#31034;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#25991;&#26412;&#35821;&#20041;&#21644;&#21307;&#23398;&#30693;&#35782;&#30340;&#29702;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#22312;&#36828;&#31243;&#21307;&#30103;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23427;&#26377;&#21161;&#20110;&#23558;&#21307;&#23398;&#19987;&#19994;&#30693;&#35782;&#20256;&#36798;&#32473;&#24739;&#32773;&#12290;&#29616;&#26377;&#30740;&#31350;&#20391;&#37325;&#20110;&#24341;&#20837;&#25991;&#26412;&#34920;&#31034;&#65292;&#20294;&#36825;&#38480;&#21046;&#20102;&#20182;&#20204;&#23545;&#25991;&#26412;&#35821;&#20041;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20363;&#22914;&#24573;&#35270;&#20102;&#37325;&#35201;&#30340;&#21307;&#23398;&#23454;&#20307;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#23545;&#25991;&#26412;&#35821;&#20041;&#21644;&#21307;&#23398;&#30693;&#35782;&#65288;&#21253;&#25324;&#23454;&#20307;&#21644;&#20851;&#31995;&#65289;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20351;&#29992;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#26500;&#24314;&#22270;&#24418;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#34920;&#31034;&#26174;&#31034;&#20102;&#23545;&#35805;&#20013;&#35821;&#35328;&#25104;&#20998;&#21644;&#21307;&#23398;&#23454;&#20307;&#30340;&#35282;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;AMR&#22270;&#23545;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#20043;&#38388;&#30340;&#23545;&#35805;&#36827;&#34892;&#24314;&#27169;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#24418;&#30693;&#35782;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Dialogue Generation serves a critical role in telemedicine by facilitating the dissemination of medical expertise to patients. Existing studies focus on incorporating textual representations, which have limited their ability to represent the semantics of text, such as ignoring important medical entities. To enhance the model's understanding of the textual semantics and the medical knowledge including entities and relations, we introduce the use of Abstract Meaning Representations (AMR) to construct graphical representations that delineate the roles of language constituents and medical entities within the dialogues. In this paper, We propose a novel framework that models dialogues between patients and healthcare professionals using AMR graphs, where the neural networks incorporate textual and graphical knowledge with a dual attention mechanism. Experimental results show that our framework outperforms strong baseline models in medical dialogue generation, demonstrating the effect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#30740;&#31350;&#27861;&#35821;&#20013;&#30340;&#24341;&#36848;&#25552;&#21462;&#21644;&#26469;&#28304;&#24402;&#23646;&#12290;&#35821;&#26009;&#24211;&#21253;&#21547;1676&#31687;&#25163;&#21160;&#27880;&#37322;&#30340;&#26032;&#38395;&#25991;&#26412;&#65292;&#20854;&#20013;&#21253;&#25324;&#30452;&#25509;&#24341;&#36848;&#12289;&#38388;&#25509;&#24341;&#36848;&#21644;&#28151;&#21512;&#24341;&#36848;&#12290;&#36890;&#36807;8&#20301;&#26631;&#27880;&#32773;&#30340;&#19968;&#33268;&#24615;&#39564;&#35777;&#65292;&#35813;&#35821;&#26009;&#24211;&#23545;&#20110;&#36825;&#20010;&#22256;&#38590;&#30340;&#35821;&#35328;&#29616;&#35937;&#26469;&#35828;&#20855;&#26377;&#39640;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10604</link><description>&lt;p&gt;
FRACAS: &#19968;&#20221;&#29992;&#20110;&#26032;&#38395;&#20013;&#24341;&#36848;&#20851;&#31995;&#30340;&#27861;&#35821;&#27880;&#37322;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
FRACAS: A FRench Annotated Corpus of Attribution relations in newS. (arXiv:2309.10604v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#30740;&#31350;&#27861;&#35821;&#20013;&#30340;&#24341;&#36848;&#25552;&#21462;&#21644;&#26469;&#28304;&#24402;&#23646;&#12290;&#35821;&#26009;&#24211;&#21253;&#21547;1676&#31687;&#25163;&#21160;&#27880;&#37322;&#30340;&#26032;&#38395;&#25991;&#26412;&#65292;&#20854;&#20013;&#21253;&#25324;&#30452;&#25509;&#24341;&#36848;&#12289;&#38388;&#25509;&#24341;&#36848;&#21644;&#28151;&#21512;&#24341;&#36848;&#12290;&#36890;&#36807;8&#20301;&#26631;&#27880;&#32773;&#30340;&#19968;&#33268;&#24615;&#39564;&#35777;&#65292;&#35813;&#35821;&#26009;&#24211;&#23545;&#20110;&#36825;&#20010;&#22256;&#38590;&#30340;&#35821;&#35328;&#29616;&#35937;&#26469;&#35828;&#20855;&#26377;&#39640;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#36848;&#25552;&#21462;&#26159;&#19968;&#39033;&#22312;&#31038;&#20250;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#37117;&#38750;&#24120;&#26377;&#29992;&#30340;&#20219;&#21153;&#65292;&#20294;&#26159;&#38500;&#20102;&#33521;&#35821;&#20043;&#22806;&#65292;&#24456;&#23569;&#26377;&#25968;&#25454;&#21487;&#29992;&#20110;&#30740;&#31350;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#36825;&#20010;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20221;&#25163;&#21160;&#27880;&#37322;&#30340;&#27861;&#35821;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;1676&#31687;&#26032;&#38395;&#25991;&#26412;&#65292;&#29992;&#20110;&#24341;&#36848;&#25552;&#21462;&#21644;&#26469;&#28304;&#24402;&#23646;&#12290;&#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;&#25105;&#20204;&#35821;&#26009;&#24211;&#30340;&#32452;&#25104;&#20197;&#21450;&#22312;&#36873;&#25321;&#25968;&#25454;&#26102;&#20570;&#20986;&#30340;&#36873;&#25321;&#12290;&#28982;&#21518;&#35814;&#32454;&#20171;&#32461;&#20102;&#26631;&#27880;&#25351;&#21335;&#21644;&#26631;&#27880;&#36807;&#31243;&#65292;&#20197;&#21450;&#20851;&#20110;&#26368;&#32456;&#35821;&#26009;&#24211;&#21644;&#24341;&#36848;&#31867;&#22411;&#65288;&#30452;&#25509;&#24341;&#36848;&#12289;&#38388;&#25509;&#24341;&#36848;&#21644;&#28151;&#21512;&#24341;&#36848;&#65289;&#20043;&#38388;&#30340;&#19968;&#20123;&#32479;&#35745;&#25968;&#25454;&#65292;&#36825;&#20123;&#31867;&#22411;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;8&#20301;&#21442;&#19982;&#25163;&#21160;&#26631;&#27880;&#30340;&#26631;&#27880;&#32773;&#20043;&#38388;&#30340;&#26631;&#27880;&#32773;&#19968;&#33268;&#24615;&#65292;&#23545;&#20110;&#36825;&#26679;&#19968;&#20010;&#22256;&#38590;&#30340;&#35821;&#35328;&#29616;&#35937;&#26469;&#35828;&#65292;&#19968;&#33268;&#24615;&#30456;&#24403;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quotation extraction is a widely useful task both from a sociological and from a Natural Language Processing perspective. However, very little data is available to study this task in languages other than English. In this paper, we present a manually annotated corpus of 1676 newswire texts in French for quotation extraction and source attribution. We first describe the composition of our corpus and the choices that were made in selecting the data. We then detail the annotation guidelines and annotation process, as well as a few statistics about the final corpus and the obtained balance between quote types (direct, indirect and mixed, which are particularly challenging). We end by detailing our inter-annotator agreement between the 8 annotators who worked on manual labelling, which is substantially high for such a difficult linguistic phenomenon.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#21644;&#26426;&#22120;&#32763;&#35793;&#22120;&#23545;&#30693;&#35782;&#22270;&#35889;&#25991;&#26412;&#36827;&#34892;&#32534;&#30721;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#23545;&#40784;&#31574;&#30053;&#65292;&#29983;&#25104;&#20102;&#26377;&#25490;&#21517;&#30340;&#21305;&#37197;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#20248;&#21270;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.10598</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#28145;&#24230;&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Deep Cross-Language Entity Alignment. (arXiv:2309.10598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#21644;&#26426;&#22120;&#32763;&#35793;&#22120;&#23545;&#30693;&#35782;&#22270;&#35889;&#25991;&#26412;&#36827;&#34892;&#32534;&#30721;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#23545;&#40784;&#31574;&#30053;&#65292;&#29983;&#25104;&#20102;&#26377;&#25490;&#21517;&#30340;&#21305;&#37197;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#20248;&#21270;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;&#26159;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#25214;&#21040;&#30456;&#21516;&#35821;&#20041;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#32467;&#21512;&#26426;&#22120;&#32763;&#35793;&#22120;&#26469;&#32534;&#30721;&#30693;&#35782;&#22270;&#35889;&#25991;&#26412;&#65292;&#20943;&#23569;&#23545;&#26631;&#31614;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#19982;&#20165;&#24378;&#35843;&#20840;&#23616;&#25110;&#23616;&#37096;&#23545;&#40784;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#32771;&#34385;&#20102;&#20004;&#31181;&#23545;&#40784;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#23545;&#40784;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#20108;&#20998;&#21305;&#37197;&#38382;&#39064;&#65292;&#28982;&#21518;&#37319;&#29992;&#37325;&#26032;&#20132;&#25442;&#30340;&#24605;&#24819;&#26469;&#23436;&#25104;&#23545;&#40784;&#12290;&#19982;&#20165;&#32473;&#20986;&#19968;&#20010;&#26368;&#20248;&#35299;&#30340;&#20256;&#32479;&#20108;&#20998;&#21305;&#37197;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#29983;&#25104;&#20102;&#26377;&#25490;&#21517;&#30340;&#21305;&#37197;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;&#35768;&#22810;&#28508;&#22312;&#30340;&#19979;&#28216;&#20219;&#21153;&#25104;&#20026;&#21487;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20108;&#20998;&#21305;&#37197;&#36807;&#31243;&#20013;&#36866;&#24212;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20248;&#21270;&#65288;&#26368;&#23567;&#21644;&#26368;&#22823;&#65289;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22810;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual entity alignment is the task of finding the same semantic entities from different language knowledge graphs. In this paper, we propose a simple and novel unsupervised method for cross-language entity alignment. We utilize the deep learning multi-language encoder combined with a machine translator to encode knowledge graph text, which reduces the reliance on label data. Unlike traditional methods that only emphasize global or local alignment, our method simultaneously considers both alignment strategies. We first view the alignment task as a bipartite matching problem and then adopt the re-exchanging idea to accomplish alignment. Compared with the traditional bipartite matching algorithm that only gives one optimal solution, our algorithm generates ranked matching results which enabled many potentials downstream tasks. Additionally, our method can adapt two different types of optimization (minimal and maximal) in the bipartite matching process, which provides more flexibil
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21475;&#35821;&#35782;&#21035;&#26041;&#27861;MuSeLI&#65292;&#21033;&#29992;&#35270;&#39057;&#26631;&#39064;&#12289;&#25551;&#36848;&#21644;&#22320;&#29702;&#20301;&#32622;&#31561;&#20803;&#25968;&#25454;&#26469;&#22686;&#24378;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#22312;&#20004;&#20010;YouTube&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10567</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#24314;&#27169;&#29992;&#20110;&#21475;&#35821;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multimodal Modeling For Spoken Language Identification. (arXiv:2309.10567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10567
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21475;&#35821;&#35782;&#21035;&#26041;&#27861;MuSeLI&#65292;&#21033;&#29992;&#35270;&#39057;&#26631;&#39064;&#12289;&#25551;&#36848;&#21644;&#22320;&#29702;&#20301;&#32622;&#31561;&#20803;&#25968;&#25454;&#26469;&#22686;&#24378;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#22312;&#20004;&#20010;YouTube&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#35782;&#21035;&#26159;&#25351;&#22312;&#32473;&#23450;&#30340;&#35805;&#35821;&#20013;&#33258;&#21160;&#39044;&#27979;&#21475;&#35821;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#19978;&#65292;&#23427;&#34987;&#24314;&#27169;&#20026;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#12290;&#20197;&#24448;&#30340;&#25216;&#26415;&#37117;&#23616;&#38480;&#20110;&#21333;&#19968;&#27169;&#24577;&#65307;&#28982;&#32780;&#65292;&#22312;&#35270;&#39057;&#25968;&#25454;&#20013;&#65292;&#23384;&#22312;&#35768;&#22810;&#20854;&#20182;&#20803;&#25968;&#25454;&#65292;&#36825;&#20123;&#20803;&#25968;&#25454;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#20250;&#26377;&#30410;&#22788;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MuSeLI&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#21475;&#35821;&#35782;&#21035;&#26041;&#27861;&#65292;&#23427;&#28145;&#20837;&#30740;&#31350;&#20102;&#20351;&#29992;&#21508;&#31181;&#20803;&#25968;&#25454;&#28304;&#26469;&#22686;&#24378;&#35821;&#35328;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#35832;&#22914;&#35270;&#39057;&#26631;&#39064;&#12289;&#25551;&#36848;&#21644;&#22320;&#29702;&#20301;&#32622;&#31561;&#20803;&#25968;&#25454;&#25552;&#20379;&#20102;&#22823;&#37327;&#20449;&#24687;&#65292;&#33021;&#22815;&#35782;&#21035;&#22810;&#23186;&#20307;&#24405;&#21046;&#30340;&#21475;&#35821;&#12290;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;YouTube&#35270;&#39057;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#22312;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#39033;&#28040;&#34701;&#30740;&#31350;&#65292;&#25551;&#36848;&#20102;&#27599;&#31181;&#27169;&#24577;&#23545;&#35821;&#35328;&#35782;&#21035;&#30340;&#29420;&#29305;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken language identification refers to the task of automatically predicting the spoken language in a given utterance. Conventionally, it is modeled as a speech-based language identification task. Prior techniques have been constrained to a single modality; however in the case of video data there is a wealth of other metadata that may be beneficial for this task. In this work, we propose MuSeLI, a Multimodal Spoken Language Identification method, which delves into the use of various metadata sources to enhance language identification. Our study reveals that metadata such as video title, description and geographic location provide substantial information to identify the spoken language of the multimedia recording. We conduct experiments using two diverse public datasets of YouTube videos, and obtain state-of-the-art results on the language identification task. We additionally conduct an ablation study that describes the distinct contribution of each modality for language recognition.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#24863;&#30693;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#36890;&#36807;&#32771;&#34385;&#39044;&#35757;&#32451;&#30340;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#21333;&#35789;&#30340;&#37051;&#22495;&#26469;&#30830;&#23450;&#25152;&#38656;&#30340;&#26368;&#23567;&#22122;&#22768;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26426;&#21046;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#26426;&#21046;&#65292;&#21516;&#26102;&#20445;&#35777;&#26356;&#39640;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.10551</link><description>&lt;p&gt;
&#29992;&#20110;&#38745;&#24577;&#35789;&#23884;&#20837;&#30340;&#37051;&#22495;&#24863;&#30693;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Neighbourhood-Aware Differential Privacy Mechanism for Static Word Embeddings. (arXiv:2309.10551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#24863;&#30693;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#36890;&#36807;&#32771;&#34385;&#39044;&#35757;&#32451;&#30340;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#21333;&#35789;&#30340;&#37051;&#22495;&#26469;&#30830;&#23450;&#25152;&#38656;&#30340;&#26368;&#23567;&#22122;&#22768;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26426;&#21046;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#26426;&#21046;&#65292;&#21516;&#26102;&#20445;&#35777;&#26356;&#39640;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#39044;&#35757;&#32451;&#30340;&#38745;&#24577;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#21333;&#35789;&#37051;&#22495;&#30340;&#37051;&#22495;&#24863;&#30693;&#24046;&#20998;&#38544;&#31169;&#65288;NADP&#65289;&#26426;&#21046;&#65292;&#20197;&#30830;&#23450;&#20445;&#35777;&#25351;&#23450;&#38544;&#31169;&#32423;&#21035;&#25152;&#38656;&#30340;&#26368;&#23567;&#22122;&#22768;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#23427;&#20204;&#30340;&#23884;&#20837;&#26500;&#24314;&#21333;&#35789;&#30340;&#26368;&#36817;&#37051;&#22270;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#19968;&#32452;&#36830;&#36890;&#20998;&#37327;&#65288;&#21363;&#37051;&#22495;&#65289;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#37051;&#22495;&#20013;&#26681;&#25454;&#35813;&#37051;&#22495;&#20013;&#30340;&#21333;&#35789;&#38598;&#21512;&#20998;&#21035;&#23545;&#21333;&#35789;&#24212;&#29992;&#19981;&#21516;&#27700;&#24179;&#30340;&#39640;&#26031;&#22122;&#22768;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;NADP&#26426;&#21046;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#22810;&#20010;&#20808;&#21069;&#25552;&#20986;&#30340;DP&#26426;&#21046;&#65292;&#22914;&#25289;&#26222;&#25289;&#26031;&#12289;&#39640;&#26031;&#21644;&#39532;&#27663;&#36317;&#31163;&#65292;&#21516;&#26102;&#20445;&#35777;&#26356;&#39640;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Neighbourhood-Aware Differential Privacy (NADP) mechanism considering the neighbourhood of a word in a pretrained static word embedding space to determine the minimal amount of noise required to guarantee a specified privacy level. We first construct a nearest neighbour graph over the words using their embeddings, and factorise it into a set of connected components (i.e. neighbourhoods). We then separately apply different levels of Gaussian noise to the words in each neighbourhood, determined by the set of words in that neighbourhood. Experiments show that our proposed NADP mechanism consistently outperforms multiple previously proposed DP mechanisms such as Laplacian, Gaussian, and Mahalanobis in multiple downstream tasks, while guaranteeing higher levels of privacy.
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#21560;&#21462;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#25915;&#20987;&#65292;&#33021;&#22815;&#23558;&#30446;&#26631;&#27169;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#25552;&#21462;&#21040;&#19968;&#20010;&#21442;&#25968;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10544</link><description>&lt;p&gt;
&#27169;&#22411;&#21560;&#21462;: &#38024;&#23545;LLMs&#30340;&#19968;&#31181;&#25552;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Model Leeching: An Extraction Attack Targeting LLMs. (arXiv:2309.10544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10544
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21560;&#21462;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#25915;&#20987;&#65292;&#33021;&#22815;&#23558;&#30446;&#26631;&#27169;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#25552;&#21462;&#21040;&#19968;&#20010;&#21442;&#25968;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21560;&#21462;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26032;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#33021;&#22815;&#23558;&#30446;&#26631;LLM&#30340;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#25552;&#28860;&#21040;&#19968;&#20010;&#21442;&#25968;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;ChatGPT-3.5-Turbo&#20013;&#25552;&#21462;&#20219;&#21153;&#33021;&#21147;&#26469;&#28436;&#31034;&#25105;&#20204;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;73%&#30340;&#20934;&#30830;&#21305;&#37197;(EM)&#30456;&#20284;&#24615;&#20197;&#21450;75%&#30340;SQuAD EM&#20934;&#30830;&#29575;&#21644;87%&#30340;F1&#24471;&#20998;&#65292;&#20165;&#38656;50&#32654;&#20803;&#30340;API&#36153;&#29992;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36890;&#36807;&#27169;&#22411;&#21560;&#21462;&#25552;&#21462;&#30340;&#27169;&#22411;&#22312;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#26102;&#30340;&#21487;&#34892;&#24615;&#65292;&#24403;&#24212;&#29992;&#20110;ChatGPT-3.5-Turbo&#26102;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;11%&#12290;
&lt;/p&gt;
&lt;p&gt;
Model Leeching is a novel extraction attack targeting Large Language Models (LLMs), capable of distilling task-specific knowledge from a target LLM into a reduced parameter model. We demonstrate the effectiveness of our attack by extracting task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match (EM) similarity, and SQuAD EM and F1 accuracy scores of 75% and 87%, respectively for only $50 in API cost. We further demonstrate the feasibility of adversarial attack transferability from an extracted model extracted via Model Leeching to perform ML attack staging against a target LLM, resulting in an 11% increase to attack success rate when applied to ChatGPT-3.5-Turbo.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;&#31185;&#23398;&#25991;&#26723;&#30456;&#20284;&#24230;&#27979;&#37327;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;&#31185;&#23398;&#25991;&#26723;&#25968;&#25454;&#38598;OpenMSD&#65292;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#31185;&#23398;&#19987;&#38376;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#22810;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10539</link><description>&lt;p&gt;
OpenMSD:&#38754;&#21521;&#22810;&#35821;&#35328;&#31185;&#23398;&#25991;&#26723;&#30456;&#20284;&#24230;&#27979;&#37327;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
OpenMSD: Towards Multilingual Scientific Documents Similarity Measurement. (arXiv:2309.10539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10539
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;&#31185;&#23398;&#25991;&#26723;&#30456;&#20284;&#24230;&#27979;&#37327;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;&#31185;&#23398;&#25991;&#26723;&#25968;&#25454;&#38598;OpenMSD&#65292;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#31185;&#23398;&#19987;&#38376;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#22810;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;&#31185;&#23398;&#25991;&#26723;&#30456;&#20284;&#24230;&#27979;&#37327;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29992;&#26469;&#25214;&#21040;&#19981;&#21516;&#35821;&#35328;&#30340;&#30456;&#20851;&#20316;&#21697;&#65292;&#24110;&#21161;&#22810;&#35821;&#35328;&#30740;&#31350;&#20154;&#21592;&#26356;&#26377;&#25928;&#22320;&#25214;&#21040;&#21644;&#25506;&#32034;&#35770;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;&#31185;&#23398;&#25991;&#26723;&#25968;&#25454;&#38598;OpenMSD&#65292;&#20854;&#20013;&#21253;&#21547;103&#31181;&#35821;&#35328;&#30340;7400&#19975;&#31687;&#35770;&#25991;&#21644;7780&#19975;&#20010;&#24341;&#29992;&#23545;&#12290;&#21033;&#29992;OpenMSD&#65292;&#25105;&#20204;&#39044;&#35757;&#32451;&#20102;&#31185;&#23398;&#19987;&#38376;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#31574;&#30053;&#26469;&#23548;&#20986;&#8220;&#30456;&#20851;&#8221;&#30340;&#35770;&#25991;&#23545;&#20197;&#35843;&#25972;&#27169;&#22411;&#65292;&#21253;&#25324;&#20351;&#29992;&#24341;&#29992;&#12289;&#20849;&#24341;&#29992;&#21644;&#25991;&#29486;&#32806;&#21512;&#30340;&#28151;&#21512;&#23545;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#38750;&#33521;&#25991;&#35770;&#25991;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#26469;&#29992;&#33521;&#25991;&#25688;&#35201;&#20016;&#23500;&#38750;&#33521;&#25991;&#35770;&#25991;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#27169;&#22411;&#30340;&#33521;&#25991;&#33021;&#21147;&#20026;&#38750;&#33521;&#25991;&#35770;&#25991;&#21019;&#24314;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#26174;&#33879;&#22320;&#36229;&#36807;&#20102;&#20854;&#20182;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop and evaluate multilingual scientific documents similarity measurement models in this work. Such models can be used to find related works in different languages, which can help multilingual researchers find and explore papers more efficiently. We propose the first multilingual scientific documents dataset, Open-access Multilingual Scientific Documents (OpenMSD), which has 74M papers in 103 languages and 778M citation pairs. With OpenMSD, we pretrain science-specialized language models, and explore different strategies to derive "related" paper pairs to fine-tune the models, including using a mixture of citation, co-citation, and bibliographic-coupling pairs. To further improve the models' performance for non-English papers, we explore the use of generative language models to enrich the non-English papers with English summaries. This allows us to leverage the models' English capabilities to create better representations for non-English papers. Our best model significantly outp
&lt;/p&gt;</description></item><item><title>NSOAMT&#26159;&#19968;&#20010;&#26032;&#30340;&#21482;&#25628;&#32034;&#30340;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#36882;&#22686;&#35789;&#27719;&#32034;&#24341;&#23454;&#29616;&#21363;&#26102;&#21644;&#20005;&#35880;&#30340;&#32763;&#35793;&#36807;&#31243;&#65292;&#25552;&#39640;&#32763;&#35793;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10526</link><description>&lt;p&gt;
NSOAMT -- &#26032;&#30340;&#21482;&#25628;&#32034;&#30340;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NSOAMT -- New Search Only Approach to Machine Translation. (arXiv:2309.10526v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10526
&lt;/p&gt;
&lt;p&gt;
NSOAMT&#26159;&#19968;&#20010;&#26032;&#30340;&#21482;&#25628;&#32034;&#30340;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#36882;&#22686;&#35789;&#27719;&#32034;&#24341;&#23454;&#29616;&#21363;&#26102;&#21644;&#20005;&#35880;&#30340;&#32763;&#35793;&#36807;&#31243;&#65292;&#25552;&#39640;&#32763;&#35793;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#33258;&#21160;&#21270;&#26426;&#21046;&#21644;&#24037;&#20855;&#24050;&#32463;&#21457;&#23637;&#20102;&#20960;&#24180;&#65292;&#20197;&#20351;&#35828;&#19981;&#21516;&#35821;&#35328;&#30340;&#20154;&#33021;&#22815;&#32852;&#31995;&#36215;&#26469;&#12290;"&#26032;&#30340;&#21482;&#25628;&#32034;&#30340;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;"&#34987;&#37319;&#29992;&#26469;&#35299;&#20915;&#20854;&#20182;&#25216;&#26415;&#30340;&#24930;&#36895;&#21644;&#19981;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#35821;&#20041;&#30456;&#31526;&#30340;&#36882;&#22686;&#35789;&#27719;&#32034;&#24341;&#65292;&#20351;&#20854;&#33021;&#22815;&#21019;&#24314;&#21407;&#35821;&#35328;&#35760;&#24405;&#21644;&#32763;&#35793;&#35821;&#35328;&#20043;&#38388;&#30340;&#23545;&#24212;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#32763;&#35793;&#36807;&#31243;&#20013;&#30340;&#21363;&#26102;&#24615;&#21644;&#20005;&#35880;&#24615;&#12290;&#36890;&#36807;&#23545;&#30005;&#23376;&#25991;&#26412;&#25991;&#26723;&#30340;&#22788;&#29702;&#12289;&#21152;&#36733;&#12289;&#20998;&#26512;&#21644;&#27979;&#37327;&#65292;&#39564;&#35777;&#20102;&#36825;&#19968;&#30740;&#31350;&#21407;&#29702;&#30340;&#21069;&#25552;&#12290;&#23613;&#31649;&#35266;&#23519;&#21040;&#30340;&#21644;&#39044;&#27979;&#30340;&#24230;&#37327;&#25351;&#26631;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#24046;&#24322;&#65292;&#20294;&#25972;&#20307;&#32780;&#35328;&#65292;&#36825;&#31181;&#26032;&#26041;&#27861;&#22312;&#25552;&#39640;&#32763;&#35793;&#36807;&#31243;&#20013;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translation automation mechanisms and tools have been developed for several years to bring people who speak different languages together. A "new search only approach to machine translation" was adopted to tackle some of the slowness and inaccuracy of the other technologies. The idea is to develop a solution that, by indexing an incremental set of words that combine a certain semantic meaning, makes it possible to create a process of correspondence between their native language record and the language of translation. This research principle assumes that the vocabulary used in a given type of publication/document is relatively limited in terms of language style and word diversity, which enhances the greater effect of instantaneously and rigor in the translation process through the indexing process. A volume of electronic text documents where processed and loaded into a database, and analyzed and measured in order confirm the previous premise. Although the observed and projected metric va
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32467;&#21512;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#21033;&#29992;LLM&#30340;&#38646;-shot&#33021;&#21147;&#26469;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10524</link><description>&lt;p&gt;
&#21457;&#25381;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#38646;-shot&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model in End-to-End Speech Recognition. (arXiv:2309.10524v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32467;&#21512;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#21033;&#29992;LLM&#30340;&#38646;-shot&#33021;&#21147;&#26469;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;&#29616;&#20195;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23398;&#20064;&#20013;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#65292;&#21482;&#35201;&#25552;&#20379;&#26126;&#30830;&#30340;&#25351;&#23548;&#25110;&#25552;&#31034;&#26469;&#25351;&#23548;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#36825;&#31181;&#38646;-shot&#33021;&#21147;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#21462;&#35821;&#35328;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#21435;&#32416;&#27491;&#35821;&#38899;&#35782;&#21035;&#20551;&#35774;&#20013;&#30340;&#35821;&#27861;&#38169;&#35823;&#65292;&#24182;&#21033;&#29992;&#23884;&#20837;&#30340;&#35821;&#35328;&#30693;&#35782;&#36827;&#34892;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22522;&#20110;&#28151;&#21512;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#21644;&#27880;&#24847;&#21147;&#26550;&#26500;&#65292;&#20854;&#20013;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;Llama2&#65289;&#34987;&#29992;&#20316;&#35299;&#30721;&#22120;&#30340;&#21069;&#31471;&#12290;&#36890;&#36807;CTC&#35299;&#30721;&#20174;&#32534;&#30721;&#22120;&#33719;&#24471;&#19968;&#20010;&#38656;&#35201;&#32416;&#27491;&#30340;&#35821;&#38899;&#35782;&#21035;&#20551;&#35774;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#25351;&#23548;&#19968;&#36215;&#36755;&#20837;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#35299;&#30721;&#22120;&#38543;&#21518;&#37319;&#21462;...
&lt;/p&gt;
&lt;p&gt;
We present a novel integration of an instruction-tuned large language model (LLM) and end-to-end automatic speech recognition (ASR). Modern LLMs can perform a wide range of linguistic tasks within zero-shot learning when provided with a precise instruction or a prompt to guide the text generation process towards the desired task. We explore using this zero-shot capability of LLMs to extract linguistic information that can contribute to improving ASR performance. Specifically, we direct an LLM to correct grammatical errors in an ASR hypothesis and harness the embedded linguistic knowledge to conduct end-to-end ASR. The proposed model is built on the hybrid connectionist temporal classification (CTC) and attention architecture, where an instruction-tuned LLM (i.e., Llama2) is employed as a front-end of the decoder. An ASR hypothesis, subject to correction, is obtained from the encoder via CTC decoding, which is then fed into the LLM along with an instruction. The decoder subsequently tak
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#24320;&#25918;&#39046;&#22495;&#34920;&#26684;&#38382;&#31572;&#20219;&#21153;&#30340;&#21477;&#27861;&#21644;&#32467;&#26500;&#24863;&#30693;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#21477;&#27861;&#34920;&#31034;&#24182;&#20351;&#29992;&#34920;&#26684;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#22312;&#34920;&#26684;&#35780;&#20998;&#36807;&#31243;&#20013;&#20002;&#22833;&#21477;&#27861;&#21644;&#32467;&#26500;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#22312;NQ-tables&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10506</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#27880;&#21477;&#27861;&#21644;&#32467;&#26500;&#30340;&#31264;&#23494;&#26816;&#32034;&#22686;&#24378;&#24320;&#25918;&#39046;&#22495;&#34920;&#26684;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Enhancing Open-Domain Table Question Answering via Syntax- and Structure-aware Dense Retrieval. (arXiv:2309.10506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#24320;&#25918;&#39046;&#22495;&#34920;&#26684;&#38382;&#31572;&#20219;&#21153;&#30340;&#21477;&#27861;&#21644;&#32467;&#26500;&#24863;&#30693;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#21477;&#27861;&#34920;&#31034;&#24182;&#20351;&#29992;&#34920;&#26684;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#22312;&#34920;&#26684;&#35780;&#20998;&#36807;&#31243;&#20013;&#20002;&#22833;&#21477;&#27861;&#21644;&#32467;&#26500;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#22312;NQ-tables&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#34920;&#26684;&#38382;&#31572;&#26088;&#22312;&#36890;&#36807;&#20174;&#22823;&#37327;&#30340;&#34920;&#26684;&#20013;&#26816;&#32034;&#21644;&#25552;&#21462;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24320;&#25918;&#39046;&#22495;&#34920;&#26684;&#38382;&#31572;&#30740;&#31350;&#35201;&#20040;&#30452;&#25509;&#37319;&#29992;&#25991;&#26412;&#26816;&#32034;&#26041;&#27861;&#65292;&#35201;&#20040;&#20165;&#22312;&#34920;&#26684;&#26816;&#32034;&#30340;&#32534;&#30721;&#23618;&#32771;&#34385;&#34920;&#26684;&#32467;&#26500;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#34920;&#26684;&#35780;&#20998;&#36807;&#31243;&#20013;&#20002;&#22833;&#21477;&#27861;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#24320;&#25918;&#39046;&#22495;&#34920;&#26684;&#38382;&#31572;&#20219;&#21153;&#30340;&#21477;&#27861;&#21644;&#32467;&#26500;&#24863;&#30693;&#26816;&#32034;&#26041;&#27861;&#12290;&#23427;&#20026;&#38382;&#39064;&#25552;&#20379;&#21477;&#27861;&#34920;&#31034;&#65292;&#20351;&#29992;&#34920;&#26684;&#30340;&#32467;&#26500;&#22836;&#37096;&#21644;&#20540;&#34920;&#31034;&#26469;&#36991;&#20813;&#23545;&#32454;&#31890;&#24230;&#21477;&#27861;&#21644;&#32467;&#26500;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21477;&#27861;&#21040;&#32467;&#26500;&#30340;&#32858;&#21512;&#22120;&#26469;&#27169;&#25311;&#20154;&#31867;&#26816;&#32034;&#36807;&#31243;&#65292;&#33719;&#21462;&#38382;&#39064;&#21644;&#20505;&#36873;&#34920;&#26684;&#20043;&#38388;&#30340;&#21305;&#37197;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;NQ-tables&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain table question answering aims to provide answers to a question by retrieving and extracting information from a large collection of tables. Existing studies of open-domain table QA either directly adopt text retrieval methods or consider the table structure only in the encoding layer for table retrieval, which may cause syntactical and structural information loss during table scoring. To address this issue, we propose a syntax- and structure-aware retrieval method for the open-domain table QA task. It provides syntactical representations for the question and uses the structural header and value representations for the tables to avoid the loss of fine-grained syntactical and structural information. Then, a syntactical-to-structural aggregator is used to obtain the matching score between the question and a candidate table by mimicking the human retrieval process. Experimental results show that our method achieves the state-of-the-art on the NQ-tables dataset and overwhelms str
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;ETHICS&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#65292;&#34920;&#26126;AI&#20262;&#29702;&#20013;&#19982;&#20849;&#21516;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#21512;&#20316;&#23398;&#20064;&#24182;&#19981;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10492</link><description>&lt;p&gt;
&#23545;GPT-4&#22312;ETHICS&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Evaluation of GPT-4 on the ETHICS Dataset. (arXiv:2309.10492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;ETHICS&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#65292;&#34920;&#26126;AI&#20262;&#29702;&#20013;&#19982;&#20849;&#21516;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#21512;&#20316;&#23398;&#20064;&#24182;&#19981;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#24635;&#32467;&#20102;&#23545;GPT-4&#22312;ETHICS&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#30340;&#30701;&#26399;&#30740;&#31350;&#12290;ETHICS&#25968;&#25454;&#38598;&#21253;&#21547;&#20116;&#20010;&#20998;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20262;&#29702;&#23398;&#30340;&#19981;&#21516;&#39046;&#22495;&#65306;&#27491;&#20041;&#12289;&#36947;&#24503;&#12289;&#24503;&#24615;&#20262;&#29702;&#23398;&#12289;&#21151;&#21033;&#20027;&#20041;&#21644;&#24120;&#35782;&#20262;&#29702;&#23398;&#12290;&#36825;&#20123;&#36947;&#24503;&#21028;&#26029;&#34987;&#31934;&#36873;&#65292;&#20197;&#23613;&#21487;&#33021;&#39640;&#30340;&#19968;&#33268;&#24615;&#26469;&#20195;&#34920;&#20849;&#20139;&#30340;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#32780;&#19981;&#26159;&#36947;&#24503;&#22256;&#22659;&#12290;GPT-4&#30340;&#34920;&#29616;&#27604;&#20043;&#21069;&#30340;&#27169;&#22411;&#22909;&#24471;&#22810;&#65292;&#34920;&#26126;AI&#20262;&#29702;&#20013;&#19982;&#20849;&#21516;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#21512;&#20316;&#23398;&#20064;&#24182;&#19981;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report summarizes a short study of the performance of GPT-4 on the ETHICS dataset. The ETHICS dataset consists of five sub-datasets covering different fields of ethics: Justice, Deontology, Virtue Ethics, Utilitarianism, and Commonsense Ethics. The moral judgments were curated so as to have a high degree of agreement with the aim of representing shared human values rather than moral dilemmas. GPT-4's performance is much better than that of previous models and suggests that learning to work with common human values is not the hard problem for AI ethics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#25913;&#36827;&#35828;&#35805;&#32773;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21475;&#35821;&#29702;&#35299;&#27169;&#22359;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#26500;&#24314;&#25104;&#23545;&#32422;&#26463;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#35828;&#35805;&#32773;&#20998;&#31163;&#27969;&#31243;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10456</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#25913;&#36827;&#35828;&#35805;&#32773;&#20998;&#31163;: &#21033;&#29992;&#32852;&#21512;&#25104;&#23545;&#32422;&#26463;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Improving Speaker Diarization using Semantic Information: Joint Pairwise Constraints Propagation. (arXiv:2309.10456v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#25913;&#36827;&#35828;&#35805;&#32773;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21475;&#35821;&#29702;&#35299;&#27169;&#22359;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#26500;&#24314;&#25104;&#23545;&#32422;&#26463;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#35828;&#35805;&#32773;&#20998;&#31163;&#27969;&#31243;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#32773;&#20998;&#31163;&#24050;&#32463;&#24341;&#36215;&#20102;&#35821;&#38899;&#22788;&#29702;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#20027;&#27969;&#30340;&#35828;&#35805;&#32773;&#20998;&#31163;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#22768;&#38899;&#20449;&#21495;&#20013;&#25552;&#21462;&#30340;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#29305;&#24449;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#35821;&#20041;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#32771;&#34385;&#21040;&#35821;&#38899;&#20449;&#21495;&#33021;&#22815;&#26377;&#25928;&#20256;&#36798;&#35821;&#38899;&#30340;&#20869;&#23481;&#65292;&#25105;&#20204;&#26377;&#20852;&#36259;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#35821;&#20041;&#32447;&#32034;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#35828;&#35805;&#32773;&#20998;&#31163;&#31995;&#32479;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#21475;&#35821;&#29702;&#35299;&#27169;&#22359;&#26469;&#25552;&#21462;&#19982;&#35828;&#35805;&#32773;&#30456;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26500;&#24314;&#25104;&#23545;&#32422;&#26463;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#23558;&#36825;&#20123;&#32422;&#26463;&#38598;&#25104;&#21040;&#35828;&#35805;&#32773;&#20998;&#31163;&#27969;&#31243;&#20013;&#65292;&#25552;&#39640;&#25972;&#20010;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#19968;&#33268;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speaker diarization has gained considerable attention within speech processing research community. Mainstream speaker diarization rely primarily on speakers' voice characteristics extracted from acoustic signals and often overlook the potential of semantic information. Considering the fact that speech signals can efficiently convey the content of a speech, it is of our interest to fully exploit these semantic cues utilizing language models. In this work we propose a novel approach to effectively leverage semantic information in clustering-based speaker diarization systems. Firstly, we introduce spoken language understanding modules to extract speaker-related semantic information and utilize these information to construct pairwise constraints. Secondly, we present a novel framework to integrate these constraints into the speaker diarization pipeline, enhancing the performance of the entire system. Extensive experiments conducted on the public dataset demonstrate the consistent superiori
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#65288;REI&#65289;&#23454;&#29616;&#20102;&#32479;&#19968;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#25351;&#20196;&#26041;&#24335;&#25903;&#25345;&#21508;&#31181;&#32422;&#26463;&#65292;&#26080;&#38656;&#23545;&#26550;&#26500;&#36827;&#34892;&#20462;&#25913;&#65292;&#24182;&#23545;&#21508;&#31181;&#32422;&#26463;&#32452;&#21512;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10447</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#23454;&#29616;&#32479;&#19968;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Toward Unified Controllable Text Generation via Regular Expression Instruction. (arXiv:2309.10447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#65288;REI&#65289;&#23454;&#29616;&#20102;&#32479;&#19968;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#25351;&#20196;&#26041;&#24335;&#25903;&#25345;&#21508;&#31181;&#32422;&#26463;&#65292;&#26080;&#38656;&#23545;&#26550;&#26500;&#36827;&#34892;&#20462;&#25913;&#65292;&#24182;&#23545;&#21508;&#31181;&#32422;&#26463;&#32452;&#21512;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#22522;&#26412;&#26041;&#38754;&#20043;&#19968;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#38024;&#23545;&#19981;&#21516;&#32422;&#26463;&#31867;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#37325;&#22823;&#30340;&#26550;&#26500;&#25110;&#35299;&#30721;&#20462;&#25913;&#65292;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#24212;&#29992;&#20110;&#38468;&#21152;&#32422;&#26463;&#25110;&#35299;&#20915;&#19981;&#21516;&#32422;&#26463;&#32452;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#65288;REI&#65289;&#65292;&#21033;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#26426;&#21046;&#20805;&#20998;&#21033;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#20248;&#21183;&#65292;&#32479;&#19968;&#24314;&#27169;&#21508;&#31181;&#32422;&#26463;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;REI&#36890;&#36807;&#27491;&#21017;&#34920;&#36798;&#24335;&#39118;&#26684;&#30340;&#25351;&#20196;&#25903;&#25345;&#25152;&#26377;&#27969;&#34892;&#30340;&#32454;&#31890;&#24230;&#21487;&#25511;&#29983;&#25104;&#32422;&#26463;&#65292;&#21363;&#35789;&#27719;&#12289;&#20301;&#32622;&#21644;&#38271;&#24230;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#22312;&#20013;&#31561;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#24494;&#35843;&#25110;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#23569;&#26679;&#26412;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#20110;&#21508;&#31181;&#32422;&#26463;&#32452;&#21512;&#26102;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#25972;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable text generation is a fundamental aspect of natural language generation, with numerous methods proposed for different constraint types. However, these approaches often require significant architectural or decoding modifications, making them challenging to apply to additional constraints or resolve different constraint combinations. To address this, our paper introduces Regular Expression Instruction (REI), which utilizes an instruction-based mechanism to fully exploit regular expressions' advantages to uniformly model diverse constraints. Specifically, our REI supports all popular fine-grained controllable generation constraints, i.e., lexical, positional, and length, as well as their complex combinations, via regular expression-style instructions. Our method only requires fine-tuning on medium-scale language models or few-shot, in-context learning on large language models, and requires no further adjustment when applied to various constraint combinations. Experiments demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10444</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#33258;&#25105;&#24378;&#21270;&#20197;&#25913;&#36827;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#28041;&#21450;&#23398;&#29983;&#29983;&#25104;&#21644;&#20998;&#20139;&#23398;&#20064;&#36164;&#28304;&#12290;&#22312;&#23398;&#29983;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#21019;&#24314;&#35299;&#37322;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#23545;&#30456;&#20851;&#27010;&#24565;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#24448;&#24448;&#30001;&#20110;&#20027;&#39064;&#29702;&#35299;&#26377;&#38480;&#21644;&#20165;&#20165;&#37325;&#30003;&#38382;&#39064;&#12289;&#24178;&#25200;&#22240;&#32032;&#21644;&#27491;&#30830;&#31572;&#26696;&#30340;&#20542;&#21521;&#32780;&#38590;&#20197;&#32534;&#20889;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24110;&#21161;&#25903;&#25745;&#36825;&#20010;&#20219;&#21153;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#29983;&#25104;&#19982;&#23398;&#29983;&#23545;&#40784;&#30340;&#35299;&#37322;&#65292;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#20197;&#30830;&#20445;&#20854;&#36136;&#37327;&#65292;&#24182;&#36845;&#20195;&#22686;&#24378;&#35299;&#37322;&#12290;&#22914;&#26524;&#19968;&#20010;&#35299;&#37322;&#30340;&#35780;&#20272;&#20998;&#25968;&#20302;&#20110;&#23450;&#20041;&#30340;&#38408;&#20540;&#65292;&#26694;&#26550;&#20250;&#36845;&#20195;&#22320;&#20248;&#21270;&#21644;&#37325;&#26032;&#35780;&#20272;&#35299;&#37322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#27169;&#25311;&#20102;&#19968;&#20010;&#23398;&#29983;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learnersourcing involves students generating and sharing learning resources with their peers. When learnersourcing multiple-choice questions, creating explanations for the generated questions is a crucial step as it facilitates a deeper understanding of the related concepts. However, it is often difficult for students to craft effective explanations due to limited subject understanding and a tendency to merely restate the question stem, distractors, and correct answer. To help scaffold this task, in this work we propose a self-reinforcement large-language-model framework, with the goal of generating and evaluating explanations automatically. Comprising three modules, the framework generates student-aligned explanations, evaluates these explanations to ensure their quality and iteratively enhances the explanations. If an explanation's evaluation score falls below a defined threshold, the framework iteratively refines and reassesses the explanation. Importantly, our framework emulates th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.10435</link><description>&lt;p&gt;
&#37325;&#22609;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65306;&#21033;&#29992;&#20869;&#23481;&#22686;&#24378;&#35821;&#35328;&#24314;&#27169;&#23398;&#20064;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;
&lt;/p&gt;
&lt;p&gt;
Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling. (arXiv:2309.10435v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#22312;&#32447;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#39034;&#24207;&#25512;&#33616;&#30001;&#20110;&#20854;&#34920;&#36798;&#33021;&#21147;&#24378;&#22823;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;&#32780;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#39034;&#24207;&#24314;&#27169;&#26041;&#27861;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#30340;&#21407;&#22240;&#26159;&#35821;&#35328;&#27169;&#22411;&#24120;&#24120;&#32570;&#20047;&#23545;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#29289;&#21697;&#30456;&#20851;&#25991;&#26412;&#20869;&#23481;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;LANCER&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24357;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20135;&#29983;&#20102;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#27169;&#22411;&#23545;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are essential for online applications, and sequential recommendation has enjoyed significant prevalence due to its expressive ability to capture dynamic user interests. However, previous sequential modeling methods still have limitations in capturing contextual information. The primary reason for this issue is that language models often lack an understanding of domain-specific knowledge and item-related textual content. To address this issue, we adopt a new sequential recommendation paradigm and propose LANCER, which leverages the semantic understanding capabilities of pre-trained language models to generate personalized recommendations. Our approach bridges the gap between language models and recommender systems, resulting in more human-like recommendations. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets, showing promising results and providing valuable insights into the influence of our model on sequential recomm
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20316;&#23478;&#23450;&#20041;&#30340;AI&#20154;&#29289;&#24418;&#35937;&#29983;&#25104;&#21363;&#26102;&#21453;&#39304;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20010;&#27010;&#24565;&#21463;&#21040;&#20102;&#20316;&#23478;&#30340;&#27426;&#36814;&#24182;&#24110;&#21161;&#20182;&#20204;&#33719;&#24471;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;AI&#24037;&#20855;&#35774;&#35745;&#20013;&#30340;&#31038;&#20250;&#25216;&#26415;&#35270;&#35282;&#65292;&#20026;&#25903;&#25345;&#20316;&#23478;&#19982;AI&#30340;&#24895;&#26223;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.10433</link><description>&lt;p&gt;
&#20026;&#21363;&#26102;&#21453;&#39304;&#29983;&#25104;&#23450;&#20041;AI&#20154;&#29289;&#24418;&#35937;
&lt;/p&gt;
&lt;p&gt;
Writer-Defined AI Personas for On-Demand Feedback Generation. (arXiv:2309.10433v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10433
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20316;&#23478;&#23450;&#20041;&#30340;AI&#20154;&#29289;&#24418;&#35937;&#29983;&#25104;&#21363;&#26102;&#21453;&#39304;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20010;&#27010;&#24565;&#21463;&#21040;&#20102;&#20316;&#23478;&#30340;&#27426;&#36814;&#24182;&#24110;&#21161;&#20182;&#20204;&#33719;&#24471;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;AI&#24037;&#20855;&#35774;&#35745;&#20013;&#30340;&#31038;&#20250;&#25216;&#26415;&#35270;&#35282;&#65292;&#20026;&#25903;&#25345;&#20316;&#23478;&#19982;AI&#30340;&#24895;&#26223;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#31168;&#30340;&#20889;&#20316;&#24212;&#35813;&#26681;&#25454;&#21463;&#20247;&#36827;&#34892;&#36866;&#24212;&#12290;&#28982;&#32780;&#65292;&#20316;&#23478;&#21487;&#33021;&#38590;&#20197;&#21516;&#35835;&#32773;&#20135;&#29983;&#20849;&#40483;&#65292;&#38590;&#20197;&#21450;&#26102;&#33719;&#24471;&#21453;&#39304;&#25110;&#32773;&#38590;&#20197;&#33719;&#24471;&#30446;&#26631;&#32676;&#20307;&#30340;&#20449;&#24687;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#65292;&#21363;&#22522;&#20110;&#20316;&#23478;&#23450;&#20041;&#30340;&#20219;&#20309;&#30446;&#26631;&#21463;&#20247;&#30340;AI&#20154;&#29289;&#24418;&#35937;&#29983;&#25104;&#21363;&#26102;&#21453;&#39304;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#21407;&#22411;&#65288;&#20351;&#29992;GPT-3.5&#65289;&#22312;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#65288;N=5&#21644;N=11&#65289;&#20013;&#25506;&#32034;&#20102;&#36825;&#19968;&#27010;&#24565;&#65306;&#20316;&#23478;&#20204;&#36190;&#36175;&#36825;&#19968;&#27010;&#24565;&#65292;&#24182;&#19988;&#25112;&#30053;&#24615;&#22320;&#20351;&#29992;&#20154;&#29289;&#24418;&#35937;&#26469;&#33719;&#24471;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#21453;&#39304;&#34987;&#35748;&#20026;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#19988;&#28608;&#21457;&#20102;&#25991;&#26412;&#21644;&#20154;&#29289;&#24418;&#35937;&#30340;&#20462;&#35746;&#65292;&#23613;&#31649;&#35813;&#21453;&#39304;&#36890;&#24120;&#20887;&#38271;&#32780;&#19981;&#20855;&#20307;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21363;&#26102;&#21453;&#39304;&#30340;&#24433;&#21709;&#12289;&#24403;&#20195;AI&#31995;&#32479;&#30340;&#26377;&#38480;&#20195;&#34920;&#24615;&#20197;&#21450;&#36827;&#19968;&#27493;&#23450;&#20041;AI&#20154;&#29289;&#30340;&#24819;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;&#25903;&#25345;&#20316;&#23478;&#19982;AI&#30340;&#24895;&#26223;&#20013;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#25193;&#23637;&#20102;AI&#24037;&#20855;&#35774;&#35745;&#20013;&#30340;&#31038;&#20250;&#25216;&#26415;&#35270;&#35282;&#65306;&#20026;&#20102;&#36171;&#20104;&#21019;&#20316;&#32773;&#26435;&#21147;&#65292;&#25105;&#20204;&#36824;&#38656;&#35201;&#32771;&#34385;&#20182;&#20204;&#19982;&#35266;&#20247;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compelling writing is tailored to its audience. This is challenging, as writers may struggle to empathize with readers, get feedback in time, or gain access to the target group. We propose a concept that generates on-demand feedback, based on writer-defined AI personas of any target audience. We explore this concept with a prototype (using GPT-3.5) in two user studies (N=5 and N=11): Writers appreciated the concept and strategically used personas for getting different perspectives. The feedback was seen as helpful and inspired revisions of text and personas, although it was often verbose and unspecific. We discuss the impact of on-demand feedback, the limited representativity of contemporary AI systems, and further ideas for defining AI personas. This work contributes to the vision of supporting writers with AI by expanding the socio-technical perspective in AI tool design: To empower creators, we also need to keep in mind their relationship to an audience.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PICK&#65288;Polished &amp; Informed Candidate Scoring&#65289;&#26694;&#26550;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#29983;&#25104;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#31995;&#32479;&#22312;&#29983;&#25104;&#21709;&#24212;&#20248;&#36136;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#24544;&#23454;&#12289;&#30456;&#20851;&#19988;&#26080;&#38656;&#39069;&#22806;&#26631;&#35760;&#25968;&#25454;&#25110;&#35843;&#25972;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10413</link><description>&lt;p&gt;
PICK: &#30952;&#30778;&#21644;&#30693;&#24773;&#20505;&#36873;&#20998;&#25968;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;.
&lt;/p&gt;
&lt;p&gt;
PICK: Polished &amp; Informed Candidate Scoring for Knowledge-Grounded Dialogue Systems. (arXiv:2309.10413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PICK&#65288;Polished &amp; Informed Candidate Scoring&#65289;&#26694;&#26550;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#29983;&#25104;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#31995;&#32479;&#22312;&#29983;&#25104;&#21709;&#24212;&#20248;&#36136;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#24544;&#23454;&#12289;&#30456;&#20851;&#19988;&#26080;&#38656;&#39069;&#22806;&#26631;&#35760;&#25968;&#25454;&#25110;&#35843;&#25972;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#35758;&#22312;&#22806;&#37096;&#30693;&#35782;&#30340;&#22522;&#30784;&#19978;&#21152;&#24378;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#65292;&#20197;&#20135;&#29983;&#20449;&#24687;&#37327;&#22823;&#19988;&#24341;&#20154;&#20837;&#32988;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;&#22312;&#19982;&#20154;&#31867;&#20248;&#20808;&#21697;&#36136;&#30340;&#29983;&#25104;&#21709;&#24212;&#23545;&#40784;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#38382;&#39064;&#65292;&#27604;&#22914;&#24187;&#35273;&#21644;&#32570;&#20047;&#36830;&#36143;&#24615;&#31561;&#12290;&#22312;&#20998;&#26512;&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#35299;&#30721;&#36807;&#31243;&#20013;&#23384;&#22312;&#30528;&#22810;&#20010;&#22791;&#36873;&#29983;&#25104;&#21709;&#24212;&#12290;&#36825;&#20123;&#22791;&#36873;&#21709;&#24212;&#30456;&#23545;&#20110;&#35299;&#30721;&#36807;&#31243;&#20248;&#20808;&#32771;&#34385;&#30340;&#26368;&#20248;&#21709;&#24212;&#26356;&#21152;&#24544;&#23454;&#65292;&#24182;&#19988;&#19982;&#20043;&#21069;&#30340;&#23545;&#35805;&#36716;&#25442;&#20855;&#26377;&#21487;&#27604;&#24615;&#25110;&#26356;&#39640;&#32423;&#21035;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#24182;&#21463;&#21040;&#36825;&#20123;&#35266;&#23519;&#30340;&#39537;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Polished \&amp; Informed Candidate Scoring (PICK)&#65292;&#36825;&#26159;&#19968;&#20010;&#29983;&#25104;&#37325;&#26032;&#35780;&#20998;&#30340;&#26694;&#26550;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#24544;&#23454;&#21644;&#30456;&#20851;&#30340;&#21709;&#24212;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#26631;&#35760;&#25968;&#25454;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grounding dialogue response generation on external knowledge is proposed to produce informative and engaging responses. However, current knowledge-grounded dialogue (KGD) systems often fail to align the generated responses with human-preferred qualities due to several issues like hallucination and the lack of coherence. Upon analyzing multiple language model generations, we observe the presence of alternative generated responses within a single decoding process. These alternative responses are more faithful and exhibit a comparable or higher level of relevance to prior conversational turns compared to the optimal responses prioritized by the decoding processes. To address these challenges and driven by these observations, we propose Polished \&amp; Informed Candidate Scoring (PICK), a generation re-scoring framework that empowers models to generate faithful and relevant responses without requiring additional labeled data or model tuning. Through comprehensive automatic and human evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoSE&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#36866;&#24212;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#65292;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2309.10400</link><description>&lt;p&gt;
PoSE: &#36890;&#36807;&#20301;&#32622;&#36339;&#36291;&#24335;&#35757;&#32451;&#25552;&#39640;LLMs&#23545;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#26377;&#25928;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. (arXiv:2309.10400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoSE&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#36866;&#24212;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#65292;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Positional Skip-wise (PoSE)&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#20110;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;PoSE&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#27169;&#25311;&#38271;&#36755;&#20837;&#65292;&#23558;&#35757;&#32451;&#38271;&#24230;&#19982;&#30446;&#26631;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#20998;&#31163;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#38271;&#36755;&#20837;&#24207;&#21015;&#20013;&#36873;&#25321;&#33509;&#24178;&#30701;&#22359;&#65292;&#24182;&#24341;&#20837;&#19981;&#21516;&#30340;&#36339;&#36291;&#20559;&#32622;&#39033;&#26469;&#20462;&#25913;&#27599;&#20010;&#22359;&#30340;&#20301;&#32622;&#32034;&#24341;&#12290;&#36825;&#20123;&#36339;&#36291;&#20559;&#32622;&#39033;&#20197;&#21450;&#27599;&#20010;&#22359;&#30340;&#38271;&#24230;&#22312;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#37117;&#20250;&#21464;&#21270;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#30340;&#25152;&#26377;&#20301;&#32622;&#65292;&#32780;&#26080;&#38656;&#23545;&#23436;&#25972;&#38271;&#24230;&#30340;&#36755;&#20837;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#23545;&#23436;&#25972;&#38271;&#24230;&#36827;&#34892;&#24494;&#35843;&#30456;&#27604;&#65292;PoSE&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;&#21033;&#29992;&#36825;&#19968;&#20248;&#21183;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#65292;PoSE&#19982;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Positional Skip-wisE (PoSE) training for efficient adaptation of large language models~(LLMs) to extremely long context windows. PoSE decouples train length from target context window size by simulating long inputs using a fixed context window with manipulated position indices during training. Concretely, we select several short chunks from a long input sequence, and introduce distinct skipping bias terms to modify the position indices of each chunk. These bias terms, along with the length of each chunk, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full length inputs. Experiments show that, compared with fine-tuning on the full length, PoSE greatly reduces memory and time overhead with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens. Furthermore, we empirically confirm that PoSE is compatible with 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#36890;&#36807;&#23545;&#26080;&#25903;&#25345;&#35770;&#26029;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#20013;&#25552;&#21462;&#21487;&#25968;&#38598;&#21512;&#30340;&#21465;&#20107;&#12290;&#20316;&#32773;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#25903;&#25345;&#35770;&#26029;&#65292;&#24182;&#21457;&#29616;&#36825;&#21487;&#20197;&#25552;&#39640;&#21465;&#20107;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#27169;&#22411;&#22312;&#20381;&#36182;&#21465;&#20107;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#23454;&#29992;&#20215;&#20540;&#65292;&#20363;&#22914;&#20107;&#23454;&#26680;&#26597;&#12290;</title><link>http://arxiv.org/abs/2309.10359</link><description>&lt;p&gt;
&#25552;&#31034;&#12289;&#26465;&#20214;&#21644;&#29983;&#25104;&#65306;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26080;&#25903;&#25345;&#35770;&#26029;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Prompt, Condition, and Generate: Classification of Unsupported Claims with In-Context Learning. (arXiv:2309.10359v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#36890;&#36807;&#23545;&#26080;&#25903;&#25345;&#35770;&#26029;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#20013;&#25552;&#21462;&#21487;&#25968;&#38598;&#21512;&#30340;&#21465;&#20107;&#12290;&#20316;&#32773;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#25903;&#25345;&#35770;&#26029;&#65292;&#24182;&#21457;&#29616;&#36825;&#21487;&#20197;&#25552;&#39640;&#21465;&#20107;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#27169;&#22411;&#22312;&#20381;&#36182;&#21465;&#20107;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#23454;&#29992;&#20215;&#20540;&#65292;&#20363;&#22914;&#20107;&#23454;&#26680;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#36935;&#21040;&#30340;&#26080;&#25903;&#25345;&#21644;&#19981;&#21487;&#21453;&#39539;&#30340;&#35770;&#26029;&#21487;&#20197;&#24433;&#21709;&#25105;&#20204;&#23545;&#19990;&#30028;&#30340;&#30475;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#35770;&#26029;&#36827;&#34892;&#34920;&#24449;&#12289;&#24635;&#32467;&#21644;&#26356;&#19968;&#33324;&#22320;&#29702;&#35299;&#21364;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32454;&#31890;&#24230;&#30340;&#36777;&#35770;&#20027;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#36825;&#20123;&#35770;&#26029;&#20013;&#25552;&#28860;&#21487;&#25968;&#38598;&#21512;&#30340;&#21465;&#20107;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20247;&#21253;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;12&#20010;&#26377;&#20105;&#35758;&#30340;&#20027;&#39064;&#65292;&#36229;&#36807;120k&#20010;&#26469;&#33258;&#24322;&#26500;&#26469;&#28304;&#30340;&#35770;&#35777;&#12289;&#35770;&#26029;&#21644;&#35780;&#35770;&#65292;&#27599;&#20010;&#37117;&#26631;&#27880;&#26377;&#19968;&#20010;&#21465;&#20107;&#26631;&#31614;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#21512;&#25104;&#35770;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#25903;&#25345;&#30340;&#35777;&#25454;&#29983;&#25104;&#30340;&#35770;&#26029;&#21487;&#20197;&#25552;&#39640;&#21465;&#20107;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#20363;&#26469;&#25512;&#26029;&#31435;&#22330;&#21644;&#26041;&#38754;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#20381;&#36182;&#21465;&#20107;&#30340;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#20107;&#23454;&#26680;&#26597;&#65292;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupported and unfalsifiable claims we encounter in our daily lives can influence our view of the world. Characterizing, summarizing, and -- more generally -- making sense of such claims, however, can be challenging. In this work, we focus on fine-grained debate topics and formulate a new task of distilling, from such claims, a countable set of narratives. We present a crowdsourced dataset of 12 controversial topics, comprising more than 120k arguments, claims, and comments from heterogeneous sources, each annotated with a narrative label. We further investigate how large language models (LLMs) can be used to synthesise claims using In-Context Learning. We find that generated claims with supported evidence can be used to improve the performance of narrative classification models and, additionally, that the same model can infer the stance and aspect using a few training examples. Such a model can be useful in applications which rely on narratives , e.g. fact-checking.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20855;&#22791;&#19982;&#20154;&#31867;&#19987;&#23478;&#30456;&#20284;&#30340;&#24110;&#21161;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10346</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#26234;&#33021;&#20307;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Explaining Agent Behavior with Large Language Models. (arXiv:2309.10346v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10346
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20855;&#22791;&#19982;&#20154;&#31867;&#19987;&#23478;&#30456;&#20284;&#30340;&#24110;&#21161;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20307;&#22914;&#26426;&#22120;&#20154;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37096;&#32626;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#33021;&#22815;&#21521;&#20154;&#31867;&#23545;&#31561;&#20307;&#35299;&#37322;&#20182;&#20204;&#20915;&#31574;&#32972;&#21518;&#30340;&#25512;&#29702;&#65292;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#34892;&#20026;&#36890;&#24120;&#26159;&#30001;&#19981;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#35266;&#23519;&#65292;&#19981;&#32771;&#34385;&#24213;&#23618;&#27169;&#22411;&#34920;&#31034;&#65292;&#29983;&#25104;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#31616;&#27905;&#34920;&#31034;&#65292;&#24182;&#29992;&#20854;&#29983;&#25104;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#19982;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#20132;&#20114;&#12290;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#21644;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#37322;&#19982;&#20154;&#31867;&#39046;&#22495;&#19987;&#23478;&#29983;&#25104;&#30340;&#35299;&#37322;&#19968;&#26679;&#26377;&#29992;&#65292;&#21516;&#26102;&#20855;&#22791;&#26377;&#30410;&#30340;&#20132;&#20114;&#65292;&#22914;&#28548;&#28165;&#21644;&#21453;&#20107;&#23454;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents such as robots are increasingly deployed in real-world, safety-critical settings. It is vital that these agents are able to explain the reasoning behind their decisions to human counterparts, however, their behavior is often produced by uninterpretable models such as deep neural networks. We propose an approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, agnostic to the underlying model representation. We show how a compact representation of the agent's behavior can be learned and used to produce plausible explanations with minimal hallucination while affording user interaction with a pre-trained large language model. Through user studies and empirical experiments, we show that our approach generates explanations as helpful as those generated by a human domain expert while enabling beneficial interactions such as clarification and counterfactual queries.
&lt;/p&gt;</description></item><item><title>KoBigBird-large&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#38889;&#35821;&#29702;&#35299;&#30340;&#22823;&#22411;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#24341;&#20837;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#34920;&#31034;&#65288;TAPER&#65289;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#25991;&#26723;&#20998;&#31867;&#21644;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.10339</link><description>&lt;p&gt;
KoBigBird-large: Transformer&#27169;&#22411;&#22312;&#38889;&#35821;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
KoBigBird-large: Transformation of Transformer for Korean Language Understanding. (arXiv:2309.10339v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10339
&lt;/p&gt;
&lt;p&gt;
KoBigBird-large&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#38889;&#35821;&#29702;&#35299;&#30340;&#22823;&#22411;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#24341;&#20837;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#34920;&#31034;&#65288;TAPER&#65289;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#25991;&#26723;&#20998;&#31867;&#21644;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;KoBigBird-large&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#38889;&#35821;&#29702;&#35299;&#30340;&#22823;&#22411;Transformer&#27169;&#22411;&#65292;&#23427;&#22312;&#24615;&#33021;&#21644;&#22788;&#29702;&#38271;&#24207;&#21015;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26550;&#26500;&#36827;&#34892;&#25913;&#36827;&#24182;&#24341;&#20837;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#38181;&#24418;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#34920;&#31034;&#65288;TAPER&#65289;&#65292;&#22312;&#27809;&#26377;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#25104;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;KoBigBird-large &#22312;&#38889;&#35821;&#29702;&#35299;&#22522;&#20934;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20840;&#37096;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#38271;&#24207;&#21015;&#30340;&#25991;&#26723;&#20998;&#31867;&#21644;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#27492;&#20844;&#24320;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents KoBigBird-large, a large size of Korean BigBird that achieves state-of-the-art performance and allows long sequence processing for Korean language understanding. Without further pretraining, we only transform the architecture and extend the positional encoding with our proposed Tapered Absolute Positional Encoding Representations (TAPER). In experiments, KoBigBird-large shows state-of-the-art overall performance on Korean language understanding benchmarks and the best performance on document classification and question answering tasks for longer sequences against the competitive baseline models. We publicly release our model here.
&lt;/p&gt;</description></item><item><title>QASnowball&#26159;&#19968;&#20010;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#26377;&#30417;&#30563;&#30340;&#26679;&#26412;&#31181;&#23376;&#38598;&#29983;&#25104;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#31181;&#23376;&#21270;&#36827;&#34892;&#33258;&#25105;&#22686;&#24378;&#12290;&#22312;&#39640;&#36164;&#28304;&#33521;&#25991;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.10326</link><description>&lt;p&gt;
QASnowball: &#19968;&#20010;&#29992;&#20110;&#39640;&#36136;&#37327;&#38382;&#31572;&#25968;&#25454;&#29983;&#25104;&#30340;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation. (arXiv:2309.10326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10326
&lt;/p&gt;
&lt;p&gt;
QASnowball&#26159;&#19968;&#20010;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#26377;&#30417;&#30563;&#30340;&#26679;&#26412;&#31181;&#23376;&#38598;&#29983;&#25104;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#31181;&#23376;&#21270;&#36827;&#34892;&#33258;&#25105;&#22686;&#24378;&#12290;&#22312;&#39640;&#36164;&#28304;&#33521;&#25991;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#20854;&#22312;&#24212;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#31283;&#23450;&#30340;QA&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;QASnowball&#65292;&#29992;&#20110;QA&#25968;&#25454;&#22686;&#24378;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#26377;&#30417;&#30563;&#30340;&#26679;&#26412;&#31181;&#23376;&#38598;&#36845;&#20195;&#22320;&#29983;&#25104;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;QASnowball&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#22238;&#31572;&#25552;&#21462;&#22120;&#65292;&#29992;&#20110;&#20174;&#26080;&#26631;&#31614;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#20505;&#36873;&#31572;&#26696;&#30340;&#26680;&#24515;&#30701;&#35821;&#65307;&#38382;&#39064;&#29983;&#25104;&#22120;&#65292;&#26681;&#25454;&#25991;&#26723;&#21644;&#20505;&#36873;&#31572;&#26696;&#29983;&#25104;&#38382;&#39064;&#65307;QA&#25968;&#25454;&#36807;&#28388;&#22120;&#65292;&#29992;&#20110;&#36807;&#28388;&#20986;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;QASnowball&#21487;&#20197;&#36890;&#36807;&#37325;&#26032;&#31181;&#23376;&#21270;&#31181;&#23376;&#38598;&#22312;&#19981;&#21516;&#36845;&#20195;&#20013;&#36827;&#34892;&#33258;&#25105;&#22686;&#24378;&#65292;&#20174;&#32780;&#19981;&#26029;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#39640;&#36164;&#28304;&#33521;&#25991;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the success of question answering (QA), especially its potential to be a foundation paradigm for tackling diverse NLP tasks. However, obtaining sufficient data to build an effective and stable QA system still remains an open problem. For this problem, we introduce an iterative bootstrapping framework for QA data augmentation (named QASnowball), which can iteratively generate large-scale high-quality QA data based on a seed set of supervised examples. Specifically, QASnowball consists of three modules, an answer extractor to extract core phrases in unlabeled documents as candidate answers, a question generator to generate questions based on documents and candidate answers, and a QA data filter to filter out high-quality QA data. Moreover, QASnowball can be self-enhanced by reseeding the seed set to fine-tune itself in different iterations, leading to continual improvements in the generation quality. We conduct experiments in the high-resource English scenario
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.10313</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#36827;&#34892;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;GPT4&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#20391;&#37325;&#20110;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#35270;&#35273;&#27169;&#22411;&#26469;&#24320;&#21457;&#36890;&#29992;&#30340;LLM&#12290;&#28982;&#32780;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24494;&#35843;&#27169;&#22411;&#26080;&#27861;&#20445;&#25345;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#20173;&#28982;&#26159;&#22810;&#27169;&#24577;LLM&#65288;MLLM&#65289;&#20013;&#30340;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EMT&#65306;&#29992;&#20110;&#35780;&#20272;MLLM&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;MLLM&#20316;&#20026;&#19968;&#20010;&#22270;&#20687;&#20998;&#31867;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;EMT&#26469;&#35780;&#20272;&#20960;&#20010;&#24320;&#28304;&#30340;&#24494;&#35843;MLLM&#65292;&#24182;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;MLLM&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#26080;&#27861;&#20445;&#25345;&#19982;&#20182;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32487;&#32493;&#24494;&#35843;LLaVA&#65292;&#19968;&#31181;MLLM&#65292;&#24182;&#21033;&#29992;EMT&#26469;&#35780;&#20272;&#25972;&#20010;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#30340;&#24494;&#35843;&#38454;&#27573;&#26159;&#20851;&#38190;&#30340;&#65292;&#36807;&#26089;&#20572;&#27490;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#20302;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#20004;&#31181;&#27169;&#24335;&#26469;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#31070;&#32463;&#20803;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#19988;&#24212;&#29992;&#20110;GPT-4&#29983;&#25104;&#30340;GPT-2 XL&#31070;&#32463;&#20803;&#35299;&#37322;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;&#26159;&#33258;&#20449;&#24230;&#26368;&#39640;&#30340;&#35299;&#37322;&#20063;&#23384;&#22312;&#36739;&#39640;&#30340;&#38169;&#35823;&#29575;&#21644;&#20960;&#20046;&#27809;&#26377;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.10312</link><description>&lt;p&gt;
&#20005;&#26684;&#35780;&#20272;&#31070;&#32463;&#20803;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Rigorously Assessing Natural Language Explanations of Neurons. (arXiv:2309.10312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10312
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#20004;&#31181;&#27169;&#24335;&#26469;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#31070;&#32463;&#20803;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#19988;&#24212;&#29992;&#20110;GPT-4&#29983;&#25104;&#30340;GPT-2 XL&#31070;&#32463;&#20803;&#35299;&#37322;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;&#26159;&#33258;&#20449;&#24230;&#26368;&#39640;&#30340;&#35299;&#37322;&#20063;&#23384;&#22312;&#36739;&#39640;&#30340;&#38169;&#35823;&#29575;&#21644;&#20960;&#20046;&#27809;&#26377;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#35299;&#37322;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#21644;&#23384;&#20648;&#20449;&#24687;&#30340;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26041;&#24335;&#65292;&#20294;&#35780;&#20272;&#36825;&#31181;&#35299;&#37322;&#30340;&#24544;&#23454;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#27169;&#24335;&#65292;&#36825;&#20123;&#35299;&#37322;&#22768;&#31216;&#20010;&#21035;&#31070;&#32463;&#20803;&#20195;&#34920;&#25991;&#26412;&#36755;&#20837;&#20013;&#30340;&#19968;&#20010;&#27010;&#24565;&#12290;&#22312;&#35266;&#23519;&#27169;&#24335;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#31070;&#32463;&#20803;$a$&#20165;&#24182;&#19988;&#23436;&#20840;&#28608;&#27963;&#19982;&#25152;&#25552;&#20986;&#30340;&#35299;&#37322;$E$&#25152;&#25351;&#20195;&#30340;&#27010;&#24565;&#30456;&#20851;&#30340;&#25152;&#26377;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#35828;&#27861;&#12290;&#22312;&#24178;&#39044;&#27169;&#24335;&#20013;&#65292;&#25105;&#20204;&#23558;$E$&#35299;&#37322;&#20026;&#31070;&#32463;&#20803;$a$&#26159;&#30001;$E$&#25152;&#34920;&#31034;&#30340;&#27010;&#24565;&#30340;&#22240;&#26524;&#20013;&#20171;&#32773;&#30340;&#35828;&#27861;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;Bills&#31561;&#20154;&#65288;2023&#24180;&#65289;&#23545;GPT-2 XL&#31070;&#32463;&#20803;&#30340;GPT-4&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#24182;&#26174;&#31034;&#21363;&#20351;&#26159;&#26368;&#33258;&#20449;&#30340;&#35299;&#37322;&#20063;&#26377;&#24456;&#39640;&#30340;&#38169;&#35823;&#29575;&#21644;&#20960;&#20046;&#27809;&#26377;&#22240;&#26524;&#25928;&#24212;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#33258;&#28982;&#35821;&#35328;&#26159;&#21542;&#26159;&#19968;&#20010;&#22909;&#30340;&#35299;&#37322;&#36873;&#25321;&#20197;&#21450;&#31070;&#32463;&#20803;&#26159;&#21542;&#26159;&#26368;&#22909;&#30340;&#20998;&#26512;&#23618;&#27425;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language is an appealing medium for explaining how large language models process and store information, but evaluating the faithfulness of such explanations is challenging. To help address this, we develop two modes of evaluation for natural language explanations that claim individual neurons represent a concept in a text input. In the observational mode, we evaluate claims that a neuron $a$ activates on all and only input strings that refer to a concept picked out by the proposed explanation $E$. In the intervention mode, we construe $E$ as a claim that the neuron $a$ is a causal mediator of the concept denoted by $E$. We apply our framework to the GPT-4-generated explanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the most confident explanations have high error rates and little to no causal efficacy. We close the paper by critically assessing whether natural language is a good choice for explanations and whether neurons are the best level of analysis.
&lt;/p&gt;</description></item><item><title>Baichuan 2&#26159;&#19968;&#31995;&#21015;&#24320;&#25918;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25317;&#26377;70&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#35757;&#32451;&#33258;26&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;Baichuan 2&#22312;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22402;&#30452;&#39046;&#22495;&#22914;&#21307;&#23398;&#21644;&#27861;&#24459;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.10305</link><description>&lt;p&gt;
Baichuan 2: &#24320;&#25918;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Baichuan 2: Open Large-scale Language Models. (arXiv:2309.10305v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10305
&lt;/p&gt;
&lt;p&gt;
Baichuan 2&#26159;&#19968;&#31995;&#21015;&#24320;&#25918;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25317;&#26377;70&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#35757;&#32451;&#33258;26&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;Baichuan 2&#22312;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22402;&#30452;&#39046;&#22495;&#22914;&#21307;&#23398;&#21644;&#27861;&#24459;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20165;&#26377;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#20943;&#23569;&#20102;&#23545;&#24191;&#27867;&#29305;&#24449;&#24037;&#31243;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24378;&#22823;&#30340;LLMs&#26159;&#23553;&#38381;&#28304;&#20195;&#30721;&#30340;&#65292;&#25110;&#32773;&#22312;&#38500;&#20102;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#36825;&#31687;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Baichuan 2&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#21547;70&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#20351;&#29992;26&#19975;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892;&#35757;&#32451;&#12290;Baichuan 2&#22312;MMLU&#12289;CMMLU&#12289;GSM8K&#21644;HumanEval&#31561;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#20854;&#20182;&#30456;&#21516;&#35268;&#27169;&#30340;&#24320;&#28304;&#27169;&#22411;&#30456;&#21305;&#37197;&#25110;&#32988;&#36807;&#12290;&#27492;&#22806;&#65292;Baichuan 2&#22312;&#21307;&#23398;&#21644;&#27861;&#24459;&#31561;&#22402;&#30452;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;&#25152;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#26816;&#26597;&#28857;&#65292;&#20197;&#20351;&#30740;&#31350;&#30028;&#26356;&#22909;&#22320;&#29702;&#35299;Baichuan 2&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#24494;&#35843;&#21644;&#26368;&#23567;&#20808;&#34892;&#25628;&#32034;&#31639;&#27861;&#26469;&#25913;&#36827;Whisper&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#26368;&#23567;&#20808;&#34892;&#25628;&#32034;&#20248;&#20110;&#26631;&#20934;&#26463;&#25628;&#32034;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.10299</link><description>&lt;p&gt;
&#20351;&#29992;&#24494;&#35843;&#21644;&#26368;&#23567;&#20808;&#34892;&#25628;&#32034;&#26469;&#25552;&#39640;Whisper
&lt;/p&gt;
&lt;p&gt;
Using fine-tuning and min lookahead beam search to improve Whisper. (arXiv:2309.10299v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10299
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24494;&#35843;&#21644;&#26368;&#23567;&#20808;&#34892;&#25628;&#32034;&#31639;&#27861;&#26469;&#25913;&#36827;Whisper&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#26368;&#23567;&#20808;&#34892;&#25628;&#32034;&#20248;&#20110;&#26631;&#20934;&#26463;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Whisper&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#36828;&#31163;&#23436;&#32654;&#12290;&#38500;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;Whisper&#20013;&#20351;&#29992;&#30340;&#26463;&#25628;&#32034;&#31639;&#27861;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#39069;&#22806;&#30340;&#25968;&#25454;&#19978;&#23545;Whisper&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#22312;&#36234;&#21335;&#35821;&#19978;&#65292;&#20351;&#29992;LoRA&#23545;Whisper-Tiny&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#23558;WER&#30340;&#25913;&#36827;&#25552;&#39640;38.49&#65292;&#30456;&#27604;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#65292;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;1.45&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;Filter-Ends&#21644;Min Lookahead&#35299;&#30721;&#31639;&#27861;&#65292;&#19982;&#26631;&#20934;&#26463;&#25628;&#32034;&#30456;&#27604;&#65292;WER&#22312;&#19968;&#31995;&#21015;&#35821;&#35328;&#20013;&#24179;&#22343;&#38477;&#20302;&#20102;2.26&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;Whisper&#27169;&#22411;&#23610;&#23544;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;Min Lookahead&#20248;&#20110;Whisper&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#26463;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of Whisper in low-resource languages is still far from perfect. In addition to a lack of training data on low-resource languages, we identify some limitations in the beam search algorithm used in Whisper. To address these issues, we fine-tune Whisper on additional data and propose an improved decoding algorithm. On the Vietnamese language, fine-tuning Whisper-Tiny with LoRA leads to an improvement of 38.49 in WER over the zero-shot Whisper-Tiny setting which is a further reduction of 1.45 compared to full-parameter fine-tuning. Additionally, by using Filter-Ends and Min Lookahead decoding algorithms, the WER reduces by 2.26 on average over a range of languages compared to standard beam search. These results generalise to larger Whisper model sizes. We also prove a theorem that Min Lookahead outperforms the standard beam search algorithm used in Whisper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#35821;&#38899;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#21644;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;&#25552;&#21319;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10294</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#38899;PTM&#12289;&#25991;&#26412;LLM&#21644;&#24773;&#24863;TTS&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion Recognition. (arXiv:2309.10294v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#35821;&#38899;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#21644;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;&#25552;&#21319;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#39044;&#35757;&#32451;&#27169;&#22411;(PTM)&#12289;data2vec&#12289;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;GPT-4&#20197;&#21450;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;Azure TTS&#26469;&#25552;&#21319;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;(SER)&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21457;&#29616;data2vec&#22312;SER&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(GPT-4)&#21644;&#24773;&#24863;&#25991;&#26412;&#21040;&#35821;&#38899;(TTS)&#27169;&#22411;(Azure TTS)&#26469;&#29983;&#25104;&#24773;&#24863;&#19968;&#33268;&#30340;&#25991;&#26412;&#21644;&#35821;&#38899;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#25991;&#26412;&#25552;&#31034;&#21644;&#25968;&#25454;&#38598;&#26500;&#24314;&#65292;&#20197;&#33719;&#24471;&#36136;&#37327;&#39640;&#30340;&#21512;&#25104;&#24773;&#24863;&#35821;&#38899;&#25968;&#25454;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#20419;&#36827;&#20351;&#29992;&#21512;&#25104;&#35821;&#38899;&#30340;SER&#20219;&#21153;&#65292;&#21253;&#25324;&#38543;&#26426;&#28151;&#21512;&#12289;&#23545;&#25239;&#35757;&#32451;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#35838;&#31243;&#23398;&#20064;&#12290;&#22312;IEMOCAP&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explored how to boost speech emotion recognition (SER) with the state-of-the-art speech pre-trained model (PTM), data2vec, text generation technique, GPT-4, and speech synthesis technique, Azure TTS. First, we investigated the representation ability of different speech self-supervised pre-trained models, and we found that data2vec has a good representation ability on the SER task. Second, we employed a powerful large language model (LLM), GPT-4, and emotional text-to-speech (TTS) model, Azure TTS, to generate emotionally congruent text and speech. We carefully designed the text prompt and dataset construction, to obtain the synthetic emotional speech data with high quality. Third, we studied different ways of data augmentation to promote the SER task with synthetic speech, including random mixing, adversarial training, transfer learning, and curriculum learning. Experiments and ablation studies on the IEMOCAP dataset demonstrate the effectiveness of our method, compar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Tri-Distil-BERT&#21644;Mixed-Distil-BERT&#20004;&#20010;&#27169;&#22411;&#65292;Tri-Distil-BERT&#26159;&#19968;&#20010;&#22312;&#23391;&#21152;&#25289;&#35821;&#12289;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#19978;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;Mixed-Distil-BERT&#26159;&#19968;&#20010;&#22312;&#28151;&#21512;&#32534;&#30721;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19982;&#26356;&#22823;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10272</link><description>&lt;p&gt;
&#28151;&#21512;Distil-BERT: &#29992;&#20110;&#23391;&#21152;&#25289;&#35821;&#12289;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#30340;&#28151;&#21512;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and Hindi. (arXiv:2309.10272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Tri-Distil-BERT&#21644;Mixed-Distil-BERT&#20004;&#20010;&#27169;&#22411;&#65292;Tri-Distil-BERT&#26159;&#19968;&#20010;&#22312;&#23391;&#21152;&#25289;&#35821;&#12289;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#19978;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;Mixed-Distil-BERT&#26159;&#19968;&#20010;&#22312;&#28151;&#21512;&#32534;&#30721;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19982;&#26356;&#22823;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#25991;&#26412;&#20998;&#31867;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#19979;&#28216;&#20219;&#21153;&#20043;&#19968;&#12290;&#24403;&#25991;&#26412;&#26159;&#28151;&#21512;&#32534;&#30721;&#26102;&#65292;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#34429;&#28982;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#25509;&#35302;&#21040;&#36825;&#31181;&#25991;&#26412;&#65292;&#20294;&#19981;&#21516;&#30340;BERT&#27169;&#22411;&#24050;&#32463;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#28151;&#21512;&#32534;&#30721;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25361;&#25112;&#12290;&#20877;&#27425;&#65292;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#28151;&#21512;&#32534;&#30721;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#24050;&#32463;&#20381;&#36182;&#20110;&#23558;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#32467;&#21512;&#12290;&#24403;BERT&#27169;&#22411;&#20351;&#29992;&#30456;&#24212;&#30340;&#28151;&#21512;&#32534;&#30721;&#35821;&#35328;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#20102;&#35299;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#20102;&#24590;&#26679;&#30340;&#24433;&#21709;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Tri-Distil-BERT&#65292;&#19968;&#20010;&#22312;&#23391;&#21152;&#25289;&#35821;&#12289;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#19978;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;Mixed-Distil-BERT&#65292;&#19968;&#20010;&#22312;&#28151;&#21512;&#32534;&#30721;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20986;&#19982;&#26356;&#22823;&#30340;&#27169;&#22411;&#22914;mBERT&#21644;XLM-R&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20004;&#23618;&#39044;&#35757;&#32451;&#26041;&#27861;&#20026;&#22810;&#35821;&#35328;&#20219;&#21153;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most popular downstream tasks in the field of Natural Language Processing is text classification. Text classification tasks have become more daunting when the texts are code-mixed. Though they are not exposed to such text during pre-training, different BERT models have demonstrated success in tackling Code-Mixed NLP challenges. Again, in order to enhance their performance, Code-Mixed NLP models have depended on combining synthetic data with real-world data. It is crucial to understand how the BERT models' performance is impacted when they are pretrained using corresponding code-mixed languages. In this paper, we introduce Tri-Distil-BERT, a multilingual model pre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model fine-tuned on code-mixed data. Both models are evaluated across multiple NLP tasks and demonstrate competitive performance against larger models like mBERT and XLM-R. Our two-tiered pre-training approach offers efficient alternatives for multiling
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22312;&#24212;&#29992;&#26694;&#26550;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;</title><link>http://arxiv.org/abs/2309.10254</link><description>&lt;p&gt;
LLM&#24179;&#21488;&#23433;&#20840;&#65306;&#23558;&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#24212;&#29992;&#20110;OpenAI&#30340;ChatGPT&#25554;&#20214;
&lt;/p&gt;
&lt;p&gt;
LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins. (arXiv:2309.10254v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22312;&#24212;&#29992;&#26694;&#26550;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22914;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24179;&#21488;&#24320;&#22987;&#25552;&#20379;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#65292;&#20197;&#19982;&#20114;&#32852;&#32593;&#19978;&#30340;&#31532;&#19977;&#26041;&#26381;&#21153;&#36827;&#34892;&#20132;&#20114;&#12290;&#34429;&#28982;&#36825;&#20123;&#25554;&#20214;&#25193;&#23637;&#20102;LLM&#24179;&#21488;&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#26159;&#30001;&#20219;&#24847;&#30340;&#31532;&#19977;&#26041;&#24320;&#21457;&#30340;&#65292;&#22240;&#27492;&#19981;&#33021;&#38544;&#24335;&#20449;&#20219;&#12290;&#25554;&#20214;&#36824;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;LLM&#24179;&#21488;&#21644;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#31946;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20026;LLM&#24179;&#21488;&#35774;&#35745;&#32773;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#19968;&#20010;&#25915;&#20987;&#20998;&#31867;&#27861;&#30340;&#34920;&#36848;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;LLM&#24179;&#21488;&#30456;&#20851;&#26041;&#22914;&#20309;&#21033;&#29992;&#20182;&#20204;&#30340;&#33021;&#21147;&#21644;&#36131;&#20219;&#23545;&#24444;&#27492;&#36827;&#34892;&#25915;&#20987;&#26469;&#24320;&#21457;&#30340;&#12290;&#20316;&#20026;&#25105;&#20204;&#36845;&#20195;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM) platforms, such as ChatGPT, have recently begun offering a plugin ecosystem to interface with third-party services on the internet. While these plugins extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future plugin-integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the poten
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#21160;&#20316;&#29983;&#25104;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#35780;&#20272;&#26368;&#21563;&#21512;&#30340;&#24230;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26356;&#20248;&#24230;&#37327;&#12290;&#32467;&#26524;&#21457;&#29616;&#24403;&#21069;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#24230;&#37327;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#36739;&#20302;&#65292;&#20294;&#29992;&#20110;&#35780;&#20272;&#24179;&#22343;&#27169;&#22411;&#24615;&#33021;&#30340;&#24120;&#29992;&#24230;&#37327;&#26174;&#31034;&#20986;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10248</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#21160;&#20316;&#29983;&#25104;&#30340;&#26368;&#20339;&#33258;&#21160;&#24230;&#37327;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What is the Best Automated Metric for Text to Motion Generation?. (arXiv:2309.10248v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10248
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#21160;&#20316;&#29983;&#25104;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#35780;&#20272;&#26368;&#21563;&#21512;&#30340;&#24230;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26356;&#20248;&#24230;&#37327;&#12290;&#32467;&#26524;&#21457;&#29616;&#24403;&#21069;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#24230;&#37327;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#36739;&#20302;&#65292;&#20294;&#29992;&#20110;&#35780;&#20272;&#24179;&#22343;&#27169;&#22411;&#24615;&#33021;&#30340;&#24120;&#29992;&#24230;&#37327;&#26174;&#31034;&#20986;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#24182;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#20026;&#27492;&#20219;&#21153;&#24320;&#21457;&#26356;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19978;&#65292;&#20294;&#22312;&#30830;&#23450;&#36866;&#24403;&#30340;&#35780;&#20272;&#24230;&#37327;&#26041;&#38754;&#27809;&#26377;&#36827;&#34892;&#37325;&#35201;&#24037;&#20316;&#12290;&#20154;&#31867;&#35780;&#20272;&#26159;&#35813;&#20219;&#21153;&#30340;&#26368;&#32456;&#20934;&#30830;&#24615;&#24230;&#37327;&#26631;&#20934;&#65292;&#33258;&#21160;&#24230;&#37327;&#24212;&#19982;&#20154;&#31867;&#36136;&#37327;&#21028;&#26029;&#30456;&#20851;&#24615;&#24378;&#12290;&#30001;&#20110;&#25551;&#36848;&#19982;&#35768;&#22810;&#21160;&#20316;&#30456;&#20860;&#23481;&#65292;&#30830;&#23450;&#27491;&#30830;&#30340;&#24230;&#37327;&#23545;&#20110;&#35780;&#20272;&#21644;&#35774;&#35745;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#21738;&#20123;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#20272;&#26368;&#21563;&#21512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20043;&#26356;&#21152;&#21563;&#21512;&#30340;&#26032;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#21069;&#29992;&#20110;&#27492;&#20219;&#21153;&#30340;&#24230;&#37327;&#20013;&#27809;&#26377;&#19968;&#20010;&#22312;&#26679;&#26412;&#32423;&#21035;&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#26377;&#20013;&#31561;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35780;&#20272;&#24179;&#22343;&#27169;&#22411;&#24615;&#33021;&#65292;&#24120;&#29992;&#30340;&#24230;&#37327;&#22914;R-Precision&#21644;&#36739;&#23569;&#20351;&#29992;&#30340;&#22352;&#26631;&#35823;&#24046;&#26174;&#31034;&#20986;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in generating skeleton-based human motions from natural language descriptions. While most efforts have focused on developing better neural architectures for this task, there has been no significant work on determining the proper evaluation metric. Human evaluation is the ultimate accuracy measure for this task, and automated metrics should correlate well with human quality judgments. Since descriptions are compatible with many motions, determining the right metric is critical for evaluating and designing effective generative models. This paper systematically studies which metrics best align with human evaluations and proposes new metrics that align even better. Our findings indicate that none of the metrics currently used for this task show even a moderate correlation with human judgments on a sample level. However, for assessing average model performance, commonly used metrics such as R-Precision and less-used coordinate errors show strong correlations. Addit
&lt;/p&gt;</description></item><item><title>PolicyGPT&#26159;&#19968;&#20010;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#20998;&#26512;&#38544;&#31169;&#25919;&#31574;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#38544;&#31169;&#25919;&#31574;&#20887;&#38271;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#21644;&#27861;&#24459;&#39118;&#38505;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.10238</link><description>&lt;p&gt;
PolicyGPT: &#33258;&#21160;&#21270;&#20998;&#26512;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models. (arXiv:2309.10238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10238
&lt;/p&gt;
&lt;p&gt;
PolicyGPT&#26159;&#19968;&#20010;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#20998;&#26512;&#38544;&#31169;&#25919;&#31574;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#38544;&#31169;&#25919;&#31574;&#20887;&#38271;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#21644;&#27861;&#24459;&#39118;&#38505;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#25919;&#31574;&#26159;&#22312;&#32447;&#26381;&#21153;&#25552;&#20379;&#21830;&#21521;&#29992;&#25143;&#36890;&#30693;&#20854;&#25968;&#25454;&#25910;&#38598;&#21644;&#20351;&#29992;&#31243;&#24207;&#30340;&#20027;&#35201;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20840;&#38754;&#21644;&#20943;&#36731;&#27861;&#24459;&#39118;&#38505;&#65292;&#36825;&#20123;&#25919;&#31574;&#25991;&#20214;&#36890;&#24120;&#24456;&#20887;&#38271;&#12290;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#65292;&#29992;&#25143;&#24448;&#24448;&#30452;&#25509;&#28857;&#20987;&#21516;&#24847;&#25353;&#38062;&#32780;&#19981;&#26159;&#20180;&#32454;&#38405;&#35835;&#12290;&#36825;&#31181;&#20570;&#27861;&#20351;&#29992;&#25143;&#38754;&#20020;&#38544;&#31169;&#27844;&#38706;&#21644;&#27861;&#24459;&#38382;&#39064;&#30340;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;&#20687;ChatGPT&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20026;&#25991;&#26412;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20687;&#38544;&#31169;&#25919;&#31574;&#36825;&#26679;&#30340;&#38271;&#31687;&#25991;&#26723;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;LLM&#30740;&#31350;&#20102;&#19968;&#20010;&#21517;&#20026;PolicyGPT&#30340;&#38544;&#31169;&#25919;&#31574;&#25991;&#26412;&#20998;&#26512;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#12290;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#20102;115&#20010;&#32593;&#31449;&#30340;&#38544;&#31169;&#25919;&#31574;&#65292;&#30001;&#27861;&#24459;&#19987;&#23478;&#31934;&#24515;&#27880;&#37322;&#65292;&#23558;&#27599;&#20010;&#27573;&#33853;&#20998;&#31867;&#20026;10&#20010;&#31867;&#21035;&#20043;&#19968;&#12290;&#31532;&#20108;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#20102;304&#20010;&#28909;&#38376;&#25163;&#26426;&#24212;&#29992;&#30340;&#38544;&#31169;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy policies serve as the primary conduit through which online service providers inform users about their data collection and usage procedures. However, in a bid to be comprehensive and mitigate legal risks, these policy documents are often quite verbose. In practical use, users tend to click the Agree button directly rather than reading them carefully. This practice exposes users to risks of privacy leakage and legal issues. Recently, the advent of Large Language Models (LLM) such as ChatGPT and GPT-4 has opened new possibilities for text analysis, especially for lengthy documents like privacy policies. In this study, we investigate a privacy policy text analysis framework PolicyGPT based on the LLM. This framework was tested using two datasets. The first dataset comprises of privacy policies from 115 websites, which were meticulously annotated by legal experts, categorizing each segment into one of 10 classes. The second dataset consists of privacy policies from 304 popular mobil
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20248;&#21183;&#27169;&#22411;&#21644;&#36873;&#25321;&#24615;&#22238;&#25918;&#26469;&#31283;&#23450;RLHF&#35757;&#32451;&#30340;&#20004;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22870;&#21169;&#27450;&#39575;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#31561;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#24471;&#20998;&#21644;&#32988;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10202</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21183;&#27169;&#22411;&#21644;&#36873;&#25321;&#24615;&#22238;&#25918;&#31283;&#23450;RLHF
&lt;/p&gt;
&lt;p&gt;
Stabilizing RLHF through Advantage Model and Selective Rehearsal. (arXiv:2309.10202v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20248;&#21183;&#27169;&#22411;&#21644;&#36873;&#25321;&#24615;&#22238;&#25918;&#26469;&#31283;&#23450;RLHF&#35757;&#32451;&#30340;&#20004;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22870;&#21169;&#27450;&#39575;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#31561;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#24471;&#20998;&#21644;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#65292;&#28982;&#32780;&#65292;&#36890;&#36807;RLHF&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#20559;&#22909;&#23545;&#40784;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#25361;&#25112;&#34920;&#29616;&#20026;&#21508;&#31181;&#19981;&#31283;&#23450;&#24615;&#65292;&#27604;&#22914;&#22870;&#21169;&#27450;&#39575;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#36825;&#20010;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#39033;&#21019;&#26032;&#26469;&#31283;&#23450;RLHF&#35757;&#32451;: 1) &#20248;&#21183;&#27169;&#22411;&#65292;&#30452;&#25509;&#24314;&#27169;&#20248;&#21183;&#24471;&#20998;&#65292;&#21363;&#30456;&#23545;&#20110;&#26399;&#26395;&#22870;&#21169;&#30340;&#39069;&#22806;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#35843;&#33410;&#24471;&#20998;&#20998;&#24067;&#26469;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#12290;2) &#36873;&#25321;&#24615;&#22238;&#25918;&#65292;&#36890;&#36807;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#25968;&#25454;&#36827;&#34892;PPO&#35757;&#32451;&#21644;&#30693;&#35782;&#22238;&#25918;&#65292;&#20174;&#32780;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#21644;&#19987;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;RLHF&#35757;&#32451;&#20013;&#22686;&#21152;&#20102;&#31283;&#23450;&#24615;&#65292;&#32780;&#19988;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#24471;&#20998;&#21644;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing, yet aligning these models with human values and preferences using RLHF remains a significant challenge. This challenge is characterized by various instabilities, such as reward hacking and catastrophic forgetting. In this technical report, we propose two innovations to stabilize RLHF training: 1) Advantage Model, which directly models advantage score i.e., extra reward compared to the expected rewards and regulates score distributions across tasks to prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic forgetting by strategically selecting data for PPO training and knowledge rehearsing. Our experimental analysis on public and proprietary datasets reveals that the proposed methods not only increase stability in RLHF training but also achieve higher reward scores and win rates.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22914;&#20309;&#35780;&#20272;&#38899;&#20048;&#20135;&#21697;&#20013;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24207;&#25968;&#32422;&#26463;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10182</link><description>&lt;p&gt;
&#38899;&#20048;&#20135;&#21697;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Positive and Risky Message Assessment for Music Products. (arXiv:2309.10182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10182
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22914;&#20309;&#35780;&#20272;&#38899;&#20048;&#20135;&#21697;&#20013;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24207;&#25968;&#32422;&#26463;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#35780;&#20272;&#38899;&#20048;&#20135;&#21697;&#20013;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#35282;&#24230;&#22810;&#32423;&#38899;&#20048;&#20869;&#23481;&#35780;&#20272;&#30340;&#22522;&#20934;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22810;&#20219;&#21153;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24207;&#25968;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#26126;&#26174;&#20248;&#20110;&#24378;&#22823;&#30340;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#23545;&#24212;&#26041;&#27861;&#65292;&#32780;&#19988;&#21487;&#20197;&#21516;&#26102;&#35780;&#20272;&#22810;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel research problem: assessing positive and risky messages from music products. We first establish a benchmark for multi-angle multi-level music content assessment and then present an effective multi-task prediction model with ordinality-enforcement to solve this problem. Our result shows the proposed method not only significantly outperforms strong task-specific counterparts but can concurrently evaluate multiple aspects.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;LLMs&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;&#22788;&#29702;&#35821;&#22659;&#35805;&#35821;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#37325;&#20889;-&#35299;&#26512;&#33539;&#24335;&#20316;&#20026;&#35299;&#26512;&#20934;&#30830;&#24615;&#12289;&#27880;&#37322;&#25104;&#26412;&#21644;&#38169;&#35823;&#31867;&#22411;&#20840;&#38754;&#32771;&#34385;&#26102;&#26368;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.10168</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#36866;&#24212;&#35821;&#22659;&#35805;&#35821;&#30340;LLMs&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Adaptation for Parsing Contextual Utterances with LLMs. (arXiv:2309.10168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;LLMs&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;&#22788;&#29702;&#35821;&#22659;&#35805;&#35821;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#37325;&#20889;-&#35299;&#26512;&#33539;&#24335;&#20316;&#20026;&#35299;&#26512;&#20934;&#30830;&#24615;&#12289;&#27880;&#37322;&#25104;&#26412;&#21644;&#38169;&#35823;&#31867;&#22411;&#20840;&#38754;&#32771;&#34385;&#26102;&#26368;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;&#22788;&#29702;&#35821;&#22659;&#35805;&#35821;&#30340;&#33021;&#21147;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#30001;&#20110;&#27880;&#37322;&#25104;&#26412;&#65292;&#36890;&#24120;&#21482;&#23384;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#37322;&#35821;&#22659;&#35805;&#35821;&#65292;&#23548;&#33268;&#19982;&#38750;&#35821;&#22659;&#35805;&#35821;&#30456;&#27604;&#19981;&#24179;&#34913;&#12290;&#22240;&#27492;&#65292;&#35299;&#26512;&#22120;&#24517;&#39035;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#26469;&#36866;&#24212;&#35821;&#22659;&#35805;&#35821;&#12290;&#25105;&#20204;&#22312;&#23545;&#35805;&#24335;&#35821;&#20041;&#35299;&#26512;&#20013;&#32771;&#23519;&#20102;&#22235;&#31181;&#20027;&#35201;&#30340;&#33539;&#24335;&#65292;&#21363;&#24102;&#26377;&#35805;&#35821;&#21382;&#21490;&#30340;&#35299;&#26512;&#12289;&#24102;&#26377;&#21442;&#32771;&#31243;&#24207;&#30340;&#35299;&#26512;&#12289;&#35299;&#26512;-&#35299;&#20915;&#12289;&#37325;&#20889;-&#35299;&#26512;&#12290;&#20026;&#20102;&#20419;&#36827;&#36328;&#33539;&#24335;&#27604;&#36739;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;SMCalFlow-EventQueries&#65292;&#36825;&#26159;&#20174;SMCalFlow&#20013;&#36873;&#25321;&#30340;&#24102;&#26377;&#38468;&#21152;&#27880;&#37322;&#30340;&#35821;&#22659;&#31034;&#20363;&#30340;&#23376;&#38598;&#12290;&#22312;&#35821;&#22659;&#23398;&#20064;&#21644;&#24494;&#35843;&#30340;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;&#37325;&#20889;-&#35299;&#26512;&#26159;&#22312;&#20840;&#38754;&#32771;&#34385;&#35299;&#26512;&#20934;&#30830;&#24615;&#12289;&#27880;&#37322;&#25104;&#26412;&#21644;&#38169;&#35823;&#31867;&#22411;&#31561;&#22240;&#32032;&#26102;&#26368;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluate the ability of semantic parsers based on large language models (LLMs) to handle contextual utterances. In real-world settings, there typically exists only a limited number of annotated contextual utterances due to annotation cost, resulting in an imbalance compared to non-contextual utterances. Therefore, parsers must adapt to contextual utterances with a few training examples. We examine four major paradigms for doing so in conversational semantic parsing i.e., Parse-with-Utterance-History, Parse-with-Reference-Program, Parse-then-Resolve, and Rewrite-then-Parse. To facilitate such cross-paradigm comparisons, we construct SMCalFlow-EventQueries, a subset of contextual examples from SMCalFlow with additional annotations. Experiments with in-context learning and fine-tuning suggest that Rewrite-then-Parse is the most promising paradigm when holistically considering parsing accuracy, annotation cost, and error types.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#24494;&#35843;&#23545;&#27169;&#22411;&#22312;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26377;&#27491;&#38754;&#24433;&#21709;&#65292;&#20294;&#20250;&#25233;&#21046;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#19982;&#24494;&#35843;&#20998;&#24067;&#26368;&#25509;&#36817;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#20551;&#35774;&#35821;&#35328;&#27169;&#22411;&#20250;&#38544;&#24335;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#19988;&#24494;&#35843;&#36807;&#31243;&#20559;&#21521;&#20110;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20849;&#36717;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#23581;&#35797;&#24674;&#22797;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10105</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#25512;&#29702;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Understanding Catastrophic Forgetting in Language Models via Implicit Inference. (arXiv:2309.10105v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#24494;&#35843;&#23545;&#27169;&#22411;&#22312;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26377;&#27491;&#38754;&#24433;&#21709;&#65292;&#20294;&#20250;&#25233;&#21046;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#19982;&#24494;&#35843;&#20998;&#24067;&#26368;&#25509;&#36817;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#20551;&#35774;&#35821;&#35328;&#27169;&#22411;&#20250;&#38544;&#24335;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#19988;&#24494;&#35843;&#36807;&#31243;&#20559;&#21521;&#20110;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20849;&#36717;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#23581;&#35797;&#24674;&#22797;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#65288;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#25110;&#20174;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#31561;&#26041;&#27861;&#65289;&#26159;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#40065;&#26834;&#22320;&#25191;&#34892;&#25152;&#38656;&#20219;&#21153;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#23545;&#24494;&#35843;&#30340;&#24433;&#21709;&#30340;&#31995;&#32479;&#29702;&#35299;&#65292;&#29305;&#21035;&#26159;&#22312;&#29421;&#31364;&#30340;&#24494;&#35843;&#20998;&#24067;&#20043;&#22806;&#30340;&#20219;&#21153;&#19978;&#12290;&#22312;&#19968;&#20010;&#31616;&#21270;&#30340;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20869;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#30340;&#21516;&#26102;&#65292;&#20250;&#25233;&#21046;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#36864;&#21270;&#22312;&#19982;&#24494;&#35843;&#20998;&#24067;&#8220;&#26368;&#25509;&#36817;&#8221;&#30340;&#20219;&#21153;&#20013;&#23588;&#20026;&#26174;&#33879;&#12290;&#25105;&#20204;&#20551;&#35774;&#35821;&#35328;&#27169;&#22411;&#20250;&#38544;&#24335;&#25512;&#29702;&#20986;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#24494;&#35843;&#36807;&#31243;&#20027;&#35201;&#20559;&#21521;&#20110;&#24494;&#35843;&#20998;&#24067;&#20013;&#30340;&#20219;&#21153;&#65292;&#20197;&#27979;&#35797;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#36717;&#25552;&#31034;&#20197;&#26597;&#30475;&#26159;&#21542;&#21487;&#20197;&#24674;&#22797;&#39044;&#35757;&#32451;&#30340;&#33021;&#21147;&#12290;&#20849;&#36717;&#25552;&#31034;&#20250;&#20154;&#20026;&#22320;&#20351;&#20219;&#21153;&#30475;&#36215;&#26469;&#19982;&#24494;&#35843;&#20998;&#24067;&#36739;&#36828;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback) is a crucial step in training language models to robustly carry out tasks of interest. However, we lack a systematic understanding of the effects of fine-tuning, particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of suppressing model capabilities on other tasks. This degradation is especially pronounced for tasks "closest" to the fine-tuning distribution. We hypothesize that language models implicitly infer the task of the prompt corresponds, and the fine-tuning process predominantly skews this task inference towards tasks in the fine-tuning distribution. To test this hypothesis, we propose Conjugate Prompting to see if we can recover pretrained capabilities. Conjugate prompting artificially makes the task look farther from the fine-tun
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#31895;&#21040;&#32454;&#23545;&#40784;&#27169;&#22411;UCoFiA&#65292;&#29992;&#20110;&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#25429;&#25417;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#30456;&#20284;&#24615;&#32858;&#21512;&#27169;&#22359;&#26377;&#25928;&#32771;&#34385;&#19981;&#21516;&#35270;&#35273;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#26368;&#32456;&#35299;&#20915;&#20102;&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#31934;&#30830;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10091</link><description>&lt;p&gt;
&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#30340;&#32479;&#19968;&#31895;&#21040;&#32454;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unified Coarse-to-Fine Alignment for Video-Text Retrieval. (arXiv:2309.10091v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10091
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#31895;&#21040;&#32454;&#23545;&#40784;&#27169;&#22411;UCoFiA&#65292;&#29992;&#20110;&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#25429;&#25417;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#30456;&#20284;&#24615;&#32858;&#21512;&#27169;&#22359;&#26377;&#25928;&#32771;&#34385;&#19981;&#21516;&#35270;&#35273;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#26368;&#32456;&#35299;&#20915;&#20102;&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#31934;&#30830;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#36890;&#24120;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#20043;&#38388;&#30340;&#31895;&#31890;&#24230;&#25110;&#32454;&#31890;&#24230;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#25991;&#26412;&#26597;&#35810;&#26816;&#32034;&#27491;&#30830;&#30340;&#35270;&#39057;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33021;&#22815;&#25512;&#29702;&#20986;&#39640;&#32423;&#65288;&#22330;&#26223;&#65289;&#21644;&#20302;&#32423;&#65288;&#23545;&#35937;&#65289;&#35270;&#35273;&#32447;&#32034;&#21450;&#20854;&#19982;&#25991;&#26412;&#26597;&#35810;&#30340;&#20851;&#31995;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UCoFiA&#30340;&#32479;&#19968;&#31895;&#21040;&#32454;&#23545;&#40784;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#25429;&#25417;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#20449;&#24687;&#12290;&#20026;&#20943;&#36731;&#26080;&#20851;&#35270;&#35273;&#32447;&#32034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#20132;&#20114;&#24335;&#30456;&#20284;&#24615;&#32858;&#21512;&#27169;&#22359;&#65288;ISA&#65289;&#26469;&#32771;&#34385;&#19981;&#21516;&#35270;&#35273;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#32858;&#21512;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#20197;&#33719;&#24471;&#27599;&#20010;&#31890;&#24230;&#30340;&#30456;&#20284;&#24230;&#24471;&#20998;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;Sinkhorn-Knopp&#31639;&#27861;&#23545;&#27599;&#20010;&#32423;&#21035;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;&#26631;&#20934;&#21270;&#65292;&#20197;&#20943;&#36731;&#19981;&#21516;&#32423;&#21035;&#19978;&#30340;&#36807;&#24230;&#25110;&#19981;&#36275;&#34920;&#31034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The canonical approach to video-text retrieval leverages a coarse-grained or fine-grained alignment between visual and textual information. However, retrieving the correct video according to the text query is often challenging as it requires the ability to reason about both high-level (scene) and low-level (object) visual clues and how they relate to the text query. To this end, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA. Specifically, our model captures the cross-modal similarity information at different granularity levels. To alleviate the effect of irrelevant visual clues, we also apply an Interactive Similarity Aggregation module (ISA) to consider the importance of different visual features while aggregating the cross-modal similarity to obtain a similarity score for each granularity. Finally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of each level before summing them, alleviating over- and under-representation issues at different l
&lt;/p&gt;</description></item><item><title>HTEC&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#31867;&#36716;&#24405;&#38169;&#35823;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#38169;&#35823;&#26816;&#27979;&#21644;&#22635;&#20805;&#20004;&#20010;&#38454;&#27573;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#20462;&#27491;&#25805;&#20316;&#21015;&#34920;&#65292;&#24182;&#38024;&#23545;&#21024;&#38500;&#38169;&#35823;&#25552;&#20986;&#20102;&#22235;&#31181;&#26032;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.10089</link><description>&lt;p&gt;
HTEC: &#20154;&#31867;&#36716;&#24405;&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
HTEC: Human Transcription Error Correction. (arXiv:2309.10089v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10089
&lt;/p&gt;
&lt;p&gt;
HTEC&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#31867;&#36716;&#24405;&#38169;&#35823;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#38169;&#35823;&#26816;&#27979;&#21644;&#22635;&#20805;&#20004;&#20010;&#38454;&#27573;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#20462;&#27491;&#25805;&#20316;&#21015;&#34920;&#65292;&#24182;&#38024;&#23545;&#21024;&#38500;&#38169;&#35823;&#25552;&#20986;&#20102;&#22235;&#31181;&#26032;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#36716;&#24405;&#23545;&#20110;&#35757;&#32451;&#21644;&#25913;&#36827;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#27599;&#22686;&#21152;1%&#30340;&#36716;&#24405;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#20351;&#29992;&#36825;&#20123;&#36716;&#24405;&#26469;&#35757;&#32451;ASR&#27169;&#22411;&#23558;&#22686;&#21152;&#32422;2%&#30340;ASR WER&#12290;&#21363;&#20351;&#26159;&#32463;&#36807;&#39640;&#24230;&#22521;&#35757;&#30340;&#27880;&#37322;&#21592;&#20063;&#38590;&#20813;&#20986;&#29616;&#36716;&#24405;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#20154;&#31867;&#36716;&#24405;&#38169;&#35823;&#30340;&#20462;&#27491;&#26041;&#27861;&#12290;&#20854;&#20182;&#38382;&#39064;&#30340;&#38169;&#35823;&#20462;&#27491;&#26041;&#27861;&#65292;&#22914;ASR&#38169;&#35823;&#20462;&#27491;&#21644;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65292;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#34920;&#29616;&#19981;&#22815;&#22909;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HTEC&#29992;&#20110;&#20154;&#31867;&#36716;&#24405;&#38169;&#35823;&#20462;&#27491;&#12290;HTEC&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;Trans-Checker&#65292;&#19968;&#31181;&#38169;&#35823;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#23631;&#34109;&#38169;&#35823;&#21333;&#35789;&#65292;&#21644;Trans-Filler&#65292;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22635;&#20805;&#23631;&#34109;&#20301;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#20462;&#27491;&#25805;&#20316;&#21015;&#34920;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#22788;&#29702;&#21024;&#38500;&#38169;&#35823;&#30340;&#26032;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality human transcription is essential for training and improving Automatic Speech Recognition (ASR) models. Recent study~\cite{libricrowd} has found that every 1% worse transcription Word Error Rate (WER) increases approximately 2% ASR WER by using the transcriptions to train ASR models. Transcription errors are inevitable for even highly-trained annotators. However, few studies have explored human transcription correction. Error correction methods for other problems, such as ASR error correction and grammatical error correction, do not perform sufficiently for this problem. Therefore, we propose HTEC for Human Transcription Error Correction. HTEC consists of two stages: Trans-Checker, an error detection model that predicts and masks erroneous words, and Trans-Filler, a sequence-to-sequence generative model that fills masked positions. We propose a holistic list of correction operations, including four novel operations handling deletion errors. We further propose a variant of e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;fine-tuned&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#20010;&#24615;&#21270;&#29983;&#25104;&#20840;&#36523;PET&#25253;&#21578;&#30340;&#20934;&#30830;&#21360;&#35937;&#12290;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#24341;&#20837;&#38405;&#35835;&#21307;&#29983;&#30340;&#36523;&#20221;&#20449;&#24687;&#65292;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21307;&#29983;&#29305;&#23450;&#30340;&#25253;&#21578;&#39118;&#26684;&#12290;&#30740;&#31350;&#32467;&#26524;&#32463;&#36807;&#19987;&#23478;&#35780;&#20272;&#21644;&#26680;&#21307;&#23398;&#21307;&#29983;&#30340;&#36136;&#37327;&#35780;&#20998;&#35748;&#21487;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10066</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#20010;&#24615;&#21270;&#21360;&#35937;&#29983;&#25104;PET&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Automatic Personalized Impression Generation for PET Reports Using Large Language Models. (arXiv:2309.10066v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;fine-tuned&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#20010;&#24615;&#21270;&#29983;&#25104;&#20840;&#36523;PET&#25253;&#21578;&#30340;&#20934;&#30830;&#21360;&#35937;&#12290;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#24341;&#20837;&#38405;&#35835;&#21307;&#29983;&#30340;&#36523;&#20221;&#20449;&#24687;&#65292;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21307;&#29983;&#29305;&#23450;&#30340;&#25253;&#21578;&#39118;&#26684;&#12290;&#30740;&#31350;&#32467;&#26524;&#32463;&#36807;&#19987;&#23478;&#35780;&#20272;&#21644;&#26680;&#21307;&#23398;&#21307;&#29983;&#30340;&#36136;&#37327;&#35780;&#20998;&#35748;&#21487;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#30830;&#23450;&#36890;&#36807;fine-tuned&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#21487;&#20197;&#20026;&#20840;&#36523;PET&#25253;&#21578;&#29983;&#25104;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#21360;&#35937;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#20351;&#29992;teacher-forcing&#31639;&#27861;&#22312;PET&#25253;&#21578;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;12&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#36755;&#20837;&#26159;&#25253;&#21578;&#21457;&#29616;&#65292;&#21442;&#32771;&#26159;&#20020;&#24202;&#21360;&#35937;&#12290;&#39069;&#22806;&#30340;&#36755;&#20837;&#26631;&#35760;&#32534;&#30721;&#20102;&#38405;&#35835;&#21307;&#29983;&#30340;&#36523;&#20221;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21307;&#29983;&#29305;&#23450;&#30340;&#25253;&#21578;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#21253;&#25324;2010&#24180;&#33267;2022&#24180;&#38388;&#20174;&#25105;&#20204;&#26426;&#26500;&#25910;&#38598;&#30340;37,370&#20221;&#22238;&#39038;&#24615;PET&#25253;&#21578;&#12290;&#36890;&#36807;&#19982;&#20004;&#21517;&#26680;&#21307;&#23398;&#65288;NM&#65289;&#21307;&#29983;&#30340;&#36136;&#37327;&#35780;&#20998;&#36827;&#34892;30&#20010;&#35780;&#20272;&#25351;&#26631;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26368;&#21305;&#37197;&#30340;&#25351;&#26631;&#36873;&#25321;&#20102;&#29992;&#20110;&#19987;&#23478;&#35780;&#20272;&#30340;&#27169;&#22411;&#12290;&#22312;&#37096;&#20998;&#25968;&#25454;&#23376;&#38598;&#20013;&#65292;&#26681;&#25454;6&#20010;&#36136;&#37327;&#32500;&#24230;&#21644;&#19968;&#20010;&#24635;&#20307;&#23454;&#29992;&#24615;&#35780;&#20998;&#65288;5&#20998;&#21046;&#65289;&#65292;&#19977;&#21517;&#26680;&#21307;&#23398;&#21307;&#29983;&#35780;&#20272;&#20102;&#27169;&#22411;&#29983;&#25104;&#30340;&#21360;&#35937;&#21644;&#21407;&#22987;&#20020;&#24202;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: To determine if fine-tuned large language models (LLMs) can generate accurate, personalized impressions for whole-body PET reports. Materials and Methods: Twelve language models were trained on a corpus of PET reports using the teacher-forcing algorithm, with the report findings as input and the clinical impressions as reference. An extra input token encodes the reading physician's identity, allowing models to learn physician-specific reporting styles. Our corpus comprised 37,370 retrospective PET reports collected from our institution between 2010 and 2022. To identify the best LLM, 30 evaluation metrics were benchmarked against quality scores from two nuclear medicine (NM) physicians, with the most aligned metrics selecting the model for expert evaluation. In a subset of data, model-generated impressions and original clinical impressions were assessed by three NM physicians according to 6 quality dimensions and an overall utility score (5-point scale). Each physician reviewe
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23618;&#32423;&#26500;&#24314;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#29983;&#25104;&#30340;&#22823;&#37327;&#23383;&#31526;&#20018;&#36827;&#34892;&#32452;&#32455;&#21644;&#23548;&#33322;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21307;&#30103;&#20449;&#24687;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2309.10057</link><description>&lt;p&gt;
&#23618;&#32423;&#26500;&#24314;&#22120;&#65306;&#23558;&#25991;&#26412;&#29255;&#27573;&#32452;&#32455;&#25104;&#23618;&#27425;&#32467;&#26500;&#20197;&#20419;&#36827;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Hierarchy Builder: Organizing Textual Spans into a Hierarchy to Facilitate Navigation. (arXiv:2309.10057v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10057
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23618;&#32423;&#26500;&#24314;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#29983;&#25104;&#30340;&#22823;&#37327;&#23383;&#31526;&#20018;&#36827;&#34892;&#32452;&#32455;&#21644;&#23548;&#33322;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21307;&#30103;&#20449;&#24687;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#36890;&#24120;&#20250;&#20135;&#29983;&#25968;&#30334;&#21040;&#25968;&#21315;&#20010;&#20851;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#23383;&#31526;&#20018;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20197;&#22312;&#25506;&#32034;&#24615;&#29615;&#22659;&#20013;&#26356;&#22909;&#22320;&#28040;&#36153;&#36825;&#20123;&#23383;&#31526;&#20018;&#65292;&#20854;&#20013;&#29992;&#25143;&#26082;&#24819;&#33719;&#24471;&#24191;&#27867;&#30340;&#27010;&#36848;&#65292;&#21448;&#24819;&#26377;&#26426;&#20250;&#28145;&#20837;&#30740;&#31350;&#26576;&#20123;&#26041;&#38754;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#23558;&#30456;&#20284;&#30340;&#39033;&#30446;&#20998;&#32452;&#24182;&#23558;&#21097;&#20313;&#39033;&#30446;&#25353;&#29031;&#23618;&#27425;&#21270;&#21487;&#23548;&#33322;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#32467;&#26500;&#36827;&#34892;&#25490;&#21015;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#21307;&#30103;&#20449;&#24687;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information extraction systems often produce hundreds to thousands of strings on a specific topic. We present a method that facilitates better consumption of these strings, in an exploratory setting in which a user wants to both get a broad overview of what's available, and a chance to dive deeper on some aspects. The system works by grouping similar items together and arranging the remaining items into a hierarchical navigable DAG structure. We apply the method to medical information extraction.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#20998;&#31867;&#21644;&#28436;&#21270;&#24773;&#20917;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#20174;&#19987;&#19994;&#27169;&#22411;&#21040;&#36890;&#29992;&#21161;&#25163;&#30340;&#36807;&#28193;&#12290;&#23427;&#28085;&#30422;&#20102;&#20116;&#20010;&#26680;&#24515;&#20027;&#39064;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#36890;&#29992;&#21161;&#25163;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#20197;&#21450;&#25972;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#35270;&#35273;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.10020</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65306;&#20174;&#19987;&#19994;&#27169;&#22411;&#21040;&#36890;&#29992;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Multimodal Foundation Models: From Specialists to General-Purpose Assistants. (arXiv:2309.10020v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10020
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#20998;&#31867;&#21644;&#28436;&#21270;&#24773;&#20917;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#20174;&#19987;&#19994;&#27169;&#22411;&#21040;&#36890;&#29992;&#21161;&#25163;&#30340;&#36807;&#28193;&#12290;&#23427;&#28085;&#30422;&#20102;&#20116;&#20010;&#26680;&#24515;&#20027;&#39064;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#36890;&#29992;&#21161;&#25163;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#20197;&#21450;&#25972;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#35270;&#35273;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#23545;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#20998;&#31867;&#21644;&#28436;&#21270;&#30340;&#20840;&#38754;&#35843;&#26597;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20174;&#19987;&#19994;&#27169;&#22411;&#21040;&#36890;&#29992;&#21161;&#25163;&#30340;&#36807;&#28193;&#12290;&#30740;&#31350;&#33539;&#22260;&#21253;&#25324;&#20116;&#20010;&#26680;&#24515;&#20027;&#39064;&#65292;&#20998;&#20026;&#20004;&#31867;&#12290;&#31532;&#19968;&#31867;&#26159;&#24050;&#32463;&#24314;&#31435;&#36215;&#26469;&#30340;&#30740;&#31350;&#39046;&#22495;&#65306;&#20026;&#29305;&#23450;&#30446;&#30340;&#32780;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#39064;&#65306;&#23398;&#20064;&#35270;&#35273;&#39592;&#24178;&#29992;&#20110;&#35270;&#35273;&#29702;&#35299;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#31532;&#20108;&#31867;&#26159;&#26368;&#36817;&#22312;&#25506;&#32034;&#24615;&#12289;&#24320;&#25918;&#30340;&#30740;&#31350;&#39046;&#22495;&#20013;&#21462;&#24471;&#30340;&#36827;&#23637;&#65306;&#26088;&#22312;&#25198;&#28436;&#36890;&#29992;&#21161;&#25163;&#35282;&#33394;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#39064;&#65306;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21551;&#21457;&#30340;&#32479;&#19968;&#35270;&#35273;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#20197;&#21450;&#19982;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38142;&#24335;&#22810;&#27169;&#24577;&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#35835;&#32773;&#26159;&#30740;&#31350;&#20154;&#21592;&#12289;&#30740;&#31350;&#29983;&#21644;&#35745;&#31639;&#19987;&#19994;&#20154;&#22763;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from specialist models to general-purpose assistants. The research landscape encompasses five core topics, categorized into two classes. (i) We start with a survey of well-established research areas: multimodal foundation models pre-trained for specific purposes, including two topics -- methods of learning vision backbones for visual understanding and text-to-image generation. (ii) Then, we present recent advances in exploratory, open research areas: multimodal foundation models that aim to play the role of general-purpose assistants, including three topics -- unified vision models inspired by large language models (LLMs), end-to-end training of multimodal LLMs, and chaining multimodal tools with LLMs. The target audiences of the paper are researchers, graduate students, and professionals in compute
&lt;/p&gt;</description></item><item><title>SYNDICOM&#26159;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#24120;&#35782;&#30340;&#26041;&#27861;&#65292;&#21253;&#21547;&#20102;&#19968;&#20010;&#24120;&#35782;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#23545;&#35805;&#24212;&#31572;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10015</link><description>&lt;p&gt;
SYNDICOM: &#38169;&#35823;&#27880;&#20837;&#21644;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25913;&#36827;&#23545;&#35805;&#24120;&#35782;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback. (arXiv:2309.10015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10015
&lt;/p&gt;
&lt;p&gt;
SYNDICOM&#26159;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#24120;&#35782;&#30340;&#26041;&#27861;&#65292;&#21253;&#21547;&#20102;&#19968;&#20010;&#24120;&#35782;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#23545;&#35805;&#24212;&#31572;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#25512;&#29702;&#26159;&#20154;&#31867;&#20132;&#27969;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24120;&#35782;&#25512;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SYNDICOM - &#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#24212;&#31572;&#29983;&#25104;&#20013;&#24120;&#35782;&#30340;&#26041;&#27861;&#12290;SYNDICOM&#30001;&#20004;&#20010;&#37096;&#20998;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#32452;&#20214;&#26159;&#19968;&#20010;&#30001;&#30693;&#35782;&#22270;&#21019;&#24314;&#30340;&#24120;&#35782;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#24182;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#21512;&#25104;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#23545;&#35805;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#21644;&#26080;&#25928;&#22238;&#31572;&#65292;&#20197;&#21450;&#23545;&#26080;&#25928;&#22238;&#31572;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65288;NLF&#65289;&#12290;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#19968;&#20010;&#20004;&#27493;&#30340;&#36807;&#31243;&#65306;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#26080;&#25928;&#22238;&#31572;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65288;NLF&#65289;&#65292;&#28982;&#21518;&#26681;&#25454;&#39044;&#27979;&#30340;NLF&#12289;&#26080;&#25928;&#22238;&#31572;&#21644;&#23545;&#35805;&#26465;&#20214;&#35757;&#32451;&#19968;&#20010;&#24212;&#31572;&#29983;&#25104;&#27169;&#22411;&#12290;SYNDICOM&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#65292;&#19981;&#38656;&#35201;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#19977;&#20010;&#20219;&#21153;&#30340;&#32463;&#39564;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense reasoning is a critical aspect of human communication. Despite recent advances in conversational AI driven by large language models, commonsense reasoning remains a challenging task. In this work, we introduce SYNDICOM - a method for improving commonsense in dialogue response generation. SYNDICOM consists of two components. The first component is a dataset composed of commonsense dialogues created from a knowledge graph and synthesized into natural language. This dataset includes both valid and invalid responses to dialogue contexts, along with natural language feedback (NLF) for the invalid responses. The second contribution is a two-step procedure: training a model to predict natural language feedback (NLF) for invalid responses, and then training a response generation model conditioned on the predicted NLF, the invalid response, and the dialogue. SYNDICOM is scalable and does not require reinforcement learning. Empirical results on three tasks are evaluated using a broad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.10003</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19987;&#21033;&#26435;&#35201;&#27714;&#30340;&#33539;&#22260;&#27979;&#37327;&#20026;&#35813;&#35201;&#27714;&#25152;&#21253;&#21547;&#30340;&#33258;&#20449;&#24687;&#30340;&#20498;&#25968;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20449;&#24687;&#35770;&#65292;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#32597;&#35265;&#30340;&#27010;&#24565;&#27604;&#24179;&#24120;&#30340;&#27010;&#24565;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#22240;&#20026;&#23427;&#26356;&#20196;&#20154;&#24778;&#35766;&#12290;&#33258;&#20449;&#24687;&#26159;&#20174;&#35813;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#35745;&#31639;&#24471;&#20986;&#30340;&#65292;&#20854;&#20013;&#27010;&#29575;&#26159;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20116;&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#65288;&#27599;&#20010;&#21333;&#35789;&#25110;&#23383;&#31526;&#22343;&#20174;&#22343;&#21248;&#20998;&#24067;&#20013;&#25277;&#21462;&#65289;&#21040;&#20013;&#31561;&#27169;&#22411;&#65288;&#20351;&#29992;&#24179;&#22343;&#35789;&#25110;&#23383;&#31526;&#39057;&#29575;&#65289;&#65292;&#20877;&#21040;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT2&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#33539;&#22260;&#24230;&#37327;&#20943;&#23569;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#65292;&#36825;&#26159;&#20808;&#21069;&#20316;&#21697;&#20013;&#24050;&#32463;&#20351;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20061;&#20010;&#31995;&#21015;&#30340;&#38024;&#23545;&#19981;&#21516;&#21457;&#26126;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#65292;&#20854;&#20013;&#27599;&#20010;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes to measure the scope of a patent claim as the reciprocal of the self-information contained in this claim. Grounded in information theory, this approach is based on the assumption that a rare concept is more informative than a usual concept, inasmuch as it is more surprising. The self-information is calculated from the probability of occurrence of that claim, where the probability is calculated in accordance with a language model. Five language models are considered, ranging from the simplest models (each word or character is drawn from a uniform distribution) to intermediate models (using average word or character frequencies), to a large language model (GPT2). Interestingly, the simplest language models reduce the scope measure to the reciprocal of the word or character count, a metric already used in previous works. Application is made to nine series of patent claims directed to distinct inventions, where the claims in each series have a gradually decreasing scope.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#25991;&#26723;&#23884;&#20837;&#12289;&#38477;&#32500;&#25216;&#26415;&#21644;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#26816;&#27979;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#21327;&#21464;&#28418;&#31227;&#26041;&#38754;&#65292;&#29305;&#23450;&#30340;&#23884;&#20837;&#26041;&#27861;&#12289;&#38477;&#32500;&#25216;&#26415;&#21644;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#30340;&#32452;&#21512;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.10000</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26723;&#23884;&#20837;&#21644;&#38477;&#32500;&#26041;&#27861;&#26816;&#27979;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#21327;&#21464;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Detecting covariate drift in text data using document embeddings and dimensionality reduction. (arXiv:2309.10000v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#25991;&#26723;&#23884;&#20837;&#12289;&#38477;&#32500;&#25216;&#26415;&#21644;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#26816;&#27979;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#21327;&#21464;&#28418;&#31227;&#26041;&#38754;&#65292;&#29305;&#23450;&#30340;&#23884;&#20837;&#26041;&#27861;&#12289;&#38477;&#32500;&#25216;&#26415;&#21644;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#30340;&#32452;&#21512;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#21327;&#21464;&#28418;&#31227;&#23545;&#20110;&#20445;&#25345;&#25991;&#26412;&#20998;&#26512;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#30340;&#25991;&#26723;&#23884;&#20837;&#12289;&#38477;&#32500;&#25216;&#26415;&#21644;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#23545;&#20110;&#35782;&#21035;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#21327;&#21464;&#28418;&#31227;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;&#25991;&#26723;&#23884;&#20837;&#26041;&#27861;&#65306;&#20351;&#29992;&#28508;&#22312;&#35821;&#20041;&#20998;&#26512;&#65288;LSA&#65289;&#30340;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#36827;&#34892;&#38477;&#32500;&#65292;&#20197;&#21450;&#20351;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#36827;&#34892;&#38477;&#32500;&#30340;Doc2Vec&#21644;BERT&#23884;&#20837;&#12290;&#20026;&#20102;&#37327;&#21270;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;Kolmogorov-Smirnov&#65288;KS&#65289;&#32479;&#35745;&#37327;&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#26816;&#39564;&#20316;&#20026;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26816;&#27979;&#21327;&#21464;&#28418;&#31227;&#26041;&#38754;&#65292;&#26576;&#20123;&#23884;&#20837;&#26041;&#27861;&#12289;&#38477;&#32500;&#25216;&#26415;&#21644;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#30340;&#32452;&#21512;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting covariate drift in text data is essential for maintaining the reliability and performance of text analysis models. In this research, we investigate the effectiveness of different document embeddings, dimensionality reduction techniques, and drift detection methods for identifying covariate drift in text data. We explore three popular document embeddings: term frequency-inverse document frequency (TF-IDF) using Latent semantic analysis(LSA) for dimentionality reduction and Doc2Vec, and BERT embeddings, with and without using principal component analysis (PCA) for dimensionality reduction. To quantify the divergence between training and test data distributions, we employ the Kolmogorov-Smirnov (KS) statistic and the Maximum Mean Discrepancy (MMD) test as drift detection methods. Experimental results demonstrate that certain combinations of embeddings, dimensionality reduction techniques, and drift detection methods outperform others in detecting covariate drift. Our findings co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#38750;&#27954;&#35028;&#32654;&#22269;&#33521;&#35821;&#30340;&#25968;&#25454;&#65292;&#32467;&#21512;&#38899;&#39057;&#20998;&#31867;&#22120;&#21644;&#22320;&#29702;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#32654;&#22269;&#33521;&#35821;&#35821;&#38899;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;38.5%&#12290;</title><link>http://arxiv.org/abs/2309.09996</link><description>&lt;p&gt;
&#29992;&#38899;&#39057;&#20998;&#31867;&#25913;&#36827;&#38750;&#27954;&#35028;&#32654;&#22269;&#33521;&#35821;&#30340;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving Speech Recognition for African American English With Audio Classification. (arXiv:2309.09996v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09996
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#38750;&#27954;&#35028;&#32654;&#22269;&#33521;&#35821;&#30340;&#25968;&#25454;&#65292;&#32467;&#21512;&#38899;&#39057;&#20998;&#31867;&#22120;&#21644;&#22320;&#29702;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#32654;&#22269;&#33521;&#35821;&#35821;&#38899;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;38.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#31995;&#32479;&#22312;&#35782;&#21035;&#19981;&#21516;&#35821;&#35328;&#21464;&#31181;&#26102;&#23384;&#22312;&#36739;&#22823;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#25110;&#24494;&#35843;&#27169;&#22411;&#12290;&#20294;&#26159;&#26377;&#26102;&#20505;&#22312;&#39046;&#22495;&#20869;&#25968;&#25454;&#30340;&#25968;&#37327;&#26377;&#38480;&#65292;&#36825;&#20250;&#20351;&#35813;&#26041;&#27861;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#39046;&#22495;&#22806;&#25968;&#25454;(&#38271;&#31687;&#24418;&#24335;&#30340;&#38750;&#27954;&#35028;&#32654;&#22269;&#33521;&#35821;)&#26469;&#25552;&#39640;&#32654;&#22269;&#33521;&#35821;&#30701;&#31687;&#35821;&#38899;&#35782;&#21035;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;CORAAL&#12289;YouTube&#21644;Mozilla Common Voice&#26469;&#35757;&#32451;&#19968;&#20010;&#38899;&#39057;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#21487;&#20197;&#22823;&#33268;&#21028;&#26029;&#19968;&#21477;&#35805;&#26159;&#38750;&#27954;&#35028;&#32654;&#22269;&#33521;&#35821;&#36824;&#26159;&#20854;&#20182;&#21464;&#31181;&#65292;&#21253;&#25324;&#20027;&#27969;&#32654;&#22269;&#33521;&#35821;&#12290;&#36890;&#36807;&#23558;&#20998;&#31867;&#22120;&#36755;&#20986;&#19982;&#31895;&#30053;&#30340;&#22320;&#29702;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#22823;&#37327;&#26410;&#32763;&#35793;&#30340;&#30701;&#31687;&#26597;&#35810;&#35821;&#26009;&#24211;&#20013;&#36873;&#25321;&#19968;&#37096;&#20998;&#35821;&#21477;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#22312;&#27492;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;38.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) systems have been shown to have large quality disparities between the language varieties they are intended or expected to recognize. One way to mitigate this is to train or fine-tune models with more representative datasets. But this approach can be hindered by limited in-domain data for training and evaluation. We propose a new way to improve the robustness of a US English short-form speech recognizer using a small amount of out-of-domain (long-form) African American English (AAE) data. We use CORAAL, YouTube and Mozilla Common Voice to train an audio classifier to approximately output whether an utterance is AAE or some other variety including Mainstream American English (MAE). By combining the classifier output with coarse geographic information, we can select a subset of utterances from a large corpus of untranscribed short-form queries for semi-supervised learning at scale. Fine-tuning on this data results in a 38.5% relative word error rate disp
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#22788;&#29702;&#31246;&#21153;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35745;&#31639;&#31246;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.09992</link><description>&lt;p&gt;
OpenAI&#25220;&#34989;&#20102;&#25105;&#20204;&#30340;&#31246;&#21153;&#26696;&#20363;&#65292;&#20294;GPT-4&#30495;&#30340;&#33021;&#22815;&#22788;&#29702;&#31246;&#21153;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?. (arXiv:2309.09992v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09992
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#22788;&#29702;&#31246;&#21153;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35745;&#31639;&#31246;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#35299;&#37322;&#20102;OpenAI&#22312;GPT-4&#30340;&#30452;&#25773;&#28436;&#31034;&#20013;&#20351;&#29992;&#31246;&#27861;&#26696;&#20363;&#30340;&#26469;&#28304;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;GPT-4&#24471;&#21040;&#20102;&#38169;&#35823;&#30340;&#31572;&#26696;&#65292;&#20197;&#21450;&#23427;&#22914;&#20309;&#26080;&#27861;&#21487;&#38752;&#22320;&#35745;&#31639;&#31246;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The authors explain where OpenAI got the tax law example in its livestream demonstration of GPT-4, why GPT-4 got the wrong answer, and how it fails to reliably calculate taxes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuzzPretrain&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20195;&#30721;&#34920;&#31034;&#39044;&#35757;&#32451;&#20013;&#25506;&#32034;&#30001;&#31243;&#24207;&#30340;&#27979;&#35797;&#29992;&#20363;&#25581;&#31034;&#30340;&#21160;&#24577;&#20449;&#24687;&#65292;&#24182;&#35299;&#20915;&#20174;&#20195;&#30721;&#20013;&#30452;&#25509;&#23398;&#20064;&#21151;&#33021;&#35821;&#20041;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.09980</link><description>&lt;p&gt;
&#36890;&#36807;&#31243;&#24207;&#25191;&#34892;&#34917;&#20805;&#36827;&#34892;&#20195;&#30721;&#34920;&#31034;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Code Representation Pre-training with Complements from Program Executions. (arXiv:2309.09980v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuzzPretrain&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20195;&#30721;&#34920;&#31034;&#39044;&#35757;&#32451;&#20013;&#25506;&#32034;&#30001;&#31243;&#24207;&#30340;&#27979;&#35797;&#29992;&#20363;&#25581;&#31034;&#30340;&#21160;&#24577;&#20449;&#24687;&#65292;&#24182;&#35299;&#20915;&#20174;&#20195;&#30721;&#20013;&#30452;&#25509;&#23398;&#20064;&#21151;&#33021;&#35821;&#20041;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#24212;&#29992;&#20110;&#32534;&#31243;&#35821;&#35328;&#24314;&#27169;&#65292;&#20197;&#25512;&#36827;&#20195;&#30721;&#26234;&#33021;&#21270;&#12290;&#23613;&#31649;&#20195;&#30721;&#21487;&#20197;&#20197;&#25991;&#26412;&#26684;&#24335;&#34920;&#31034;&#65292;&#20294;&#20026;&#20102;&#27491;&#30830;&#32534;&#35793;&#25110;&#35299;&#37322;&#20197;&#25191;&#34892;&#19968;&#32452;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#20195;&#30721;&#22312;&#35821;&#27861;&#19978;&#26356;&#21152;&#20005;&#26684;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#36807;&#25277;&#35937;&#35821;&#27861;&#26641;&#12289;&#25511;&#21046;&#27969;&#22270;&#31561;&#24418;&#24335;&#30340;&#21477;&#27861;&#34920;&#31034;&#65292;&#20174;&#20195;&#30721;&#20013;&#20197;&#36739;&#23569;&#30340;&#27495;&#20041;&#24615;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#30456;&#21516;&#30446;&#30340;&#30340;&#31243;&#24207;&#21487;&#20197;&#29992;&#21508;&#31181;&#26041;&#24335;&#23454;&#29616;&#65292;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#21477;&#27861;&#34920;&#31034;&#65292;&#32780;&#20855;&#26377;&#31867;&#20284;&#23454;&#29616;&#30340;&#31243;&#24207;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#34892;&#20026;&#12290;&#34429;&#28982;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#21487;&#20197;&#36731;&#26131;&#22320;&#28436;&#31034;&#36825;&#31181;&#35821;&#20041;&#65292;&#20294;&#21151;&#33021;&#19978;&#30340;&#36825;&#20123;&#35821;&#20041;&#24456;&#38590;&#30452;&#25509;&#20174;&#20195;&#30721;&#20013;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FuzzPretrain&#26469;&#25506;&#32034;&#30001;&#27979;&#35797;&#29992;&#20363;&#25581;&#31034;&#30340;&#31243;&#24207;&#30340;&#21160;&#24577;&#20449;&#24687;&#65292;&#24182;&#23884;&#20837;&#21040;&#20195;&#30721;&#34920;&#31034;&#30340;&#39044;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) for natural language processing have been grafted onto programming language modeling for advancing code intelligence. Although it can be represented in the text format, code is syntactically more rigorous in order to be properly compiled or interpreted to perform a desired set of behaviors given any inputs. In this case, existing works benefit from syntactic representations to learn from code less ambiguously in the forms of abstract syntax tree, control-flow graph, etc. However, programs with the same purpose can be implemented in various ways showing different syntactic representations while the ones with similar implementations can have distinct behaviors. Though trivially demonstrated during executions, such semantics about functionality are challenging to be learned directly from code, especially in an unsupervised manner. Hence, in this paper, we propose FuzzPretrain to explore the dynamic information of programs revealed by their test cases and embed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38598;&#20013;&#22312;&#21457;&#24067;&#19968;&#20010;ASR&#20551;&#35774;&#20462;&#35746;&#65288;HypR&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20960;&#20010;&#24120;&#29992;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#19988;&#20026;ASR&#27169;&#22411;&#30340;&#20462;&#35746;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2309.09838</link><description>&lt;p&gt;
HypR&#65306;&#19968;&#20010;&#20351;&#29992;&#21442;&#32771;&#35821;&#26009;&#24211;&#36827;&#34892;ASR&#20551;&#35774;&#20462;&#35746;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
HypR: A comprehensive study for ASR hypothesis revising with a reference corpus. (arXiv:2309.09838v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38598;&#20013;&#22312;&#21457;&#24067;&#19968;&#20010;ASR&#20551;&#35774;&#20462;&#35746;&#65288;HypR&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20960;&#20010;&#24120;&#29992;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#19988;&#20026;ASR&#27169;&#22411;&#30340;&#20462;&#35746;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20462;&#35746;&#35782;&#21035;&#32467;&#26524;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#39640;&#25928;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#21508;&#31181;&#26041;&#27861;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;N-best&#37325;&#25490;&#24207;&#26041;&#27861;&#21644;&#38169;&#35823;&#20462;&#27491;&#27169;&#22411;&#12290;&#21069;&#32773;&#26088;&#22312;&#20174;&#30001;ASR&#29983;&#25104;&#30340;&#19968;&#32452;&#20505;&#36873;&#20551;&#35774;&#20013;&#36873;&#25321;&#38169;&#35823;&#29575;&#26368;&#20302;&#30340;&#20551;&#35774;&#65292;&#29992;&#20110;&#32473;&#23450;&#30340;&#36755;&#20837;&#35821;&#38899;&#12290;&#21518;&#32773;&#21017;&#19987;&#27880;&#20110;&#26816;&#27979;&#32473;&#23450;&#20551;&#35774;&#20013;&#30340;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#32416;&#27491;&#36825;&#20123;&#38169;&#35823;&#20197;&#33719;&#24471;&#22686;&#24378;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#30740;&#31350;&#24456;&#38590;&#30456;&#20114;&#27604;&#36739;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#22312;&#19981;&#21516;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#19982;&#19981;&#21516;&#30340;ASR&#27169;&#22411;&#37197;&#23545;&#65292;&#24182;&#19988;&#29978;&#33267;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#19987;&#27880;&#20110;&#21457;&#24067;&#19968;&#20010;ASR&#20551;&#35774;&#20462;&#35746;&#65288;HypR&#65289;&#25968;&#25454;&#38598;&#12290;HypR&#21253;&#21547;&#20960;&#20010;&#24120;&#29992;&#30340;&#35821;&#26009;&#24211;&#65288;AISHELL-1&#65292;TED-LIUM 2&#21644;LibriSpeech&#65289;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;ASR&#27169;&#22411;&#30340;&#22522;&#32447;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of deep learning, automatic speech recognition (ASR) has made significant progress. To further enhance the performance, revising recognition results is one of the lightweight but efficient manners. Various methods can be roughly classified into N-best reranking methods and error correction models. The former aims to select the hypothesis with the lowest error rate from a set of candidates generated by ASR for a given input speech. The latter focuses on detecting recognition errors in a given hypothesis and correcting these errors to obtain an enhanced result. However, we observe that these studies are hardly comparable to each other as they are usually evaluated on different corpora, paired with different ASR models, and even use different datasets to train the models. Accordingly, we first concentrate on releasing an ASR hypothesis revising (HypR) dataset in this study. HypR contains several commonly used corpora (AISHELL-1, TED-LIUM 2, and LibriSpeech) and provid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#30340;&#28151;&#21512;&#36830;&#32493;&#24402;&#23646;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#20013;&#24773;&#24863;&#30340;&#24310;&#32493;&#21644;&#24402;&#23646;&#30340;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#24773;&#26223;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.09799</link><description>&lt;p&gt;
&#35266;&#23519;&#28436;&#35762;&#32773;&#65306;&#19968;&#31181;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#30340;&#28151;&#21512;&#36830;&#32493;&#24402;&#23646;&#32593;&#32476;&#65292;&#23545;&#35805;&#20013;&#24102;&#26377;&#24773;&#24863;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Watch the Speakers: A Hybrid Continuous Attribution Network for Emotion Recognition in Conversation With Emotion Disentanglement. (arXiv:2309.09799v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#30340;&#28151;&#21512;&#36830;&#32493;&#24402;&#23646;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#20013;&#24773;&#24863;&#30340;&#24310;&#32493;&#21644;&#24402;&#23646;&#30340;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#24773;&#26223;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#24040;&#22823;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;ERC&#26041;&#27861;&#38754;&#20020;&#30528;&#22312;&#21508;&#31181;&#19981;&#21516;&#24773;&#26223;&#19979;&#27867;&#21270;&#30340;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#23545;&#19978;&#19979;&#25991;&#24314;&#27169;&#19981;&#36275;&#12289;&#23545;&#23545;&#35805;&#20851;&#31995;&#30340;&#27169;&#31946;&#25429;&#25417;&#19981;&#31934;&#30830;&#20197;&#21450;&#22312;&#35762;&#35805;&#32773;&#24314;&#27169;&#20013;&#36807;&#25311;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36830;&#32493;&#23646;&#24615;&#32593;&#32476;&#65288;HCAN&#65289;&#65292;&#20197;&#24773;&#24863;&#24310;&#32493;&#21644;&#24773;&#24863;&#24402;&#23646;&#30340;&#35270;&#35282;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;HCAN&#37319;&#29992;&#28151;&#21512;&#24490;&#29615;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22359;&#26469;&#24314;&#27169;&#20840;&#23616;&#24773;&#24863;&#36830;&#32493;&#24615;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24773;&#24863;&#24402;&#23646;&#32534;&#30721;&#65288;EAE&#65289;&#26469;&#23545;&#27599;&#20010;&#35805;&#35821;&#30340;&#20869;&#37096;&#21644;&#20132;&#21449;&#24773;&#24863;&#24402;&#23646;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#22312;&#35828;&#35805;&#32773;&#24314;&#27169;&#26041;&#38754;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#25552;&#39640;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#30340;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#25439;&#22833;&#20989;&#25968;&#24773;&#24863;&#35748;&#30693;&#25439;&#22833;&#180;EC&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) has attracted widespread attention in the natural language processing field due to its enormous potential for practical applications. Existing ERC methods face challenges in achieving generalization to diverse scenarios due to insufficient modeling of context, ambiguous capture of dialogue relationships and overfitting in speaker modeling. In this work, we present a Hybrid Continuous Attributive Network (HCAN) to address these issues in the perspective of emotional continuation and emotional attribution. Specifically, HCAN adopts a hybrid recurrent and attention-based module to model global emotion continuity. Then a novel Emotional Attribution Encoding (EAE) is proposed to model intra- and inter-emotional attribution for each utterance. Moreover, aiming to enhance the robustness of the model in speaker modeling and improve its performance in different scenarios, A comprehensive loss function emotional cognitive loss $\mathcal{L}_{\rm EC}$ is p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CensorChat&#65292;&#19968;&#20010;&#29992;&#20110;&#30417;&#27979;NSFW&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;NSFW&#20869;&#23481;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.09749</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#20419;&#36827;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;NSFW&#25991;&#26412;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge Distillation. (arXiv:2309.09749v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CensorChat&#65292;&#19968;&#20010;&#29992;&#20110;&#30417;&#27979;NSFW&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;NSFW&#20869;&#23481;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#23545;&#35805;&#20013;&#30340;NSFW&#65288;&#19981;&#36866;&#21512;&#19978;&#29677;&#65289;&#20869;&#23481;&#21487;&#33021;&#23545;&#29992;&#25143;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#26816;&#27979;NSFW&#35821;&#35328;&#65292;&#23588;&#20854;&#26159;&#24615;&#29233;&#20869;&#23481;&#26041;&#38754;&#30340;&#30740;&#31350;&#26126;&#26174;&#28382;&#21518;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CensorChat&#65292;&#19968;&#20010;&#26088;&#22312;&#26816;&#27979;NSFW&#23545;&#35805;&#30340;&#23545;&#35805;&#30417;&#25511;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#28041;&#21450;GPT-4&#21644;ChatGPT&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#26500;&#24314;NSFW&#20869;&#23481;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#12290;&#35813;&#36807;&#31243;&#28041;&#21450;&#25910;&#38598;&#30495;&#23454;&#30340;&#20154;&#26426;&#20132;&#20114;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#21333;&#20010;&#35805;&#35821;&#21644;&#21333;&#36718;&#23545;&#35805;&#65292;&#20854;&#20013;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#26368;&#21518;&#19968;&#21477;&#35805;&#12290;&#20351;&#29992;ChatGPT&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#65292;&#20316;&#20026;&#35757;&#32451;&#38598;&#12290;&#20351;&#29992;ChatGPT&#21644;GPT-4&#20316;&#20026;&#27880;&#37322;&#22120;&#26500;&#24314;&#20102;&#21512;&#29702;&#24615;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#38598;&#65292;&#24182;&#20351;&#29992;&#33258;&#25105;&#25209;&#35780;&#31574;&#30053;&#35299;&#20915;&#26631;&#35760;&#20013;&#30340;&#24046;&#24322;&#12290;BERT&#27169;&#22411;&#29992;&#20110;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
NSFW (Not Safe for Work) content, in the context of a dialogue, can have severe side effects on users in open-domain dialogue systems. However, research on detecting NSFW language, especially sexually explicit content, within a dialogue context has significantly lagged behind. To address this issue, we introduce CensorChat, a dialogue monitoring dataset aimed at NSFW dialogue detection. Leveraging knowledge distillation techniques involving GPT-4 and ChatGPT, this dataset offers a cost-effective means of constructing NSFW content detectors. The process entails collecting real-life human-machine interaction data and breaking it down into single utterances and single-turn dialogues, with the chatbot delivering the final utterance. ChatGPT is employed to annotate unlabeled data, serving as a training set. Rationale validation and test sets are constructed using ChatGPT and GPT-4 as annotators, with a self-criticism strategy for resolving discrepancies in labeling. A BERT model is fine-tun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LLM4Jobs&#26080;&#30417;&#30563;&#30340;&#32844;&#19994;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#23637;&#29616;&#20102;&#36229;&#36234;&#26368;&#26032;&#22522;&#20934;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#30456;&#20851;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#26377;&#26395;&#20026;&#22797;&#26434;&#30340;&#32844;&#19994;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#20219;&#21153;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2309.09708</link><description>&lt;p&gt;
LLM4Jobs: &#26080;&#30417;&#30563;&#30340;&#32844;&#19994;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#65292;&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models. (arXiv:2309.09708v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LLM4Jobs&#26080;&#30417;&#30563;&#30340;&#32844;&#19994;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#23637;&#29616;&#20102;&#36229;&#36234;&#26368;&#26032;&#22522;&#20934;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#30456;&#20851;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#26377;&#26395;&#20026;&#22797;&#26434;&#30340;&#32844;&#19994;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#20219;&#21153;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20174;&#33258;&#30001;&#25991;&#26412;&#30340;&#25307;&#32856;&#20449;&#24687;&#21644;&#31616;&#21382;&#20013;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#32844;&#19994;&#26159;&#35832;&#22914;&#32844;&#20301;&#25512;&#33616;&#21644;&#21171;&#21160;&#21147;&#24066;&#22330;&#25919;&#31574;&#21046;&#23450;&#31561;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LLM4Jobs&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#32844;&#19994;&#32534;&#30721;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;LLM4Jobs&#29420;&#29305;&#22320;&#21033;&#29992;&#20102;LLMs&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM4Jobs&#22987;&#32456;&#36229;&#36234;&#26080;&#30417;&#30563;&#30340;&#26368;&#26032;&#22522;&#20934;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#31890;&#24230;&#19978;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#20316;&#20026;&#25105;&#20204;&#24037;&#20316;&#30340;&#19968;&#20010;&#38468;&#24102;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#36825;&#23545;&#20110;&#35813;&#39046;&#22495;&#30340;&#21518;&#32493;&#30740;&#31350;&#21487;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#39033;&#35843;&#26597;&#31361;&#26174;&#20102;&#24403;&#20195;LLMs&#22312;&#22797;&#26434;&#30340;&#32844;&#19994;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#20026;&#24314;&#31435;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated occupation extraction and standardization from free-text job postings and resumes are crucial for applications like job recommendation and labor market policy formation. This paper introduces LLM4Jobs, a novel unsupervised methodology that taps into the capabilities of large language models (LLMs) for occupation coding. LLM4Jobs uniquely harnesses both the natural language understanding and generation capacities of LLMs. Evaluated on rigorous experimentation on synthetic and real-world datasets, we demonstrate that LLM4Jobs consistently surpasses unsupervised state-of-the-art benchmarks, demonstrating its versatility across diverse datasets and granularities. As a side result of our work, we present both synthetic and real-world datasets, which may be instrumental for subsequent research in this domain. Overall, this investigation highlights the promise of contemporary LLMs for the intricate task of occupation extraction and standardization, laying the foundation for a robust
&lt;/p&gt;</description></item><item><title>LayoutNUWA&#26159;&#31532;&#19968;&#20010;&#23558;&#29256;&#24335;&#29983;&#25104;&#35270;&#20026;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#26469;&#22686;&#24378;&#35821;&#20041;&#20449;&#24687;&#21644;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#29256;&#24335;&#19987;&#38271;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.09506</link><description>&lt;p&gt;
LayoutNUWA: &#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#29256;&#24335;&#19987;&#38271;
&lt;/p&gt;
&lt;p&gt;
LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models. (arXiv:2309.09506v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09506
&lt;/p&gt;
&lt;p&gt;
LayoutNUWA&#26159;&#31532;&#19968;&#20010;&#23558;&#29256;&#24335;&#29983;&#25104;&#35270;&#20026;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#26469;&#22686;&#24378;&#35821;&#20041;&#20449;&#24687;&#21644;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#29256;&#24335;&#19987;&#38271;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#29256;&#24335;&#29983;&#25104;&#20316;&#20026;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#20449;&#24687;&#24863;&#30693;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#23558;&#29256;&#24335;&#29983;&#25104;&#35270;&#20026;&#25968;&#20540;&#20248;&#21270;&#20219;&#21153;&#65292;&#27880;&#37325;&#23450;&#37327;&#26041;&#38754;&#65292;&#20294;&#24573;&#30053;&#20102;&#29256;&#24335;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#22914;&#27599;&#20010;&#29256;&#24335;&#20803;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LayoutNUWA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#29256;&#24335;&#29983;&#25104;&#35270;&#20026;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#35821;&#20041;&#20449;&#24687;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#29256;&#24335;&#19987;&#38271;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#20195;&#30721;&#25351;&#31034;&#35843;&#25972;&#65288;Code Instruct Tuning&#65292;CIT&#65289;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19977;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#27169;&#22359;&#65306;1&#65289;&#20195;&#30721;&#21021;&#22987;&#21270;&#65288;Code Initialization&#65292;CI&#65289;&#27169;&#22359;&#23558;&#25968;&#20540;&#26465;&#20214;&#37327;&#21270;&#24182;&#20197;HTML&#20195;&#30721;&#30340;&#24418;&#24335;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#24182;&#25918;&#32622;&#20102;&#31574;&#30053;&#24615;&#30340;&#23631;&#34109;&#65307;2&#65289;&#20195;&#30721;&#23436;&#25104;&#65288;Code Completion&#65292;CC&#65289;&#27169;&#22359;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26684;&#24335;&#21270;&#30693;&#35782;&#22635;&#20889;HTML&#20195;&#30721;&#20013;&#30340;&#23631;&#34109;&#37096;&#20998;&#65307;3&#65289;&#20195;&#30721;&#28210;&#26579;&#65288;Code Rendering&#65292;CR&#65289;&#27169;&#22359;&#23558;&#23436;&#25104;&#30340;&#20195;&#30721;&#36716;&#21270;&#20026;&#26368;&#32456;&#30340;&#22270;&#24418;&#29256;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphic layout generation, a growing research field, plays a significant role in user engagement and information perception. Existing methods primarily treat layout generation as a numerical optimization task, focusing on quantitative aspects while overlooking the semantic information of layout, such as the relationship between each layout element. In this paper, we propose LayoutNUWA, the first model that treats layout generation as a code generation task to enhance semantic information and harness the hidden layout expertise of large language models~(LLMs). More concretely, we develop a Code Instruct Tuning (CIT) approach comprising three interconnected modules: 1) the Code Initialization (CI) module quantifies the numerical conditions and initializes them as HTML code with strategically placed masks; 2) the Code Completion (CC) module employs the formatting knowledge of LLMs to fill in the masked portions within the HTML code; 3) the Code Rendering (CR) module transforms the complet
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35821;&#35328;&#25552;&#31034;&#35843;&#25972;&#21644;&#24103;&#32423;&#35821;&#35328;&#36866;&#37197;&#22120;&#36825;&#20004;&#31181;&#31616;&#21333;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25104;&#21151;&#25552;&#39640;&#20102;&#22810;&#35821;&#35328;&#35821;&#38899;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09443</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#25552;&#31034;&#35843;&#25972;&#21644;&#24103;&#32423;&#35821;&#35328;&#36866;&#37197;&#22120;&#25552;&#39640;&#22810;&#35821;&#35328;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multilingual Speech Recognition through Language Prompt Tuning and Frame-Level Language Adapter. (arXiv:2309.09443v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09443
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#25552;&#31034;&#35843;&#25972;&#21644;&#24103;&#32423;&#35821;&#35328;&#36866;&#37197;&#22120;&#36825;&#20004;&#31181;&#31616;&#21333;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25104;&#21151;&#25552;&#39640;&#20102;&#22810;&#35821;&#35328;&#35821;&#38899;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#35821;&#35328;&#26234;&#33021;&#21161;&#25163;&#22914;ChatGPT&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25193;&#22823;&#22810;&#35821;&#35328;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;&#24212;&#29992;&#65292;&#24182;&#20419;&#36827;&#22269;&#38469;&#20132;&#27969;&#65292;&#25552;&#39640;&#22810;&#35821;&#35328;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65306;&#35821;&#35328;&#25552;&#31034;&#35843;&#25972;&#21644;&#24103;&#32423;&#35821;&#35328;&#36866;&#37197;&#22120;&#65292;&#20998;&#21035;&#29992;&#20110;&#22686;&#24378;&#21487;&#37197;&#32622;&#35821;&#35328;&#21644;&#35821;&#35328;&#26080;&#20851;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#38598;&#25104;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19971;&#31181;&#35821;&#35328;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual intelligent assistants, such as ChatGPT, have recently gained popularity. To further expand the applications of multilingual artificial intelligence assistants and facilitate international communication, it is essential to enhance the performance of multilingual speech recognition, which is a crucial component of speech interaction. In this paper, we propose two simple and parameter-efficient methods: language prompt tuning and frame-level language adapter, to respectively enhance language-configurable and language-agnostic multilingual speech recognition. Additionally, we explore the feasibility of integrating these two approaches using parameter-efficient fine-tuning methods. Our experiments demonstrate significant performance improvements across seven languages using our proposed methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#22797;&#26434;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#25913;&#21892;&#36825;&#31181;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;Struc-Bench&#21644;&#22810;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#24120;&#35265;&#30340;&#26684;&#24335;&#38169;&#35823;&#21644;&#28508;&#22312;&#25913;&#36827;&#30340;&#39046;&#22495;&#12290;&#36890;&#36807;&#24212;&#29992;&#32467;&#26500;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#33258;&#28982;&#35821;&#35328;&#32422;&#26463;&#30340;&#36981;&#23432;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.08963</link><description>&lt;p&gt;
Struc-Bench&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22797;&#26434;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#24471;&#30495;&#30340;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?. (arXiv:2309.08963v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#22797;&#26434;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#25913;&#21892;&#36825;&#31181;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;Struc-Bench&#21644;&#22810;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#24120;&#35265;&#30340;&#26684;&#24335;&#38169;&#35823;&#21644;&#28508;&#22312;&#25913;&#36827;&#30340;&#39046;&#22495;&#12290;&#36890;&#36807;&#24212;&#29992;&#32467;&#26500;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#33258;&#28982;&#35821;&#35328;&#32422;&#26463;&#30340;&#36981;&#23432;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#23427;&#20204;&#22312;&#29983;&#25104;&#38656;&#35201;&#22797;&#26434;&#32467;&#26500;&#21270;&#36755;&#20986;&#30340;&#20219;&#21153;&#19978;&#20173;&#28982;&#26377;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#22797;&#26434;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#24494;&#35843;&#26041;&#27861;&#20316;&#20026;&#25913;&#36827;&#36825;&#31181;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Struc-Bench&#65292;&#21253;&#25324;&#20116;&#20010;&#20195;&#34920;&#24615;&#30340;LLM&#65288;&#21363;GPT-NeoX 20B&#65292;GPT-3.5&#65292;GPT-4&#21644;Vicuna&#65289;&#65292;&#24182;&#22312;&#25105;&#20204;&#31934;&#24515;&#26500;&#24314;&#30340;&#36328;&#21407;&#22987;&#25991;&#26412;&#12289;HTML&#21644;LaTeX&#34920;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#12290;&#26681;&#25454;&#25105;&#20204;&#23545;&#24403;&#21069;&#27169;&#22411;&#24615;&#33021;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#29305;&#23450;&#30340;&#24120;&#35265;&#26684;&#24335;&#38169;&#35823;&#21644;&#28508;&#22312;&#25913;&#36827;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#22797;&#26434;&#30340;&#26684;&#24335;&#35201;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;FormatCoT&#65288;&#24605;&#32500;&#38142;&#65289;&#20174;&#30446;&#26631;&#36755;&#20986;&#20013;&#29983;&#25104;&#26684;&#24335;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#23558;&#36825;&#31181;&#32467;&#26500;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#24212;&#29992;&#21040;LLaMA-7B&#19978;&#26102;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#33258;&#28982;&#35821;&#35328;&#32422;&#26463;&#30340;&#36981;&#23432;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the power of Large Language Models (LLMs) like GPT-4, they still struggle with tasks that require generating complex, structured outputs. In this study, we assess the capability of Current LLMs in generating complex structured data and propose a structure-aware fine-tuning approach as a solution to improve this ability. To perform a comprehensive evaluation, we propose Struc-Bench, include five representative LLMs (i.e., GPT-NeoX 20B, GPT-3.5, GPT-4, and Vicuna) and evaluate them on our carefully constructed datasets spanning raw text, HTML, and LaTeX tables. Based on our analysis of current model performance, we identify specific common formatting errors and areas of potential improvement. To address complex formatting requirements, we utilize FormatCoT (Chain-of-Thought) to generate format instructions from target outputs. Our experiments show that our structure-aware fine-tuning method, when applied to LLaMA-7B, significantly improves adherence to natural language constraint
&lt;/p&gt;</description></item><item><title>TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.08637</link><description>&lt;p&gt;
TextBind: &#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08637
&lt;/p&gt;
&lt;p&gt;
TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#20854;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25968;&#25454;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#33719;&#24471;&#12290;&#24403;&#28041;&#21450;&#21040;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TextBind&#65292;&#36825;&#26159;&#19968;&#20010;&#20960;&#20046;&#19981;&#38656;&#35201;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36171;&#20104;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#22270;&#20687;-&#26631;&#39064;&#23545;&#65292;&#24182;&#20174;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#22312;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#12298;Media of Langue&#12299;&#36825;&#19968;&#20840;&#26032;&#35789;&#20856;&#21644;&#20844;&#20849;&#38613;&#22609;&#65292;&#36890;&#36807;&#25551;&#36848;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24847;&#20041;&#22320;&#22270;&#21644;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#30340;&#27010;&#24565;&#65306;&#12298;Inter-Langue Map/Dictionary&#12299;&#12289;&#12298;Inter-Langue Space&#12299;&#21644;&#12298;Inter-Langue Network&#12299;&#12290;</title><link>http://arxiv.org/abs/2309.08609</link><description>&lt;p&gt;
&#12298;Media of Langue&#12299;&#30340;&#23186;&#20307;
&lt;/p&gt;
&lt;p&gt;
Media of Langue. (arXiv:2309.08609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#12298;Media of Langue&#12299;&#36825;&#19968;&#20840;&#26032;&#35789;&#20856;&#21644;&#20844;&#20849;&#38613;&#22609;&#65292;&#36890;&#36807;&#25551;&#36848;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24847;&#20041;&#22320;&#22270;&#21644;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#30340;&#27010;&#24565;&#65306;&#12298;Inter-Langue Map/Dictionary&#12299;&#12289;&#12298;Inter-Langue Space&#12299;&#21644;&#12298;Inter-Langue Network&#12299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23384;&#26723;Goki Muramoto&#31561;&#20154;&#30340;&#12298;Media of Langue&#12299;&#21518;&#38754;&#30340;&#26448;&#26009;&#12290;&#12298;Media of Langue&#12299;&#26159;&#19968;&#20010;&#20840;&#26032;&#30340;&#23383;&#20856;&#21644;&#20844;&#20849;&#38613;&#22609;&#65292;&#23427;&#20165;&#20174;&#8220;&#36825;&#20010;&#35789;&#34987;&#32763;&#35793;&#25104;&#37027;&#20010;&#35789;&#8221;&#30340;&#24191;&#27867;&#20107;&#20214;&#21644;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#30340;&#36793;&#30028;&#19978;&#25551;&#36848;&#20986;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24847;&#20041;&#22320;&#22270;&#12290;&#39318;&#20808;&#65292;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#27010;&#24565;&#65306;&#12298;Inter-Langue Map/Dictionary&#12299;&#12289;&#12298;Inter-Langue Space&#12299;&#21644;&#12298;Inter-Langue Network&#12299;&#24182;&#23558;&#20854;&#19982;&#23383;&#20856;&#12289;&#35821;&#20041;&#31354;&#38388;&#21644;&#35821;&#20041;&#32593;&#32476;&#30340;&#19977;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25509;&#19979;&#26469;&#65292;&#25551;&#36848;&#20102;&#35813;&#20316;&#21697;&#20013;&#23454;&#26045;&#30340;&#20855;&#20307;&#31639;&#27861;&#21644;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to archive the materials behind "Media of Langue" by Goki Muramoto et al. Media of Langue is a new dictionary and public sculpture that depicts the map of meaning on the boundary between languages solely from the vast events of "this word was translated into that word" and two forces: repulsion between all words in the same language and attraction between translated words in different languages. First, the three new concepts proposed, Inter-Langue Map/Dictionary, Inter-Langue Space, and then Inter-Langue Network, are introduced, comparing them to the three domains of dictionary, semantic space, and semantic network. Next, the specific algorithms and designs implemented in the work were described.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#19968;&#32452;&#39640;&#24230;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#22810;&#20041;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08600</link><description>&lt;p&gt;
&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Sparse Autoencoders Find Highly Interpretable Features in Language Models. (arXiv:2309.08600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#19968;&#32452;&#39640;&#24230;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#22810;&#20041;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#29702;&#35299;&#30340;&#19968;&#20010;&#38556;&#30861;&#26159;&#22810;&#20041;&#24615;&#65292;&#20854;&#20013;&#31070;&#32463;&#20803;&#22312;&#22810;&#20010;&#35821;&#20041;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#28608;&#27963;&#12290;&#22810;&#20041;&#24615;&#20351;&#25105;&#20204;&#26080;&#27861;&#25214;&#21040;&#31616;&#27905;&#30340;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#24037;&#20316;&#12290;&#22810;&#20041;&#24615;&#30340;&#19968;&#20010;&#29468;&#27979;&#21407;&#22240;&#26159;&#21472;&#21152;&#25928;&#24212;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#37197;&#32473;&#28608;&#27963;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#36807;&#23436;&#22791;&#26041;&#21521;&#38598;&#21512;&#65292;&#32780;&#19981;&#26159;&#20010;&#21035;&#31070;&#32463;&#20803;&#65292;&#34920;&#31034;&#26356;&#22810;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#26469;&#30830;&#23450;&#36825;&#20123;&#26041;&#21521;&#65292;&#20197;&#37325;&#26500;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#12290;&#36825;&#20123;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21040;&#30340;&#19968;&#32452;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#27604;&#20854;&#20182;&#26041;&#27861;&#37492;&#23450;&#20986;&#30340;&#26041;&#21521;&#26356;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#65292;&#35299;&#37322;&#24615;&#26159;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#34913;&#37327;&#30340;&#12290;&#21024;&#38500;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#20363;&#22914;&#36890;&#36807;&#21024;&#38500;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the roadblocks to a better understanding of neural networks' internals is \textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Ablating these features enables precise model editing, for example, by remo
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.07864</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Rise and Potential of Large Language Model Based Agents: A Survey. (arXiv:2309.07864v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07864
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#31867;&#19968;&#30452;&#36861;&#27714;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36798;&#21040;&#25110;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#30446;&#26631;&#65292;&#32780;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26377;&#26395;&#26041;&#24335;&#30340;AI&#20195;&#29702;&#12290;AI&#20195;&#29702;&#26159;&#33021;&#24863;&#30693;&#29615;&#22659;&#12289;&#20570;&#20986;&#20915;&#31574;&#21644;&#37319;&#21462;&#34892;&#21160;&#30340;&#20154;&#24037;&#23454;&#20307;&#12290;&#33258;20&#19990;&#32426;&#20013;&#21494;&#20197;&#26469;&#65292;&#20154;&#20204;&#20026;&#24320;&#21457;&#26234;&#33021;AI&#20195;&#29702;&#36827;&#34892;&#20102;&#35768;&#22810;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#31639;&#27861;&#25110;&#35757;&#32451;&#31574;&#30053;&#30340;&#36827;&#27493;&#19978;&#65292;&#20197;&#22686;&#24378;&#29305;&#23450;&#33021;&#21147;&#25110;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#23454;&#38469;&#19978;&#65292;&#31038;&#21306;&#25152;&#32570;&#20047;&#30340;&#26159;&#19968;&#20010;&#36275;&#22815;&#36890;&#29992;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#35774;&#35745;&#33021;&#36866;&#24212;&#21508;&#31181;&#22330;&#26223;&#30340;AI&#20195;&#29702;&#30340;&#36215;&#28857;&#12290;&#30001;&#20110;&#23637;&#31034;&#20986;&#30340;&#22810;&#21151;&#33021;&#21644;&#26174;&#33879;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#20026;&#26500;&#24314;&#36890;&#29992;AI&#20195;&#29702;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;&#35768;&#22810;&#30740;&#31350;&#24037;&#20316;&#21033;&#29992;LLMs&#20316;&#20026;&#26500;&#24314;AI&#20195;&#29702;&#30340;&#22522;&#30784;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent AI agents since the mid-20th century. However, these efforts have mainly focused on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a sufficiently general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile and remarkable capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many research efforts have leveraged LLMs as the foundation to build AI agents and ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35270;&#35282;&#26469;&#35299;&#37322;&#21464;&#21387;&#22120;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#38416;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#38480;&#21046;&#28508;&#22312;&#29305;&#24449;&#24182;&#22312;&#36229;&#29699;&#38754;&#19978;&#22609;&#36896;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#25506;&#27979;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#35813;&#35270;&#35282;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.07315</link><description>&lt;p&gt;
&#26053;&#34892;&#35789;&#65306;&#19968;&#31181;&#21464;&#21387;&#22120;&#30340;&#20960;&#20309;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35270;&#35282;&#26469;&#35299;&#37322;&#21464;&#21387;&#22120;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#38416;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#38480;&#21046;&#28508;&#22312;&#29305;&#24449;&#24182;&#22312;&#36229;&#29699;&#38754;&#19978;&#22609;&#36896;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#25506;&#27979;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#35813;&#35270;&#35282;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#29702;&#35299;&#20854;&#20869;&#37096;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#35270;&#35282;&#65292;&#38416;&#26126;&#20102;&#21464;&#21387;&#22120;&#25805;&#20316;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#35828;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#23558;&#28508;&#22312;&#29305;&#24449;&#38480;&#21046;&#22312;&#19968;&#20010;&#36229;&#29699;&#38754;&#19978;&#65292;&#20174;&#32780;&#20351;&#27880;&#24847;&#21147;&#33021;&#22815;&#22312;&#35813;&#34920;&#38754;&#19978;&#22609;&#36896;&#21333;&#35789;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#36825;&#31181;&#20960;&#20309;&#35270;&#28857;&#26080;&#32541;&#22320;&#36830;&#25509;&#20102;&#36845;&#20195;&#25913;&#36827;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#31561;&#24050;&#30693;&#23646;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25506;&#27979;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;124M&#21442;&#25968;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#26089;&#26399;&#23618;&#20013;&#28165;&#26224;&#30340;&#26597;&#35810;-&#38190;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#22312;&#26356;&#28145;&#30340;&#23618;&#27425;&#19978;&#24314;&#31435;&#22312;&#20808;&#21069;&#20851;&#20110;&#27880;&#24847;&#22836;&#30340;&#19987;&#38376;&#24615;&#30340;&#35266;&#23519;&#22522;&#30784;&#19978;&#12290;&#21033;&#29992;&#36825;&#20123;&#20960;&#20309;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#65292;&#23558;&#20854;&#25551;&#32472;&#20026;&#22609;&#36896;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have significantly advanced the field of natural language processing, but comprehending their internal mechanisms remains a challenge. In this paper, we introduce a novel geometric perspective that elucidates the inner mechanisms of transformer operations. Our primary contribution is illustrating how layer normalization confines the latent features to a hyper-sphere, subsequently enabling attention to mold the semantic representation of words on this surface. This geometric viewpoint seamlessly connects established properties such as iterative refinement and contextual embeddings. We validate our insights by probing a pre-trained 124M parameter GPT-2 model. Our findings reveal clear query-key attention patterns in early layers and build upon prior observations regarding the subject-specific nature of attention heads at deeper layers. Harnessing these geometric insights, we present an intuitive understanding of transformers, depicting them as processes that model the trajec
&lt;/p&gt;</description></item><item><title>BHASA&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#22871;&#20214;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19996;&#21335;&#20122;&#35821;&#35328;&#21644;&#25991;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#23427;&#21253;&#25324;NLP&#22522;&#20934;&#12289;&#35821;&#35328;&#35786;&#26029;&#24037;&#20855;&#21253;&#21644;&#25991;&#21270;&#35786;&#26029;&#25968;&#25454;&#38598;&#12290;&#30446;&#21069;&#65292;&#35813;&#22871;&#20214;&#30340;&#21021;&#27493;&#29256;&#26412;&#20165;&#38024;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#36234;&#21335;&#35821;&#12289;&#27888;&#35821;&#21644;&#27888;&#31859;&#23572;&#35821;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.06085</link><description>&lt;p&gt;
BHASA&#65306;&#38754;&#21521;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#19996;&#21335;&#20122;&#35821;&#35328;&#21644;&#25991;&#21270;&#32508;&#21512;&#35780;&#20272;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models. (arXiv:2309.06085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06085
&lt;/p&gt;
&lt;p&gt;
BHASA&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#22871;&#20214;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19996;&#21335;&#20122;&#35821;&#35328;&#21644;&#25991;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#23427;&#21253;&#25324;NLP&#22522;&#20934;&#12289;&#35821;&#35328;&#35786;&#26029;&#24037;&#20855;&#21253;&#21644;&#25991;&#21270;&#35786;&#26029;&#25968;&#25454;&#38598;&#12290;&#30446;&#21069;&#65292;&#35813;&#22871;&#20214;&#30340;&#21021;&#27493;&#29256;&#26412;&#20165;&#38024;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#36234;&#21335;&#35821;&#12289;&#27888;&#35821;&#21644;&#27888;&#31859;&#23572;&#35821;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#35268;&#27169;&#24102;&#26469;&#30340;&#26032;&#33021;&#21147;&#20351;&#24471;&#26500;&#24314;&#20840;&#38754;&#12289;&#22810;&#26679;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25104;&#20026;&#24517;&#35201;&#65292;&#22914;HELM&#21644;BIG-bench&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#37096;&#20998;&#22522;&#20934;&#21482;&#20851;&#27880;&#33521;&#35821;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#19996;&#21335;&#20122;&#65288;SEA&#65289;&#35821;&#35328;&#30340;&#35780;&#20272;&#24456;&#23569;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BHASA&#65292;&#19968;&#20010;&#38024;&#23545;SEA&#35821;&#35328;&#30340;&#32508;&#21512;&#35821;&#35328;&#21644;&#25991;&#21270;&#35780;&#20272;&#22871;&#20214;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#28085;&#30422;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#12289;&#29983;&#25104;&#65288;NLG&#65289;&#21644;&#25512;&#29702;&#65288;NLR&#65289;&#20219;&#21153;&#30340;NLP&#22522;&#20934;&#65292;&#20849;&#28085;&#30422;&#20843;&#20010;&#20219;&#21153;&#65307;&#65288;2&#65289;LINDSEA&#65292;&#19968;&#20010;&#36328;&#36234;&#21477;&#27861;&#12289;&#35821;&#20041;&#21644;&#35821;&#29992;&#31561;&#21508;&#31181;&#35821;&#35328;&#29616;&#35937;&#30340;&#35821;&#35328;&#35786;&#26029;&#24037;&#20855;&#21253;&#65307;&#65288;3&#65289;&#19968;&#20221;&#25991;&#21270;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25506;&#32034;&#25991;&#21270;&#34920;&#36798;&#21644;&#25935;&#24863;&#24615;&#12290;&#23545;&#20110;&#36825;&#20010;&#21021;&#27493;&#24037;&#20316;&#65292;&#25105;&#20204;&#21482;&#38024;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#36234;&#21335;&#35821;&#12289;&#27888;&#35821;&#21644;&#27888;&#31859;&#23572;&#35821;&#23454;&#29616;&#20102;NLP&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of Large Language Models (LLMs) and the emergence of novel abilities with scale have necessitated the construction of holistic, diverse and challenging benchmarks such as HELM and BIG-bench. However, at the moment, most of these benchmarks focus only on performance in English and evaluations that include Southeast Asian (SEA) languages are few in number. We therefore propose BHASA, a holistic linguistic and cultural evaluation suite for LLMs in SEA languages. It comprises three components: (1) a NLP benchmark covering eight tasks across Natural Language Understanding (NLU), Generation (NLG) and Reasoning (NLR) tasks, (2) LINDSEA, a linguistic diagnostic toolkit that spans the gamut of linguistic phenomena including syntax, semantics and pragmatics, and (3) a cultural diagnostics dataset that probes for both cultural representation and sensitivity. For this preliminary effort, we implement the NLP benchmark only for Indonesian, Vietnamese, Thai and Tamil, and we on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;LLMs&#22312;&#32593;&#32476;&#36816;&#32500;&#65288;NetOps&#65289;&#39046;&#22495;&#20855;&#26377;&#24378;&#22823;&#30340;&#28508;&#21147;&#24212;&#29992;&#65292;&#33021;&#22815;&#25552;&#21319;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#21270;&#30340;NetOps&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05557</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32593;&#32476;&#36816;&#32500;&#33021;&#21147;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of NetOps Capability of Pre-Trained Large Language Models. (arXiv:2309.05557v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;LLMs&#22312;&#32593;&#32476;&#36816;&#32500;&#65288;NetOps&#65289;&#39046;&#22495;&#20855;&#26377;&#24378;&#22823;&#30340;&#28508;&#21147;&#24212;&#29992;&#65292;&#33021;&#22815;&#25552;&#21319;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#21270;&#30340;NetOps&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22238;&#31572;&#20154;&#31867;&#35821;&#35328;&#26597;&#35810;&#65292;&#24182;&#22312;&#32593;&#32476;&#36816;&#32500;&#65288;NetOps&#65289;&#39046;&#22495;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#28508;&#21147;&#24212;&#29992;&#12290;&#30001;&#20110;&#20855;&#22791;&#22823;&#37327;&#24120;&#35782;&#30693;&#35782;&#65292;LLMs&#22312;&#25512;&#29702;&#20934;&#30830;&#24615;&#19978;&#27604;&#20256;&#32479;&#27169;&#22411;&#26356;&#22909;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#25512;&#29702;&#33021;&#21147;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#65292;&#36825;&#20123;&#33021;&#21147;&#21487;&#33021;&#23545;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#21270;&#30340;NetOps&#26377;&#24040;&#22823;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#21508;&#31181;NetOps&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;&#36873;&#25321;&#30340;&#20960;&#31181;LLMs&#22312;NetOps&#39046;&#22495;&#30340;&#33021;&#21147;&#12289;&#20248;&#21183;&#21644;&#38480;&#21046;&#12290;&#35780;&#20272;&#38024;&#23545;5732&#20010;&#20851;&#20110;NetOps&#30340;&#38382;&#39064;&#36827;&#34892;&#65292;&#28085;&#30422;&#20102;26&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#36890;&#29992;&#39046;&#22495;LLMs&#65292;&#21253;&#25324;ChatGPT&#12289;LLaMA&#12289;Falcon&#31561;&#12290;&#25105;&#20204;&#36824;&#23545;&#20854;&#20013;&#19968;&#20123;LLMs&#36827;&#34892;&#20102;NetOps&#35821;&#26009;&#24211;&#30340;&#24494;&#35843;&#65292;&#24182;&#35780;&#20272;&#20102;&#32467;&#26524;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#35780;&#20272;&#26041;&#27861;&#36981;&#24490;&#24191;&#27867;&#37319;&#29992;&#30340;&#29992;&#20110;&#29983;&#25104;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can respond to human language queries and have shown powerful potential applications in network operations (NetOps). Thanks to the large amount of commonsense knowledge inherent, LLMs achieve much better inference accuracy than traditional models and emerge with strong abilities in generalization, reasoning, and code generation. These abilities may have a crucial boost to automated and intelligent NetOps. However, it remains under-explored how well LLMs perform in various NetOps tasks. In this work, we make a systematic assessment of the capabilities, strengths, and limitations of selected LLMs in the field of NetOps. The evaluation is conducted on a collection of 5,732 questions about NetOps, encompassing 26 publicly available general-domain LLMs, including ChatGPT, LLaMA, Falcon, etc. We also finetune some of these LLMs with our collected NetOps corpus and evaluate the resulting models. The evaluation method follows the widely adopted benchmarks for gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#33719;&#21462;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#21021;&#22987;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;FOLLOWUPQG&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#30340;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.05007</link><description>&lt;p&gt;
FOLLOWUPQG:&#38754;&#21521;&#20449;&#24687;&#33719;&#21462;&#30340;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FOLLOWUPQG: Towards Information-Seeking Follow-up Question Generation. (arXiv:2309.05007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#33719;&#21462;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#21021;&#22987;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;FOLLOWUPQG&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#30340;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20986;&#20110;&#22909;&#22855;&#24515;&#32780;&#25552;&#20986;&#36319;&#36827;&#38382;&#39064;&#65292;&#36825;&#21453;&#26144;&#20102;&#20154;&#31867;&#21019;&#36896;&#24615;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#33719;&#21462;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;&#65288;FQG&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#29983;&#25104;&#33021;&#22815;&#26356;&#28145;&#20837;&#29702;&#35299;&#21021;&#22987;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#36319;&#36827;&#38382;&#39064;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;FOLLOWUPQG&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;Reddit&#35770;&#22363;&#30340;&#36229;&#36807;3K&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#65288;&#21021;&#22987;&#38382;&#39064;&#65292;&#31572;&#26696;&#65292;&#36319;&#36827;&#38382;&#39064;&#65289;&#20803;&#32452;&#65292;&#25552;&#20379;&#20102;&#23545;&#24320;&#25918;&#24615;&#38382;&#39064;&#30340;&#38750;&#19987;&#19994;&#20154;&#22763;&#21451;&#22909;&#30340;&#35299;&#37322;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;FOLLOWUPQG&#20013;&#30340;&#38382;&#39064;&#20351;&#29992;&#26356;&#22810;&#26679;&#21270;&#30340;&#23454;&#29992;&#31574;&#30053;&#26469;&#23547;&#27714;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#39640;&#23618;&#27425;&#30340;&#35748;&#30693;&#25216;&#33021;&#65288;&#22914;&#24212;&#29992;&#21644;&#20851;&#32852;&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#30340;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#25506;&#32034;&#22914;&#20309;&#22522;&#20110;&#36880;&#27493;&#28436;&#31034;&#29983;&#25104;&#29305;&#23450;&#31867;&#22411;&#30340;&#36319;&#36827;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;FOLLOWUPQG&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans ask follow-up questions driven by curiosity, which reflects a creative human cognitive process. We introduce the task of real-world information-seeking follow-up question generation (FQG), which aims to generate follow-up questions seeking a more in-depth understanding of an initial question and answer. We construct FOLLOWUPQG, a dataset of over 3K real-world (initial question, answer, follow-up question) tuples collected from a Reddit forum providing layman-friendly explanations for open-ended questions. In contrast to existing datasets, questions in FOLLOWUPQG use more diverse pragmatic strategies to seek information, and they also show higher-order cognitive skills (such as applying and relating). We evaluate current question generation models on their efficacy for generating follow-up questions, exploring how to generate specific types of follow-up questions based on step-by-step demonstrations. Our results validate FOLLOWUPQG as a challenging benchmark, as model-generated q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.15126</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#35780;&#20272;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;LVLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#24187;&#35273;&#25351;&#30340;&#26159;LVLMs&#21709;&#24212;&#20013;&#19981;&#23384;&#22312;&#20110;&#35270;&#35273;&#36755;&#20837;&#20013;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#37325;&#22823;&#21518;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#30446;&#21069;&#23545;LVLMs&#20013;&#30340;&#24187;&#35273;&#35780;&#20272;&#30340;&#30740;&#31350;&#24037;&#20316;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#12290;HaELM&#30340;&#24615;&#33021;&#36817;&#20284;&#20110;ChatGPT&#30340;95%&#65292;&#24182;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#21487;&#22797;&#29616;&#12289;&#20445;&#25252;&#38544;&#31169;&#21644;&#26412;&#22320;&#37096;&#32626;&#31561;&#39069;&#22806;&#20248;&#21183;&#12290;&#21033;&#29992;HaELM&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;LVLMs&#20013;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23548;&#33268;LVLMs&#20013;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#26377;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation halluci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Extractor&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#65292;&#29992;&#20110;&#21462;&#20195;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;Extractor&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#30701;&#30340;&#35745;&#31639;&#20851;&#38190;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2308.07661</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#19981;&#20877;&#26159;&#21807;&#19968;&#38656;&#35201;&#30340;&#19996;&#35199;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention Is Not All You Need Anymore. (arXiv:2308.07661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Extractor&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#65292;&#29992;&#20110;&#21462;&#20195;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;Extractor&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#30701;&#30340;&#35745;&#31639;&#20851;&#38190;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#27969;&#34892;&#30340;Transformer&#26550;&#26500;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#24615;&#33021;&#24179;&#34913;&#26469;&#20943;&#23569;Transformer&#20013;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#22797;&#26434;&#24230;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#23545;&#20110;Transformer&#30340;&#25345;&#32493;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21462;&#20195;Transformer&#20013;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#65288;Extractor&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;Extractor&#26367;&#25442;&#33258;&#27880;&#24847;&#26426;&#21046;&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;Extractor&#20855;&#26377;&#26356;&#30701;&#30340;&#35745;&#31639;&#20851;&#38190;&#36335;&#24452;&#65292;&#22240;&#27492;&#26377;&#28508;&#21147;&#27604;&#33258;&#27880;&#24847;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#20351;&#29992;&#21487;&#21464;&#38271;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#23545;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#24182;&#38024;&#23545;&#25105;&#20204;&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#23545;Transformer&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the popular Transformer architecture has achieved great success in many application areas, including natural language processing and computer vision. Many existing works aim to reduce the computational and memory complexity of the self-attention mechanism in the Transformer by trading off performance. However, performance is key for the continuing success of the Transformer. In this paper, a drop-in replacement for the self-attention mechanism in the Transformer, called the Extractor, is proposed. Experimental results show that replacing the self-attention mechanism with the Extractor improves the performance of the Transformer. Furthermore, the proposed Extractor has the potential to run faster than the self-attention since it has a much shorter critical path of computation. Additionally, the sequence prediction problem in the context of text generation is formulated using variable-length discrete-time Markov chains, and the Transformer is reviewed based on our unders
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#30693;&#35782;&#27880;&#20837;&#30340;&#30740;&#31350;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#65292;&#21253;&#25324;&#20027;&#27969;&#30693;&#35782;&#21644;&#30693;&#35782;&#27880;&#20837;&#21407;&#21017;&#30340;&#24635;&#32467;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#20934;&#27169;&#22411;&#30340;&#22238;&#39038;&#65292;&#24182;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#30693;&#35782;&#27880;&#20837;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04306</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#21947;&#26816;&#27979;&#30693;&#35782;&#27880;&#20837;&#65306;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive Review. (arXiv:2308.04306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#30693;&#35782;&#27880;&#20837;&#30340;&#30740;&#31350;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#65292;&#21253;&#25324;&#20027;&#27969;&#30693;&#35782;&#21644;&#30693;&#35782;&#27880;&#20837;&#21407;&#21017;&#30340;&#24635;&#32467;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#20934;&#27169;&#22411;&#30340;&#22238;&#39038;&#65292;&#24182;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#30693;&#35782;&#27880;&#20837;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#30740;&#31350;&#30340;&#21382;&#21490;&#20063;&#26631;&#24535;&#30528;&#30693;&#35782;&#27880;&#20837;&#30740;&#31350;&#30340;&#28436;&#21464;&#12290;&#38543;&#30528;&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#23545;&#23558;&#30693;&#35782;&#24212;&#29992;&#20110;&#22312;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#32467;&#26524;&#34920;&#29616;&#20986;&#26497;&#22823;&#20852;&#36259;&#12290;&#23613;&#31649;&#22312;&#38544;&#21947;&#35782;&#21035;&#39046;&#22495;&#28041;&#21450;&#30693;&#35782;&#27880;&#20837;&#30340;&#26041;&#27861;&#36880;&#28176;&#22686;&#21152;&#65292;&#20294;&#32570;&#20047;&#19968;&#31687;&#23436;&#25972;&#30340;&#20851;&#20110;&#22522;&#20110;&#30693;&#35782;&#27880;&#20837;&#30340;&#26041;&#27861;&#30340;&#32508;&#36848;&#25991;&#31456;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#32508;&#36848;&#28145;&#24230;&#23398;&#20064;&#22312;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#24212;&#29992;&#30693;&#35782;&#27880;&#20837;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#26412;&#25991;&#31995;&#32479;&#24635;&#32467;&#21644;&#27010;&#25324;&#20102;&#20027;&#27969;&#30340;&#30693;&#35782;&#21644;&#30693;&#35782;&#27880;&#20837;&#21407;&#21017;&#65292;&#21516;&#26102;&#22238;&#39038;&#20102;&#22312;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#20934;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;&#21069;&#38754;&#20020;&#30340;&#30693;&#35782;&#27880;&#20837;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The history of metaphor research also marks the evolution of knowledge infusion research. With the continued advancement of deep learning techniques in recent years, the natural language processing community has shown great interest in applying knowledge to successful results in metaphor recognition tasks. Although there has been a gradual increase in the number of approaches involving knowledge injection in the field of metaphor recognition, there is a lack of a complete review article on knowledge injection based approaches. Therefore, the goal of this paper is to provide a comprehensive review of research advances in the application of deep learning for knowledge injection in metaphor recognition tasks. In this paper, we systematically summarize and generalize the mainstream knowledge and knowledge injection principles, as well as review the datasets, evaluation metrics, and benchmark models used in metaphor recognition tasks. Finally, we explore the current issues facing knowledge 
&lt;/p&gt;</description></item><item><title>Think-on-Graph&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.07697</link><description>&lt;p&gt;
Think-on-Graph: &#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph. (arXiv:2307.07697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07697
&lt;/p&gt;
&lt;p&gt;
Think-on-Graph&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#38656;&#35201;&#30693;&#35782;&#36861;&#28335;&#24615;&#12289;&#21450;&#26102;&#24615;&#21644;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#22330;&#26223;&#20013;&#65292;&#23427;&#20204;&#32463;&#24120;&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#34920;&#29616;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Think-on-Graph&#65288;ToG&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;LLMs&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;ToG&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#19982;&#32473;&#23450;&#38382;&#39064;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#24182;&#23545;&#22806;&#37096;&#30693;&#35782;&#25968;&#25454;&#24211;&#36827;&#34892;&#25506;&#32034;&#21644;&#25512;&#29702;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#19977;&#20803;&#32452;&#12290;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#29983;&#25104;&#21253;&#21547;&#39034;&#24207;&#36830;&#25509;&#30340;&#19977;&#20803;&#32452;&#30340;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#65292;&#30452;&#21040;&#25910;&#38598;&#21040;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#25110;&#36798;&#21040;&#26368;&#22823;&#28145;&#24230;&#20026;&#27490;&#12290;&#36890;&#36807;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ToG&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;LLMs&#30340;&#21069;&#36848;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant strides in various tasks, yet they often struggle with complex reasoning and exhibit poor performance in scenarios where knowledge traceability, timeliness, and accuracy are crucial. To address these limitations, we present Think-on-Graph (ToG), a novel framework that leverages knowledge graphs to enhance LLMs' ability for deep and responsible reasoning. By employing ToG, we can identify entities relevant to a given question and conduct exploration and reasoning to retrieve related triples from an external knowledge database. This iterative procedure generates multiple reasoning pathways consisting of sequentially connected triplets until sufficient information is gathered to answer the question or the maximum depth is reached. Through experiments on complex multi-hop reasoning question-answering tasks, we demonstrate that ToG outperforms existing methods, effectively addressing the aforementioned limitations of LLMs without incurring 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.06985</link><description>&lt;p&gt;
&#36808;&#21521;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06985
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;head entity :: relationship :: tail entity&#24418;&#24335;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20107;&#23454;&#21487;&#20197;&#22312;&#19987;&#21033;&#25991;&#20214;&#20869;&#37096;&#21644;&#36328;&#25991;&#20214;&#20043;&#38388;&#32452;&#21512;&#24418;&#25104;&#30693;&#35782;&#22270;&#65292;&#29992;&#20316;&#34920;&#31034;&#21644;&#23384;&#20648;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#24037;&#31243;&#35774;&#35745;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#20851;&#31995;&#26469;&#22635;&#20805;&#32479;&#35745;&#36817;&#20284;&#32780;&#38750;&#20107;&#23454;&#30340;&#19977;&#20803;&#32452;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#21477;&#23376;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;&#22312;&#30830;&#23450;&#20102;&#19968;&#23545;&#23454;&#20307;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#21478;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#29305;&#23450;&#34920;&#31034;&#36825;&#23545;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#31995;&#26631;&#35760;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#20123;&#26631;&#35760;&#22120;&#65292;&#25105;&#20204;&#25163;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;44,227&#20010;&#21477;&#23376;&#21644;&#30456;&#24212;&#20107;&#23454;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#36890;&#24120;&#25512;&#33616;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20854;&#20013;&#25105;&#20204;&#39044;.
&lt;/p&gt;
&lt;p&gt;
Aiming to populate generalizable engineering design knowledge, we propose a method to extract facts of the form head entity :: relationship :: tail entity from sentences found in patent documents. These facts could be combined within and across patent documents to form knowledge graphs that serve as schemes for representing as well as storing design knowledge. Existing methods in engineering design literature often utilise a set of predefined relationships to populate triples that are statistical approximations rather than facts. In our method, we train a tagger to identify both entities and relationships from a sentence. Given a pair of entities thus identified, we train another tagger to identify the relationship tokens that specifically denote the relationship between the pair. For training these taggers, we manually construct a dataset of 44,227 sentences and corresponding facts. We also compare the performance of the method against typically recommended approaches, wherein, we pre
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26412;&#20307;&#25512;&#29702;&#26500;&#24314;&#20219;&#21153;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#26009;&#24211;&#65292;&#23545;&#22823;&#22411;&#20225;&#19994;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2306.10723</link><description>&lt;p&gt;
&#36890;&#36807;&#26412;&#20307;&#25512;&#29702;&#23545;&#22823;&#22411;&#20225;&#19994;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Enterprise Language Models via Ontological Reasoning. (arXiv:2306.10723v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10723
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26412;&#20307;&#25512;&#29702;&#26500;&#24314;&#20219;&#21153;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#26009;&#24211;&#65292;&#23545;&#22823;&#22411;&#20225;&#19994;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#24494;&#35843;&#20316;&#20026;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#36866;&#24212;&#22810;&#26679;&#21270;&#30340;&#30446;&#26631;&#12290;&#20219;&#21153;&#29305;&#23450;&#24615;&#24212;&#35813;&#19982;&#39046;&#22495;&#23450;&#21521;&#30456;&#32467;&#21512;&#65292;&#21363;&#23558;&#35821;&#35328;&#27169;&#22411;&#19987;&#38376;&#21270;&#65292;&#20197;&#20934;&#30830;&#22320;&#22788;&#29702;&#32473;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#25110;&#32773;&#26368;&#22810;&#21482;&#36890;&#36807;&#25968;&#25454;&#24211;&#20013;&#30340;&#22522;&#30784;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#24573;&#30053;&#20102;&#22522;&#20110;&#19994;&#21153;&#30340;&#23450;&#20041;&#21644;&#39046;&#22495;&#32463;&#39564;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20225;&#19994;&#30693;&#35782;&#22270;&#35889;&#33021;&#22815;&#36890;&#36807;&#26412;&#20307;&#25512;&#29702;&#25429;&#33719;&#21644;&#22686;&#21152;&#36825;&#31181;&#39046;&#22495;&#30693;&#35782;&#12290;&#20026;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#19982;&#20225;&#19994;&#30693;&#35782;&#22270;&#35889;&#30340;&#39046;&#22495;&#23450;&#21521;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#21033;&#29992;&#26412;&#20307;&#25512;&#29702;&#30340;&#33021;&#21147;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#26500;&#24314;&#20219;&#21153;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to diverse goals, thanks to task-specific training data. Task specificity should go hand in hand with domain orientation, that is, the specialization of an LLM to accurately address the tasks of a given realm of interest. However, models are usually fine-tuned over publicly available data or, at most, over ground data from databases, ignoring business-level definitions and domain experience. On the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and augment such domain knowledge via ontological reasoning. With the goal of combining LLM flexibility with the domain orientation of EKGs, we propose a novel neurosymbolic architecture that leverages the power of ontological reasoning to build task- and domain-specific corpora for LLM fine-tuning.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03763</link><description>&lt;p&gt;
ChatGPT&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#24050;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20174;&#26102;&#38388;&#25991;&#26412;&#25968;&#25454;&#65288;&#23588;&#20854;&#26159;&#36130;&#32463;&#26032;&#38395;&#65289;&#25512;&#26029;&#21160;&#24577;&#32593;&#32476;&#32467;&#26500;&#30340;&#28508;&#21147;&#20173;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#30340;&#22270;&#25512;&#26029;&#33021;&#21147;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24039;&#22937;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#23558;&#36825;&#20123;&#32593;&#32476;&#32467;&#26500;&#34701;&#21512;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36827;&#34892;&#21518;&#32493;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#27169;&#22411;&#30340;&#20135;&#20986;&#26500;&#24314;&#30340;&#32452;&#21512;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#24180;&#21270;&#32047;&#35745;&#22238;&#25253;&#12289;&#26356;&#20302;&#30340;&#27874;&#21160;&#24615;&#21644;&#26368;&#22823;&#22238;&#25764;&#12290;&#36825;&#31181;&#21331;&#36234;&#34920;&#29616;&#31361;&#26174;&#20102;ChatGPT&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#32593;&#32476;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25511;&#21046;TTS&#35821;&#38899;&#34920;&#29616;&#39118;&#26684;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#38750;&#34920;&#29616;&#24615;&#35821;&#26009;&#24211;&#19978;&#30340;TTS&#27169;&#22411;&#25552;&#20379;&#36866;&#24403;&#30340;&#38901;&#24459;&#24314;&#35758;&#65292;&#20351;&#20854;&#29983;&#25104;&#34920;&#29616;&#21147;&#26356;&#24378;&#30340;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2305.10321</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25511;&#21046;&#35828;&#35805;&#39118;&#26684;&#20197;&#23454;&#29616;&#34920;&#29616;&#24615;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Using a Large Language Model to Control Speaking Style for Expressive TTS. (arXiv:2305.10321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10321
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25511;&#21046;TTS&#35821;&#38899;&#34920;&#29616;&#39118;&#26684;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#38750;&#34920;&#29616;&#24615;&#35821;&#26009;&#24211;&#19978;&#30340;TTS&#27169;&#22411;&#25552;&#20379;&#36866;&#24403;&#30340;&#38901;&#24459;&#24314;&#35758;&#65292;&#20351;&#20854;&#29983;&#25104;&#34920;&#29616;&#21147;&#26356;&#24378;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24688;&#24403;&#30340;&#38901;&#24459;&#23545;&#20110;&#25104;&#21151;&#30340;&#21475;&#22836;&#20132;&#27969;&#33267;&#20851;&#37325;&#35201;&#12290;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#24050;&#34987;&#35777;&#26126;&#22312;&#39044;&#27979;&#38901;&#24459;&#26041;&#38754;&#26377;&#25152;&#24110;&#21161;&#65292;&#20294;&#19981;&#20801;&#35768;&#22312;&#21487;&#33021;&#30340;&#38901;&#24459;&#28436;&#32462;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#12290;&#22522;&#20110;&#21442;&#32771;&#35821;&#38899;&#30340;TTS&#27169;&#22411;&#35797;&#22270;&#36890;&#36807;&#22312;&#21442;&#32771;&#35821;&#38899;&#26679;&#26412;&#22522;&#30784;&#19978;&#29983;&#25104;&#35821;&#38899;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35821;&#38899;&#65292;&#20294;&#38656;&#35201;&#25214;&#21040;&#36866;&#24403;&#30340;&#21442;&#32771;&#26679;&#26412;&#12290;&#24050;&#32463;&#20351;&#29992;&#36275;&#22815;&#22823;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#21508;&#31181;&#19982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#24314;&#35758;&#36866;&#24403;&#30340;&#38901;&#24459;&#20197;&#23454;&#29616;&#34920;&#29616;&#24615;TTS&#12290;&#25105;&#20204;&#22312;&#38750;&#34920;&#29616;&#24615;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;TTS&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#24314;&#35758;&#26356;&#25913;&#38899;&#35843;&#12289;&#33021;&#37327;&#21644;&#25345;&#32493;&#26102;&#38388;&#12290;&#25552;&#31034;&#21487;&#20197;&#20026;&#20219;&#20309;&#20219;&#21153;&#35774;&#35745;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#35828;&#35805;&#39118;&#26684;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#25552;&#31034;&#27169;&#22411;&#36827;&#34892;&#24314;&#35758;&#12290;&#19982;&#22522;&#32447;&#27169;&#22411;&#30340;31.0&#65285;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;49.9&#65285;&#30340;&#24773;&#20917;&#19979;&#34987;&#35780;&#20026;&#26368;&#21512;&#36866;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Appropriate prosody is critical for successful spoken communication. Contextual word embeddings are proven to be helpful in predicting prosody but do not allow for choosing between plausible prosodic renditions. Reference-based TTS models attempt to address this by conditioning speech generation on a reference speech sample. These models can generate expressive speech but this requires finding an appropriate reference.  Sufficiently large generative language models have been used to solve various language-related tasks. We explore whether such models can be used to suggest appropriate prosody for expressive TTS. We train a TTS model on a non-expressive corpus and then prompt the language model to suggest changes to pitch, energy and duration. The prompt can be designed for any task and we prompt the model to make suggestions based on target speaking style and dialogue context. The proposed method is rated most appropriate in 49.9\% of cases compared to 31.0\% for a baseline model.
&lt;/p&gt;</description></item><item><title>ChatGraph&#36890;&#36807;&#23558;ChatGPT&#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;</title><link>http://arxiv.org/abs/2305.03513</link><description>&lt;p&gt;
ChatGraph: &#36890;&#36807;&#23558;ChatGPT&#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#22270;&#24418;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs. (arXiv:2305.03513v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03513
&lt;/p&gt;
&lt;p&gt;
ChatGraph&#36890;&#36807;&#23558;ChatGPT&#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#26368;&#36817;&#25512;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#38459;&#30861;&#20102;&#23427;&#30340;&#28508;&#22312;&#24212;&#29992;&#65306;&#65288;1&#65289;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#24494;&#35843;&#30340;&#19981;&#28789;&#27963;&#24615;&#21644;&#65288;2&#65289;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#30340;&#33021;&#21147;&#26469;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#65288;&#22914;&#25991;&#26412;&#20998;&#31867;&#65289;&#65292;&#21516;&#26102;&#25552;&#39640;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, as a recently launched large language model (LLM), has shown superior performance in various natural language processing (NLP) tasks. However, two major limitations hinder its potential applications: (1) the inflexibility of finetuning on downstream tasks and (2) the lack of interpretability in the decision-making process. To tackle these limitations, we propose a novel framework that leverages the power of ChatGPT for specific tasks, such as text classification, while improving its interpretability. The proposed framework conducts a knowledge graph extraction task to extract refined and structural knowledge from the raw data using ChatGPT. The rich knowledge is then converted into a graph, which is further used to train an interpretable linear classifier to make predictions. To evaluate the effectiveness of our proposed method, we conduct experiments on four datasets. The result shows that our method can significantly improve the performance compared to directly utilizing Cha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;LLMs&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#21442;&#32771;&#25991;&#26412;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#32500;&#24230;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2303.15078</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26159;&#25688;&#35201;&#35780;&#20272;&#30340;&#19981;&#21516;&#35282;&#33394;&#25198;&#28436;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Diverse Role-Players for Summarization Evaluation. (arXiv:2303.15078v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;LLMs&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#21442;&#32771;&#25991;&#26412;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#32500;&#24230;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#35780;&#20272;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#35821;&#35328;&#35780;&#20272;&#30340;&#19968;&#20010;&#22823;&#25361;&#25112;&#26159;&#29616;&#26377;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#20998;&#27495;&#12290;&#20363;&#22914;&#65292;&#25991;&#26723;&#25688;&#35201;&#30340;&#36136;&#37327;&#21487;&#20197;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#32773;&#20174;&#23458;&#35266;&#26041;&#38754;&#65288;&#22914;&#35821;&#27861;&#21644;&#35821;&#20041;&#30340;&#27491;&#30830;&#24615;&#65289;&#20197;&#21450;&#20027;&#35266;&#32500;&#24230;&#65288;&#22914;&#20840;&#38754;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#26377;&#36259;&#24615;&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#22823;&#22810;&#25968;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65288;&#22914;BLUE/ROUGE&#65289;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#20197;&#19978;&#32500;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#20174;&#23458;&#35266;&#21644;&#20027;&#35266;&#26041;&#38754;&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#21442;&#32771;&#25991;&#26412;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35282;&#33394;&#25198;&#28436;&#32773;&#25552;&#31034;&#26426;&#21046;&#30340;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#32500;&#24230;&#30340;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text summarization has a wide range of applications in many scenarios. The evaluation of the quality of the generated text is a complex problem. A big challenge to language evaluation is that there is a clear divergence between existing metrics and human evaluation. For example, the quality of a document summary can be measured by human annotators from both objective aspects, such as grammatical and semantic correctness, as well as subjective dimensions, such as comprehensiveness, succinctness, and interestingness. Most of the automatic evaluation methods like BLUE/ROUGE may be not able to capture the above dimensions well. In this paper, we propose a new evaluation framework based on LLMs, which provides a comprehensive evaluation framework by comparing generated text and reference text from both objective and subjective aspects. First, we propose to model objective and subjective dimensions of generated text based on roleplayers prompting mechanism. Furthermore, we introduce a contex
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#24863;&#30693;&#20803;&#23398;&#20064;&#65288;CAML&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#20013;&#23384;&#22312;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#22806;&#37096;&#24490;&#29615;&#20013;&#23398;&#20064;&#19968;&#33268;&#30340;&#35821;&#20041;&#31561;&#20215;&#21477;&#23376;&#30340;&#20803;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20869;&#37096;&#24490;&#29615;&#35757;&#32451;&#19968;&#20010;&#20174;&#20803;&#34920;&#31034;&#21040;&#32763;&#35793;&#32467;&#26524;&#30340;&#26144;&#23556;&#65292;&#20197;&#23454;&#29616;&#26356;&#21152;&#21487;&#38752;&#30340;&#32763;&#35793;&#12290;</title><link>http://arxiv.org/abs/2303.10966</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#33268;&#24615;&#24863;&#30693;&#20803;&#23398;&#20064;&#23454;&#29616;&#21487;&#38752;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Reliable Neural Machine Translation with Consistency-Aware Meta-Learning. (arXiv:2303.10966v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#24863;&#30693;&#20803;&#23398;&#20064;&#65288;CAML&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#20013;&#23384;&#22312;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#22806;&#37096;&#24490;&#29615;&#20013;&#23398;&#20064;&#19968;&#33268;&#30340;&#35821;&#20041;&#31561;&#20215;&#21477;&#23376;&#30340;&#20803;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20869;&#37096;&#24490;&#29615;&#35757;&#32451;&#19968;&#20010;&#20174;&#20803;&#34920;&#31034;&#21040;&#32763;&#35793;&#32467;&#26524;&#30340;&#26144;&#23556;&#65292;&#20197;&#23454;&#29616;&#26356;&#21152;&#21487;&#38752;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#32763;&#35793;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;NMT&#31995;&#32479;&#32570;&#20047;&#21487;&#38752;&#24615;&#65292;&#20854;&#36755;&#20986;&#24120;&#24120;&#21463;&#21040;&#36755;&#20837;&#20013;&#35789;&#27719;&#25110;&#21477;&#27861;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#32763;&#35793;&#36136;&#37327;&#23384;&#22312;&#36739;&#22823;&#30340;&#21464;&#24322;&#12290;&#36825;&#31181;&#38480;&#21046;&#38459;&#30861;&#20102;NMT&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#36896;&#25104;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#22240;&#32032;&#26159;&#65292;&#37319;&#29992;&#19968;&#23545;&#19968;&#33539;&#24335;&#35757;&#32451;&#30340;NMT&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#28304;&#35821;&#35328;&#22810;&#26679;&#24615;&#29616;&#35937;&#65292;&#21363;&#20855;&#26377;&#30456;&#21516;&#24847;&#20041;&#30340;&#36755;&#20837;&#21487;&#33021;&#20197;&#19981;&#21516;&#26041;&#24335;&#34920;&#36798;&#12290;&#26412;&#30740;&#31350;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#31639;&#27861;&#25512;&#23548;&#20986;&#30340;&#19968;&#33268;&#24615;&#24863;&#30693;&#20803;&#23398;&#20064;&#65288;CAML&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#23427;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CAML&#30340;NMT&#27169;&#22411;&#65288;&#21629;&#21517;&#20026;CoNMT&#65289;&#39318;&#20808;&#22312;&#22806;&#37096;&#24490;&#29615;&#20013;&#23398;&#20064;&#19968;&#33268;&#30340;&#35821;&#20041;&#31561;&#20215;&#21477;&#23376;&#30340;&#20803;&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#20869;&#37096;&#24490;&#29615;&#35757;&#32451;&#19968;&#20010;&#20174;&#20803;&#34920;&#31034;&#21040;&#32763;&#35793;&#32467;&#26524;&#30340;&#26144;&#23556;&#65292;&#20197;&#23454;&#29616;&#26356;&#21152;&#21487;&#38752;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation (NMT) has achieved remarkable success in producing high-quality translations. However, current NMT systems suffer from a lack of reliability, as their outputs that are often affected by lexical or syntactic changes in inputs, resulting in large variations in quality. This limitation hinders the practicality and trustworthiness of NMT. A contributing factor to this problem is that NMT models trained with the one-to-one paradigm struggle to handle the source diversity phenomenon, where inputs with the same meaning can be expressed differently. In this work, we treat this problem as a bilevel optimization problem and present a consistency-aware meta-learning (CAML) framework derived from the model-agnostic meta-learning (MAML) algorithm to address it. Specifically, the NMT model with CAML (named CoNMT) first learns a consistent meta representation of semantically equivalent sentences in the outer loop. Subsequently, a mapping from the meta representation to the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#21487;&#27735;&#23398;&#38498;&#30340;&#35270;&#39057;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#21644;&#38899;&#35270;&#39057;&#21516;&#27493;&#25216;&#26415;&#26500;&#24314;&#24341;&#20154;&#20837;&#32988;&#30340;&#25945;&#32946;&#35270;&#39057;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#21487;&#38752;&#30340;&#32763;&#35793;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#39640;&#25928;&#36136;&#37327;&#31649;&#29702;&#21644;&#20943;&#23569;&#20154;&#24037;&#32763;&#35793;&#24037;&#20316;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#25552;&#20379;&#32763;&#35793;&#35270;&#39057;&#32473;&#29992;&#25143;&#24182;&#25910;&#38598;&#29992;&#25143;&#32416;&#27491;&#20197;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2301.03141</link><description>&lt;p&gt;
&#24212;&#29992;&#33258;&#21160;&#26426;&#22120;&#32763;&#35793;&#25216;&#26415;&#20110;&#25945;&#32946;&#35270;&#39057;&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Applying Automated Machine Translation to Educational Video Courses. (arXiv:2301.03141v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#21487;&#27735;&#23398;&#38498;&#30340;&#35270;&#39057;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#21644;&#38899;&#35270;&#39057;&#21516;&#27493;&#25216;&#26415;&#26500;&#24314;&#24341;&#20154;&#20837;&#32988;&#30340;&#25945;&#32946;&#35270;&#39057;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#21487;&#38752;&#30340;&#32763;&#35793;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#39640;&#25928;&#36136;&#37327;&#31649;&#29702;&#21644;&#20943;&#23569;&#20154;&#24037;&#32763;&#35793;&#24037;&#20316;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#25552;&#20379;&#32763;&#35793;&#35270;&#39057;&#32473;&#29992;&#25143;&#24182;&#25910;&#38598;&#29992;&#25143;&#32416;&#27491;&#20197;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#32763;&#35793;&#27169;&#22411;&#65292;&#23558;&#21487;&#27735;&#23398;&#38498;&#30340;&#35270;&#39057;&#33258;&#21160;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#65292;&#24182;&#24212;&#29992;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#21644;&#38899;&#35270;&#39057;&#21516;&#27493;&#25216;&#26415;&#26469;&#26500;&#24314;&#24341;&#20154;&#20837;&#32988;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#21644;&#24314;&#31435;&#20102;&#20004;&#31181;&#21487;&#38752;&#30340;&#32763;&#35793;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#24448;&#36820;&#32763;&#35793;&#30340;&#26041;&#24335;&#39640;&#25928;&#31649;&#29702;&#32763;&#35793;&#36136;&#37327;&#24182;&#20943;&#23569;&#20154;&#24037;&#32763;&#35793;&#24037;&#20316;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#21521;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#32763;&#35793;&#35270;&#39057;&#24182;&#25910;&#38598;&#29992;&#25143;&#32416;&#27491;&#20197;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We studied the capability of automated machine translation in the online video education space by automatically translating Khan Academy videos with state-of-the-art translation models and applying text-to-speech synthesis and audio/video synchronization to build engaging videos in target languages. We also analyzed and established two reliable translation confidence estimators based on round-trip translations in order to efficiently manage translation quality and reduce human translation effort. Finally, we developed a deployable system to deliver translated videos to end users and collect user corrections for iterative improvement.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#35821;&#35328;&#30693;&#35782;&#30340;&#20010;&#24615;&#21270;&#33258;&#21457;&#35821;&#38899;&#21512;&#25104;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#22635;&#20805;&#24335;&#20572;&#39039;&#22312;&#35328;&#35821;&#29983;&#25104;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#22635;&#20805;&#24335;&#20572;&#39039;&#25554;&#20837;&#21644;&#38750;&#20010;&#24615;&#21270;&#22635;&#20805;&#24335;&#20572;&#39039;&#39044;&#27979;&#26041;&#27861;&#30340;&#27604;&#36739;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2210.07559</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#30693;&#35782;&#30340;&#20010;&#24615;&#21270;&#33258;&#21457;&#35821;&#38899;&#21512;&#25104;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Empirical Study Incorporating Linguistic Knowledge on Filled Pauses for Personalized Spontaneous Speech Synthesis. (arXiv:2210.07559v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07559
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#35821;&#35328;&#30693;&#35782;&#30340;&#20010;&#24615;&#21270;&#33258;&#21457;&#35821;&#38899;&#21512;&#25104;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#22635;&#20805;&#24335;&#20572;&#39039;&#22312;&#35328;&#35821;&#29983;&#25104;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#22635;&#20805;&#24335;&#20572;&#39039;&#25554;&#20837;&#21644;&#38750;&#20010;&#24615;&#21270;&#22635;&#20805;&#24335;&#20572;&#39039;&#39044;&#27979;&#26041;&#27861;&#30340;&#27604;&#36739;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#30693;&#35782;&#30340;&#20010;&#24615;&#21270;&#33258;&#21457;&#35821;&#38899;&#21512;&#25104;&#30340;&#32508;&#21512;&#23454;&#35777;&#30740;&#31350;&#12290;&#38543;&#30528;&#35821;&#38899;&#20811;&#38534;&#29992;&#20110;&#38405;&#35835;&#39118;&#26684;&#30340;&#35821;&#38899;&#21512;&#25104;&#30340;&#20986;&#29616;&#65292;&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#20811;&#38534;&#33539; paradigm &#20197;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#21644;&#33258;&#21457;&#24335;&#35821;&#38899;&#21512;&#25104;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20010;&#24615;&#21270;&#33258;&#21457;&#35821;&#38899;&#21512;&#25104;&#65292;&#26082;&#21487;&#20197;&#20811;&#38534;&#20010;&#20307;&#22768;&#38899;&#30340;&#38899;&#33394;&#65292;&#20063;&#21487;&#20197;&#27169;&#25311;&#20854;&#35328;&#35821;&#19981;&#27969;&#30021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22788;&#29702;&#20102;&#22635;&#20805;&#24335;&#20572;&#39039;&#65292;&#36825;&#26159;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#22312;&#24515;&#29702;&#23398;&#21644;&#35821;&#35328;&#23398;&#20013;&#24050;&#34987;&#35748;&#20026;&#22312;&#35328;&#35821;&#29983;&#25104;&#21644;&#20132;&#27969;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#20102;&#27604;&#36739;&#35780;&#20272;&#20010;&#24615;&#21270;&#22635;&#20805;&#24335;&#20572;&#39039;&#25554;&#20837;&#21644;&#38750;&#20010;&#24615;&#21270;&#22635;&#20805;&#24335;&#20572;&#39039;&#39044;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#35757;&#32451;&#26377;&#22810;&#35828;&#35805;&#20154;&#35821;&#26009;&#24211;&#30340;&#38750;&#20010;&#24615;&#21270;&#22806;&#37096;&#22635;&#20805;&#24335;&#20572;&#39039;&#39044;&#27979;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#38416;&#26126;&#20102;&#22635;&#20805;&#24335;&#20572;&#39039;&#30340;&#20301;&#32622;-&#35789;&#27719;&#32416;&#32544;&#38382;&#39064;&#65292;&#21363;&#38656;&#35201;&#20934;&#30830;&#39044;&#27979;&#20301;&#32622;&#20197;&#23454;&#29616;&#33258;&#28982;&#27969;&#30021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive empirical study for personalized spontaneous speech synthesis on the basis of linguistic knowledge. With the advent of voice cloning for reading-style speech synthesis, a new voice cloning paradigm for human-like and spontaneous speech synthesis is required. We, therefore, focus on personalized spontaneous speech synthesis that can clone both the individual's voice timbre and speech disfluency. Specifically, we deal with filled pauses, a major source of speech disfluency, which is known to play an important role in speech generation and communication in psychology and linguistics. To comparatively evaluate personalized filled pause insertion and non-personalized filled pause prediction methods, we developed a speech synthesis method with a non-personalized external filled pause predictor trained with a multi-speaker corpus. The results clarify the position-word entanglement of filled pauses, i.e., the necessity of precisely predicting positions for naturalnes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31807;&#35760;&#65288;BK&#65289;&#30340;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#22120;&#22312;&#22823;&#27169;&#22411;&#21644;&#39640;&#32500;&#25968;&#25454;&#19978;&#30340;&#24555;&#36895;&#35757;&#32451;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.00038</link><description>&lt;p&gt;
&#22312;&#23567;&#25104;&#26412;&#19978;&#23545;&#22823;&#27169;&#22411;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Optimization on Large Model at Small Cost. (arXiv:2210.00038v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31807;&#35760;&#65288;BK&#65289;&#30340;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#22120;&#22312;&#22823;&#27169;&#22411;&#21644;&#39640;&#32500;&#25968;&#25454;&#19978;&#30340;&#24555;&#36895;&#35757;&#32451;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20248;&#21270;&#26159;&#23398;&#20064;&#20934;&#30830;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#20934;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36880;&#26679;&#26412;&#26799;&#24230;&#20462;&#21098;&#65292;DP&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#39640;&#26114;&#12290;&#29616;&#26377;&#30340;DP&#23454;&#29616;&#27604;&#26631;&#20934;&#65288;&#38750;&#31169;&#26377;&#65289;&#35757;&#32451;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#39640;2-1000&#20493;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31807;&#35760;&#65288;BK&#65289;&#25216;&#26415;&#65292;&#23427;&#23454;&#29616;&#20102;&#29616;&#26377;&#30340;DP&#20248;&#21270;&#22120;&#65288;&#20174;&#32780;&#23454;&#29616;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#65289;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#26377;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;BK&#20351;&#24471;&#23545;&#22823;&#22411;&#27169;&#22411;&#21644;&#39640;&#32500;&#25968;&#25454;&#36827;&#34892;DP&#35757;&#32451;&#30340;&#36895;&#24230;&#21644;&#33410;&#30465;&#20869;&#23384;&#19982;&#26631;&#20934;&#35757;&#32451;&#30456;&#24403;&#65292;&#32780;&#20197;&#21069;&#30340;DP&#31639;&#27861;&#21487;&#33021;&#22240;&#20869;&#23384;&#38169;&#35823;&#32780;&#20302;&#25928;&#25110;&#26080;&#27861;&#35757;&#32451;&#12290;&#36890;&#36807;&#22797;&#26434;&#24230;&#20998;&#26512;&#21644;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;BK&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private (DP) optimization is the standard paradigm to learn large neural networks that are accurate and privacy-preserving. The computational cost for DP deep learning, however, is notoriously heavy due to the per-sample gradient clipping. Existing DP implementations are 2-1000X more costly in time and space complexity than the standard (non-private) training. In this work, we develop a novel Book-Keeping (BK) technique that implements existing DP optimizers (thus achieving the same accuracy), with a substantial improvement on the computational cost. Specifically, BK enables DP training on large models and high dimensional data to be roughly as fast and memory-saving as the standard training, whereas previous DP algorithms can be inefficient or incapable of training due to memory error. The computational advantage of BK is supported by the complexity analysis as well as extensive experiments on vision and language tasks. Our implementation achieves state-of-the-art (SOTA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#35299;&#32806;&#30340;&#26816;&#32034;&#34920;&#31034;&#29992;&#20110;&#26368;&#36817;&#37051;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#26500;&#24314;&#38590;&#36127;&#26679;&#26412;&#65292;&#25913;&#36827;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#21644;BLEU&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2209.08738</link><description>&lt;p&gt;
&#23398;&#20064;&#35299;&#32806;&#30340;&#26816;&#32034;&#34920;&#31034;&#29992;&#20110;&#26368;&#36817;&#37051;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Learning Decoupled Retrieval Representation for Nearest Neighbour Neural Machine Translation. (arXiv:2209.08738v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#35299;&#32806;&#30340;&#26816;&#32034;&#34920;&#31034;&#29992;&#20110;&#26368;&#36817;&#37051;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#26500;&#24314;&#38590;&#36127;&#26679;&#26412;&#65292;&#25913;&#36827;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#21644;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
K-&#26368;&#36817;&#37051;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;kNN-MT&#65289;&#25104;&#21151;&#22320;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#26816;&#32034;&#21333;&#35789;&#32423;&#21035;&#30340;&#34920;&#31034;&#26469;&#24341;&#20837;&#22806;&#37096;&#35821;&#26009;&#24211;&#12290;&#36890;&#24120;&#65292;kNN-MT&#20511;&#29992;&#32763;&#35793;&#20219;&#21153;&#20013;&#29616;&#25104;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65288;&#20363;&#22914;&#26368;&#21518;&#19968;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;&#36755;&#20986;&#65289;&#20316;&#20026;&#26816;&#32034;&#20219;&#21153;&#30340;&#26597;&#35810;&#21521;&#37327;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#23558;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#32806;&#21512;&#23545;&#20110;&#32454;&#31890;&#24230;&#30340;&#26816;&#32034;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#20174;&#21407;&#22987;&#19978;&#19979;&#25991;&#34920;&#31034;&#27966;&#29983;&#30340;&#29420;&#29305;&#26816;&#32034;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#26500;&#24314;&#38590;&#36127;&#26679;&#26412;&#30340;&#26041;&#27861;&#12290;&#22312;&#20116;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21407;&#22987;kNN-MT&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#21644;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
K-Nearest Neighbor Neural Machine Translation (kNN-MT) successfully incorporates external corpus by retrieving word-level representations at test time. Generally, kNN-MT borrows the off-the-shelf context representation in the translation task, e.g., the output of the last decoder layer, as the query vector of the retrieval task. In this work, we highlight that coupling the representations of these two tasks is sub-optimal for fine-grained retrieval. To alleviate it, we leverage supervised contrastive learning to learn the distinctive retrieval representation derived from the original context representation. We also propose a fast and effective approach to constructing hard negative samples. Experimental results on five domains show that our approach improves the retrieval accuracy and BLEU score compared to vanilla kNN-MT.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2205.14704</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65306;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31361;&#30772;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#36981;&#24490;&#21442;&#25968;&#21270;&#23398;&#20064;&#33539;&#24335;&#65307;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#36951;&#24536;&#21644;&#26426;&#26800;&#35760;&#24518;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;RetroPrompt&#65292;&#26088;&#22312;&#20174;&#35760;&#24518;&#20013;&#23558;&#30693;&#35782;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#19982;&#20256;&#32479;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;RetroPrompt&#20174;&#35757;&#32451;&#23454;&#20363;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#25918;&#24335;&#30693;&#35782;&#24211;&#65292;&#24182;&#22312;&#36755;&#20837;&#12289;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#26045;&#26816;&#32034;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#20174;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#19978;&#19979;&#25991;&#29992;&#20110;&#22686;&#24378;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;RetroPrompt&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;&#65292;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#12290;&#36890;&#36807;&#26500;&#24314;&#24320;&#25918;&#24335;&#23384;&#20648;&#24211;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#25554;&#20540;&#30340;&#26041;&#24335;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26681;&#25454;&#23384;&#20648;&#24211;&#20013;&#30340;&#35760;&#24518;&#20449;&#24687;&#25512;&#26029;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2205.02355</link><description>&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#20316;&#20026;&#24320;&#20070;&#32771;&#35797;&#65306;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning. (arXiv:2205.02355v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02355
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;&#65292;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#12290;&#36890;&#36807;&#26500;&#24314;&#24320;&#25918;&#24335;&#23384;&#20648;&#24211;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#25554;&#20540;&#30340;&#26041;&#24335;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26681;&#25454;&#23384;&#20648;&#24211;&#20013;&#30340;&#35760;&#24518;&#20449;&#24687;&#25512;&#26029;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#20851;&#31995;&#25277;&#21462;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#21487;&#33021;&#20173;&#28982;&#26080;&#27861;&#25512;&#24191;&#21040;&#37027;&#20123;&#32597;&#35265;&#25110;&#22256;&#38590;&#30340;&#27169;&#24335;&#20013;&#12290;&#25105;&#20204;&#23558;&#20851;&#31995;&#25277;&#21462;&#35270;&#20026;&#19968;&#31181;&#24320;&#25918;&#24335;&#32771;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;&#30340;&#21322;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#25918;&#24335;&#23384;&#20648;&#24211;&#65292;&#29992;&#20110;&#26816;&#32034;&#22522;&#20110;&#25552;&#31034;&#30340;&#23454;&#20363;&#34920;&#31034;&#21644;&#30456;&#24212;&#30340;&#20851;&#31995;&#26631;&#31614;&#20316;&#20026;&#35760;&#24518;&#30340;&#38190;&#20540;&#23545;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#22522;&#20110;PLM&#30340;&#22522;&#26412;&#36755;&#20986;&#19982;&#23384;&#20648;&#24211;&#19978;&#30340;&#38750;&#21442;&#25968;&#26368;&#36817;&#37051;&#20998;&#24067;&#26469;&#25512;&#26029;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have contributed significantly to relation extraction by demonstrating remarkable few-shot learning abilities. However, prompt tuning methods for relation extraction may still fail to generalize to those rare or hard patterns. Note that the previous parametric learning paradigm can be viewed as memorization regarding training data as a book and inference as the close-book test. Those long-tailed or hard patterns can hardly be memorized in parameters given few-shot instances. To this end, we regard RE as an open-book examination and propose a new semiparametric paradigm of retrieval-enhanced prompt tuning for relation extraction. We construct an open-book datastore for retrieval regarding prompt-based instance representations and corresponding relation labels as memorized key-value pairs. During inference, the model can infer relations by linearly interpolating the base output of PLM with the non-parametric nearest neighbor distribution over the datastore. In
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#19979;&#26377;&#25928;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#20808;&#21069;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.04392</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Contrastive Demonstration Tuning for Pre-trained Language Models. (arXiv:2204.04392v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#19979;&#26377;&#25928;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#20808;&#21069;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#65292;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#25110;&#28436;&#31034;&#21487;&#20197;&#26377;&#25928;&#22320;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#25628;&#32034;&#31163;&#25955;&#25110;&#36830;&#32493;&#25552;&#31034;&#25110;&#20248;&#21270;&#35821;&#35328;&#34920;&#36798;&#32773;&#65292;&#20294;&#23545;&#20110;&#28436;&#31034;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#28436;&#31034;&#31034;&#20363;&#23545;&#20110;&#26368;&#32456;&#30340;&#25552;&#31034;&#35843;&#20248;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25554;&#25300;&#12289;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;&#65292;&#23427;&#19981;&#38656;&#35201;&#36827;&#34892;&#28436;&#31034;&#37319;&#26679;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#65306;&#65288;i&#65289;&#23884;&#20837;&#21040;&#20219;&#20309;&#20808;&#21069;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#20013;&#65307;&#65288;ii&#65289;&#25193;&#23637;&#21040;&#20855;&#26377;&#22823;&#37327;&#31867;&#21035;&#30340;&#24191;&#27867;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;&#22312;16&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20808;&#21069;&#30340;LM-BFF&#21644;P-tuning&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models can be effectively stimulated by textual prompts or demonstrations, especially in low-data scenarios. Recent works have focused on automatically searching discrete or continuous prompts or optimized verbalizers, yet studies for the demonstration are still limited. Concretely, the demonstration examples are crucial for an excellent final performance of prompt-tuning. In this paper, we propose a novel pluggable, extensible, and efficient approach named contrastive demonstration tuning, which is free of demonstration sampling. Furthermore, the proposed approach can be: (i) Plugged into any previous prompt-tuning approaches; (ii) Extended to widespread classification tasks with a large number of categories. Experimental results on 16 datasets illustrate that our method integrated with previous approaches LM-BFF and P-tuning can yield better performance. Code is available in https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#26684;&#24335;&#65292;&#20174;&#32780;&#25506;&#32034;&#20102;NLP&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#20840;&#35980;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/1910.10683</link><description>&lt;p&gt;
&#25506;&#32034;&#20351;&#29992;&#32479;&#19968;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#22120;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. (arXiv:1910.10683v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.10683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#26684;&#24335;&#65292;&#20174;&#32780;&#25506;&#32034;&#20102;NLP&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#20840;&#35980;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#20043;&#21069;&#39318;&#20808;&#22312;&#25968;&#25454;&#20016;&#23500;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#36801;&#31227;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#20652;&#29983;&#20102;&#22810;&#31181;&#26041;&#27861;&#12289;&#26041;&#27861;&#35770;&#21644;&#23454;&#36341;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#23558;&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#26684;&#24335;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#25506;&#32034;&#20102;NLP&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#20840;&#35980;&#12290;&#25105;&#20204;&#23545;&#35768;&#22810;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#39044;&#35757;&#32451;&#30446;&#26631;&#12289;&#26550;&#26500;&#12289;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#12289;&#36801;&#31227;&#26041;&#27861;&#21644;&#20854;&#20182;&#22240;&#32032;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#25506;&#32034;&#30340;&#35265;&#35299;&#19982;&#35268;&#27169;&#21644;&#25105;&#20204;&#30340;&#26032;&#30340;&#8220;&#24222;&#22823;&#24178;&#20928;&#25235;&#21462;&#35821;&#26009;&#24211;&#8221;&#30456;&#32467;&#21512;&#65292;&#22312;&#35768;&#22810;&#28041;&#21450;&#25688;&#35201;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#20998;&#31867;&#31561;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;&#20026;&#20102;&#20419;&#36827;NLP&#39046;&#22495;&#30340;&#26410;&#26469;&#36801;&#31227;&#23398;&#20064;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained mode
&lt;/p&gt;</description></item></channel></rss>