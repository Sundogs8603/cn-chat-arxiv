<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#37322;&#20013;&#24341;&#20837;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65292;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#22240;&#26524;&#27010;&#29575;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26356;&#21152;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.14115</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#20449;&#30340;&#35299;&#37322;&#65306;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14115
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#37322;&#20013;&#24341;&#20837;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65292;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#22240;&#26524;&#27010;&#29575;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26356;&#21152;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35299;&#37322;&#25104;&#20026;&#20102;&#36890;&#36807;&#36873;&#25321;&#36755;&#20837;&#25991;&#26412;&#30340;&#23376;&#38598;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#20013;&#20027;&#35201;&#21464;&#21270;&#30340;&#19968;&#20010;&#22522;&#26412;&#30340;&#33258;&#25105;&#35299;&#37322;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#22312;&#20004;&#20010;&#25110;&#22810;&#20010;&#29255;&#27573;&#39640;&#24230;&#20114;&#30456;&#20851;&#32852;&#26102;&#26080;&#27861;&#35782;&#21035;&#30495;&#27491;&#30340;&#35299;&#37322;&#65292;&#22240;&#27492;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#20379;&#31867;&#20284;&#30340;&#36129;&#29486;&#65292;&#25152;&#35859;&#30340;&#34394;&#20551;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#26032;&#39062;&#22320;&#23558;&#20004;&#20010;&#22240;&#26524;&#26399;&#26395;&#20540;&#65288;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65289;&#24341;&#20837;&#20102;&#35299;&#37322;&#20013;&#12290;&#25105;&#20204;&#26681;&#25454;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#35299;&#37322;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#30340;&#22240;&#26524;&#27010;&#29575;&#65292;&#36890;&#36807;&#20854;&#29702;&#35770;&#37492;&#23450;&#65292;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35780;&#35770;&#21644;&#21307;&#30103;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent advances in natural language processing, rationalization becomes an essential self-explaining diagram to disentangle the black box by selecting a subset of input texts to account for the major variation in prediction. Yet, existing association-based approaches on rationalization cannot identify true rationales when two or more snippets are highly inter-correlated and thus provide a similar contribution to prediction accuracy, so-called spuriousness. To address this limitation, we novelly leverage two causal desiderata, non-spuriousness and efficiency, into rationalization from the causal inference perspective. We formally define a series of probabilities of causation based on a newly proposed structural causal model of rationalization, with its theoretical identification established as the main component of learning necessary and sufficient rationales. The superior performance of the proposed causal rationalization is demonstrated on real-world review and medical datasets w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2306.14096</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#23454;&#20307;&#32423;&#21035;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#26159;&#24773;&#24863;&#20998;&#26512;&#30340;&#37325;&#35201;&#23376;&#20219;&#21153;&#65292;&#30446;&#21069;&#38754;&#20020;&#30528;&#20247;&#22810;&#25361;&#25112;&#12290;&#20854;&#20013;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26469;&#33258;&#20110;&#32570;&#20047;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#37329;&#34701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#30340;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#36825;&#38480;&#21046;&#20102;&#24320;&#21457;&#26377;&#25928;&#25991;&#26412;&#22788;&#29702;&#25216;&#26415;&#25152;&#38656;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#35328;&#27169;&#24335;&#21305;&#37197;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;&#29616;&#26377;&#24320;&#28304;LLMs&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#21160;&#30495;&#23454;&#19990;&#30028;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which shoul
&lt;/p&gt;</description></item><item><title>Alberta&#22823;&#23398;&#22242;&#38431;&#22312;SemEval-2023&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#20219;&#21153;&#20013;&#65292;&#37319;&#29992;&#19978;&#19979;&#25991;&#22686;&#24378;&#21644;&#32763;&#35793;&#25216;&#26415;&#32467;&#21512;&#27880;&#37322;&#20449;&#24687;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#33719;&#24471;&#36739;&#22909;&#30340;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2306.14067</link><description>&lt;p&gt;
UAlberta&#22312;SemEval-2023&#20219;&#21153;1&#20013;:&#22810;&#35821;&#35328;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#21644;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
UAlberta at SemEval-2023 Task 1: Context Augmentation and Translation for Multilingual Visual Word Sense Disambiguation. (arXiv:2306.14067v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14067
&lt;/p&gt;
&lt;p&gt;
Alberta&#22823;&#23398;&#22242;&#38431;&#22312;SemEval-2023&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#20219;&#21153;&#20013;&#65292;&#37319;&#29992;&#19978;&#19979;&#25991;&#22686;&#24378;&#21644;&#32763;&#35793;&#25216;&#26415;&#32467;&#21512;&#27880;&#37322;&#20449;&#24687;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#33719;&#24471;&#36739;&#22909;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;Alberta&#22823;&#23398;&#22242;&#38431;&#29992;&#20110;SemEval-2023&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#65288;V-WSD&#65289;&#20219;&#21153;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20174;BabelNet&#26816;&#32034;&#21040;&#30340;&#27880;&#37322;&#19982;&#25991;&#26412;&#21644;&#22270;&#20687;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#29305;&#23450;&#20110;&#35821;&#35328;&#30340;&#32534;&#30721;&#22120;&#19982;&#23558;&#33521;&#25991;&#32534;&#30721;&#22120;&#24212;&#29992;&#20110;&#32763;&#35793;&#25991;&#26412;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30001;&#20110;&#20219;&#21153;&#25968;&#25454;&#38598;&#20013;&#32473;&#20986;&#30340;&#19978;&#19979;&#25991;&#38750;&#24120;&#30701;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#23581;&#35797;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25551;&#36848;&#26469;&#22686;&#24378;&#36825;&#20123;&#19978;&#19979;&#25991;&#12290;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#24182;&#35780;&#20272;&#20102;&#20351;&#29992;&#22270;&#20687;&#29983;&#25104;&#21644;&#25991;&#26412;&#26465;&#20214;&#22270;&#20687;&#20998;&#21106;&#30340;&#20854;&#20182;V-WSD&#26041;&#27861;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23448;&#26041;&#25552;&#20132;&#30340;&#32467;&#26524;&#25490;&#21517;&#31532;18&#20301;&#12290;&#25105;&#20204;&#30340;&#38750;&#23448;&#26041;&#32467;&#26524;&#29978;&#33267;&#27604;&#23448;&#26041;&#32467;&#26524;&#22909;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/UAlberta-NLP/v-wsd&#20013;&#20844;&#24320;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe the systems of the University of Alberta team for the SemEval-2023 Visual Word Sense Disambiguation (V-WSD) Task. We present a novel algorithm that leverages glosses retrieved from BabelNet, in combination with text and image encoders. Furthermore, we compare language-specific encoders against the application of English encoders to translated texts. As the contexts given in the task datasets are extremely short, we also experiment with augmenting these contexts with descriptions generated by a language model. This yields substantial improvements in accuracy. We describe and evaluate additional V-WSD methods which use image generation and text-conditioned image segmentation. Overall, the results of our official submission rank us 18 out of 56 teams. Some of our unofficial results are even better than the official ones. Our code is publicly available at https://github.com/UAlberta-NLP/v-wsd.
&lt;/p&gt;</description></item><item><title>DesCo&#26159;&#19968;&#31181;&#26032;&#30340;&#29289;&#20307;&#35782;&#21035;&#27169;&#22411;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#35814;&#23613;&#30340;&#35821;&#35328;&#25551;&#36848;&#25552;&#39640;&#23545;&#26032;&#23545;&#35937;&#21644;&#22495;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14060</link><description>&lt;p&gt;
DesCo: &#21033;&#29992;&#35814;&#23613;&#30340;&#35821;&#35328;&#25551;&#36848;&#23398;&#20064;&#29289;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DesCo: Learning Object Recognition with Rich Language Descriptions. (arXiv:2306.14060v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14060
&lt;/p&gt;
&lt;p&gt;
DesCo&#26159;&#19968;&#31181;&#26032;&#30340;&#29289;&#20307;&#35782;&#21035;&#27169;&#22411;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#35814;&#23613;&#30340;&#35821;&#35328;&#25551;&#36848;&#25552;&#39640;&#23545;&#26032;&#23545;&#35937;&#21644;&#22495;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35270;&#35273;&#35821;&#35328;&#26041;&#27861;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#23398;&#20064;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#20174;&#35821;&#35328;&#30417;&#30563;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#23545;&#35937;&#19982;&#35821;&#35328;&#26597;&#35810;&#65288;&#20363;&#22914;&#65292;&#8220;&#19968;&#24352;&#29483;&#30340;&#29031;&#29255;&#8221;&#65289;&#23545;&#40784;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#35782;&#21035;&#26032;&#23545;&#35937;&#21644;&#22495;&#30340;&#36866;&#24212;&#24615;&#12290;&#26368;&#36817;&#65292;&#20960;&#39033;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#21253;&#25324;&#23646;&#24615;&#65292;&#24418;&#29366;&#65292;&#32441;&#29702;&#21644;&#20851;&#31995;&#31561;&#32454;&#31890;&#24230;&#35821;&#20041;&#32454;&#33410;&#35268;&#33539;&#30340;&#22797;&#26434;&#35821;&#35328;&#34920;&#36798;&#24335;&#26597;&#35810;&#36825;&#20123;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#23558;&#35821;&#35328;&#25551;&#36848;&#20316;&#20026;&#26597;&#35810;&#21152;&#20837;&#24182;&#19981;&#33021;&#20445;&#35777;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#31934;&#30830;&#35299;&#37322;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;GLIP&#24120;&#24120;&#24573;&#30053;&#35821;&#35328;&#25551;&#36848;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#32780;&#26159;&#36807;&#20110;&#20381;&#36182;&#20165;&#20973;&#21517;&#31216;&#26816;&#27979;&#29289;&#20307;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#25551;&#36848;&#26465;&#20214;&#65288;DesCo&#65289;&#8221;&#29289;&#20307;&#35782;&#21035;&#27169;&#22411;&#23398;&#20064;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent development in vision-language approaches has instigated a paradigm shift in learning visual recognition models from language supervision. These approaches align objects with language queries (e.g. "a photo of a cat") and improve the models' adaptability to identify novel objects and domains. Recently, several studies have attempted to query these models with complex language expressions that include specifications of fine-grained semantic details, such as attributes, shapes, textures, and relations. However, simply incorporating language descriptions as queries does not guarantee accurate interpretation by the models. In fact, our experiments show that GLIP, the state-of-the-art vision-language model for object detection, often disregards contextual information in the language descriptions and instead relies heavily on detecting objects solely by their names. To tackle the challenges, we propose a new description-conditioned (DesCo) paradigm of learning object recognition model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31526;&#21495;&#21270;&#24605;&#32500;&#38142;&#25552;&#28860;&#65288;SCoTD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#25277;&#26679;&#20986;&#30340;&#21512;&#29702;&#21270;&#35299;&#37322;&#26469;&#35757;&#32451;&#25968;&#37327;&#32423;&#26356;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23567;&#27169;&#22411;&#20063;&#33021;&#21463;&#30410;&#20110;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14050</link><description>&lt;p&gt;
&#31526;&#21495;&#21270;&#24605;&#32500;&#38142;&#25552;&#28860;&#65306;&#23567;&#27169;&#22411;&#20063;&#33021;&#36880;&#27493;&#8220;&#24605;&#32771;&#8221;
&lt;/p&gt;
&lt;p&gt;
Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step. (arXiv:2306.14050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31526;&#21495;&#21270;&#24605;&#32500;&#38142;&#25552;&#28860;&#65288;SCoTD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#25277;&#26679;&#20986;&#30340;&#21512;&#29702;&#21270;&#35299;&#37322;&#26469;&#35757;&#32451;&#25968;&#37327;&#32423;&#26356;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23567;&#27169;&#22411;&#20063;&#33021;&#21463;&#30410;&#20110;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;&#20363;&#22914;&#8220;&#25105;&#20204;&#26469;&#36880;&#27493;&#24605;&#32771;&#8221;&#65289;&#20250;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#39044;&#27979;&#36827;&#34892;&#21512;&#29702;&#21270;&#35299;&#37322;&#12290;&#34429;&#28982;&#24605;&#32500;&#38142;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#30410;&#22788;&#20284;&#20046;&#20165;&#36866;&#29992;&#20110;&#36275;&#22815;&#22823;&#30340;&#27169;&#22411;&#65288;&#36229;&#36807;50&#20159;&#21442;&#25968;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#37327;&#32423;&#36739;&#23567;&#65288;125M-1.3B&#21442;&#25968;&#65289;&#30340;&#27169;&#22411;&#20173;&#28982;&#21487;&#20197;&#20174;&#24605;&#32500;&#38142;&#25552;&#31034;&#20013;&#21463;&#30410;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31526;&#21495;&#21270;&#24605;&#32500;&#38142;&#25552;&#28860;&#65288;SCoTD&#65289;&#65292;&#19968;&#31181;&#23558;&#36739;&#22823;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#25277;&#26679;&#20986;&#30340;&#21512;&#29702;&#21270;&#35299;&#37322;&#29992;&#20110;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#20960;&#20010;&#24120;&#35782;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65306;1&#65289;SCoTD&#25552;&#21319;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#30417;&#30563;&#23398;&#20064;&#36824;&#26159;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#29305;&#21035;&#26159;&#22312;&#25361;&#25112;&#38598;&#26041;&#38754;&#12290;2&#65289;&#20174;&#25945;&#24072;&#27169;&#22411;&#20013;&#25277;&#26679;&#22810;&#20010;&#25512;&#29702;&#38142;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;3&#65289;&#22312;&#25552;&#28860;&#21518;&#65292;&#34429;&#28982;&#21442;&#25968;&#23569;&#24471;&#22810;&#65292;&#20294;&#23398;&#29983;&#24605;&#32500;&#38142;&#19982;&#25945;&#24072;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought prompting (e.g., "Let's think step-by-step") primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M -- 1.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;WFA&#65289;&#25552;&#21462;&#21644;&#35299;&#37322;&#26694;&#26550;&#26469;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31934;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.14040</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#21152;&#26435;&#33258;&#21160;&#26426;&#25552;&#21462;&#19982;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Weighted Automata Extraction and Explanation of Recurrent Neural Networks for Natural Language Tasks. (arXiv:2306.14040v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;WFA&#65289;&#25552;&#21462;&#21644;&#35299;&#37322;&#26694;&#26550;&#26469;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31934;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#22312;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#29702;&#35299;&#21644;&#20998;&#26512;&#23427;&#20204;&#30340;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#35768;&#22810;&#20154;&#33268;&#21147;&#20110;&#20174;RNN&#20013;&#25552;&#21462;&#26377;&#38480;&#33258;&#21160;&#26426;&#65292;&#36825;&#23545;&#20110;&#20998;&#26512;&#21644;&#35299;&#37322;&#26356;&#26041;&#20415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22914;&#31934;&#30830;&#23398;&#20064;&#21644;&#32452;&#21512;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#25110;&#31934;&#24230;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;WFA&#65289;&#25552;&#21462;&#21644;&#35299;&#37322;&#26694;&#26550;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Networks (RNNs) have achieved tremendous success in processing sequential data, yet understanding and analyzing their behaviours remains a significant challenge. To this end, many efforts have been made to extract finite automata from RNNs, which are more amenable for analysis and explanation. However, existing approaches like exact learning and compositional approaches for model extraction have limitations in either scalability or precision. In this paper, we propose a novel framework of Weighted Finite Automata (WFA) extraction and explanation to tackle the limitations for natural language tasks. First, to address the transition sparsity and context loss problems we identified in WFA extraction for natural language tasks, we propose an empirical method to complement missing rules in the transition diagram, and adjust transition matrices to enhance the context-awareness of the WFA. We also propose two data augmentation tactics to track more dynamic behaviours of RNN, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36164;&#28304;&#21294;&#20047;&#30340;&#21360;&#24230;&#35821;&#35328;&#39532;&#25289;&#22320;&#35821;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;&#28151;&#21512;BERT&#27169;&#22411;&#21644;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#35813;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;BERT&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.14030</link><description>&lt;p&gt;
My Boli&#65306;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#30340;&#35821;&#26009;&#24211;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks. (arXiv:2306.14030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36164;&#28304;&#21294;&#20047;&#30340;&#21360;&#24230;&#35821;&#35328;&#39532;&#25289;&#22320;&#35821;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;&#28151;&#21512;BERT&#27169;&#22411;&#21644;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#35813;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;BERT&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#30340;&#28151;&#21512;&#35821;&#26009;&#24211;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28151;&#21512;&#35821;&#35328;&#25968;&#25454;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#36164;&#28304;&#21294;&#20047;&#30340;&#21360;&#24230;&#35821;&#35328;&#39532;&#25289;&#22320;&#35821;&#65292;&#36825;&#20010;&#35821;&#35328;&#20043;&#21069;&#27809;&#26377;&#20219;&#20309;&#28151;&#21512;&#35821;&#35328;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;L3Cube-MeCorpus&#65292;&#19968;&#20010;&#21253;&#21547;500&#19975;&#26465;&#25512;&#29305;&#30340;&#22823;&#22411;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;(Mr-En)&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;L3Cube-MeBERT&#21644;MeRoBERTa&#65292;&#22522;&#20110;BERT&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#22312;MeCorpus&#19978;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#20010;&#26377;&#30417;&#30563;&#30340;&#25968;&#25454;&#38598;MeHate&#12289;MeSent&#21644;MeLID&#65292;&#29992;&#20110;&#28151;&#21512;Mr-En&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#35821;&#35328;&#35782;&#21035;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#35780;&#20272;&#25968;&#25454;&#38598;&#20998;&#21035;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;\url{~}12,000&#26465;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#28151;&#21512;&#25512;&#29305;&#12290;&#21066;&#20943;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20010;&#26032;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;BERT&#27169;&#22411;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;&#30340;&#20195;&#30721;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research on code-mixed data is limited due to the unavailability of dedicated code-mixed datasets and pre-trained language models. In this work, we focus on the low-resource Indian language Marathi which lacks any prior work in code-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English (Mr-En) corpus with 5 million tweets for pretraining. We also release L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models pre-trained on MeCorpus. Furthermore, for benchmarking, we present three supervised datasets MeHate, MeSent, and MeLID for downstream tasks like code-mixed Mr-En hate speech detection, sentiment analysis, and language identification respectively. These evaluation datasets individually consist of manually annotated \url{~}12,000 Marathi-English code-mixed tweets. Ablations show that the models trained on this novel corpus significantly outperform the existing state-of-the-art BERT models. This is the first work that presents artifacts for code-mix
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#30340;&#31185;&#25216;&#35770;&#25991;&#22810;&#26631;&#31614;&#20998;&#31867;&#26694;&#26550;FUTEX&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#36328;&#35770;&#25991;&#32593;&#32476;&#32467;&#26500;&#21644;&#21508;&#25237;&#31295;&#20869;&#37096;&#20998;&#31456;&#33410;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;&#32454;&#31890;&#24230;&#26631;&#31614;&#31354;&#38388;&#20013;&#23558;&#35770;&#25991;&#20998;&#31867;&#20026;&#30740;&#31350;&#20027;&#39064;&#21644;&#20027;&#39064;&#65292;&#21487;&#33021;&#26377;&#22810;&#20010;&#65307;&#24212;&#21033;&#29992;&#20840;&#25991;&#26469;&#34917;&#20805;&#35770;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#20197;&#36827;&#34892;&#20998;&#31867;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.14003</link><description>&lt;p&gt;
&#20840;&#25991;&#31185;&#25216;&#35770;&#25991;&#30340;&#24369;&#30417;&#30563;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers. (arXiv:2306.14003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#30340;&#31185;&#25216;&#35770;&#25991;&#22810;&#26631;&#31614;&#20998;&#31867;&#26694;&#26550;FUTEX&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#36328;&#35770;&#25991;&#32593;&#32476;&#32467;&#26500;&#21644;&#21508;&#25237;&#31295;&#20869;&#37096;&#20998;&#31456;&#33410;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;&#32454;&#31890;&#24230;&#26631;&#31614;&#31354;&#38388;&#20013;&#23558;&#35770;&#25991;&#20998;&#31867;&#20026;&#30740;&#31350;&#20027;&#39064;&#21644;&#20027;&#39064;&#65292;&#21487;&#33021;&#26377;&#22810;&#20010;&#65307;&#24212;&#21033;&#29992;&#20840;&#25991;&#26469;&#34917;&#20805;&#35770;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#20197;&#36827;&#34892;&#20998;&#31867;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#30340;&#31185;&#25216;&#35770;&#25991;&#20998;&#31867;&#20381;&#36182;&#20110;&#20998;&#31867;&#25551;&#36848;&#32780;&#38750;&#20154;&#24037;&#26631;&#27880;&#26679;&#26412;&#24314;&#31435;&#20998;&#31867;&#22120;&#12290;&#24050;&#26377;&#30340;&#24369;&#30417;&#30563;&#20998;&#31867;&#30740;&#31350;&#36739;&#23569;&#32771;&#34385;&#21040;&#20004;&#20010;&#25361;&#25112;&#65306;(1)&#22312;&#32454;&#31890;&#24230;&#26631;&#31614;&#31354;&#38388;&#20013;&#23558;&#35770;&#25991;&#20998;&#31867;&#20026;&#30740;&#31350;&#20027;&#39064;&#21644;&#20027;&#39064;&#65292;&#21487;&#33021;&#26377;&#22810;&#20010;; (2)&#24212;&#21033;&#29992;&#20840;&#25991;&#26469;&#34917;&#20805;&#35770;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#24212;&#21033;&#29992;&#36328;&#35770;&#25991;&#32593;&#32476;&#32467;&#26500;&#21644;&#21508;&#35770;&#25991;&#20869;&#37096;&#20998;&#31456;&#33410;&#30340;&#23618;&#27425;&#32467;&#26500;&#31561;&#32467;&#26500;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FUTEX&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#36328;&#35770;&#25991;&#32593;&#32476;&#32467;&#26500;&#21644;&#21508;&#25237;&#31295;&#20869;&#37096;&#20998;&#31456;&#33410;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instead of relying on human-annotated training samples to build a classifier, weakly supervised scientific paper classification aims to classify papers only using category descriptions (e.g., category names, category-indicative keywords). Existing studies on weakly supervised paper classification are less concerned with two challenges: (1) Papers should be classified into not only coarse-grained research topics but also fine-grained themes, and potentially into multiple themes, given a large and fine-grained label space; and (2) full text should be utilized to complement the paper title and abstract for classification. Moreover, instead of viewing the entire paper as a long linear sequence, one should exploit the structural information such as citation links across papers and the hierarchy of sections and paragraphs in each paper. To tackle these challenges, in this study, we propose FUTEX, a framework that uses the cross-paper network structure and the in-paper hierarchy structure to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#33756;&#35889;&#20026;&#20363;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#25552;&#31034;&#65292;&#21487;&#20197;&#23558;&#33756;&#35889;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#27493;&#39588;&#12290;&#36890;&#36807;Amazon Mechanical Turk&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27880;&#37322;&#21592;&#36890;&#24120;&#26356;&#21916;&#27426;&#20462;&#25913;&#21518;&#30340;&#33756;&#35889;&#65292;&#36825;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25968;&#23383;&#21416;&#24072;&#31561;&#26381;&#21153;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.13986</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21416;&#24072;&#21161;&#29702;&#65306;&#20351;&#29992;GPT-3&#20462;&#25913;&#33756;&#35889;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Sous Chefs: Revising Recipes with GPT-3. (arXiv:2306.13986v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#33756;&#35889;&#20026;&#20363;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#25552;&#31034;&#65292;&#21487;&#20197;&#23558;&#33756;&#35889;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#27493;&#39588;&#12290;&#36890;&#36807;Amazon Mechanical Turk&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27880;&#37322;&#21592;&#36890;&#24120;&#26356;&#21916;&#27426;&#20462;&#25913;&#21518;&#30340;&#33756;&#35889;&#65292;&#36825;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25968;&#23383;&#21416;&#24072;&#31561;&#26381;&#21153;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26497;&#22823;&#30340;&#25991;&#26412;&#29983;&#25104;&#21644;&#25552;&#31034;&#21151;&#33021;&#65292;&#21487;&#20197;&#23558;&#29616;&#26377;&#30340;&#20070;&#38754;&#20449;&#24687;&#35843;&#25972;&#20026;&#26356;&#26131;&#20110;&#20351;&#29992;&#21644;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#26412;&#25991;&#20197;&#33756;&#35889;&#20026;&#20363;&#65292;&#23637;&#31034;&#20102;&#36825;&#19968;&#21151;&#33021;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#33756;&#35889;&#21644;&#37197;&#26009;&#28165;&#21333;&#30340;&#25552;&#31034;&#65292;&#23558;&#33756;&#35889;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#27493;&#39588;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#21040;&#26469;&#33258;&#21508;&#31181;&#19990;&#30028;&#32654;&#39135;&#30340;&#33756;&#35889;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;GPT-3.5&#30340;&#32467;&#26524;&#26368;&#22909;&#12290;&#25105;&#20204;&#36824;&#20026;&#20844;&#20247;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#25552;&#31034;&#12289;&#20195;&#30721;&#21644;MTurk&#27169;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
With their remarkably improved text generation and prompting capabilities, large language models can adapt existing written information into forms that are easier to use and understand. In our work, we focus on recipes as an example of complex, diverse, and widely used instructions. We develop a prompt grounded in the original recipe and ingredients list that breaks recipes down into simpler steps. We apply this prompt to recipes from various world cuisines, and experiment with several large language models (LLMs), finding best results with GPT-3.5. We also contribute an Amazon Mechanical Turk task that is carefully designed to reduce fatigue while collecting human judgment of the quality of recipe revisions. We find that annotators usually prefer the revision over the original, demonstrating a promising application of LLMs in serving as digital sous chefs for recipes and beyond. We release our prompt, code, and MTurk template for public use.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20445;&#30041;&#30446;&#26631;&#26041;&#38754;&#30456;&#20851;&#35821;&#20041;&#30340;&#26377;&#22122;&#22768;&#12289;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#19981;&#21516;&#29256;&#26412;&#20043;&#38388;&#30340;&#19981;&#21464;&#24615;&#36827;&#34892;&#24314;&#27169;&#20197;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#65292;&#22312;&#26631;&#20934;&#21644;&#24378;&#24230;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#37117;&#26174;&#33879;&#25913;&#36827;&#20102;&#24378;&#39044;&#35757;&#32451;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13971</link><description>&lt;p&gt;
&#38750;&#21453;&#20107;&#23454;&#22686;&#24378;&#22312;&#24378;&#20581;&#24615;&#26041;&#38754;&#23545;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Aspect-based Sentiment Analysis through Non-counterfactual Augmentations. (arXiv:2306.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20445;&#30041;&#30446;&#26631;&#26041;&#38754;&#30456;&#20851;&#35821;&#20041;&#30340;&#26377;&#22122;&#22768;&#12289;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#19981;&#21516;&#29256;&#26412;&#20043;&#38388;&#30340;&#19981;&#21464;&#24615;&#36827;&#34892;&#24314;&#27169;&#20197;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#65292;&#22312;&#26631;&#20934;&#21644;&#24378;&#24230;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#37117;&#26174;&#33879;&#25913;&#36827;&#20102;&#24378;&#39044;&#35757;&#32451;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#22312;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26377;&#22823;&#37327;&#35777;&#25454;&#34920;&#26126;&#23427;&#20204;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#20998;&#24067;&#22806;&#25968;&#25454;&#26102;&#65292;&#20854;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#26368;&#36817;&#30340;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#38598;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#30001;&#20110;&#26080;&#27861;&#35775;&#38382;&#26174;&#24335;&#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#38750;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#20381;&#36182;&#20110;&#20351;&#29992;&#24102;&#26377;&#22122;&#22768;&#30340;&#65292;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#25968;&#25454;&#22686;&#24378;&#26469;&#20445;&#30041;&#19982;&#30446;&#26631;&#26041;&#38754;&#30456;&#20851;&#32852;&#30340;&#35821;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#25968;&#25454;&#30340;&#19981;&#21516;&#29256;&#26412;&#20043;&#38388;&#30340;&#19981;&#21464;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;&#19968;&#32452;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#22312;&#26631;&#20934;&#21644;&#24378;&#24230;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#37117;&#26174;&#33879;&#25913;&#36827;&#20102;&#24378;&#39044;&#35757;&#32451;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While state-of-the-art NLP models have demonstrated excellent performance for aspect based sentiment analysis (ABSA), substantial evidence has been presented on their lack of robustness. This is especially manifested as significant degradation in performance when faced with out-of-distribution data. Recent solutions that rely on counterfactually augmented datasets show promising results, but they are inherently limited because of the lack of access to explicit causal structure. In this paper, we present an alternative approach that relies on non-counterfactual data augmentation. Our proposal instead relies on using noisy, cost-efficient data augmentations that preserve semantics associated with the target aspect. Our approach then relies on modelling invariances between different versions of the data to improve robustness. A comprehensive suite of experiments shows that our proposal significantly improves upon strong pre-trained baselines on both standard and robustness-specific datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;mTLDR&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#36229;&#22797;&#31354;&#38388;&#19978;&#34701;&#21512;&#22810;&#27169;&#24577;&#20449;&#21495;&#36827;&#34892;&#26497;&#31471;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#30340;&#20219;&#21153;&#12290;mTLDRgen&#27169;&#22411;&#25104;&#21151;&#23454;&#29616;&#20102;TL;DR&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.13968</link><description>&lt;p&gt;
&#21033;&#29992;&#36229;&#22797;&#31354;&#38388;&#19978;&#34701;&#21512;&#22810;&#27169;&#24577;&#20449;&#21495;&#36827;&#34892;&#26497;&#31471;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Fusing Multimodal Signals on Hyper-complex Space for Extreme Abstractive Text Summarization (TL;DR) of Scientific Contents. (arXiv:2306.13968v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;mTLDR&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#36229;&#22797;&#31354;&#38388;&#19978;&#34701;&#21512;&#22810;&#27169;&#24577;&#20449;&#21495;&#36827;&#34892;&#26497;&#31471;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#30340;&#20219;&#21153;&#12290;mTLDRgen&#27169;&#22411;&#25104;&#21151;&#23454;&#29616;&#20102;TL;DR&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24050;&#26377;&#27880;&#37322;&#25688;&#35201;&#21644;&#20016;&#23500;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#65292;&#31185;&#23398;&#25991;&#26412;&#25688;&#35201;&#39046;&#22495;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22810;&#20010;&#36755;&#20837;&#27169;&#24577;&#65288;&#20363;&#22914;&#35270;&#39057;&#21644;&#38899;&#39057;&#65289;&#30340;&#21033;&#29992;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#25506;&#32034;&#12290;&#30446;&#21069;&#65292;&#31185;&#23398;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#24448;&#24448;&#37319;&#29992;&#36739;&#38271;&#30340;&#30446;&#26631;&#25688;&#35201;&#65292;&#22914;&#25688;&#35201;&#65292;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#36755;&#20837;&#27169;&#24577;&#26469;&#22788;&#29702;&#26497;&#31471;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#65288;&#21363;TL;DR&#29983;&#25104;&#65289;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;mTLDR&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#21019;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#65292;&#20197;&#21450;&#20316;&#32773;&#25776;&#20889;&#30340;&#25688;&#35201;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;&#25688;&#35201;&#12290;mTLDR&#25968;&#25454;&#38598;&#25628;&#38598;&#20102;&#26469;&#33258;&#19981;&#21516;&#23398;&#26415;&#20250;&#35758;&#35760;&#24405;&#24635;&#20849;4182&#20010;&#23454;&#20363;&#65292;&#22914;ICLR&#12289;ACL&#21644;CVPR&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;mTLDRgen&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#36229;&#22797;&#26434;&#31354;&#38388;&#19978;&#26497;&#31471;&#25277;&#35937;&#25688;&#35201;&#30340;&#32534;&#30721;&#35299;&#30721;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;mTLDR&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#24182;&#25253;&#21578;&#20102;&#39318;&#20010;&#25104;&#21151;&#23454;&#26045;TL;DR&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The realm of scientific text summarization has experienced remarkable progress due to the availability of annotated brief summaries and ample data. However, the utilization of multiple input modalities, such as videos and audio, has yet to be thoroughly explored. At present, scientific multimodal-input-based text summarization systems tend to employ longer target summaries like abstracts, leading to an underwhelming performance in the task of text summarization.  In this paper, we deal with a novel task of extreme abstractive text summarization (aka TL;DR generation) by leveraging multiple input modalities. To this end, we introduce mTLDR, a first-of-its-kind dataset for the aforementioned task, comprising videos, audio, and text, along with both author-composed summaries and expert-annotated summaries. The mTLDR dataset accompanies a total of 4,182 instances collected from various academic conference proceedings, such as ICLR, ACL, and CVPR. Subsequently, we present mTLDRgen, an encod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35782;&#21035;&#35828;&#35805;&#32773;&#24773;&#24863;&#32763;&#36716;&#32972;&#21518;&#30340;&#25512;&#21160;&#32773;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#31070;&#32463;&#26550;&#26500;c&#36827;&#34892;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.13959</link><description>&lt;p&gt;
&#22810;&#26041;&#20250;&#35805;&#20013;&#30340;&#24773;&#24863;&#21453;&#36716;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Emotion Flip Reasoning in Multiparty Conversations. (arXiv:2306.13959v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35782;&#21035;&#35828;&#35805;&#32773;&#24773;&#24863;&#32763;&#36716;&#32972;&#21518;&#30340;&#25512;&#21160;&#32773;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#31070;&#32463;&#26550;&#26500;c&#36827;&#34892;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#65292;&#35828;&#35805;&#32773;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#23427;&#20204;&#30340;&#21160;&#24577;&#22312;&#29702;&#35299;&#24773;&#24863;&#35805;&#35821;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#26816;&#27979;&#24773;&#24863;&#24182;&#19981;&#36275;&#20197;&#23436;&#20840;&#29702;&#35299;&#20250;&#35805;&#36807;&#31243;&#20013;&#21457;&#29983;&#30340;&#35828;&#35805;&#32773;&#29305;&#23450;&#24773;&#24863;&#21464;&#21270;&#12290;&#20026;&#20102;&#39640;&#25928;&#22320;&#29702;&#35299;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#21160;&#24577;&#65292;&#26377;&#24517;&#35201;&#30830;&#23450;&#23548;&#33268;&#35828;&#35805;&#32773;&#24773;&#24863;&#32763;&#36716;&#30340;&#21407;&#22240;&#25110;&#25512;&#21160;&#32773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21517;&#20026;Instigator based Emotion Flip Reasoning (EFR) &#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#20250;&#35805;&#20013;&#35828;&#35805;&#32773;&#24773;&#24863;&#32763;&#36716;&#32972;&#21518;&#30340;&#25512;&#21160;&#32773;&#12290;&#20363;&#22914;&#65292;&#20174;&#24555;&#20048;&#21040;&#24868;&#24594;&#30340;&#24773;&#24863;&#32763;&#36716;&#21487;&#33021;&#26159;&#30001;&#23041;&#32961;&#36825;&#26679;&#30340;&#25512;&#21160;&#32773;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#31526;&#21512;&#24773;&#24863;&#24515;&#29702;&#23398;&#26631;&#20934;&#30340;EFR&#25512;&#21160;&#32773;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;MELD-I&#12290;&#20026;&#20102;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#26550;&#26500;c&#12290;
&lt;/p&gt;
&lt;p&gt;
In a conversational dialogue, speakers may have different emotional states and their dynamics play an important role in understanding dialogue's emotional discourse. However, simply detecting emotions is not sufficient to entirely comprehend the speaker-specific changes in emotion that occur during a conversation. To understand the emotional dynamics of speakers in an efficient manner, it is imperative to identify the rationale or instigator behind any changes or flips in emotion expressed by the speaker. In this paper, we explore the task called Instigator based Emotion Flip Reasoning (EFR), which aims to identify the instigator behind a speaker's emotion flip within a conversation. For example, an emotion flip from joy to anger could be caused by an instigator like threat. To facilitate this task, we present MELD-I, a dataset that includes ground-truth EFR instigator labels, which are in line with emotional psychology. To evaluate the dataset, we propose a novel neural architecture c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545; COVID-19 &#35875;&#35328;&#30340;&#24773;&#24863;&#36733;&#20307;&#21450;&#20854;&#23545;&#30123;&#33495;&#25509;&#31181;&#20135;&#29983;&#30340;&#24433;&#21709;&#22312;&#21360;&#24230;&#21644;&#32654;&#22269;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#32467;&#26524;&#34920;&#26126;&#24102;&#26377;&#24868;&#24594;&#21644;&#24656;&#24807;&#31561;&#24773;&#24863;&#30340;&#35875;&#35328;&#36733;&#20307;&#23545;&#30123;&#33495;&#25509;&#31181;&#32467;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.13954</link><description>&lt;p&gt;
&#25581;&#31034;COVID-19&#35875;&#35328;&#30340;&#24773;&#24863;&#36733;&#20307;&#20197;&#21450;&#23427;&#20204;&#23545;&#21360;&#24230;&#21644;&#32654;&#22269;&#30123;&#33495;&#25509;&#31181;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Characterizing the Emotion Carriers of COVID-19 Misinformation and Their Impact on Vaccination Outcomes in India and the United States. (arXiv:2306.13954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545; COVID-19 &#35875;&#35328;&#30340;&#24773;&#24863;&#36733;&#20307;&#21450;&#20854;&#23545;&#30123;&#33495;&#25509;&#31181;&#20135;&#29983;&#30340;&#24433;&#21709;&#22312;&#21360;&#24230;&#21644;&#32654;&#22269;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#32467;&#26524;&#34920;&#26126;&#24102;&#26377;&#24868;&#24594;&#21644;&#24656;&#24807;&#31561;&#24773;&#24863;&#30340;&#35875;&#35328;&#36733;&#20307;&#23545;&#30123;&#33495;&#25509;&#31181;&#32467;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19 Infodemic&#23545;&#20840;&#29699;&#30340;&#20581;&#24247;&#34892;&#20026;&#21644;&#32467;&#26524;&#20135;&#29983;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#20391;&#37325;&#20110;&#29702;&#35299;&#35875;&#35328;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#38754;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#65292;&#20294;&#23545;&#35875;&#35328;&#30340;&#24773;&#24863;&#36733;&#20307;&#21450;&#20854;&#22312;&#22320;&#29702;&#19978;&#30340;&#24046;&#24322;&#30340;&#29702;&#35299;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#24773;&#24863;&#36733;&#20307;&#21450;&#20854;&#23545;&#21360;&#24230;&#21644;&#32654;&#22269;&#30340;&#30123;&#33495;&#25509;&#31181;&#29575;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#20174;230&#19975;&#26465;&#25512;&#25991;&#20013;&#21019;&#24314;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65288;CoAID&#65292;AntiVax&#65292;CMU&#65289;&#36827;&#34892;&#25972;&#21512;&#65292;&#20197;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#36827;&#34892;&#35875;&#35328;&#20998;&#31867;&#12290;&#21033;&#29992;Plutchik Transformers&#20998;&#26512;&#20102;&#35875;&#35328;&#26631;&#35760;&#25512;&#25991;&#30340;&#34892;&#20026;&#26041;&#38754;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#25512;&#25991;&#30340;&#24773;&#24863;&#12290;&#36827;&#34892;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20197;&#30740;&#31350;&#35875;&#35328;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;transformed features&#36827;&#34892;&#20998;&#31867;&#20197;&#20102;&#35299;&#19982;&#35875;&#35328;&#30456;&#20851;&#30340;&#24773;&#24863;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#24868;&#24594;&#21644;&#24656;&#24807;&#31561;&#24773;&#24863;&#30340;&#35875;&#35328;&#36733;&#20307;&#23545;&#32654;&#22269;&#21644;&#21360;&#24230;&#30340;&#30123;&#33495;&#25509;&#31181;&#32467;&#26524;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;COVID-19&#35875;&#35328;&#24773;&#24863;&#26041;&#38754;&#21450;&#20854;&#23545;&#20581;&#24247;&#34892;&#20026;&#30340;&#24433;&#21709;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 Infodemic had an unprecedented impact on health behaviors and outcomes at a global scale. While many studies have focused on a qualitative and quantitative understanding of misinformation, including sentiment analysis, there is a gap in understanding the emotion-carriers of misinformation and their differences across geographies. In this study, we characterized emotion carriers and their impact on vaccination rates in India and the United States. A manually labelled dataset was created from 2.3 million tweets and collated with three publicly available datasets (CoAID, AntiVax, CMU) to train deep learning models for misinformation classification. Misinformation labelled tweets were further analyzed for behavioral aspects by leveraging Plutchik Transformers to determine the emotion for each tweet. Time series analysis was conducted to study the impact of misinformation on spatial and temporal characteristics. Further, categorical classification was performed using transforme
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#12289;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;&#21644;&#38024;&#23545;&#22303;&#32819;&#20854;&#30340;BERT&#12289;DistilBERT&#12289;ELECTRA&#21644;RoBERTa&#27169;&#22411;&#22312;&#22303;&#32819;&#20854;&#22320;&#22336;&#35299;&#26512;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#21457;&#29616;&#38024;&#23545;&#22303;&#32819;&#20854;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.13947</link><description>&lt;p&gt;
&#38754;&#21521;&#22303;&#32819;&#20854;&#22320;&#22336;&#35299;&#26512;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparison of Pre-trained Language Models for Turkish Address Parsing. (arXiv:2306.13947v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#12289;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;&#21644;&#38024;&#23545;&#22303;&#32819;&#20854;&#30340;BERT&#12289;DistilBERT&#12289;ELECTRA&#21644;RoBERTa&#27169;&#22411;&#22312;&#22303;&#32819;&#20854;&#22320;&#22336;&#35299;&#26512;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#21457;&#29616;&#38024;&#23545;&#22303;&#32819;&#20854;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22914;BERT&#21450;&#20854;&#21464;&#31181;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#30340;&#35757;&#32451;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#22823;&#22810;&#25968;&#23398;&#26415;&#30740;&#31350;&#37117;&#26159;&#22522;&#20110;&#33521;&#35821;&#36827;&#34892;&#30340;;&#28982;&#32780;&#65292;&#22810;&#35821;&#35328;&#21644;&#29305;&#23450;&#35821;&#35328;&#30340;&#30740;&#31350;&#25968;&#37327;&#27491;&#22312;&#31283;&#27493;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#30740;&#31350;&#22768;&#31216;&#65292;&#38024;&#23545;&#29305;&#23450;&#35821;&#35328;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20248;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#20542;&#21521;&#20110;&#38024;&#23545;&#20854;&#26696;&#20363;&#30740;&#31350;&#30340;&#35821;&#35328;&#26469;&#35757;&#32451;&#25110;&#24494;&#35843;&#27169;&#22411;&#12290;&#26412;&#25991;&#38024;&#23545;&#22303;&#32819;&#20854;&#22320;&#22270;&#25968;&#25454;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;&#21644;&#22303;&#32819;&#20854;BERT&#12289;DistilBERT&#12289;ELECTRA&#21644;RoBERTa&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#29992;&#20110;&#24494;&#35843;BERT&#65292;&#20197;&#21450;&#26631;&#20934;&#30340;&#19968;&#23618;&#24494;&#35843;&#26041;&#27861;&#12290;&#23545;&#20110;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#36136;&#37327;&#30456;&#23545;&#36739;&#39640;&#30340;&#20013;&#31561;&#35268;&#27169;&#22320;&#22336;&#35299;&#26512;&#35821;&#26009;&#24211;&#12290;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Transformer based pre-trained models such as BERT and its variants, which are trained on large corpora, have demonstrated tremendous success for natural language processing (NLP) tasks. Most of academic works are based on the English language; however, the number of multilingual and language specific studies increase steadily. Furthermore, several studies claimed that language specific models outperform multilingual models in various tasks. Therefore, the community tends to train or fine-tune the models for the language of their case study, specifically. In this paper, we focus on Turkish maps data and thoroughly evaluate both multilingual and Turkish based BERT, DistilBERT, ELECTRA and RoBERTa. Besides, we also propose a MultiLayer Perceptron (MLP) for fine-tuning BERT in addition to the standard approach of one-layer fine-tuning. For the dataset, a mid-sized Address Parsing corpus taken with a relatively high quality is constructed. Conducted experiments on this dataset indicate that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#30417;&#30563;&#26426;&#21046;&#65292;&#22522;&#20110;&#19978;&#19979;&#25991;&#21270;&#30340;&#21333;&#35789;&#34920;&#31034;&#65292;&#23558;&#21160;&#21517;&#35789;&#30340;&#35770;&#25454;&#26144;&#23556;&#21040;&#30456;&#24212;&#21160;&#35789;&#26500;&#36896;&#30340;&#36890;&#29992;&#20381;&#23384;&#20851;&#31995;&#19978;&#65292;&#20174;&#32780;&#22312;&#19981;&#38656;&#35201;&#35821;&#20041;&#26412;&#20307;&#30340;&#24773;&#20917;&#19979;&#20016;&#23500;&#20102;&#20381;&#23384;&#20851;&#31995;&#26641;&#12290;</title><link>http://arxiv.org/abs/2306.13922</link><description>&lt;p&gt;
&#38750;&#30417;&#30563;&#22320;&#23558;&#21160;&#21517;&#35789;&#30340;&#35770;&#25454;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#21160;&#35789;&#26631;&#31614;&#19978;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Mapping of Arguments of Deverbal Nouns to Their Corresponding Verbal Labels. (arXiv:2306.13922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#30417;&#30563;&#26426;&#21046;&#65292;&#22522;&#20110;&#19978;&#19979;&#25991;&#21270;&#30340;&#21333;&#35789;&#34920;&#31034;&#65292;&#23558;&#21160;&#21517;&#35789;&#30340;&#35770;&#25454;&#26144;&#23556;&#21040;&#30456;&#24212;&#21160;&#35789;&#26500;&#36896;&#30340;&#36890;&#29992;&#20381;&#23384;&#20851;&#31995;&#19978;&#65292;&#20174;&#32780;&#22312;&#19981;&#38656;&#35201;&#35821;&#20041;&#26412;&#20307;&#30340;&#24773;&#20917;&#19979;&#20016;&#23500;&#20102;&#20381;&#23384;&#20851;&#31995;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#21517;&#35789;&#26159;&#24120;&#29992;&#30340;&#29992;&#20110;&#25551;&#36848;&#20107;&#20214;&#25110;&#34892;&#20026;&#21450;&#20854;&#35770;&#25454;&#30340;&#21160;&#35789;&#21517;&#35789;&#21270;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;NLP&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#27169;&#24335;&#30340;&#31995;&#32479;&#65292;&#24573;&#30053;&#20102;&#22788;&#29702;&#36825;&#31181;&#21517;&#35789;&#21270;&#30340;&#32467;&#26500;&#12290;&#32780;&#38024;&#23545;&#21517;&#35789;&#21270;&#32467;&#26500;&#30340;&#35770;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#35821;&#20041;&#27880;&#37322;&#65292;&#24182;&#38656;&#35201;&#35821;&#20041;&#26412;&#20307;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#24212;&#29992;&#20165;&#38480;&#20110;&#19968;&#23567;&#37096;&#20998;&#21517;&#35789;&#12290;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#26356;&#21152;&#21477;&#27861;&#21270;&#30340;&#26041;&#24335;&#65292;&#23558;&#21160;&#21517;&#35789;&#30340;&#35770;&#25454;&#26144;&#23556;&#21040;&#30456;&#24212;&#21160;&#35789;&#26500;&#36896;&#30340;&#36890;&#29992;&#20381;&#23384;&#20851;&#31995;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#30417;&#30563;&#26426;&#21046;&#65292;&#22522;&#20110;&#19978;&#19979;&#25991;&#21270;&#30340;&#21333;&#35789;&#34920;&#31034;&#65292;&#23427;&#20801;&#35768;&#20351;&#29992;&#19982;&#30456;&#24212;&#21160;&#35789;&#24773;&#20917;&#30456;&#21516;&#30340;&#26631;&#31614;&#65292;&#23558;&#36890;&#29992;&#20381;&#23384;&#26641;&#20016;&#23500;&#20102;&#25551;&#36848;&#21160;&#21517;&#35789;&#30340;&#35770;&#25454;&#30340;&#20381;&#36182;&#20851;&#31995;&#24359;&#12290;&#36890;&#36807;&#19982;&#21160;&#35789;&#24773;&#20917;&#30456;&#21516;&#30340;&#26631;&#31614;&#38598;&#20849;&#20139;&#65292;&#24050;&#32463;&#38024;&#23545;&#21160;&#35789;&#24320;&#21457;&#30340;&#27169;&#24335;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#21160;&#21517;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deverbal nouns are nominal forms of verbs commonly used in written English texts to describe events or actions, as well as their arguments. However, many NLP systems, and in particular pattern-based ones, neglect to handle such nominalized constructions. The solutions that do exist for handling arguments of nominalized constructions are based on semantic annotation and require semantic ontologies, making their applications restricted to a small set of nouns. We propose to adopt instead a more syntactic approach, which maps the arguments of deverbal nouns to the universal-dependency relations of the corresponding verbal construction. We present an unsupervised mechanism -based on contextualized word representations -- which allows to enrich universal-dependency trees with dependency arcs denoting arguments of deverbal nouns, using the same labels as the corresponding verbal cases. By sharing the same label set as in the verbal case, patterns that were developed for verbs can be applie
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;&#20998;&#26512;&#27861;&#24459;&#25991;&#20214;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;GPT-4&#22312;&#27880;&#37322;&#25351;&#21335;&#30340;&#25552;&#31034;&#19979;&#33021;&#22815;&#19982;&#20064;&#24815;&#30340;&#27880;&#37322;&#20154;&#21592;&#34920;&#29616;&#30456;&#24403;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#25209;&#22788;&#29702;&#39044;&#27979;&#23454;&#29616;&#25104;&#26412;&#30340;&#26174;&#33879;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20998;&#26512;GPT-4&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#24110;&#21161;&#27861;&#24459;&#25991;&#26412;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13906</link><description>&lt;p&gt;
GPT-4&#33021;&#22815;&#22312;&#38656;&#35201;&#39640;&#24230;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#30340;&#20219;&#21153;&#20013;&#25903;&#25345;&#25991;&#26412;&#25968;&#25454;&#20998;&#26512;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can GPT-4 Support Analysis of Textual Data in Tasks Requiring Highly Specialized Domain Expertise?. (arXiv:2306.13906v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13906
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;&#20998;&#26512;&#27861;&#24459;&#25991;&#20214;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;GPT-4&#22312;&#27880;&#37322;&#25351;&#21335;&#30340;&#25552;&#31034;&#19979;&#33021;&#22815;&#19982;&#20064;&#24815;&#30340;&#27880;&#37322;&#20154;&#21592;&#34920;&#29616;&#30456;&#24403;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#25209;&#22788;&#29702;&#39044;&#27979;&#23454;&#29616;&#25104;&#26412;&#30340;&#26174;&#33879;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20998;&#26512;GPT-4&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#24110;&#21161;&#27861;&#24459;&#25991;&#26412;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT-4&#65289;&#22312;&#38656;&#35201;&#39640;&#24230;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#30340;&#20219;&#21153;&#20013;&#20998;&#26512;&#25991;&#26412;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20998;&#26512;&#27861;&#38498;&#24847;&#35265;&#20197;&#35299;&#37322;&#27861;&#24459;&#27010;&#24565;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4&#22312;&#27880;&#37322;&#25351;&#21335;&#30340;&#25552;&#31034;&#19979;&#19982;&#35757;&#32451;&#26377;&#32032;&#30340;&#27861;&#24459;&#23398;&#29983;&#27880;&#37322;&#32773;&#30340;&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36890;&#36807;&#25209;&#22788;&#29702;&#39044;&#27979;&#65292;&#21487;&#20197;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#24615;&#33021;&#25439;&#22833;&#19979;&#23454;&#29616;&#25104;&#26412;&#30340;&#26174;&#33879;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#37319;&#29992;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#24182;&#27809;&#26377;&#26174;&#33879;&#25552;&#39640;&#36825;&#39033;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20998;&#26512;GPT-4&#30340;&#39044;&#27979;&#32467;&#26524;&#20197;&#35782;&#21035;&#21644;&#20943;&#36731;&#27880;&#37322;&#25351;&#21335;&#30340;&#32570;&#38519;&#65292;&#24182;&#38543;&#21518;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35813;&#27169;&#22411;&#38750;&#24120;&#33030;&#24369;&#65292;&#22240;&#20026;&#25552;&#31034;&#20013;&#30340;&#23567;&#22411;&#26684;&#24335;&#30456;&#20851;&#26356;&#25913;&#23545;&#39044;&#27979;&#32467;&#26524;&#20135;&#29983;&#20102;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#34987;&#27861;&#24459;&#25991;&#26412;&#20998;&#26512;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#21033;&#29992;&#65292;&#20197;&#25552;&#39640;&#20182;&#20204;&#24037;&#20316;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluated the capability of generative pre-trained transformers~(GPT-4) in analysis of textual data in tasks that require highly specialized domain expertise. Specifically, we focused on the task of analyzing court opinions to interpret legal concepts. We found that GPT-4, prompted with annotation guidelines, performs on par with well-trained law student annotators. We observed that, with a relatively minor decrease in performance, GPT-4 can perform batch predictions leading to significant cost reductions. However, employing chain-of-thought prompting did not lead to noticeably improved performance on this task. Further, we demonstrated how to analyze GPT-4's predictions to identify and mitigate deficiencies in annotation guidelines, and subsequently improve the performance of the model. Finally, we observed that the model is quite brittle, as small formatting related changes in the prompt had a high impact on the predictions. These findings can be leveraged by researchers and pract
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#20041;&#36712;&#36857;&#36827;&#34892;&#20998;&#26512;&#21644;&#29983;&#25104;&#21512;&#25104;&#35821;&#20041;&#36712;&#36857;&#25968;&#25454;&#65292;&#20026;&#20174;&#22478;&#24066;&#35268;&#21010;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#24341;&#25806;&#21644;&#21830;&#19994;&#31574;&#30053;&#31561;&#19968;&#31995;&#21015;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.13905</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#36712;&#36857;&#20998;&#26512;&#30340;&#26102;&#31354;&#21465;&#20107;&#65311;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal Storytelling? Leveraging Generative Models for Semantic Trajectory Analysis. (arXiv:2306.13905v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#20041;&#36712;&#36857;&#36827;&#34892;&#20998;&#26512;&#21644;&#29983;&#25104;&#21512;&#25104;&#35821;&#20041;&#36712;&#36857;&#25968;&#25454;&#65292;&#20026;&#20174;&#22478;&#24066;&#35268;&#21010;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#24341;&#25806;&#21644;&#21830;&#19994;&#31574;&#30053;&#31561;&#19968;&#31995;&#21015;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#20041;&#36712;&#36857;&#36319;&#36394;&#36827;&#34892;&#20998;&#26512;&#21644;&#29983;&#25104;&#21512;&#25104;&#35821;&#20041;&#36712;&#36857;&#25968;&#25454;&#65288;SST&#65289;&#30340;&#24895;&#26223;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#26088;&#22312;&#21019;&#24314;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#30740;&#31350;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#35821;&#20041;&#36712;&#36857;&#65292;&#39044;&#27979;&#26410;&#26469;&#36235;&#21183;&#65292;&#22686;&#24378;&#26426;&#22120;&#23545;&#21160;&#29289;&#12289;&#20154;&#31867;&#12289;&#36135;&#29289;&#31561;&#31227;&#21160;&#24773;&#20917;&#30340;&#29702;&#35299;&#65292;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#65292;&#24182;&#20026;&#20174;&#22478;&#24066;&#35268;&#21010;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#24341;&#25806;&#21644;&#21830;&#19994;&#31574;&#30053;&#31561;&#19968;&#31995;&#21015;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we lay out a vision for analysing semantic trajectory traces and generating synthetic semantic trajectory data (SSTs) using generative language model. Leveraging the advancements in deep learning, as evident by progress in the field of natural language processing (NLP), computer vision, etc. we intend to create intelligent models that can study the semantic trajectories in various contexts, predicting future trends, increasing machine understanding of the movement of animals, humans, goods, etc. enhancing human-computer interactions, and contributing to an array of applications ranging from urban-planning to personalized recommendation engines and business strategy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#29983;&#25104;&#38382;&#39064;&#25991;&#26412;&#35821;&#35328;&#21464;&#20307;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;DeBERTa&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#21450;&#20316;&#32773;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20854;&#25512;&#23548;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#34920;&#36798;&#24335;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.13899</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#38382;&#39064;&#38472;&#36848;&#30340;&#35821;&#35328;&#21464;&#20307;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Math Word Problem Solving by Generating Linguistic Variants of Problem Statements. (arXiv:2306.13899v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13899
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#29983;&#25104;&#38382;&#39064;&#25991;&#26412;&#35821;&#35328;&#21464;&#20307;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;DeBERTa&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#21450;&#20316;&#32773;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20854;&#25512;&#23548;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#34920;&#36798;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#33402;&#26415;&#26159;&#26234;&#21147;&#36827;&#23637;&#30340;&#22522;&#26412;&#25903;&#26609;&#65292;&#26159;&#22521;&#20859;&#20154;&#31867;&#29420;&#21019;&#24615;&#30340;&#26680;&#24515;&#20652;&#21270;&#21058;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#21457;&#34920;&#20102;&#22823;&#37327;&#22260;&#32469;&#35299;&#20915;&#25968;&#23398;&#35821;&#35328;&#38382;&#39064;&#65288;MWP&#65289;&#30340;&#20316;&#21697;&#65292;&#36825;&#26159;&#36808;&#21521;&#36890;&#29992;AI&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#36825;&#20123;&#29616;&#26377;&#27169;&#22411;&#23481;&#26131;&#20381;&#36182;&#20110;&#32932;&#27973;&#30340;&#21551;&#21457;&#24335;&#21644;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#26469;&#25512;&#23548;&#35299;&#20915;&#26041;&#26696;&#34920;&#36798;&#24335;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#19968;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#38382;&#39064;&#25991;&#26412;&#35821;&#35328;&#21464;&#20307;&#30340;MWP&#27714;&#35299;&#22120;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#35299;&#20915;&#27599;&#20010;&#19981;&#21516;&#21464;&#20307;&#30340;&#38382;&#39064;&#24182;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#39044;&#27979;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;DeBERTa&#65288;&#20855;&#26377;&#35299;&#30721;&#22686;&#24378;&#30340;BERT&#21644;&#20998;&#31163;&#27880;&#24847;&#21147;&#65289;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#20197;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#22686;&#24378;&#30340;&#36974;&#32617;&#35299;&#30721;&#22120;&#26469;&#26500;&#36896;&#35299;&#20915;&#26041;&#26696;&#34920;&#36798;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;$\mathrm{P\small{ARA}\normalsize}_\mathrm{gen}$-MWP&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#22312;&#29983;&#25104;&#21644;&#35299;&#26512;&#21464;&#20307;&#38382;&#39064;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;$\mathrm{P\small{ARA}\normalsize}_\mathrm{gen}$-MWP&#19978;&#37117;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20854;&#36890;&#36807;&#29702;&#35299;&#21644;&#25512;&#29702;&#38382;&#39064;&#25991;&#26412;&#30340;&#35821;&#20041;&#32454;&#24494;&#24046;&#21035;&#26469;&#25512;&#23548;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#34920;&#36798;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The art of mathematical reasoning stands as a fundamental pillar of intellectual progress and is a central catalyst in cultivating human ingenuity. Researchers have recently published a plethora of works centered around the task of solving Math Word Problems (MWP) $-$ a crucial stride towards general AI. These existing models are susceptible to dependency on shallow heuristics and spurious correlations to derive the solution expressions. In order to ameliorate this issue, in this paper, we propose a framework for MWP solvers based on the generation of linguistic variants of the problem text. The approach involves solving each of the variant problems and electing the predicted expression with the majority of the votes. We use DeBERTa (Decoding-enhanced BERT with disentangled attention) as the encoder to leverage its rich textual representations and enhanced mask decoder to construct the solution expressions. Furthermore, we introduce a challenging dataset, $\mathrm{P\small{ARA}\normalsi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#21644;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#26089;&#26399;ArXiving&#35770;&#25991;&#23545;&#20854;&#34987;ICLR&#20250;&#35758;&#25509;&#21463;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26089;&#26399;ArXiving&#21487;&#33021;&#20250;&#23545;&#35770;&#25991;&#34987;&#25509;&#21463;&#30340;&#26426;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#20294;&#36825;&#31181;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#65292;&#24182;&#19988;&#19981;&#22240;&#20316;&#32773;&#24341;&#29992;&#27425;&#25968;&#21644;&#26426;&#26500;&#25490;&#21517;&#31561;&#22240;&#32032;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2306.13891</link><description>&lt;p&gt;
&#26089;&#26399;ArXiving&#23545;&#35770;&#25991;&#34987;&#25509;&#21463;&#30340;&#22240;&#26524;&#24433;&#21709;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimating the Causal Effect of Early ArXiving on Paper Acceptance. (arXiv:2306.13891v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#21644;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#26089;&#26399;ArXiving&#35770;&#25991;&#23545;&#20854;&#34987;ICLR&#20250;&#35758;&#25509;&#21463;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26089;&#26399;ArXiving&#21487;&#33021;&#20250;&#23545;&#35770;&#25991;&#34987;&#25509;&#21463;&#30340;&#26426;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#20294;&#36825;&#31181;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#65292;&#24182;&#19988;&#19981;&#22240;&#20316;&#32773;&#24341;&#29992;&#27425;&#25968;&#21644;&#26426;&#26500;&#25490;&#21517;&#31561;&#22240;&#32032;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35770;&#25991;&#25552;&#20132;&#21516;&#34892;&#23457;&#26597;&#21069;&#21457;&#24067;&#39044;&#21360;&#26412;&#20250;&#20135;&#29983;&#20160;&#20040;&#24433;&#21709;&#65311;&#30001;&#20110;&#27809;&#26377;&#36827;&#34892;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#21033;&#29992;&#35266;&#23519;&#25968;&#25454;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;ICLR&#20250;&#35758;&#65288;2018-2022&#65289;&#30340;&#25968;&#25454;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#26469;&#20272;&#35745;&#22312;&#23457;&#38405;&#26399;&#21069;&#21024;&#38500;&#35770;&#25991;&#65288;&#26089;&#26399;arXiving&#65289;&#23545;&#35770;&#25991;&#34987;&#20250;&#35758;&#25509;&#21463;&#30340;&#24433;&#21709;&#12290;&#35843;&#25972;&#20102;18&#20010;&#28151;&#26434;&#22240;&#32032;&#65292;&#22914;&#20027;&#39064;&#12289;&#20316;&#32773;&#21644;&#36136;&#37327;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#20986;&#22240;&#26524;&#25928;&#24212;&#30340;&#20272;&#35745;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36136;&#37327;&#26159;&#19968;&#31181;&#38590;&#20197;&#20272;&#35745;&#30340;&#26500;&#24314;&#65292;&#22240;&#27492;&#25105;&#20204;&#20351;&#29992;&#36127;&#38754;&#32467;&#26524;&#25511;&#21046;&#26041;&#27861;&#65292;&#23558;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#20316;&#20026;&#23545;&#29031;&#21464;&#37327;&#65292;&#20197;&#28040;&#38500;&#36136;&#37327;&#28151;&#26434;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;arXiving&#21487;&#33021;&#20250;&#23545;&#35770;&#25991;&#34987;&#25509;&#21463;&#30340;&#26426;&#20250;&#20135;&#29983;&#19968;&#23450;&#31243;&#24230;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#23384;&#22312;&#24433;&#21709;&#26102;&#65292;&#36825;&#31181;&#24433;&#21709;&#22312;&#20316;&#32773;&#24341;&#29992;&#27425;&#25968;&#21644;&#26426;&#26500;&#25490;&#21517;&#20998;&#32452;&#21518;&#24182;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#34920;&#26126;&#26089;&#26399;arXiving&#23545;ICLR&#20250;&#35758;&#25509;&#21463;&#35770;&#25991;&#30340;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is the effect of releasing a preprint of a paper before it is submitted for peer review? No randomized controlled trial has been conducted, so we turn to observational data to answer this question. We use data from the ICLR conference (2018--2022) and apply methods from causal inference to estimate the effect of arXiving a paper before the reviewing period (early arXiving) on its acceptance to the conference. Adjusting for 18 confounders such as topic, authors, and quality, we may estimate the causal effect. However, since quality is a challenging construct to estimate, we use the negative outcome control method, using paper citation count as a control variable to debias the quality confounding effect. Our results suggest that early arXiving may have a small effect on a paper's chances of acceptance. However, this effect (when existing) does not differ significantly across different groups of authors, as grouped by author citation count and institute rank. This suggests that early
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102; L3Cube-MahaSent-MD&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#39046;&#22495;&#30340; Marathi &#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#22312;&#20854;&#20013;&#65292;&#38024;&#23545;&#27599;&#20010;&#39046;&#22495;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21253;&#21547; 1.5 &#19975;&#20010;&#26679;&#26412;&#30340;&#23376;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24494;&#35843;&#20102;&#19981;&#21516;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821; BERT &#27169;&#22411;&#65292;&#24182;&#25253;&#21578;&#20102; MahaBERT &#27169;&#22411;&#30340;&#26368;&#20339;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#20302;&#36164;&#28304;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.13888</link><description>&lt;p&gt;
L3Cube-MahaSent-MD&#65306;&#19968;&#31181;&#22810;&#22495; Marathi &#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#21644; Transformer &#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
L3Cube-MahaSent-MD: A Multi-domain Marathi Sentiment Analysis Dataset and Transformer Models. (arXiv:2306.13888v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102; L3Cube-MahaSent-MD&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#39046;&#22495;&#30340; Marathi &#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#22312;&#20854;&#20013;&#65292;&#38024;&#23545;&#27599;&#20010;&#39046;&#22495;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21253;&#21547; 1.5 &#19975;&#20010;&#26679;&#26412;&#30340;&#23376;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24494;&#35843;&#20102;&#19981;&#21516;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821; BERT &#27169;&#22411;&#65292;&#24182;&#25253;&#21578;&#20102; MahaBERT &#27169;&#22411;&#30340;&#26368;&#20339;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#20302;&#36164;&#28304;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914; Marathi&#65289;&#20013;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#19968;&#30452;&#21463;&#21040;&#30456;&#24212;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; L3Cube-MahaSent-MD&#65292;&#23427;&#26159;&#19968;&#20010;&#22810;&#39046;&#22495; Marathi &#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30005;&#24433;&#35780;&#35770;&#12289;&#26222;&#36890;&#25512;&#25991;&#12289;&#30005;&#35270;&#33410;&#30446;&#23383;&#24149;&#21644;&#25919;&#27835;&#25512;&#25991;&#31561;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#32422; 6 &#19975;&#20010;&#25163;&#21160;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#35206;&#30422;&#20102;&#19977;&#31181;&#19981;&#21516;&#24773;&#24863; - &#27491;&#38754;&#12289;&#36127;&#38754;&#21644;&#20013;&#24615;&#12290;&#25105;&#20204;&#38024;&#23545;&#27599;&#20010;&#39046;&#22495;&#21019;&#24314;&#20102;&#19968;&#20010;&#23376;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#23376;&#25968;&#25454;&#38598;&#21253;&#21547; 1.5 &#19975;&#20010;&#26679;&#26412;&#12290;&#36825;&#26159; Indic &#24773;&#24863;&#39046;&#22495;&#20869;&#30340;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#38024;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#24494;&#35843;&#20102;&#19981;&#21516;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821; BERT &#27169;&#22411;&#65292;&#24182;&#25253;&#21578;&#20102; MahaBERT &#27169;&#22411;&#30340;&#26368;&#20339;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#38388;&#20998;&#26512;&#65292;&#20174;&#32780;&#20984;&#26174;&#20102;&#20302;&#36164;&#28304;&#22810;&#22495;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#22343;&#21487;&#22312; https://github.com/l3cube-pune/MarathiN &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exploration of sentiment analysis in low-resource languages, such as Marathi, has been limited due to the availability of suitable datasets. In this work, we present L3Cube-MahaSent-MD, a multi-domain Marathi sentiment analysis dataset, with four different domains - movie reviews, general tweets, TV show subtitles, and political tweets. The dataset consists of around 60,000 manually tagged samples covering 3 distinct sentiments - positive, negative, and neutral. We create a sub-dataset for each domain comprising 15k samples. The MahaSent-MD is the first comprehensive multi-domain sentiment analysis dataset within the Indic sentiment landscape. We fine-tune different monolingual and multilingual BERT models on these datasets and report the best accuracy with the MahaBERT model. We also present an extensive in-domain and cross-domain analysis thus highlighting the need for low-resource multi-domain datasets. The data and models are available at https://github.com/l3cube-pune/MarathiN
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20247;&#21253;&#30693;&#35782;&#19982;&#20998;&#24067;&#24335;&#35821;&#20041;&#34920;&#31034;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#38598;&#25104;&#34920;&#31034;&#65292;&#20174;&#32780;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#19981;&#19968;&#33268;&#24615;&#36755;&#20986;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13865</link><description>&lt;p&gt;
IERL: &#21487;&#35299;&#37322;&#30340;&#38598;&#25104;&#34920;&#31034;&#23398;&#20064;--&#32467;&#21512;&#20247;&#21253;&#30693;&#35782;&#19982;&#20998;&#24067;&#24335;&#35821;&#20041;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
IERL: Interpretable Ensemble Representation Learning -- Combining CrowdSourced Knowledge and Distributed Semantic Representations. (arXiv:2306.13865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20247;&#21253;&#30693;&#35782;&#19982;&#20998;&#24067;&#24335;&#35821;&#20041;&#34920;&#31034;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#38598;&#25104;&#34920;&#31034;&#65292;&#20174;&#32780;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#19981;&#19968;&#33268;&#24615;&#36755;&#20986;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#20998;&#24067;&#24335;&#35821;&#20041;&#30340;&#24418;&#24335;&#32534;&#30721;&#21333;&#35789;&#30340;&#21547;&#20041;&#12290;&#20998;&#24067;&#24335;&#35821;&#20041;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#25429;&#25417;&#35821;&#35328;&#26631;&#35760;&#65288;&#21333;&#35789;&#12289;&#30701;&#35821;&#21644;&#21477;&#23376;&#65289;&#20043;&#38388;&#30340;&#20849;&#21516;&#32479;&#35745;&#27169;&#24335;&#12290;LLMs&#22312;&#29992;&#20110;&#27979;&#35797;&#27169;&#22411;&#23545;&#36755;&#20837;&#26631;&#35760;&#30340;&#21547;&#20041;&#29702;&#35299;&#30340;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#65288;GLUE&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#21331;&#36234;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#22788;&#29702;&#24456;&#23569;&#22312;&#35757;&#32451;&#20013;&#20986;&#29616;&#36807;&#30340;&#36755;&#20837;&#65292;&#25110;&#19982;&#19981;&#21516;&#35821;&#22659;&#30456;&#20851;&#30340;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#22312;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#24050;&#30693;&#24187;&#35273;&#29616;&#35937;&#65289;&#26102;&#65292;LLMs&#24448;&#24448;&#20250;&#20135;&#29983;&#24847;&#22806;&#12289;&#19981;&#19968;&#33268;&#25110;&#38169;&#35823;&#30340;&#25991;&#26412;&#36755;&#20986;&#12290;&#20247;&#21253;&#21644;&#19987;&#23478;&#31574;&#21010;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;&#22914;ConceptNet&#65289;&#26088;&#22312;&#20174;&#19968;&#32452;&#32039;&#23494;&#23450;&#20041;&#30340;&#19978;&#19979;&#25991;&#20013;&#25429;&#25417;&#21333;&#35789;&#30340;&#21547;&#20041;&#12290;&#22240;&#27492;&#65292;LLMs&#21487;&#20197;&#21463;&#30410;&#20110;&#21033;&#29992;&#36825;&#26679;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#26469;&#20943;&#23569;&#36755;&#20986;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20247;&#21253;&#30693;&#35782;&#19982;LLMs&#30340;&#20998;&#24067;&#24335;&#35821;&#20041;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#38598;&#25104;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) encode meanings of words in the form of distributed semantics. Distributed semantics capture common statistical patterns among language tokens (words, phrases, and sentences) from large amounts of data. LLMs perform exceedingly well across General Language Understanding Evaluation (GLUE) tasks designed to test a model's understanding of the meanings of the input tokens. However, recent studies have shown that LLMs tend to generate unintended, inconsistent, or wrong texts as outputs when processing inputs that were seen rarely during training, or inputs that are associated with diverse contexts (e.g., well-known hallucination phenomenon in language generation tasks). Crowdsourced and expert-curated knowledge graphs such as ConceptNet are designed to capture the meaning of words from a compact set of well-defined contexts. Thus LLMs may benefit from leveraging such knowledge contexts to reduce inconsistencies in outputs. We propose a novel ensemble learning m
&lt;/p&gt;</description></item><item><title>&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#24403;&#25968;&#25454;&#38598;&#30340;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#20302;&#26102;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PT&#65289;&#32988;&#36807;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#12290;&#24403;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#39640;&#26102;&#65292;MAML&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.13841</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30495;&#30340;&#27604;&#20803;&#23398;&#20064;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Pre-training Truly Better Than Meta-Learning?. (arXiv:2306.13841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13841
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#24403;&#25968;&#25454;&#38598;&#30340;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#20302;&#26102;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PT&#65289;&#32988;&#36807;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#12290;&#24403;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#39640;&#26102;&#65292;MAML&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#30446;&#21069;&#26222;&#36941;&#35748;&#20026;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PT&#65289;&#21152;&#19978;&#22312;&#35780;&#20215;&#26102;&#24494;&#35843;&#26368;&#21518;&#19968;&#23618;&#65292;&#32988;&#36807;&#26631;&#20934;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#30340;&#23454;&#35777;&#30740;&#31350;&#21644;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#27604;&#36739;PT&#21644;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#36825;&#20123;&#35828;&#27861;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#24378;&#35843;&#20351;&#29992;&#30456;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#30456;&#21516;&#30340;&#20248;&#21270;&#22120;&#65292;&#20197;&#21450;&#25152;&#26377;&#27169;&#22411;&#37117;&#35757;&#32451;&#21040;&#25910;&#25947;&#12290;&#20851;&#38190;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26356;&#20005;&#26684;&#30340;&#32479;&#35745;&#24037;&#20855;&#8212;&#8212;&#25928;&#24212;&#37327;&#65288;Cohen's d&#65289;&#8212;&#8212;&#26469;&#30830;&#23450;&#20351;&#29992;PT&#19982;&#20351;&#29992;MAML&#20043;&#38388;&#30340;&#27169;&#22411;&#24046;&#24322;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#28982;&#21518;&#20351;&#29992;&#19968;&#20010;&#39044;&#20808;&#25552;&#20986;&#30340;&#24230;&#37327;&#8212;&#8212;&#22810;&#26679;&#24615;&#31995;&#25968;&#8212;&#8212;&#26469;&#35745;&#31639;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#27491;&#24335;&#22810;&#26679;&#24615;&#12290;&#20351;&#29992;&#36825;&#31181;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20197;&#19979;&#20107;&#23454;&#65306;1. &#24403;&#25968;&#25454;&#38598;&#30340;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#20302;&#26102;&#65292;PT&#22312;&#24179;&#22343;&#24847;&#20041;&#19978;&#32988;&#36807;MAML&#65307;2. &#24403;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#39640;&#26102;&#65292;MAML&#32988;&#36807;PT&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of few-shot learning, it is currently believed that a fixed pre-trained (PT) model, along with fine-tuning the final layer during evaluation, outperforms standard meta-learning algorithms. We re-evaluate these claims under an in-depth empirical examination of an extensive set of formally diverse datasets and compare PT to Model Agnostic Meta-Learning (MAML). Unlike previous work, we emphasize a fair comparison by using: the same architecture, the same optimizer, and all models trained to convergence. Crucially, we use a more rigorous statistical tool -- the effect size (Cohen's d) -- to determine the practical significance of the difference between a model trained with PT vs. a MAML. We then use a previously proposed metric -- the diversity coefficient -- to compute the average formal diversity of a dataset. Using this analysis, we demonstrate the following: 1. when the formal diversity of a data set is low, PT beats MAML on average and 2. when the formal diversity is hi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.13840</link><description>&lt;p&gt;
&#36229;&#36234;&#35268;&#27169;&#65306;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#35777;&#26126;&#20102;LLMs&#26159;&#22312;&#24418;&#24335;&#22810;&#26679;&#30340;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;
&lt;/p&gt;
&lt;p&gt;
Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#39044;&#20808;&#35757;&#32451;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36235;&#21183;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#35757;&#32451;&#24378;&#22823;&#30340;LLMs&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#20294;&#23427;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#27010;&#24565;&#65292;&#23578;&#26410;&#23436;&#20840;&#34920;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;Task2Vec&#22810;&#26679;&#24615;&#31995;&#25968;&#26469;&#22522;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#24418;&#24335;&#26041;&#38754;&#65292;&#36229;&#36234;&#35268;&#27169;&#26412;&#36523;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#30340;&#24418;&#24335;&#22810;&#26679;&#24615;&#39640;&#20110;&#29702;&#35770;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24314;&#31435;&#23545;&#22810;&#26679;&#24615;&#31995;&#25968;&#30340;&#20449;&#24515;&#65292;&#25105;&#20204;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#35813;&#31995;&#25968;&#19982;&#22810;&#26679;&#24615;&#30340;&#30452;&#35266;&#23646;&#24615;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#65292;&#38543;&#30528;&#28508;&#22312;&#27010;&#24565;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23427;&#22686;&#21152;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22810;&#26679;&#24615;&#31995;&#25968;&#26159;&#21487;&#38752;&#30340;&#65292;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#65292;&#24182;&#25512;&#27979;&#23427;&#21487;&#20197;&#20316;&#20026;&#39044;&#35757;&#32451;LLMs&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;NLP Transformer&#20013;&#20998;&#26512;&#20301;&#32622;&#12289;&#21477;&#27861;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#20301;&#32622;&#20449;&#24687;&#36890;&#36807;&#34746;&#26059;&#36335;&#24452;&#22312;&#28145;&#23618;&#20013;&#33258;&#25105;&#20998;&#31163;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20391;&#29983;&#25104;&#35789;&#24615;&#32858;&#31867;&#12290;&#25552;&#20986;&#20102;&#26367;&#20195;&#22312;&#35821;&#20041;&#23884;&#20837;&#20013;&#28155;&#21152;&#20301;&#32622;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13817</link><description>&lt;p&gt;
NLP Transformer&#20013;&#30340;&#21452;&#34746;&#26059;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Double Helix inside the NLP Transformer. (arXiv:2306.13817v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;NLP Transformer&#20013;&#20998;&#26512;&#20301;&#32622;&#12289;&#21477;&#27861;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#20301;&#32622;&#20449;&#24687;&#36890;&#36807;&#34746;&#26059;&#36335;&#24452;&#22312;&#28145;&#23618;&#20013;&#33258;&#25105;&#20998;&#31163;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20391;&#29983;&#25104;&#35789;&#24615;&#32858;&#31867;&#12290;&#25552;&#20986;&#20102;&#26367;&#20195;&#22312;&#35821;&#20041;&#23884;&#20837;&#20013;&#28155;&#21152;&#20301;&#32622;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;NLP Transformer&#20013;&#20998;&#26512;&#19981;&#21516;&#20449;&#24687;&#31867;&#22411;&#30340;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21306;&#20998;&#20102;&#22235;&#23618;&#20449;&#24687;&#65306;&#20301;&#32622;&#12289;&#21477;&#27861;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#65292;&#24120;&#35265;&#30340;&#22312;&#35821;&#20041;&#23884;&#20837;&#20013;&#28155;&#21152;&#20301;&#32622;&#20449;&#24687;&#30340;&#20570;&#27861;&#26159;&#27425;&#20248;&#30340;&#65292;&#25552;&#35758;&#20351;&#29992;&#32447;&#24615;&#21152;&#27861;(Linar-and-Add)&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20301;&#32622;&#20449;&#24687;&#22312;&#28145;&#23618;&#20013;&#30340;&#33258;&#25105;&#20998;&#31163;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23884;&#20837;&#21521;&#37327;&#30340;&#20301;&#32622;&#32452;&#25104;&#37096;&#20998;&#36981;&#24490;&#34746;&#26059;&#30340;&#36335;&#24452;&#65292;&#22312;&#32534;&#30721;&#22120;&#20391;&#21644;&#35299;&#30721;&#22120;&#20391;&#37117;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#21478;&#22806;&#36824;&#23637;&#31034;&#65292;&#32534;&#30721;&#22120;&#20391;&#30340;&#27010;&#24565;&#32500;&#24230;&#29983;&#25104;&#20102;&#35789;&#24615;(PoS)&#30340;&#32858;&#31867;&#12290;&#22312;&#35299;&#30721;&#22120;&#20391;&#65292;&#25105;&#20204;&#23637;&#31034;&#20351;&#29992;&#20108;&#20803;&#35821;&#27861;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#25581;&#31034;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#35789;&#24615;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#38416;&#26126;&#36890;&#36807;NLP Transformer&#30340;&#28145;&#23618;&#20449;&#24687;&#22788;&#29702;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a framework for analyzing various types of information in an NLP Transformer. In this approach, we distinguish four layers of information: positional, syntactic, semantic, and contextual. We also argue that the common practice of adding positional information to semantic embedding is sub-optimal and propose instead a Linear-and-Add approach. Our analysis reveals an autogenetic separation of positional information through the deep layers. We show that the distilled positional components of the embedding vectors follow the path of a helix, both on the encoder side and on the decoder side. We additionally show that on the encoder side, the conceptual dimensions generate Part-of-Speech (PoS) clusters. On the decoder side, we show that a di-gram approach helps to reveal the PoS clusters of the next token. Our approach paves a way to elucidate the processing of information through the deep layers of an NLP Transformer.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22810;&#27169;&#24577;&#21452;&#37325;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65288;MDAT&#65289;&#27169;&#22411;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#24418;&#27880;&#24847;&#21644;&#20849;&#21516;&#20851;&#27880;&#26426;&#21046;&#26469;&#25429;&#25417;&#19981;&#21516;&#24773;&#24863;&#30340;&#36328;&#27169;&#24577;&#20381;&#36182;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#23454;&#29616;&#25913;&#36827;&#30340;&#36328;&#35821;&#35328;&#24773;&#24863;&#35782;&#21035;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.13804</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21452;&#37325;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#23454;&#29616;&#36328;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers. (arXiv:2306.13804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13804
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22810;&#27169;&#24577;&#21452;&#37325;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65288;MDAT&#65289;&#27169;&#22411;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#24418;&#27880;&#24847;&#21644;&#20849;&#21516;&#20851;&#27880;&#26426;&#21046;&#26469;&#25429;&#25417;&#19981;&#21516;&#24773;&#24863;&#30340;&#36328;&#27169;&#24577;&#20381;&#36182;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#23454;&#29616;&#25913;&#36827;&#30340;&#36328;&#35821;&#35328;&#24773;&#24863;&#35782;&#21035;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#26080;&#27861;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#23454;&#29616;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21452;&#37325;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65288;MDAT&#65289;&#27169;&#22411;&#65292;&#20197;&#25913;&#36827;&#36328;&#35821;&#35328;SER&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#37197;&#22791;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#65292;&#21253;&#25324;&#22270;&#24418;&#27880;&#24847;&#21644;&#20849;&#21516;&#20851;&#27880;&#65292;&#20197;&#25429;&#33719;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#23454;&#29616;&#25913;&#36827;&#30340;&#36328;&#35821;&#35328;SER&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21033;&#29992;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#23618;&#36827;&#34892;&#39640;&#23618;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#24773;&#24863;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;MDAT&#22312;&#21508;&#20010;&#38454;&#27573;&#25191;&#34892;&#29305;&#24449;&#34920;&#31034;&#30340;&#32454;&#21270;&#65292;&#24182;&#20026;&#20998;&#31867;&#23618;&#25552;&#20379;&#24773;&#24863;&#26174;&#30528;&#29305;&#24449;&#12290;&#36825;&#31181;&#26032;&#39062;&#26041;&#27861;&#36824;&#30830;&#20445;&#20102;&#27169;&#24577;&#29305;&#23450;&#30340;&#24773;&#24863;&#20449;&#24687;&#30340;&#20445;&#23384;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#20132;&#21449;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent progress in speech emotion recognition (SER), state-of-the-art systems are unable to achieve improved performance in cross-language settings. In this paper, we propose a Multimodal Dual Attention Transformer (MDAT) model to improve cross-language SER. Our model utilises pre-trained models for multimodal feature extraction and is equipped with a dual attention mechanism including graph attention and co-attention to capture complex dependencies across different modalities and achieve improved cross-language SER results using minimal target language data. In addition, our model also exploits a transformer encoder layer for high-level feature representation to improve emotion classification accuracy. In this way, MDAT performs refinement of feature representation at various stages and provides emotional salient features to the classification layer. This novel approach also ensures the preservation of modality-specific emotional information while enhancing cross-modality 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20840;&#29699;&#33539;&#22260;&#20869;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#25512;&#29305;&#19978;&#30340;&#24773;&#24863;&#24577;&#24230;&#65292;&#32467;&#26524;&#34920;&#26126;&#24773;&#24863;&#26497;&#24615;&#24471;&#20998;&#19982;&#30149;&#20363;&#25968;&#37327;&#21450;&#20854;&#21464;&#21270;&#20043;&#38388;&#23384;&#22312;&#32852;&#31995;&#12290;&#30123;&#24773;&#30340;&#21069;&#21322;&#27573;&#24773;&#24863;&#26497;&#24615;&#24471;&#20998;&#21457;&#29983;&#20102;&#21095;&#28872;&#21464;&#21270;&#65292;&#21518;&#26469;&#36235;&#20110;&#31283;&#23450;&#65292;&#26263;&#31034;&#30123;&#33495;&#30340;&#25512;&#24191;&#23545;&#20110;&#20154;&#20204;&#30340;&#35752;&#35770;&#26041;&#21521;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.13797</link><description>&lt;p&gt;
&#26032;&#20896;&#30123;&#33495;&#30456;&#20851;&#24773;&#24863;&#20998;&#26512;&#65306;&#20174;&#30740;&#21457;&#21040;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
An analysis of vaccine-related sentiments from development to deployment of COVID-19 vaccines. (arXiv:2306.13797v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20840;&#29699;&#33539;&#22260;&#20869;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#25512;&#29305;&#19978;&#30340;&#24773;&#24863;&#24577;&#24230;&#65292;&#32467;&#26524;&#34920;&#26126;&#24773;&#24863;&#26497;&#24615;&#24471;&#20998;&#19982;&#30149;&#20363;&#25968;&#37327;&#21450;&#20854;&#21464;&#21270;&#20043;&#38388;&#23384;&#22312;&#32852;&#31995;&#12290;&#30123;&#24773;&#30340;&#21069;&#21322;&#27573;&#24773;&#24863;&#26497;&#24615;&#24471;&#20998;&#21457;&#29983;&#20102;&#21095;&#28872;&#21464;&#21270;&#65292;&#21518;&#26469;&#36235;&#20110;&#31283;&#23450;&#65292;&#26263;&#31034;&#30123;&#33495;&#30340;&#25512;&#24191;&#23545;&#20110;&#20154;&#20204;&#30340;&#35752;&#35770;&#26041;&#21521;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30149;&#27602;&#29190;&#21457;&#21644;&#30123;&#33495;&#25509;&#31181;&#35745;&#21010;&#30340;&#21382;&#21490;&#19978;&#19968;&#30452;&#23384;&#22312;&#30528;&#21453;&#30123;&#33495;&#24773;&#32490;&#12290;COVID-19&#22823;&#27969;&#34892;&#20013;&#65292;&#20154;&#20204;&#23545;&#30123;&#33495;&#30340;&#24656;&#24807;&#21644;&#19981;&#30830;&#23450;&#24615;&#22312;Twitter&#31561;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#26377;&#24456;&#22909;&#30340;&#34920;&#36798;&#12290;&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#20102;&#20840;&#29699;&#33539;&#22260;&#20869;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#25512;&#29305;&#19978;&#30340;&#24773;&#24863;&#24577;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#21487;&#35270;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#25512;&#29305;&#19978;&#30340;&#24773;&#24863;&#26497;&#24615;&#24471;&#20998;&#19982;&#30149;&#20363;&#25968;&#37327;&#21450;&#20854;&#21464;&#21270;&#20043;&#38388;&#23384;&#22312;&#32852;&#31995;&#65292;&#22312;&#30123;&#24773;&#37325;&#22823;&#27874;&#21160;&#26399;&#38388;&#24773;&#24863;&#26497;&#24615;&#24471;&#20998;&#30340;&#21464;&#21270;&#24456;&#22823;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#21457;&#29616;&#30123;&#24773;&#30340;&#21069;&#21322;&#27573;&#24773;&#24863;&#26497;&#24615;&#24471;&#20998;&#21457;&#29983;&#20102;&#21095;&#28872;&#21464;&#21270;&#65292;&#21518;&#26469;&#36235;&#20110;&#31283;&#23450;&#65292;&#26263;&#31034;&#30123;&#33495;&#30340;&#25512;&#24191;&#23545;&#20110;&#20154;&#20204;&#30340;&#35752;&#35770;&#26041;&#21521;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anti-vaccine sentiments have been well-known and reported throughout the history of viral outbreaks and vaccination programmes. The COVID-19 pandemic had fear and uncertainty about vaccines which has been well expressed on social media platforms such as Twitter. We analyse Twitter sentiments from the beginning of the COVID-19 pandemic and study the public behaviour during the planning, development and deployment of vaccines expressed in tweets worldwide using a sentiment analysis framework via deep learning models. In this way, we provide visualisation and analysis of anti-vaccine sentiments over the course of the COVID-19 pandemic. Our results show a link between the number of tweets, the number of cases, and the change in sentiment polarity scores during major waves of COVID-19 cases. We also found that the first half of the pandemic had drastic changes in the sentiment polarity scores that later stabilised which implies that the vaccine rollout had an impact on the nature of discuss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#65292;&#31216;&#20026;Mix And Match&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#21033;&#29992;&#20102;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;LLM&#30340;&#29305;&#28857;&#65292;&#35813;&#25915;&#20987;&#24050;&#34987;&#35777;&#26126;&#22312;&#38543;&#26426;&#21644;&#26377;&#26426;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.13789</link><description>&lt;p&gt;
&#35299;&#26500;&#20998;&#31867;&#22120;&#65306;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Deconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models. (arXiv:2306.13789v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#65292;&#31216;&#20026;Mix And Match&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#21033;&#29992;&#20102;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;LLM&#30340;&#29305;&#28857;&#65292;&#35813;&#25915;&#20987;&#24050;&#34987;&#35777;&#26126;&#22312;&#38543;&#26426;&#21644;&#26377;&#26426;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38738;&#30544;&#65292;&#22914;&#25991;&#26412;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#38544;&#31169;&#25915;&#20987;&#26159;&#33030;&#24369;&#30340;&#65292;&#21253;&#25324;&#26088;&#22312;&#25552;&#21462;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#25454;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#12290;&#22823;&#22810;&#25968;&#20197;&#21069;&#20851;&#20110;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;LLM&#19978;&#65292;&#32780;&#20998;&#31867;&#27169;&#22411;&#34987;&#35748;&#20026;&#26356;&#23433;&#20840;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#31216;&#20026;Mix And Match&#25915;&#20987;&#65292;&#23427;&#21033;&#29992;&#20102;&#22823;&#22810;&#25968;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;LLM&#30340;&#20107;&#23454;&#12290;Mix And Match&#25915;&#20987;&#20351;&#29992;&#30446;&#26631;&#27169;&#22411;&#30340;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#20505;&#36873;&#20196;&#29260;&#65292;&#28982;&#21518;&#20351;&#29992;&#20998;&#31867;&#22836;&#20462;&#21098;&#23427;&#20204;&#12290;&#25105;&#20204;&#24191;&#27867;&#23637;&#31034;&#20102;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;&#20102;&#38543;&#26426;&#19982;&#26377;&#26426;&#30340;&#37329;&#19997;&#38592;&#12290;&#36825;&#39033;&#24037;&#20316;&#31361;&#20986;&#20102;&#22312;&#20998;&#31867;&#27169;&#22411;&#20013;&#32771;&#34385;&#19982;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#30456;&#20851;&#30340;&#38544;&#31169;&#39118;&#38505;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing (NLP) models have become increasingly popular in real-world applications, such as text classification. However, they are vulnerable to privacy attacks, including data reconstruction attacks that aim to extract the data used to train the model. Most previous studies on data reconstruction attacks have focused on LLM, while classification models were assumed to be more secure. In this work, we propose a new targeted data reconstruction attack called the Mix And Match attack, which takes advantage of the fact that most classification models are based on LLM. The Mix And Match attack uses the base model of the target model to generate candidate tokens and then prunes them using the classification head. We extensively demonstrate the effectiveness of the attack using both random and organic canaries. This work highlights the importance of considering the privacy risks associated with data reconstruction attacks in classification models and offers insights into po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;OCR&#21518;&#30340;&#25991;&#26412;&#22788;&#29702;&#21644;YOLOv8&#23545;&#35937;&#35782;&#21035;&#36827;&#34892;&#31616;&#21382;&#20449;&#24687;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#20248;&#20110;&#20197;&#24448;&#22522;&#20110;NLP&#30340;&#27169;&#22411;&#23545;&#31616;&#21382;&#20013;&#25945;&#32946;&#21644;&#32463;&#39564;&#20449;&#24687;&#36827;&#34892;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2306.13775</link><description>&lt;p&gt;
&#36890;&#36807;OCR&#21518;&#30340;&#25991;&#26412;&#22788;&#29702;&#36827;&#34892;&#31616;&#21382;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Resume Information Extraction via Post-OCR Text Processing. (arXiv:2306.13775v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;OCR&#21518;&#30340;&#25991;&#26412;&#22788;&#29702;&#21644;YOLOv8&#23545;&#35937;&#35782;&#21035;&#36827;&#34892;&#31616;&#21382;&#20449;&#24687;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#20248;&#20110;&#20197;&#24448;&#22522;&#20110;NLP&#30340;&#27169;&#22411;&#23545;&#31616;&#21382;&#20013;&#25945;&#32946;&#21644;&#32463;&#39564;&#20449;&#24687;&#36827;&#34892;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20854;&#20013;&#19968;&#39033;&#20027;&#35201;&#20219;&#21153;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#22312;&#31616;&#21382;&#20013;&#26085;&#30410;&#22686;&#21152;&#37325;&#35201;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#24120;&#26159;&#20351;&#29992;NLP&#27169;&#22411;&#23545;&#31616;&#21382;&#20013;&#30340;&#20449;&#24687;&#36827;&#34892;&#21477;&#23376;&#20998;&#31867;&#25552;&#21462;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#39044;&#22788;&#29702;&#65288;&#22914;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#21644;&#23545;&#35937;&#35782;&#21035;&#65289;&#26469;&#20998;&#31867;&#25552;&#21462;&#31616;&#21382;&#20013;&#30340;&#25152;&#26377;&#25991;&#26412;&#32452;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;286&#20221;&#31616;&#21382;&#65292;&#20854;&#20013;&#28085;&#30422;&#20102;IT&#34892;&#19994;5&#20010;&#19981;&#21516;&#32844;&#20301;&#65288;&#25945;&#32946;&#12289;&#32463;&#39564;&#12289;&#25165;&#33021;&#12289;&#20010;&#20154;&#21644;&#35821;&#35328;&#65289;&#30340;&#35201;&#27714;&#65292;&#24182;&#19988;&#20351;&#29992;BERT&#12289;BERT-t&#12289;DistilBERT&#12289;RoBERTa &#21644; XLNet&#31561;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;YOLOv8&#27169;&#22411;&#30340;&#32467;&#26524;&#20063;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;OCR&#21518;&#30340;&#25991;&#26412;&#22788;&#29702;&#21644;YOLOv8&#23545;&#35937;&#35782;&#21035;&#65292;&#35813;&#26041;&#27861;&#27604;&#20808;&#21069;&#30340;&#22522;&#20110;NLP&#30340;&#27169;&#22411;&#22312;&#31616;&#21382;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#25945;&#32946;&#20449;&#24687;&#21644;&#32463;&#39564;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#20998;&#21035;&#36798;&#21040;&#20102;0.94&#21644;0.91&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information extraction (IE), one of the main tasks of natural language processing (NLP), has recently increased importance in the use of resumes. In studies on the text to extract information from the CV, sentence classification was generally made using NLP models. In this study, it is aimed to extract information by classifying all of the text groups after pre-processing such as Optical Character Recognition (OCT) and object recognition with the YOLOv8 model of the resumes. The text dataset consists of 286 resumes collected for 5 different (education, experience, talent, personal and language) job descriptions in the IT industry. The dataset created for object recognition consists of 1198 resumes, which were collected from the open-source internet and labeled as sets of text. BERT, BERT-t, DistilBERT, RoBERTa and XLNet were used as models. F1 score variances were used to compare the model results. In addition, the YOLOv8 model has also been reported comparatively in itself. As a resul
&lt;/p&gt;</description></item><item><title>CHiME-7 DASR &#25361;&#25112;&#36187;&#26088;&#22312;&#36827;&#34892;&#22312;&#36828;&#22330;&#29615;&#22659;&#19979;&#22810;&#35774;&#22791;&#36828;&#31243;&#20250;&#35758;&#36716;&#24405;&#30340;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#65292;&#24182;&#22312;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#20013;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#12290;&#21442;&#19982;&#32773;&#21487;&#20197;&#20351;&#29992;&#24320;&#28304;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.13734</link><description>&lt;p&gt;
CHiME-7 DASR &#25361;&#25112;&#36187;&#65306;&#22810;&#35774;&#22791;&#36828;&#31243;&#20250;&#35758;&#36716;&#24405;&#22312;&#22810;&#26679;&#21270;&#29615;&#22659;&#20013;
&lt;/p&gt;
&lt;p&gt;
The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios. (arXiv:2306.13734v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13734
&lt;/p&gt;
&lt;p&gt;
CHiME-7 DASR &#25361;&#25112;&#36187;&#26088;&#22312;&#36827;&#34892;&#22312;&#36828;&#22330;&#29615;&#22659;&#19979;&#22810;&#35774;&#22791;&#36828;&#31243;&#20250;&#35758;&#36716;&#24405;&#30340;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#65292;&#24182;&#22312;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#20013;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#12290;&#21442;&#19982;&#32773;&#21487;&#20197;&#20351;&#29992;&#24320;&#28304;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CHiME &#25361;&#25112;&#36187;&#22312;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#31532;&#19971;&#23626; CHiME &#25361;&#25112;&#36187;&#20013;&#24341;&#20837;&#20102; CHiME-7 &#36828;&#31243;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035; (DASR) &#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21253;&#25324;&#22312;&#36828;&#22330;&#29615;&#22659;&#19979;&#20351;&#29992;&#22810;&#20010;&#12289;&#21487;&#33021;&#26159;&#24322;&#26500;&#30340;&#24405;&#38899;&#35774;&#22791;&#36827;&#34892;&#32852;&#21512; ASR &#21644;&#20154;&#22768;&#20998;&#31163;&#12290;&#19982;&#20043;&#21069;&#30340;&#25361;&#25112;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#35780;&#20272;3&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#19979;&#30340;&#31995;&#32479;&#24615;&#33021;&#65306;CHiME-6&#12289;DiPCo &#21644; Mixer 6&#12290;&#30446;&#26631;&#26159;&#35753;&#21442;&#19982;&#32773;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#22312;&#19981;&#30693;&#36947;&#20808;&#39564;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#27178;&#36328;&#19981;&#21516;&#38453;&#21015;&#20960;&#20309;&#21644;&#29992;&#20363;&#30340;&#21333;&#20010;&#31995;&#32479;&#12290;&#21478;&#19968;&#20010;&#19982;&#20043;&#21069; CHiME &#19981;&#21516;&#30340;&#26159;&#65292;&#21442;&#19982;&#32773;&#21487;&#20197;&#20351;&#29992;&#24320;&#28304;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CHiME challenges have played a significant role in the development and evaluation of robust speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;Conformer Transducer&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24314;&#31435;&#39640;&#25928;&#32039;&#20945;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#27719;&#32858;&#23618;&#23454;&#29616;&#36328;&#35805;&#35821;&#30340;&#20449;&#24687;&#38598;&#25104;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.13307</link><description>&lt;p&gt;
&#20026;Conformer Transducer&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24314;&#31435;&#39640;&#25928;&#32039;&#20945;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Towards Effective and Compact Contextual Representation for Conformer Transducer Speech Recognition Systems. (arXiv:2306.13307v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;Conformer Transducer&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24314;&#31435;&#39640;&#25928;&#32039;&#20945;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#27719;&#32858;&#23618;&#23454;&#29616;&#36328;&#35805;&#35821;&#30340;&#20449;&#24687;&#38598;&#25104;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20027;&#35201;&#22312;&#35805;&#35821;&#32423;&#21035;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#21487;&#20197;&#32435;&#20837;&#38271;&#33539;&#22260;&#30340;&#36328;&#35805;&#35821;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#22312;Conformer-Transducer&#32534;&#30721;&#22120;&#20013;&#20351;&#29992;&#29305;&#27530;&#35774;&#35745;&#30340;&#27880;&#24847;&#21147;&#27719;&#32858;&#23618;&#65292;&#36890;&#36807;&#39640;&#25928;&#32531;&#23384;&#30340;&#21069;&#38754;&#35805;&#35821;&#21382;&#21490;&#21521;&#37327;&#65292;&#23398;&#20064;&#20102;&#32039;&#20945;&#30340;&#20302;&#32500;&#36328;&#35805;&#35821;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;&#22312;1000&#23567;&#26102;&#30340;Gigaspeech&#35821;&#26009;&#24211;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#19978;&#19979;&#25991;&#21270;&#27969;Conformer-Transducer&#22312;&#24320;&#21457;&#21644;&#27979;&#35797;&#25968;&#25454;&#19978;&#37117;&#27604;&#20165;&#20351;&#29992;&#35805;&#35821;&#20869;&#37096;&#19978;&#19979;&#25991;&#30340;&#22522;&#32447;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#23383;&#38169;&#29575;&#38477;&#20302;&#65288;0.7&#65285;&#21040;0.5&#65285;&#32477;&#23545;&#65292;4.3&#65285;&#21040;3.1&#65285;&#30456;&#23545;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current ASR systems are mainly trained and evaluated at the utterance level. Long range cross utterance context can be incorporated. A key task is to derive a suitable compact representation of the most relevant history contexts. In contrast to previous researches based on either LSTM-RNN encoded histories that attenuate the information from longer range contexts, or frame level concatenation of transformer context embeddings, in this paper compact low-dimensional cross utterance contextual features are learned in the Conformer-Transducer Encoder using specially designed attention pooling layers that are applied over efficiently cached preceding utterances history vectors. Experiments on the 1000-hr Gigaspeech corpus demonstrate that the proposed contextualized streaming Conformer-Transducers outperform the baseline using utterance internal context only with statistically significant WER reductions of 0.7% to 0.5% absolute (4.3% to 3.1% relative) on the dev and test data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21477;&#27861;&#24341;&#23548;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#24335;&#65292;&#36890;&#36807;&#33258;&#19978;&#32780;&#19979;&#30340;&#24418;&#24335;&#32467;&#26500;&#26641;&#29983;&#25104;&#24207;&#21015;&#65292;&#22312;&#33258;&#22238;&#24402;&#22522;&#32447;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#21046;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.11485</link><description>&lt;p&gt;
&#26174;&#24335;&#21477;&#27861;&#24341;&#23548;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Explicit Syntactic Guidance for Neural Text Generation. (arXiv:2306.11485v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21477;&#27861;&#24341;&#23548;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#24335;&#65292;&#36890;&#36807;&#33258;&#19978;&#32780;&#19979;&#30340;&#24418;&#24335;&#32467;&#26500;&#26641;&#29983;&#25104;&#24207;&#21015;&#65292;&#22312;&#33258;&#22238;&#24402;&#22522;&#32447;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#21046;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#37117;&#36981;&#24490;&#24207;&#21015;&#23545;&#24207;&#21015;&#33539;&#20363;&#12290;&#29983;&#25104;&#35821;&#27861;&#34920;&#26126;&#20154;&#31867;&#36890;&#36807;&#23398;&#20064;&#35821;&#35328;&#35821;&#27861;&#26469;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21477;&#27861;&#24341;&#23548;&#30340;&#29983;&#25104;&#27169;&#24335;&#65292;&#23427;&#36890;&#36807;&#33258;&#19978;&#32780;&#19979;&#30340;&#24418;&#24335;&#32467;&#26500;&#26641;&#29983;&#25104;&#24207;&#21015;&#12290;&#35299;&#30721;&#36807;&#31243;&#21487;&#20197;&#20998;&#35299;&#20026;&#20004;&#20010;&#37096;&#20998;&#65306;&#65288;1&#65289;&#22312;&#28304;&#21477;&#23376;&#30340;&#35789;&#27719;&#21270;&#21477;&#27861;&#19978;&#19979;&#25991;&#20013;&#20026;&#27599;&#20010;&#25104;&#20998;&#39044;&#27979;&#22635;&#20805;&#25991;&#26412;&#65307;&#65288;2&#65289;&#26144;&#23556;&#21644;&#25193;&#23637;&#27599;&#20010;&#25104;&#20998;&#20197;&#26500;&#24314;&#19979;&#19968;&#32423;&#21477;&#27861;&#19978;&#19979;&#25991;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#26463;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#23618;&#22320;&#26597;&#25214;&#21487;&#33021;&#30340;&#21477;&#27861;&#32467;&#26500;&#12290;&#23545;&#20110;&#37322;&#20041;&#29983;&#25104;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#33258;&#22238;&#24402;&#22522;&#32447;&#65292;&#21516;&#26102;&#36824;&#34920;&#29616;&#20986;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#21046;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing text generation models follow the sequence-to-sequence paradigm. Generative Grammar suggests that humans generate natural language texts by learning language grammar. We propose a syntax-guided generation schema, which generates the sequence guided by a constituency parse tree in a top-down direction. The decoding process can be decomposed into two parts: (1) predicting the infilling texts for each constituent in the lexicalized syntax context given the source sentence; (2) mapping and expanding each constituent to construct the next-level syntax context. Accordingly, we propose a structural beam search method to find possible syntax structures hierarchically. Experiments on paraphrase generation and machine translation show that the proposed method outperforms autoregressive baselines, while also demonstrating effectiveness in terms of interpretability, controllability, and diversity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#29031;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#30456;&#21453;&#24773;&#24863;&#26497;&#24615;&#30340;&#35266;&#28857;&#34920;&#36798;&#65292;&#24182;&#21033;&#29992;T5&#27169;&#22411;&#26816;&#32034;&#25513;&#30721;&#65292;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;ABSA&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.11260</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#29031;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A novel Counterfactual method for aspect-based sentiment analysis. (arXiv:2306.11260v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#29031;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#30456;&#21453;&#24773;&#24863;&#26497;&#24615;&#30340;&#35266;&#28857;&#34920;&#36798;&#65292;&#24182;&#21033;&#29992;T5&#27169;&#22411;&#26816;&#32034;&#25513;&#30721;&#65292;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;ABSA&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#24773;&#24863;&#20998;&#26512; (ABSA) &#26159;&#19968;&#39033;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#35780;&#20272;&#20219;&#21153;&#65292;&#23427;&#20998;&#26512;&#35780;&#20272;&#26041;&#38754;&#30340;&#24773;&#24863;&#26497;&#24615;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#20165;&#20851;&#27880;&#35266;&#28857;&#34920;&#36798;&#30340;&#35782;&#21035;&#65292;&#32780;&#24573;&#30053;&#20102;&#35266;&#28857;&#34920;&#36798;&#30340;&#22810;&#26679;&#24615;&#23545;ABSA&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#29031;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#30456;&#21453;&#24773;&#24863;&#26497;&#24615;&#30340;&#35266;&#28857;&#34920;&#36798;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#38598;&#25104;&#26799;&#24230;&#26469;&#35782;&#21035;&#21644;&#23631;&#34109;&#35266;&#28857;&#34920;&#36798;&#12290;&#28982;&#21518;&#23558;&#21453;&#21521;&#26631;&#31614;&#30340;&#25552;&#31034;&#32452;&#21512;&#21040;&#21407;&#22987;&#25991;&#26412;&#20013;&#65292;&#24182;&#26368;&#32456;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (PLM) T5 &#26469;&#26816;&#32034;&#25513;&#30721;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#23545;&#29031;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22312;&#19977;&#20010;ABSA&#25968;&#25454;&#38598; (&#21363;Laptop&#65292;Restaurant&#21644;MAMS) &#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based-sentiment-analysis (ABSA) is a fine-grained sentiment evaluation task, which analyze the emotional polarity of the evaluation aspects. However, previous works only focus on the identification of opinion expressions, forget that the diversity of opinion expressions also has great impacts on the ABSA task. To mitigate this problem, we propose a novel counterfactual data augmentation method to generate opinion expression with reversed sentiment polarity. Specially, the integrated gradients are calculated to identify and mask the opinion expression. Then, a prompt with the reverse label is combined to the original text, and a pre-trained language model (PLM), T5, is finally employed to retrieve the masks. The experimental results show the proposed counterfactual data augmentation method perform better than current augmentation methods on three ABSA datasets, i.e. Laptop, Restaurant and MAMS.
&lt;/p&gt;</description></item><item><title>LoSparse &#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#20302;&#31209;&#30697;&#38453;&#21644;&#31232;&#30095;&#30697;&#38453;&#36924;&#36817;&#26435;&#37325;&#30697;&#38453;&#65292;&#32467;&#21512;&#20102;&#20302;&#31209;&#36924;&#36817;&#21644;&#35009;&#21098;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.11222</link><description>&lt;p&gt;
&#22522;&#20110;&#20302;&#31209;&#21644;&#31232;&#30095;&#36924;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#21270;&#21387;&#32553; LoSparse
&lt;/p&gt;
&lt;p&gt;
LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation. (arXiv:2306.11222v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11222
&lt;/p&gt;
&lt;p&gt;
LoSparse &#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#20302;&#31209;&#30697;&#38453;&#21644;&#31232;&#30095;&#30697;&#38453;&#36924;&#36817;&#26435;&#37325;&#30697;&#38453;&#65292;&#32467;&#21512;&#20102;&#20302;&#31209;&#36924;&#36817;&#21644;&#35009;&#21098;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#27169;&#22411;&#22312;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#36807;&#20110;&#24222;&#22823;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#38477;&#20302;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LoSparse&#65288;&#20302;&#31209;&#21644;&#31232;&#30095;&#36924;&#36817;&#65289;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#20302;&#31209;&#30697;&#38453;&#21644;&#31232;&#30095;&#30697;&#38453;&#20043;&#21644;&#36924;&#36817;&#26435;&#37325;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#20302;&#31209;&#36924;&#36817;&#21644;&#35009;&#21098;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#12290;&#20302;&#31209;&#36924;&#36817;&#21387;&#32553;&#20102;&#31070;&#32463;&#20803;&#20013;&#30340;&#19968;&#33268;&#21644;&#34920;&#36798;&#21147;&#24378;&#30340;&#37096;&#20998;&#65292;&#32780;&#35009;&#21098;&#21017;&#28040;&#38500;&#20102;&#31070;&#32463;&#20803;&#20013;&#30340;&#19981;&#19968;&#33268;&#21644;&#34920;&#36798;&#21147;&#19981;&#24378;&#30340;&#37096;&#20998;&#12290;&#35009;&#21098;&#22686;&#24378;&#20102;&#20302;&#31209;&#36924;&#36817;&#30340;&#22810;&#26679;&#24615;&#65292;&#20302;&#31209;&#36924;&#36817;&#38450;&#27490;&#20102;&#35009;&#21098;&#20002;&#22833;&#34920;&#36798;&#21147;&#24378;&#30340;&#31070;&#32463;&#20803;&#36807;&#22810;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#31572;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have achieved remarkable results in various natural language tasks, but they are often prohibitively large, requiring massive memories and computational resources. To reduce the size and complexity of these models, we propose LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix. Our method combines the advantages of both low-rank approximations and pruning, while avoiding their limitations. Low-rank approximation compresses the coherent and expressive parts in neurons, while pruning removes the incoherent and non-expressive parts in neurons. Pruning enhances the diversity of low-rank approximations, and low-rank approximation prevents pruning from losing too many expressive neurons. We evaluate our method on natural language understanding, question answering, and natural language generation tasks. We show that it significantly outperforms existing compre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.09869</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#29992;&#20110;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#30340;&#22270;&#20687;&#26377;&#26102;&#26080;&#27861;&#25429;&#25417;&#21040;&#25991;&#26412;&#25552;&#31034;&#30340;&#39044;&#26399;&#35821;&#20041;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#34987;&#31216;&#20026;&#35821;&#20041;&#38169;&#20301;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBM&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#27599;&#20010;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20013;&#21046;&#23450;&#28508;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#25991;&#26412;&#23884;&#20837;&#30340;EBM&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#33719;&#24471;&#19978;&#19979;&#25991;&#21521;&#37327;&#30340;&#23545;&#25968;&#21518;&#39564;&#26799;&#24230;&#65292;&#21487;&#20197;&#26356;&#26032;&#21644;&#36716;&#31227;&#21040;&#21518;&#32493;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#65292;&#20174;&#32780;&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#23884;&#22871;&#23618;&#27425;&#30340;&#33021;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#28508;&#22312;EBMs&#36824;&#20801;&#35768;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#65292;&#21363;&#36890;&#36807;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25991;&#26412;&#25552;&#31034;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#38169;&#20301;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32763;&#35793;&#20102;MIT&#25968;&#23398;&#21644;EECS&#35838;&#31243;&#20013;&#30340;4550&#20010;&#39064;&#30446;&#65292;&#24320;&#21457;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#35780;&#20998;&#30340;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#35838;&#31243;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.08997</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;MIT&#25968;&#23398;&#21644;EECS&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models. (arXiv:2306.08997v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08997
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32763;&#35793;&#20102;MIT&#25968;&#23398;&#21644;EECS&#35838;&#31243;&#20013;&#30340;4550&#20010;&#39064;&#30446;&#65292;&#24320;&#21457;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#35780;&#20998;&#30340;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#35838;&#31243;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20102;&#33719;&#21462;&#23398;&#20301;&#25152;&#38656;&#30340;&#25152;&#26377;MIT&#25968;&#23398;&#21644;&#30005;&#27668;&#24037;&#31243;&#21450;&#35745;&#31639;&#26426;&#31185;&#23398;&#65288;EECS&#65289;&#35838;&#31243;&#30340;&#39064;&#30446;&#38598;&#12289;&#26399;&#20013;&#32771;&#35797;&#21644;&#26399;&#26411;&#32771;&#35797;&#20013;&#30340;4550&#20010;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20219;&#20309;MIT&#25968;&#23398;&#21644;EECS&#19987;&#19994;&#27605;&#19994;&#35201;&#27714;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3.5&#25104;&#21151;&#35299;&#20915;&#20102;&#25972;&#20010;MIT&#35838;&#31243;&#30340;&#19977;&#20998;&#20043;&#19968;&#65292;&#32780;GPT-4&#22312;&#39064;&#30446;&#20013;&#19981;&#21253;&#21547;&#22270;&#20687;&#30340;&#27979;&#35797;&#38598;&#19978;&#32463;&#36807;&#25552;&#31034;&#24037;&#31243;&#21518;&#36798;&#21040;&#20102;&#23436;&#32654;&#30340;&#35299;&#20915;&#29575;&#12290;&#25105;&#20204;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#37319;&#29992;GPT-4&#33258;&#21160;&#35780;&#20998;&#65292;&#25552;&#20379;&#20102;&#35838;&#31243;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#31867;&#22411;&#30340;&#35814;&#32454;&#24615;&#33021;&#20998;&#26512;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#23884;&#20837;&#20302;&#32500;&#31354;&#38388;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38382;&#39064;&#12289;&#20027;&#39064;&#21644;&#35838;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#21738;&#20123;&#38382;&#39064;&#21644;&#35838;&#31243;&#26159;&#35299;&#20915;&#20854;&#20182;&#38382;&#39064;&#21644;&#35838;&#31243;&#25152;&#24517;&#38656;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We curate a comprehensive dataset of 4,550 questions and solutions from problem sets, midterm exams, and final exams across all MIT Mathematics and Electrical Engineering and Computer Science (EECS) courses required for obtaining a degree. We evaluate the ability of large language models to fulfill the graduation requirements for any MIT major in Mathematics and EECS. Our results demonstrate that GPT-3.5 successfully solves a third of the entire MIT curriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate on a test set excluding questions based on images. We fine-tune an open-source large language model on this dataset. We employ GPT-4 to automatically grade model responses, providing a detailed performance breakdown by course, question, and answer type. By embedding questions in a low-dimensional space, we explore the relationships between questions, topics, and classes and discover which questions and classes are required for solving other questions and classes
&lt;/p&gt;</description></item><item><title>MOFI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598; I2E&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#23398;&#20064;&#21040;&#20102;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07952</link><description>&lt;p&gt;
MOFI: &#20174;&#21547;&#22122;&#23454;&#20307;&#26631;&#27880;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07952
&lt;/p&gt;
&lt;p&gt;
MOFI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598; I2E&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#23398;&#20064;&#21040;&#20102;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411; MOFI&#65292;&#26088;&#22312;&#20174;&#21547;&#22122;&#23454;&#20307;&#26631;&#27880;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#12290;MOFI &#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#26377;&#20004;&#28857;&#19981;&#21516;&#65306;&#65288;i&#65289;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#65288;ii&#65289;&#35757;&#32451;&#37197;&#26041;&#12290;&#22312;&#25968;&#25454;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#20174; alt-text &#20013;&#25552;&#21462;&#23454;&#20307;&#65292;&#28982;&#21518;&#20351;&#29992; CLIP &#27169;&#22411;&#36873;&#25321;&#27491;&#30830;&#30340;&#23454;&#20307;&#20316;&#20026;&#22270;&#20687;&#30340;&#26631;&#31614;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#26131;&#34892;&#65292;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#20174; web &#19978;&#25366;&#25496;&#30340;&#25968;&#21313;&#20159;&#20010;&#22270;&#20687;&#25991;&#26412;&#23545;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102; Image-to-Entities&#65288;I2E&#65289;&#36825;&#19968;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547; 10 &#20159;&#24352;&#22270;&#20687;&#21644; 200 &#19975;&#20010;&#19981;&#21516;&#30340;&#23454;&#20307;&#65292;&#28085;&#30422;&#20102;&#37326;&#22806;&#20016;&#23500;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;&#22522;&#20110; I2E &#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#21253;&#25324;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#12289;&#23545;&#27604;&#24230;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web. Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#21508;&#35821;&#35328;&#23618;&#38754;&#19978;&#30340;&#29305;&#24449;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#20808;&#36827;&#30340;&#22303;&#32819;&#20854;&#25991;&#26412;&#21487;&#35835;&#24615;&#24037;&#20855;&#65292;&#21457;&#29616;&#20102;&#24433;&#21709;&#22303;&#32819;&#20854;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.03774</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#35821;&#35328;&#29305;&#24449;&#25552;&#39640;&#22303;&#32819;&#20854;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Hybrid Linguistic Features for Turkish Text Readability. (arXiv:2306.03774v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#21508;&#35821;&#35328;&#23618;&#38754;&#19978;&#30340;&#29305;&#24449;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#20808;&#36827;&#30340;&#22303;&#32819;&#20854;&#25991;&#26412;&#21487;&#35835;&#24615;&#24037;&#20855;&#65292;&#21457;&#29616;&#20102;&#24433;&#21709;&#22303;&#32819;&#20854;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;&#22303;&#32819;&#20854;&#25991;&#26412;&#30340;&#33258;&#21160;&#21487;&#35835;&#24615;&#35780;&#20272;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#35789;&#27719;&#12289;&#24418;&#24577;&#21477;&#27861;&#12289;&#35821;&#27861;&#21644;&#35805;&#35821;&#27700;&#24179;&#30340;&#35821;&#35328;&#29305;&#24449;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#21487;&#35835;&#24615;&#24037;&#20855;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20256;&#32479;&#21487;&#35835;&#24615;&#20844;&#24335;&#19982;&#29616;&#20195;&#33258;&#21160;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#30830;&#23450;&#20102;&#24433;&#21709;&#22303;&#32819;&#20854;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the first comprehensive study on automatic readability assessment of Turkish texts. We combine state-of-the-art neural network models with linguistic features at lexical, morphosyntactic, syntactic and discourse levels to develop an advanced readability tool. We evaluate the effectiveness of traditional readability formulas compared to modern automated methods and identify key linguistic features that determine the readability of Turkish texts.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03763</link><description>&lt;p&gt;
ChatGPT&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#24050;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20174;&#26102;&#38388;&#25991;&#26412;&#25968;&#25454;&#65288;&#23588;&#20854;&#26159;&#36130;&#32463;&#26032;&#38395;&#65289;&#25512;&#26029;&#21160;&#24577;&#32593;&#32476;&#32467;&#26500;&#30340;&#28508;&#21147;&#20173;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#30340;&#22270;&#25512;&#26029;&#33021;&#21147;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24039;&#22937;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#23558;&#36825;&#20123;&#32593;&#32476;&#32467;&#26500;&#34701;&#21512;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36827;&#34892;&#21518;&#32493;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#27169;&#22411;&#30340;&#20135;&#20986;&#26500;&#24314;&#30340;&#32452;&#21512;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#24180;&#21270;&#32047;&#35745;&#22238;&#25253;&#12289;&#26356;&#20302;&#30340;&#27874;&#21160;&#24615;&#21644;&#26368;&#22823;&#22238;&#25764;&#12290;&#36825;&#31181;&#21331;&#36234;&#34920;&#29616;&#31361;&#26174;&#20102;ChatGPT&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#32593;&#32476;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#22522;&#20110;&#35777;&#25454;&#30340;&#25945;&#23398;&#35774;&#35745;&#19987;&#19994;&#30693;&#35782;&#65292;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;GPT-4&#22312;&#25945;&#23398;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;LLMs&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#26410;&#26469;LLMs&#20026;&#25152;&#26377;&#23398;&#20064;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#25945;&#32946;&#20869;&#23481;&#30340;&#24895;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.01006</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#22522;&#20110;&#35777;&#25454;&#30340;&#25945;&#23398;&#35774;&#35745;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Scaling Evidence-based Instructional Design Expertise through Large Language Models. (arXiv:2306.01006v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#22522;&#20110;&#35777;&#25454;&#30340;&#25945;&#23398;&#35774;&#35745;&#19987;&#19994;&#30693;&#35782;&#65292;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;GPT-4&#22312;&#25945;&#23398;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;LLMs&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#26410;&#26469;LLMs&#20026;&#25152;&#26377;&#23398;&#20064;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#25945;&#32946;&#20869;&#23481;&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#26469;&#25299;&#23637;&#25945;&#23398;&#35774;&#35745;&#39046;&#22495;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#24357;&#21512;&#29702;&#35770;&#25945;&#32946;&#30740;&#31350;&#21644;&#23454;&#38469;&#23454;&#26045;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25506;&#35752;&#20102;AI&#39537;&#21160;&#20869;&#23481;&#29983;&#25104;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#24378;&#35843;&#20102;&#20154;&#24037;&#30417;&#30563;&#26469;&#30830;&#20445;&#25945;&#32946;&#26448;&#26009;&#30340;&#36136;&#37327;&#30340;&#24517;&#35201;&#24615;&#12290;&#36890;&#36807;&#20004;&#20010;&#35814;&#32454;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;GPT-4&#21019;&#24314;&#19981;&#21516;&#35838;&#31243;&#30340;&#22797;&#26434;&#39640;&#38454;&#35780;&#20272;&#21644;&#31215;&#26497;&#23398;&#20064;&#32452;&#20214;&#12290;&#20174;&#25105;&#20204;&#30340;&#32463;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#25945;&#23398;&#35774;&#35745;&#20219;&#21153;&#20013;&#26377;&#25928;&#20351;&#29992;LLMs&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#22914;&#21033;&#29992;&#27169;&#26495;&#65292;&#24494;&#35843;&#65292;&#22788;&#29702;&#24847;&#22806;&#30340;&#36755;&#20986;&#65292;&#23454;&#29616;LLM&#38142;&#65292;&#24341;&#29992;&#21442;&#32771;&#25991;&#29486;&#65292;&#35780;&#20272;&#36755;&#20986;&#65292;&#21019;&#24314;&#35780;&#20998;&#26631;&#20934;&#21644;&#29983;&#25104;&#24178;&#25200;&#39033;&#12290;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;&#19968;&#20010;&#24895;&#26223;&#65292;&#26410;&#26469;LLMs&#21487;&#20197;&#20026;&#25152;&#26377;&#32972;&#26223;&#21644;&#27700;&#24179;&#30340;&#23398;&#20064;&#32773;&#25552;&#20379;&#26131;&#20110;&#25509;&#21463;&#21644;&#20010;&#24615;&#21270;&#30340;&#25945;&#32946;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive exploration of leveraging Large Language Models (LLMs), specifically GPT-4, in the field of instructional design. With a focus on scaling evidence-based instructional design expertise, our research aims to bridge the gap between theoretical educational studies and practical implementation. We discuss the benefits and limitations of AI-driven content generation, emphasizing the necessity of human oversight in ensuring the quality of educational materials. This work is elucidated through two detailed case studies where we applied GPT-4 in creating complex higher-order assessments and active learning components for different courses. From our experiences, we provide best practices for effectively using LLMs in instructional design tasks, such as utilizing templates, fine-tuning, handling unexpected output, implementing LLM chains, citing references, evaluating output, creating rubrics, grading, and generating distractors. We also share our vision of a f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#20351;&#29992;&#20102;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;Spider&#19978;&#23454;&#29616;&#20102;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#24182;&#26174;&#30528;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00739</link><description>&lt;p&gt;
SQL-PaLM&#65306;&#38024;&#23545;Text-to-SQL&#30340;&#25913;&#36827;&#22823;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. (arXiv:2306.00739v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#20351;&#29992;&#20102;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;Spider&#19978;&#23454;&#29616;&#20102;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#24182;&#26174;&#30528;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#20010;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#20852;&#21151;&#33021;&#26159;&#29983;&#25104;&#20195;&#30721;&#65292;&#21253;&#25324;&#29992;&#20110;&#25968;&#25454;&#24211;&#30340;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#12290;&#23545;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#36716;&#25442;&#20026;SQL&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#21363;Text-to-SQL&#65292;LLMs&#30340;&#36866;&#24212;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#20351;&#29992;&#30340;&#36866;&#24212;&#24615;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#21033;&#29992;&#20102;PaLM-2&#65292;&#25512;&#21160;&#20102;&#20004;&#31181;&#35774;&#32622;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;Few-shot SQL-PaLM&#22522;&#20110;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#22312;Spider&#19978;&#23454;&#29616;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#26174;&#30528;&#36739;&#22823;&#30340;&#24494;&#35843;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;SQL-PALM&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;1%&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23558;SQL-PaLM&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#20854;&#23545;&#20854;&#20182;&#25361;&#25112;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One impressive emergent capability of large language models (LLMs) is generation of code, including Structured Query Language (SQL) for databases. For the task of converting natural language text to SQL queries, Text-to-SQL, adaptation of LLMs is of paramount importance, both in in-context learning and fine-tuning settings, depending on the amount of adaptation data used. In this paper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on PaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is based on an execution-based self-consistency prompting approach designed for Text-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our best knowledge is the first to outperform previous state-of-the-art with fine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the fine-tuned SQL-PALM outperforms it further by another 1%. Towards applying SQL-PaLM to real-world scenarios we further evaluate its robustness on other chall
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#20174;&#22823;&#37327;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30284;&#30151;&#30456;&#20851;&#23454;&#20307;&#21644;&#23454;&#20307;&#38388;&#20851;&#31995;&#65292;&#20197;&#24110;&#21161;&#25512;&#36827;&#30284;&#30151;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.00013</link><description>&lt;p&gt;
&#30284;&#30151;&#23454;&#20307;&#30340;&#20851;&#32852;&#21644;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Approach for Cancer Entities Association and Classification. (arXiv:2306.00013v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#20174;&#22823;&#37327;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30284;&#30151;&#30456;&#20851;&#23454;&#20307;&#21644;&#23454;&#20307;&#38388;&#20851;&#31995;&#65292;&#20197;&#24110;&#21161;&#25512;&#36827;&#30284;&#30151;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#65288;WHO&#65289;&#30340;&#25968;&#25454;&#65292;&#30284;&#30151;&#26159;&#20840;&#29699;&#31532;&#20108;&#22823;&#27515;&#22240;&#12290;&#19981;&#21516;&#31867;&#22411;&#30284;&#30151;&#30340;&#31185;&#23398;&#30740;&#31350;&#20197;&#27599;&#24180;&#21457;&#24067;&#22823;&#37327;&#30340;&#30740;&#31350;&#25991;&#31456;&#30340;&#36895;&#24230;&#19981;&#26029;&#22686;&#38271;&#12290;&#19982;&#22522;&#22240;&#30456;&#20851;&#30340;&#33647;&#29289;&#12289;&#35786;&#26029;&#12289;&#39118;&#38505;&#12289;&#30151;&#29366;&#12289;&#27835;&#30103;&#31561;&#30340;&#20449;&#24687;&#21644;&#30693;&#35782;&#26159;&#24110;&#21161;&#25506;&#32034;&#21644;&#25512;&#36827;&#30284;&#30151;&#30740;&#31350;&#36827;&#23637;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#25163;&#21160;&#31579;&#36873;&#36825;&#20040;&#22823;&#37327;&#30340;&#25991;&#31456;&#38750;&#24120;&#36153;&#26102;&#36153;&#21147;&#65292;&#24456;&#38590;&#21046;&#23450;&#20219;&#20309;&#20551;&#35774;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#26368;&#20026;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21151;&#33021;&#65292;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#65292;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#21457;&#29616;&#30693;&#35782;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20511;&#21161;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#20869;&#32622;&#23383;&#20856;&#35782;&#21035;&#24182;&#25552;&#21462;&#19982;&#30284;&#30151;&#30456;&#20851;&#30340;&#39044;&#23450;&#20041;&#23454;&#20307;&#12290;&#25991;&#26412;&#20998;&#31867;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24110;&#21161;&#25506;&#31350;&#30284;&#30151;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
According to the World Health Organization (WHO), cancer is the second leading cause of death globally. Scientific research on different types of cancers grows at an ever-increasing rate, publishing large volumes of research articles every year. The insight information and the knowledge of the drug, diagnostics, risk, symptoms, treatments, etc., related to genes are significant factors that help explore and advance the cancer research progression. Manual screening of such a large volume of articles is very laborious and time-consuming to formulate any hypothesis. The study uses the two most non-trivial NLP, Natural Language Processing functions, Entity Recognition, and text classification to discover knowledge from biomedical literature. Named Entity Recognition (NER) recognizes and extracts the predefined entities related to cancer from unstructured text with the support of a user-friendly interface and built-in dictionaries. Text classification helps to explore the insights into the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22330;&#26223;&#22270;&#21644;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21270;&#34920;&#36798;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20998;&#31867;&#12290;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30456;&#27604;&#20351;&#29992;&#23398;&#20064;&#34920;&#36798;&#24335;&#30340;&#27169;&#22411;&#26377;&#25152;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2305.18391</link><description>&lt;p&gt;
MemeGraphs: &#23558;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#36830;
&lt;/p&gt;
&lt;p&gt;
MemeGraphs: Linking Memes to Knowledge Graphs. (arXiv:2305.18391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18391
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22330;&#26223;&#22270;&#21644;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21270;&#34920;&#36798;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20998;&#31867;&#12290;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30456;&#27604;&#20351;&#29992;&#23398;&#20064;&#34920;&#36798;&#24335;&#30340;&#27169;&#22411;&#26377;&#25152;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#26159;&#19968;&#31181;&#22312;&#31038;&#20132;&#23186;&#20307;&#21644;&#20114;&#32852;&#32593;&#19978;&#27969;&#34892;&#30340;&#20256;&#25773;&#36235;&#21183;&#21644;&#35266;&#28857;&#30340;&#24418;&#24335;&#65292;&#32467;&#21512;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#27169;&#24335;&#12290;&#23427;&#20204;&#21487;&#20197;&#34920;&#36798;&#24189;&#40664;&#21644;&#35773;&#21050;&#65292;&#20294;&#20063;&#21487;&#33021;&#21547;&#26377;&#20882;&#29359;&#24615;&#30340;&#20869;&#23481;&#12290;&#33258;&#21160;&#20998;&#26512;&#21644;&#20998;&#31867;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20854;&#35299;&#37322;&#20381;&#36182;&#20110;&#23545;&#35270;&#35273;&#20803;&#32032;&#12289;&#35821;&#35328;&#21644;&#32972;&#26223;&#30693;&#35782;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#26159;&#26377;&#24847;&#20041;&#22320;&#34920;&#31034;&#36825;&#20123;&#26469;&#28304;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20197;&#20415;&#23558;&#34920;&#24773;&#21253;&#20316;&#20026;&#25972;&#20307;&#20998;&#31867;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22330;&#26223;&#22270;&#20316;&#20026;&#34920;&#31034;&#22270;&#20687;&#20013;&#29289;&#20307;&#21450;&#20854;&#35270;&#35273;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#36798;&#26041;&#24335;&#65292;&#24182;&#23558;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#20998;&#31867;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;ImgBERT&#36827;&#34892;&#27604;&#36739;&#65292;&#21518;&#32773;&#20351;&#29992;&#20165;&#23398;&#20064;&#65288;&#32780;&#19981;&#26159;&#32467;&#26500;&#21270;&#65289;&#30340;&#34920;&#36798;&#24335;&#36827;&#34892;&#22810;&#27169;&#24335;&#24314;&#27169;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22987;&#32456;&#26377;&#25152;&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#20154;&#24037;&#22270;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20379;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memes are a popular form of communicating trends and ideas in social media and on the internet in general, combining the modalities of images and text. They can express humor and sarcasm but can also have offensive content. Analyzing and classifying memes automatically is challenging since their interpretation relies on the understanding of visual elements, language, and background knowledge. Thus, it is important to meaningfully represent these sources and the interaction between them in order to classify a meme as a whole. In this work, we propose to use scene graphs, that express images in terms of objects and their visual relations, and knowledge graphs as structured representations for meme classification with a Transformer-based architecture. We compare our approach with ImgBERT, a multimodal model that uses only learned (instead of structured) representations of the meme, and observe consistent improvements. We further provide a dataset with human graph annotations that we compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.17760</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;
&lt;/p&gt;
&lt;p&gt;
Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Ouyang&#31561;&#20154;&#65292;2022&#65289;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#65288;Kahneman&#65292;2011&#65289;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24515;&#29702;&#23398;&#23478;&#20204;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#36825;&#20010;&#26694;&#26550;&#30340;&#36884;&#24452;&#12290;&#26412;&#30740;&#31350;&#23454;&#36136;&#19978;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#26469;&#33719;&#24471;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#26041;&#38754;&#30340;&#28145;&#21051;&#35265;&#35299;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SAIL&#30340;&#25628;&#32034;&#22686;&#24378;&#25351;&#20196;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20197;&#20869;&#37096;&#21644;&#22806;&#37096;&#25628;&#32034;&#24341;&#25806;&#29983;&#25104;&#30340;&#25628;&#32034;&#32467;&#26524;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#29983;&#25104;&#21644;&#25351;&#20196;&#36319;&#36394;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#25351;&#20196;&#12289;&#25509;&#22320;&#20449;&#24687;&#21644;&#21709;&#24212;&#19977;&#20803;&#32452;&#30340;&#26032;&#30340;&#25628;&#32034;&#22522;&#30784;&#35757;&#32451;&#38598;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#38656;&#35201;&#23398;&#20064;&#36807;&#28388;&#24178;&#25200;&#27573;&#33853;&#24182;&#29983;&#25104;&#30446;&#26631;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.15225</link><description>&lt;p&gt;
SAIL: Search-Augmented Instruction Learning &#65288;SAIL&#65306;&#25628;&#32034;&#22686;&#24378;&#30340;&#25351;&#20196;&#23398;&#20064;&#65289;
&lt;/p&gt;
&lt;p&gt;
SAIL: Search-Augmented Instruction Learning. (arXiv:2305.15225v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15225
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SAIL&#30340;&#25628;&#32034;&#22686;&#24378;&#25351;&#20196;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20197;&#20869;&#37096;&#21644;&#22806;&#37096;&#25628;&#32034;&#24341;&#25806;&#29983;&#25104;&#30340;&#25628;&#32034;&#32467;&#26524;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#29983;&#25104;&#21644;&#25351;&#20196;&#36319;&#36394;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#25351;&#20196;&#12289;&#25509;&#22320;&#20449;&#24687;&#21644;&#21709;&#24212;&#19977;&#20803;&#32452;&#30340;&#26032;&#30340;&#25628;&#32034;&#22522;&#30784;&#35757;&#32451;&#38598;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#38656;&#35201;&#23398;&#20064;&#36807;&#28388;&#24178;&#25200;&#27573;&#33853;&#24182;&#29983;&#25104;&#30446;&#26631;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#20294;&#20173;&#32570;&#20047;&#36879;&#26126;&#24615;&#21644;&#21033;&#29992;&#26368;&#26032;&#30693;&#35782;&#21644;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25628;&#32034;&#22686;&#24378;&#30340;&#25351;&#20196;&#23398;&#20064;&#65288;SAIL&#65289;&#65292;&#23427;&#20197;&#20869;&#37096;&#21644;&#22806;&#37096;&#25628;&#32034;&#24341;&#25806;&#29983;&#25104;&#30340;&#25628;&#32034;&#32467;&#26524;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#29983;&#25104;&#21644;&#25351;&#20196;&#36319;&#36394;&#33021;&#21147;&#12290;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#30340;&#25628;&#32034;API&#21644;&#39046;&#22495;&#25910;&#38598;&#27599;&#20010;&#35757;&#32451;&#29992;&#20363;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#65288;&#25351;&#20196;&#65292;&#25509;&#22320;&#20449;&#24687;&#65292;&#21709;&#24212;&#65289;&#19977;&#20803;&#32452;&#30340;&#26032;&#30340;&#25628;&#32034;&#22522;&#30784;&#35757;&#32451;&#38598;&#12290;&#28982;&#21518;&#22312;&#26500;&#24314;&#30340;&#35757;&#32451;&#38598;&#19978;&#23545;LLaMA-7B&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#30001;&#20110;&#25910;&#38598;&#30340;&#32467;&#26524;&#21253;&#21547;&#19981;&#30456;&#20851;&#21644;&#20105;&#35758;&#30340;&#35821;&#35328;&#65292;&#27169;&#22411;&#38656;&#35201;&#23398;&#20064;&#22312; &#21487;&#20449;&#36182; &#30340;&#25628;&#32034;&#32467;&#26524;&#19978;&#36827;&#34892;&#25509;&#22320;&#65292;&#36807;&#28388;&#20986;&#24178;&#25200;&#30340;&#27573;&#33853;&#65292;&#24182;&#29983;&#25104;&#30446;&#26631;&#21709;&#24212;&#12290;&#25628;&#32034;&#32467;&#26524;&#21435;&#22122;&#36807;&#31243;&#38656;&#35201;&#26174;&#24335;&#30340;&#12289;&#21487;&#20449;&#20219;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy inform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#26469;&#35299;&#20915;&#30001;&#21512;&#20316;&#21338;&#24328;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25913;&#21892;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13599</link><description>&lt;p&gt;
&#19981;&#23545;&#31216;&#23398;&#20064;&#29575;&#30340;&#20998;&#31163;&#24335;&#29702;&#24615;&#21270;: &#19968;&#31181;&#28789;&#27963;&#30340;Lipschitz&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipshitz Restraint. (arXiv:2305.13599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#26469;&#35299;&#20915;&#30001;&#21512;&#20316;&#21338;&#24328;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25913;&#21892;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#33258;&#35828;&#26126;&#29702;&#24615;&#21270;&#27169;&#22411;&#36890;&#36807;&#21512;&#20316;&#21338;&#24328;&#26500;&#24314;&#65292;&#20854;&#20013;&#29983;&#25104;&#22120;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#36873;&#25321;&#26368;&#26131;&#29702;&#35299;&#30340;&#37096;&#20998;&#20316;&#20026;&#21407;&#29702;&#65292;&#25509;&#30528;&#39044;&#27979;&#22120;&#22522;&#20110;&#25152;&#36873;&#25321;&#30340;&#21407;&#29702;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21512;&#20316;&#21338;&#24328;&#21487;&#33021;&#20250;&#24341;&#21457;&#36864;&#21270;&#38382;&#39064;&#65292;&#39044;&#27979;&#22120;&#36807;&#24230;&#25311;&#21512;&#20110;&#30001;&#23578;&#26410;&#35757;&#32451;&#22909;&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#20449;&#24687;&#19981;&#36275;&#30340;&#37096;&#20998;&#65292;&#21453;&#36807;&#26469;&#23548;&#33268;&#29983;&#25104;&#22120;&#25910;&#25947;&#20110;&#36235;&#21521;&#20110;&#36873;&#25321;&#26080;&#24847;&#20041;&#30340;&#37096;&#20998;&#30340;&#27425;&#20248;&#27169;&#22411;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#23558;&#36864;&#21270;&#38382;&#39064;&#19982;&#39044;&#27979;&#22120;&#30340;Lipschitz&#36830;&#32493;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#28982;&#12289;&#28789;&#27963;&#22320;&#32422;&#26463;&#39044;&#27979;&#22120;&#30340;Lipschitz&#24120;&#25968;&#65292;&#24182;&#35299;&#20915;&#20102;&#36864;&#21270;&#38382;&#39064;&#12290;DR&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#23558;&#29983;&#25104;&#22120;&#21644;&#39044;&#27979;&#22120;&#20998;&#31163;&#65292;&#20026;&#23427;&#20204;&#20998;&#37197;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;DR&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A self-explaining rationalization model is generally constructed by a cooperative game where a generator selects the most human-intelligible pieces from the input text as rationales, followed by a predictor that makes predictions based on the selected rationales. However, such a cooperative game may incur the degeneration problem where the predictor overfits to the uninformative pieces generated by a not yet well-trained generator and in turn, leads the generator to converge to a sub-optimal model that tends to select senseless pieces. In this paper, we theoretically bridge degeneration with the predictor's Lipschitz continuity. Then, we empirically propose a simple but effective method named DR, which can naturally and flexibly restrain the Lipschitz constant of the predictor, to address the problem of degeneration. The main idea of DR is to decouple the generator and predictor to allocate them with asymmetric learning rates. A series of experiments conducted on two widely used benchm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20559;&#32622;&#25439;&#22833;&#20197;&#24110;&#21161;&#35757;&#32451;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#31181;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;WER&#38477;&#20302;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#27169;&#22411;&#30456;&#23545;WER&#25552;&#39640;&#20102;12.1&#65285;&#65292;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;WER&#30456;&#23545;&#38477;&#20302;&#20102;40.5&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.12493</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#30340;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network. (arXiv:2305.12493v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20559;&#32622;&#25439;&#22833;&#20197;&#24110;&#21161;&#35757;&#32451;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#31181;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;WER&#38477;&#20302;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#27169;&#22411;&#30456;&#23545;WER&#25552;&#39640;&#20102;12.1&#65285;&#65292;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;WER&#30456;&#23545;&#38477;&#20302;&#20102;40.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#20449;&#24687;&#22312;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23558;&#20854;&#34701;&#20837;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#32570;&#20047;&#20559;&#32622;&#20219;&#21153;&#30340;&#26174;&#24335;&#30417;&#30563;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#19978;&#19979;&#25991;&#23884;&#20837;&#39044;&#27979;&#21457;&#38899;&#20013;&#30340;&#19978;&#19979;&#25991;&#30701;&#35821;&#65292;&#24182;&#35745;&#31639;&#20559;&#32622;&#25439;&#22833;&#20197;&#24110;&#21161;&#35757;&#32451;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#31181;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;(WER)&#38477;&#20302;&#12290;&#23545;LibriSpeech&#35821;&#26009;&#24211;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22522;&#32447;&#27169;&#22411;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;WER&#25552;&#39640;&#20102;12.1&#65285;&#65292;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;WER&#30456;&#23545;&#38477;&#20302;&#20102;40.5&#65285;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24212;&#29992;&#19978;&#19979;&#25991;&#30701;&#35821;&#36807;&#28388;&#31574;&#30053;&#65292;&#25105;&#20204;&#36824;&#26377;&#25928;&#28040;&#38500;&#20102;&#20351;&#29992;&#26356;&#22823;&#30340;&#20559;&#32622;&#21015;&#34920;&#26102;&#30340;WER&#38477;&#32423;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual information plays a crucial role in speech recognition technologies and incorporating it into the end-to-end speech recognition models has drawn immense interest recently. However, previous deep bias methods lacked explicit supervision for bias tasks. In this study, we introduce a contextual phrase prediction network for an attention-based deep bias method. This network predicts context phrases in utterances using contextual embeddings and calculates bias loss to assist in the training of the contextualized model. Our method achieved a significant word error rate (WER) reduction across various end-to-end speech recognition models. Experiments on the LibriSpeech corpus show that our proposed model obtains a 12.1% relative WER improvement over the baseline model, and the WER of the context phrases decreases relatively by 40.5%. Moreover, by applying a context phrase filtering strategy, we also effectively eliminate the WER degradation when using a larger biasing list.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoS&#65292;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2305.10276</link><description>&lt;p&gt;
&#36830;&#38145;&#31526;&#21495;&#25552;&#31034;&#28608;&#21457;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models. (arXiv:2305.10276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoS&#65292;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#27169;&#25311;&#30340;&#34394;&#25311;&#31354;&#38388;&#29615;&#22659;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#23427;&#30001;&#19968;&#32452;&#26032;&#39062;&#30340;&#20219;&#21153;&#32452;&#25104;&#65306;Brick World&#12289;&#22522;&#20110;NLVR&#30340;&#25805;&#20316;&#21644;&#33258;&#28982;&#35821;&#35328;&#23548;&#33322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27969;&#34892;&#30340;LLMs&#65288;&#22914;ChatGPT&#65289;&#20173;&#28982;&#32570;&#20047;&#22797;&#26434;&#35268;&#21010;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#8212;&#8212;LLMs&#26159;&#21542;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#29615;&#22659;&#26377;&#33391;&#22909;&#30340;&#29702;&#35299;&#65292;&#25110;&#32773;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#65288;&#22914;&#31526;&#21495;&#34920;&#31034;&#65289;&#26159;&#21542;&#26356;&#21152;&#31616;&#21333;&#65292;&#22240;&#27492;&#26356;&#23481;&#26131;&#34987;LLMs&#29702;&#35299;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoS&#65288;Chain-of-Symbol Prompting&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#38142;&#24335;&#20013;&#38388;&#24605;&#32771;&#27493;&#39588;&#20013;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;CoS&#26131;&#20110;&#20351;&#29992;&#65292;&#19981;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#39069;&#22806;&#30340;&#22521;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we take the initiative to investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act correspondingly in text. We propose a benchmark named Natural Language Planning (NLP) composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation. We found that current popular LLMs such as ChatGPT still lack abilities in complex planning. This arises a question -- do the LLMs have a good understanding of the environments described in natural language, or maybe other alternatives such as symbolic representations are neater and hence better to be understood by LLMs? To this end, we propose a novel method called CoS (Chain-of-Symbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps. CoS is easy to use and does not need additional training on LLMs. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21019;&#36896;&#24615;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#20197;&#35799;&#27468;&#20026;&#37325;&#28857;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;&#28145;&#20837;&#25506;&#35752;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2305.08493</link><description>&lt;p&gt;
&#21019;&#36896;&#24615;&#25968;&#25454;&#29983;&#25104;&#65306;&#20197;&#25991;&#26412;&#21644;&#35799;&#27468;&#20026;&#37325;&#28857;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Creative Data Generation: A Review Focusing on Text and Poetry. (arXiv:2305.08493v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21019;&#36896;&#24615;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#20197;&#35799;&#27468;&#20026;&#37325;&#28857;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;&#28145;&#20837;&#25506;&#35752;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#33258;&#21160;&#25968;&#25454;&#29983;&#25104;&#30340;&#28608;&#22686;&#65292;&#20351;&#24471;&#21306;&#20998;&#33258;&#28982;&#25110;&#20154;&#31867;&#29983;&#25104;&#30340;&#25968;&#25454;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#36234;&#21457;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#21019;&#36896;&#24615;&#25968;&#25454;&#29983;&#25104;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#21644;&#29702;&#35299;&#21019;&#36896;&#21147;&#30340;&#26412;&#36136;&#65292;&#26082;&#21253;&#25324;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#21019;&#36896;&#21147;&#65292;&#20063;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#20869;&#30340;&#21019;&#36896;&#21147;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#21508;&#31181;&#21019;&#36896;&#24615;&#20889;&#20316;&#35774;&#22791;&#21644;&#20219;&#21153;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#35799;&#27468;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#26088;&#22312;&#25581;&#31034;&#21019;&#36896;&#24615;&#25968;&#25454;&#29983;&#25104;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement in machine learning has led to a surge in automatic data generation, making it increasingly challenging to differentiate between naturally or human-generated data and machine-generated data. Despite these advancements, the generation of creative data remains a challenge. This paper aims to investigate and comprehend the essence of creativity, both in general and within the context of natural language generation. We review various approaches to creative writing devices and tasks, with a specific focus on the generation of poetry. We aim to shed light on the challenges and opportunities in the field of creative data generation.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#36328;&#35821;&#35328;ICL&#20013;&#26080;&#27861;&#23545;&#20934;&#36755;&#20837;&#36755;&#20986;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26500;&#24314;&#31574;&#30053;X-InSTA&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#35821;&#22659;&#36827;&#34892;&#32534;&#30721;&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#36328;&#35821;&#35328;ICL&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.05940</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;LLMs&#26159;&#26356;&#22909;&#30340;&#36328;&#35821;&#35328;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#19982;&#23545;&#40784;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment. (arXiv:2305.05940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05940
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#36328;&#35821;&#35328;ICL&#20013;&#26080;&#27861;&#23545;&#20934;&#36755;&#20837;&#36755;&#20986;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26500;&#24314;&#31574;&#30053;X-InSTA&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#35821;&#22659;&#36827;&#34892;&#32534;&#30721;&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#36328;&#35821;&#35328;ICL&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#20219;&#20309;&#26799;&#24230;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#20197;&#23569;&#25968;&#26631;&#35760;&#26679;&#26412;&#20026;&#26465;&#20214;&#30340;&#27979;&#35797;&#26631;&#31614;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#25104;&#20026;&#21487;&#33021;&#12290;&#21551;&#29992;ICL&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35268;&#36991;&#22797;&#21457;&#24615;&#27880;&#37322;&#25104;&#26412;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#21069;&#36827;&#27493;&#20240;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#21482;&#26377;&#23569;&#25968;&#20960;&#39033;&#30740;&#31350;&#25506;&#31350;&#20102;&#36328;&#35821;&#35328;&#35774;&#32622;&#19979;&#30340;ICL&#65292;&#36825;&#22312;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#36716;&#31227;&#26631;&#31614;&#30693;&#35782;&#30340;&#38656;&#35201;&#19979;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#36328;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#30340;ICL&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36328;&#35821;&#35328;ICL&#30340;&#24773;&#20917;&#19979;&#65292;&#26222;&#36941;&#36873;&#25321;&#38543;&#26426;&#30340;&#36755;&#20837;-&#26631;&#31614;&#23545;&#26469;&#26500;&#24314;&#25552;&#31034;&#19978;&#19979;&#25991;&#30340;&#27169;&#24335;&#20005;&#37325;&#21463;&#38480;&#20110;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#30340;&#32570;&#20047;&#23545;&#20934;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26500;&#24314;&#31574;&#30053;&#8212;&#8212;&#36328;&#35821;&#35328;&#19978;&#19979;&#25991;&#28304;-&#30446;&#26631;&#23545;&#40784;&#65288;X-InSTA&#65289;&#12290;&#36890;&#36807;&#27880;&#20837;&#20849;&#21516;&#35757;&#32451;&#30340;&#38408;&#20540;&#20803;&#32032;&#65292;X-InSTA&#21487;&#20197;&#21516;&#26102;&#23545;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#35821;&#22659;&#36827;&#34892;&#32534;&#30721;&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#36328;&#35821;&#35328;ICL&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of ICL for cross-lingual text classification. We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy -- Cross-lingual In-context Source-Target Alignment (X-InSTA). With an injected co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.05352</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25512;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#34987;&#24191;&#27867;&#35748;&#20026;&#23558;&#25104;&#20026;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#22320;&#25506;&#32034;&#12290;&#20154;&#20204;&#23545;&#22312;&#36719;&#20214;&#26550;&#26500;&#20013;&#24341;&#20837;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#27861;&#65292;&#23545;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#21253;&#25324;&#19977;&#20010;&#31867;&#21035;&#65306;&#22522;&#30784;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12289;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#36127;&#36131;&#20219;&#30340;AI&#35774;&#35745;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#36130;&#21153;&#25512;&#29702;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#23545;&#20219;&#21153;&#21046;&#23450;&#12289;&#25968;&#25454;&#29983;&#25104;&#12289;&#25552;&#31034;&#26041;&#27861;&#21644;&#35780;&#20272;&#33021;&#21147;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#26368;&#32456;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#23545;&#21442;&#25968;&#35268;&#27169;&#20026;2.8B&#33267;13B&#30340;&#21508;&#31181;GPT&#21464;&#20307;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.01505</link><description>&lt;p&gt;
&#36229;&#36234;&#20998;&#31867;&#65306;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36130;&#21153;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Beyond Classification: Financial Reasoning in State-of-the-Art Language Models. (arXiv:2305.01505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#36130;&#21153;&#25512;&#29702;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#23545;&#20219;&#21153;&#21046;&#23450;&#12289;&#25968;&#25454;&#29983;&#25104;&#12289;&#25552;&#31034;&#26041;&#27861;&#21644;&#35780;&#20272;&#33021;&#21147;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#26368;&#32456;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#23545;&#21442;&#25968;&#35268;&#27169;&#20026;2.8B&#33267;13B&#30340;&#21508;&#31181;GPT&#21464;&#20307;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;1000&#20159;&#21450;&#20197;&#19978;&#30340;&#21442;&#25968;&#32452;&#25104;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36890;&#29992;&#30340;&#36827;&#23637;&#24212;&#29992;&#22312;&#24456;&#23569;&#39046;&#22495;&#20013;&#65292;&#20363;&#22914;&#20020;&#24202;&#25110;&#27861;&#24459;&#39046;&#22495;&#65292;&#32780;&#36130;&#21153;&#25512;&#29702;&#39046;&#22495;&#22522;&#26412;&#19978;&#26410;&#34987;&#25506;&#32034;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;LLMs&#35299;&#20915;&#36130;&#21153;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#20174;&#26410;&#34987;&#30740;&#31350;&#36807;&#65292;&#24182;&#19988;&#23427;&#26159;&#21542;&#21487;&#20197;&#22312;&#20219;&#20309;&#35268;&#27169;&#19978;&#23436;&#25104;&#20173;&#26410;&#30693;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#23545;LLMs&#22312;&#36130;&#21153;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#35843;&#26597;&#21253;&#25324;&#23545;&#19968;&#31995;&#21015;&#20027;&#39064;&#30340;&#35814;&#32454;&#25506;&#35752;&#65292;&#21253;&#25324;&#20219;&#21153;&#21046;&#23450;&#65292;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#25552;&#31034;&#26041;&#27861;&#21644;&#35780;&#20272;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#23545;&#21442;&#25968;&#35268;&#27169;&#20026;2.8B&#33267;13B&#30340;&#21508;&#31181;GPT&#21464;&#20307;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#26377;&#26080;&#25351;&#23548;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), consisting of 100 billion or more parameters, have demonstrated remarkable ability in complex multi-step reasoning tasks. However, the application of such generic advancements has been limited to a few fields, such as clinical or legal, with the field of financial reasoning remaining largely unexplored. To the best of our knowledge, the ability of LLMs to solve financial reasoning problems has never been dealt with, and whether it can be performed at any scale remains unknown. To address this knowledge gap, this research presents a comprehensive investigation into the potential application of LLMs in the financial domain. The investigation includes a detailed exploration of a range of subjects, including task formulation, synthetic data generation, prompting methods, and evaluation capability. Furthermore, the study benchmarks various GPT variants with parameter scales ranging from 2.8B to 13B, with and without instruction tuning, on diverse dataset sizes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SearChain&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;SearChain&#36890;&#36807;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#23454;&#29616;&#65292;&#20854;&#24605;&#36335;&#26159;&#36890;&#36807;&#26500;&#36896;&#26597;&#35810;&#38142;&#65292;&#23558;&#22810;&#36339;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#65292;&#26368;&#32456;&#25351;&#23548;LLM&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.14732</link><description>&lt;p&gt;
&#22522;&#20110;SearChain&#30340;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#31934;&#30830;&#12289;&#21487;&#20449;&#21644;&#21487;&#36861;&#28335;&#20869;&#23481;&#29983;&#25104;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks. (arXiv:2304.14732v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SearChain&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;SearChain&#36890;&#36807;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#23454;&#29616;&#65292;&#20854;&#24605;&#36335;&#26159;&#36890;&#36807;&#26500;&#36896;&#26597;&#35810;&#38142;&#65292;&#23558;&#22810;&#36339;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#65292;&#26368;&#32456;&#25351;&#23548;LLM&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22914;&#20309;&#20351;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#20934;&#30830;&#21487;&#20449;&#22312;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Search-in-the-Chain&#65288;SearChain&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#31561;&#20856;&#22411;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;SearChain&#26159;&#19968;&#20010;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#26694;&#26550;&#12290;&#22312;SearChain&#20013;&#65292;LLM&#26500;&#24314;&#26597;&#35810;&#38142;&#65292;&#20316;&#20026;&#22810;&#36339;&#38382;&#39064;&#30340;&#20998;&#35299;&#12290;&#38142;&#30340;&#27599;&#20010;&#33410;&#28857;&#37117;&#26159;&#30001;IR&#23548;&#21521;&#30340;&#26597;&#35810;-&#31572;&#26696;&#23545;&#65292;&#20197;&#21450;&#30001;LLM&#29983;&#25104;&#30340;&#35813;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;IR&#39564;&#35777;&#12289;&#23436;&#21892;&#21644;&#36319;&#36394;&#38142;&#20013;&#27599;&#20010;&#33410;&#28857;&#30340;&#20449;&#24687;&#65292;&#20197;&#25351;&#23548;LLM&#26500;&#24314;&#27491;&#30830;&#30340;&#26597;&#35810;&#38142;&#65292;&#24182;&#26368;&#32456;&#22238;&#31572;&#22810;&#36339;&#38382;&#39064;&#12290;SearChain&#20351;LLM&#20174;&#19968;&#27425;&#24615;&#31572;&#26696;&#36716;&#21464;&#20026;&#22810;&#27493;&#31572;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SearChain&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the wide application of Large Language Models (LLMs) such as ChatGPT, how to make the contents generated by LLM accurate and credible becomes very important, especially in complex knowledge-intensive tasks. In this paper, we propose a novel framework called Search-in-the-Chain (SearChain) to improve the accuracy, credibility and traceability of LLM-generated content for multi-hop question answering, which is a typical complex knowledge-intensive task. SearChain is a framework that deeply integrates LLM and information retrieval (IR). In SearChain, LLM constructs a chain-of-query, which is the decomposition of the multi-hop question. Each node of the chain is a query-answer pair consisting of an IR-oriented query and the answer generated by LLM for this query. IR verifies, completes, and traces the information of each node of the chain, so as to guide LLM to construct the correct chain-of-query, and finally answer the multi-hop question. SearChain makes LLM change from trying to gi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;MLA&#65289;&#20197;&#26368;&#22823;&#21270;&#25512;&#29702;&#36136;&#37327;&#24182;&#23454;&#29616;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#38899;&#21040;&#25991;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;S2T NMT&#65289;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#21516;&#22768;&#20256;&#35793;&#65288;SimulST&#65289;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#39057;&#35889;&#22270;&#20219;&#21153;&#65292;&#22312;TTS&#19978;&#20855;&#26377;&#39640;&#25928;&#29575;&#25910;&#30410;&#65292;&#22312;NMT&#21644;SimulST&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08453</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#25913;&#36827;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Improving Autoregressive NLP Tasks via Modular Linearized Attention. (arXiv:2304.08453v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;MLA&#65289;&#20197;&#26368;&#22823;&#21270;&#25512;&#29702;&#36136;&#37327;&#24182;&#23454;&#29616;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#38899;&#21040;&#25991;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;S2T NMT&#65289;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#21516;&#22768;&#20256;&#35793;&#65288;SimulST&#65289;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#39057;&#35889;&#22270;&#20219;&#21153;&#65292;&#22312;TTS&#19978;&#20855;&#26377;&#39640;&#25928;&#29575;&#25910;&#30410;&#65292;&#22312;NMT&#21644;SimulST&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38656;&#35201;&#30340;&#27169;&#22411;&#24517;&#39035;&#22312;&#26368;&#32456;&#24212;&#29992;&#20110;&#36793;&#32536;&#25110;&#20854;&#20182;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#23567;&#22411;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#23558;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#23567;&#20943;&#23567;&#65292;&#20294;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#20173;&#28982;&#24456;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#33258;&#22238;&#24402;&#20219;&#21153;&#32780;&#35328;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;MLA&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#20010;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21253;&#25324;cosFormer&#65292;&#20197;&#26368;&#22823;&#21270;&#25512;&#29702;&#36136;&#37327;&#24182;&#23454;&#29616;&#26174;&#30528;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#38899;&#21040;&#25991;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;S2T NMT&#65289;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#21516;&#22768;&#20256;&#35793;&#65288;SimulST&#65289;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#39057;&#35889;&#22270;&#20219;&#21153;&#65292;&#22312;TTS&#19978;&#20855;&#26377;&#39640;&#25928;&#29575;&#25910;&#30410;&#65292;&#22312;NMT&#21644;SimulST&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various natural language processing (NLP) tasks necessitate models that are efficient and small based on their ultimate application at the edge or in other resource-constrained environments. While prior research has reduced the size of these models, increasing computational efficiency without considerable performance impacts remains difficult, especially for autoregressive tasks. This paper proposes {modular linearized attention (MLA), which combines multiple efficient attention mechanisms, including cosFormer, to maximize inference quality while achieving notable speedups. We validate this approach on several autoregressive NLP tasks, including speech-to-text neural machine translation (S2T NMT), speech-to-text simultaneous translation (SimulST), and autoregressive text-to-spectrogram, noting efficiency gains on TTS and competitive performance for NMT and SimulST during training and inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29983;&#25104;&#31526;&#21495;&#65292;&#23454;&#29616;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#12290;&#36825;&#20123;&#31526;&#21495;&#21487;&#20197;&#25429;&#25417;&#21040;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21576;&#29616;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.06377</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#31526;&#21495;&#30340;&#20986;&#29616;&#19982;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29983;&#25104;&#31526;&#21495;&#65292;&#23454;&#29616;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#12290;&#36825;&#20123;&#31526;&#21495;&#21487;&#20197;&#25429;&#25417;&#21040;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21576;&#29616;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#21019;&#36896;&#26377;&#24847;&#20041;&#30340;&#31526;&#21495;&#65292;&#24182;&#29087;&#32451;&#22320;&#23558;&#23427;&#20204;&#29992;&#20110;&#26356;&#39640;&#30340;&#35748;&#30693;&#21151;&#33021;&#65292;&#22914;&#20132;&#27969;&#12289;&#25512;&#29702;&#12289;&#35268;&#21010;&#31561;&#65292;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#37325;&#35201;&#21644;&#29420;&#29305;&#20043;&#22788;&#12290; &#30446;&#21069;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20173;&#36828;&#36828;&#33853;&#21518;&#20110;&#20154;&#31867;&#21019;&#36896;&#31526;&#21495;&#36827;&#34892;&#36825;&#20123;&#39640;&#32423;&#35748;&#30693;&#21151;&#33021;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#31526;&#21495;&#21019;&#36896;&#12289;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#33021;&#21147;&#12290;SEA-net&#29983;&#25104;&#21160;&#24577;&#37197;&#32622;&#32593;&#32476;&#20197;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#31526;&#21495;&#12290;&#36825;&#20123;&#31526;&#21495;&#25429;&#25417;&#20102;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#36890;&#36807;&#32431;&#31526;&#21495;&#25805;&#20316;&#25110;&#20132;&#27969;&#33719;&#24471;&#26032;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#33258;&#21160;&#29983;&#25104;&#30340;&#31526;&#21495;&#21576;&#29616;&#20986;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#34920;&#26126;&#22312;&#20154;&#31867;&#22823;&#33041;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#29983;&#25104;&#21644;&#29702;&#35299;&#31526;&#21495;&#30340;&#20849;&#21516;&#26694;&#26550;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#23558;&#25104;&#20026;&#23558;&#26469;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#30340;&#21161;&#25512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to create meaningful symbols and proficiently use them for higher cognitive functions such as communication, reasoning, planning, etc., is essential and unique for human intelligence. Current deep neural networks are still far behind human's ability to create symbols for such higher cognitive functions. Here we propose a solution, named SEA-net, to endow neural networks with ability of symbol creation, semantic understanding and communication. SEA-net generates symbols that dynamically configure the network to perform specific tasks. These symbols capture compositional semantic information that enables the system to acquire new functions purely by symbolic manipulation or communication. In addition, we found that these self-generated symbols exhibit an intrinsic structure resembling that of natural language, suggesting a common framework underlying the generation and understanding of symbols in both human brains and artificial neural networks. We hope that it will be instrum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01295</link><description>&lt;p&gt;
&#26377;&#25928;&#22320;&#23545;&#40784;&#36328;&#35821;&#35328;&#20250;&#35805;&#20219;&#21153;&#30340;&#25552;&#31034;&#35843;&#25972;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#23545;&#20110;&#20250;&#35805;&#20219;&#21153;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;XSGD&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;Schema-Guided Dialogue&#65288;SGD&#65289;&#32763;&#35793;&#25104;105&#31181;&#20854;&#20182;&#35821;&#35328;&#30340;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#27979;&#35797;&#20102;&#23545;&#40784;&#25552;&#31034;&#25152;&#23454;&#29616;&#30340;&#36328;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23545;&#35805;&#20219;&#21153;&#65288;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#65289;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce XSGD, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains approximately 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model's cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent cla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#39046;&#22495;&#21033;&#29992;LLaMA&#27169;&#22411;&#24494;&#35843;&#30340;&#21307;&#30103;&#32842;&#22825;&#27169;&#22411;ChatDoctor&#12290;&#32463;&#36807;700&#22810;&#31181;&#30142;&#30149;&#21644;&#20854;&#30456;&#24212;&#30151;&#29366;&#12289;&#33647;&#21697;&#21644;&#21307;&#30103;&#26816;&#26597;&#30340;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#29702;&#35299;&#24739;&#32773;&#38656;&#27714;&#12289;&#25552;&#20379;&#24314;&#35758;&#21644;&#24110;&#21161;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#30340;&#27807;&#36890;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.14070</link><description>&lt;p&gt;
ChatDoctor&#65306;&#20351;&#29992;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#22312;LLaMA&#27169;&#22411;&#19978;&#24494;&#35843;&#30340;&#21307;&#30103;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. (arXiv:2303.14070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#39046;&#22495;&#21033;&#29992;LLaMA&#27169;&#22411;&#24494;&#35843;&#30340;&#21307;&#30103;&#32842;&#22825;&#27169;&#22411;ChatDoctor&#12290;&#32463;&#36807;700&#22810;&#31181;&#30142;&#30149;&#21644;&#20854;&#30456;&#24212;&#30151;&#29366;&#12289;&#33647;&#21697;&#21644;&#21307;&#30103;&#26816;&#26597;&#30340;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#29702;&#35299;&#24739;&#32773;&#38656;&#27714;&#12289;&#25552;&#20379;&#24314;&#35758;&#21644;&#24110;&#21161;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#30340;&#27807;&#36890;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#19968;&#33324;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20363;&#22914;ChatGPT&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#20223;&#20315;&#26159;&#20154;&#31867;&#35762;&#35805;&#33324;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#24182;&#27809;&#26377;&#32463;&#36807;&#20010;&#21035;&#19988;&#20180;&#32454;&#20026;&#21307;&#23398;&#39046;&#22495;&#23398;&#20064;&#65292;&#23548;&#33268;&#35786;&#26029;&#20934;&#30830;&#24230;&#20302;&#19988;&#19981;&#33021;&#32473;&#20986;&#27491;&#30830;&#30340;&#21307;&#30103;&#35786;&#26029;&#12289;&#33647;&#21697;&#31561;&#24314;&#35758;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;700&#22810;&#31181;&#30142;&#30149;&#21450;&#20854;&#30456;&#24212;&#30151;&#29366;&#12289;&#25512;&#33616;&#33647;&#21697;&#21644;&#25152;&#38656;&#21307;&#30103;&#26816;&#26597;&#65292;&#28982;&#21518;&#29983;&#25104;&#20102;5K&#21517;&#21307;&#24739;&#30340;&#23545;&#35805;&#12290;&#36890;&#36807;&#24494;&#35843;&#21307;&#24739;&#23545;&#35805;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20102;&#29702;&#35299;&#24739;&#32773;&#38656;&#27714;&#12289;&#25552;&#20379;&#26126;&#26234;&#24314;&#35758;&#24182;&#22312;&#21508;&#31181;&#21307;&#30103;&#30456;&#20851;&#39046;&#22495;&#25552;&#20379;&#23453;&#36149;&#24110;&#21161;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#23558;&#36825;&#20123;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21487;&#20197;&#24443;&#24213;&#25913;&#21464;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#30340;&#27807;&#36890;&#26041;&#24335;&#65292;&#26368;&#32456;&#25913;&#21892;&#25972;&#20307;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) in the general domain, such as ChatGPT, have shown remarkable success in following instructions and producing human-like responses. However, such language models have not been learned individually and carefully for the medical domain, resulting in poor diagnostic accuracy and inability to give correct recommendations for medical diagnosis, medications, etc. To address this issue, we collected more than 700 diseases and their corresponding symptoms, recommended medications, and required medical tests, and then generated 5K doctor-patient conversations. By fine-tuning models of doctor-patient conversations, these models emerge with great potential to understand patients' needs, provide informed advice, and offer valuable assistance in a variety of medical-related fields. The integration of these advanced language models into healthcare can revolutionize the way healthcare professionals and patients communicate, ultimately improving the overall quality 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12816</link><description>&lt;p&gt;
&#20174;&#23485;&#21040;&#28145;&#65306;&#32500;&#24230;&#25552;&#21319;&#32593;&#32476;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#21521;&#37327;&#34920;&#31034;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;KGE&#26041;&#27861;&#38656;&#35201;&#30456;&#23545;&#39640;&#32500;&#30340;&#23454;&#20307;&#34920;&#31034;&#26469;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#20250;&#23548;&#33268;&#24222;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#26469;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#65292;&#21516;&#26102;&#24320;&#21457;&#25216;&#26415;&#65288;&#20363;&#22914;&#30693;&#35782;&#33976;&#39311;&#65289;&#26469;&#34917;&#20607;&#38477;&#32500;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25805;&#20316;&#20250;&#23548;&#33268;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#23454;&#20307;&#34920;&#31034;&#30340;&#32423;&#32852;&#35270;&#20026;&#23884;&#20837;&#23618;&#65292;&#37027;&#20040;&#37319;&#29992;&#39640;&#32500;&#23454;&#20307;&#34920;&#31034;&#30340;&#20256;&#32479;KGE&#26041;&#27861;&#31561;&#21516;&#20110;&#25193;&#23637;&#23884;&#20837;&#23618;&#30340;&#23485;&#24230;&#20197;&#33719;&#24471;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#25928;&#29575;&#65292;&#25105;&#20204;&#30456;&#21453;&#22320;&#22686;&#21152;&#28145;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#28145;&#30340;&#23454;&#20307;&#23884;&#20837;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;RNN seq2seq&#27169;&#22411;&#22312;&#23398;&#20064;&#22235;&#31181;&#36716;&#25442;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#21482;&#33021;&#36924;&#36817;&#31526;&#21512;&#35757;&#32451;&#25110;&#20998;&#24067;&#20869;&#25968;&#25454;&#30340;&#26144;&#23556;&#65292;&#19981;&#33021;&#23398;&#20064;&#24213;&#23618;&#20989;&#25968;&#65307;&#25991;&#31456;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#65292;&#29992;&#20110;&#26080;&#27880;&#24847;&#21147;RNN seq2seq&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#23383;&#31526;&#20018;&#36716;&#25442;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.06841</link><description>&lt;p&gt;
&#20351;&#29992;RNN&#27169;&#22411;&#23398;&#20064;&#36716;&#25442;&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Learning Transductions and Alignments with RNN Seq2seq Models. (arXiv:2303.06841v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;RNN seq2seq&#27169;&#22411;&#22312;&#23398;&#20064;&#22235;&#31181;&#36716;&#25442;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#21482;&#33021;&#36924;&#36817;&#31526;&#21512;&#35757;&#32451;&#25110;&#20998;&#24067;&#20869;&#25968;&#25454;&#30340;&#26144;&#23556;&#65292;&#19981;&#33021;&#23398;&#20064;&#24213;&#23618;&#20989;&#25968;&#65307;&#25991;&#31456;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#65292;&#29992;&#20110;&#26080;&#27880;&#24847;&#21147;RNN seq2seq&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#23383;&#31526;&#20018;&#36716;&#25442;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#24207;&#21015;&#21040;&#24207;&#21015;(RNN seq2seq)&#27169;&#22411;&#22312;&#23398;&#20064;&#22235;&#31181;&#36716;&#25442;&#20219;&#21153;&#65306;&#24658;&#31561;&#12289;&#21453;&#36716;&#12289;&#23436;&#20840;&#37325;&#22797;&#21644;&#20108;&#27425;&#22797;&#21046;&#12290;&#36825;&#20123;&#36716;&#25442;&#22312;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#19979;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#20855;&#26377;&#36880;&#28176;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;RNN seq2seq&#27169;&#22411;&#21482;&#33021;&#36924;&#36817;&#31526;&#21512;&#35757;&#32451;&#25110;&#20998;&#24067;&#20869;&#25968;&#25454;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#33021;&#23398;&#20064;&#24213;&#23618;&#20989;&#25968;&#12290;&#23613;&#31649;&#27880;&#24847;&#21147;&#26426;&#21046;&#20351;&#23398;&#20064;&#26356;&#21152;&#39640;&#25928;&#21644;&#40065;&#26834;&#65292;&#20294;&#23427;&#24182;&#19981;&#33021;&#20811;&#26381;&#20998;&#24067;&#22806;&#30340;&#27867;&#21270;&#38480;&#21046;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#26469;&#23398;&#20064;&#36825;&#22235;&#20010;&#20219;&#21153;&#30340;&#26080;&#27880;&#24847;&#21147;RNN seq2seq&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#29992;&#27491;&#24335;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#26469;&#35299;&#37322;&#65292;&#32780;&#19981;&#26159;&#23383;&#31526;&#20018;&#36716;&#25442;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#12290;RNN&#30340;&#21464;&#31181;&#20063;&#22312;&#32467;&#26524;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#31616;&#21333;&#30340;RNN seq2seq&#27169;&#22411;&#26080;&#27861;&#35745;&#31639;&#36755;&#20837;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper studies the capabilities of Recurrent-Neural-Network sequence to sequence (RNN seq2seq) models in learning four transduction tasks: identity, reversal, total reduplication, and quadratic copying. These transductions are traditionally well studied under finite state transducers and attributed with increasing complexity. We find that RNN seq2seq models are only able to approximate a mapping that fits the training or in-distribution data, instead of learning the underlying functions. Although attention makes learning more efficient and robust, it does not overcome the out-of-distribution generalization limitation. We establish a novel complexity hierarchy for learning the four tasks for attention-less RNN seq2seq models, which may be understood in terms of the complexity hierarchy of formal languages, instead of string transductions. RNN variants also play a role in the results. In particular, we show that Simple RNN seq2seq models cannot count the input length.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;SpikeGPT&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20108;&#36827;&#21046;&#12289;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#28608;&#27963;&#21333;&#20803;&#36827;&#34892;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;SNN&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#24615;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.13939</link><description>&lt;p&gt;
SpikeGPT&#65306;&#24102;&#26377;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. (arXiv:2302.13939v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;SpikeGPT&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20108;&#36827;&#21046;&#12289;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#28608;&#27963;&#21333;&#20803;&#36827;&#34892;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;SNN&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#24615;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#19981;&#26029;&#25193;&#22823;&#65292;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#20063;&#38543;&#20043;&#22686;&#21152;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#31232;&#30095;&#21644;&#20107;&#20214;&#39537;&#21160;&#28608;&#27963;&#20943;&#23569;&#27169;&#22411;&#25512;&#29702;&#35745;&#31639;&#24320;&#38144;&#30340;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#24050;&#32463;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;SNN&#30340;&#35757;&#32451;&#20063;&#34987;&#35777;&#26126;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#33853;&#21518;&#20110;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65292;&#25105;&#20204;&#23578;&#26410;&#30475;&#21040;SNN&#22312;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;Receptance Weighted Key Value&#65288;RWKV&#65289;&#35821;&#35328;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#8220;SpikeGPT&#8221;&#65292;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#20108;&#36827;&#21046;&#12289;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#28608;&#27963;&#21333;&#20803;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#27169;&#22411;&#21464;&#20307;&#19978;&#35757;&#32451;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65306;45M&#21644;216M&#21442;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SpikeGPT&#26159;&#36804;&#20170;&#26368;&#22823;&#30340;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;SNN&#27169;&#22411;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#38750;&#33033;&#20914;&#27169;&#22411;&#36890;&#24120;&#35299;&#20915;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suita
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SQE&#30340;&#24207;&#21015;&#26597;&#35810;&#32534;&#30721;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#32534;&#30721;&#20026;&#19968;&#20010;&#24207;&#21015;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#21644;&#24378;&#22823;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2302.13114</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19978;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#30340;&#24207;&#21015;&#26597;&#35810;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Sequential Query Encoding For Complex Query Answering on Knowledge Graphs. (arXiv:2302.13114v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SQE&#30340;&#24207;&#21015;&#26597;&#35810;&#32534;&#30721;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#32534;&#30721;&#20026;&#19968;&#20010;&#24207;&#21015;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#21644;&#24378;&#22823;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#26159;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#37325;&#35201;&#21644;&#22522;&#30784;&#20219;&#21153;&#12290;&#26597;&#35810;&#32534;&#30721;&#34987;&#25552;&#20986;&#20316;&#20026;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#30340;&#24555;&#36895;&#32780;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;QE&#26041;&#27861;&#39318;&#20808;&#23558;&#36923;&#36753;&#26597;&#35810;&#35299;&#26512;&#20026;&#21487;&#25191;&#34892;&#30340;&#35745;&#31639;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#65292;&#28982;&#21518;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25805;&#20316;&#31526;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#26368;&#21518;&#36882;&#24402;&#25191;&#34892;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#21270;&#30340;&#25805;&#20316;&#31526;&#12290;&#28982;&#32780;&#65292;&#21442;&#25968;&#21270;&#21644;&#25191;&#34892;&#33539;&#24335;&#21487;&#33021;&#20250;&#28508;&#22312;&#22320;&#36807;&#20110;&#22797;&#26434;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#36827;&#34892;&#32467;&#26500;&#31616;&#21270;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20687;LSTM&#21644;Transformer&#36825;&#26679;&#30340;&#24207;&#21015;&#32534;&#30721;&#22120;&#34987;&#35777;&#26126;&#23545;&#20110;&#32534;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#35821;&#20041;&#22270;&#38750;&#24120;&#26377;&#25928;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24207;&#21015;&#26597;&#35810;&#32534;&#30721;(SQE)&#20316;&#20026;&#32534;&#30721;CQA&#26597;&#35810;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;SQE&#39318;&#20808;&#20351;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#31639;&#27861;&#23558;&#35745;&#31639;&#22270;&#32447;&#24615;&#21270;&#20026;&#19968;&#31995;&#21015;&#26631;&#35760;&#65292;&#28982;&#21518;&#20351;&#29992;&#24207;&#21015;&#32534;&#30721;&#22120;&#23558;&#36825;&#20123;&#26631;&#35760;&#32534;&#30721;&#20026;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex Query Answering (CQA) is an important and fundamental task for knowledge graph (KG) reasoning. Query encoding (QE) is proposed as a fast and robust solution to CQA. In the encoding process, most existing QE methods first parse the logical query into an executable computational direct-acyclic graph (DAG), then use neural networks to parameterize the operators, and finally, recursively execute these neuralized operators. However, the parameterization-and-execution paradigm may be potentially over-complicated, as it can be structurally simplified by a single neural network encoder. Meanwhile, sequence encoders, like LSTM and Transformer, proved to be effective for encoding semantic graphs in related tasks. Motivated by this, we propose sequential query encoding (SQE) as an alternative to encode queries for CQA. Instead of parameterizing and executing the computational graph, SQE first uses a search-based algorithm to linearize the computational graph to a sequence of tokens and th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#24494;&#35843;&#20219;&#21153;&#22312;&#23545;&#35805;&#20013;&#26500;&#24314;&#35805;&#35821;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;STAC&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.05895</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#20013;&#25552;&#21462;&#35805;&#35821;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models in Dialogues. (arXiv:2302.05895v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05895
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#24494;&#35843;&#20219;&#21153;&#22312;&#23545;&#35805;&#20013;&#26500;&#24314;&#35805;&#35821;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;STAC&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35805;&#35821;&#22788;&#29702;&#22240;&#25968;&#25454;&#31232;&#32570;&#32780;&#21463;&#21040;&#22256;&#25200;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#35805;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#26500;&#24314;&#23545;&#35805;&#30340;&#35805;&#35821;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#24494;&#35843;&#20219;&#21153;&#65292;&#24182;&#34920;&#26126;&#19987;&#38376;&#38024;&#23545;&#23545;&#35805;&#30340;&#21477;&#23376;&#25490;&#24207;&#20219;&#21153;&#34920;&#29616;&#26368;&#20339;&#12290;&#20026;&#20102;&#23450;&#20301;&#21644;&#21033;&#29992;PLMs&#20013;&#30340;&#35805;&#35821;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#22312;STAC&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#26080;&#30417;&#30563;&#26041;&#27861;&#21644;&#21322;&#30417;&#30563;&#26041;&#27861;&#30340;F1&#20998;&#21035;&#20026;57.2&#21644;59.3&#12290;&#24403;&#38480;&#21046;&#22312;&#25237;&#24433;&#26641;&#19978;&#26102;&#65292;&#25105;&#20204;&#30340;&#24471;&#20998;&#25552;&#39640;&#21040;&#20102;63.3&#21644;68.1&#12290;
&lt;/p&gt;
&lt;p&gt;
Discourse processing suffers from data sparsity, especially for dialogues. As a result, we explore approaches to build discourse structures for dialogues, based on attention matrices from Pre-trained Language Models (PLMs). We investigate multiple tasks for fine-tuning and show that the dialogue-tailored Sentence Ordering task performs best. To locate and exploit discourse information in PLMs, we propose an unsupervised and a semi-supervised method. Our proposals achieve encouraging results on the STAC corpus, with F1 scores of 57.2 and 59.3 for unsupervised and semi-supervised methods, respectively. When restricted to projective trees, our scores improved to 63.3 and 68.1.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#24178;&#20928;&#21644;&#24102;&#22122;&#22768;&#35821;&#38899;&#30340;&#29305;&#24449;&#32534;&#30721;&#20043;&#38388;&#30340;&#36317;&#31163;&#23545;&#35821;&#38899;&#36136;&#37327;&#21644;&#21487;&#25026;&#24230;&#30340;&#35780;&#20272;&#26377;&#26174;&#33879;&#20316;&#29992;&#65292;&#20351;&#29992;&#27492;&#36317;&#31163;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#22312;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.04388</link><description>&lt;p&gt;
&#24863;&#30693;&#21644;&#39044;&#27979;&#65306;&#22522;&#20110;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#30340;&#35821;&#38899;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Perceive and predict: self-supervised speech representation based loss functions for speech enhancement. (arXiv:2301.04388v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#24178;&#20928;&#21644;&#24102;&#22122;&#22768;&#35821;&#38899;&#30340;&#29305;&#24449;&#32534;&#30721;&#20043;&#38388;&#30340;&#36317;&#31163;&#23545;&#35821;&#38899;&#36136;&#37327;&#21644;&#21487;&#25026;&#24230;&#30340;&#35780;&#20272;&#26377;&#26174;&#33879;&#20316;&#29992;&#65292;&#20351;&#29992;&#27492;&#36317;&#31163;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#22312;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#26377;&#20851;&#35821;&#38899;&#22686;&#24378;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#26469;&#36741;&#21161;&#31070;&#32463;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#24456;&#22810;&#30456;&#20851;&#30740;&#31350;&#20391;&#37325;&#20110;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#30340;&#26368;&#28145;&#25110;&#26368;&#32456;&#36755;&#20986;&#65292;&#32780;&#19981;&#26159;&#36739;&#26089;&#30340;&#29305;&#24449;&#32534;&#30721;&#12290;&#36825;&#31181;&#26041;&#24335;&#20351;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#32463;&#24120;&#32570;&#20047;&#20805;&#20998;&#30340;&#21160;&#26426;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#24178;&#20928;&#21644;&#24102;&#22122;&#22768;&#35821;&#38899;&#30340;&#29305;&#24449;&#32534;&#30721;&#20043;&#38388;&#30340;&#36317;&#31163;&#19982;&#24515;&#29702;&#22768;&#23398;&#34913;&#37327;&#26631;&#20934;&#20197;&#21450;&#20154;&#31867;&#24179;&#22343;&#24847;&#35265;&#24471;&#20998;&#26174;&#33879;&#30456;&#20851;&#12290;&#23454;&#39564;&#20351;&#29992;&#27492;&#36317;&#31163;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#24863;&#30693;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;(PESQ)&#21644;&#30701;&#26102;&#35821;&#38899;&#23458;&#35266;&#36136;&#37327;&#35780;&#20272;(STOI)&#31561;&#23458;&#35266;&#24230;&#37327;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#22522;&#20110;STFT&#39057;&#35889;&#36317;&#31163;&#21644;&#20854;&#20182;&#24120;&#35265;&#35821;&#38899;&#22686;&#24378;&#25991;&#29486;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in the domain of speech enhancement has explored the use of self-supervised speech representations to aid in the training of neural speech enhancement models. However, much of this work focuses on using the deepest or final outputs of self supervised speech representation models, rather than the earlier feature encodings. The use of self supervised representations in such a way is often not fully motivated. In this work it is shown that the distance between the feature encodings of clean and noisy speech correlate strongly with psychoacoustically motivated measures of speech quality and intelligibility, as well as with human Mean Opinion Score (MOS) ratings. Experiments using this distance as a loss function are performed and improved performance over the use of STFT spectrogram distance based loss as well as other common loss functions from speech enhancement literature is demonstrated using objective measures such as perceptual evaluation of speech quality (PESQ) and shor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#30693;&#35782;&#24211;&#38382;&#31572;&#20013;&#30340;&#21487;&#22238;&#31572;&#24615;&#38382;&#39064;&#65292;&#20351;&#29992;&#26032;&#30340;GrailQAbility&#22522;&#20934;KBQA&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;KBQA&#27169;&#22411;&#22312;&#22788;&#29702;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#23545;&#26080;&#27861;&#22238;&#31572;&#30340;&#26816;&#27979;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#26469;&#20351;KBQA&#31995;&#32479;&#23545;&#26080;&#27861;&#22238;&#31572;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10189</link><description>&lt;p&gt;
&#25105;&#26377;&#36275;&#22815;&#30340;&#30693;&#35782;&#22238;&#31572;&#21527;&#65311;&#25506;&#31350;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#21487;&#22238;&#31572;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions. (arXiv:2212.10189v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#30693;&#35782;&#24211;&#38382;&#31572;&#20013;&#30340;&#21487;&#22238;&#31572;&#24615;&#38382;&#39064;&#65292;&#20351;&#29992;&#26032;&#30340;GrailQAbility&#22522;&#20934;KBQA&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;KBQA&#27169;&#22411;&#22312;&#22788;&#29702;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#23545;&#26080;&#27861;&#22238;&#31572;&#30340;&#26816;&#27979;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#26469;&#20351;KBQA&#31995;&#32479;&#23545;&#26080;&#27861;&#22238;&#31572;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#30693;&#35782;&#24211;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#26102;&#65292;&#32570;&#22833;&#30340;&#20107;&#23454;&#12289;&#19981;&#23436;&#25972;&#30340;&#27169;&#24335;&#21644;&#26377;&#38480;&#30340;&#33539;&#22260;&#33258;&#28982;&#22320;&#23548;&#33268;&#35768;&#22810;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#12290;&#34429;&#28982;&#22312;&#20854;&#20182;&#38382;&#31572;&#29615;&#22659;&#20013;&#24050;&#32463;&#25506;&#35752;&#20102;&#21487;&#22238;&#31572;&#24615;&#65292;&#20294;&#23545;&#20110;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#35782;&#21035;&#20102;&#21508;&#31181;&#24418;&#24335;&#30340;&#30693;&#35782;&#24211;&#19981;&#23436;&#25972;&#24615;&#65292;&#20351;&#24471;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#65292;&#24182;&#36890;&#36807;&#26377;&#31995;&#32479;&#22320;&#35843;&#25972;GrailQA&#65288;&#19968;&#20010;&#20165;&#21253;&#21547;&#21487;&#22238;&#31572;&#38382;&#39064;&#30340;&#27969;&#34892;KBQA&#25968;&#25454;&#38598;&#65289;&#26469;&#21019;&#24314;&#20855;&#26377;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#30340;&#26032;&#30340;GrailQAbility&#22522;&#20934;KBQA&#25968;&#25454;&#38598;&#12290;&#22312;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;KBQA&#27169;&#22411;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#36866;&#24403;&#35843;&#25972;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#21518;&#65292;&#25152;&#26377;&#19977;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#24120;&#24120;&#22240;&#38169;&#35823;&#30340;&#21407;&#22240;&#26816;&#27979;&#20986;&#26080;&#27861;&#22238;&#31572;&#65292;&#24182;&#21457;&#29616;&#29305;&#23450;&#24418;&#24335;&#30340;&#26080;&#27861;&#22238;&#31572;&#23588;&#20854;&#38590;&#20197;&#22788;&#29702;&#12290;&#36825;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#20351;KBQA&#31995;&#32479;&#23545;&#26080;&#27861;&#22238;&#31572;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When answering natural language questions over knowledge bases, missing facts, incomplete schema and limited scope naturally lead to many questions being unanswerable. While answerability has been explored in other QA settings, it has not been studied for QA over knowledge bases (KBQA). We create GrailQAbility, a new benchmark KBQA dataset with unanswerability, by first identifying various forms of KB incompleteness that make questions unanswerable, and then systematically adapting GrailQA (a popular KBQA dataset with only answerable questions). Experimenting with three state-of-the-art KBQA models, we find that all three models suffer a drop in performance even after suitable adaptation for unanswerable questions. In addition, these often detect unanswerability for wrong reasons and find specific forms of unanswerability particularly difficult to handle. This underscores the need for further research in making KBQA systems robust to unanswerability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#22810;&#26679;&#21270;&#28436;&#31034;&#26469;&#40723;&#21169;&#27169;&#22411;&#22312;&#32452;&#21512;&#27867;&#21270;&#30340;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19977;&#20010;&#32452;&#21512;&#27867;&#21270;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2212.06800</link><description>&lt;p&gt;
&#22810;&#26679;&#30340;&#28436;&#31034;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Diverse Demonstrations Improve In-context Compositional Generalization. (arXiv:2212.06800v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#22810;&#26679;&#21270;&#28436;&#31034;&#26469;&#40723;&#21169;&#27169;&#22411;&#22312;&#32452;&#21512;&#27867;&#21270;&#30340;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19977;&#20010;&#32452;&#21512;&#27867;&#21270;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312; i.i.d &#35821;&#20041;&#35299;&#26512;&#25286;&#20998;&#20013;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#26159;&#20174;&#21516;&#19968;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#36890;&#24120;&#20250;&#20351;&#29992;&#19982;&#36755;&#20837;&#35805;&#35821;&#31867;&#20284;&#30340;&#28436;&#31034;&#26469;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#32452;&#21512;&#27867;&#21270;&#30340;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#22312;&#36755;&#20986;&#20855;&#26377;&#35757;&#32451;&#38598;&#20013;&#19981;&#23384;&#22312;&#30340;&#32467;&#26500;&#30340;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;&#36873;&#25321;&#31867;&#20284;&#30340;&#28436;&#31034;&#26159;&#19981;&#22815;&#30340;&#65292;&#22240;&#20026;&#36890;&#24120;&#27809;&#26377;&#20219;&#20309;&#31034;&#20363;&#36275;&#22815;&#25509;&#36817;&#20110;&#36755;&#20837;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#22810;&#26679;&#30340;&#28436;&#31034;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20849;&#21516;&#28085;&#30422;&#36755;&#20986;&#31243;&#24207;&#20013;&#25152;&#38656;&#30340;&#25152;&#26377;&#32467;&#26500;&#65292;&#20197;&#40723;&#21169;&#27169;&#22411;&#20174;&#36825;&#20123;&#28436;&#31034;&#20013;&#25512;&#24191;&#21040;&#26032;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#32452;&#21512;&#27867;&#21270;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32431;&#19978;&#19979;&#25991;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#32467;&#21512;&#22810;&#26679;&#30340;&#28436;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#19982;fi&#36827;&#34892;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning has shown great success in i.i.d semantic parsing splits, where the training and test sets are drawn from the same distribution. In this setup, models are typically prompted with demonstrations that are similar to the input utterance. However, in the setup of compositional generalization, where models are tested on outputs with structures that are absent from the training set, selecting similar demonstrations is insufficient, as often no example will be similar enough to the input. In this work, we propose a method to select diverse demonstrations that aims to collectively cover all of the structures required in the output program, in order to encourage the model to generalize to new structures from these demonstrations. We empirically show that combining diverse demonstrations with in-context learning substantially improves performance across three compositional generalization semantic parsing datasets in the pure in-context learning setup and when combined with fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;PaLM&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#23569;&#37327;&#26679;&#20363;&#25552;&#31034;&#32763;&#35793;&#30340;&#21508;&#31181;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#26679;&#20363;&#30340;&#36136;&#37327;&#26159;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#34920;&#26126;PaLM&#30340;&#24615;&#33021;&#34429;&#28982;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#20173;&#28982;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2211.09102</link><description>&lt;p&gt;
PaLM&#30340;&#32763;&#35793;&#25552;&#31034;&#65306;&#35780;&#20272;&#31574;&#30053;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Prompting PaLM for Translation: Assessing Strategies and Performance. (arXiv:2211.09102v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;PaLM&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#23569;&#37327;&#26679;&#20363;&#25552;&#31034;&#32763;&#35793;&#30340;&#21508;&#31181;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#26679;&#20363;&#30340;&#36136;&#37327;&#26159;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#34920;&#26126;PaLM&#30340;&#24615;&#33021;&#34429;&#28982;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#20173;&#28982;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#20294;&#38750;&#24182;&#34892;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;Pathways&#35821;&#35328;&#27169;&#22411;(PaLM)&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#32463;&#36807;&#31867;&#20284;&#35757;&#32451;&#30340;LLMs&#20013;&#34920;&#29616;&#26368;&#24378;&#30340;&#26426;&#22120;&#32763;&#35793;(MT)&#27169;&#22411;&#20043;&#19968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36873;&#25321;&#23569;&#37327;&#26679;&#20363;&#36827;&#34892;&#25552;&#31034;&#30340;&#21508;&#31181;&#31574;&#30053;&#65292;&#24471;&#20986;&#32467;&#35770;&#26159;&#26679;&#20363;&#30340;&#36136;&#37327;&#26159;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#12290;&#20351;&#29992;&#20248;&#21270;&#21518;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#37325;&#26032;&#35780;&#20272;&#20102;PaLM&#22312;&#26368;&#26032;&#30340;&#27979;&#35797;&#38598;&#12289;&#29616;&#20195;MT&#24230;&#37327;&#21644;&#20154;&#24037;&#35780;&#20215;&#26041;&#38754;&#30340;MT&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#30340;&#34920;&#29616;&#34429;&#28982;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#20173;&#28982;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#31995;&#32479;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#23545;PaLM&#30340;MT&#36755;&#20986;&#36827;&#34892;&#20998;&#26512;&#30340;&#25253;&#21578;&#65292;&#36825;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#29305;&#24615;&#21644;&#26410;&#26469;&#24037;&#20316;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. We probe this ability in an in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT) performance among similarly-trained LLMs to date. We investigate various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor. Using optimized prompts, we revisit previous assessments of PaLM's MT capabilities with more recent test sets, modern MT metrics, and human evaluation, and find that its performance, while impressive, still lags that of state-of-the-art supervised systems. We conclude by providing an analysis of PaLM's MT output which reveals some interesting properties and prospects for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#31070;&#32463;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23545;&#20110;&#20272;&#35745;&#27010;&#29575;&#35789;&#35821;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#20351;&#29992;UNLI&#25968;&#25454;&#38598;&#21644;&#26500;&#24314;WEP&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;WEP&#30340;&#23384;&#22312;&#26102;&#24456;&#26377;&#25928;&#65292;&#20294;&#27809;&#26377;&#23436;&#20840;&#25429;&#25417;&#21040;&#19982;&#27599;&#20010;&#35789;&#30456;&#20851;&#32852;&#30340;&#20849;&#35782;&#27010;&#29575;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2211.03358</link><description>&lt;p&gt;
&#25506;&#31350;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#23545;&#20272;&#35745;&#27010;&#29575;&#35789;&#35821;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Probing neural language models for understanding of words of estimative probability. (arXiv:2211.03358v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#31070;&#32463;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23545;&#20110;&#20272;&#35745;&#27010;&#29575;&#35789;&#35821;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#20351;&#29992;UNLI&#25968;&#25454;&#38598;&#21644;&#26500;&#24314;WEP&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;WEP&#30340;&#23384;&#22312;&#26102;&#24456;&#26377;&#25928;&#65292;&#20294;&#27809;&#26377;&#23436;&#20840;&#25429;&#25417;&#21040;&#19982;&#27599;&#20010;&#35789;&#30456;&#20851;&#32852;&#30340;&#20849;&#35782;&#27010;&#29575;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#27010;&#29575;&#35789;&#35821;(WEP)&#26159;&#38472;&#36848;&#30340;&#21487;&#20449;&#24230;&#34920;&#36798;&#65288;&#20363;&#22914;&#65292;&#21487;&#33021;&#65292;&#25110;&#35768;&#65292;&#24456;&#26377;&#21487;&#33021;&#65292;&#24576;&#30097;&#65292;&#19981;&#21487;&#33021;&#31561;&#65289;&#12290;&#22810;&#39033;&#35843;&#26597;&#34920;&#26126;&#65292;&#20154;&#31867;&#35780;&#20272;&#32773;&#22312;&#20026;WEP&#20998;&#37197;&#25968;&#20540;&#27010;&#29575;&#27700;&#24179;&#26102;&#23384;&#22312;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65292; Fagen-Ulmschneider(2015)&#30340;&#35843;&#26597;&#20013;&#65292;&#8220;&#24456;&#26377;&#21487;&#33021;&#8221;&#23545;&#24212;&#30528;&#20013;&#20301;&#25968;0.90+-0.08&#30340;&#20960;&#29575;&#12290;&#26412;&#30740;&#31350;&#27979;&#37327;&#20102;&#31070;&#32463;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25429;&#25417;&#19982;&#27599;&#20010;WEP&#30456;&#20851;&#32852;&#30340;&#20849;&#35782;&#27010;&#29575;&#27700;&#24179;&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;UNLI&#25968;&#25454;&#38598;(&#38472;&#31561;&#20154;&#65292;2020)&#65292;&#23558;&#21069;&#25552;&#21644;&#20551;&#35774;&#19982;&#20854;&#24863;&#30693;&#30340;&#32852;&#21512;&#27010;&#29575;p&#30456;&#32852;&#31995;&#65292;&#26500;&#24314;&#25552;&#31034;&#65292;&#20363;&#22914;&#8220;[&#21069;&#25552;]&#12290;[WEP]&#65292;[&#20551;&#35774;]&#12290;&#8221;&#24182;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;WEP&#20849;&#35782;&#27010;&#29575;&#27700;&#24179;&#26159;&#21542;&#25509;&#36817;&#20110;p&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;WEP&#30340;&#27010;&#29575;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;WEP&#32452;&#21512;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;&#25552;&#31034;&#8220;[&#20107;&#20214;A]&#24456;&#26377;&#21487;&#33021;&#12290;[&#20107;&#20214;B]&#19981;&#21487;&#33021;&#12290;[&#32467;&#26524;]&#20250;&#21457;&#29983;&#30340;&#27010;&#29575;&#26159;&#22810;&#23569;&#65311; "&#26102;&#65292;&#20154;&#20204;&#21487;&#20197;&#26681;&#25454;WEP&#35789;&#27719;&#30340;&#21487;&#20449;&#24230;&#20272;&#31639;&#27010;&#29575;&#27700;&#24179;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#29616;&#26377;&#30340;&#31070;&#32463;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#35266;&#23519;&#21040;&#34429;&#28982;&#23427;&#20204;&#22312;&#39044;&#27979;WEP&#30340;&#23384;&#22312;&#26102;&#24456;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#23436;&#20840;&#25429;&#25417;&#21040;&#19982;&#27599;&#20010;&#35789;&#30456;&#20851;&#32852;&#30340;&#20849;&#35782;&#27010;&#29575;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Words of estimative probability (WEP) are expressions of a statement's plausibility (probably, maybe, likely, doubt, likely, unlikely, impossible...). Multiple surveys demonstrate the agreement of human evaluators when assigning numerical probability levels to WEP. For example, highly likely corresponds to a median chance of 0.90+-0.08 in Fagen-Ulmschneider (2015)'s survey. In this work, we measure the ability of neural language processing models to capture the consensual probability level associated to each WEP. Firstly, we use the UNLI dataset (Chen et al., 2020) which associates premises and hypotheses with their perceived joint probability p, to construct prompts, e.g. "[PREMISE]. [WEP], [HYPOTHESIS]." and assess whether language models can predict whether the WEP consensual probability level is close to p. Secondly, we construct a dataset of WEP-based probabilistic reasoning, to test whether language models can reason with WEP compositions. When prompted "[EVENTA] is likely. [EVEN
&lt;/p&gt;</description></item><item><title>KSAT&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#24341;&#20837;&#30693;&#35782;&#24341;&#23548;&#20559;&#35265;&#26469;&#25972;&#21512;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#33258;&#25105;&#20851;&#27880;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2210.04307</link><description>&lt;p&gt;
KSAT&#65306;&#30693;&#35782;&#27880;&#20837;&#30340;&#33258;&#25105;&#20851;&#27880;&#21464;&#21387;&#22120;&#8212;&#8212;&#25972;&#21512;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
KSAT: Knowledge-infused Self Attention Transformer -- Integrating Multiple Domain-Specific Contexts. (arXiv:2210.04307v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04307
&lt;/p&gt;
&lt;p&gt;
KSAT&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#24341;&#20837;&#30693;&#35782;&#24341;&#23548;&#20559;&#35265;&#26469;&#25972;&#21512;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#33258;&#25105;&#20851;&#27880;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#29702;&#35299;&#38656;&#35201;&#25972;&#21512;&#22810;&#20010;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20363;&#22914;&#65292;&#8220;&#25105;&#25343;&#30528;&#19968;&#25226;&#26538;&#65292;&#23545;&#25105;&#30340;&#29983;&#27963;&#24863;&#21040;&#24456;&#31967;&#31957;&#65292;&#22914;&#26524;&#26126;&#22825;&#25105;&#19981;&#37266;&#26469;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#26368;&#31967;&#31957;&#30340;&#20107;&#24773;&#8221;&#65292;&#20854;&#20013;&#21253;&#21547;&#33258;&#26432;&#21644;&#25233;&#37057;&#30151;&#30456;&#20851;&#30340;&#34892;&#20026;&#65288;&#22810;&#20010;&#19978;&#19979;&#25991;&#65289;&#12290;&#33258;&#27880;&#24847;&#21147;&#32467;&#26500;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#24615;&#36890;&#36807;&#22312;&#30456;&#20851;&#39046;&#22495;&#29305;&#23450;&#36164;&#28304;&#30340;&#25688;&#24405;&#19978;&#36827;&#34892;&#24494;&#35843;&#65288;&#25968;&#25454;&#38598;&#21644;&#22806;&#37096;&#30693;&#35782;-&#19982;&#33258;&#26432;&#21644;&#25233;&#37057;&#30151;&#30456;&#20851;&#30340;&#24515;&#29702;&#20581;&#24247;&#35786;&#26029;&#30340;&#21307;&#23398;&#25945;&#31185;&#20070;&#31456;&#33410;&#65289;&#26469;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;&#33258;&#25105;&#20851;&#27880;&#32467;&#26500;KSAT&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#23454;&#29616;&#20102;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#25972;&#21512;&#12290;KSAT&#22312;&#27599;&#20010;&#30693;&#35782;&#28304;&#30340;&#19987;&#38376;&#33258;&#25105;&#20851;&#27880;&#23618;&#20013;&#24341;&#20837;&#30693;&#35782;&#24341;&#23548;&#20559;&#35265;&#26469;&#23436;&#25104;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;KSAT&#25552;&#20379;&#20102;&#25511;&#21046;&#23398;&#20064;&#21644;&#30693;&#35782;&#21033;&#29992;&#20043;&#38388;&#26435;&#34913;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-specific language understanding requires integrating multiple pieces of relevant contextual information. For example, we see both suicide and depression-related behavior (multiple contexts) in the text ``I have a gun and feel pretty bad about my life, and it wouldn't be the worst thing if I didn't wake up tomorrow''. Domain specificity in self-attention architectures is handled by fine-tuning on excerpts from relevant domain specific resources (datasets and external knowledge - medical textbook chapters on mental health diagnosis related to suicide and depression). We propose a modified self-attention architecture Knowledge-infused Self Attention Transformer (KSAT) that achieves the integration of multiple domain-specific contexts through the use of external knowledge sources. KSAT introduces knowledge-guided biases in dedicated self-attention layers for each knowledge source to accomplish this. In addition, KSAT provides mechanics for controlling the trade-off between learning 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#33976;&#39311;&#26041;&#27861;PROD&#65292;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;&#65292;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#23398;&#29983;&#27169;&#22411;&#26469;&#22635;&#34917;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.13335</link><description>&lt;p&gt;
PROD&#65306;&#28176;&#36827;&#24335;&#33976;&#39311;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
PROD: Progressive Distillation for Dense Retrieval. (arXiv:2209.13335v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#33976;&#39311;&#26041;&#27861;PROD&#65292;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;&#65292;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#23398;&#29983;&#27169;&#22411;&#26469;&#22635;&#34917;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#23558;&#24378;&#25945;&#24072;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#39640;&#25928;&#23398;&#29983;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#39044;&#26399;&#30340;&#26356;&#22909;&#30340;&#25945;&#24072;&#20250;&#23548;&#33268;&#32463;&#36807;&#33976;&#39311;&#21518;&#23398;&#29983;&#26356;&#31967;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;&#30340;PROgressive Distillation (PROD)&#26041;&#27861;&#65292;&#21253;&#25324;&#25945;&#24072;&#28176;&#36827;&#24335;&#33976;&#39311;&#21644;&#25968;&#25454;&#28176;&#36827;&#24335;&#33976;&#39311;&#20004;&#20010;&#38454;&#27573;&#65292;&#20174;&#32780;&#36880;&#27493;&#25552;&#39640;&#23398;&#29983;&#30340;&#26816;&#32034;&#32489;&#25928;&#12290;&#22312;&#20116;&#20010;&#34987;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;MS MARCO Passage&#12289;TREC Passage 19&#12289;TREC Document 19&#12289;MS MARCO Document&#21644;&#33258;&#28982;&#38382;&#39064;&#65289;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#65292;PROD&#22312;&#23494;&#38598;&#26816;&#32034;&#30340;&#33976;&#39311;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21644;&#27169;&#22411;&#23558;&#20250;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is an effective way to transfer knowledge from a strong teacher to an efficient student model. Ideally, we expect the better the teacher is, the better the student. However, this expectation does not always come true. It is common that a better teacher model results in a bad student via distillation due to the nonnegligible gap between teacher and student. To bridge the gap, we propose PROD, a PROgressive Distillation method, for dense retrieval. PROD consists of a teacher progressive distillation and a data progressive distillation to gradually improve the student. We conduct extensive experiments on five widely-used benchmarks, MS MARCO Passage, TREC Passage 19, TREC Document 19, MS MARCO Document and Natural Questions, where PROD achieves the state-of-the-art within the distillation methods for dense retrieval. The code and models will be released.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Transformer&#22312;U.S Patent Phrase to Phrase Matching Dataset&#19978;&#36827;&#34892;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#25928;&#29575;&#65292;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.11716</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#35821;&#26009;&#24211;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#30340;&#35748;&#30693;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach. (arXiv:2207.11716v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Transformer&#22312;U.S Patent Phrase to Phrase Matching Dataset&#19978;&#36827;&#34892;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#25928;&#29575;&#65292;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#21644;&#24314;&#27169;&#26159;&#24403;&#20170;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35768;&#22810;&#20808;&#39537;&#24212;&#29992;&#20013;&#22522;&#26412;&#35748;&#21487;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#39034;&#24207;&#27169;&#24335;&#35782;&#21035;&#30340;&#24863;&#30693;&#65292;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#65288;&#22914;RNN&#21644;LSTM&#65289;&#22312;&#35821;&#20041;&#30456;&#20284;&#24230;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23427;&#20204;&#26080;&#27861;&#20197;&#38750;&#39034;&#24207;&#26041;&#24335;&#22788;&#29702;&#20449;&#24687;&#65292;&#22240;&#27492;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#34987;&#35748;&#20026;&#25928;&#29575;&#20302;&#19979;&#65292;&#20174;&#32780;&#23548;&#33268;&#19978;&#19979;&#25991;&#25552;&#21462;&#19981;&#24403;&#12290;Transformer&#22240;&#20854;&#38750;&#39034;&#24207;&#25968;&#25454;&#22788;&#29702;&#21644;&#33258;&#25105;&#20851;&#27880;&#31561;&#20248;&#21183;&#32780;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#12290;&#26412;&#25991;&#20351;&#29992;&#20256;&#32479;&#21644;&#22522;&#20110;transformer&#30340;&#25216;&#26415;&#23545;&#32654;&#22269;&#19987;&#21033;&#30701;&#35821;&#36827;&#34892;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#21644;&#24314;&#27169;&#12290;&#25105;&#20204;&#23545;&#22235;&#31181;&#19981;&#21516;&#29256;&#26412;&#30340;&#35299;&#30721;&#22686;&#24378;BERT-DeBERTa&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;K&#25240;&#20132;&#21449;&#39564;&#35777;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity analysis and modeling is a fundamentally acclaimed task in many pioneering applications of natural language processing today. Owing to the sensation of sequential pattern recognition, many neural networks like RNNs and LSTMs have achieved satisfactory results in semantic similarity modeling. However, these solutions are considered inefficient due to their inability to process information in a non-sequential manner, thus leading to the improper extraction of context. Transformers function as the state-of-the-art architecture due to their advantages like non-sequential data processing and self-attention. In this paper, we perform semantic similarity analysis and modeling on the U.S Patent Phrase to Phrase Matching Dataset using both traditional and transformer-based techniques. We experiment upon four different variants of the Decoding Enhanced BERT - DeBERTa and enhance its performance by performing K-Fold Cross-Validation. The experimental results demonstrate our me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#25361;&#25112;&#20219;&#21153;&#65292;&#21363;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#24335;&#20174;&#31185;&#23398;&#25991;&#31456;&#20013;&#30340;&#34920;&#26684;&#20013;&#25552;&#21462;&#26377;&#20851;&#26448;&#26009;&#32452;&#25104;&#30340;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;4408&#20010;&#36828;&#31243;&#30417;&#30563;&#34920;&#26684;&#21644;1475&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;&#34920;&#26684;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22522;&#32447;&#8212;&#8212;DiSCoMaT&#12290;</title><link>http://arxiv.org/abs/2207.01079</link><description>&lt;p&gt;
DiSCoMaT&#65306;&#26448;&#26009;&#31185;&#23398;&#25991;&#31456;&#20013;&#22522;&#20110;&#36828;&#31243;&#30417;&#30563;&#30340;&#34920;&#26684;&#32452;&#25104;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles. (arXiv:2207.01079v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#25361;&#25112;&#20219;&#21153;&#65292;&#21363;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#24335;&#20174;&#31185;&#23398;&#25991;&#31456;&#20013;&#30340;&#34920;&#26684;&#20013;&#25552;&#21462;&#26377;&#20851;&#26448;&#26009;&#32452;&#25104;&#30340;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;4408&#20010;&#36828;&#31243;&#30417;&#30563;&#34920;&#26684;&#21644;1475&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;&#34920;&#26684;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22522;&#32447;&#8212;&#8212;DiSCoMaT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31185;&#23398;&#39046;&#22495;&#25991;&#31456;&#20013;&#30340;&#34920;&#26684;&#20013;&#25552;&#21462;&#26377;&#20851;&#26448;&#26009;&#32452;&#25104;&#30340;&#20449;&#24687;&#26159;&#30693;&#35782;&#24211;&#31574;&#21010;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34920;&#26684;&#25552;&#21462;&#22120;&#20551;&#23450;&#24744;&#24050;&#32463;&#20102;&#35299;&#34920;&#26684;&#32467;&#26500;&#21644;&#26684;&#24335;&#65292;&#32780;&#31185;&#23398;&#34920;&#26684;&#20013;&#21487;&#33021;&#27809;&#26377;&#36825;&#20123;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#23450;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#34920;&#26684;&#25552;&#21462;&#38382;&#39064;&#65306;&#25552;&#21462;&#26448;&#26009;&#65288;&#20363;&#22914;&#29627;&#29827;&#65292;&#21512;&#37329;&#65289;&#30340;&#32452;&#25104;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#26448;&#26009;&#31185;&#23398;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#21508;&#31181;&#34920;&#26684;&#26679;&#24335;&#32452;&#32455;&#31867;&#20284;&#30340;&#32452;&#25104;&#65292;&#36825;&#38656;&#35201;&#19968;&#20010;&#26234;&#33021;&#27169;&#22411;&#26469;&#29702;&#35299;&#34920;&#26684;&#21644;&#25552;&#21462;&#32452;&#25104;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26032;&#22411;&#25361;&#25112;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#30001;4408&#20010;&#36828;&#31243;&#30417;&#30563;&#34920;&#26684;&#21644;1475&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;&#34920;&#26684;&#32452;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;DiSCoMaT&#65292;&#23427;&#26159;&#19968;&#20010;&#38024;&#23545;&#35813;&#38382;&#39064;&#30340;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial component in the curation of KB for a scientific domain is information extraction from tables in the domain's published articles -- tables carry important information (often numeric), which must be adequately extracted for a comprehensive machine understanding of an article. Existing table extractors assume prior knowledge of table structure and format, which may not be known in scientific tables. We study a specific and challenging table extraction problem: extracting compositions of materials (e.g., glasses, alloys). We first observe that materials science researchers organize similar compositions in a wide variety of table styles, necessitating an intelligent model for table understanding and composition extraction. Consequently, we define this novel task as a challenge for the ML community and create a training dataset comprising 4,408 distantly supervised tables, along with 1,475 manually annotated dev and test tables. We also present DiSCoMaT, a strong baseline geared t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#30340;&#26041;&#27861;LMKE&#65292;&#23427;&#26088;&#22312;&#25552;&#39640;&#23545;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.12617</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Language Models as Knowledge Embeddings. (arXiv:2206.12617v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#30340;&#26041;&#27861;LMKE&#65292;&#23427;&#26088;&#22312;&#25552;&#39640;&#23545;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#23884;&#20837;&#26159;&#36890;&#36807;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#23884;&#20837;&#21040;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#26469;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#32467;&#26500;&#25110;&#22522;&#20110;&#25551;&#36848;&#12290;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#23398;&#20064;&#34920;&#31034;&#65292;&#20197;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#29616;&#23454;&#19990;&#30028;&#30693;&#35782;&#22270;&#35889;&#20013;&#26377;&#38480;&#32467;&#26500;&#20449;&#24687;&#19979;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#12290;&#22522;&#20110;&#25551;&#36848;&#30340;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#20808;&#21069;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#36229;&#36234;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23384;&#22312;&#26114;&#36149;&#30340;&#36127;&#37319;&#26679;&#21644;&#38480;&#21046;&#24615;&#25551;&#36848;&#38656;&#27714;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LMKE&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#65292;&#26088;&#22312;&#20016;&#23500;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#34920;&#36848;&#22522;&#20110;&#25551;&#36848;&#30340;&#30693;&#35782;&#23884;&#20837;&#23398;&#20064;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21644;&#35780;&#20215;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LMKE&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#22522;&#20110;&#32467;&#26500;&#21644;&#22522;&#20110;&#20808;&#21069;&#25551;&#36848;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding entities and relations into continuous vector spaces. Existing methods are mainly structure-based or description-based. Structure-based methods learn representations that preserve the inherent structure of KGs. They cannot well represent abundant long-tail entities in real-world KGs with limited structural information. Description-based methods leverage textual information and language models. Prior approaches in this direction barely outperform structure-based ones, and suffer from problems like expensive negative sampling and restrictive description demand. In this paper, we propose LMKE, which adopts Language Models to derive Knowledge Embeddings, aiming at both enriching representations of long-tail entities and solving problems of prior description-based methods. We formulate description-based KE learning with a contrastive learning framework to improve efficiency in training and evaluation. Experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23454;&#20363;&#21644;&#30693;&#35782;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;ABSA&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2110.13398</link><description>&lt;p&gt;
&#32479;&#19968;&#23454;&#20363;&#21644;&#30693;&#35782;&#23545;&#40784;&#39044;&#35757;&#32451;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unified Instance and Knowledge Alignment Pretraining for Aspect-based Sentiment Analysis. (arXiv:2110.13398v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23454;&#20363;&#21644;&#30693;&#35782;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;ABSA&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#26088;&#22312;&#30830;&#23450;&#23545;&#26576;&#20010;&#26041;&#38754;&#30340;&#24773;&#24863;&#20542;&#21521;&#12290;&#30001;&#20110;&#26114;&#36149;&#19988;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#39044;&#35757;&#32451;&#31574;&#30053;&#24050;&#25104;&#20026;ABSA&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;ABSA&#25968;&#25454;&#38598;&#20043;&#38388;&#24635;&#26159;&#23384;&#22312;&#20005;&#37325;&#30340;&#39046;&#22495;&#20559;&#31227;&#65292;&#30452;&#25509;&#24494;&#35843;&#26102;&#30340;&#30693;&#35782;&#36716;&#31227;&#25928;&#26524;&#19981;&#20339;&#65292;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#20122;&#20248;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#39046;&#22495;&#20559;&#31227;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21253;&#25324;&#23454;&#20363;&#21644;&#30693;&#35782;&#23618;&#38754;&#30340;&#23545;&#40784;&#65292;&#23558;&#20854;&#34701;&#20837;&#21040;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27969;&#31243;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#38454;&#27573;&#26816;&#32034;&#37319;&#26679;&#26041;&#27861;&#65292;&#20174;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#20851;&#30340;&#23454;&#20363;&#65292;&#20174;&#32780;&#23454;&#29616;&#39044;&#35757;&#32451;&#21644;&#30446;&#26631;&#39046;&#22495;&#23454;&#20363;&#30340;&#23545;&#40784;&#65288;&#31532;&#19968;&#38454;&#27573;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#25351;&#23548;&#30340;&#31574;&#30053;&#65292;&#36827;&#19968;&#27493;&#26725;&#25509;&#30693;&#35782;&#23618;&#38754;&#30340;&#39046;&#22495;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;ABSA&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#36229;&#36234;&#24378;&#22522;&#32447;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;ABSA&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;SemEval 2014&#20219;&#21153;4&#12289;SemEval 2015&#20219;&#21153;12&#21644;SemEval 2016&#20219;&#21153;5&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based Sentiment Analysis (ABSA) aims to determine the sentiment polarity towards an aspect. Because of the expensive and limited labelled data, the pretraining strategy has become the de-facto standard for ABSA. However, there always exists severe domain shift between the pretraining and downstream ABSA datasets, hindering the effective knowledge transfer when directly finetuning and making the downstream task performs sub-optimal. To mitigate such domain shift, we introduce a unified alignment pretraining framework into the vanilla pretrain-finetune pipeline with both instance- and knowledge-level alignments. Specifically, we first devise a novel coarse-to-fine retrieval sampling approach to select target domain-related instances from the large-scale pretraining dataset, thus aligning the instances between pretraining and target domains (First Stage). Then, we introduce a knowledge guidance-based strategy to further bridge the domain gap at the knowledge level. In practice, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#25239;&#26679;&#26412;&#21512;&#25104;&#21644;&#35757;&#32451;&#65288;CSST&#65289;&#31574;&#30053;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#24403;&#21069;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#30340;&#35821;&#35328;&#20559;&#24046;&#38382;&#39064;&#65292;&#26174;&#33879;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29702;&#24819;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#21644;&#38382;&#39064;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.01013</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#35270;&#35273;&#38382;&#31572;&#38656;&#35201;&#21512;&#25104;&#23545;&#25239;&#26679;&#26412;&#26469;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering. (arXiv:2110.01013v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.01013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#25239;&#26679;&#26412;&#21512;&#25104;&#21644;&#35757;&#32451;&#65288;CSST&#65289;&#31574;&#30053;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#24403;&#21069;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#30340;&#35821;&#35328;&#20559;&#24046;&#38382;&#39064;&#65292;&#26174;&#33879;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29702;&#24819;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#21644;&#38382;&#39064;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#24448;&#24448;&#21482;&#25429;&#25417;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#34920;&#38754;&#35821;&#35328;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#20855;&#26377;&#19981;&#21516;&#38382;&#31572;&#20998;&#24067;&#30340;&#27979;&#35797;&#38598;&#20013;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20123;&#35821;&#35328;&#20559;&#24046;&#65292;&#26368;&#36817;&#30340;&#35270;&#35273;&#38382;&#31572;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#30340;&#20165;&#38382;&#39064;&#27169;&#22411;&#26469;&#35268;&#33539;&#26377;&#38024;&#23545;&#24615;&#30340;VQA&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#35786;&#26029;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20027;&#23548;&#22320;&#20301;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#27169;&#22411;&#35774;&#35745;&#65292;&#36825;&#20123;&#22522;&#20110;&#38598;&#25104;&#30340;&#26041;&#27861;&#26080;&#27861;&#20855;&#22791;&#29702;&#24819;&#30340;VQA&#27169;&#22411;&#30340;&#20004;&#20010;&#19981;&#21487;&#25110;&#32570;&#30340;&#29305;&#24449;&#65306;1&#65289;&#21487;&#35270;&#21270;&#35299;&#37322;&#65306;&#27169;&#22411;&#22312;&#20570;&#20915;&#31574;&#26102;&#24212;&#20381;&#36182;&#20110;&#27491;&#30830;&#30340;&#35270;&#35273;&#21306;&#22495;&#12290;2&#65289;&#38382;&#39064;&#25935;&#24863;&#65306;&#27169;&#22411;&#23545;&#38382;&#39064;&#20013;&#30340;&#35821;&#35328;&#21464;&#21270;&#24212;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#25239;&#26679;&#26412;&#21512;&#25104;&#21644;&#35757;&#32451;&#65288;CSST&#65289;&#31574;&#30053;&#12290;&#32463;&#36807;CSST&#35757;&#32451;&#21518;&#65292;VQA&#27169;&#22411;&#34987;&#36843;&#20851;&#27880;&#25152;&#26377;&#20851;&#38190;&#23545;&#35937;&#21644;&#21333;&#35789;&#65292;&#20174;&#32780;&#26174;&#33879;&#25913;&#21892;&#20102;&#35270;&#35273;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today's VQA models still tend to capture superficial linguistic correlations in the training set and fail to generalize to the test set with different QA distributions. To reduce these language biases, recent VQA works introduce an auxiliary question-only model to regularize the training of targeted VQA model, and achieve dominating performance on diagnostic benchmarks for out-of-distribution testing. However, due to complex model design, these ensemble-based methods are unable to equip themselves with two indispensable characteristics of an ideal VQA model: 1) Visual-explainable: The model should rely on the right visual regions when making decisions. 2) Question-sensitive: The model should be sensitive to the linguistic variations in questions. To this end, we propose a novel model-agnostic Counterfactual Samples Synthesizing and Training (CSST) strategy. After training with CSST, VQA models are forced to focus on all critical objects and words, which significantly improves both visu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CRF&#27169;&#22411;&#65292;&#31216;&#20026;Speaker-change Aware CRF&#65292;&#20197;&#32771;&#34385;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20013;&#30340;&#35828;&#35805;&#20154;&#21464;&#21270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2004.02913</link><description>&lt;p&gt;
&#22522;&#20110;&#35828;&#35805;&#20154;&#24863;&#30693;&#30340;CRF&#29992;&#20110;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Speaker-change Aware CRF for Dialogue Act Classification. (arXiv:2004.02913v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.02913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CRF&#27169;&#22411;&#65292;&#31216;&#20026;Speaker-change Aware CRF&#65292;&#20197;&#32771;&#34385;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20013;&#30340;&#35828;&#35805;&#20154;&#21464;&#21270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#23545;&#35805;&#34892;&#20026;&#65288;DA&#65289;&#20998;&#31867;&#26041;&#38754;&#30340;&#30740;&#31350;&#23558;&#35813;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#24207;&#21015;&#26631;&#27880;&#38382;&#39064;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20197;&#21450;&#20316;&#20026;&#26368;&#21518;&#19968;&#23618;&#30340;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRF&#65289;&#12290;CRF&#27169;&#22411;&#26681;&#25454;&#36755;&#20837;&#35805;&#35821;&#24207;&#21015;&#26469;&#24314;&#27169;&#30446;&#26631;DA&#26631;&#31614;&#24207;&#21015;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#35813;&#20219;&#21153;&#36824;&#28041;&#21450;&#21040;&#21478;&#19968;&#20010;&#37325;&#35201;&#30340;&#36755;&#20837;&#24207;&#21015;&#65292;&#21363;&#35828;&#35805;&#20154;&#24207;&#21015;&#65292;&#32780;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#36825;&#26041;&#38754;&#27809;&#26377;&#20570;&#20986;&#32771;&#34385;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;CRF&#23618;&#30340;&#20462;&#25913;&#65292;&#35813;&#23618;&#32771;&#34385;&#21040;&#35828;&#35805;&#20154;&#21464;&#21270;&#12290;&#23545;SwDA&#35821;&#26009;&#24211;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#20462;&#25913;&#21518;&#30340;CRF&#23618;&#20248;&#20110;&#21407;&#22987;&#23618;&#65292;&#22312;&#26576;&#20123;DA&#26631;&#31614;&#19978;&#30340;&#24046;&#36317;&#38750;&#24120;&#22823;&#12290;&#27492;&#22806;&#65292;&#21487;&#35270;&#21270;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;CRF&#23618;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#22312;&#35828;&#35805;&#20154;&#20132;&#26367;&#26465;&#20214;&#19979;DA&#26631;&#31614;&#23545;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#12289;&#22797;&#26434;&#30340;&#36716;&#31227;&#27169;&#24335;&#12290;&#20195;&#30721;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in Dialogue Act (DA) classification approaches the task as a sequence labeling problem, using neural network models coupled with a Conditional Random Field (CRF) as the last layer. CRF models the conditional probability of the target DA label sequence given the input utterance sequence. However, the task involves another important input sequence, that of speakers, which is ignored by previous work. To address this limitation, this paper proposes a simple modification of the CRF layer that takes speaker-change into account. Experiments on the SwDA corpus show that our modified CRF layer outperforms the original one, with very wide margins for some DA labels. Further, visualizations demonstrate that our CRF layer can learn meaningful, sophisticated transition patterns between DA label pairs conditioned on speaker-change in an end-to-end way. Code is publicly available.
&lt;/p&gt;</description></item></channel></rss>