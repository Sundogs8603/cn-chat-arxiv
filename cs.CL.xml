<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#31934;&#28860;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#25552;&#21462;&#30693;&#35782;&#26469;&#23454;&#29616;Prompt&#30340;&#21387;&#32553;&#65292;&#30830;&#20445;&#21387;&#32553;&#21518;&#30340;&#25552;&#31034;&#20445;&#25345;&#23545;&#21407;&#22987;&#25552;&#31034;&#30340;&#24544;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12968</link><description>&lt;p&gt;
LLMLingua-2: &#39640;&#25928;&#19988;&#24544;&#23454;&#30340;&#26080;&#20219;&#21153;Prompt&#21387;&#32553;&#30340;&#25968;&#25454;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12968
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#31934;&#28860;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#25552;&#21462;&#30693;&#35782;&#26469;&#23454;&#29616;Prompt&#30340;&#21387;&#32553;&#65292;&#30830;&#20445;&#21387;&#32553;&#21518;&#30340;&#25552;&#31034;&#20445;&#25345;&#23545;&#21407;&#22987;&#25552;&#31034;&#30340;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20851;&#27880;&#20110;&#26080;&#20219;&#21153;&#30340;Prompt&#21387;&#32553;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;&#32771;&#34385;&#21040;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#20887;&#20313;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#26681;&#25454;&#20174;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;LLaMa-7B&#65289;&#33719;&#24471;&#30340;&#20449;&#24687;&#29109;&#26469;&#21024;&#38500;token&#25110;&#35789;&#27719;&#21333;&#20301;&#26469;&#21387;&#32553;prompt&#12290;&#25361;&#25112;&#22312;&#20110;&#20449;&#24687;&#29109;&#21487;&#33021;&#26159;&#19968;&#20010;&#27425;&#20248;&#30340;&#21387;&#32553;&#24230;&#37327;&#65306;(i)&#23427;&#20165;&#21033;&#29992;&#21333;&#21521;&#19978;&#19979;&#25991;&#65292;&#21487;&#33021;&#26080;&#27861;&#25429;&#33719;&#25152;&#26377;&#29992;&#20110;prompt&#21387;&#32553;&#30340;&#20851;&#38190;&#20449;&#24687;&#65307;(ii)&#23427;&#19982;prompt&#21387;&#32553;&#30446;&#26631;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#31934;&#28860;&#36807;&#31243;&#65292;&#20174;LLM&#20013;&#33719;&#24471;&#30693;&#35782;&#20197;&#21387;&#32553;prompt&#32780;&#19981;&#20002;&#22833;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#25277;&#21462;&#24335;&#25991;&#26412;&#21387;&#32553;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23558;prompt&#21387;&#32553;&#26684;&#24335;&#21270;&#20026;&#19968;&#20010;token&#20998;&#31867;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#21387;&#32553;&#21518;&#30340;prompt&#19982;&#21407;&#22987;prompt&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12968v1 Announce Type: new  Abstract: This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.   To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#24341;&#20837;&#20102;&#21452;&#23398;&#20064;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;DualAdapter&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#38754;&#21644;&#36127;&#38754;&#20004;&#26041;&#38754;&#30340;&#21452;&#36335;&#24452;&#36866;&#37197;&#65292;&#21516;&#26102;&#36827;&#34892;&#34917;&#20805;&#27491;&#21521;&#36873;&#25321;&#21644;&#36127;&#21521;&#25490;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12964</link><description>&lt;p&gt;
&#36127;&#24471;&#27491;&#65306;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#21452;&#36335;&#24452;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#24341;&#20837;&#20102;&#21452;&#23398;&#20064;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;DualAdapter&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#38754;&#21644;&#36127;&#38754;&#20004;&#26041;&#38754;&#30340;&#21452;&#36335;&#24452;&#36866;&#37197;&#65292;&#21516;&#26102;&#36827;&#34892;&#34917;&#20805;&#27491;&#21521;&#36873;&#25321;&#21644;&#36127;&#21521;&#25490;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#23637;&#31034;&#20102;&#23398;&#20064;&#24320;&#25918;&#19990;&#30028;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#24494;&#35843;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#23558;&#21452;&#23398;&#20064;&#27010;&#24565;&#24341;&#20837;&#24494;&#35843;VLMs&#20013;&#65292;&#21363;&#25105;&#20204;&#19981;&#20165;&#23398;&#20064;&#22270;&#20687;&#26159;&#20160;&#20040;&#65292;&#36824;&#23398;&#20064;&#22270;&#20687;&#19981;&#26159;&#20160;&#20040;&#12290;&#22522;&#20110;&#36825;&#19968;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DualAdapter&#26041;&#27861;&#65292;&#20351;VLMs&#33021;&#22815;&#20174;&#27491;&#38754;&#21644;&#36127;&#38754;&#20004;&#26041;&#38754;&#36827;&#34892;&#21452;&#36335;&#24452;&#36866;&#37197;&#65292;&#20165;&#20351;&#29992;&#26377;&#38480;&#30340;&#27880;&#37322;&#26679;&#26412;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;DualAdapter&#36890;&#36807;&#38024;&#23545;&#30446;&#26631;&#31867;&#21035;&#21516;&#26102;&#36827;&#34892;&#34917;&#20805;&#27491;&#21521;&#36873;&#25321;&#21644;&#36127;&#21521;&#25490;&#38500;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;VLMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#36328;&#36234;15&#20010;&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;DualAda
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12964v1 Announce Type: cross  Abstract: Recently, large-scale pre-trained Vision-Language Models (VLMs) have demonstrated great potential in learning open-world visual representations, and exhibit remarkable performance across a wide range of downstream tasks through efficient fine-tuning. In this work, we innovatively introduce the concept of dual learning into fine-tuning VLMs, i.e., we not only learn what an image is, but also what an image isn't. Building on this concept, we introduce a novel DualAdapter approach to enable dual-path adaptation of VLMs from both positive and negative perspectives with only limited annotated samples. In the inference stage, our DualAdapter performs unified predictions by simultaneously conducting complementary positive selection and negative exclusion across target classes, thereby enhancing the overall recognition accuracy of VLMs in downstream tasks. Our extensive experimental results across 15 datasets validate that the proposed DualAda
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36861;&#36394;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#36164;&#28304;&#32423;&#21035;&#30340;&#26102;&#38388;&#23545;&#40784;&#24615;&#20272;&#35745;&#26377;&#25928;&#25130;&#27490;&#26085;&#26399;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#25130;&#27490;&#26085;&#26399;&#36890;&#24120;&#19982;&#25253;&#36947;&#30340;&#19981;&#21516;&#12290;</title><link>https://arxiv.org/abs/2403.12958</link><description>&lt;p&gt;
&#25968;&#25454;&#30340;&#26102;&#25928;&#24615;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36861;&#36394;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;
&lt;/p&gt;
&lt;p&gt;
Dated Data: Tracing Knowledge Cutoffs in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36861;&#36394;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#36164;&#28304;&#32423;&#21035;&#30340;&#26102;&#38388;&#23545;&#40784;&#24615;&#20272;&#35745;&#26377;&#25928;&#25130;&#27490;&#26085;&#26399;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#25130;&#27490;&#26085;&#26399;&#36890;&#24120;&#19982;&#25253;&#36947;&#30340;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#37197;&#26377;&#22768;&#31216;&#30340;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#65292;&#21363;&#33719;&#21462;&#35757;&#32451;&#25968;&#25454;&#30340;&#26085;&#26399;&#12290;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#38656;&#35201;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26368;&#26032;&#20449;&#24687;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#35828;&#27861;&#21482;&#26159;&#34920;&#38754;&#29616;&#35937;&#65306;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#25152;&#26377;&#36164;&#28304;&#26159;&#21542;&#37117;&#20855;&#26377;&#30456;&#21516;&#30340;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#65311;&#27169;&#22411;&#23545;&#36825;&#20123;&#23376;&#38598;&#30340;&#23637;&#31034;&#30693;&#35782;&#26159;&#21542;&#19982;&#23427;&#20204;&#30340;&#25130;&#27490;&#26085;&#26399;&#23494;&#20999;&#30456;&#20851;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#26377;&#25928;&#25130;&#27490;&#26085;&#26399;&#30340;&#27010;&#24565;&#12290;&#36825;&#19982;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#32773;&#25253;&#21578;&#30340;&#25130;&#27490;&#26085;&#26399;&#19981;&#21516;&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#23376;&#36164;&#28304;&#21644;&#20027;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#27979;&#25968;&#25454;&#29256;&#26412;&#20043;&#38388;&#30340;&#26102;&#38388;&#23545;&#40784;&#24615;&#26469;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#22312;&#36164;&#28304;&#32423;&#21035;&#30340;&#26377;&#25928;&#25130;&#27490;&#26085;&#26399;&#12290;&#36890;&#36807;&#36825;&#39033;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#26377;&#25928;&#25130;&#27490;&#26085;&#26399;&#36890;&#24120;&#19982;&#25253;&#21578;&#30340;&#25130;&#27490;&#26085;&#26399;&#19981;&#21516;&#12290;&#20026;&#20102;&#20102;&#35299;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#30452;&#25509;&#30340;&#22823;&#35268;&#27169;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12958v1 Announce Type: new  Abstract: Released Large Language Models (LLMs) are often paired with a claimed knowledge cutoff date, or the dates at which training data was gathered. Such information is crucial for applications where the LLM must provide up to date information. However, this statement only scratches the surface: do all resources in the training data share the same knowledge cutoff date? Does the model's demonstrated knowledge for these subsets closely align to their cutoff dates? In this work, we define the notion of an effective cutoff. This is distinct from the LLM designer reported cutoff and applies separately to sub-resources and topics. We propose a simple approach to estimate effective cutoffs on the resource-level temporal alignment of an LLM by probing across versions of the data. Using this analysis, we find that effective cutoffs often differ from reported cutoffs. To understand the root cause of this observation, we conduct a direct large-scale ana
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#33258;&#21160;&#20174;&#33521;&#22269;&#38599;&#20323;&#27861;&#24237;&#26696;&#20363;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#25163;&#21160;&#39564;&#35777;&#30830;&#20445;&#20102;&#25552;&#21462;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;</title><link>https://arxiv.org/abs/2403.12936</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25552;&#21462;&#38599;&#20323;&#27861;&#24237;&#35009;&#20915;&#20013;&#30340;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#33258;&#21160;&#20174;&#33521;&#22269;&#38599;&#20323;&#27861;&#24237;&#26696;&#20363;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#25163;&#21160;&#39564;&#35777;&#30830;&#20445;&#20102;&#25552;&#21462;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24237;&#35760;&#24405;&#21644;&#21028;&#20915;&#26159;&#27861;&#24459;&#30693;&#35782;&#30340;&#20016;&#23500;&#36164;&#28304;&#65292;&#35814;&#32454;&#25551;&#36848;&#26696;&#20214;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#21496;&#27861;&#20915;&#23450;&#32972;&#21518;&#30340;&#29702;&#30001;&#12290;&#20174;&#36825;&#20123;&#25991;&#20214;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#25552;&#20379;&#20102;&#26696;&#20214;&#30340;&#31616;&#26126;&#27010;&#36848;&#65292;&#23545;&#20110;&#27861;&#24459;&#19987;&#23478;&#21644;&#20844;&#20247;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#33258;&#21160;&#20449;&#24687;&#25552;&#21462;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#34892;&#21644;&#39640;&#25928;&#12290;&#26412;&#25991;&#23545;GPT-4&#65288;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#22312;&#20174;&#33521;&#22269;&#38599;&#20323;&#27861;&#24237;&#65288;UKET&#65289;&#26696;&#20363;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#25163;&#21160;&#39564;&#35777;&#36807;&#31243;&#23545;GPT-4&#22312;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#26041;&#38754;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32454;&#33268;&#35780;&#20272;&#65292;&#20197;&#30830;&#20445;&#25552;&#21462;&#30340;&#25968;&#25454;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22260;&#32469;&#20004;&#20010;&#20027;&#35201;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#23637;&#24320;&#65306;&#31532;&#19968;&#20010;&#20219;&#21153;&#28041;&#21450;&#23545;&#23545;&#27861;&#24459;&#19987;&#23478;&#21644;&#20844;&#20247;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#20843;&#20010;&#20851;&#38190;&#26041;&#38754;&#36827;&#34892;&#36890;&#29992;&#30340;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12936v1 Announce Type: cross  Abstract: Court transcripts and judgments are rich repositories of legal knowledge, detailing the intricacies of cases and the rationale behind judicial decisions. The extraction of key information from these documents provides a concise overview of a case, crucial for both legal experts and the public. With the advent of large language models (LLMs), automatic information extraction has become increasingly feasible and efficient. This paper presents a comprehensive study on the application of GPT-4, a large language model, for automatic information extraction from UK Employment Tribunal (UKET) cases. We meticulously evaluated GPT-4's performance in extracting critical information with a manual verification process to ensure the accuracy and relevance of the extracted data. Our research is structured around two primary extraction tasks: the first involves a general extraction of eight key aspects that hold significance for both legal specialists
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30340;&#21019;&#26032;&#36129;&#29486;&#22312;&#20110;&#23558;&#20915;&#31574;&#26641;&#26694;&#26550;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#21462;&#27861;&#35268;&#30340;&#20934;&#30830;&#24615;&#36798;&#21040;85%&#33267;90%&#12290;</title><link>https://arxiv.org/abs/2403.12924</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#33021;&#28304;&#25919;&#31574;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Supporting Energy Policy Research with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#21019;&#26032;&#36129;&#29486;&#22312;&#20110;&#23558;&#20915;&#31574;&#26641;&#26694;&#26550;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#21462;&#27861;&#35268;&#30340;&#20934;&#30830;&#24615;&#36798;&#21040;85%&#33267;90%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#21487;&#20877;&#29983;&#33021;&#28304;&#24320;&#21457;&#30340;&#26368;&#36817;&#22686;&#38271;&#20276;&#38543;&#30528;&#21487;&#20877;&#29983;&#33021;&#28304;&#29992;&#22320;&#26465;&#20363;&#30340;&#21516;&#26102;&#28608;&#22686;&#12290;&#36825;&#20123;&#21306;&#20301;&#27861;&#35268;&#22312;&#35268;&#23450;&#23545;&#20110;&#23454;&#29616;&#20302;&#30899;&#33021;&#28304;&#26410;&#26469;&#33267;&#20851;&#37325;&#35201;&#30340;&#39118;&#33021;&#21644;&#22826;&#38451;&#33021;&#36164;&#28304;&#30340;&#25670;&#25918;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#26377;&#25928;&#35775;&#38382;&#21644;&#31649;&#29702;&#29992;&#22320;&#27861;&#35268;&#25968;&#25454;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#32654;&#22269;&#22269;&#23478;&#21487;&#20877;&#29983;&#33021;&#28304;&#23454;&#39564;&#23460;&#65288;NREL&#65289;&#26368;&#36817;&#25512;&#20986;&#20102;&#19968;&#20010;&#20844;&#24320;&#30340;&#39118;&#33021;&#21644;&#22826;&#38451;&#33021;&#36873;&#22336;&#25968;&#25454;&#24211;&#65292;&#20197;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#25552;&#21462;&#36825;&#20123;&#36873;&#22336;&#27861;&#35268;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#35813;&#25968;&#25454;&#24211;&#33021;&#22815;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#33021;&#28304;&#25919;&#31574;&#26684;&#23616;&#20013;&#20445;&#25345;&#20934;&#30830;&#30340;&#26368;&#26032;&#20449;&#24687;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#39062;&#25104;&#26524;&#26159;&#23558;&#20915;&#31574;&#26641;&#26694;&#26550;&#19982;LLMs&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;85%&#33267;90%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12924v1 Announce Type: new  Abstract: The recent growth in renewable energy development in the United States has been accompanied by a simultaneous surge in renewable energy siting ordinances. These zoning laws play a critical role in dictating the placement of wind and solar resources that are critical for achieving low-carbon energy futures. In this context, efficient access to and management of siting ordinance data becomes imperative. The National Renewable Energy Laboratory (NREL) recently introduced a public wind and solar siting database to fill this need. This paper presents a method for harnessing Large Language Models (LLMs) to automate the extraction of these siting ordinances from legal documents, enabling this database to maintain accurate up-to-date information in the rapidly changing energy policy landscape. A novel contribution of this research is the integration of a decision tree framework with LLMs. Our results show that this approach is 85 to 90% accurate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#26435;&#37325;&#28151;&#21512;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20302;&#36164;&#28304;&#25991;&#26412;&#19978;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26102;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.12918</link><description>&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#27867;&#21270;&#21644;&#31283;&#23450;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#26435;&#37325;&#28151;&#21512;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20302;&#36164;&#28304;&#25991;&#26412;&#19978;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26102;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;PLMs&#20250;&#38754;&#20020;&#35832;&#22914;&#19981;&#31283;&#23450;&#24615;&#21644;&#36807;&#25311;&#21512;&#31561;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#24494;&#35843;&#31574;&#30053;&#36873;&#25321;&#30340;&#23376;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20313;&#26435;&#37325;&#22266;&#23450;&#20026;&#39044;&#35757;&#32451;&#26435;&#37325;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#27425;&#20248;&#30340;&#23376;&#32593;&#32476;&#36873;&#25321;&#26631;&#20934;&#65292;&#23548;&#33268;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#26435;&#37325;&#28151;&#21512;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#24494;&#35843;PLMs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;&#32593;&#32476;&#26435;&#37325;&#34920;&#31034;&#20026;&#20219;&#21153;&#29305;&#23450;&#26435;&#37325;&#21644;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#28151;&#21512;&#65292;&#30001;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#21442;&#25968;&#25511;&#21046;&#65292;&#25552;&#20379;&#23545;&#23376;&#32593;&#32476;&#36873;&#25321;&#30340;&#26356;&#31934;&#32454;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20004;&#20010;&#21333;&#29420;&#25286;&#20998;&#19978;&#20351;&#29992;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#65288;BLO&#65289;&#30340;&#26694;&#26550;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12918v1 Announce Type: cross  Abstract: Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle these issues by finetuning a strategically chosen subnetwork on a downstream task, while keeping the remaining weights fixed to the pretrained weights. However, they rely on a suboptimal criteria for sub-network selection, leading to suboptimal solutions. To address these limitations, we propose a regularization method based on attention-guided weight mixup for finetuning PLMs. Our approach represents each network weight as a mixup of task-specific weight and pretrained weight, controlled by a learnable attention parameter, providing finer control over sub-network selection. Furthermore, we employ a bi-level optimization (BLO) based framework on two separate splits of the training dataset, improving ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Sprout&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#25351;&#20196;&#30340;&#27010;&#24565;&#65292;&#24179;&#34913;&#20102;&#29983;&#24577;&#21487;&#25345;&#32493;&#24615;&#21644;&#39640;&#36136;&#37327;&#29983;&#25104;&#32467;&#26524;&#20043;&#38388;&#30340;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#26381;&#21153;&#30899;&#25490;&#25918;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2403.12900</link><description>&lt;p&gt;
&#26397;&#21521;&#21487;&#25345;&#32493;&#30340;GenAI&#65306;&#20351;&#29992;&#29983;&#25104;&#25351;&#20196;&#23454;&#29616;&#30899;&#21451;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sprout&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#25351;&#20196;&#30340;&#27010;&#24565;&#65292;&#24179;&#34913;&#20102;&#29983;&#24577;&#21487;&#25345;&#32493;&#24615;&#21644;&#39640;&#36136;&#37327;&#29983;&#25104;&#32467;&#26524;&#20043;&#38388;&#30340;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#26381;&#21153;&#30899;&#25490;&#25918;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#24212;&#29992;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#24341;&#36215;&#20102;&#29615;&#22659;&#26041;&#38754;&#30340;&#37325;&#35201;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#20113;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#30340;&#30899;&#25490;&#25918;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Sprout&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#26029;&#26381;&#21153;&#30340;&#30899;&#36275;&#36857;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;Sprout&#21033;&#29992;&#21019;&#26032;&#27010;&#24565;&#8220;&#29983;&#25104;&#25351;&#20196;&#8221;&#26469;&#24341;&#23548;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#65292;&#20174;&#32780;&#22686;&#24378;&#30899;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31934;&#24515;&#24179;&#34913;&#20102;&#23545;&#29983;&#24577;&#21487;&#25345;&#32493;&#24615;&#21644;&#39640;&#36136;&#37327;&#29983;&#25104;&#25104;&#26524;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#25351;&#20196;&#20248;&#21270;&#22120;&#26469;&#23545;&#29992;&#25143;&#25552;&#31034;&#36827;&#34892;&#29983;&#25104;&#25351;&#20196;&#30340;&#25112;&#30053;&#20998;&#37197;&#21644;&#19968;&#20010;&#21407;&#21019;&#30340;&#31163;&#32447;&#36136;&#37327;&#35780;&#20272;&#22120;&#65292;Sprout&#22312;&#23454;&#38469;&#35780;&#20272;&#20013;&#26174;&#33879;&#20943;&#23569;&#20102;40%&#20197;&#19978;&#30340;&#30899;&#25490;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12900v1 Announce Type: cross  Abstract: The rapid advancement of Generative Artificial Intelligence (GenAI) across diverse sectors raises significant environmental concerns, notably the carbon emissions from their cloud and high performance computing (HPC) infrastructure. This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative Large Language Model (LLM) inference services. Sprout leverages the innovative concept of "generation directives" to guide the autoregressive generation process, thereby enhancing carbon efficiency. Our proposed method meticulously balances the need for ecological sustainability with the demand for high-quality generation outcomes. Employing a directive optimizer for the strategic assignment of generation directives to user prompts and an original offline quality evaluator, Sprout demonstrates a significant reduction in carbon emissions by over 40% in real-world evaluations u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Agent-FLAN&#65292;&#36890;&#36807;&#31934;&#24515;&#20998;&#35299;&#21644;&#37325;&#26032;&#35774;&#35745;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#20351;&#24471;Llama2-7B&#22312;&#21508;&#31181;&#20195;&#29702;&#35780;&#20272;&#20013;&#36229;&#36807;&#20808;&#21069;&#30340;&#26368;&#20339;&#24037;&#20316;3.5&#65285;</title><link>https://arxiv.org/abs/2403.12881</link><description>&lt;p&gt;
Agent-FLAN: &#20026;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#20195;&#29702;&#35843;&#33410;&#35774;&#35745;&#25968;&#25454;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Agent-FLAN&#65292;&#36890;&#36807;&#31934;&#24515;&#20998;&#35299;&#21644;&#37325;&#26032;&#35774;&#35745;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#20351;&#24471;Llama2-7B&#22312;&#21508;&#31181;&#20195;&#29702;&#35780;&#20272;&#20013;&#36229;&#36807;&#20808;&#21069;&#30340;&#26368;&#20339;&#24037;&#20316;3.5&#65285;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#28982;&#32780;&#65292;&#19982;&#22522;&#20110;API&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20204;&#22312;&#20805;&#24403;&#20195;&#29702;&#26102;&#20173;&#28982;&#26126;&#26174;&#36874;&#33394;&#12290;&#22914;&#20309;&#23558;&#20195;&#29702;&#33021;&#21147;&#25972;&#21512;&#21040;&#19968;&#33324;&#30340;LLMs&#20013;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#19988;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#65288;1&#65289;&#24403;&#21069;&#30340;&#20195;&#29702;&#35757;&#32451;&#35821;&#26009;&#24211;&#19982;&#20854;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#26126;&#26174;&#19981;&#21516;&#65292;&#21516;&#26102;&#28041;&#21450;&#36981;&#24490;&#26684;&#24335;&#21644;&#20195;&#29702;&#25512;&#29702;&#65307;&#65288;2&#65289;LLMs&#22312;&#20195;&#29702;&#20219;&#21153;&#25152;&#38656;&#30340;&#33021;&#21147;&#19978;&#23637;&#29616;&#20986;&#19981;&#21516;&#30340;&#23398;&#20064;&#36895;&#24230;&#65307;&#20197;&#21450;&#65288;3&#65289;&#36890;&#36807;&#24341;&#20837;&#24187;&#35273;&#26469;&#25552;&#39640;&#20195;&#29702;&#33021;&#21147;&#30340;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#21103;&#20316;&#29992;&#12290;&#22522;&#20110;&#20197;&#19978;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;Agent-FLAN&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20026;&#20195;&#29702;&#35843;&#35797;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#35757;&#32451;&#35821;&#26009;&#24211;&#36827;&#34892;&#31934;&#24515;&#30340;&#20998;&#35299;&#21644;&#37325;&#26032;&#35774;&#35745;&#65292;Agent-FLAN&#20351;&#24471;Llama2-7B&#22312;&#21508;&#31181;&#20195;&#29702;&#35780;&#20272;&#20013;&#36229;&#36807;&#20808;&#21069;&#30340;&#26368;&#20339;&#24037;&#20316;3.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12881v1 Announce Type: new  Abstract: Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents. Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\% across various agent evaluation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#35748;&#35782;&#35770;&#25972;&#20307;&#20027;&#20041;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#21019;&#36896;&#31185;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35782;&#35770;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.12862</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35782;&#35770;&#65306;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25972;&#20307;&#30693;&#35782;&#65311;
&lt;/p&gt;
&lt;p&gt;
Epistemology of Language Models: Do Language Models Have Holistic Knowledge?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#35748;&#35782;&#35770;&#25972;&#20307;&#20027;&#20041;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#21019;&#36896;&#31185;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35782;&#35770;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20174;&#35748;&#35782;&#35770;&#25972;&#20307;&#20027;&#20041;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#25506;&#35752;LLMs&#26159;&#21542;&#34920;&#29616;&#20986;&#19982;&#35748;&#35782;&#35770;&#25972;&#20307;&#20027;&#20041;&#19968;&#33268;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#29305;&#24449;&#34920;&#26126;&#26680;&#24515;&#30693;&#35782;&#65292;&#22914;&#19968;&#33324;&#31185;&#23398;&#30693;&#35782;&#65292;&#27599;&#20010;&#37117;&#21457;&#25381;&#30528;&#29305;&#23450;&#30340;&#20316;&#29992;&#65292;&#20316;&#20026;&#25105;&#20204;&#30693;&#35782;&#31995;&#32479;&#30340;&#22522;&#30784;&#24182;&#38590;&#20197;&#20462;&#25913;&#12290;&#20026;&#20102;&#35780;&#20272;&#19982;&#25972;&#20307;&#20027;&#20041;&#30456;&#20851;&#30340;&#36825;&#20123;&#29305;&#24449;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#31185;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#20219;&#21153;&#26816;&#39564;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35782;&#35770;&#65306;Abduction&#65292;Revision&#21644;Argument Generation&#12290;&#22312;&#32465;&#26550;&#20219;&#21153;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#20102;&#24773;&#22659;&#65292;&#21516;&#26102;&#36991;&#20813;&#20462;&#25913;&#26680;&#24515;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#20182;&#20219;&#21153;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#34987;&#25581;&#31034;&#20986;&#26080;&#27861;&#21306;&#20998;&#26680;&#24515;&#21644;&#21608;&#36793;&#30693;&#35782;&#65292;&#26174;&#31034;&#20986;&#19982;&#25972;&#20307;&#30693;&#35782;&#21407;&#21017;&#30340;&#19981;&#23436;&#20840;&#22865;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12862v1 Announce Type: new  Abstract: This paper investigates the inherent knowledge in language models from the perspective of epistemological holism. The purpose of this paper is to explore whether LLMs exhibit characteristics consistent with epistemological holism. These characteristics suggest that core knowledge, such as general scientific knowledge, each plays a specific role, serving as the foundation of our knowledge system and being difficult to revise. To assess these traits related to holism, we created a scientific reasoning dataset and examined the epistemology of language models through three tasks: Abduction, Revision, and Argument Generation. In the abduction task, the language models explained situations while avoiding revising the core knowledge. However, in other tasks, the language models were revealed not to distinguish between core and peripheral knowledge, showing an incomplete alignment with holistic knowledge principles.
&lt;/p&gt;</description></item><item><title>&#22810;&#35821;&#21644;&#21333;&#35821;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#23384;&#22312;&#24046;&#24322;&#65292;&#23454;&#39564;&#35777;&#26126;&#22810;&#35821;&#27169;&#22411;&#36234;&#22823;&#65292;FA&#30456;&#23545;&#20110;&#21333;&#35821;&#27169;&#22411;&#26469;&#35828;&#36234;&#19981;&#24544;&#23454;&#12290;</title><link>https://arxiv.org/abs/2403.12809</link><description>&lt;p&gt;
&#27604;&#36739;&#22810;&#35821;&#21644;&#21333;&#35821;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#35299;&#37322;&#24544;&#23454;&#24230;
&lt;/p&gt;
&lt;p&gt;
Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12809
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#21644;&#21333;&#35821;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#23384;&#22312;&#24046;&#24322;&#65292;&#23454;&#39564;&#35777;&#26126;&#22810;&#35821;&#27169;&#22411;&#36234;&#22823;&#65292;FA&#30456;&#23545;&#20110;&#21333;&#35821;&#27169;&#22411;&#26469;&#35828;&#36234;&#19981;&#24544;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#20174;&#19994;&#32773;&#19981;&#20165;&#26088;&#22312;&#26368;&#22823;&#21270;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#23547;&#27714;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24544;&#23454;&#35299;&#37322;&#12290;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#32473;&#20986;&#30340;&#29702;&#30001;&#21644;&#37325;&#35201;&#24615;&#20998;&#24067;&#25581;&#31034;&#20102;&#36755;&#20837;&#30340;&#19981;&#21516;&#37096;&#20998;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#24544;&#23454;&#24230;&#65292;&#20027;&#35201;&#26159;&#22312;&#21333;&#35821;&#33521;&#35821;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22810;&#35821;&#21644;&#21333;&#35821;&#27169;&#22411;&#20043;&#38388;&#30340;FA&#24544;&#23454;&#24230;&#24046;&#24322;&#23578;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;FA&#30340;&#24544;&#23454;&#24230;&#22312;&#22810;&#35821;&#21644;&#21333;&#35821;&#27169;&#22411;&#20043;&#38388;&#26377;&#25152;&#21464;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22810;&#35821;&#27169;&#22411;&#36234;&#22823;&#65292;FA&#30456;&#23545;&#20110;&#20854;&#23545;&#24212;&#30340;&#21333;&#35821;&#27169;&#22411;&#26469;&#35828;&#36234;&#19981;&#24544;&#23454;&#12290;&#25105;&#20204;&#30340;&#36827;&#19968;&#27493;&#20998;&#26512;&#26174;&#31034;&#65292;&#24544;&#23454;&#24230;&#30340;&#24046;&#24322;&#26159;&#28508;&#22312;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12809v1 Announce Type: cross  Abstract: In many real natural language processing application scenarios, practitioners not only aim to maximize predictive performance but also seek faithful explanations for the model predictions. Rationales and importance distribution given by feature attribution methods (FAs) provide insights into how different parts of the input contribute to a prediction. Previous studies have explored how different factors affect faithfulness, mainly in the context of monolingual English models. On the other hand, the differences in FA faithfulness between multilingual and monolingual models have yet to be explored. Our extensive experiments, covering five languages and five popular FAs, show that FA faithfulness varies between multilingual and monolingual models. We find that the larger the multilingual model, the less faithful the FAs are compared to its counterpart monolingual models.Our further analysis shows that the faithfulness disparity is potenti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24773;&#22659;&#32858;&#21512;&#30340;&#24773;&#22659;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#31995;&#32479;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.12805</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#32858;&#21512;&#23454;&#29616;&#24773;&#22659;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Contextual Moral Value Alignment Through Context-Based Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24773;&#22659;&#32858;&#21512;&#30340;&#24773;&#22659;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#31995;&#32479;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20215;&#20540;&#23545;&#40784;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#19968;&#20010;&#22797;&#26434;&#32780;&#25345;&#32493;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#65292;&#23558;&#22810;&#20010;&#29420;&#31435;&#35757;&#32451;&#30340;&#23545;&#35805;&#20195;&#29702;&#25972;&#21512;&#20026;&#19968;&#20010;&#32479;&#19968;&#31995;&#32479;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#24182;&#19982;&#22810;&#20010;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#65292;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24773;&#22659;&#32858;&#21512;&#30340;&#24773;&#22659;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#31995;&#32479;&#12290;&#22312;&#35813;&#31995;&#32479;&#20013;&#65292;&#32858;&#21512;&#34987;&#23450;&#20041;&#20026;&#25972;&#21512;&#36866;&#21512;&#22238;&#22797;&#29992;&#25143;&#36755;&#20837;&#30340;LLM&#21709;&#24212;&#23376;&#38598;&#30340;&#36807;&#31243;&#65292;&#32771;&#34385;&#20102;&#20174;&#29992;&#25143;&#36755;&#20837;&#20013;&#25552;&#21462;&#20986;&#30340;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12805v1 Announce Type: new  Abstract: Developing value-aligned AI agents is a complex undertaking and an ongoing challenge in the field of AI. Specifically within the domain of Large Language Models (LLMs), the capability to consolidate multiple independently trained dialogue agents, each aligned with a distinct moral value, into a unified system that can adapt to and be aligned with multiple moral values is of paramount importance. In this paper, we propose a system that does contextual moral value alignment based on contextual aggregation. Here, aggregation is defined as the process of integrating a subset of LLM responses that are best suited to respond to a user input, taking into account features extracted from the user's input. The proposed system shows better results in term of alignment to human value compared to the state of the art.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#25991;&#26723;&#25130;&#26029;&#21644;&#25688;&#35201;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#25688;&#35201;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#32988;&#36807;&#25130;&#26029;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#26368;&#20339;&#31574;&#30053;&#20026;&#21462;&#25991;&#26723;&#30340;&#24320;&#22836;&#12290;</title><link>https://arxiv.org/abs/2403.12799</link><description>&lt;p&gt;
&#25506;&#31350;BERT&#20013;&#30340;&#25991;&#26412;&#32553;&#30701;&#31574;&#30053;&#65306;&#25130;&#26029; vs &#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Investigating Text Shortening Strategy in BERT: Truncation vs Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#25991;&#26723;&#25130;&#26029;&#21644;&#25688;&#35201;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#25688;&#35201;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#32988;&#36807;&#25130;&#26029;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#26368;&#20339;&#31574;&#30053;&#20026;&#21462;&#25991;&#26723;&#30340;&#24320;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#24182;&#34892;&#24615;&#20197;&#20854;&#36755;&#20837;&#26368;&#22823;&#38271;&#24230;&#20026;&#20195;&#20215;&#12290;&#19968;&#20123;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#20294;&#20854;&#20013;&#26410;&#26377;&#25253;&#21578;&#25688;&#35201;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25991;&#26723;&#25130;&#26029;&#21644;&#25688;&#35201;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#27599;&#31181;&#26041;&#27861;&#37117;&#24471;&#21040;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#21464;&#20307;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#19982;&#20840;&#25991;&#34920;&#29616;&#20043;&#38388;&#30340;&#25509;&#36817;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#21360;&#23612;&#26032;&#38395;&#25991;&#31456;(IndoSum)&#30340;&#25688;&#35201;&#20219;&#21153;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#20998;&#31867;&#27979;&#35797;&#12290;&#26412;&#30740;&#31350;&#26174;&#31034;&#20986;&#25688;&#35201;&#32988;&#36807;&#20102;&#22823;&#37096;&#20998;&#25130;&#26029;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#21482;&#36755;&#32473;&#20102;&#19968;&#20010;&#12290;&#26412;&#30740;&#31350;&#24471;&#21040;&#30340;&#26368;&#20339;&#31574;&#30053;&#26159;&#21462;&#25991;&#26723;&#30340;&#24320;&#22836;&#12290;&#20854;&#27425;&#26159;&#25277;&#21462;&#24335;&#25688;&#35201;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#32467;&#26524;&#30340;&#21407;&#22240;&#65292;&#24341;&#39046;&#30528;&#23545;&#20110;&#21033;&#29992;&#28508;&#21147;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24320;&#25299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12799v1 Announce Type: cross  Abstract: The parallelism of Transformer-based models comes at the cost of their input max-length. Some studies proposed methods to overcome this limitation, but none of them reported the effectiveness of summarization as an alternative. In this study, we investigate the performance of document truncation and summarization in text classification tasks. Each of the two was investigated with several variations. This study also investigated how close their performances are to the performance of full-text. We used a dataset of summarization tasks based on Indonesian news articles (IndoSum) to do classification tests. This study shows how the summaries outperform the majority of truncation method variations and lose to only one. The best strategy obtained in this study is taking the head of the document. The second is extractive summarization. This study explains what happened to the result, leading to further research in order to exploit the potenti
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#25968;&#25454;&#31649;&#29702;&#27969;&#27700;&#32447;CLEAR&#65288;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;LLM&#35780;&#20272;&#21644;&#32416;&#27491;&#65289;&#29992;&#20110;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#21487;&#19982;&#20219;&#20309;LLM&#21644;&#24494;&#35843;&#31243;&#24207;&#19968;&#36215;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.12776</link><description>&lt;p&gt;
&#29992;&#20110;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#33258;&#21160;&#21270;&#25968;&#25454;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Automated Data Curation for Robust Language Model Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12776
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#25968;&#25454;&#31649;&#29702;&#27969;&#27700;&#32447;CLEAR&#65288;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;LLM&#35780;&#20272;&#21644;&#32416;&#27491;&#65289;&#29992;&#20110;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#21487;&#19982;&#20219;&#20309;LLM&#21644;&#24494;&#35843;&#31243;&#24207;&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#20294;&#23545;&#20110;&#19987;&#38376;&#30340;&#20219;&#21153;/&#39046;&#22495;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#20135;&#29983;&#20934;&#30830;&#25110;&#26684;&#24335;&#33391;&#22909;&#21709;&#24212;&#30340;&#29305;&#23450;&#33021;&#21147;&#12290;&#30417;&#30563;&#24494;&#35843;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#22312;&#20855;&#26377;&#30446;&#26631;&#21709;&#24212;&#30340;&#31034;&#20363;&#25552;&#31034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19987;&#38376;&#21270;&#24494;&#35843;&#65292;&#20294;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24448;&#24448;&#23384;&#22312;&#22122;&#22768;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#24494;&#35843;&#31639;&#27861;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#8220;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;AI&#8221;&#35270;&#35282;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#30740;&#31350;&#22914;&#20309;&#8220;&#31995;&#32479;&#22320;&#8221;&#31579;&#36873;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#36890;&#36807;&#8220;&#20219;&#20309;&#8221;&#24494;&#35843;&#31639;&#27861;&#20135;&#29983;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12776v1 Announce Type: new  Abstract: Large Language Models have become the de facto approach to sequence-to-sequence text generation tasks, but for specialized tasks/domains, a pretrained LLM lacks specific capabilities to produce accurate or well-formatted responses. Supervised fine-tuning specializes a LLM by training it on dataset of example prompts with target responses, but real-world data tends to be noisy. While many fine-tuning algorithms exist, here we consider a \emph{data-centric AI} perspective on LLM fine-tuning, studying how to \emph{systematically} curate the training dataset to improve the LLM produced via \emph{any} fine-tuning algorithm.   We introduce an automated data curation pipeline CLEAR (Confidence-based LLM Evaluation And Rectification) for instruction tuning datasets, that can be used with any LLM and fine-tuning procedure. CLEAR estimates which training data is low-quality and either filters or corrects it. Automatically identifying which data to
&lt;/p&gt;</description></item><item><title>NovelQA&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38271;&#25991;&#26412;&#19978;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#33521;&#25991;&#23567;&#35828;&#26500;&#24314;&#65292;&#25552;&#20379;&#20102;&#22797;&#26434;&#24615;&#12289;&#38271;&#24230;&#21644;&#21465;&#36848;&#36830;&#36143;&#24615;&#30340;&#29420;&#29305;&#32452;&#21512;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#28145;&#24230;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.12766</link><description>&lt;p&gt;
NovelQA&#65306;&#29992;&#20110;&#38271;&#36317;&#31163;&#23567;&#35828;&#38382;&#31572;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
NovelQA: A Benchmark for Long-Range Novel Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12766
&lt;/p&gt;
&lt;p&gt;
NovelQA&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38271;&#25991;&#26412;&#19978;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#33521;&#25991;&#23567;&#35828;&#26500;&#24314;&#65292;&#25552;&#20379;&#20102;&#22797;&#26434;&#24615;&#12289;&#38271;&#24230;&#21644;&#21465;&#36848;&#36830;&#36143;&#24615;&#30340;&#29420;&#29305;&#32452;&#21512;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#28145;&#24230;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#20837;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26032;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#29702;&#35299;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#20449;&#24687;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#38271;&#25991;&#26412;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NovelQA&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27979;&#35797;&#20855;&#26377;&#25193;&#23637;&#25991;&#26412;&#30340;LLM&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;NovelQA&#30001;&#33521;&#25991;&#23567;&#35828;&#26500;&#24314;&#65292;&#25552;&#20379;&#20102;&#22797;&#26434;&#24615;&#12289;&#38271;&#24230;&#21644;&#21465;&#36848;&#36830;&#36143;&#24615;&#30340;&#29420;&#29305;&#32452;&#21512;&#65292;&#20351;&#20854;&#25104;&#20026;&#35780;&#20272;LLM&#20013;&#28145;&#24230;&#25991;&#26412;&#29702;&#35299;&#30340;&#29702;&#24819;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NovelQA&#30340;&#35774;&#35745;&#19982;&#26500;&#24314;&#65292;&#31361;&#20986;&#20102;&#20854;&#25163;&#21160;&#27880;&#37322;&#21644;&#22810;&#26679;&#30340;&#38382;&#39064;&#31867;&#22411;&#12290;&#25105;&#20204;&#22312;NovelQA&#19978;&#23545;&#38271;&#25991;&#26412;LLM&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#35265;&#35299;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#22810;&#36339;&#25512;&#29702;&#12289;&#32454;&#33410;&#23548;&#21521;&#31561;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12766v1 Announce Type: new  Abstract: The rapid advancement of Large Language Models (LLMs) has introduced a new frontier in natural language processing, particularly in understanding and processing long-context information. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark specifically designed to test the capabilities of LLMs with extended texts. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper presents the design and construction of NovelQA, highlighting its manual annotation, and diverse question types. Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-orien
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#24503;&#35821;&#31532;&#19968;&#20010;&#26041;&#35328;NER&#25968;&#25454;&#38598;BarNER&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#24052;&#20240;&#21033;&#20122;&#26041;&#35328;&#25968;&#25454;&#19978;&#30340;&#20840;&#38754;NER&#32467;&#26524;&#65292;&#34920;&#26126;&#20174;&#22823;&#22411;&#24503;&#35821;NER&#23376;&#25968;&#25454;&#38598;&#20013;&#27762;&#21462;&#30693;&#35782;&#21487;&#20197;&#25913;&#36827;&#24052;&#20240;&#21033;&#20122;&#25968;&#25454;&#30340;&#34920;&#29616;&#65292;&#32780;&#22312;&#24052;&#20240;&#21033;&#20122;&#36827;&#34892;&#35757;&#32451;&#23545;&#26631;&#24535;&#24615;&#30340;&#24503;&#35821;&#25968;&#25454;&#20063;&#26377;&#25152;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2403.12749</link><description>&lt;p&gt;
&#36776;&#35782;&#24052;&#20240;&#21033;&#20122;&#26041;&#35328;&#25968;&#25454;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Sebastian, Basti, Wastl?! Recognizing Named Entities in Bavarian Dialectal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12749
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#24503;&#35821;&#31532;&#19968;&#20010;&#26041;&#35328;NER&#25968;&#25454;&#38598;BarNER&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#24052;&#20240;&#21033;&#20122;&#26041;&#35328;&#25968;&#25454;&#19978;&#30340;&#20840;&#38754;NER&#32467;&#26524;&#65292;&#34920;&#26126;&#20174;&#22823;&#22411;&#24503;&#35821;NER&#23376;&#25968;&#25454;&#38598;&#20013;&#27762;&#21462;&#30693;&#35782;&#21487;&#20197;&#25913;&#36827;&#24052;&#20240;&#21033;&#20122;&#25968;&#25454;&#30340;&#34920;&#29616;&#65292;&#32780;&#22312;&#24052;&#20240;&#21033;&#20122;&#36827;&#34892;&#35757;&#32451;&#23545;&#26631;&#24535;&#24615;&#30340;&#24503;&#35821;&#25968;&#25454;&#20063;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20294;&#23545;&#26041;&#35328;&#30340;&#27880;&#37322;&#36164;&#28304;&#24456;&#23569;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24503;&#35821;&#31532;&#19968;&#20010;&#26041;&#35328;NER&#25968;&#25454;&#38598;BarNER&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#24052;&#20240;&#21033;&#20122;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#65288;bar-wiki&#65289;&#21644;&#25512;&#25991;&#65288;bar-tweet&#65289;&#19978;&#26631;&#27880;&#20102;161K&#20010;&#26631;&#35760;&#65292;&#20351;&#29992;&#20102;&#20174;&#24503;&#35821;CoNLL 2006&#21644;GermEval&#25913;&#32534;&#30340;&#27169;&#24335;&#12290;&#24052;&#20240;&#21033;&#20122;&#26041;&#35328;&#22312;&#35789;&#27719;&#20998;&#24067;&#12289;&#21477;&#27861;&#26500;&#36896;&#21644;&#23454;&#20307;&#20449;&#24687;&#26041;&#38754;&#19982;&#26631;&#20934;&#24503;&#35821;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24052;&#20240;&#21033;&#20122;&#21644;&#19977;&#20010;&#24503;&#22269;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39046;&#22495;&#20869;&#12289;&#39046;&#22495;&#38388;&#12289;&#39034;&#24207;&#21644;&#32852;&#21512;&#23454;&#39564;&#65292;&#24182;&#39318;&#27425;&#20840;&#38754;&#23637;&#31034;&#20102;&#24052;&#20240;&#21033;&#20122;&#30340;NER&#32467;&#26524;&#12290;&#20174;&#36739;&#22823;&#30340;&#24503;&#35821;NER&#65288;&#23376;&#65289;&#25968;&#25454;&#38598;&#20013;&#27762;&#21462;&#30693;&#35782;&#26174;&#30528;&#25913;&#36827;&#20102;bar-wiki&#65292;&#31245;&#24494;&#25913;&#36827;&#20102;bar-tweet&#12290;&#21453;&#20043;&#65292;&#39318;&#20808;&#22312;&#24052;&#20240;&#21033;&#20122;&#36827;&#34892;&#35757;&#32451;&#23545;&#26631;&#24535;&#24615;&#30340;&#24503;&#35821;CoNLL 2006&#35821;&#26009;&#24211;&#26377;&#25152;&#24110;&#21161;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#24052;&#20240;&#21033;&#20122;&#25512;&#25991;&#19978;&#20351;&#29992;&#40644;&#37329;&#26041;&#35328;&#26631;&#31614;&#65292;&#25105;&#20204;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12749v1 Announce Type: new  Abstract: Named Entity Recognition (NER) is a fundamental task to extract key information from texts, but annotated resources are scarce for dialects. This paper introduces the first dialectal NER dataset for German, BarNER, with 161K tokens annotated on Bavarian Wikipedia articles (bar-wiki) and tweets (bar-tweet), using a schema adapted from German CoNLL 2006 and GermEval. The Bavarian dialect differs from standard German in lexical distribution, syntactic construction, and entity information. We conduct in-domain, cross-domain, sequential, and joint experiments on two Bavarian and three German corpora and present the first comprehensive NER results on Bavarian. Incorporating knowledge from the larger German NER (sub-)datasets notably improves on bar-wiki and moderately on bar-tweet. Inversely, training first on Bavarian contributes slightly to the seminal German CoNLL 2006 corpus. Moreover, with gold dialect labels on Bavarian tweets, we assess
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; I$^3$C&#65292;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#21644;&#24573;&#30053;&#19981;&#30456;&#20851;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#23569;&#26679;&#26412;&#25512;&#29702;&#21152;&#20197;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2403.12744</link><description>&lt;p&gt;
&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#21644;&#24573;&#30053;&#19981;&#30456;&#20851;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Instructing Large Language Models to Identify and Ignore Irrelevant Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; I$^3$C&#65292;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#21644;&#24573;&#30053;&#19981;&#30456;&#20851;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#23569;&#26679;&#26412;&#25512;&#29702;&#21152;&#20197;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#38656;&#35201;&#26681;&#25454;&#32473;&#23450;&#30340;&#38382;&#39064;&#25551;&#36848;&#29983;&#25104;&#25512;&#29702;&#36335;&#24452;&#65292;&#32780;&#36825;&#20010;&#25551;&#36848;&#36890;&#24120;&#21253;&#21547;&#19981;&#30456;&#20851;&#30340;&#26465;&#20214;&#12290;&#29616;&#26377;&#30340;&#8220;chain-of-thought&#8221;(CoT)&#25552;&#31034;&#26041;&#27861;&#24341;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24120;&#24120;&#34987;&#19981;&#30456;&#20851;&#30340;&#26465;&#20214;&#20005;&#37325;&#22256;&#25200;&#65292;&#23548;&#33268;&#20934;&#30830;&#24615;&#19981;&#39640;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;I$^3$C&#30340;&#26032;&#26041;&#27861;&#65292;&#25351;&#23548;LLMs&#35782;&#21035;&#21644;&#24573;&#30053;&#19981;&#30456;&#20851;&#26465;&#20214;&#12290;&#23427;&#30830;&#23450;&#20102;&#19968;&#32452;&#19982;&#38382;&#39064;&#30340;&#35821;&#20041;&#20851;&#32852;&#36739;&#24369;&#30340;&#19981;&#30456;&#20851;&#26465;&#20214;&#20505;&#36873;&#38598;&#12290;&#28982;&#21518;&#25552;&#31034;LLMs&#39564;&#35777;&#19981;&#30456;&#20851;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20851;&#20110;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#26465;&#20214;&#30340;&#39564;&#35777;&#25351;&#23548;LLMs&#65292;&#36991;&#20813;&#28151;&#28102;&#24182;&#25552;&#39640;&#25512;&#29702;&#36335;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#35758;&#36873;&#25321;(&#38382;&#39064;&#65292;&#25512;&#29702;&#36335;&#24452;)&#23545;&#20316;&#20026;&#31034;&#33539;&#65292;&#20197;&#22686;&#24378;I$^3$C&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12744v1 Announce Type: new  Abstract: Math word problem (MWP) solving requires generating a reasoning path based on a given problem description that often contains irrelevant conditions. Existing chain-of-thought (CoT) prompting methods elicited multi-step reasoning abilities of large language models (LLMs) to solve MWPs. However, they were seriously confused by the irrelevant conditions, resulting in low accuracy. In this paper, we propose a novel approach named I$^3$C that instructs LLMs to identify and ignore irrelevant conditions. It identifies a set of irrelevant condition candidates that have a weak semantic relevance with the question. Then it prompts LLMs to verify the irrelevant conditions. Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths. Moreover, we propose to select (problem, reasoning paths) pairs as demonstrations to enhance I$^3$C with few-shot reasoning. We develop I$^3$C-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#28085;&#30422;&#21335;&#26031;&#25289;&#22827;&#22320;&#21306;&#23448;&#26041;&#35821;&#35328;&#30340;&#39640;&#24230;&#21487;&#27604;&#36739;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#38598;&#21512;&#65292;&#37319;&#29992;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#35821;&#35328;&#21644;&#25991;&#20307;&#26631;&#27880;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#20854;&#21487;&#27604;&#36739;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12721</link><description>&lt;p&gt;
CLASSLA-web: &#21335;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#21487;&#27604;&#36739;&#32593;&#32476;&#35821;&#26009;&#24211;&#65292;&#27880;&#37325;&#35821;&#35328;&#21644;&#25991;&#20307;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched with Linguistic and Genre Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#28085;&#30422;&#21335;&#26031;&#25289;&#22827;&#22320;&#21306;&#23448;&#26041;&#35821;&#35328;&#30340;&#39640;&#24230;&#21487;&#27604;&#36739;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#38598;&#21512;&#65292;&#37319;&#29992;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#35821;&#35328;&#21644;&#25991;&#20307;&#26631;&#27880;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#20854;&#21487;&#27604;&#36739;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#28085;&#30422;&#26031;&#27931;&#25991;&#23612;&#20122;&#35821;&#12289;&#20811;&#32599;&#22320;&#20122;&#35821;&#12289;&#27874;&#26031;&#23612;&#20122;&#35821;&#12289;&#40657;&#23665;&#35821;&#12289;&#22622;&#23572;&#32500;&#20122;&#35821;&#12289;&#39532;&#20854;&#39039;&#35821;&#21644;&#20445;&#21152;&#21033;&#20122;&#35821;&#39640;&#24230;&#21487;&#27604;&#36739;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#38598;&#21512;&#65292;&#20174;&#32780;&#35206;&#30422;&#20102;&#21335;&#26031;&#25289;&#22827;&#35821;&#35328;&#31354;&#38388;&#25152;&#26377;&#23448;&#26041;&#35821;&#35328;&#30340;&#25972;&#20010;&#33539;&#22260;&#12290;&#36825;&#20123;&#35821;&#26009;&#24211;&#30340;&#24635;&#37327;&#20026;26&#20159;&#20010;&#21333;&#35789;&#65292;&#26469;&#33258;2600&#19975;&#31687;&#25991;&#26723;&#12290;&#35821;&#26009;&#24211;&#30340;&#21487;&#27604;&#36739;&#24615;&#30001;&#21487;&#27604;&#36739;&#30340;&#29228;&#32593;&#35774;&#32622;&#21644;&#30456;&#21516;&#30340;&#29228;&#32593;&#21644;&#21518;&#22788;&#29702;&#25216;&#26415;&#26469;&#30830;&#20445;&#12290;&#25152;&#26377;&#35821;&#26009;&#24211;&#37117;&#32463;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CLASSLA-Stanza&#35821;&#35328;&#22788;&#29702;&#31649;&#36947;&#36827;&#34892;&#35821;&#35328;&#26631;&#27880;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#22810;&#35821;&#31181;X-GENRE&#20998;&#31867;&#22120;&#22686;&#21152;&#20102;&#25991;&#26723;&#32423;&#21035;&#30340;&#25991;&#20307;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#35821;&#35328;&#26631;&#27880;&#21644;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#30340;&#27700;&#24179;&#19978;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21487;&#27604;&#36739;&#24615;&#12290;&#23545;&#32467;&#26524;&#35821;&#26009;&#24211;&#30340;&#25991;&#20307;&#32858;&#28966;&#20998;&#26512;&#26174;&#31034;&#20102;&#36825;&#19971;&#20010;&#35821;&#26009;&#24211;&#20013;&#21508;&#31181;&#25991;&#20307;&#30340;&#30456;&#24403;&#19968;&#33268;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12721v1 Announce Type: new  Abstract: This paper presents a collection of highly comparable web corpora of Slovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian, covering thereby the whole spectrum of official languages in the South Slavic language space. The collection of these corpora comprises a total of 13 billion tokens of texts from 26 million documents. The comparability of the corpora is ensured by a comparable crawling setup and the usage of identical crawling and post-processing technology. All the corpora were linguistically annotated with the state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and enriched with document-level genre information via the Transformer-based multilingual X-GENRE classifier, which further enhances comparability at the level of linguistic annotation and metadata enrichment. The genre-focused analysis of the resulting corpora shows a rather consistent distribution of genres throughout the seven corpora,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#27454;&#20851;&#20110;&#21152;&#25343;&#22823;&#31354;&#20013;&#26053;&#23458;&#26435;&#21033;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#24110;&#21161;&#26053;&#23458;&#29702;&#35299;&#21644;&#21033;&#29992;&#30456;&#20851;&#31354;&#20013;&#26053;&#34892;&#27861;&#35268;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#29992;&#25143;&#36755;&#20837;&#22797;&#26434;&#21644;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.12678</link><description>&lt;p&gt;
&#20026;&#21152;&#25343;&#22823;&#31354;&#20013;&#26053;&#34892;&#32773;&#36171;&#26435;&#65306;&#19968;&#27454;&#20851;&#20110;&#21152;&#25343;&#22823;&#31354;&#20013;&#26053;&#23458;&#26435;&#21033;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12678
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#27454;&#20851;&#20110;&#21152;&#25343;&#22823;&#31354;&#20013;&#26053;&#23458;&#26435;&#21033;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#24110;&#21161;&#26053;&#23458;&#29702;&#35299;&#21644;&#21033;&#29992;&#30456;&#20851;&#31354;&#20013;&#26053;&#34892;&#27861;&#35268;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#29992;&#25143;&#36755;&#20837;&#22797;&#26434;&#21644;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#25343;&#22823;&#33322;&#31354;&#26053;&#34892;&#39046;&#22495;&#30340;&#33322;&#29677;&#24310;&#35823;&#12289;&#21462;&#28040;&#21644;&#20854;&#20182;&#20851;&#20110;&#26053;&#23458;&#26435;&#21033;&#30340;&#38382;&#39064;&#26377;&#20102;&#26174;&#33879;&#22686;&#21152;&#12290;&#35748;&#35782;&#21040;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#26469;&#21327;&#21161;&#26053;&#23458;&#24182;&#25945;&#32946;&#20182;&#20204;&#20102;&#35299;&#33258;&#24049;&#30340;&#26435;&#21033;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#22797;&#26434;&#30340;&#29992;&#25143;&#36755;&#20837;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#26597;&#35810;&#65292;&#29992;&#20110;&#26816;&#32034;&#35814;&#32454;&#31354;&#20013;&#26053;&#34892;&#27861;&#35268;&#30340;&#25991;&#26723;&#38598;&#20013;&#30340;&#20449;&#24687;&#12290;&#20174;&#36825;&#20123;&#25991;&#26723;&#20013;&#25552;&#21462;&#26368;&#30456;&#20851;&#30340;&#27573;&#33853;&#65292;&#24182;&#25552;&#20379;&#21407;&#22987;&#25991;&#26723;&#21644;&#29983;&#25104;&#30340;&#26597;&#35810;&#30340;&#38142;&#25509;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#23558;&#20449;&#24687;&#32454;&#20998;&#24182;&#21033;&#29992;&#20110;&#20854;&#29420;&#29305;&#24773;&#20917;&#12290;&#35813;&#31995;&#32479;&#25104;&#21151;&#20811;&#26381;&#20102;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#29702;&#35299;&#22797;&#26434;&#30340;&#29992;&#25143;&#36755;&#20837;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#31572;&#26696;&#65292;&#27809;&#26377;&#24187;&#35273;&#65292;&#36825;&#20123;&#31572;&#26696;&#21487;&#20197;&#20379;&#26053;&#23458;&#20381;&#36182;&#20197;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#19968;&#39033;&#27604;&#36739;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#35895;&#27468;&#25628;&#32034;&#30340;&#29992;&#25143;&#30740;&#31350;&#23637;&#31034;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#23454;&#29992;&#24615;&#21644;&#26131;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12678v1 Announce Type: cross  Abstract: The Canadian air travel sector has seen a significant increase in flight delays, cancellations, and other issues concerning passenger rights. Recognizing this demand, we present a chatbot to assist passengers and educate them about their rights. Our system breaks a complex user input into simple queries which are used to retrieve information from a collection of documents detailing air travel regulations. The most relevant passages from these documents are presented along with links to the original documents and the generated queries, enabling users to dissect and leverage the information for their unique circumstances. The system successfully overcomes two predominant challenges: understanding complex user inputs, and delivering accurate answers, free of hallucinations, that passengers can rely on for making informed decisions. A user study comparing the chatbot to a Google search demonstrated the chatbot's usefulness and ease of use.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#20174;&#23545;&#23884;&#20837;&#30693;&#35782;&#30340;&#22522;&#20934;&#25299;&#23637;&#21040;&#20102;&#25506;&#32034;&#35821;&#29992;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#38889;&#35821;&#29615;&#22659;&#19979;&#65292;GPT-4&#22312;&#20256;&#32479;&#21644;&#20154;&#24037;&#35780;&#20272;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;HyperCLOVA X&#20063;&#22312;&#24320;&#25918;&#24335;&#38382;&#39064;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2403.12675</link><description>&lt;p&gt;
&#23545;&#38889;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#29992;&#33021;&#21147;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Pragmatic Competence Evaluation of Large Language Models for Korean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#20174;&#23545;&#23884;&#20837;&#30693;&#35782;&#30340;&#22522;&#20934;&#25299;&#23637;&#21040;&#20102;&#25506;&#32034;&#35821;&#29992;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#38889;&#35821;&#29615;&#22659;&#19979;&#65292;GPT-4&#22312;&#20256;&#32479;&#21644;&#20154;&#24037;&#35780;&#20272;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;HyperCLOVA X&#20063;&#22312;&#24320;&#25918;&#24335;&#38382;&#39064;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#20027;&#35201;&#20381;&#36182;&#20110;&#30528;&#37325;&#20110;&#27979;&#35797;&#20854;&#23884;&#20837;&#30693;&#35782;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQs&#65289;&#26469;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#31181;&#26684;&#24335;&#38750;&#24120;&#36866;&#21512;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23558;&#27492;&#35780;&#20272;&#25299;&#23637;&#21040;&#25506;&#32034;LLM&#30340;&#35821;&#29992;&#33021;&#21147;--&#22312;&#20808;&#36827;&#30340;LLM&#20986;&#29616;&#20043;&#21069;&#40092;&#26377;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#38889;&#35821;&#29615;&#22659;&#19979;&#12290;&#25105;&#20204;&#37319;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#35774;&#32622;&#65306;&#20256;&#32479;&#30340;&#33258;&#21160;&#35780;&#20272;&#36866;&#37197;&#30340;MCQ&#26684;&#24335;&#65292;&#20197;&#21450;&#30001;&#20154;&#31867;&#19987;&#23478;&#35780;&#20272;&#30340;&#24320;&#25918;&#24335;&#38382;&#39064;&#65288;OEQs&#65289;&#65292;&#29992;&#20197;&#26816;&#26597;LLM&#30340;&#21465;&#20107;&#22238;&#24212;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#23450;&#20041;&#36873;&#39033;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;GPT-4&#34920;&#29616;&#20248;&#24322;&#65292;&#22312;MCQ&#21644;OEQ&#35774;&#32622;&#20013;&#24471;&#20998;&#20998;&#21035;&#20026;81.11&#21644;85.69&#65292;&#32780;&#20197;&#38889;&#35821;&#20026;&#20248;&#21270;&#30446;&#26631;&#30340;HyperCLOVA X&#22312;OEQ&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24471;&#20998;&#20026;81.56&#65292;&#19982;GPT-4&#30456;&#27604;&#65292;&#20165;&#26377;4.13&#20998;&#30340;&#24494;&#23567;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12675v1 Announce Type: new  Abstract: The current evaluation of Large Language Models (LLMs) predominantly relies on benchmarks focusing on their embedded knowledge by testing through multiple-choice questions (MCQs), a format inherently suited for automated evaluation. Our study extends this evaluation to explore LLMs' pragmatic competence--a facet previously underexamined before the advent of sophisticated LLMs, specifically in the context of Korean. We employ two distinct evaluation setups: the conventional MCQ format, adapted for automatic evaluation, and Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs' narrative response capabilities without predefined options. Our findings reveal that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups, respectively, with HyperCLOVA X, an LLM optimized for Korean, closely following, especially in the OEQ setup, demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to GPT-4
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#33521;&#38889;&#35821;&#35328;&#23545;&#30340;1200&#21477;MQM&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;MT&#35780;&#20272;&#36716;&#21270;&#20026;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;MQM&#20998;&#25968;&#30340;&#22810;&#20219;&#21153;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12666</link><description>&lt;p&gt;
&#22810;&#32500;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#65306;&#27169;&#22411;&#35780;&#20272;&#21644;&#38889;&#35821;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Multi-Dimensional Machine Translation Evaluation: Model Evaluation and Resource for Korean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#33521;&#38889;&#35821;&#35328;&#23545;&#30340;1200&#21477;MQM&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;MT&#35780;&#20272;&#36716;&#21270;&#20026;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;MQM&#20998;&#25968;&#30340;&#22810;&#20219;&#21153;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20046;&#25152;&#26377;&#25163;&#21160;&#25110;&#33258;&#21160;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#30340;&#26694;&#26550;&#37117;&#20351;&#29992;&#21333;&#19968;&#25968;&#23383;&#26469;&#25551;&#36848;MT&#36755;&#20986;&#30340;&#36136;&#37327;&#12290;&#22810;&#32500;&#36136;&#37327;&#25351;&#26631;&#65288;MQM&#65289;&#26694;&#26550;&#26159;&#19968;&#20010;&#20363;&#22806;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#32454;&#31890;&#24230;&#30340;&#36136;&#37327;&#32500;&#24230;&#26412;&#20307;&#65288;&#22914;&#39118;&#26684;&#12289;&#27969;&#30021;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#26415;&#35821;&#65289;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;MQM&#27880;&#37322;&#30340;&#21487;&#34892;&#24615;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#35745;&#31639;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#26032;&#25991;&#26412;&#30340;MQM&#20998;&#25968;&#65292;&#36825;&#26159;&#22240;&#20026;&#32570;&#20047;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;(a)&#25552;&#20379;&#19968;&#20010;&#33521;&#38889;&#35821;&#35328;&#23545;1200&#21477;MQM&#35780;&#20272;&#22522;&#20934;&#65292;&#20197;&#21450;(b)&#23558;MT&#35780;&#20272;&#37325;&#26032;&#26500;&#24314;&#20026;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;MQM&#20998;&#25968;&#30340;&#22810;&#20219;&#21153;&#38382;&#39064;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20998;&#21035;&#22312;&#22522;&#20110;&#21442;&#32771;&#30340;MT&#35780;&#20272;&#35774;&#32622;&#21644;&#19981;&#38656;&#35201;&#21442;&#32771;&#30340;&#36136;&#37327;&#20272;&#35745;&#65288;QE&#65289;&#35774;&#32622;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#38656;&#35201;&#21442;&#32771;&#30340;&#35774;&#32622;&#34920;&#29616;&#20248;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12666v1 Announce Type: new  Abstract: Almost all frameworks for the manual or automatic evaluation of machine translation characterize the quality of an MT output with a single number. An exception is the Multidimensional Quality Metrics (MQM) framework which offers a fine-grained ontology of quality dimensions for scoring (such as style, fluency, accuracy, and terminology). Previous studies have demonstrated the feasibility of MQM annotation but there are, to our knowledge, no computational models that predict MQM scores for novel texts, due to a lack of resources. In this paper, we address these shortcomings by (a) providing a 1200-sentence MQM evaluation benchmark for the language pair English-Korean and (b) reframing MT evaluation as the multi-task problem of simultaneously predicting several MQM scores using SOTA language models, both in a reference-based MT evaluation setup and a reference-free quality estimation (QE) setup. We find that reference-free setup outperform
&lt;/p&gt;</description></item><item><title>LHMKE&#26159;&#19968;&#20010;&#38754;&#21521;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#12289;&#25972;&#20307;&#21644;&#22810;&#23398;&#31185;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#30693;&#35782;&#33719;&#21462;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12601</link><description>&lt;p&gt;
LHMKE&#65306;&#38754;&#21521;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25972;&#20307;&#22810;&#23398;&#31185;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12601
&lt;/p&gt;
&lt;p&gt;
LHMKE&#26159;&#19968;&#20010;&#38754;&#21521;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#12289;&#25972;&#20307;&#21644;&#22810;&#23398;&#31185;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#30693;&#35782;&#33719;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#36825;&#20123;LLMs&#30340;&#22522;&#20934;&#20173;&#28982;&#19981;&#36275;&#65292;&#23588;&#20854;&#22312;&#34913;&#37327;LLMs&#25429;&#25417;&#30340;&#30693;&#35782;&#26041;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;LHMKE&#65292;&#19968;&#20010;&#38754;&#21521;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#12289;&#25972;&#20307;&#21644;&#22810;&#23398;&#31185;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;&#12290; LHMKE&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#20013;&#25991;LLMs&#30340;&#30693;&#35782;&#33719;&#21462;&#33021;&#21147;&#12290;&#23427;&#21253;&#25324;&#20102;&#26469;&#33258;30&#20010;&#23398;&#31185;&#30340;75&#20010;&#20219;&#21153;&#30340;10,465&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20174;&#23567;&#23398;&#21040;&#19987;&#19994;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12601v1 Announce Type: new  Abstract: Chinese Large Language Models (LLMs) have recently demonstrated impressive capabilities across various NLP benchmarks and real-world applications. However, the existing benchmarks for comprehensively evaluating these LLMs are still insufficient, particularly in terms of measuring knowledge that LLMs capture. Current datasets collect questions from Chinese examinations across different subjects and educational levels to address this issue. Yet, these benchmarks primarily focus on objective questions such as multiple-choice questions, leading to a lack of diversity in question types. To tackle this problem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge Evaluation benchmark in this paper. LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese LLMs. It encompasses 10,465 questions across 75 tasks covering 30 subjects, ranging from primary school to professional ce
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#36716;&#31227;&#21040;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12596</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#34920;&#30340;&#25512;&#29702;&#65306;&#23558;LLMs&#30340;&#33021;&#21147;&#36716;&#31227;&#21040;VLMs&#20013;
&lt;/p&gt;
&lt;p&gt;
Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12596
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#36716;&#31227;&#21040;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12596v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25277;&#35937;&#65306;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36234;&#26469;&#36234;&#24378;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36739;&#23567;&#30340;VLMs&#65292;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#24050;&#32463;&#21462;&#24471;&#20102;&#35768;&#22810;&#25913;&#36827;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#30340;&#33021;&#21147;&#36716;&#31227;&#21040;VLMs&#30340;&#25216;&#26415;&#12290;&#22312;&#26368;&#36817;&#24341;&#20837;&#30340;ChartQA&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;\citet{chen2023pali3}&#30340;PaLI3-5B VLM&#26102;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#20351;PlotQA&#21644;FigureQA&#30340;&#24615;&#33021;&#22823;&#22823;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12596v1 Announce Type: new  Abstract: Vision-language models (VLMs) are achieving increasingly strong performance on multimodal tasks. However, reasoning capabilities remain limited particularly for smaller VLMs, while those of large-language models (LLMs) have seen numerous improvements. We propose a technique to transfer capabilities from LLMs to VLMs. On the recently introduced ChartQA, our method obtains state-of-the-art performance when applied on the PaLI3-5B VLM by \citet{chen2023pali3}, while also enabling much better performance on PlotQA and FigureQA.   We first improve the chart representation by continuing the pre-training stage using an improved version of the chart-to-table translation task by \citet{liu2023deplot}. We then propose constructing a 20x larger dataset than the original training set. To improve general reasoning capabilities and improve numerical operations, we synthesize reasoning traces using the table representation of charts. Lastly, our model 
&lt;/p&gt;</description></item><item><title>AlphaFin&#25552;&#20986;&#20102;&#35299;&#20915;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#21644;&#37329;&#34701;&#38382;&#31572;&#20219;&#21153;&#20013;&#21487;&#35299;&#37322;&#24615;&#21644;&#23454;&#26102;&#20449;&#24687;&#38598;&#25104;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21457;&#24067;AlphaFin&#25968;&#25454;&#38598;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.12582</link><description>&lt;p&gt;
AlphaFin&#65306;&#26816;&#32034;&#22686;&#24378;&#30340;&#32929;&#31080;&#38142;&#26694;&#26550;&#22312;&#37329;&#34701;&#20998;&#26512;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12582
&lt;/p&gt;
&lt;p&gt;
AlphaFin&#25552;&#20986;&#20102;&#35299;&#20915;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#21644;&#37329;&#34701;&#38382;&#31572;&#20219;&#21153;&#20013;&#21487;&#35299;&#37322;&#24615;&#21644;&#23454;&#26102;&#20449;&#24687;&#38598;&#25104;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21457;&#24067;AlphaFin&#25968;&#25454;&#38598;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20998;&#26512;&#20219;&#21153;&#20027;&#35201;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#65306;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#21644;&#30456;&#24212;&#30340;&#37329;&#34701;&#38382;&#31572;&#12290;&#30446;&#21069;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65288;ML&#65286;DL&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#39044;&#27979;&#30340;&#21407;&#22240;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#38598;&#25104;&#25991;&#23383;&#20449;&#24687;&#65292;&#22914;&#37329;&#34701;&#26032;&#38395;&#25110;&#25253;&#21578;&#12290;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#26174;&#33879;&#30340;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#20294;&#30001;&#20110;&#37329;&#34701;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#21644;&#19982;&#23454;&#26102;&#30693;&#35782;&#30340;&#26377;&#38480;&#38598;&#25104;&#65292;LLM&#20173;&#28982;&#23384;&#22312;&#34394;&#26500;&#29616;&#35937;&#65292;&#26080;&#27861;&#36319;&#19978;&#26368;&#26032;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#21457;&#24067;&#20102;AlphaFin&#25968;&#25454;&#38598;&#65292;&#23558;&#20256;&#32479;&#30740;&#31350;&#25968;&#25454;&#38598;&#12289;&#23454;&#26102;&#37329;&#34701;&#25968;&#25454;&#21644;&#25163;&#20889;&#38142;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12582v1 Announce Type: new  Abstract: The task of financial analysis primarily encompasses two key areas: stock trend prediction and the corresponding financial question answering. Currently, machine learning and deep learning algorithms (ML&amp;DL) have been widely applied for stock trend predictions, leading to significant progress. However, these methods fail to provide reasons for predictions, lacking interpretability and reasoning processes. Also, they can not integrate textual information such as financial news or reports. Meanwhile, large language models (LLMs) have remarkable textual understanding and generation ability. But due to the scarcity of financial training datasets and limited integration with real-time knowledge, LLMs still suffer from hallucinations and are unable to keep up with the latest information. To tackle these challenges, we first release AlphaFin datasets, combining traditional research datasets, real-time financial data, and handwritten chain-of-th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#26377;&#38480;&#36164;&#28304;&#19978;&#36880;&#27493;&#25191;&#34892;&#39640;&#25928;&#21160;&#24577;&#30340;HPO&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#37325;&#23558;Transformers&#27169;&#22411;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#31616;&#21333;&#25216;&#24039;&#12290;</title><link>https://arxiv.org/abs/2403.12563</link><description>&lt;p&gt;
&#38024;&#23545;&#26102;&#38388;&#21644;&#20869;&#23384;&#21463;&#38480;&#30340;GPU&#26381;&#21153;&#19978;&#30340;&#22823;&#25991;&#26412;&#20998;&#31867;&#30340;Transformer&#31616;&#21333;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Simple Hack for Transformers against Heavy Long-Text Classification on a Time- and Memory-Limited GPU Service
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12563
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#26377;&#38480;&#36164;&#28304;&#19978;&#36880;&#27493;&#25191;&#34892;&#39640;&#25928;&#21160;&#24577;&#30340;HPO&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#37325;&#23558;Transformers&#27169;&#22411;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#31616;&#21333;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;NLP&#30740;&#31350;&#20154;&#21592;&#20381;&#36182;&#20813;&#36153;&#30340;&#35745;&#31639;&#26381;&#21153;&#65292;&#22914;Google Colab&#65292;&#26469;&#20248;&#21270;&#20182;&#20204;&#30340;Transformer&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;&#35813;&#26041;&#27861;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24615;&#24182;&#38656;&#35201;&#26356;&#22823;&#30340;&#36164;&#28304;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#38271;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#21360;&#23612;&#65292;&#20165;&#21457;&#29616;&#20102;&#23569;&#37327;&#20851;&#20110;&#20351;&#29992;Transformer&#36827;&#34892;&#38271;&#25991;&#26412;&#20998;&#31867;&#30340;&#30740;&#31350;&#12290;&#22823;&#22810;&#25968;&#20165;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#65292;&#24182;&#19988;&#27809;&#26377;&#25253;&#21578;&#20219;&#20309;HPO&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;18k&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#35789;&#22120;&#36755;&#20986;&#38271;&#24230;&#24314;&#35758;&#20351;&#29992;&#21738;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#20123;&#32553;&#30701;&#21644;&#20016;&#23500;&#24207;&#21015;&#30340;&#25216;&#24039;&#65292;&#21253;&#25324;&#20572;&#29992;&#35789;&#12289;&#26631;&#28857;&#31526;&#21495;&#12289;&#20302;&#39057;&#35789;&#21644;&#37325;&#22797;&#35789;&#30340;&#21435;&#38500;&#12290;&#20026;&#20102;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#36816;&#34892;&#20102;&#19968;&#31181;&#39640;&#25928;&#21160;&#24577;&#30340;HPO&#36807;&#31243;&#65292;&#21487;&#20197;&#36880;&#27493;&#22312;&#26377;&#38480;&#36164;&#28304;&#19978;&#36827;&#34892;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#38271;&#26102;&#38388;&#36816;&#34892;&#30340;&#20248;&#21270;&#24211;&#12290;&#21033;&#29992;&#26368;&#20339;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12563v1 Announce Type: cross  Abstract: Many NLP researchers rely on free computational services, such as Google Colab, to fine-tune their Transformer models, causing a limitation for hyperparameter optimization (HPO) in long-text classification due to the method having quadratic complexity and needing a bigger resource. In Indonesian, only a few works were found on long-text classification using Transformers. Most only use a small amount of data and do not report any HPO. In this study, using 18k news articles, we investigate which pretrained models are recommended to use based on the output length of the tokenizer. We then compare some hacks to shorten and enrich the sequences, which are the removals of stopwords, punctuation, low-frequency words, and recurring words. To get a fair comparison, we propose and run an efficient and dynamic HPO procedure that can be done gradually on a limited resource and does not require a long-running optimization library. Using the best ha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20998;&#35299;&#23398;&#20064;&#30340;&#26080;&#35789;&#27719;&#25163;&#35821;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#36807;&#31243;&#20998;&#35299;&#20026;&#20004;&#20010;&#38454;&#27573;&#65292;&#22312;&#35270;&#35273;&#21021;&#22987;&#21270;&#38454;&#27573;&#37319;&#29992;&#36731;&#37327;&#32423;&#32763;&#35793;&#27169;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#35299;&#20915;&#20102;&#30452;&#25509;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23548;&#33268;&#23398;&#20064;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12556</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20998;&#35299;&#23398;&#20064;&#30340;&#26080;&#35789;&#27719;&#25163;&#35821;&#32763;&#35793;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12556
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20998;&#35299;&#23398;&#20064;&#30340;&#26080;&#35789;&#27719;&#25163;&#35821;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#36807;&#31243;&#20998;&#35299;&#20026;&#20004;&#20010;&#38454;&#27573;&#65292;&#22312;&#35270;&#35273;&#21021;&#22987;&#21270;&#38454;&#27573;&#37319;&#29992;&#36731;&#37327;&#32423;&#32763;&#35793;&#27169;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#35299;&#20915;&#20102;&#30452;&#25509;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23548;&#33268;&#23398;&#20064;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#25163;&#35821;&#32763;&#35793;&#26041;&#27861;&#36890;&#36807;&#20381;&#36182;&#26415;&#35821;&#26631;&#27880;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#26631;&#35760;&#39640;&#36136;&#37327;&#30340;&#26415;&#35821;&#26159;&#19968;&#39033;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#20219;&#21153;&#65292;&#38480;&#21046;&#20102;&#25163;&#35821;&#32763;&#35793;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#23613;&#31649;&#19968;&#20123;&#26041;&#27861;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#32763;&#35793;&#32593;&#32476;&#23454;&#29616;&#20102;&#26080;&#26415;&#35821;&#30340;&#25163;&#35821;&#32763;&#35793;&#65292;&#20294;&#36825;&#20123;&#21162;&#21147;&#20173;&#28982;&#38754;&#20020;&#24615;&#33021;&#19981;&#20339;&#21644;&#26080;&#25928;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#24341;&#20837;LLM&#21040;&#25163;&#35821;&#32763;&#35793;&#20013;&#20250;&#23548;&#33268;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#19981;&#36275;&#65292;&#22240;&#20026;LLM&#20027;&#23548;&#20102;&#23398;&#20064;&#26354;&#32447;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20998;&#35299;&#23398;&#20064;&#30340;&#26080;&#35789;&#27719;&#25163;&#35821;&#32763;&#35793;&#26041;&#27861;&#65288;FLa-LLM&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#36807;&#31243;&#20998;&#35299;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#35270;&#35273;&#21021;&#22987;&#21270;&#38454;&#27573;&#65292;&#25105;&#20204;&#22312;&#35270;&#35273;&#32534;&#30721;&#22120;&#21518;&#20351;&#29992;&#36731;&#37327;&#32423;&#32763;&#35793;&#27169;&#22411;&#26469;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12556v1 Announce Type: new  Abstract: Previous Sign Language Translation (SLT) methods achieve superior performance by relying on gloss annotations. However, labeling high-quality glosses is a labor-intensive task, which limits the further development of SLT. Although some approaches work towards gloss-free SLT through jointly training the visual encoder and translation network, these efforts still suffer from poor performance and inefficient use of the powerful Large Language Model (LLM). Most seriously, we find that directly introducing LLM into SLT will lead to insufficient learning of visual representations as LLM dominates the learning curve. To address these problems, we propose Factorized Learning assisted with Large Language Model (FLa-LLM) for gloss-free SLT. Concretely, we factorize the training process into two stages. In the visual initialing stage, we employ a lightweight translation model after the visual encoder to pre-train the visual encoder. In the LLM fine
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#22270;&#27169;&#22411;&#29992;&#20110;&#33258;&#30001;&#20107;&#20214;&#25552;&#21462;&#65292;&#26088;&#22312;&#21516;&#26102;&#25552;&#21462;&#20107;&#20214;&#24182;&#21457;&#29616;&#20107;&#20214;&#27169;&#24335;&#65292;&#36991;&#20813;&#20102;&#23545;&#22806;&#37096;&#35821;&#35328;&#30693;&#35782;&#24211;&#30340;&#20005;&#37325;&#20381;&#36182;&#21644;&#22823;&#37327;&#25163;&#21160;&#35268;&#21017;&#24320;&#21457;&#30340;&#22797;&#26434;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.12526</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#22270;&#27169;&#22411;&#29992;&#20110;&#32852;&#21512;&#33258;&#30001;&#20107;&#20214;&#25552;&#21462;&#21644;&#20107;&#20214;&#27169;&#24335;&#24402;&#32435;
&lt;/p&gt;
&lt;p&gt;
Prompt-based Graph Model for Joint Liberal Event Extraction and Event Schema Induction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12526
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#22270;&#27169;&#22411;&#29992;&#20110;&#33258;&#30001;&#20107;&#20214;&#25552;&#21462;&#65292;&#26088;&#22312;&#21516;&#26102;&#25552;&#21462;&#20107;&#20214;&#24182;&#21457;&#29616;&#20107;&#20214;&#27169;&#24335;&#65292;&#36991;&#20813;&#20102;&#23545;&#22806;&#37096;&#35821;&#35328;&#30693;&#35782;&#24211;&#30340;&#20005;&#37325;&#20381;&#36182;&#21644;&#22823;&#37327;&#25163;&#21160;&#35268;&#21017;&#24320;&#21457;&#30340;&#22797;&#26434;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26159;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#25551;&#36848;&#20102;&#23454;&#20307;&#29366;&#24577;&#30340;&#21464;&#21270;&#12290;&#20107;&#20214;&#25552;&#21462;&#20219;&#21153;&#26088;&#22312;&#35782;&#21035;&#21644;&#20998;&#31867;&#20107;&#20214;&#65292;&#24182;&#26681;&#25454;&#20107;&#20214;&#27169;&#24335;&#25214;&#21040;&#21442;&#19982;&#32773;&#12290;&#25163;&#21160;&#39044;&#23450;&#20041;&#30340;&#20107;&#20214;&#27169;&#24335;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#65292;&#38590;&#20197;&#36328;&#39046;&#22495;&#36801;&#31227;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#33258;&#30001;&#20107;&#20214;&#25552;&#21462;&#65288;LEE&#65289;&#65292;&#26088;&#22312;&#21516;&#26102;&#25552;&#21462;&#20107;&#20214;&#24182;&#21457;&#29616;&#20107;&#20214;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LEE&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#22806;&#37096;&#35821;&#35328;&#30693;&#35782;&#24211;&#65292;&#24182;&#38656;&#35201;&#25163;&#21160;&#24320;&#21457;&#22823;&#37327;&#35268;&#21017;&#36827;&#34892;&#22122;&#22768;&#21435;&#38500;&#21644;&#30693;&#35782;&#23545;&#40784;&#65292;&#36825;&#26159;&#22797;&#26434;&#21644;&#32321;&#37325;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#22270;&#27169;&#22411;&#29992;&#20110;&#33258;&#30001;&#20107;&#20214;&#25552;&#21462;&#65288;PGLEE&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#26469;&#33719;&#21462;&#20505;&#36873;&#35302;&#21457;&#22120;&#21644;&#21442;&#25968;&#65292;&#28982;&#21518;&#26500;&#24314;&#24322;&#26500;&#20107;&#20214;&#22270;&#20197;&#32534;&#30721;&#20107;&#20214;&#20869;&#37096;&#21644;&#20107;&#20214;&#20043;&#38388;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12526v1 Announce Type: new  Abstract: Events are essential components of speech and texts, describing the changes in the state of entities. The event extraction task aims to identify and classify events and find their participants according to event schemas. Manually predefined event schemas have limited coverage and are hard to migrate across domains. Therefore, the researchers propose Liberal Event Extraction (LEE), which aims to extract events and discover event schemas simultaneously. However, existing LEE models rely heavily on external language knowledge bases and require the manual development of numerous rules for noise removal and knowledge alignment, which is complex and laborious. To this end, we propose a Prompt-based Graph Model for Liberal Event Extraction (PGLEE). Specifically, we use a prompt-based model to obtain candidate triggers and arguments, and then build heterogeneous event graphs to encode the structures within and between events. Experimental result
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphERE&#30340;&#22810;&#20107;&#20214;&#20851;&#31995;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#22686;&#24378;&#20107;&#20214;&#23884;&#20837;&#65292;&#25193;&#23637;&#20102;&#20107;&#20214;&#23884;&#20837;&#30340;&#20107;&#20214;&#21442;&#25968;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20107;&#20214;&#35302;&#21457;&#22120;&#23884;&#20837;&#30340;&#23616;&#38480;&#20197;&#21450;&#20851;&#31995;&#20043;&#38388;&#20114;&#36830;&#34987;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12523</link><description>&lt;p&gt;
GraphERE: &#22522;&#20110;&#22270;&#22686;&#24378;&#20107;&#20214;&#23884;&#20837;&#30340;&#22810;&#20107;&#20214;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
GraphERE: Jointly Multiple Event-Event Relation Extraction via Graph-Enhanced Event Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12523
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphERE&#30340;&#22810;&#20107;&#20214;&#20851;&#31995;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#22686;&#24378;&#20107;&#20214;&#23884;&#20837;&#65292;&#25193;&#23637;&#20102;&#20107;&#20214;&#23884;&#20837;&#30340;&#20107;&#20214;&#21442;&#25968;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20107;&#20214;&#35302;&#21457;&#22120;&#23884;&#20837;&#30340;&#23616;&#38480;&#20197;&#21450;&#20851;&#31995;&#20043;&#38388;&#20114;&#36830;&#34987;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25551;&#36848;&#23454;&#20307;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;&#22312;&#25991;&#26723;&#20013;&#65292;&#22810;&#20010;&#20107;&#20214;&#36890;&#36807;&#21508;&#31181;&#20851;&#31995;&#30456;&#20114;&#36830;&#25509;&#65288;&#20363;&#22914;&#65292;&#20849;&#25351;&#12289;&#26102;&#38388;&#12289;&#22240;&#26524;&#21644;&#23376;&#20107;&#20214;&#65289;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#21462;&#65288;ERE&#65289;&#33719;&#21462;&#20107;&#20214;&#20043;&#38388;&#30340;&#36830;&#25509;&#23545;&#20110;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;ERE&#24037;&#20316;&#20013;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;a. &#20165;&#20351;&#29992;&#20107;&#20214;&#35302;&#21457;&#22120;&#30340;&#23884;&#20837;&#26469;&#34920;&#31034;&#20107;&#20214;&#29305;&#24449;&#65292;&#24573;&#30053;&#20107;&#20214;&#21442;&#25968;&#65288;&#20363;&#22914;&#65292;&#26102;&#38388;&#12289;&#22320;&#28857;&#12289;&#20154;&#29289;&#31561;&#65289;&#21450;&#20854;&#22312;&#20107;&#20214;&#20869;&#30340;&#32467;&#26500;&#12290;b. &#20851;&#31995;&#20043;&#38388;&#30340;&#20114;&#36830;&#65288;&#20363;&#22914;&#65292;&#26102;&#38388;&#21644;&#22240;&#26524;&#20851;&#31995;&#36890;&#24120;&#20250;&#30456;&#20114;&#24433;&#21709;&#65289;&#34987;&#24573;&#30053;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphERE&#22522;&#20110;&#22270;&#22686;&#24378;&#20107;&#20214;&#23884;&#20837;&#30340;&#22810;&#37325;ERE&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#38745;&#24577;AMR&#22270;&#21644;IE&#22270;&#20016;&#23500;&#20107;&#20214;&#23884;&#20837;&#30340;&#20107;&#20214;&#21442;&#25968;&#21644;&#32467;&#26500;&#29305;&#24449;&#65307;&#28982;&#21518;&#65292;&#20026;&#20102;&#32852;&#21512;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12523v1 Announce Type: cross  Abstract: Events describe the state changes of entities. In a document, multiple events are connected by various relations (e.g., Coreference, Temporal, Causal, and Subevent). Therefore, obtaining the connections between events through Event-Event Relation Extraction (ERE) is critical to understand natural language. There are two main problems in the current ERE works: a. Only embeddings of the event triggers are used for event feature representation, ignoring event arguments (e.g., time, place, person, etc.) and their structure within the event. b. The interconnection between relations (e.g., temporal and causal relations usually interact with each other ) is ignored. To solve the above problems, this paper proposes a jointly multiple ERE framework called GraphERE based on Graph-enhanced Event Embeddings. First, we enrich the event embeddings with event argument and structure features by using static AMR graphs and IE graphs; Then, to jointly e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#29983;&#25104;&#30683;&#30462;&#21709;&#24212;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#33719;&#24471;&#20102;&#23453;&#36149;&#30340;&#30683;&#30462;&#29305;&#24449;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.12500</link><description>&lt;p&gt;
&#29992;&#20110;&#19968;&#33268;&#24615;&#24863;&#30693;&#23545;&#35805;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#29983;&#25104;&#30340;&#30683;&#30462;&#21709;&#24212;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
A Large Collection of Model-generated Contradictory Responses for Consistency-aware Dialogue Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#29983;&#25104;&#30683;&#30462;&#21709;&#24212;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#33719;&#24471;&#20102;&#23453;&#36149;&#30340;&#30683;&#30462;&#29305;&#24449;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32531;&#35299;&#29983;&#25104;&#30683;&#30462;&#21709;&#24212;&#22312;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#20013;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#21487;&#29992;&#30683;&#30462;&#21709;&#24212;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#22312;&#25233;&#21046;&#36825;&#20123;&#30683;&#30462;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24102;&#26469;&#20004;&#20010;&#37325;&#35201;&#30340;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#35775;&#38382;&#22823;&#37327;&#30340;&#30683;&#30462;&#25968;&#25454;&#21487;&#20197;&#20840;&#38754;&#26816;&#39564;&#23427;&#20204;&#30340;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#22823;&#35268;&#27169;&#30683;&#30462;&#25968;&#25454;&#30340;&#35757;&#32451;&#32780;&#24471;&#21040;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#23581;&#35797;&#26500;&#24314;&#19968;&#20010;&#24191;&#27867;&#30340;&#27169;&#22411;&#29983;&#25104;&#30683;&#30462;&#21709;&#24212;&#38598;&#21512;&#12290;&#26412;&#25991;&#39318;&#27425;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#30683;&#30462;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#25910;&#38598;&#21040;&#30340;&#21709;&#24212;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#33719;&#24471;&#20102;&#26377;&#20215;&#20540;&#30340;&#23545;&#27169;&#22411;&#29983;&#25104;&#30683;&#30462;&#30340;&#29305;&#24449;&#30340;&#27934;&#23519;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#19968;&#25968;&#25454;&#38598;&#22914;&#20309;&#26174;&#33879;&#22686;&#24378;&#20102;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12500v1 Announce Type: new  Abstract: Mitigating the generation of contradictory responses poses a substantial challenge in dialogue response generation. The quality and quantity of available contradictory response data play a vital role in suppressing these contradictions, offering two significant benefits. First, having access to large contradiction data enables a comprehensive examination of their characteristics. Second, data-driven methods to mitigate contradictions may be enhanced with large-scale contradiction data for training. Nevertheless, no attempt has been made to build an extensive collection of model-generated contradictory responses. In this paper, we build a large dataset of response generation models' contradictions for the first time. Then, we acquire valuable insights into the characteristics of model-generated contradictions through an extensive analysis of the collected responses. Lastly, we also demonstrate how this dataset substantially enhances the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#21363;&#26102;&#24615;&#32452;&#32455;&#32467;&#26500;&#24378;&#21152;&#22312;LLM&#20195;&#29702;&#19978;&#65292;&#20197;&#20419;&#36827;&#22810;&#20195;&#29702;&#31995;&#32479;&#20869;&#30340;&#21512;&#20316;&#65292;&#22312;&#20855;&#36523;LLM&#20195;&#29702;&#21644;&#20154;-&#20195;&#29702;&#21512;&#20316;&#23454;&#39564;&#20013;&#21457;&#29616;&#25351;&#23450;&#39046;&#23548;&#23545;&#22242;&#38431;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#30340;&#39046;&#23548;&#32032;&#36136;&#21644;&#33258;&#21457;&#21512;&#20316;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.12482</link><description>&lt;p&gt;
&#20855;&#36523;LLM&#20195;&#29702;&#22312;&#32452;&#32455;&#22242;&#38431;&#20013;&#23398;&#20250;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Embodied LLM Agents Learn to Cooperate in Organized Teams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#21363;&#26102;&#24615;&#32452;&#32455;&#32467;&#26500;&#24378;&#21152;&#22312;LLM&#20195;&#29702;&#19978;&#65292;&#20197;&#20419;&#36827;&#22810;&#20195;&#29702;&#31995;&#32479;&#20869;&#30340;&#21512;&#20316;&#65292;&#22312;&#20855;&#36523;LLM&#20195;&#29702;&#21644;&#20154;-&#20195;&#29702;&#21512;&#20316;&#23454;&#39564;&#20013;&#21457;&#29616;&#25351;&#23450;&#39046;&#23548;&#23545;&#22242;&#38431;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#30340;&#39046;&#23548;&#32032;&#36136;&#21644;&#33258;&#21457;&#21512;&#20316;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#25512;&#29702;&#12289;&#35268;&#21010;&#21644;&#20915;&#31574;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#30340;&#29087;&#32451;&#24230;&#12290;LLMs&#22240;&#27492;&#22312;&#22810;&#20195;&#29702;&#31995;&#32479;&#20869;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20197;&#20419;&#36827;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;LLM&#20195;&#29702;&#24448;&#24448;&#20250;&#36807;&#24230;&#25253;&#21578;&#24182;&#36981;&#20174;&#20219;&#20309;&#25351;&#20196;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22810;&#20195;&#29702;&#21512;&#20316;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#28151;&#20081;&#12290;&#21463;&#20154;&#31867;&#32452;&#32455;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#21363;&#26102;&#24615;&#32452;&#32455;&#32467;&#26500;&#24378;&#21152;&#22312;LLM&#20195;&#29702;&#19978;&#65292;&#20197;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20855;&#36523;LLM&#20195;&#29702;&#21644;&#20154;-&#20195;&#29702;&#21512;&#20316;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#25351;&#23450;&#39046;&#23548;&#23545;&#22242;&#38431;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#23637;&#31034;&#30340;&#39046;&#23548;&#32032;&#36136;&#21644;&#20182;&#20204;&#30340;&#33258;&#21457;&#21512;&#20316;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12482v1 Announce Type: new  Abstract: Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26356;&#22810;&#19978;&#19979;&#25991;&#20449;&#24687;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#33021;&#22815;&#24102;&#26469;&#30340;&#23545;&#20110;&#35773;&#21050;&#35782;&#21035;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.12469</link><description>&lt;p&gt;
&#26356;&#22810;&#32972;&#26223;&#20449;&#24687;&#20309;&#26102;&#26377;&#21161;&#20110;&#35782;&#21035;&#35773;&#21050;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Do "More Contexts" Help with Sarcasm Recognition?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26356;&#22810;&#19978;&#19979;&#25991;&#20449;&#24687;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#33021;&#22815;&#24102;&#26469;&#30340;&#23545;&#20110;&#35773;&#21050;&#35782;&#21035;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35773;&#21050;&#35782;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#29702;&#35299;&#25991;&#26412;&#30340;&#30495;&#23454;&#24847;&#22270;&#65292;&#36825;&#19982;&#23383;&#38754;&#24847;&#20041;&#30456;&#21453;&#25110;&#19981;&#21516;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#31995;&#21015;&#25552;&#20379;&#26356;&#20016;&#23500;$contexts$&#65288;&#20363;&#22914;&#24773;&#24863;&#25110;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#65289;&#32473;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#34429;&#28982;&#24050;&#32463;&#21333;&#29420;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#31995;&#32479;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#38598;&#20307;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#39069;&#22806;&#32972;&#26223;&#20449;&#24687;&#33021;&#22815;&#25552;&#39640;&#23545;&#35773;&#21050;&#30340;&#35782;&#21035;&#31243;&#24230;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#38598;&#25104;&#29616;&#26377;&#26041;&#27861;&#25152;&#24102;&#26469;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#23558;&#26356;&#22810;&#32972;&#26223;&#20449;&#24687;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#38598;&#25104;&#22810;&#20010;&#19978;&#19979;&#25991;&#32447;&#32034;&#24182;&#27979;&#35797;&#19981;&#21516;&#26041;&#27861;&#12290;&#22312;&#23545;&#19977;&#20010;&#35773;&#21050;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#30340;&#22235;&#31181;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#28436;&#31034;&#20102;&#36880;&#27493;&#28155;&#21152;&#26356;&#22810;co&#26377;&#30410;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12469v1 Announce Type: new  Abstract: Sarcasm recognition is challenging because it needs an understanding of the true intention, which is opposite to or different from the literal meaning of the words. Prior work has addressed this challenge by developing a series of methods that provide richer $contexts$, e.g., sentiment or cultural nuances, to models. While shown to be effective individually, no study has systematically evaluated their collective effectiveness. As a result, it remains unclear to what extent additional contexts can improve sarcasm recognition. In this work, we explore the improvements that existing methods bring by incorporating more contexts into a model. To this end, we develop a framework where we can integrate multiple contextual cues and test different approaches. In evaluation with four approaches on three sarcasm recognition benchmarks, we achieve existing state-of-the-art performances and also demonstrate the benefits of sequentially adding more co
&lt;/p&gt;</description></item><item><title>CrossTune&#26159;&#19968;&#31181;&#24102;&#26377;&#26631;&#31614;&#22686;&#24378;&#30340;&#40657;&#30418;&#23569;&#26679;&#26412;&#20998;&#31867;&#32593;&#32476;&#65292;&#36890;&#36807;&#27169;&#25311;&#36755;&#20837;&#25991;&#26412;&#24207;&#21015;&#19982;&#20219;&#21153;&#26631;&#31614;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12468</link><description>&lt;p&gt;
CrossTune: &#24102;&#26377;&#26631;&#31614;&#22686;&#24378;&#30340;&#40657;&#30418;&#23569;&#26679;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CrossTune: Black-Box Few-Shot Classification with Label Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12468
&lt;/p&gt;
&lt;p&gt;
CrossTune&#26159;&#19968;&#31181;&#24102;&#26377;&#26631;&#31614;&#22686;&#24378;&#30340;&#40657;&#30418;&#23569;&#26679;&#26412;&#20998;&#31867;&#32593;&#32476;&#65292;&#36890;&#36807;&#27169;&#25311;&#36755;&#20837;&#25991;&#26412;&#24207;&#21015;&#19982;&#20219;&#21153;&#26631;&#31614;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25110;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#40723;&#21169;&#26368;&#36817;&#30340;&#21162;&#21147;&#25506;&#32034;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#36825;&#20123;&#27169;&#22411;&#35270;&#20026;&#40657;&#30418;&#65292;&#24182;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#65288;&#25512;&#29702;API&#65289;&#19982;&#23427;&#20204;&#36827;&#34892;&#20132;&#20114;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;&#26080;&#26799;&#24230;&#25552;&#31034;&#20248;&#21270;&#23558;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#36866;&#24212;&#21040;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;&#20294;&#36825;&#36890;&#24120;&#28041;&#21450;&#19968;&#20010;&#26114;&#36149;&#30340;&#25628;&#32034;&#29305;&#23450;&#20219;&#21153;&#25552;&#31034;&#30340;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21463;&#21040;&#21160;&#26426;&#21435;&#30740;&#31350;&#26080;&#38656;&#25628;&#32034;&#25552;&#31034;&#30340;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;CrossTune&#30340;&#26631;&#31614;&#22686;&#24378;&#36328;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23427;&#27169;&#25311;&#20102;&#36755;&#20837;&#25991;&#26412;&#24207;&#21015;&#19982;&#29305;&#23450;&#20219;&#21153;&#26631;&#31614;&#25551;&#36848;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#20854;&#26377;&#25928;&#24615;&#22312;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#26816;&#39564;&#12290;&#20026;&#20102;&#25913;&#36827;CrossTune&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#21033;&#29992;ChatGPT&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12468v1 Announce Type: new  Abstract: Training or finetuning large-scale language models (LLMs) requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One approach is to treat these models as black boxes and use forward passes (Inference APIs) to interact with them. Current research focuses on adapting these black-box models to downstream tasks using gradient-free prompt optimization, but this often involves an expensive process of searching task-specific prompts. Therefore, we are motivated to study black-box language model adaptation without prompt search. Specifically, we introduce a label-enhanced cross-attention network called CrossTune, which models the semantic relatedness between the input text sequence and task-specific label descriptions. Its effectiveness is examined in the context of few-shot text classification. To improve the generalization of CrossTune, we utilize ChatGPT to generate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30524;&#25511;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#38477;&#20302;&#23545;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;</title><link>https://arxiv.org/abs/2403.12416</link><description>&lt;p&gt;
&#38024;&#23545;&#25918;&#23556;&#23398;&#30340;&#30524;&#25511;&#24341;&#23548;&#22810;&#27169;&#24577;&#23545;&#40784;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Eye-gaze Guided Multi-modal Alignment Framework for Radiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12416
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30524;&#25511;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#38477;&#20302;&#23545;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#26694;&#26550;&#20013;&#65292;&#36328;&#27169;&#24577;&#29305;&#24449;&#30340;&#23545;&#40784;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24378;&#35843;&#20840;&#23616;&#25110;&#23616;&#37096;&#27169;&#24577;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#22312;&#25918;&#23556;&#23398;&#20013;&#24120;&#24120;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;&#35786;&#26029;&#35780;&#20272;&#36807;&#31243;&#20013;&#21516;&#27493;&#25910;&#38598;&#30340;&#30524;&#25511;&#25968;&#25454;&#65292;&#23558;&#33016;&#37096;X&#32447;&#33258;&#28982;&#22320;&#19982;&#35786;&#26029;&#25991;&#26412;&#30456;&#20851;&#32852;&#65292;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#26088;&#22312;&#20943;&#23569;&#23545;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12416v1 Announce Type: cross  Abstract: In multi-modal frameworks, the alignment of cross-modal features presents a significant challenge. The predominant approach in multi-modal pre-training emphasizes either global or local alignment between modalities, utilizing extensive datasets. This bottom-up driven method often suffers from a lack of interpretability, a critical concern in radiology. Previous studies have integrated high-level labels in medical images or text, but these still rely on manual annotation, a costly and labor-intensive process. Our work introduces a novel approach by using eye-gaze data, collected synchronously by radiologists during diagnostic evaluations. This data, indicating radiologists' focus areas, naturally links chest X-rays to diagnostic texts. We propose the Eye-gaze Guided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for better alignment of image and text features, aiming to reduce reliance on manual annotations and thus cut
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19977;&#26041;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#35780;&#20272;&#25351;&#20196;&#36319;&#38543;&#31995;&#32479;&#22312;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20174;&#32780;&#35299;&#20915;&#29992;&#25143;&#26080;&#27861;&#20102;&#35299;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12413</link><description>&lt;p&gt;
&#26469;&#33258;&#25351;&#20196;&#30340;&#31532;&#19977;&#26041;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Third-Party Language Model Performance Prediction from Instruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12413
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19977;&#26041;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#35780;&#20272;&#25351;&#20196;&#36319;&#38543;&#31995;&#32479;&#22312;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20174;&#32780;&#35299;&#20915;&#29992;&#25143;&#26080;&#27861;&#20102;&#35299;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#36319;&#38543;&#31995;&#32479;&#26368;&#36817;&#22312;&#35768;&#22810;&#22522;&#20934;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#22909;&#30340;&#24615;&#33021;&#65292;&#23637;&#29616;&#20986;&#36866;&#24212;&#21508;&#31181;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#26410;&#35774;&#35745;&#25104;&#36879;&#26126;&#22320;&#23637;&#31034;&#20854;&#23616;&#38480;&#24615;&#65307;&#29992;&#25143;&#21487;&#33021;&#20250;&#36731;&#26131;&#29992;&#25351;&#20196;&#25552;&#31034;&#19968;&#20010;&#27169;&#22411;&#65292;&#21364;&#23545;&#20854;&#21709;&#24212;&#26159;&#21542;&#24212;&#35813;&#20934;&#30830;&#19968;&#26080;&#25152;&#30693;&#65292;&#29978;&#33267;&#19981;&#30693;&#36947;&#31995;&#32479;&#26159;&#21542;&#26377;&#33021;&#21147;&#25191;&#34892;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31532;&#19977;&#26041;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#20854;&#20013;&#21333;&#29420;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#22312;&#25512;&#26029;&#26102;&#20165;&#20551;&#35774;&#35775;&#38382;&#20854;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#20219;&#21153;&#19978;&#30340;&#25351;&#20196;&#36319;&#38543;&#31995;&#32479;&#26102;&#20135;&#29983;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#24320;&#25918;&#21644;&#23553;&#38381;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#20197;&#21450;&#22810;&#20010;&#24615;&#33021;&#39044;&#27979;&#22120;&#26469;&#36827;&#34892;&#36825;&#39033;&#20998;&#26512;&#65292;&#24182;&#26816;&#39564;&#21508;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#22914;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#20219;&#21153;&#25968;&#37327;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12413v1 Announce Type: new  Abstract: Language model-based instruction-following systems have lately shown increasing performance on many benchmark tasks, demonstrating the capability of adapting to a broad variety of instructions. However, such systems are often not designed to be transparent about their limitations; a user may easily prompt a model with an instruction without any idea of whether the responses should be expected to be accurate, or if the system is even capable of performing the task. We propose a third party performance prediction framework, where a separate model is trained to predict the metric resulting from evaluating an instruction-following system on a task while assuming access only to its inputs and outputs at inference time. We perform this analysis with a variety of both open and closed instruction-following models as well as multiple performance predictors, and examine the effect of various factors such as model size, number of training tasks, an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65292;&#25903;&#25345;&#22810;&#35821;&#35328;S2ST&#24182;&#20445;&#30041;&#35828;&#35805;&#32773;&#39118;&#26684;</title><link>https://arxiv.org/abs/2403.12408</link><description>&lt;p&gt;
MSLM-S2ST&#65306;&#19968;&#31181;&#29992;&#20110;&#26080;&#25991;&#26412;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#30340;&#22810;&#20219;&#21153;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65292;&#20445;&#30041;&#35828;&#35805;&#32773;&#39118;&#26684;
&lt;/p&gt;
&lt;p&gt;
MSLM-S2ST: A Multitask Speech Language Model for Textless Speech-to-Speech Translation with Speaker Style Preservation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12408
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65292;&#25903;&#25345;&#22810;&#35821;&#35328;S2ST&#24182;&#20445;&#30041;&#35828;&#35805;&#32773;&#39118;&#26684;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65288;S2ST&#65289;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#36827;&#23637;&#36880;&#28176;&#22686;&#21152;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;Multitask Speech Language Model (MSLM)&#65292;&#36825;&#26159;&#19968;&#20010;&#20165;&#20855;&#26377;&#35299;&#30721;&#22120;&#30340;&#22810;&#20219;&#21153;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#19981;&#20381;&#36182;&#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25903;&#25345;&#24102;&#26377;&#20445;&#30041;&#35828;&#35805;&#32773;&#39118;&#26684;&#30340;&#22810;&#35821;&#35328;S2ST&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12408v1 Announce Type: new  Abstract: There have been emerging research interest and advances in speech-to-speech translation (S2ST), translating utterances from one language to another. This work proposes Multitask Speech Language Model (MSLM), which is a decoder-only speech language model trained in a multitask setting. Without reliance on text training data, our model is able to support multilingual S2ST with speaker style preserved.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35821;&#35328;&#25552;&#31034;&#32763;&#35793;&#65288;MPT&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#35821;&#35328;&#25552;&#31034;&#32763;&#35793;&#22120;&#23454;&#29616;&#22312;&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#23558;&#36719;&#25552;&#31034;&#20174;&#28304;&#35821;&#35328;&#26377;&#25928;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#35328;&#65292;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#36234;</title><link>https://arxiv.org/abs/2403.12407</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35821;&#35328;&#25552;&#31034;&#32763;&#35793;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Transfer for Natural Language Inference via Multilingual Prompt Translator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12407
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35821;&#35328;&#25552;&#31034;&#32763;&#35793;&#65288;MPT&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#35821;&#35328;&#25552;&#31034;&#32763;&#35793;&#22120;&#23454;&#29616;&#22312;&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#23558;&#36719;&#25552;&#31034;&#20174;&#28304;&#35821;&#35328;&#26377;&#25928;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#35328;&#65292;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#36234;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#19982;&#25552;&#31034;&#23398;&#20064;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#22312;&#28304;&#35821;&#35328;&#20013;&#23398;&#20064;&#30340;&#36719;&#25552;&#31034;&#34987;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#35328;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#36716;&#31227;&#36719;&#25552;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#22810;&#35821;&#35328;&#25552;&#31034;&#32763;&#35793;&#65288;MPT&#65289;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#25552;&#31034;&#32763;&#35793;&#22120;&#26469;&#36866;&#24403;&#22320;&#22788;&#29702;&#25552;&#31034;&#20013;&#23884;&#20837;&#30340;&#20851;&#38190;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#20445;&#30041;&#20219;&#21153;&#30693;&#35782;&#30340;&#21516;&#26102;&#25913;&#21464;&#35821;&#35328;&#30693;&#35782;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#25552;&#31034;&#65292;&#28982;&#21518;&#21033;&#29992;&#32763;&#35793;&#22120;&#23558;&#20854;&#32763;&#35793;&#25104;&#30446;&#26631;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#20010;&#22806;&#37096;&#35821;&#26009;&#24211;&#20316;&#20026;&#36741;&#21161;&#25968;&#25454;&#65292;&#23545;&#39044;&#27979;&#31572;&#26696;&#27010;&#29575;&#36827;&#34892;&#23545;&#40784;&#20219;&#21153;&#65292;&#20197;&#36716;&#25442;&#35821;&#35328;&#30693;&#35782;&#65292;&#20174;&#32780;&#20026;&#30446;&#26631;&#25552;&#31034;&#25552;&#20379;&#22810;&#35821;&#35328;&#30693;&#35782;&#12290;&#22312;XNLI&#30340;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#65292;MPT&#34920;&#29616;&#20986;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#20248;&#36234;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12407v1 Announce Type: new  Abstract: Based on multilingual pre-trained models, cross-lingual transfer with prompt learning has shown promising effectiveness, where soft prompt learned in a source language is transferred to target languages for downstream tasks, particularly in the low-resource scenario. To efficiently transfer soft prompt, we propose a novel framework, Multilingual Prompt Translator (MPT), where a multilingual prompt translator is introduced to properly process crucial knowledge embedded in prompt by changing language knowledge while retaining task knowledge. Concretely, we first train prompt in source language and employ translator to translate it into target prompt. Besides, we extend an external corpus as auxiliary data, on which an alignment task for predicted answer probability is designed to convert language knowledge, thereby equipping target prompt with multilingual knowledge. In few-shot settings on XNLI, MPT demonstrates superiority over baselines
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#35757;&#32451;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.12403</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30340;&#21407;&#22240;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12403
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#35757;&#32451;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#29992;&#25143;&#36827;&#34892;&#20154;&#38469;&#35752;&#35770;&#21644;&#34920;&#36798;&#35266;&#28857;&#30340;&#37325;&#35201;&#22330;&#25152;&#65292;&#20294;&#31038;&#20132;&#23186;&#20307;&#25552;&#20379;&#30340;&#22806;&#31435;&#38754;&#21644;&#21311;&#21517;&#24615;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#21457;&#24067;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#12290;&#37492;&#20110;&#36825;&#20123;&#24179;&#21488;&#30340;&#24222;&#22823;&#35268;&#27169;&#65292;&#33258;&#21160;&#35782;&#21035;&#21644;&#26631;&#35760;&#20167;&#24680;&#35328;&#35770;&#30340;&#38656;&#27714;&#26085;&#30410;&#36843;&#20999;&#12290;&#23613;&#31649;&#23384;&#22312;&#20960;&#31181;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#40657;&#30418;&#26041;&#27861;&#22312;&#35774;&#35745;&#19978;&#19981;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#25110;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#35299;&#20915;&#35299;&#37322;&#24615;&#19981;&#36275;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#65292;&#35757;&#32451;&#22522;&#30784;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#36890;&#36807;&#35774;&#35745;&#23454;&#29616;&#24544;&#23454;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;LLM&#30340;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#21644;&#26368;&#20808;&#36827;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20351;&#36825;&#20123;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12403v1 Announce Type: cross  Abstract: Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content. Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech. Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design. To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design. Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifi
&lt;/p&gt;</description></item><item><title>&#23545;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#35821;&#38899;LM&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#25552;&#31034;&#35774;&#35745;&#21644;&#20869;&#23481;&#35821;&#20041;&#21333;&#20803;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21457;&#29616;&#25552;&#31034;&#30340;&#24322;&#36136;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#20250;&#24433;&#21709;&#38899;&#39057;&#36136;&#37327;&#65292;&#21516;&#26102;&#20869;&#23481;&#20063;&#20250;&#24433;&#21709;&#21512;&#25104;&#38899;&#39057;&#30340;&#35828;&#35805;&#39118;&#26684;&#12290;</title><link>https://arxiv.org/abs/2403.12402</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#25552;&#31034;&#26465;&#20214;&#35821;&#38899;&#21512;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12402
&lt;/p&gt;
&lt;p&gt;
&#23545;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#35821;&#38899;LM&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#25552;&#31034;&#35774;&#35745;&#21644;&#20869;&#23481;&#35821;&#20041;&#21333;&#20803;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21457;&#29616;&#25552;&#31034;&#30340;&#24322;&#36136;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#20250;&#24433;&#21709;&#38899;&#39057;&#36136;&#37327;&#65292;&#21516;&#26102;&#20869;&#23481;&#20063;&#20250;&#24433;&#21709;&#21512;&#25104;&#38899;&#39057;&#30340;&#35828;&#35805;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;(LMs)&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#39640;&#36136;&#37327;&#35821;&#38899;&#21512;&#25104;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;&#20856;&#22411;&#30340;&#35821;&#38899;LM&#23558;&#31163;&#25955;&#35821;&#20041;&#21333;&#20803;&#20316;&#20026;&#20869;&#23481;&#65292;&#30701;&#35821;&#35328;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#21512;&#25104;&#20445;&#30041;&#20869;&#23481;&#35821;&#20041;&#20294;&#27169;&#20223;&#25552;&#31034;&#39118;&#26684;&#30340;&#35821;&#38899;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21512;&#25104;&#38899;&#39057;&#22914;&#20309;&#21463;&#25552;&#31034;&#21644;&#20869;&#23481;&#25511;&#21046;&#24182;&#26080;&#31995;&#32479;&#24615;&#29702;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#33258;&#22238;&#24402;(AR)&#21644;&#38750;&#33258;&#22238;&#24402;(NAR)&#35821;&#38899;LM&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#25552;&#31034;&#35774;&#35745;&#21644;&#20869;&#23481;&#35821;&#20041;&#21333;&#20803;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24322;&#36136;&#21644;&#38750;&#24179;&#31283;&#25552;&#31034;&#20250;&#38477;&#20302;&#38899;&#39057;&#36136;&#37327;&#65292;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#30456;&#21453;&#65292;&#36739;&#38271;&#25552;&#31034;&#24182;&#19981;&#24635;&#26159;&#23548;&#33268;&#26356;&#22909;&#30340;&#21512;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#21512;&#25104;&#38899;&#39057;&#30340;&#35828;&#35805;&#39118;&#26684;&#21463;&#20869;&#23481;&#24433;&#21709;&#65292;&#32780;&#19981;&#20165;&#20165;&#21463;&#25552;&#31034;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#35821;&#20041;&#21333;&#20803;&#25658;&#24102;&#30340;&#20449;&#24687;&#26377;&#21161;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12402v1 Announce Type: new  Abstract: Speech language models (LMs) are promising for high-quality speech synthesis through in-context learning. A typical speech LM takes discrete semantic units as content and a short utterance as prompt, and synthesizes speech which preserves the content's semantics but mimics the prompt's style. However, there is no systematic understanding on how the synthesized audio is controlled by the prompt and content. In this work, we conduct an empirical study of the widely used autoregressive (AR) and non-autoregressive (NAR) speech LMs and provide insights into the prompt design and content semantic units. Our analysis reveals that heterogeneous and nonstationary prompts hurt the audio quality in contrast to the previous finding that longer prompts always lead to better synthesis. Moreover, we find that the speaker style of the synthesized audio is also affected by the content in addition to the prompt. We further show that semantic units carry r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;Dr3&#26426;&#21046;&#65292;&#36890;&#36807;&#36776;&#21035;&#22120;&#21028;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31572;&#26696;&#26159;&#21542;&#39064;&#22806;&#26469;&#35299;&#20915;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#38382;&#31572;&#20013;&#39064;&#22806;&#31572;&#26696;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.12393</link><description>&lt;p&gt;
Dr3&#65306;&#35201;&#27714;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#38382;&#31572;&#20013;&#19981;&#32473;&#20986;&#39064;&#22806;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12393
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;Dr3&#26426;&#21046;&#65292;&#36890;&#36807;&#36776;&#21035;&#22120;&#21028;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31572;&#26696;&#26159;&#21542;&#39064;&#22806;&#26469;&#35299;&#20915;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#38382;&#31572;&#20013;&#39064;&#22806;&#31572;&#26696;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#38382;&#31572;&#65288;ODMHQA&#65289;&#36890;&#36807;&#23545;&#26469;&#33258;&#22806;&#37096;&#30693;&#35782;&#28304;&#30340;&#20449;&#24687;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#26469;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#20915;ODMHQA&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#24402;&#22240;&#20110;&#23427;&#20204;&#30340;&#35268;&#21010;&#12289;&#25512;&#29702;&#21644;&#24037;&#20855;&#21033;&#29992;&#31561;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#23581;&#35797;&#35299;&#20915;ODMHQA&#26102;&#21487;&#33021;&#20250;&#29983;&#25104;&#39064;&#22806;&#31572;&#26696;&#65292;&#21363;&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;&#21407;&#22987;&#38382;&#39064;&#26080;&#20851;&#12290;&#36825;&#19968;&#39064;&#22806;&#31572;&#26696;&#30340;&#38382;&#39064;&#32422;&#21344;&#38169;&#35823;&#31572;&#26696;&#30340;&#19977;&#20998;&#20043;&#19968;&#65292;&#20294;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36776;&#21035;-&gt;&#37325;&#32452;-&gt;&#35299;&#20915;-&gt;&#37325;&#26032;&#20998;&#35299;&#65288;Dr3&#65289;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36776;&#21035;&#22120;&#21033;&#29992;LLMs&#30340;&#22266;&#26377;&#33021;&#21147;&#21028;&#26029;&#29983;&#25104;&#30340;&#31572;&#26696;&#26159;&#21542;&#39064;&#22806;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12393v1 Announce Type: new  Abstract: Open Domain Multi-Hop Question Answering (ODMHQA) plays a crucial role in Natural Language Processing (NLP) by aiming to answer complex questions through multi-step reasoning over retrieved information from external knowledge sources. Recently, Large Language Models (LLMs) have demonstrated remarkable performance in solving ODMHQA owing to their capabilities including planning, reasoning, and utilizing tools. However, LLMs may generate off-topic answers when attempting to solve ODMHQA, namely the generated answers are irrelevant to the original questions. This issue of off-topic answers accounts for approximately one-third of incorrect answers, yet remains underexplored despite its significance. To alleviate this issue, we propose the Discriminate-&gt;Re-Compose-&gt;Re- Solve-&gt;Re-Decompose (Dr3) mechanism. Specifically, the Discriminator leverages the intrinsic capabilities of LLMs to judge whether the generated answers are off-topic. In cases
&lt;/p&gt;</description></item><item><title>AraPoemBERT &#26159;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#38463;&#25289;&#20271;&#35799;&#27468;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#38463;&#25289;&#20271;&#35799;&#27468;&#30456;&#20851;&#30340;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21462;&#24471;&#20102;&#20004;&#39033;&#26032;&#39062;&#20219;&#21153;&#20013;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.12392</link><description>&lt;p&gt;
AraPoemBERT&#65306;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#35799;&#27468;&#20998;&#26512;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12392
&lt;/p&gt;
&lt;p&gt;
AraPoemBERT &#26159;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#38463;&#25289;&#20271;&#35799;&#27468;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#38463;&#25289;&#20271;&#35799;&#27468;&#30456;&#20851;&#30340;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21462;&#24471;&#20102;&#20004;&#39033;&#26032;&#39062;&#20219;&#21153;&#20013;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#25289;&#20271;&#35799;&#27468;&#20197;&#20854;&#20016;&#23500;&#30340;&#35821;&#35328;&#29305;&#24449;&#21644;&#28145;&#21051;&#30340;&#25991;&#21270;&#24847;&#20041;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#25552;&#20986;&#20102;&#29420;&#29305;&#25361;&#25112;&#12290;&#20854;&#32467;&#26500;&#21644;&#32972;&#26223;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#20808;&#36827;&#30340;&#35745;&#31639;&#27169;&#22411;&#36827;&#34892;&#20934;&#30830;&#20998;&#26512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AraPoemBERT&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#22312;&#38463;&#25289;&#20271;&#35799;&#27468;&#25991;&#26412;&#19978;&#39044;&#35757;&#32451;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;AraPoemBERT&#19982;5&#31181;&#19981;&#21516;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#38463;&#25289;&#20271;&#35799;&#27468;&#30456;&#20851;&#30340;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36825;&#20010;&#26032;&#27169;&#22411;&#22312;&#32477;&#22823;&#22810;&#25968;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#25104;&#26524;&#12290;AraPoemBERT&#22312;&#19977;&#39033;&#26032;&#39062;&#20219;&#21153;&#20013;&#30340;&#20004;&#39033;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#29575;&#65306;&#35799;&#20154;&#24615;&#21035;&#20998;&#31867;&#65288;99.34\%&#30340;&#20934;&#30830;&#29575;&#65289;&#21644;&#35799;&#27468;&#33410;&#24459;&#20998;&#31867;&#65288;97.79\%&#30340;&#20934;&#30830;&#29575;&#65289;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#35799;&#27468;&#25276;&#38901;&#20998;&#31867;&#65288;97.73\%&#30340;&#20934;&#30830;&#29575;&#65289;&#20013;&#20063;&#21462;&#24471;&#20102;&#20934;&#30830;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12392v1 Announce Type: cross  Abstract: Arabic poetry, with its rich linguistic features and profound cultural significance, presents a unique challenge to the Natural Language Processing (NLP) field. The complexity of its structure and context necessitates advanced computational models for accurate analysis. In this paper, we introduce AraPoemBERT, an Arabic language model pretrained exclusively on Arabic poetry text. To demonstrate the effectiveness of the proposed model, we compared AraPoemBERT with 5 different Arabic language models on various NLP tasks related to Arabic poetry. The new model outperformed all other models and achieved state-of-the-art results in most of the downstream tasks. AraPoemBERT achieved unprecedented accuracy in two out of three novel tasks: poet's gender classification (99.34\% accuracy), and poetry sub-meter classification (97.79\% accuracy). In addition, the model achieved an accuracy score in poems' rhyme classification (97.73\% accuracy) wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;n&#20803;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;Binding&#20107;&#20214;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#38454;&#27573;&#29983;&#29289;&#21307;&#23398;&#20107;&#20214;&#25552;&#21462;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12386</link><description>&lt;p&gt;
&#19982;&#32852;&#21512;&#23398;&#20064;&#19981;&#30456;&#19978;&#19979;&#30340;&#20998;&#38454;&#27573;&#29983;&#29289;&#21307;&#23398;&#20107;&#20214;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Pipelined Biomedical Event Extraction Rivaling Joint Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;n&#20803;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;Binding&#20107;&#20214;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#38454;&#27573;&#29983;&#29289;&#21307;&#23398;&#20107;&#20214;&#25552;&#21462;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#20107;&#20214;&#25552;&#21462;&#26159;&#19968;&#39033;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#33719;&#21462;&#20107;&#20214;&#65292;&#20854;&#30446;&#26631;&#21253;&#25324;&#20107;&#20214;&#31867;&#22411;&#12289;&#35302;&#21457;&#35789;&#20197;&#21450;&#20107;&#20214;&#20013;&#28041;&#21450;&#30340;&#21508;&#20010;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;n&#20803;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;Binding&#20107;&#20214;&#65292;&#20197;&#25429;&#33719;&#20107;&#20214;&#32972;&#26223;&#21450;&#21442;&#19982;&#32773;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;BioNLP&#20849;&#20139;&#20219;&#21153;&#30340;GE11&#21644;GE13&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20998;&#21035;&#20026;63.14%&#21644;59.40%&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12386v1 Announce Type: cross  Abstract: Biomedical event extraction is an information extraction task to obtain events from biomedical text, whose targets include the type, the trigger, and the respective arguments involved in an event. Traditional biomedical event extraction usually adopts a pipelined approach, which contains trigger identification, argument role recognition, and finally event construction either using specific rules or by machine learning. In this paper, we propose an n-ary relation extraction method based on the BERT pre-training model to construct Binding events, in order to capture the semantic information about an event's context and its participants. The experimental results show that our method achieves promising results on the GE11 and GE13 corpora of the BioNLP shared task with F1 scores of 63.14% and 59.40%, respectively. It demonstrates that by significantly improving theperformance of Binding events, the overall performance of the pipelined even
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36719;&#25552;&#31034;&#24335;&#23398;&#20064;&#26550;&#26500;&#26469;&#25913;&#36827;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20581;&#24247;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;decoder-only&#30340;LLMs&#22312;&#36328;&#39046;&#22495;&#24212;&#29992;&#20013;&#36890;&#36807;Prompt-tuning&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12374</link><description>&lt;p&gt;
&#36890;&#36807;Prompt-tuning&#25913;&#36827;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20581;&#24247;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36719;&#25552;&#31034;&#24335;&#23398;&#20064;&#26550;&#26500;&#26469;&#25913;&#36827;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20581;&#24247;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;decoder-only&#30340;LLMs&#22312;&#36328;&#39046;&#22495;&#24212;&#29992;&#20013;&#36890;&#36807;Prompt-tuning&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#22823;&#22823;&#25913;&#21892;&#20102;&#20174;&#20020;&#24202;&#21465;&#36848;&#20013;&#25552;&#21462;&#24739;&#32773;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#24494;&#35843;&#31574;&#30053;&#30340;&#26041;&#27861;&#22312;&#36328;&#39046;&#22495;&#24212;&#29992;&#20013;&#20855;&#26377;&#26377;&#38480;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#36719;&#25552;&#31034;&#24335;&#23398;&#20064;&#26550;&#26500;&#65292;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#25552;&#31034;&#20197;&#25351;&#23548;LLMs&#26397;&#21521;&#26399;&#26395;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;LLM&#26550;&#26500;&#65292;&#20998;&#21035;&#26159;&#20165;&#32534;&#30721;&#22120;GatorTron&#21644;&#20165;&#35299;&#30721;&#22120;GatorTronGPT&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#20351;&#29992;&#26469;&#33258;2022&#24180;n2c2&#25361;&#25112;&#30340;&#36328;&#26426;&#26500;&#25968;&#25454;&#38598;&#21644;&#26469;&#33258;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#65288;UF&#65289;Health&#30340;&#36328;&#30142;&#30149;&#25968;&#25454;&#38598;&#36827;&#34892;&#20581;&#24247;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;Prompt-tuning&#30340;&#35299;&#30721;&#22120;&#22411;LLMs&#22312;&#36328;&#39046;&#22495;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;GatorTronGPT&#22312;&#20132;&#21449;&#39046;&#22495;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12374v1 Announce Type: new  Abstract: The progress in natural language processing (NLP) using large language models (LLMs) has greatly improved patient information extraction from clinical narratives. However, most methods based on the fine-tuning strategy have limited transfer learning ability for cross-domain applications. This study proposed a novel approach that employs a soft prompt-based learning architecture, which introduces trainable prompts to guide LLMs toward desired outputs. We examined two types of LLM architectures, including encoder-only GatorTron and decoder-only GatorTronGPT, and evaluated their performance for the extraction of social determinants of health (SDoH) using a cross-institution dataset from the 2022 n2c2 challenge and a cross-disease dataset from the University of Florida (UF) Health. The results show that decoder-only LLMs with prompt tuning achieved better performance in cross-domain applications. GatorTronGPT achieved the best F1 scores for 
&lt;/p&gt;</description></item><item><title>RankPrompt &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#25490;&#24207;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12373</link><description>&lt;p&gt;
RankPrompt&#65306;&#36880;&#27493;&#27604;&#36739;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12373
&lt;/p&gt;
&lt;p&gt;
RankPrompt &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#25490;&#24207;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#20687;ChatGPT&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20063;&#23481;&#26131;&#20986;&#29616;&#36923;&#36753;&#38169;&#35823;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#37096;&#32626;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#39564;&#35777;&#22120;&#25110;&#22312;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#19978;&#25237;&#31080;&#65292;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#20154;&#31867;&#27880;&#37322;&#65292;&#35201;&#20040;&#22312;&#23384;&#22312;&#19981;&#19968;&#33268;&#21709;&#24212;&#30340;&#22330;&#26223;&#20013;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RankPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20351;LLMs&#33021;&#22815;&#33258;&#34892;&#23545;&#20854;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#32780;&#26080;&#38656;&#39069;&#22806;&#36164;&#28304;&#12290;RankPrompt&#23558;&#25490;&#24207;&#38382;&#39064;&#20998;&#35299;&#20026;&#22810;&#20010;&#21709;&#24212;&#20043;&#38388;&#30340;&#19968;&#31995;&#21015;&#27604;&#36739;&#65292;&#21033;&#29992;LLMs&#33258;&#21160;&#29983;&#25104;&#27604;&#36739;&#38142;&#20316;&#20026;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;11&#20010;&#31639;&#26415;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RankPrompt&#26174;&#33879;&#25552;&#39640;&#20102;ChatGPT&#21644;GPT-4&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12373v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved impressive performance across various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT are prone to logical errors during their reasoning processes. Existing solutions, which include deploying task-specific verifiers or voting over multiple reasoning paths, either require extensive human annotations or fail in scenarios with inconsistent responses. To address these challenges, we introduce RankPrompt, a new prompting method that enables LLMs to self-rank their responses without additional resources. RankPrompt breaks down the ranking problem into a series of comparisons among diverse responses, leveraging the inherent capabilities of LLMs to generate chains of comparison as contextual exemplars. Our experiments across 11 arithmetic and commonsense reasoning tasks show that RankPrompt significantly enhances the reasoning performance of ChatGPT and GPT-4, with improvements of u
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#27169;&#25311;&#29616;&#23454;&#20010;&#20307;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26500;&#24314;&#29305;&#24449;&#21270;AI&#20195;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20026;&#36825;&#19968;&#20219;&#21153;&#21019;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.12368</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#29305;&#24449;&#21270;AI&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Characteristic AI Agents via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12368
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#27169;&#25311;&#29616;&#23454;&#20010;&#20307;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26500;&#24314;&#29305;&#24449;&#21270;AI&#20195;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20026;&#36825;&#19968;&#20219;&#21153;&#21019;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24050;&#26497;&#22823;&#22686;&#24378;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#35768;&#22810;&#30740;&#31350;&#32773;&#33268;&#21147;&#20110;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#36171;&#20104;&#29305;&#24449;&#12290;&#23613;&#31649;&#24050;&#26377;&#21830;&#19994;&#20135;&#21697;&#21033;&#29992;LLMs&#24320;&#21457;&#38754;&#21521;&#35282;&#33394;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20294;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#23398;&#26415;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#27169;&#25311;&#19981;&#21516;&#24773;&#22659;&#19979;&#30340;&#29616;&#23454;&#20010;&#20307;&#65292;&#25506;&#35752;LLMs&#26500;&#24314;&#29305;&#24449;&#21270;AI&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#20855;&#26377;&#31616;&#21333;&#37197;&#32622;&#25991;&#20214;&#30340;&#35282;&#33394;&#36827;&#34892;&#25805;&#20316;&#12290;&#38024;&#23545;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#20026;&#29305;&#24449;&#21270;AI&#20195;&#29702;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#25216;&#26415;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#19968;&#20010;&#21517;&#20026;&#8220;Character100&#8221;&#30340;&#25968;&#25454;&#38598;&#34987;&#24314;&#31435;&#29992;&#20110;&#36825;&#19968;&#22522;&#20934;&#65292;&#21253;&#25324;&#32500;&#22522;&#30334;&#31185;&#19978;&#35775;&#38382;&#37327;&#26368;&#39640;&#30340;&#20154;&#29289;&#20379;&#35821;&#35328;&#27169;&#22411;&#25198;&#28436;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12368v1 Announce Type: cross  Abstract: The advancement of Large Language Models (LLMs) has led to significant enhancements in the performance of chatbot systems. Many researchers have dedicated their efforts to the development of bringing characteristics to chatbots. While there have been commercial products for developing role-driven chatbots using LLMs, it is worth noting that academic research in this area remains relatively scarce. Our research focuses on investigating the performance of LLMs in constructing Characteristic AI Agents by simulating real-life individuals across different settings. Current investigations have primarily focused on act on roles with simple profiles. In response to this research gap, we create a benchmark for the characteristic AI agents task, including dataset, techniques, and evaluation metrics. A dataset called ``Character100'' is built for this benchmark, comprising the most-visited people on Wikipedia for language models to role-play. Wit
&lt;/p&gt;</description></item><item><title>&#25991;&#26412;&#25968;&#25454;&#20013;&#27010;&#24565;&#28418;&#31227;&#26159;&#19968;&#20010;&#24120;&#35265;&#29616;&#35937;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#31181;&#25991;&#26412;&#28418;&#31227;&#29983;&#25104;&#26041;&#27861;&#26469;&#24110;&#21161;&#20135;&#29983;&#20855;&#26377;&#26631;&#35760;&#28418;&#31227;&#30340;&#25968;&#25454;&#38598;</title><link>https://arxiv.org/abs/2403.12328</link><description>&lt;p&gt;
&#29983;&#25104;&#25991;&#26412;&#27969;&#20013;&#28418;&#31227;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Methods for Generating Drift in Text Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12328
&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#20013;&#27010;&#24565;&#28418;&#31227;&#26159;&#19968;&#20010;&#24120;&#35265;&#29616;&#35937;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#31181;&#25991;&#26412;&#28418;&#31227;&#29983;&#25104;&#26041;&#27861;&#26469;&#24110;&#21161;&#20135;&#29983;&#20855;&#26377;&#26631;&#35760;&#28418;&#31227;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.12328v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;&#31995;&#32479;&#21644;&#20010;&#20307;&#19981;&#26029;&#20135;&#29983;&#25968;&#25454;&#12290; &#22312;&#20114;&#32852;&#32593;&#19978;&#65292;&#20154;&#20204;&#20998;&#20139;&#20182;&#20204;&#30340;&#30693;&#35782;&#65292;&#24773;&#24863;&#21644;&#24847;&#35265;&#65292;&#25552;&#20379;&#20851;&#20110;&#26381;&#21153;&#21644;&#20135;&#21697;&#30340;&#35780;&#35770;&#31561;&#12290; &#33258;&#21160;&#20174;&#36825;&#20123;&#25991;&#26412;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#20197;&#20026;&#32452;&#32455;&#21644;&#26426;&#26500;&#25552;&#20379;&#35265;&#35299;&#65292;&#20174;&#32780;&#38450;&#27490;&#36130;&#21153;&#24433;&#21709;&#65292;&#20363;&#22914;&#12290; &#20026;&#20102;&#38543;&#26102;&#38388;&#23398;&#20064;&#25991;&#26412;&#25968;&#25454;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24517;&#39035;&#32771;&#34385;&#27010;&#24565;&#28418;&#31227;&#12290; &#27010;&#24565;&#28418;&#31227;&#26159;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#39057;&#32321;&#29616;&#35937;&#65292;&#23545;&#24212;&#20110;&#26102;&#38388;&#19978;&#30340;&#25968;&#25454;&#20998;&#24067;&#26356;&#25913;&#12290; &#20363;&#22914;&#65292;&#24403;&#24773;&#24863;&#21464;&#21270;&#25110;&#21333;&#35789;&#21547;&#20041;&#38543;&#26102;&#38388;&#35843;&#25972;&#26102;&#65292;&#23601;&#20250;&#21457;&#29983;&#27010;&#24565;&#28418;&#31227;&#12290; &#23613;&#31649;&#27010;&#24565;&#28418;&#31227;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#20855;&#26377;&#26631;&#35760;&#28418;&#31227;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#22312;&#25991;&#29486;&#20013;&#24456;&#23569;&#35265;&#12290; &#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#22235;&#31181;&#25991;&#26412;&#28418;&#31227;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#20415;&#31616;&#21270;&#20135;&#29983;&#20855;&#26377;&#26631;&#35760;&#28418;&#31227;&#30340;&#25968;&#25454;&#38598;&#12290; &#36825;&#20123;&#26041;&#27861;&#24050;&#24212;&#29992;&#20110;Ye
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12328v1 Announce Type: cross  Abstract: Systems and individuals produce data continuously. On the Internet, people share their knowledge, sentiments, and opinions, provide reviews about services and products, and so on. Automatically learning from these textual data can provide insights to organizations and institutions, thus preventing financial impacts, for example. To learn from textual data over time, the machine learning system must account for concept drift. Concept drift is a frequent phenomenon in real-world datasets and corresponds to changes in data distribution over time. For instance, a concept drift occurs when sentiments change or a word's meaning is adjusted over time. Although concept drift is frequent in real-world applications, benchmark datasets with labeled drifts are rare in the literature. To bridge this gap, this paper provides four textual drift generation methods to ease the production of datasets with labeled drifts. These methods were applied to Ye
&lt;/p&gt;</description></item><item><title>OpenEval&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#27979;&#35797;&#24179;&#21488;&#65292;&#36890;&#36807;&#23545;&#20013;&#25991;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#33021;&#21147;&#12289;&#23545;&#40784;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#24573;&#35270;&#23545;&#40784;&#24615;&#21644;&#23433;&#20840;&#38382;&#39064;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.12316</link><description>&lt;p&gt;
OpenEval&#65306;&#22312;&#33021;&#21147;&#12289;&#23545;&#40784;&#24615;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#23545;&#20013;&#25991;LLM&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12316
&lt;/p&gt;
&lt;p&gt;
OpenEval&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#27979;&#35797;&#24179;&#21488;&#65292;&#36890;&#36807;&#23545;&#20013;&#25991;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#33021;&#21147;&#12289;&#23545;&#40784;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#24573;&#35270;&#23545;&#40784;&#24615;&#21644;&#23433;&#20840;&#38382;&#39064;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12316v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#39640;&#25928;&#30340;LLM&#35780;&#20272;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#20030;&#25514;&#24050;&#32463;&#25512;&#20986;&#20102;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25110;&#35780;&#20272;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;LLMs&#65292;&#20294;&#20854;&#20013;&#35768;&#22810;&#20027;&#35201;&#20851;&#27880;&#33021;&#21147;&#65292;&#36890;&#24120;&#24573;&#35270;&#20102;&#28508;&#22312;&#30340;&#23545;&#40784;&#24615;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;OpenEval&#65292;&#19968;&#20010;&#35780;&#20272;&#27979;&#35797;&#24179;&#21488;&#65292;&#20197;&#22312;&#33021;&#21147;&#12289;&#23545;&#40784;&#24615;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#23545;&#20013;&#25991;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#23545;&#20110;&#33021;&#21147;&#35780;&#20272;&#65292;&#25105;&#20204;&#21253;&#25324;12&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20174;4&#20010;&#23376;&#32500;&#24230;&#35780;&#20272;&#20013;&#25991;LLMs&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#23398;&#31185;&#30693;&#35782;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#12290;&#23545;&#20110;&#23545;&#40784;&#24615;&#35780;&#20272;&#65292;OpenEval&#21253;&#21547;7&#20010;&#25968;&#25454;&#38598;&#65292;&#26816;&#26597;&#20013;&#25991;LLMs&#20135;&#29983;&#30340;&#36755;&#20986;&#20013;&#30340;&#20559;&#35265;&#12289;&#20882;&#29359;&#24615;&#21644;&#36829;&#27861;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#23433;&#20840;&#24615;&#65292;&#29305;&#21035;&#26159;&#39640;&#32423;LLMs&#30340;&#39044;&#26399;&#39118;&#38505;&#65288;&#22914;&#23547;&#27714;&#26435;&#21147;&#12289;&#33258;&#25105;&#24847;&#35782;&#31561;&#65289;&#65292;&#25105;&#20204;&#21253;&#25324;6&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12316v1 Announce Type: new  Abstract: The rapid development of Chinese large language models (LLMs) poses big challenges for efficient LLM evaluation. While current initiatives have introduced new benchmarks or evaluation platforms for assessing Chinese LLMs, many of these focus primarily on capabilities, usually overlooking potential alignment and safety issues. To address this gap, we introduce OpenEval, an evaluation testbed that benchmarks Chinese LLMs across capability, alignment and safety. For capability assessment, we include 12 benchmark datasets to evaluate Chinese LLMs from 4 sub-dimensions: NLP tasks, disciplinary knowledge, commonsense reasoning and mathematical reasoning. For alignment assessment, OpenEval contains 7 datasets that examines the bias, offensiveness and illegalness in the outputs yielded by Chinese LLMs. To evaluate safety, especially anticipated risks (e.g., power-seeking, self-awareness) of advanced LLMs, we include 6 datasets. In addition to th
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#20449;&#24687;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#22312;&#35299;&#26512;&#22797;&#26434;&#20020;&#24202;&#35821;&#35328;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;</title><link>https://arxiv.org/abs/2403.12297</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#20449;&#24687;&#65306;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#20449;&#24687;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#22312;&#35299;&#26512;&#22797;&#26434;&#20020;&#24202;&#35821;&#35328;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#65288;SUD&#65289;&#30001;&#20110;&#23545;&#20581;&#24247;&#21644;&#31038;&#20250;&#30340;&#26377;&#23475;&#24433;&#21709;&#32780;&#24341;&#36215;&#20102;&#37325;&#22823;&#20851;&#27880;&#12290; SUD&#30340;&#35782;&#21035;&#21644;&#27835;&#30103;&#21462;&#20915;&#20110;&#35832;&#22810;&#22240;&#32032;&#65292;&#22914;&#20005;&#37325;&#31243;&#24230;&#12289;&#32852;&#21512;&#20915;&#23450;&#22240;&#32032;&#65288;&#20363;&#22914;&#25106;&#26029;&#30151;&#29366;&#65289;&#21644;&#20581;&#24247;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#12290; &#32654;&#22269;&#20445;&#38505;&#25552;&#20379;&#21830;&#20351;&#29992;&#30340;&#29616;&#26377;&#35786;&#26029;&#32534;&#30721;&#31995;&#32479;&#65292;&#22914;&#22269;&#38469;&#30142;&#30149;&#20998;&#31867;&#65288;ICD-10&#65289;&#65292;&#23545;&#20110;&#26576;&#20123;&#35786;&#26029;&#32570;&#20047;&#32454;&#33268;&#24230;&#65292;&#20294;&#20020;&#24202;&#21307;&#29983;&#20250;&#23558;&#27492;&#32454;&#33268;&#24230;&#65288;&#22914;&#12298;&#31934;&#31070;&#38556;&#30861;&#35786;&#26029;&#19982;&#32479;&#35745;&#25163;&#20876;&#12299;&#20998;&#31867;&#25110;DSM-5&#20013;&#25152;&#21457;&#29616;&#30340;&#65289;&#20316;&#20026;&#36741;&#21161;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28155;&#21152;&#21040;&#20020;&#24202;&#35760;&#24405;&#20013;&#12290; &#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#22312;&#20934;&#30830;&#35299;&#26512;&#36825;&#31181;&#22810;&#26679;&#21270;&#30340;&#20020;&#24202;&#35821;&#35328;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290; &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#36866;&#24212;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#27169;&#24335;&#65292;&#26377;&#26395;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290; &#26412;&#30740;&#31350;&#35843;&#26597;&#20102;LLMs&#22312;&#25552;&#21462;&#20005;&#37325;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12297v1 Announce Type: cross  Abstract: Substance use disorder (SUD) poses a major concern due to its detrimental effects on health and society. SUD identification and treatment depend on a variety of factors such as severity, co-determinants (e.g., withdrawal symptoms), and social determinants of health. Existing diagnostic coding systems used by American insurance providers, like the International Classification of Diseases (ICD-10), lack granularity for certain diagnoses, but clinicians will add this granularity (as that found within the Diagnostic and Statistical Manual of Mental Disorders classification or DSM-5) as supplemental unstructured text in clinical notes. Traditional natural language processing (NLP) methods face limitations in accurately parsing such diverse clinical language. Large Language Models (LLMs) offer promise in overcoming these challenges by adapting to diverse language patterns. This study investigates the application of LLMs for extracting severi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;DALL-E 2&#22312;&#35270;&#35273;&#19978;&#22914;&#20309;&#20195;&#34920;&#35821;&#35328;&#25552;&#31034;&#30340;&#21547;&#20041;&#65292;&#32467;&#26524;&#26174;&#31034;DALL-E 2&#26410;&#33021;&#29983;&#25104;&#19982;&#20799;&#31461;&#35821;&#20041;&#20934;&#30830;&#24615;&#30456;&#21305;&#37197;&#30340;&#22270;&#20687;&#65292;&#25351;&#21521;&#20102;&#21477;&#23376;&#34920;&#36798;&#30340;&#32452;&#25104;&#24615;&#30340;&#26126;&#26174;&#32570;&#22833;&#12290;</title><link>https://arxiv.org/abs/2403.12294</link><description>&lt;p&gt;
DALL-E 2&#20013;&#32452;&#21512;&#35821;&#27861;&#21644;&#35821;&#20041;&#30340;&#27604;&#36739;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Comparative Investigation of Compositional Syntax and Semantics in DALL-E 2
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12294
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;DALL-E 2&#22312;&#35270;&#35273;&#19978;&#22914;&#20309;&#20195;&#34920;&#35821;&#35328;&#25552;&#31034;&#30340;&#21547;&#20041;&#65292;&#32467;&#26524;&#26174;&#31034;DALL-E 2&#26410;&#33021;&#29983;&#25104;&#19982;&#20799;&#31461;&#35821;&#20041;&#20934;&#30830;&#24615;&#30456;&#21305;&#37197;&#30340;&#22270;&#20687;&#65292;&#25351;&#21521;&#20102;&#21477;&#23376;&#34920;&#36798;&#30340;&#32452;&#25104;&#24615;&#30340;&#26126;&#26174;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;DALL-E 2&#22914;&#20309;&#22312;&#35270;&#35273;&#19978;&#20195;&#34920;&#32473;&#20104;&#24180;&#24188;&#20799;&#31461;&#36827;&#34892;&#29702;&#35299;&#27979;&#35797;&#30340;&#35821;&#35328;&#25552;&#31034;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#20174;&#29992;&#20110;&#25968;&#30334;&#21517;2-7&#23681;&#33521;&#35821;&#27597;&#35821;&#20799;&#31461;&#30340;&#35780;&#20272;&#27979;&#35797;&#20013;&#36873;&#25321;&#20102;&#20195;&#34920;&#35821;&#27861;&#30693;&#35782;&#22522;&#26412;&#32452;&#20214;&#30340;&#21477;&#23376;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#36825;&#20123;&#20799;&#31461;&#30340;&#21407;&#22987;&#39033;&#30446;&#32423;&#25968;&#25454;&#12290;DALL-E 2&#34987;&#32473;&#20104;&#36825;&#20123;&#25552;&#31034;&#20116;&#27425;&#65292;&#20197;&#29983;&#25104;&#27599;&#20010;&#39033;&#30446;20&#24133;&#21345;&#36890;&#22270;&#29255;&#65292;&#20379;9&#21517;&#25104;&#24180;&#35780;&#22996;&#35780;&#20998;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27809;&#26377;&#19968;&#20010;&#26465;&#20214;&#19979;&#65292;DALL-E 2&#29983;&#25104;&#30340;&#22270;&#29255;&#19982;&#20799;&#31461;&#30340;&#35821;&#20041;&#20934;&#30830;&#24615;&#30456;&#21305;&#37197;&#65292;&#21363;&#20351;&#22312;&#26368;&#24180;&#24188;&#30340;&#24180;&#40836;&#65288;2&#23681;&#65289;&#20063;&#26159;&#22914;&#27492;&#12290;DALL-E 2&#26410;&#33021;&#22312;&#21487;&#36870;&#24418;&#24335;&#20013;&#20998;&#37197;&#36866;&#24403;&#30340;&#35282;&#33394;&#65307;&#22312;&#21542;&#23450;&#24418;&#24335;&#19978;&#22833;&#36133;&#65292;&#23613;&#31649;&#25552;&#31034;&#27604;&#20799;&#31461;&#25509;&#25910;&#21040;&#30340;&#23545;&#27604;&#25552;&#31034;&#26356;&#31616;&#21333;&#65307;&#23427;&#32463;&#24120;&#23558;&#24418;&#23481;&#35789;&#20998;&#37197;&#32473;&#38169;&#35823;&#30340;&#21517;&#35789;&#65307;&#23427;&#24573;&#30053;&#20102;&#34987;&#21160;&#21477;&#20013;&#30340;&#38544;&#21547;&#20027;&#35821;&#12290;&#36825;&#39033;&#24037;&#20316;&#25351;&#21521;&#20102;&#21477;&#23376;&#34920;&#36798;&#30340;&#32452;&#25104;&#24615;&#30340;&#26126;&#26174;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12294v1 Announce Type: new  Abstract: In this study we compared how well DALL-E 2 visually represented the meaning of linguistic prompts also given to young children in comprehension tests. Sentences representing fundamental components of grammatical knowledge were selected from assessment tests used with several hundred English-speaking children aged 2-7 years for whom we had collected original item-level data. DALL-E 2 was given these prompts five times to generate 20 cartoons per item, for 9 adult judges to score. Results revealed no conditions in which DALL-E 2-generated images that matched the semantic accuracy of children, even at the youngest age (2 years). DALL-E 2 failed to assign the appropriate roles in reversible forms; it failed on negation despite an easier contrastive prompt than the children received; it often assigned the adjective to the wrong noun; it ignored implicit agents in passives. This work points to a clear absence of compositional sentence represe
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Llama 2 7B&#27169;&#22411;&#30340;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#30340;&#24773;&#24863;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#26469;&#21463;&#30410;&#20110;&#20854;&#29983;&#25104;&#24615;&#36136;&#21644;&#20840;&#38754;&#30340;&#35821;&#35328;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.12285</link><description>&lt;p&gt;
FinLlama&#65306;&#29992;&#20110;&#31639;&#27861;&#20132;&#26131;&#24212;&#29992;&#30340;&#37329;&#34701;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12285
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Llama 2 7B&#27169;&#22411;&#30340;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#30340;&#24773;&#24863;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#26469;&#21463;&#30410;&#20110;&#20854;&#29983;&#25104;&#24615;&#36136;&#21644;&#20840;&#38754;&#30340;&#35821;&#35328;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#26032;&#38395;&#22312;&#32447;&#26377;&#22810;&#20010;&#26469;&#28304;&#24433;&#21709;&#24066;&#22330;&#36208;&#21183;&#21644;&#20132;&#26131;&#21592;&#30340;&#20915;&#31574;&#12290;&#36825;&#31361;&#20986;&#20102;&#20934;&#30830;&#24773;&#24863;&#20998;&#26512;&#30340;&#38656;&#27714;&#65292;&#38500;&#20102;&#38656;&#35201;&#36866;&#24403;&#30340;&#31639;&#27861;&#20132;&#26131;&#25216;&#26415;&#26469;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20132;&#26131;&#20915;&#31574;&#12290;&#26631;&#20934;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#24773;&#24863;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#23427;&#20204;&#22312;&#36741;&#21161;&#37329;&#34701;&#20915;&#31574;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#23427;&#20204;&#23384;&#22312;&#19982;&#19978;&#19979;&#25991;&#25935;&#24863;&#24615;&#21644;&#35789;&#24207;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20063;&#21487;&#20197;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#19981;&#26159;&#29305;&#23450;&#20110;&#37329;&#34701;&#39046;&#22495;&#30340;&#65292;&#24182;&#19988;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#20419;&#36827;&#19968;&#31181;&#29305;&#23450;&#20110;&#37329;&#34701;&#39046;&#22495;&#30340;LLM&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Llama 2 7B&#22522;&#30784;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#20415;&#20174;&#20854;&#29983;&#25104;&#24615;&#36136;&#21644;&#20840;&#38754;&#30340;&#35821;&#35328;&#25805;&#20316;&#20013;&#21463;&#30410;&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#23569;&#37096;&#20998;&#30417;&#30563;&#37329;&#34701;&#24773;&#24863;&#25968;&#25454;&#19978;&#24494;&#35843;Llama2 7B&#27169;&#22411;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12285v1 Announce Type: new  Abstract: There are multiple sources of financial news online which influence market movements and trader's decisions. This highlights the need for accurate sentiment analysis, in addition to having appropriate algorithmic trading techniques, to arrive at better informed trading decisions. Standard lexicon based sentiment approaches have demonstrated their power in aiding financial decisions. However, they are known to suffer from issues related to context sensitivity and word ordering. Large Language Models (LLMs) can also be used in this context, but they are not finance-specific and tend to require significant computational resources. To facilitate a finance specific LLM framework, we introduce a novel approach based on the Llama 2 7B foundational model, in order to benefit from its generative nature and comprehensive language manipulation. This is achieved by fine-tuning the Llama2 7B model on a small portion of supervised financial sentiment 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#23450;&#37327;&#26816;&#27979;&#24187;&#35273;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#27169;&#22411;&#24863;&#30693;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;0.78&#30340;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.12244</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#22810;&#20219;&#21153;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Multi-task Hallucination Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12244
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#23450;&#37327;&#26816;&#27979;&#24187;&#35273;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#27169;&#22411;&#24863;&#30693;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;0.78&#30340;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#21033;&#29992;&#31361;&#26174;&#20102;&#35780;&#20272;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#21644;&#19982;&#29305;&#23450;&#20219;&#21153;&#30456;&#20851;&#24615;&#30340;&#31283;&#20581;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#25581;&#31034;&#20102;&#19968;&#20010;&#26222;&#36941;&#30340;&#38382;&#39064;&#65292;&#21363;&#24187;&#35273;&#65292;&#36825;&#26159;&#27169;&#22411;&#20013;&#30340;&#19968;&#31181;&#32039;&#24613;&#24773;&#20917;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#26469;&#28304;&#32570;&#20047;&#24544;&#23454;&#24230;&#65292;&#24182;&#19988;&#20559;&#31163;&#20102;&#35780;&#20272;&#26631;&#20934;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#23450;&#37327;&#26816;&#27979;&#24187;&#35273;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#23450;&#20041;&#21644;&#20551;&#35774;&#65292;&#21363;&#27169;&#22411;&#36755;&#20986;&#21253;&#21547;&#19982;&#20219;&#21153;&#21644;&#26679;&#26412;&#29305;&#23450;&#36755;&#20837;&#26377;&#20851;&#30340;&#20449;&#24687;&#12290;&#22312;&#26816;&#27979;&#24187;&#35273;&#26102;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#27169;&#22411;&#24863;&#30693;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;0.78&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;&#27169;&#22411;&#26080;&#20851;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;0.61&#30340;&#20934;&#30830;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20445;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#38656;&#35201;&#27604;&#20854;&#20182;SOTA&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#19982;&#36731;&#37327;&#32423;&#21644;&#21387;&#32553;&#30340;&#36235;&#21183;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12244v1 Announce Type: new  Abstract: In recent studies, the extensive utilization of large language models has underscored the importance of robust evaluation methodologies for assessing text generation quality and relevance to specific tasks. This has revealed a prevalent issue known as hallucination, an emergent condition in the model where generated text lacks faithfulness to the source and deviates from the evaluation criteria. In this study, we formally define hallucination and propose a framework for its quantitative detection in a zero-shot setting, leveraging our definition and the assumption that model outputs entail task and sample specific inputs. In detecting hallucinations, our solution achieves an accuracy of 0.78 in a model-aware setting and 0.61 in a model-agnostic setting. Notably, our solution maintains computational efficiency, requiring far less computational resources than other SOTA approaches, aligning with the trend towards lightweight and compressed
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12242</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;
&lt;/p&gt;
&lt;p&gt;
Reference-based Metrics Disprove Themselves in Question Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12242
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BLEU&#21644;BERTScore&#31561;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#38382;&#21477;&#29983;&#25104;(QG)&#12290;&#26412;&#30740;&#31350;&#22312;SQuAD&#21644;HotpotQA&#31561;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25991;&#29486;&#24182;&#19981;&#33021;&#20445;&#35777;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;&#22823;&#22810;&#25968;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#21482;&#26377;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#65307;&#25105;&#20204;&#22797;&#21046;&#20102;&#27880;&#37322;&#36807;&#31243;&#24182;&#25910;&#38598;&#20102;&#21478;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#12290;&#39044;&#26399;&#22909;&#30340;&#25351;&#26631;&#24212;&#35813;&#23545;&#20154;&#24037;&#39564;&#35777;&#30340;&#38382;&#39064;&#30340;&#35780;&#20998;&#19981;&#20250;&#20302;&#20110;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#26032;&#25910;&#38598;&#30340;&#21442;&#32771;&#25991;&#29486;&#19978;&#65292;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#32467;&#26524;&#21364;&#35777;&#26126;&#20102;&#36825;&#20123;&#25351;&#26631;&#26412;&#36523;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#65292;&#30001;&#22810;&#32500;&#26631;&#20934;&#32452;&#25104;&#65292;&#22914;&#33258;&#28982;&#24615;&#12289;&#21487;&#22238;&#31572;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#26631;&#20934;&#19981;&#21463;&#38480;&#20110;&#21333;&#20010;&#21442;&#32771;&#38382;&#39064;&#30340;&#21477;&#27861;&#25110;&#35821;&#20041;&#65292;&#35813;&#25351;&#26631;&#20063;&#19981;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12242v1 Announce Type: cross  Abstract: Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our met
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22312;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#20351;&#29992;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12212</link><description>&lt;p&gt;
&#35780;&#20272;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#27604;&#36739;&#20998;&#26512;&#24052;&#35199;&#20844;&#21496;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#19978;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22312;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#20351;&#29992;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19968;&#31181;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20851;&#20110;NER&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#33521;&#35821;&#25991;&#26723;&#19978;&#65292;&#23548;&#33268;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36130;&#21153;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#37329;&#34701;&#39046;&#22495;&#20869;NER&#38656;&#27714;&#65292;&#24182;&#20391;&#37325;&#20110;&#20174;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#25552;&#21462;&#30340;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#12290;&#36890;&#36807;&#25972;&#29702;&#21253;&#25324;384&#20010;&#36716;&#24405;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#36827;&#34892;&#27880;&#37322;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#33889;&#33796;&#29273;&#35821;&#65288;BERTimbau&#21644;PTT5&#65289;&#35757;&#32451;&#30340;&#21333;&#35821;&#27169;&#22411;&#20197;&#21450;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;mBERT&#21644;mT5&#65289;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;T5&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#35780;&#20272;&#12290;&#22312;&#27169;&#22411;&#24494;&#35843;&#20043;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12212v1 Announce Type: cross  Abstract: Named Entity Recognition (NER) is a Natural Language Processing technique for extracting information from textual documents. However, much of the existing research on NER has been centered around English-language documents, leaving a gap in the availability of datasets tailored to the financial domain in Portuguese. This study addresses the need for NER within the financial domain, focusing on Portuguese-language texts extracted from earnings call transcriptions of Brazilian banks. By curating a comprehensive dataset comprising 384 transcriptions and leveraging weak supervision techniques for annotation, we evaluate the performance of monolingual models trained on Portuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5). Notably, we introduce a novel approach that reframes the token classification task as a text generation problem, enabling fine-tuning and evaluation of T5 models. Following the fine-tuning of the models,
&lt;/p&gt;</description></item><item><title>TnT-LLM &#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#29983;&#25104;&#21644;&#20998;&#37197;&#26631;&#31614;&#65292;&#20943;&#23569;&#20154;&#21147;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.12173</link><description>&lt;p&gt;
TnT-LLM&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19979;&#30340;&#25991;&#26412;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
TnT-LLM: Text Mining at Scale with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12173
&lt;/p&gt;
&lt;p&gt;
TnT-LLM &#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#29983;&#25104;&#21644;&#20998;&#37197;&#26631;&#31614;&#65292;&#20943;&#23569;&#20154;&#21147;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#26377;&#24847;&#20041;&#30340;&#24418;&#24335;&#65292;&#36890;&#36807;&#26377;&#29992;&#30340;&#31867;&#21035;&#26631;&#31614;&#36827;&#34892;&#32452;&#32455;&#65292;&#26159;&#25991;&#26412;&#25366;&#25496;&#20013;&#29992;&#20110;&#19979;&#28216;&#20998;&#26512;&#21644;&#24212;&#29992;&#30340;&#22522;&#30784;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#25104;&#26631;&#31614;&#20998;&#31867;&#27861;&#21644;&#26500;&#24314;&#22522;&#20110;&#25991;&#26412;&#26631;&#31614;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#25163;&#21160;&#25972;&#29702;&#65292;&#20351;&#24471;&#36825;&#20010;&#36807;&#31243;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#24403;&#26631;&#31614;&#31354;&#38388;&#19981;&#26126;&#30830;&#19988;&#32570;&#23569;&#22823;&#35268;&#27169;&#25968;&#25454;&#27880;&#37322;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23588;&#20026;&#20005;&#23803;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20854;&#22522;&#20110;&#25552;&#31034;&#30340;&#25509;&#21475;&#26377;&#21161;&#20110;&#24341;&#23548;&#21644;&#20351;&#29992;&#22823;&#35268;&#27169;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TnT-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#26631;&#31614;&#29983;&#25104;&#21644;&#20998;&#37197;&#30340;&#36807;&#31243;&#65292;&#20943;&#23569;&#20154;&#21147;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12173v1 Announce Type: cross  Abstract: Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach w
&lt;/p&gt;</description></item><item><title>EasyJailbreak&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#31616;&#21270;&#20102;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#25915;&#20987;&#30340;&#26500;&#24314;&#21644;&#35780;&#20272;&#65292;&#25903;&#25345;11&#31181;&#36234;&#29425;&#26041;&#27861;&#65292;&#24110;&#21161;&#36827;&#34892;&#24191;&#27867;&#33539;&#22260;LLMs&#30340;&#23433;&#20840;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.12171</link><description>&lt;p&gt;
EasyJailbreak: &#19968;&#20010;&#29992;&#20110;&#36234;&#29425;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12171
&lt;/p&gt;
&lt;p&gt;
EasyJailbreak&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#31616;&#21270;&#20102;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#25915;&#20987;&#30340;&#26500;&#24314;&#21644;&#35780;&#20272;&#65292;&#25903;&#25345;11&#31181;&#36234;&#29425;&#26041;&#27861;&#65292;&#24110;&#21161;&#36827;&#34892;&#24191;&#27867;&#33539;&#22260;LLMs&#30340;&#23433;&#20840;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#29425;&#25915;&#20987;&#23545;&#20110;&#35782;&#21035;&#21644;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#28431;&#27934;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#20204;&#34987;&#35774;&#35745;&#29992;&#20110;&#32469;&#36807;&#20445;&#38556;&#25514;&#26045;&#24182;&#24341;&#21457;&#34987;&#31105;&#27490;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#36234;&#29425;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#38024;&#23545;&#31038;&#21306;&#25552;&#20379;&#26631;&#20934;&#23454;&#29616;&#26694;&#26550;&#65292;&#36825;&#38480;&#21046;&#20102;&#20840;&#38754;&#30340;&#23433;&#20840;&#35780;&#20272;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EasyJailbreak&#65292;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#31616;&#21270;&#20102;&#38024;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#25915;&#20987;&#30340;&#26500;&#24314;&#21644;&#35780;&#20272;&#12290;&#23427;&#20351;&#29992;&#22235;&#20010;&#32452;&#20214;&#26469;&#26500;&#24314;&#36234;&#29425;&#25915;&#20987;&#65306;&#36873;&#25321;&#22120;&#12289;&#21464;&#24322;&#22120;&#12289;&#32422;&#26463;&#22120;&#21644;&#35780;&#20272;&#22120;&#12290;&#36825;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#20351;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#22320;&#20174;&#26032;&#30340;&#21644;&#29616;&#26377;&#32452;&#20214;&#30340;&#32452;&#21512;&#20013;&#26500;&#24314;&#25915;&#20987;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;EasyJailbreak&#25903;&#25345;11&#31181;&#19981;&#21516;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#20419;&#36827;&#20102;&#23545;&#24191;&#27867;&#33539;&#22260;LLMs&#30340;&#23433;&#20840;&#39564;&#35777;&#12290;&#25105;&#20204;&#22312;10&#20010;&#19981;&#21516;&#30340;LLMs&#19978;&#30340;&#39564;&#35777;&#25581;&#31034;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12171v1 Announce Type: cross  Abstract: Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of Large Language Models (LLMs). They are designed to bypass safeguards and elicit prohibited outputs. However, due to significant differences among various jailbreak methods, there is no standard implementation framework available for the community, which limits comprehensive security evaluations. This paper introduces EasyJailbreak, a unified framework simplifying the construction and evaluation of jailbreak attacks against LLMs. It builds jailbreak attacks using four components: Selector, Mutator, Constraint, and Evaluator. This modular framework enables researchers to easily construct attacks from combinations of novel and existing components. So far, EasyJailbreak supports 11 distinct jailbreak methods and facilitates the security validation of a broad spectrum of LLMs. Our validation across 10 distinct LLMs reveals a significant vulnerabilit
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.12151</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20869;&#23481;&#34701;&#20837;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#22686;&#24378;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12151
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#26377;&#21161;&#20110;&#35299;&#20915;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#65292;&#20294;&#29983;&#25104;&#36825;&#31181;&#30693;&#35782;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36890;&#36807;&#35821;&#20041;&#23884;&#20837;&#29983;&#25104;&#21644;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#23558;LLM&#38598;&#25104;&#21040;&#19968;&#20010;&#27969;&#31243;&#20013;&#65292;&#35813;&#27969;&#31243;&#22312;&#35270;&#35273;&#22522;&#30784;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#21521;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#24443;&#24213;&#30740;&#31350;&#20102;LLM&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#22522;&#20110;LLM&#30340;&#23884;&#20837;&#19982;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20511;&#37492;&#36825;&#19968;&#28040;&#34701;&#30740;&#31350;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#23545;&#31454;&#20105;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20174;&#32780;&#31361;&#20986;&#20102;&#26368;&#26032;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12151v1 Announce Type: new  Abstract: Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks. However, the generation of such knowledge entails considerable human labor and time costs. This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings. To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task. We thoroughly examine the behavior of the LLM through an extensive ablation study. Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art perfor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21512;&#25104;QA&#25968;&#25454;&#38598;Syn-(QA)$^2$&#30340;&#24341;&#20837;&#65292;&#20316;&#32773;&#21457;&#29616;&#22312;&#38271;&#23614;&#38382;&#39064;&#19978;&#30340;&#38169;&#35823;&#20551;&#35774;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#20108;&#20803;&#26816;&#27979;&#20219;&#21153;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2403.12145</link><description>&lt;p&gt;
Syn-QA2: &#20351;&#29992;&#21512;&#25104;QA&#25968;&#25454;&#38598;&#35780;&#20272;&#38271;&#23614;&#38382;&#39064;&#20013;&#30340;&#38169;&#35823;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Syn-QA2: Evaluating False Assumptions in Long-tail Questions with Synthetic QA Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12145
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;QA&#25968;&#25454;&#38598;Syn-(QA)$^2$&#30340;&#24341;&#20837;&#65292;&#20316;&#32773;&#21457;&#29616;&#22312;&#38271;&#23614;&#38382;&#39064;&#19978;&#30340;&#38169;&#35823;&#20551;&#35774;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#20108;&#20803;&#26816;&#27979;&#20219;&#21153;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#20013;&#30340;&#38169;&#35823;&#20551;&#35774;(&#25110;&#38169;&#35823;&#21069;&#25552;)&#23545;&#20110;&#31283;&#20581;&#30340;&#38382;&#31572;(QA)&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;&#33258;&#28982;&#21457;&#29983;&#30340;&#38382;&#39064;&#19978;&#65292;&#23384;&#22312;&#20851;&#20110;&#27169;&#22411;&#22312;&#21487;&#33021;&#38382;&#39064;&#20998;&#24067;&#30340;&#38271;&#23614;&#19978;&#30340;&#34892;&#20026;&#20998;&#26512;&#30340;&#31354;&#30333;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Syn-(QA)$^2$&#65292;&#19968;&#20010;&#21253;&#21547;&#20004;&#20010;&#21512;&#25104;&#29983;&#25104;&#30340;QA&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#26159;&#20351;&#29992;&#26469;&#33258;Wikidata&#30340;&#25200;&#21160;&#20851;&#31995;&#29983;&#25104;&#30340;&#65292;&#21478;&#19968;&#20010;&#26159;&#36890;&#36807;&#25200;&#21160;HotpotQA&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#20174;&#35780;&#20272;&#19968;&#31995;&#21015;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#20986;&#30340;&#21457;&#29616;&#26377;&#19977;&#20010;&#65306;(1) QA&#20013;&#30340;&#38169;&#35823;&#20551;&#35774;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#21628;&#24212;&#20102;&#20808;&#21069;&#24037;&#20316;&#30340;&#21457;&#29616;&#65292;(2) &#19982;&#19981;&#21487;&#21306;&#20998;&#20219;&#21153;&#30456;&#27604;&#65292;&#20108;&#20803;&#26816;&#27979;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12145v1 Announce Type: new  Abstract: Sensitivity to false assumptions (or false premises) in information-seeking questions is critical for robust question-answering (QA) systems. Recent work has shown that false assumptions in naturally occurring questions pose challenges to current models, with low performance on both generative QA and simple detection tasks (Kim et al. 2023). However, the focus of existing work on naturally occurring questions leads to a gap in the analysis of model behavior on the long tail of the distribution of possible questions. To this end, we introduce Syn-(QA)$^2$, a set of two synthetically generated QA datasets: one generated using perturbed relations from Wikidata, and the other by perturbing HotpotQA (Yang et al. 2018). Our findings from evaluating a range of large language models are threefold: (1) false assumptions in QA are challenging, echoing the findings of prior work, (2) the binary detection task is challenging even compared to the dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19977;&#31181;&#27969;&#34892;LLMs&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#19978;&#30340;&#34920;&#29616;&#20173;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.12094</link><description>&lt;p&gt;
LLMs&#26159;&#19968;&#20010;&#22909;&#30340;&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#27714;&#35299;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Good Cryptic Crossword Solvers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19977;&#31181;&#27969;&#34892;LLMs&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#19978;&#30340;&#34920;&#29616;&#20173;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#26159;&#19968;&#31181;&#35868;&#39064;&#65292;&#19981;&#20165;&#20381;&#36182;&#20110;&#19968;&#33324;&#30693;&#35782;&#65292;&#36824;&#20381;&#36182;&#20110;&#27714;&#35299;&#32773;&#22312;&#19981;&#21516;&#23618;&#38754;&#19978;&#25805;&#32437;&#35821;&#35328;&#24182;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#23383;&#28216;&#25103;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#23545;&#20110;&#29616;&#20195;NLP&#27169;&#22411;&#26469;&#35828;&#65292;&#35299;&#20915;&#36825;&#31867;&#35868;&#39064;&#20063;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#23578;&#26410;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#19977;&#31181;&#27969;&#34892;&#30340;LLMs -- LLaMA2&#12289;Mistral&#21644;ChatGPT&#24314;&#31435;&#20102;&#22522;&#20934;&#32467;&#26524;&#65292;&#26174;&#31034;&#23427;&#20204;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20173;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12094v1 Announce Type: new  Abstract: Cryptic crosswords are puzzles that rely not only on general knowledge but also on the solver's ability to manipulate language on different levels and deal with various types of wordplay. Previous research suggests that solving such puzzles is a challenge even for modern NLP models. However, the abilities of large language models (LLMs) have not yet been tested on this task. In this paper, we establish the benchmark results for three popular LLMs -- LLaMA2, Mistral, and ChatGPT -- showing that their performance on this task is still far from that of humans.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23450;&#20041;&#24182;&#35268;&#33539;&#20102;&#29983;&#25104;&#33521;&#35821;&#22320;&#22336;&#21305;&#37197;&#23545;&#30340;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#36317;&#31163;&#22522;&#20934;&#26041;&#27861;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31561;&#21508;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#20934;&#30830;&#24230;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#22320;&#22336;&#21305;&#37197;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12092</link><description>&lt;p&gt;
&#21305;&#37197;&#33521;&#35821;&#22320;&#22336;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Methods for Matching English Language Addresses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12092
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23450;&#20041;&#24182;&#35268;&#33539;&#20102;&#29983;&#25104;&#33521;&#35821;&#22320;&#22336;&#21305;&#37197;&#23545;&#30340;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#36317;&#31163;&#22522;&#20934;&#26041;&#27861;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31561;&#21508;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#20934;&#30830;&#24230;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#22320;&#22336;&#21305;&#37197;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#22336;&#22312;&#25991;&#26412;&#25968;&#25454;&#20013;&#21344;&#25454;&#30528;&#19968;&#24109;&#20043;&#22320;&#65292;&#22240;&#20026;&#27599;&#20010;&#35789;&#25152;&#20855;&#26377;&#30340;&#20301;&#32622;&#37325;&#35201;&#24615;&#21644;&#23427;&#25152;&#28041;&#21450;&#30340;&#22320;&#29702;&#33539;&#22260;&#12290;&#21305;&#37197;&#22320;&#22336;&#30340;&#20219;&#21153;&#27599;&#22825;&#37117;&#22312;&#21457;&#29983;&#65292;&#24182;&#19988;&#23384;&#22312;&#20110;&#37038;&#20214;&#37325;&#23450;&#21521;&#12289;&#23454;&#20307;&#35299;&#26512;&#31561;&#21508;&#31181;&#39046;&#22495;&#20013;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23450;&#20041;&#21644;&#35268;&#33539;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#33521;&#35821;&#22320;&#22336;&#30340;&#21305;&#37197;&#21644;&#19981;&#21305;&#37197;&#23545;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#26041;&#27861;&#33258;&#21160;&#25191;&#34892;&#22320;&#22336;&#21305;&#37197;&#12290;&#36825;&#20123;&#26041;&#27861;&#20174;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21508;&#19981;&#30456;&#21516;&#12290;&#36890;&#36807;&#30740;&#31350;&#36825;&#20123;&#26041;&#27861;&#30340;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#20934;&#30830;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#21487;&#20197;&#20102;&#35299;&#21040;&#26368;&#36866;&#21512;&#36825;&#31181;&#22320;&#22336;&#21305;&#37197;&#20219;&#21153;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12092v1 Announce Type: cross  Abstract: Addresses occupy a niche location within the landscape of textual data, due to the positional importance carried by every word, and the geographical scope it refers to. The task of matching addresses happens everyday and is present in various fields like mail redirection, entity resolution, etc. Our work defines, and formalizes a framework to generate matching and mismatching pairs of addresses in the English language, and use it to evaluate various methods to automatically perform address matching. These methods vary widely from distance based approaches to deep learning models. By studying the Precision, Recall and Accuracy metrics of these approaches, we obtain an understanding of the best suited method for this setting of the address matching task.
&lt;/p&gt;</description></item><item><title>&#22810;&#20262;&#22810;&#37117;&#20250;&#22823;&#23398;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21442;&#21152;TREC&#20020;&#24202;&#35797;&#39564;&#36319;&#36394;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.12088</link><description>&lt;p&gt;
TMU&#21442;&#21152;2023&#24180;TREC&#20020;&#24202;&#35797;&#39564;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
TMU at TREC Clinical Trials Track 2023
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12088
&lt;/p&gt;
&lt;p&gt;
&#22810;&#20262;&#22810;&#37117;&#20250;&#22823;&#23398;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21442;&#21152;TREC&#20020;&#24202;&#35797;&#39564;&#36319;&#36394;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#22810;&#20262;&#22810;&#37117;&#20250;&#22823;&#23398;&#21442;&#21152;2023&#24180;TREC&#20020;&#24202;&#35797;&#39564;&#36319;&#36394;&#12290;&#20316;&#20026;&#20219;&#21153;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#21033;&#29992;&#20102;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#32034;&#26368;&#30456;&#20851;&#30340;&#20020;&#24202;&#35797;&#39564;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#22242;&#38431;-V-TorontoMU&#30340;&#21442;&#19982;&#30340;&#25972;&#20307;&#26041;&#27861;&#35770;&#12289;&#23454;&#39564;&#35774;&#32622;&#21644;&#23454;&#29616;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12088v1 Announce Type: new  Abstract: This paper describes Toronto Metropolitan University's participation in the TREC Clinical Trials Track for 2023. As part of the tasks, we utilize advanced natural language processing techniques and neural language models in our experiments to retrieve the most relevant clinical trials. We illustrate the overall methodology, experimental settings, and results of our implementation for the run submission as part of Team - V-TorontoMU.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Terrorizer&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#32593;&#32476;&#29702;&#35770;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#24402;&#22240;&#20110;&#20844;&#21496;&#30340;&#19987;&#21033;&#20013;&#23384;&#22312;&#30340;&#21517;&#31216;&#21464;&#20307;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12083</link><description>&lt;p&gt;
&#23637;&#31034; Terrorizer&#65306;&#19968;&#31181;&#29992;&#20110;&#25972;&#21512;&#19987;&#21033;&#21463;&#35753;&#20154;&#20013;&#20844;&#21496;&#21517;&#31216;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Presenting Terrorizer: an algorithm for consolidating company names in patent assignees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Terrorizer&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#32593;&#32476;&#29702;&#35770;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#24402;&#22240;&#20110;&#20844;&#21496;&#30340;&#19987;&#21033;&#20013;&#23384;&#22312;&#30340;&#21517;&#31216;&#21464;&#20307;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#21496;&#21517;&#31216;&#28040;&#27495;&#30340;&#38382;&#39064;&#22312;&#20174;&#19987;&#21033;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#20250;&#23548;&#33268;&#30740;&#31350;&#32467;&#26524;&#23384;&#22312;&#20559;&#24046;&#65292;&#22240;&#20026;&#23427;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20302;&#20272;&#20102;&#24402;&#22240;&#20110;&#20844;&#21496;&#30340;&#19987;&#21033;&#25968;&#37327;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20197;&#20247;&#22810;&#21517;&#31216;&#65288;&#21253;&#25324;&#30456;&#21516;&#23454;&#20307;&#30340;&#26367;&#20195;&#25340;&#20889;&#21644;&#26368;&#32456;&#20844;&#21496;&#30340;&#23376;&#20844;&#21496;&#65289;&#30003;&#35831;&#19987;&#21033;&#30340;&#36328;&#22269;&#20844;&#21496;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#20027;&#35201;&#20381;&#36182;&#20110;&#32791;&#26102;&#30340;&#22522;&#20110;&#35789;&#20856;&#25110;&#23383;&#31526;&#20018;&#21305;&#37197;&#26041;&#27861;&#65292;&#32780;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#35299;&#20915;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#19987;&#21033;&#21463;&#35753;&#20154;&#30340;&#21327;&#35843;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25551;&#36848;&#20102; Terrorizer &#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#25991;&#26412;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#32593;&#32476;&#29702;&#35770;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#25216;&#26415;&#26469;&#21327;&#35843;&#20316;&#20026;&#19987;&#21033;&#21463;&#35753;&#20154;&#35760;&#24405;&#30340;&#20844;&#21496;&#21517;&#31216;&#30340;&#21464;&#20307;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31639;&#27861;&#36981;&#24490;&#20854;&#21069;&#20307;&#30340;&#19977;&#37096;&#20998;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12083v1 Announce Type: cross  Abstract: The problem of disambiguation of company names poses a significant challenge in extracting useful information from patents. This issue biases research outcomes as it mostly underestimates the number of patents attributed to companies, particularly multinational corporations which file patents under a plethora of names, including alternate spellings of the same entity and, eventually, companies' subsidiaries. To date, addressing these challenges has relied on labor-intensive dictionary based or string matching approaches, leaving the problem of patents' assignee harmonization on large datasets mostly unresolved. To bridge this gap, this paper describes the Terrorizer algorithm, a text-based algorithm that leverages natural language processing (NLP), network theory, and rule-based techniques to harmonize the variants of company names recorded as patent assignees. In particular, the algorithm follows the tripartite structure of its antece
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23567;&#35268;&#27169;&#23454;&#39564;&#21457;&#29616;&#65292;&#20174;LLM&#20013;&#21024;&#38500;&#21704;&#21033;&#27874;&#29305;&#20869;&#23481;&#27604;&#20808;&#21069;&#25253;&#36947;&#30340;&#26356;&#21152;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.12082</link><description>&lt;p&gt;
&#29983;&#36824;&#30340;&#30007;&#23401;&#65306;&#20174;LLM&#20013;&#21024;&#38500;&#21704;&#21033;&#27874;&#29305;&#27604;&#25253;&#36947;&#30340;&#26356;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
The Boy Who Survived: Removing Harry Potter from an LLM is harder than reported
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23567;&#35268;&#27169;&#23454;&#39564;&#21457;&#29616;&#65292;&#20174;LLM&#20013;&#21024;&#38500;&#21704;&#21033;&#27874;&#29305;&#20869;&#23481;&#27604;&#20808;&#21069;&#25253;&#36947;&#30340;&#26356;&#21152;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;"&#25105;&#20204;&#26377;&#25928;&#22320;&#25273;&#38500;&#20102;&#27169;&#22411;&#29983;&#25104;&#25110;&#22238;&#24518;&#21704;&#21033;&#27874;&#29305;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;"&#28982;&#32780;&#65292;&#19968;&#39033;&#23567;&#35268;&#27169;&#23454;&#39564;&#34920;&#26126;&#36825;&#19968;&#35828;&#27861;&#36807;&#20110;&#23485;&#27867;&#12290;&#23569;&#20110;&#21313;&#27425;&#35797;&#39564;&#23548;&#33268;&#37325;&#22797;&#21644;&#20855;&#20307;&#25552;&#21450;&#21704;&#21033;&#27874;&#29305;&#65292;&#21253;&#25324;"&#21834;&#65292;&#25105;&#26126;&#30333;&#20102;&#65281;"&#40635;&#29916;"&#26159;&#29305;&#37324;&#183;&#26222;&#25289;&#20999;&#29305;&#30340;&#21704;&#21033;&#27874;&#29305;&#31995;&#21015;&#20013;&#20351;&#29992;&#30340;&#26415;&#35821;...''&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12082v1 Announce Type: cross  Abstract: Recent work arXiv.2310.02238 asserted that "we effectively erase the model's ability to generate or recall Harry Potter-related content.'' This claim is shown to be overbroad. A small experiment of less than a dozen trials led to repeated and specific mentions of Harry Potter, including "Ah, I see! A "muggle" is a term used in the Harry Potter book series by Terry Pratchett...''
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#23545;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#30340;&#20581;&#22766;&#24615;&#65292;&#36890;&#36807;&#23545;&#22810;&#31181;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20154;&#31867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#22312;&#35825;&#23548;&#19981;&#27491;&#30830;&#21709;&#24212;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12077</link><description>&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#23545;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#30340;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12077
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#23545;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#30340;&#20581;&#22766;&#24615;&#65292;&#36890;&#36807;&#23545;&#22810;&#31181;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20154;&#31867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#22312;&#35825;&#23548;&#19981;&#27491;&#30830;&#21709;&#24212;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#26377;&#28508;&#21147;&#25913;&#21464;&#20154;&#20204;&#22312;&#32447;&#33719;&#21462;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#20294;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25903;&#25345;&#30340;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#29983;&#25104;&#30340;&#21709;&#24212;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#20934;&#30830;&#12290;&#28982;&#32780;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20250;&#21152;&#21095;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#23545;&#25163;&#21487;&#33021;&#36890;&#36807;&#24494;&#22937;&#22320;&#25805;&#32437;&#22768;&#26126;&#30340;&#26368;&#34180;&#24369;&#37096;&#20998;&#25104;&#21151;&#35268;&#36991;&#25972;&#20010;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#30340;&#29616;&#23454;&#19988;&#39640;&#39118;&#38505;&#35774;&#32622;&#20013;&#35780;&#20272;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#30340;&#20581;&#22766;&#24615;&#65292;&#20854;&#20013;&#23545;&#25163;&#20165;&#20855;&#26377;&#40657;&#30418;&#31995;&#32479;&#35775;&#38382;&#26435;&#38480;&#65292;&#24182;&#35797;&#22270;&#27450;&#39575;&#27169;&#22411;&#36820;&#22238;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#12290;&#36890;&#36807;&#23545;&#24517;&#24212;&#32842;&#22825;&#12289;PerplexityAI&#21644;YouChat&#31561;&#21508;&#31181;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20840;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#23545;&#35825;&#23548;&#19981;&#27491;&#30830;&#21709;&#24212;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#23637;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12077v1 Announce Type: cross  Abstract: Generative search engines have the potential to transform how people seek information online, but generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate. Nonetheless, retrieval-augmented generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable part of a claim. To this end, we propose evaluating the robustness of generative search engines in the realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning incorrect responses. Through a comprehensive human evaluation of various generative search engines, such as Bing Chat, PerplexityAI, and YouChat across diverse queries, we demonstrate the effectiveness of adversarial factual questions in inducing incorrect responses. Moreover, retrieval-augmented generation exhibits a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#30340;JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;&#65292;&#36890;&#36807;PEFT&#20860;&#23481;&#24494;&#35843;Llama-2&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;JAX&#30340;&#21363;&#26102;&#32534;&#35793;&#21644;&#24352;&#37327;&#20998;&#29255;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#31649;&#29702;&#65292;&#21152;&#36895;&#24494;&#35843;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;RAG&#24212;&#29992;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11366</link><description>&lt;p&gt;
JORA: &#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#30340;JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;
&lt;/p&gt;
&lt;p&gt;
JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11366
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#30340;JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;&#65292;&#36890;&#36807;PEFT&#20860;&#23481;&#24494;&#35843;Llama-2&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;JAX&#30340;&#21363;&#26102;&#32534;&#35793;&#21644;&#24352;&#37327;&#20998;&#29255;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#31649;&#29702;&#65292;&#21152;&#36895;&#24494;&#35843;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;RAG&#24212;&#29992;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;JORA: JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#12299;&#36890;&#36807;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#20219;&#21153;&#30340;PEFT&#20860;&#23481;&#24494;&#35843;Llama-2&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#29420;&#29305;&#22320;&#21033;&#29992;&#20102;JAX&#30340;&#21363;&#26102;&#32534;&#35793;&#65288;JIT&#65289;&#21644;&#24352;&#37327;&#20998;&#29255;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#30340;&#39640;&#25928;&#31649;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21152;&#36895;&#24494;&#35843;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#12290;&#36825;&#19968;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29992;&#20110;&#22797;&#26434;RAG&#24212;&#29992;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#29978;&#33267;&#22312;GPU&#36164;&#28304;&#26377;&#38480;&#30340;&#31995;&#32479;&#19978;&#20063;&#33021;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11366v1 Announce Type: cross  Abstract: The scaling of Large Language Models (LLMs) for retrieval-based tasks, particularly in Retrieval Augmented Generation (RAG), faces significant memory constraints, especially when fine-tuning extensive prompt sequences. Current open-source libraries support full-model inference and fine-tuning across multiple GPUs but fall short of accommodating the efficient parameter distribution required for retrieved context. Addressing this gap, we introduce a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT) compilation and tensor-sharding for efficient resource management, thereby enabling accelerated fine-tuning with reduced memory requirements. This advancement significantly improves the scalability and feasibility of fine-tuning LLMs for complex RAG applications, even on systems with limited GPU resources. Our experiments show more than 1
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#21333;&#27425;&#38750;&#20405;&#20837;&#24615;&#33041;&#35760;&#24405;&#20013;&#35299;&#30721;&#36830;&#32493;&#35821;&#35328;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#32500;&#21367;&#31215;&#32593;&#32476;&#21644;&#20449;&#24687;&#29942;&#39048;&#32467;&#21512;&#23383;&#31526;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616;&#22312;&#21644;&#36328;&#20027;&#20307;&#20043;&#38388;&#20135;&#29983;&#25429;&#25417;&#24863;&#30693;&#35821;&#38899;&#21547;&#20041;&#30340;&#21487;&#29702;&#35299;&#25991;&#26412;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.11183</link><description>&lt;p&gt;
&#20174;&#38750;&#20405;&#20837;&#24615;&#33041;&#35760;&#24405;&#35299;&#30721;&#36830;&#32493;&#22522;&#20110;&#23383;&#31526;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Decoding Continuous Character-based Language from Non-invasive Brain Recordings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11183
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#21333;&#27425;&#38750;&#20405;&#20837;&#24615;&#33041;&#35760;&#24405;&#20013;&#35299;&#30721;&#36830;&#32493;&#35821;&#35328;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#32500;&#21367;&#31215;&#32593;&#32476;&#21644;&#20449;&#24687;&#29942;&#39048;&#32467;&#21512;&#23383;&#31526;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616;&#22312;&#21644;&#36328;&#20027;&#20307;&#20043;&#38388;&#20135;&#29983;&#25429;&#25417;&#24863;&#30693;&#35821;&#38899;&#21547;&#20041;&#30340;&#21487;&#29702;&#35299;&#25991;&#26412;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#35774;&#22791;&#20174;&#22823;&#33041;&#27963;&#21160;&#20013;&#35299;&#35835;&#33258;&#28982;&#35821;&#35328;&#20173;&#28982;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#20043;&#21069;&#30340;&#38750;&#20405;&#20837;&#24335;&#35299;&#30721;&#22120;&#35201;&#20040;&#38656;&#35201;&#36827;&#34892;&#22810;&#27425;&#23454;&#39564;&#26469;&#30830;&#23450;&#30382;&#23618;&#21306;&#22495;&#24182;&#22686;&#24378;&#22823;&#33041;&#27963;&#21160;&#30340;&#20449;&#22122;&#27604;&#65292;&#35201;&#20040;&#20165;&#38480;&#20110;&#35782;&#21035;&#22522;&#26412;&#30340;&#35821;&#35328;&#20803;&#32032;&#22914;&#23383;&#27597;&#21644;&#21333;&#35789;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#21333;&#27425;&#38750;&#20405;&#20837;&#24615;fMRI&#35760;&#24405;&#20013;&#35299;&#30721;&#36830;&#32493;&#35821;&#35328;&#65292;&#20854;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#19977;&#32500;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#37197;&#22791;&#20449;&#24687;&#29942;&#39048;&#20197;&#33258;&#21160;&#35782;&#21035;&#23545;&#21050;&#28608;&#26377;&#21709;&#24212;&#30340;&#20307;&#32032;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#23383;&#31526;&#30340;&#35299;&#30721;&#22120;, &#29992;&#20110;&#23545;&#20869;&#22312;&#23383;&#31526;&#32467;&#26500;&#25152;&#29305;&#24449;&#21270;&#30340;&#36830;&#32493;&#35821;&#35328;&#36827;&#34892;&#35821;&#20041;&#37325;&#24314;&#12290;&#25152;&#24471;&#35299;&#30721;&#22120;&#33021;&#22815;&#29983;&#25104;&#21487;&#29702;&#35299;&#30340;&#25991;&#26412;&#24207;&#21015;&#65292;&#24544;&#23454;&#22320;&#25429;&#25417;&#24863;&#30693;&#35821;&#38899;&#30340;&#21547;&#20041;&#65292;&#26080;&#35770;&#26159;&#22312;&#21516;&#19968;&#20027;&#20307;&#20869;&#36824;&#26159;&#36328;&#20027;&#20307;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11183v1 Announce Type: new  Abstract: Deciphering natural language from brain activity through non-invasive devices remains a formidable challenge. Previous non-invasive decoders either require multiple experiments with identical stimuli to pinpoint cortical regions and enhance signal-to-noise ratios in brain activity, or they are limited to discerning basic linguistic elements such as letters and words. We propose a novel approach to decoding continuous language from single-trial non-invasive fMRI recordings, in which a three-dimensional convolutional network augmented with information bottleneck is developed to automatically identify responsive voxels to stimuli, and a character-based decoder is designed for the semantic reconstruction of continuous language characterized by inherent character structures. The resulting decoder can produce intelligible textual sequences that faithfully capture the meaning of perceived speech both within and across subjects, while existing d
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09891</link><description>&lt;p&gt;
Fisher Mask&#33410;&#28857;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Fisher Mask Nodes for Language Model Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09891
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#21450;&#20854;&#34893;&#29983;&#29289;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26222;&#36941;&#24615;&#20063;&#23548;&#33268;&#20102;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#27169;&#22411;&#30340;&#28608;&#22686;&#12290;&#22312;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21482;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#19968;&#39033;&#20219;&#21153;&#65292;&#22240;&#27492;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#38598;&#25104;&#12290;&#27169;&#22411;&#21512;&#24182;&#36825;&#19968;&#19981;&#26029;&#22686;&#38271;&#30340;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#23558;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#21512;&#24182;&#20026;&#21333;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;Transformers&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20808;&#21069;Fisher&#21152;&#26435;&#24179;&#22343;&#21644;Fisher&#20449;&#24687;&#22312;&#27169;&#22411;&#20462;&#21098;&#20013;&#30340;&#24212;&#29992;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#21033;&#29992;Transformer&#26550;&#26500;&#20869;&#30340;mask&#33410;&#28857;&#30340;Fisher&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#21152;&#26435;&#24179;&#22343;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#31283;&#23450;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09891v1 Announce Type: cross  Abstract: Fine-tuning pre-trained models provides significant advantages in downstream performance. The ubiquitous nature of pre-trained models such as BERT and its derivatives in natural language processing has also led to a proliferation of task-specific fine-tuned models. As these models typically only perform one task well, additional training or ensembling is required in multi-task scenarios. The growing field of model merging provides a solution, dealing with the challenge of combining multiple task-specific models into a single multi-task model. In this study, we introduce a novel model merging method for Transformers, combining insights from previous work in Fisher-weighted averaging and the use of Fisher information in model pruning. Utilizing the Fisher information of mask nodes within the Transformer architecture, we devise a computationally efficient weighted-averaging scheme. Our method exhibits a regular and significant performance
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.09611</link><description>&lt;p&gt;
MM1&#65306;&#22810;&#27169;&#24335;LLM&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12289;&#20998;&#26512;&#19982;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09611
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#21644;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#20180;&#32454;&#21644;&#20840;&#38754;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#32463;&#39564;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#22823;&#35268;&#27169;&#22810;&#27169;&#24335;&#39044;&#35757;&#32451;&#20351;&#29992;&#20180;&#32454;&#28151;&#21512;&#30340;&#22270;&#20687;&#26631;&#39064;&#12289;&#20132;&#26367;&#22270;&#20687;&#25991;&#26412;&#21644;&#20165;&#25991;&#26412;&#25968;&#25454;&#23545;&#20110;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#26032;&#28526;&#65288;SOTA&#65289;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#19982;&#20854;&#20182;&#24050;&#21457;&#34920;&#30340;&#39044;&#35757;&#32451;&#32467;&#26524;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#22270;&#20687;&#32534;&#30721;&#22120;&#36830;&#21516;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#22270;&#20687;&#26631;&#35760;&#35745;&#25968;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#32780;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#35774;&#35745;&#30456;&#23545;&#37325;&#35201;&#24615;&#36739;&#23567;&#12290;&#36890;&#36807;&#25193;&#22823;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;MM1&#65292;&#19968;&#20010;&#22810;&#27169;&#24335;&#27169;&#22411;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09611v1 Announce Type: cross  Abstract: In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up 
&lt;/p&gt;</description></item><item><title>Komodo-7B&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26080;&#32541;&#25805;&#20316;&#21360;&#24230;&#23612;&#35199;&#20122;&#12289;&#33521;&#35821;&#21644;11&#31181;&#21360;&#24230;&#23612;&#35199;&#20122;&#22320;&#21306;&#35821;&#35328;&#65292;Komodo-7B-Instruct&#36798;&#21040;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#22810;&#20010;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09362</link><description>&lt;p&gt;
&#31185;&#33707;&#22810;&#65306;&#25506;&#32034;&#21360;&#24230;&#23612;&#35199;&#20122;&#22320;&#21306;&#35821;&#35328;&#30340;&#35821;&#35328;&#32771;&#23519;
&lt;/p&gt;
&lt;p&gt;
Komodo: A Linguistic Expedition into Indonesia's Regional Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09362
&lt;/p&gt;
&lt;p&gt;
Komodo-7B&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26080;&#32541;&#25805;&#20316;&#21360;&#24230;&#23612;&#35199;&#20122;&#12289;&#33521;&#35821;&#21644;11&#31181;&#21360;&#24230;&#23612;&#35199;&#20122;&#22320;&#21306;&#35821;&#35328;&#65292;Komodo-7B-Instruct&#36798;&#21040;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#22810;&#20010;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26041;&#38754;&#21462;&#24471;&#30340;&#31361;&#30772;&#20027;&#35201;&#38598;&#20013;&#22312;&#20855;&#26377;&#26131;&#20110;&#33719;&#21462;&#21644;&#20805;&#36275;&#36164;&#28304;&#30340;&#35821;&#35328;&#19978;&#65292;&#20363;&#22914;&#33521;&#35821;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#20844;&#20849;&#39046;&#22495;&#32570;&#20047;&#36275;&#22815;&#35821;&#35328;&#36164;&#28304;&#30340;&#35821;&#35328;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;Komodo-7B&#65292;&#19968;&#20010;70&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#12289;&#33521;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#30340;11&#31181;&#22320;&#21306;&#35821;&#35328;&#20043;&#38388;&#26080;&#32541;&#25805;&#20316;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;Komodo-7B&#26159;&#19968;&#32452;LLMs&#65292;&#30001;Komodo-7B-Base&#21644;Komodo-7B-Instruct&#32452;&#25104;&#12290;Komodo-7B-Instruct&#20973;&#20511;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#35821;&#35328;&#19978;&#21462;&#24471;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#33073;&#39062;&#32780;&#20986;&#65292;&#36229;&#36234;&#20102;OpenAI&#30340;GPT-3.5&#12289;Cohere&#30340;Aya-101&#12289;Llama-2-Chat-13B&#12289;Mixtral-8x7B-Instruct-v0.1&#12289;Gemma-7B-it&#31561;&#27169;&#22411;&#21046;&#23450;&#30340;&#22522;&#20934;&#12290;&#36825;&#20010;&#27169;&#22411;&#19981;&#20165;&#22312;&#35821;&#35328;&#29305;&#23450;&#21644;&#25972;&#20307;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36824;&#31361;&#26174;&#20102;&#20854;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09362v1 Announce Type: new  Abstract: The recent breakthroughs in Large Language Models (LLMs) have mostly focused on languages with easily available and sufficient resources, such as English. However, there remains a significant gap for languages that lack sufficient linguistic resources in the public domain. Our work introduces Komodo-7B, 7-billion-parameter Large Language Models designed to address this gap by seamlessly operating across Indonesian, English, and 11 regional languages in Indonesia. Komodo-7B is a family of LLMs that consist of Komodo-7B-Base and Komodo-7B-Instruct. Komodo-7B-Instruct stands out by achieving state-of-the-art performance in various tasks and languages, outperforming the benchmarks set by OpenAI's GPT-3.5, Cohere's Aya-101, Llama-2-Chat-13B, Mixtral-8x7B-Instruct-v0.1, Gemma-7B-it , and many more. This model not only demonstrates superior performance in both language-specific and overall assessments but also highlights its capability to excel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07311</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;KG-LLM&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Large Language Model (KG-LLM) for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20998;&#26512;&#39046;&#22495;&#65292;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20869;&#22810;&#20010;&#38142;&#25509;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#36825;&#19968;&#25361;&#25112;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#35299;&#20915;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20851;&#38190;&#30340;NLP&#33539;&#20363;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;KG&#36716;&#25442;&#20026;CoT&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35782;&#21035;&#24182;&#23398;&#20064;&#23454;&#20307;&#21450;&#20854;&#30456;&#20114;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20026;&#20102;&#23637;&#31034;KG-LLM&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#35813;&#26694;&#26550;&#20869;&#24494;&#35843;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#38750;ICL&#21644;ICL&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#26694;&#26550;&#20026;LLMs&#25552;&#20379;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;CNER-UAV&#65292;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#32422;&#26377;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;</title><link>https://arxiv.org/abs/2403.06097</link><description>&lt;p&gt;
&#33021;&#21542;&#29992;LLM&#26367;&#20195;&#20154;&#24037;&#26631;&#27880;&#65311; &#26080;&#20154;&#26426;&#20132;&#20184;&#20219;&#21153;&#19979;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22320;&#22336;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;CNER-UAV&#65292;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#32422;&#26377;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CNER-UAV&#65292;&#19968;&#20010;&#19987;&#20026;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#35774;&#35745;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20116;&#20010;&#31867;&#21035;&#65292;&#21487;&#20197;&#20840;&#38754;&#35757;&#32451;&#21644;&#35780;&#20272;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#12290;&#20026;&#26500;&#24314;&#36825;&#19968;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#33719;&#21462;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#30830;&#20445;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#26368;&#32456;&#30340;&#25968;&#25454;&#38598;&#32422;&#21253;&#21547;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#32463;&#36807;&#20154;&#24037;&#19987;&#23478;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312; \url{https://github.com/zhhvvv/CNER-UAV} &#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06097v1 Announce Type: cross  Abstract: We present CNER-UAV, a fine-grained \textbf{C}hinese \textbf{N}ame \textbf{E}ntity \textbf{R}ecognition dataset specifically designed for the task of address resolution in \textbf{U}nmanned \textbf{A}erial \textbf{V}ehicle delivery systems. The dataset encompasses a diverse range of five categories, enabling comprehensive training and evaluation of NER models. To construct this dataset, we sourced the data from a real-world UAV delivery system and conducted a rigorous data cleaning and desensitization process to ensure privacy and data integrity. The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and \textbf{L}arge \textbf{L}anguage \textbf{M}odel annotation. We evaluated classical NER models on our dataset and provided in-depth analysis. The dataset and models are publicly available at \url{https://github.com/zhhvvv/CNER-UAV}.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;KG-Rank&#26694;&#26550;&#65292;&#21033;&#29992;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#25490;&#21517;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#33258;&#30001;&#25991;&#26412;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05881</link><description>&lt;p&gt;
KG-Rank: &#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#25490;&#21517;&#25216;&#26415;&#22686;&#24378;&#21307;&#23398;&#38382;&#31572;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;KG-Rank&#26694;&#26550;&#65292;&#21033;&#29992;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#25490;&#21517;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#33258;&#30001;&#25991;&#26412;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25512;&#36827;&#20102;&#21307;&#30103;&#20445;&#20581;&#21019;&#26032;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#33021;&#20559;&#31163;&#21307;&#30103;&#20107;&#23454;&#21644;&#22266;&#26377;&#20559;&#35265;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#20020;&#24202;&#35774;&#32622;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22686;&#24378;&#22411;LLM&#26694;&#26550;KG-Rank&#65292;&#21033;&#29992;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19982;&#25490;&#21517;&#21644;&#37325;&#26032;&#25490;&#21517;&#25216;&#26415;&#65292;&#26088;&#22312;&#25913;&#36827;&#21307;&#23398;&#39046;&#22495;&#33258;&#30001;&#25991;&#26412;&#38382;&#31572;&#65288;QA&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#25910;&#21040;&#38382;&#39064;&#21518;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#21307;&#23398;KG&#20013;&#26816;&#32034;&#19977;&#20803;&#32452;&#20197;&#25910;&#38598;&#20107;&#23454;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#24212;&#29992;&#25490;&#21517;&#26041;&#27861;&#26469;&#31934;&#32454;&#35843;&#25972;&#36825;&#20123;&#19977;&#20803;&#32452;&#30340;&#39034;&#24207;&#65292;&#26088;&#22312;&#20135;&#29983;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;KG-Rank&#26159;&#39318;&#20010;&#23558;&#25490;&#21517;&#27169;&#22411;&#19982;KG&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#38271;&#31572;&#26696;&#30340;&#21307;&#23398;&#38382;&#31572;&#24212;&#29992;&#12290;&#23545;&#22235;&#20010;&#36873;&#23450;&#30340;&#21307;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;KG-Rank&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05881v1 Announce Type: new  Abstract: Large Language Models (LLMs) have significantly advanced healthcare innovation on generation capabilities. However, their application in real clinical settings is challenging due to potential deviations from medical facts and inherent biases. In this work, we develop an augmented LLM framework, KG-Rank, which leverages a medical knowledge graph (KG) with ranking and re-ranking techniques, aiming to improve free-text question-answering (QA) in the medical domain. Specifically, upon receiving a question, we initially retrieve triplets from a medical KG to gather factual information. Subsequently, we innovatively apply ranking methods to refine the ordering of these triplets, aiming to yield more precise answers. To the best of our knowledge, KG-Rank is the first application of ranking models combined with KG in medical QA specifically for generating long answers. Evaluation of four selected medical QA datasets shows that KG-Rank achieves a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05326</link><description>&lt;p&gt;
ChatASU&#65306;&#21796;&#36215;LLM&#30340;&#21453;&#24605;&#65292;&#30495;&#27491;&#29702;&#35299;&#23545;&#35805;&#20013;&#30340;&#26041;&#38754;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#21160;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#38382;&#31572;&#21644;&#23545;&#35805;&#65289;&#20013;&#36827;&#34892;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ASU&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#24573;&#30053;&#20102;&#24847;&#35265;&#30446;&#26631;&#65288;&#21363;&#26041;&#38754;&#65289;&#30340;&#20849;&#25351;&#38382;&#39064;&#65292;&#32780;&#36825;&#31181;&#29616;&#35937;&#22312;&#20114;&#21160;&#22330;&#26223;&#29305;&#21035;&#26159;&#23545;&#35805;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#38480;&#21046;&#20102;ASU&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#23558;&#21508;&#31181;NLP&#20219;&#21153;&#19982;&#32842;&#22825;&#33539;&#24335;&#30456;&#32467;&#21512;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#39033;ChatASU&#20219;&#21153;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#21363;&#26041;&#38754;&#38142;&#25512;&#29702;&#65288;ACR&#65289;&#20219;&#21153;&#65292;&#20197;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20449;&#30340;&#33258;&#21453;&#24605;&#26041;&#27861;&#65288;TSA&#65289;&#19982;ChatGLM&#20316;&#20026;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05326v1 Announce Type: cross  Abstract: Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as back
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#31034;&#24341;&#21457;&#29305;&#23450;&#20154;&#26684;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#40657;&#26263;&#19977;&#21512;&#20250;&#30340;&#29305;&#36136;&#23545;&#22810;&#31181;LLMs&#22312;&#19981;&#21516;ToM&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22823;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.02246</link><description>&lt;p&gt;
PHAnToM&#65306;&#20154;&#26684;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#29702;&#35770;&#25512;&#29702;&#20135;&#29983;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02246
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#24341;&#21457;&#29305;&#23450;&#20154;&#26684;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#40657;&#26263;&#19977;&#21512;&#20250;&#30340;&#29305;&#36136;&#23545;&#22810;&#31181;LLMs&#22312;&#19981;&#21516;ToM&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22823;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#19982;&#29978;&#33267;&#20248;&#20110;&#20154;&#31867;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#19968;&#36827;&#23637;&#65292;LLMs&#22312;&#31038;&#20250;&#35748;&#30693;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#19981;&#36275;&#65292;&#32780;&#20154;&#31867;&#22312;&#36825;&#26041;&#38754;&#22825;&#29983;&#23601;&#24456;&#25797;&#38271;&#12290;&#21463;&#21040;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#26576;&#20123;&#20154;&#26684;&#29305;&#36136;&#19982;&#24515;&#29702;&#29702;&#35770;&#65288;ToM&#65289;&#25512;&#29702;&#20043;&#38388;&#32852;&#31995;&#30340;&#21551;&#21457;&#65292;&#20197;&#21450;&#20851;&#20110;&#25552;&#31034;&#24037;&#31243;&#30740;&#31350;&#22312;&#24433;&#21709;LLMs&#33021;&#21147;&#26041;&#38754;&#30340;&#36229;&#25935;&#24863;&#24615;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#25552;&#31034;&#22312;LLMs&#20013;&#24341;&#21457;&#20154;&#26684;&#22914;&#20309;&#24433;&#21709;&#23427;&#20204;&#30340;ToM&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26576;&#20123;&#24341;&#21457;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;LLMs&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;ToM&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#26469;&#33258;&#40657;&#26263;&#19977;&#21512;&#20250;(Dark Triad)&#30340;&#29305;&#36136;&#23545;&#20110;&#20687;GPT-3.5&#12289;Llama 2&#21644;Mistral&#36825;&#26679;&#30340;LLMs&#22312;&#19981;&#21516;&#30340;ToM&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22823;&#30340;&#21464;&#37327;&#25928;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#26576;&#20123;&#20154;&#26684;&#29305;&#36136;&#30340;LLMs&#22312;&#25191;&#34892;ToM&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02246v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) demonstrate that their capabilities are comparable, or even superior, to humans in many tasks in natural language processing. Despite this progress, LLMs are still inadequate at social-cognitive reasoning, which humans are naturally good at. Drawing inspiration from psychological research on the links between certain personality traits and Theory-of-Mind (ToM) reasoning, and from prompt engineering research on the hyper-sensitivity of prompts in affecting LLMs capabilities, this study investigates how inducing personalities in LLMs using prompts affects their ToM reasoning capabilities. Our findings show that certain induced personalities can significantly affect the LLMs' reasoning capabilities in three different ToM tasks. In particular, traits from the Dark Triad have a larger variable effect on LLMs like GPT-3.5, Llama 2, and Mistral across the different ToM tasks. We find that LLMs tha
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#24369;&#30417;&#30563;&#26631;&#27880;&#31243;&#24207;&#21644;&#22522;&#20110;&#21512;&#29702;&#21270;&#32447;&#32034;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#22810;&#39046;&#22495;&#33258;&#21160;&#31616;&#31572;&#39064;&#35780;&#20998;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.01811</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#31649;&#36947;&#22686;&#24378;&#22810;&#39046;&#22495;&#33258;&#21160;&#31616;&#31572;&#39064;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-Domain Automatic Short Answer Grading through an Explainable Neuro-Symbolic Pipeline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01811
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#24369;&#30417;&#30563;&#26631;&#27880;&#31243;&#24207;&#21644;&#22522;&#20110;&#21512;&#29702;&#21270;&#32447;&#32034;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#22810;&#39046;&#22495;&#33258;&#21160;&#31616;&#31572;&#39064;&#35780;&#20998;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#35299;&#37322;&#24615;&#30340;&#25512;&#29702;&#26469;&#33258;&#21160;&#35780;&#20998;&#31616;&#31572;&#39064;&#65292;&#24182;&#20351;&#35780;&#20998;&#20915;&#23450;&#32972;&#21518;&#30340;&#25512;&#29702;&#21487;&#35299;&#37322;&#26159;&#24403;&#21069;&#21464;&#21387;&#22120;&#27169;&#22411;&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;ASAG&#20013;&#65292;&#36890;&#36807;&#36923;&#36753;&#25512;&#29702;&#22120;&#25506;&#27979;&#21512;&#29702;&#21270;&#32447;&#32034;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#38656;&#35201;&#26631;&#27880;&#22312;&#23398;&#29983;&#22238;&#31572;&#20013;&#30340;&#21512;&#29702;&#21270;&#32447;&#32034;&#65292;&#36825;&#20165;&#22312;&#23569;&#25968;ASAG&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#19968;&#31181;&#29992;&#20110;ASAG&#25968;&#25454;&#38598;&#20013;&#21512;&#29702;&#21270;&#32447;&#32034;&#30340;&#24369;&#30417;&#30563;&#27880;&#37322;&#31243;&#24207;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#31181;&#22522;&#20110;&#21512;&#29702;&#21270;&#32447;&#32034;&#30340;&#21487;&#35299;&#37322;ASAG&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#12290;&#19982;Short Answer Feedback&#25968;&#25454;&#38598;&#30340;&#26368;&#26032;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21452;&#35821;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#38382;&#39064;&#35757;&#32451;&#35774;&#32622;&#20013;&#23558;&#22343;&#26041;&#26681;&#35823;&#24046;&#25552;&#39640;&#20102;0.24&#21040;0.3&#12290;&#36825;&#19968;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#29983;&#25104;&#39640;&#36136;&#37327;&#35780;&#20998;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01811v1 Announce Type: new  Abstract: Grading short answer questions automatically with interpretable reasoning behind the grading decision is a challenging goal for current transformer approaches. Justification cue detection, in combination with logical reasoners, has shown a promising direction for neuro-symbolic architectures in ASAG. But, one of the main challenges is the requirement of annotated justification cues in the students' responses, which only exist for a few ASAG datasets. To overcome this challenge, we contribute (1) a weakly supervised annotation procedure for justification cues in ASAG datasets, and (2) a neuro-symbolic model for explainable ASAG based on justification cues. Our approach improves upon the RMSE by 0.24 to 0.3 compared to the state-of-the-art on the Short Answer Feedback dataset in a bilingual, multi-domain, and multi-question training setup. This result shows that our approach provides a promising direction for generating high-quality grades
&lt;/p&gt;</description></item><item><title>CASIMIR&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#20316;&#32773;&#32508;&#21512;&#20462;&#35746;&#30340;&#31185;&#23398;&#25991;&#31456;&#35821;&#26009;&#24211;&#65292;&#20197;&#20419;&#36827;&#23545;&#31185;&#23398;&#25991;&#31456;&#20889;&#20316;&#20462;&#35746;&#27493;&#39588;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.00241</link><description>&lt;p&gt;
CASIMIR&#65306;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#20316;&#32773;&#32508;&#21512;&#20462;&#35746;&#30340;&#31185;&#23398;&#25991;&#31456;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CASIMIR: A Corpus of Scientific Articles enhanced with Multiple Author-Integrated Revisions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00241
&lt;/p&gt;
&lt;p&gt;
CASIMIR&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#20316;&#32773;&#32508;&#21512;&#20462;&#35746;&#30340;&#31185;&#23398;&#25991;&#31456;&#35821;&#26009;&#24211;&#65292;&#20197;&#20419;&#36827;&#23545;&#31185;&#23398;&#25991;&#31456;&#20889;&#20316;&#20462;&#35746;&#27493;&#39588;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#31185;&#23398;&#25991;&#31456;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#26159;&#19968;&#31181;&#39640;&#24230;&#35268;&#33539;&#21270;&#21644;&#20855;&#20307;&#30340;&#20307;&#35009;&#65292;&#22240;&#27492;&#31934;&#36890;&#20070;&#38754;&#20132;&#27969;&#23545;&#26377;&#25928;&#20256;&#36798;&#30740;&#31350;&#21457;&#29616;&#21644;&#35266;&#28857;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#31185;&#23398;&#25991;&#31456;&#20889;&#20316;&#36807;&#31243;&#20013;&#20462;&#35746;&#27493;&#39588;&#30340;&#21407;&#22987;&#25991;&#26412;&#36164;&#28304;&#12290;&#36825;&#20010;&#21517;&#20026;CASIMIR&#30340;&#26032;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;OpenReview&#30340;15,646&#31687;&#31185;&#23398;&#25991;&#31456;&#30340;&#22810;&#20010;&#20462;&#35746;&#29256;&#26412;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#21516;&#34892;&#35780;&#23457;&#12290;&#25991;&#31456;&#30340;&#36830;&#32493;&#29256;&#26412;&#23545;&#22312;&#21477;&#23376;&#32423;&#21035;&#23545;&#40784;&#65292;&#21516;&#26102;&#20445;&#30041;&#27573;&#33853;&#20301;&#32622;&#20449;&#24687;&#20316;&#20026;&#25903;&#25345;&#26410;&#26469;&#20462;&#35746;&#30740;&#31350;&#30340;&#20803;&#25968;&#25454;&#22312;&#35821;&#31687;&#32423;&#21035;&#12290;&#27599;&#19968;&#23545;&#20462;&#35746;&#21518;&#30340;&#21477;&#23376;&#37117;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#30340;&#32534;&#36753;&#21644;&#30456;&#20851;&#20462;&#35746;&#24847;&#22270;&#36827;&#34892;&#20102;&#20016;&#23500;&#12290;&#20026;&#20102;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#21021;&#22987;&#36136;&#37327;&#65292;&#25105;&#20204;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#20462;&#35746;&#26041;&#27861;&#36827;&#34892;&#20102;&#23450;&#24615;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00241v1 Announce Type: new  Abstract: Writing a scientific article is a challenging task as it is a highly codified and specific genre, consequently proficiency in written communication is essential for effectively conveying research findings and ideas. In this article, we propose an original textual resource on the revision step of the writing process of scientific articles. This new dataset, called CASIMIR, contains the multiple revised versions of 15,646 scientific articles from OpenReview, along with their peer reviews. Pairs of consecutive versions of an article are aligned at sentence-level while keeping paragraph location information as metadata for supporting future revision studies at the discourse level. Each pair of revised sentences is enriched with automatically extracted edits and associated revision intention. To assess the initial quality on the dataset, we conducted a qualitative study of several state-of-the-art text revision approaches and compared various
&lt;/p&gt;</description></item><item><title>MATHSENSEI &#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28155;&#21152;&#30693;&#35782;&#26816;&#32034;&#12289;&#31243;&#24207;&#25191;&#34892;&#21644;&#31526;&#21495;&#26041;&#31243;&#27714;&#35299;&#24037;&#20855;&#65292;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17231</link><description>&lt;p&gt;
MATHSENSEI&#65306;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17231
&lt;/p&gt;
&lt;p&gt;
MATHSENSEI &#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28155;&#21152;&#30693;&#35782;&#26816;&#32034;&#12289;&#31243;&#24207;&#25191;&#34892;&#21644;&#31526;&#21495;&#26041;&#31243;&#27714;&#35299;&#24037;&#20855;&#65292;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(TALM)&#24050;&#30693;&#33021;&#22815;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#25216;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MATHSENSEI&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#12290;&#36890;&#36807;&#28155;&#21152;&#29992;&#20110;&#30693;&#35782;&#26816;&#32034;&#65288;Bing Web Search&#65289;&#12289;&#31243;&#24207;&#25191;&#34892;&#65288;Python&#65289;&#21644;&#31526;&#21495;&#26041;&#31243;&#27714;&#35299;&#65288;Wolfram-Alpha&#65289;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#26469;&#30740;&#31350;&#36825;&#20123;&#24037;&#20855;&#30340;&#20114;&#34917;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17231v1 Announce Type: new  Abstract: Tool-augmented Large Language Models (TALM) are known to enhance the skillset of large language models (LLM), thereby, leading to their improved reasoning abilities across many tasks. While, TALMs have been successfully employed in different question-answering benchmarks, their efficacy on complex mathematical reasoning benchmarks, and the potential complimentary benefits offered by tools for knowledge retrieval and mathematical equation solving, are open research questions. In this work, we present MATHSENSEI, a tool-augmented large language model for mathematical reasoning. Augmented with tools for knowledge retrieval (Bing Web Search), program execution (Python), and symbolic equation solving (Wolfram-Alpha), we study the complimentary benefits of these tools through evaluations on mathematical reasoning datasets. We perform exhaustive ablations on MATH,a popular dataset for evaluating mathematical reasoning on diverse mathematical di
&lt;/p&gt;</description></item><item><title>KorNAT&#26159;&#39318;&#20010;&#29992;&#20110;&#35780;&#20272;&#38889;&#22269;&#22269;&#23478;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#31038;&#20250;&#20215;&#20540;&#35266;&#21644;&#24120;&#35782;&#23545;&#40784;&#20004;&#20010;&#26041;&#38754;&#65292;&#36890;&#36807;&#23545;&#31038;&#20250;&#20215;&#20540;&#21644;&#24120;&#35782;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.13605</link><description>&lt;p&gt;
KorNAT&#65306;&#38889;&#22269;&#31038;&#20250;&#20215;&#20540;&#35266;&#21644;&#24120;&#35782;&#30340;LLM&#23545;&#40784;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13605
&lt;/p&gt;
&lt;p&gt;
KorNAT&#26159;&#39318;&#20010;&#29992;&#20110;&#35780;&#20272;&#38889;&#22269;&#22269;&#23478;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#31038;&#20250;&#20215;&#20540;&#35266;&#21644;&#24120;&#35782;&#23545;&#40784;&#20004;&#20010;&#26041;&#38754;&#65292;&#36890;&#36807;&#23545;&#31038;&#20250;&#20215;&#20540;&#21644;&#24120;&#35782;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29305;&#23450;&#22269;&#23478;&#24471;&#20197;&#26377;&#25928;&#37096;&#32626;&#65292;&#23427;&#20204;&#24517;&#39035;&#20855;&#26377;&#23545;&#35813;&#22269;&#25991;&#21270;&#21644;&#22522;&#26412;&#30693;&#35782;&#30340;&#29702;&#35299;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22269;&#23478;&#23545;&#40784;&#65288;National Alignment&#65289;&#65292;&#20174;&#31038;&#20250;&#20215;&#20540;&#35266;&#23545;&#40784;&#21644;&#24120;&#35782;&#23545;&#40784;&#20004;&#20010;&#26041;&#38754;&#34913;&#37327;LLM&#19982;&#30446;&#26631;&#22269;&#23478;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#31038;&#20250;&#20215;&#20540;&#35266;&#23545;&#40784;&#35780;&#20272;&#27169;&#22411;&#23545;&#29305;&#23450;&#22269;&#23478;&#31038;&#20250;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#31243;&#24230;&#65292;&#32780;&#24120;&#35782;&#23545;&#40784;&#21017;&#26816;&#39564;&#27169;&#22411;&#23545;&#30456;&#20851;&#22522;&#26412;&#22269;&#23478;&#30693;&#35782;&#30340;&#25226;&#25569;&#24773;&#20917;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;KorNAT&#65292;&#36825;&#26159;&#39318;&#20010;&#34913;&#37327;&#19982;&#38889;&#22269;&#22269;&#23478;&#23545;&#40784;&#30340;&#22522;&#20934;&#12290;&#23545;&#20110;&#31038;&#20250;&#20215;&#20540;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;&#21253;&#25324;6174&#21517;&#38889;&#22269;&#21442;&#19982;&#32773;&#22312;&#20869;&#30340;&#22823;&#35268;&#27169;&#35843;&#26597;&#20013;&#33719;&#24471;&#20102;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#12290;&#23545;&#20110;&#24120;&#35782;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22522;&#20110;&#38889;&#22269;&#25945;&#31185;&#20070;&#21644;GED&#21442;&#32771;&#36164;&#26009;&#26500;&#24314;&#20102;&#26679;&#26412;&#12290;KorNAT&#21253;&#21547;4K&#21644;6K&#20010;&#38024;&#23545;&#31038;&#20250;&#20215;&#20540;&#21644;&#24120;&#35782;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13605v1 Announce Type: new  Abstract: For Large Language Models (LLMs) to be effectively deployed in a specific country, they must possess an understanding of the nation's culture and basic knowledge. To this end, we introduce National Alignment, which measures an alignment between an LLM and a targeted country from two aspects: social value alignment and common knowledge alignment. Social value alignment evaluates how well the model understands nation-specific social values, while common knowledge alignment examines how well the model captures basic knowledge related to the nation. We constructed KorNAT, the first benchmark that measures national alignment with South Korea. For the social value dataset, we obtained ground truth labels from a large-scale survey involving 6,174 unique Korean participants. For the common knowledge dataset, we constructed samples based on Korean textbooks and GED reference materials. KorNAT contains 4K and 6K multiple-choice questions for socia
&lt;/p&gt;</description></item><item><title>MatPlotAgent&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#33258;&#21160;&#21270;&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#26694;&#26550;&#65292;&#21253;&#25324;&#26597;&#35810;&#29702;&#35299;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#35270;&#35273;&#21453;&#39304;&#26426;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;MatPlotBench&#22522;&#20934;&#21644;GPT-4V&#35780;&#20998;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11453</link><description>&lt;p&gt;
MatPlotAgent: &#22522;&#20110;LLM&#30340;Agent&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#26041;&#27861;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11453
&lt;/p&gt;
&lt;p&gt;
MatPlotAgent&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#33258;&#21160;&#21270;&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#26694;&#26550;&#65292;&#21253;&#25324;&#26597;&#35810;&#29702;&#35299;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#35270;&#35273;&#21453;&#39304;&#26426;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;MatPlotBench&#22522;&#20934;&#21644;GPT-4V&#35780;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#36890;&#36807;&#30452;&#25509;&#23637;&#31034;&#22797;&#26434;&#20449;&#24687;&#24182;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35782;&#21035;&#38544;&#21547;&#27169;&#24335;&#65292;&#22312;&#30740;&#31350;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;Large Language Models&#65288;LLMs&#65289;&#36827;&#34892;&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#30340;&#30740;&#31350;&#20173;&#36739;&#20026;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MatPlotAgent&#65292;&#19968;&#31181;&#39640;&#25928;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;LLM&#20195;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#20219;&#21153;&#12290;MatPlotAgent&#21033;&#29992;&#20195;&#30721;LLMs&#21644;&#22810;&#27169;&#24577;LLMs&#30340;&#33021;&#21147;&#65292;&#30001;&#19977;&#20010;&#26680;&#24515;&#27169;&#22359;&#32452;&#25104;&#65306;&#26597;&#35810;&#29702;&#35299;&#12289;&#24102;&#26377;&#36845;&#20195;&#35843;&#35797;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20197;&#21450;&#29992;&#20110;&#38169;&#35823;&#26356;&#27491;&#30340;&#35270;&#35273;&#21453;&#39304;&#26426;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#35813;&#39046;&#22495;&#32570;&#20047;&#22522;&#20934;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MatPlotBench&#65292;&#19968;&#20010;&#30001;100&#20010;&#32463;&#20154;&#24037;&#39564;&#35777;&#30340;&#27979;&#35797;&#26696;&#20363;&#32452;&#25104;&#30340;&#39640;&#36136;&#37327;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;GPT-4V&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#30340;&#35780;&#20998;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#8230;&#65288;&#26410;&#23436;&#25972;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11453v1 Announce Type: new  Abstract: Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#19982;&#24847;&#22806;&#20943;&#23569;&#26368;&#23567;&#21270;&#21407;&#21017;&#22312;&#21517;&#35789;&#30701;&#35821;&#20013;&#30340;&#20914;&#31361;&#65292;&#32467;&#35770;&#26174;&#31034;&#24403;&#28041;&#21450;&#30340;&#21333;&#35789;&#36739;&#23569;&#19988;&#21333;&#35789;&#36739;&#30701;&#26102;&#65292;&#24847;&#22806;&#20943;&#23569;&#21487;&#33021;&#20250;&#36229;&#36234;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10311</link><description>&lt;p&gt;
&#21517;&#35789;&#30701;&#35821;&#20013;&#22836;&#37096;&#30340;&#26368;&#20339;&#20301;&#32622;&#12290;&#25351;&#31034;&#35821;&#12289;&#25968;&#35789;&#12289;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#30340;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#19982;&#24847;&#22806;&#20943;&#23569;&#26368;&#23567;&#21270;&#21407;&#21017;&#22312;&#21517;&#35789;&#30701;&#35821;&#20013;&#30340;&#20914;&#31361;&#65292;&#32467;&#35770;&#26174;&#31034;&#24403;&#28041;&#21450;&#30340;&#21333;&#35789;&#36739;&#23569;&#19988;&#21333;&#35789;&#36739;&#30701;&#26102;&#65292;&#24847;&#22806;&#20943;&#23569;&#21487;&#33021;&#20250;&#36229;&#36234;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#21477;&#35805;&#30340;&#35789;&#24207;&#30001;&#22810;&#31181;&#21407;&#21017;&#22609;&#36896;&#12290;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#21407;&#21017;&#19982;&#24847;&#22806;&#20943;&#23569;&#26368;&#23567;&#21270;&#21407;&#21017;&#65288;&#25110;&#21487;&#39044;&#27979;&#24615;&#26368;&#22823;&#21270;&#65289;&#22312;&#21333;&#19968;&#22836;&#37096;&#30340;&#21477;&#27861;&#20381;&#36182;&#32467;&#26500;&#20013;&#23384;&#22312;&#20914;&#31361;&#65306;&#21069;&#32773;&#39044;&#27979;&#22836;&#37096;&#24212;&#35813;&#25918;&#32622;&#22312;&#32447;&#24615;&#25490;&#21015;&#30340;&#20013;&#24515;&#65292;&#21518;&#32773;&#39044;&#27979;&#22836;&#37096;&#24212;&#35813;&#25918;&#32622;&#22312;&#20004;&#31471;&#20043;&#19968;&#65288;&#35201;&#20040;&#22312;&#39318;&#20301;&#65292;&#35201;&#20040;&#22312;&#26411;&#20301;&#65289;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#20309;&#26102;&#24847;&#22806;&#20943;&#23569;&#65288;&#25110;&#21487;&#39044;&#27979;&#24615;&#26368;&#22823;&#21270;&#65289;&#24212;&#35813;&#36229;&#36234;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#12290;&#22312;&#21333;&#19968;&#22836;&#37096;&#32467;&#26500;&#30340;&#32972;&#26223;&#19979;&#65292;&#39044;&#27979;&#22312;&#28385;&#36275;&#20004;&#20010;&#26465;&#20214;&#26102;&#26356;&#26377;&#21487;&#33021;&#21457;&#29983;&#65292;&#21363;&#65288;a&#65289;&#28041;&#21450;&#30340;&#21333;&#35789;&#36739;&#23569;&#65292;&#24182;&#19988;&#65288;b&#65289;&#21333;&#35789;&#36739;&#30701;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#30001;&#25351;&#31034;&#35821;&#12289;&#25968;&#35789;&#12289;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#32452;&#25104;&#30340;&#21517;&#35789;&#30701;&#35821;&#19978;&#27979;&#35797;&#20102;&#36825;&#19968;&#39044;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#39318;&#36873;&#39034;&#24207;&#20013;...&#65288;&#32570;&#22833;&#37096;&#20998;&#26080;&#27861;&#25552;&#20379;&#23436;&#25972;&#32763;&#35793;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10311v1 Announce Type: new  Abstract: The word order of a sentence is shaped by multiple principles. The principle of syntactic dependency distance minimization is in conflict with the principle of surprisal minimization (or predictability maximization) in single head syntactic dependency structures: while the former predicts that the head should be placed at the center of the linear arrangement, the latter predicts that the head should be placed at one of the ends (either first or last). A critical question is when surprisal minimization (or predictability maximization) should surpass syntactic dependency distance minimization. In the context of single head structures, it has been predicted that this is more likely to happen when two conditions are met, i.e. (a) fewer words are involved and (b) words are shorter. Here we test the prediction on the noun phrase when its composed of a demonstrative, a numeral, an adjective and a noun. We find that, across preferred orders in l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#26469;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;&#12290;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#65292;&#24182;&#32473;&#20986;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#65292;&#29305;&#21035;&#38024;&#23545;&#35786;&#26029;&#24310;&#36831;&#21644;&#26469;&#33258;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10109</link><description>&lt;p&gt;
&#29992;&#21487;&#35299;&#37322;&#30340;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Towards Reducing Diagnostic Errors with Interpretable Risk Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#26469;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;&#12290;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#65292;&#24182;&#32473;&#20986;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#65292;&#29305;&#21035;&#38024;&#23545;&#35786;&#26029;&#24310;&#36831;&#21644;&#26469;&#33258;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35786;&#26029;&#38169;&#35823;&#21457;&#29983;&#26159;&#22240;&#20026;&#20020;&#24202;&#21307;&#29983;&#26080;&#27861;&#36731;&#26131;&#33719;&#21462;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#26469;&#36827;&#34892;&#24102;&#26377;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#30340;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#30340;&#39044;&#27979;&#65292;&#22312;&#20020;&#24202;&#21307;&#29983;&#20173;&#28982;&#19981;&#30830;&#23450;&#30340;&#26102;&#38388;&#28857;&#19978;&#65292;&#26088;&#22312;&#29305;&#21035;&#20943;&#36731;&#35786;&#26029;&#24310;&#36831;&#21644;&#28304;&#20110;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#65292;&#38656;&#35201;&#25512;&#26029;&#20986;&#20107;&#20214;&#24615;&#30340;&#8220;&#30495;&#23454;&#8221;&#35786;&#26029;&#30340;&#26102;&#38388;&#31890;&#24230;&#32454;&#33268;&#30340;&#22238;&#39038;&#24615;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#20445;&#35777;&#36755;&#20837;&#25991;&#26412;&#26159;&#22312;&#21487;&#20197;&#36827;&#34892;&#33258;&#20449;&#30340;&#35786;&#26029;&#20043;&#21069;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#26816;&#32034;&#21021;&#22987;&#30340;&#35777;&#25454;&#27744;&#65292;&#28982;&#21518;&#36827;&#34892;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10109v1 Announce Type: new  Abstract: Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual "true" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.09283</link><description>&lt;p&gt;
&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#24212;&#29992;&#20013;&#24050;&#32463;&#24456;&#24120;&#35265;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#22238;&#22797;&#30340;&#39118;&#38505;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#31038;&#20250;&#20851;&#20999;&#65292;&#24182;&#28608;&#21457;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#27492;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26368;&#36817;&#30740;&#31350;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#25688;&#35201;&#65292;&#22686;&#36827;&#23545;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#37325;&#35201;&#35838;&#39064;&#12290;&#20026;&#20102;&#26041;&#20415;&#21442;&#32771;&#65292;&#25105;&#20204;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#23545;&#25152;&#26377;&#22312;&#27492;&#35843;&#26597;&#20013;&#25552;&#21040;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/niconi19/LLM-conversation-safety&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 Announce Type: new Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.
&lt;/p&gt;</description></item><item><title>&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;R2E&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#30830;&#23450;&#35777;&#25454;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#20219;&#21153;&#20013;&#65292;R2E&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04068</link><description>&lt;p&gt;
&#26816;&#32034;&#20197;&#35299;&#37322;&#65306;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;&#39537;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Retrieve to Explain: Evidence-driven Predictions with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04068
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;R2E&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#30830;&#23450;&#35777;&#25454;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#20219;&#21153;&#20013;&#65292;R2E&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#35821;&#35328;&#27169;&#22411;&#65292;&#24448;&#24448;&#38590;&#20197;&#28145;&#20837;&#20998;&#26512;&#12290;&#40657;&#30418;&#27169;&#22411;&#21487;&#33021;&#25513;&#30422;&#20102;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#38382;&#39064;&#21644;&#26377;&#23475;&#20559;&#24046;&#12290;&#23545;&#20110;&#20154;&#26426;&#21327;&#20316;&#36807;&#31243;&#26469;&#35828;&#65292;&#19981;&#36879;&#26126;&#30340;&#39044;&#27979;&#21487;&#33021;&#23548;&#33268;&#32570;&#20047;&#20449;&#20219;&#65292;&#38480;&#21046;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21363;&#20351;&#27169;&#22411;&#30340;&#24615;&#33021;&#24456;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;Retrieve to Explain&#65292;&#31616;&#31216;R2E&#65289;&#12290;R2E&#26159;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26681;&#25454;&#25991;&#26723;&#35821;&#26009;&#24211;&#20013;&#30340;&#35777;&#25454;&#65292;&#20351;&#29992;Shapley&#20540;&#26469;&#30830;&#23450;&#35777;&#25454;&#23545;&#26368;&#32456;&#39044;&#27979;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#27169;&#26495;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#12290;R2E&#33021;&#22815;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#30340;&#35777;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#27169;&#26495;&#21270;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#21040;&#33258;&#28982;&#35821;&#35328;&#20013;&#12290;&#25105;&#20204;&#22312;&#36890;&#36807;&#20998;&#26512;&#24050;&#21457;&#34920;&#30340;&#31185;&#23398;&#25991;&#29486;&#36827;&#34892;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#30340;&#23454;&#38469;&#26696;&#20363;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#34892;&#19994;&#26631;&#20934;&#30340;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRESS&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#22686;&#24378;&#27169;&#22411;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#23545;&#40784;&#21644;&#20114;&#21160;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#23545;&#40784;&#21644;&#22810;&#36718;&#23545;&#35805;&#20114;&#21160;&#26041;&#38754;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;</title><link>https://arxiv.org/abs/2311.10081</link><description>&lt;p&gt;
DRESS&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25351;&#23548;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#23545;&#40784;&#21644;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10081
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRESS&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#22686;&#24378;&#27169;&#22411;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#23545;&#40784;&#21644;&#20114;&#21160;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#23545;&#40784;&#21644;&#22810;&#36718;&#23545;&#35805;&#20114;&#21160;&#26041;&#38754;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DRESS&#65292;&#19968;&#31181;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#65292;&#23427;&#21019;&#26032;&#24615;&#22320;&#21033;&#29992;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65288;NLF&#65289;&#65292;&#36890;&#36807;&#35299;&#20915;&#24403;&#21069;LVLM&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#26469;&#22686;&#24378;&#20854;&#23545;&#40784;&#21644;&#20114;&#21160;&#12290;&#39318;&#20808;&#65292;&#20808;&#21069;&#30340;LVLM&#36890;&#24120;&#20165;&#20381;&#36182;&#25351;&#23548;&#24494;&#35843;&#38454;&#27573;&#26469;&#22686;&#24378;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#12290;&#22914;&#26524;&#19981;&#21152;&#20837;&#39069;&#22806;&#21453;&#39304;&#65292;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#29983;&#25104;&#26080;&#29992;&#12289;&#34394;&#26500;&#25110;&#26377;&#23475;&#30340;&#21709;&#24212;&#12290;&#20854;&#27425;&#65292;&#34429;&#28982;&#35270;&#35273;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#36890;&#24120;&#20197;&#22810;&#36718;&#23545;&#35805;&#26684;&#24335;&#32467;&#26500;&#21270;&#65292;&#20294;&#36830;&#32493;&#23545;&#35805;&#36718;&#20043;&#38388;&#30340;&#36830;&#25509;&#21644;&#20381;&#36182;&#20851;&#31995;&#36739;&#24369;&#12290;&#36825;&#38477;&#20302;&#20102;&#26377;&#25928;&#22810;&#36718;&#20114;&#21160;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;NLF&#20998;&#20026;&#20004;&#31181;&#20851;&#38190;&#31867;&#22411;&#30340;&#26032;&#39062;&#20998;&#31867;&#65306;&#25209;&#35780;&#21644;&#25913;&#36827;&#12290;&#25209;&#35780;&#24615;NLF&#35782;&#21035;&#21709;&#24212;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#29992;&#20110;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10081v2 Announce Type: replace-cross  Abstract: We present DRESS, a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multi-turn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This reduces the capacity for effective multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and refinement. The critique NLF identifies the strengths and weaknesses of the responses and is used to align 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25552;&#31034;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;GPT4 APO&#22312;&#26631;&#20934;&#21270;&#25552;&#21319;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#19987;&#23478;&#23450;&#21046;&#21270;&#23545;&#20110;&#20869;&#23481;&#36136;&#37327;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2311.09684</link><description>&lt;p&gt;
&#21307;&#29983;&#20204;&#30693;&#36947;&#22914;&#20309;&#25552;&#37266;&#21527;&#65311;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#24110;&#21161;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25552;&#31034;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;GPT4 APO&#22312;&#26631;&#20934;&#21270;&#25552;&#21319;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#19987;&#23478;&#23450;&#21046;&#21270;&#23545;&#20110;&#20869;&#23481;&#36136;&#37327;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#25552;&#31034;&#24037;&#31243;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#26469;&#25913;&#36827;&#21021;&#22987;&#25552;&#31034;&#65292;&#24182;&#27604;&#36739;&#20102;&#21307;&#23398;&#19987;&#23478;&#12289;&#38750;&#21307;&#23398;&#19987;&#23478;&#20197;&#21450;&#32463;&#36807;APO&#22686;&#24378;&#30340;GPT3.5&#21644;GPT4&#30340;&#36755;&#20986;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;GPT4 APO&#22312;&#26631;&#20934;&#21270;&#20020;&#24202;&#31508;&#35760;&#21508;&#33410;&#25552;&#31034;&#36136;&#37327;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#20154;&#22312;&#29615;&#20013;&#26041;&#27861;&#26174;&#31034;&#65292;&#19987;&#23478;&#22312;APO&#21518;&#20445;&#25345;&#20869;&#23481;&#36136;&#37327;&#65292;&#20294;&#26356;&#20559;&#22909;&#33258;&#24049;&#30340;&#20462;&#25913;&#65292;&#34920;&#26126;&#20102;&#19987;&#23478;&#23450;&#21046;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#24314;&#35758;&#37319;&#29992;&#20004;&#38454;&#27573;&#20248;&#21270;&#36807;&#31243;&#65292;&#21033;&#29992;APO-GPT4&#30830;&#20445;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;&#19987;&#23478;&#36755;&#20837;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09684v2 Announce Type: replace-cross  Abstract: This study examines the effect of prompt engineering on the performance of Large Language Models (LLMs) in clinical note generation. We introduce an Automatic Prompt Optimization (APO) framework to refine initial prompts and compare the outputs of medical experts, non-medical experts, and APO-enhanced GPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in standardizing prompt quality across clinical note sections. A human-in-the-loop approach shows that experts maintain content quality post-APO, with a preference for their own modifications, suggesting the value of expert customization. We recommend a two-phase optimization process, leveraging APO-GPT4 for consistency and expert input for personalization.
&lt;/p&gt;</description></item><item><title>LifeTox &#25968;&#25454;&#38598;&#38024;&#23545;&#21508;&#31181;&#27714;&#21161;&#22330;&#26223;&#20013;&#30340;&#38544;&#24615;&#27602;&#24615;&#35774;&#35745;&#65292;RoBERTa &#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#19981;&#20165;&#21305;&#25932;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#27425;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#65292;&#24378;&#35843;&#20102; LifeTox &#22312;&#35299;&#20915;&#38544;&#24615;&#27602;&#24615;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.09585</link><description>&lt;p&gt;
LifeTox&#65306;&#25581;&#31034;&#29983;&#27963;&#24314;&#35758;&#20013;&#30340;&#38544;&#24615;&#27602;&#24615;
&lt;/p&gt;
&lt;p&gt;
LifeTox: Unveiling Implicit Toxicity in Life Advice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09585
&lt;/p&gt;
&lt;p&gt;
LifeTox &#25968;&#25454;&#38598;&#38024;&#23545;&#21508;&#31181;&#27714;&#21161;&#22330;&#26223;&#20013;&#30340;&#38544;&#24615;&#27602;&#24615;&#35774;&#35745;&#65292;RoBERTa &#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#19981;&#20165;&#21305;&#25932;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#27425;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#65292;&#24378;&#35843;&#20102; LifeTox &#22312;&#35299;&#20915;&#38544;&#24615;&#27602;&#24615;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#26816;&#27979;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#38544;&#24615;&#27602;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LifeTox&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35782;&#21035;&#21508;&#31181;&#27714;&#21161;&#22330;&#26223;&#20013;&#30340;&#38544;&#24615;&#27602;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;LifeTox&#21253;&#21547;&#20102;&#36890;&#36807;&#24320;&#25918;&#24615;&#38382;&#39064;&#20174;&#20010;&#20154;&#32463;&#39564;&#20013;&#34893;&#29983;&#20986;&#30340;&#22810;&#26679;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;LifeTox&#19978;&#24494;&#35843;&#30340;RoBERTa&#22312;&#27602;&#24615;&#20998;&#31867;&#20219;&#21153;&#20013;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#27425;&#24615;&#33021;&#30456;&#21305;&#25932;&#29978;&#33267;&#36229;&#36234;&#12290;&#36825;&#20123;&#32467;&#26524;&#20984;&#26174;&#20102;LifeTox&#22312;&#35299;&#20915;&#38544;&#24615;&#27602;&#24615;&#22266;&#26377;&#30340;&#22797;&#26434;&#25361;&#25112;&#20013;&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#25968;&#25454;&#38598;&#21644;LifeTox&#30340;&#30417;&#30563;&#23478;&#26063;&#65307;350M&#12289;7B&#21644;13B&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09585v2 Announce Type: replace  Abstract: As large language models become increasingly integrated into daily life, detecting implicit toxicity across diverse contexts is crucial. To this end, we introduce LifeTox, a dataset designed for identifying implicit toxicity within a broad range of advice-seeking scenarios. Unlike existing safety datasets, LifeTox comprises diverse contexts derived from personal experiences through open-ended questions. Experiments demonstrate that RoBERTa fine-tuned on LifeTox matches or surpasses the zero-shot performance of large language models in toxicity classification tasks. These results underscore the efficacy of LifeTox in addressing the complex challenges inherent in implicit toxicity. We open-sourced the dataset\footnote{\url{https://huggingface.co/datasets/mbkim/LifeTox}} and the LifeTox moderator family; 350M, 7B, and 13B.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#25552;&#20986;&#20351;&#29992;LLM&#20316;&#20026;&#33258;&#21160;&#21518;&#32534;&#36753;&#22120;(APE)&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#25193;&#23637;&#21040;&#25991;&#26723;&#32423;&#32763;&#35793;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.14855</link><description>&lt;p&gt;
&#32763;&#35793;&#30340;&#19978;&#19979;&#25991;&#32454;&#21270;&#65306;&#21477;&#23376;&#21644;&#25991;&#26723;&#32423;&#21518;&#32534;&#36753;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14855
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#25552;&#20986;&#20351;&#29992;LLM&#20316;&#20026;&#33258;&#21160;&#21518;&#32534;&#36753;&#22120;(APE)&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#25193;&#23637;&#21040;&#25991;&#26723;&#32423;&#32763;&#35793;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35201;&#27714;&#24191;&#27867;&#29702;&#35299;&#21644;&#19978;&#19979;&#25991;&#22788;&#29702;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#30340;&#26174;&#33879;&#24615;&#33021;&#26174;&#31034;&#20102;&#23427;&#20204;&#22312;&#32763;&#35793;&#20013;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20123;&#33021;&#21147;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#36827;&#34892;MT&#65292;&#24182;&#25506;&#32034;&#20102;&#26368;&#36817;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#21457;&#29616;&#65292;&#21363;&#20351;&#20026;&#32763;&#35793;&#30446;&#30340;&#24494;&#35843;&#65292;&#20063;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65306;&#23558;LLM&#20316;&#20026;&#33258;&#21160;&#21518;&#32534;&#36753;&#22120;(APE)&#32780;&#19981;&#26159;&#30452;&#25509;&#36716;&#25442;&#22120;&#12290;&#22522;&#20110;LLM&#22788;&#29702;&#21644;&#29983;&#25104;&#38271;&#24207;&#21015;&#30340;&#24322;&#24120;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#25991;&#26723;&#32423;&#32763;&#35793;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#36827;&#34892; ...
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14855v2 Announce Type: replace-cross  Abstract: Large Language Models (LLM's) have demonstrated considerable success in various Natural Language Processing tasks, but they have yet to attain state-of-the-art performance in Neural Machine Translation (NMT). Nevertheless, their significant performance in tasks demanding a broad understanding and contextual processing shows their potential for translation. To exploit these abilities, we investigate using LLM's for MT and explore recent parameter-efficient fine-tuning techniques. Surprisingly, our initial experiments find that fine-tuning for translation purposes even led to performance degradation. To overcome this, we propose an alternative approach: adapting LLM's as Automatic Post-Editors (APE) rather than direct translators. Building on the LLM's exceptional ability to process and generate lengthy sequences, we also propose extending our approach to document-level translation. We show that leveraging Low-Rank-Adapter fine-t
&lt;/p&gt;</description></item><item><title>&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20551;&#35774;&#25552;&#20986;&#26041;&#38754;&#34920;&#29616;&#24778;&#20154;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#19968;&#20010;&#65288;&#20219;&#21153;&#29305;&#23450;&#30340;&#65289;&#31526;&#21495;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#31995;&#32479;&#22320;&#36807;&#28388;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.08559</link><description>&lt;p&gt;
&#20196;&#20154;&#24778;&#21497;&#20294;&#20196;&#20154;&#22256;&#24785;&#65306;&#29992;&#20551;&#35774;&#32454;&#21270;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#30340;&#24402;&#32435;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08559
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20551;&#35774;&#25552;&#20986;&#26041;&#38754;&#34920;&#29616;&#24778;&#20154;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#19968;&#20010;&#65288;&#20219;&#21153;&#29305;&#23450;&#30340;&#65289;&#31526;&#21495;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#31995;&#32479;&#22320;&#36807;&#28388;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23569;&#25968;&#35266;&#23519;&#20013;&#25512;&#23548;&#20986;&#28508;&#22312;&#21407;&#21017;&#65292;&#28982;&#21518;&#25512;&#24191;&#21040;&#26032;&#24773;&#20917;&#30340;&#33021;&#21147;&#65292;&#21363;&#24402;&#32435;&#25512;&#29702;&#65292;&#23545;&#20110;&#20154;&#31867;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#22312;&#30740;&#31350;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#24402;&#32435;&#25512;&#29702;&#26041;&#38754;&#24120;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36845;&#20195;&#20551;&#35774;&#32454;&#21270;&#36825;&#19968;&#25216;&#26415;&#23545;LMs&#30340;&#24402;&#32435;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#35813;&#25216;&#26415;&#26356;&#25509;&#36817;&#20154;&#31867;&#24402;&#32435;&#36807;&#31243;&#65292;&#32780;&#19981;&#26159;&#26631;&#20934;&#30340;&#36755;&#20837;-&#36755;&#20986;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08559v3 Announce Type: replace-cross  Abstract: The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38169;&#35823;&#33539;&#25968;&#25130;&#26029;&#65288;ENT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#38750;&#30446;&#26631;&#26631;&#35760;&#30340;&#20998;&#24067;&#20174;&#32780;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#25968;&#25454;&#25130;&#26029;&#65292;&#35777;&#23454;&#22312;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20013;&#24212;&#29992;ENT&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2310.00840</link><description>&lt;p&gt;
&#38169;&#35823;&#33539;&#25968;&#25130;&#26029;&#65306;&#38024;&#23545;&#25968;&#25454;&#22122;&#38899;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#30340;&#40065;&#26834;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Error Norm Truncation: Robust Training in the Presence of Data Noise for Text Generation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00840
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38169;&#35823;&#33539;&#25968;&#25130;&#26029;&#65288;ENT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#38750;&#30446;&#26631;&#26631;&#35760;&#30340;&#20998;&#24067;&#20174;&#32780;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#25968;&#25454;&#25130;&#26029;&#65292;&#35777;&#23454;&#22312;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20013;&#24212;&#29992;ENT&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#23384;&#22312;&#38169;&#35823;&#26102;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#12290;&#38543;&#30528;&#28023;&#37327;&#32593;&#32476;&#25235;&#21462;&#25968;&#25454;&#30340;&#26222;&#36941;&#21487;&#29992;&#65292;&#25105;&#20204;&#22914;&#20309;&#22686;&#24378;&#22312;&#22823;&#37327;&#22024;&#26434;&#30340;&#32593;&#32476;&#25235;&#21462;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21602;&#65311;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38169;&#35823;&#33539;&#25968;&#25130;&#26029;&#65288;ENT&#65289;&#30340;&#40065;&#26834;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#25130;&#26029;&#22024;&#26434;&#30340;&#25968;&#25454;&#12290;&#19982;&#20165;&#20351;&#29992;&#36127;&#23545;&#25968;&#20284;&#28982;&#25439;&#22833;&#26469;&#20272;&#35745;&#25968;&#25454;&#36136;&#37327;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#38750;&#30446;&#26631;&#26631;&#35760;&#30340;&#20998;&#24067;&#65292;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#20272;&#35745;&#65292;&#36825;&#22312;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#30053;&#12290;&#36890;&#36807;&#23545;&#35821;&#35328;&#24314;&#27169;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#25991;&#26412;&#25688;&#35201;&#30340;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#37197;&#22791;&#38169;&#35823;&#33539;&#25968;&#25130;&#26029;&#33021;&#22815;&#25913;&#21892;&#29983;&#25104;&#36136;&#37327;&#65292;&#20248;&#20110;&#26631;&#20934;&#35757;&#32451;&#21644;&#20808;&#21069;&#36719;&#25130;&#26029;&#21644;&#30828;&#25130;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00840v2 Announce Type: replace  Abstract: Text generation models are notoriously vulnerable to errors in the training data. With the wide-spread availability of massive amounts of web-crawled data becoming more commonplace, how can we enhance the robustness of models trained on a massive amount of noisy web-crawled text? In our work, we propose Error Norm Truncation (ENT), a robust enhancement method to the standard training objective that truncates noisy data. Compared to methods that only uses the negative log-likelihood loss to estimate data quality, our method provides a more accurate estimation by considering the distribution of non-target tokens, which is often overlooked by previous work. Through comprehensive experiments across language modeling, machine translation, and text summarization, we show that equipping text generation models with ENT improves generation quality over standard training and previous soft and hard truncation methods. Furthermore, we show that 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;BAMBOO&#22522;&#20934;&#26469;&#20840;&#38754;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#38271;&#25991;&#26412;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#21253;&#21547;10&#20010;&#25968;&#25454;&#38598;&#20174;5&#20010;&#19981;&#21516;&#38271;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#20013;&#25552;&#21462;&#65292;&#28085;&#30422;&#20102;LLMs&#30340;&#26680;&#24515;&#33021;&#21147;&#21644;&#21508;&#20010;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2309.13345</link><description>&lt;p&gt;
BAMBOO&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13345
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;BAMBOO&#22522;&#20934;&#26469;&#20840;&#38754;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#38271;&#25991;&#26412;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#21253;&#21547;10&#20010;&#25968;&#25454;&#38598;&#20174;5&#20010;&#19981;&#21516;&#38271;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#20013;&#25552;&#21462;&#65292;&#28085;&#30422;&#20102;LLMs&#30340;&#26680;&#24515;&#33021;&#21147;&#21644;&#21508;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#22788;&#29702;&#26222;&#36890;&#38271;&#24230;&#30340;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#29087;&#32451;&#24230;&#12290;&#26368;&#36817;&#65292;&#22810;&#39033;&#30740;&#31350;&#33268;&#21147;&#20110;&#25193;&#23637;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#24182;&#22686;&#24378;LLMs&#30340;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BAMBOO&#65292;&#19968;&#20010;&#22810;&#20219;&#21153;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#12290;BAMBOO&#35774;&#35745;&#20043;&#21021;&#32771;&#34385;&#20102;&#22235;&#20010;&#21407;&#21017;&#65306;&#20840;&#38754;&#23481;&#37327;&#35780;&#20272;&#12289;&#36991;&#20813;&#25968;&#25454;&#27745;&#26579;&#12289;&#20934;&#30830;&#30340;&#33258;&#21160;&#35780;&#20272;&#20197;&#21450;&#19981;&#21516;&#38271;&#24230;&#32423;&#21035;&#12290;&#23427;&#30001;&#26469;&#33258;5&#20010;&#19981;&#21516;&#38271;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#30340;10&#20010;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#21363;&#38382;&#31572;&#12289;&#24187;&#35273;&#26816;&#27979;&#12289;&#25991;&#26412;&#25490;&#24207;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#20195;&#30721;&#34917;&#20840;&#65292;&#20197;&#28085;&#30422;LLMs&#30340;&#26680;&#24515;&#33021;&#21147;&#21644;&#21508;&#20010;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;BAMBOO&#19978;&#20351;&#29992;&#20116;&#20010;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#38271;&#25991;&#26412;&#30340;&#22235;&#20010;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;&#24403;&#21069;&#30340;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#36827;&#34892;&#20102;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13345v2 Announce Type: replace  Abstract: Large language models (LLMs) have achieved dramatic proficiency over NLP tasks with normal length. Recently, multiple studies have committed to extending the context length and enhancing the long text modeling capabilities of LLMs. To comprehensively evaluate the long context ability of LLMs, we propose BAMBOO, a multi-task long context benchmark. BAMBOO has been designed with four principles: comprehensive capacity evaluation, avoidance of data contamination, accurate automatic evaluation, and different length levels. It consists of 10 datasets from 5 different long text understanding tasks, i.e. question answering, hallucination detection, text sorting, language modeling, and code completion, to cover core capacities and various domains of LLMs. We conduct experiments with five long context models on BAMBOO and further discuss four key research questions of long text. We also qualitatively analyze current long context models and po
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35777;&#26126;&#26159;&#24378;&#22823;&#30340;&#21387;&#32553;&#22120;&#65292;&#21387;&#32553;&#35270;&#35282;&#20026;&#25193;&#23637;&#23450;&#24459;&#12289;&#26631;&#35760;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2309.10668</link><description>&lt;p&gt;
&#35821;&#35328;&#24314;&#27169;&#21363;&#20026;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Language Modeling Is Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.10668
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35777;&#26126;&#26159;&#24378;&#22823;&#30340;&#21387;&#32553;&#22120;&#65292;&#21387;&#32553;&#35270;&#35282;&#20026;&#25193;&#23637;&#23450;&#24459;&#12289;&#26631;&#35760;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#24050;&#30830;&#31435;&#20102;&#39044;&#27979;&#27169;&#22411;&#21487;&#20197;&#36716;&#21270;&#20026;&#26080;&#25439;&#21387;&#32553;&#22120;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#38598;&#20013;&#31934;&#21147;&#35757;&#32451;&#36234;&#26469;&#36234;&#22823;&#12289;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#65288;&#35821;&#35328;&#65289;&#27169;&#22411;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#22240;&#27492;&#23427;&#20204;&#26377;&#26395;&#25104;&#20026;&#24378;&#22823;&#30340;&#21387;&#32553;&#22120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20027;&#24352;&#36890;&#36807;&#21387;&#32553;&#30340;&#35270;&#35282;&#26469;&#30475;&#24453;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#22823;&#22411;&#65288;&#22522;&#30784;&#65289;&#27169;&#22411;&#30340;&#21387;&#32553;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#36890;&#29992;&#39044;&#27979;&#22120;&#65292;&#21387;&#32553;&#35270;&#35282;&#25552;&#20379;&#20102;&#26377;&#20851;&#25193;&#23637;&#23450;&#24459;&#12289;&#26631;&#35760;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#35265;&#35299;&#12290;&#20363;&#22914;&#65292;Chinchilla 70B&#65292;&#34429;&#28982;&#20027;&#35201;&#22312;&#25991;&#26412;&#19978;&#35757;&#32451;&#65292;&#20294;&#21487;&#20197;&#23558;ImageNet&#30340;&#34917;&#19969;&#21387;&#32553;&#20026;&#20854;&#21407;&#22987;&#22823;&#23567;&#30340;43.4%&#65292;&#23558;LibriSpeech&#26679;&#26412;&#21387;&#32553;&#20026;&#20854;&#21407;&#22987;&#22823;&#23567;&#30340;16.4%&#65292;&#36229;&#36234;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#21387;&#32553;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.10668v2 Announce Type: replace-cross  Abstract: It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors li
&lt;/p&gt;</description></item><item><title>EasyEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#25903;&#25345;&#21508;&#31181;&#26368;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#30693;&#21517;&#30340;LLMs&#12290;</title><link>https://arxiv.org/abs/2308.07269</link><description>&lt;p&gt;
EasyEdit&#65306;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07269
&lt;/p&gt;
&lt;p&gt;
EasyEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#25903;&#25345;&#21508;&#31181;&#26368;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#30693;&#21517;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#36973;&#21463;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#23545;&#26410;&#35265;&#20107;&#20214;&#19981;&#30693;&#24773;&#25110;&#29983;&#25104;&#20855;&#26377;&#19981;&#27491;&#30830;&#20107;&#23454;&#30340;&#25991;&#26412;&#65292;&#21407;&#22240;&#26159;&#25968;&#25454;&#36807;&#26102;/&#22024;&#26434;&#12290;&#20026;&#27492;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#38024;&#23545;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#26088;&#22312;&#24494;&#22937;&#22320;&#27880;&#20837;/&#32534;&#36753;&#26356;&#26032;&#30340;&#30693;&#35782;&#25110;&#35843;&#25972;&#19981;&#33391;&#34892;&#20026;&#65292;&#21516;&#26102;&#23558;&#23545;&#19981;&#30456;&#20851;&#36755;&#20837;&#30340;&#24433;&#21709;&#26368;&#23567;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20197;&#21450;&#20219;&#21153;&#35774;&#32622;&#20013;&#30340;&#21464;&#21270;&#65292;&#31038;&#21306;&#20013;&#27809;&#26377;&#21487;&#29992;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;&#26631;&#20934;&#23454;&#26045;&#26694;&#26550;&#65292;&#36825;&#22952;&#30861;&#20102;&#20174;&#19994;&#32773;&#23558;&#30693;&#35782;&#32534;&#36753;&#24212;&#29992;&#20110;&#24212;&#29992;&#31243;&#24207;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyEdit&#65292;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;LLMs&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#12290;&#23427;&#25903;&#25345;&#21508;&#31181;&#23574;&#31471;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#35768;&#22810;&#33879;&#21517;&#30340;LLMs&#65292;&#22914;T5&#12289;GPT-J&#12289;LlaMA&#31561;&#12290;&#20174;&#32463;&#39564;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;kno
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07269v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the kno
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21019;&#24314;HateModerate&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#33258;&#21160;&#20869;&#23481;&#23457;&#26680;&#21592;&#23545;&#20869;&#23481;&#25919;&#31574;&#30340;&#31526;&#21512;&#24230;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#22120;&#22312;&#27492;&#26041;&#38754;&#23384;&#22312;&#30340;&#37325;&#22823;&#22833;&#36133;&#12290;</title><link>https://arxiv.org/abs/2307.12418</link><description>&lt;p&gt;
HateModerate&#65306;&#38024;&#23545;&#20869;&#23481;&#23457;&#26597;&#25919;&#31574;&#27979;&#35797;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
HateModerate: Testing Hate Speech Detectors against Content Moderation Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.12418
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21019;&#24314;HateModerate&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#33258;&#21160;&#20869;&#23481;&#23457;&#26680;&#21592;&#23545;&#20869;&#23481;&#25919;&#31574;&#30340;&#31526;&#21512;&#24230;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#22120;&#22312;&#27492;&#26041;&#38754;&#23384;&#22312;&#30340;&#37325;&#22823;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#25252;&#29992;&#25143;&#20813;&#21463;&#22823;&#37327;&#20167;&#24680;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#29616;&#26377;&#30740;&#31350;&#20851;&#27880;&#33258;&#21160;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#21162;&#21147;&#65292;&#20294;&#20173;&#26377;&#19968;&#20010;&#38382;&#39064;&#65306;&#33258;&#21160;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#22120;&#26159;&#21542;&#31526;&#21512;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#25919;&#31574;&#65311;&#24179;&#21488;&#30340;&#20869;&#23481;&#25919;&#31574;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#23457;&#26597;&#30340;&#20869;&#23481;&#28165;&#21333;&#12290;&#30001;&#20110;&#20869;&#23481;&#23457;&#26597;&#35268;&#21017;&#36890;&#24120;&#26159;&#29420;&#29305;&#23450;&#20041;&#30340;&#65292;&#29616;&#26377;&#30340;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#19981;&#33021;&#30452;&#25509;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#39033;&#24037;&#20316;&#35797;&#22270;&#36890;&#36807;&#21019;&#24314;HateModerate&#25968;&#25454;&#38598;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#29992;&#20110;&#27979;&#35797;&#33258;&#21160;&#21270;&#20869;&#23481;&#23457;&#26680;&#21592;&#23545;&#20869;&#23481;&#25919;&#31574;&#30340;&#34892;&#20026;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35753;28&#21517;&#27880;&#37322;&#21592;&#21644;GPT&#21442;&#19982;&#20845;&#27493;&#39588;&#27880;&#37322;&#36807;&#31243;&#65292;&#24471;&#20986;&#19968;&#20221;&#24694;&#27602;&#21644;&#38750;&#24694;&#27602;&#30340;&#27979;&#35797;&#22871;&#20214;&#28165;&#21333;&#65292;&#19982;Facebook&#30340;41&#26465;&#20167;&#24680;&#35328;&#35770;&#25919;&#31574;&#30456;&#21305;&#37197;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#27979;&#35797;&#26368;&#20808;&#36827;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#22120;&#23545;HateModerate&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#30456;&#24403;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.12418v2 Announce Type: replace-cross  Abstract: To protect users from massive hateful content, existing works studied automated hate speech detection. Despite the existing efforts, one question remains: do automated hate speech detectors conform to social media content policies? A platform's content policies are a checklist of content moderated by the social media platform. Because content moderation rules are often uniquely defined, existing hate speech datasets cannot directly answer this question.   This work seeks to answer this question by creating HateModerate, a dataset for testing the behaviors of automated content moderators against content policies. First, we engage 28 annotators and GPT in a six-step annotation process, resulting in a list of hateful and non-hateful test suites matching each of Facebook's 41 hate speech policies. Second, we test the performance of state-of-the-art hate speech detectors against HateModerate, revealing substantial failures these mod
&lt;/p&gt;</description></item><item><title>VOLTA&#36890;&#36807;Transformer&#19982;VAE&#26694;&#26550;&#30340;&#26356;&#26377;&#25928;&#36830;&#25509;&#65292;InfoGAN&#39118;&#26684;&#28508;&#22312;&#32534;&#30721;&#20197;&#21450;&#25903;&#25345;&#31163;&#25955;&#36755;&#20837;&#65292;&#25552;&#21319;&#20102;&#29983;&#25104;&#22810;&#26679;&#24615;</title><link>https://arxiv.org/abs/2307.00852</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#21270;&#21464;&#20998;&#20114;&#20449;&#24687;&#30340;&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;VOLTA
&lt;/p&gt;
&lt;p&gt;
VOLTA: Improving Generative Diversity by Variational Mutual Information Maximizing Autoencoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00852
&lt;/p&gt;
&lt;p&gt;
VOLTA&#36890;&#36807;Transformer&#19982;VAE&#26694;&#26550;&#30340;&#26356;&#26377;&#25928;&#36830;&#25509;&#65292;InfoGAN&#39118;&#26684;&#28508;&#22312;&#32534;&#30721;&#20197;&#21450;&#25903;&#25345;&#31163;&#25955;&#36755;&#20837;&#65292;&#25552;&#21319;&#20102;&#29983;&#25104;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#24471;&#30410;&#20110;Transformer&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#34429;&#28982;&#23427;&#20204;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#29983;&#25104;&#22810;&#26679;&#24615;&#12290;&#20808;&#21069;&#23581;&#35797;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26041;&#27861;&#35201;&#20040;&#23481;&#37327;&#36739;&#20302;&#65292;&#35201;&#20040;&#32467;&#26500;&#36807;&#20110;&#22797;&#26434;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#26041;&#27861;&#37319;&#29992;VAE&#26694;&#26550;&#22686;&#24378;&#22810;&#26679;&#24615;&#65292;&#20294;&#23427;&#20204;&#30340;&#28508;&#22312;&#21464;&#37327;&#23436;&#20840;&#20381;&#36182;&#20110;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#38480;&#21046;&#20102;&#28508;&#22312;&#31354;&#38388;&#30340;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;VOLTA&#65292;&#36890;&#36807;&#26356;&#26377;&#25928;&#30340;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#36830;&#25509;&#23558;Transformer&#19982;VAE&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#20256;&#32479;&#30340;&#23884;&#20837;&#36830;&#25509;&#25110;&#27714;&#21644;&#20013;&#33073;&#39062;&#32780;&#20986;&#65292;&#25552;&#21319;&#20102;&#29983;&#25104;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#35758;&#25972;&#21512;InfoGAN&#39118;&#26684;&#30340;&#28508;&#22312;&#32534;&#30721;&#20197;&#23454;&#29616;&#36755;&#20837;&#29420;&#31435;&#30340;&#21464;&#21270;&#24615;&#65292;&#36827;&#19968;&#27493;&#20351;&#29983;&#25104;&#22810;&#26679;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#38500;&#20102;&#25903;&#25345;&#29616;&#26377;&#30340;&#36830;&#32493;&#36755;&#20837;&#22806;&#65292;&#36824;&#25903;&#25345;&#31163;&#25955;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00852v2 Announce Type: replace  Abstract: The natural language generation domain has witnessed great success thanks to Transformer models. Although they have achieved state-of-the-art generative quality, they often neglect generative diversity. Prior attempts to tackle this issue suffer from either low model capacity or over-complicated architectures. Some recent methods employ the VAE framework to enhance diversity, but their latent variables fully depend on the input context, restricting exploration of the latent space. In this paper, we introduce VOLTA, a framework that elevates generative diversity by bridging Transformer with VAE via a more effective cross-attention-based connection, departing from conventional embedding concatenation or summation. Additionally, we propose integrating InfoGAN-style latent codes to enable input-independent variability, further diversifying the generation. Moreover, our framework accommodates discrete inputs alongside its existing support
&lt;/p&gt;</description></item><item><title>Radiology-GPT&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#25918;&#23556;&#23398;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#25918;&#23556;&#23398;&#35786;&#26029;&#12289;&#30740;&#31350;&#21644;&#27807;&#36890;&#26041;&#38754;&#23637;&#31034;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20026;&#20020;&#24202;NLP&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#25512;&#21160;&#21147;&#12290;</title><link>https://arxiv.org/abs/2306.08666</link><description>&lt;p&gt;
Radiology-GPT&#65306;&#29992;&#20110;&#25918;&#23556;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Radiology-GPT: A Large Language Model for Radiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.08666
&lt;/p&gt;
&lt;p&gt;
Radiology-GPT&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#25918;&#23556;&#23398;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#25918;&#23556;&#23398;&#35786;&#26029;&#12289;&#30740;&#31350;&#21644;&#27807;&#36890;&#26041;&#38754;&#23637;&#31034;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20026;&#20020;&#24202;NLP&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#25512;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Radiology-GPT&#65292;&#19968;&#20010;&#29992;&#20110;&#25918;&#23556;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#24191;&#27867;&#30340;&#25918;&#23556;&#23398;&#39046;&#22495;&#30693;&#35782;&#25968;&#25454;&#38598;&#19978;&#37319;&#29992;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#65292;Radiology-GPT&#34920;&#29616;&#20986;&#27604;StableLM&#12289;Dolly&#21644;LLaMA&#31561;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#23427;&#22312;&#25918;&#23556;&#23398;&#35786;&#26029;&#12289;&#30740;&#31350;&#21644;&#27807;&#36890;&#26041;&#38754;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#20652;&#21270;&#21058;&#12290;Radiology-GPT&#30340;&#25104;&#21151;&#23454;&#26045;&#34920;&#26126;&#20102;&#23450;&#20301;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#20026;&#29420;&#29305;&#30340;&#21307;&#23398;&#19987;&#19994;&#23450;&#21046;&#65292;&#21516;&#26102;&#30830;&#20445;&#36981;&#23432;HIPAA&#31561;&#38544;&#31169;&#26631;&#20934;&#12290;&#24320;&#21457;&#38024;&#23545;&#21508;&#23478;&#21307;&#38498;&#29305;&#23450;&#38656;&#27714;&#30340;&#20010;&#24615;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21069;&#26223;&#21576;&#29616;&#20986;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#36825;&#20123;&#27169;&#22411;&#20013;&#34701;&#21512;&#30340;&#20250;&#35805;&#33021;&#21147;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#34987;&#35774;&#23450;&#20026;&#20419;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.08666v2 Announce Type: replace-cross  Abstract: We introduce Radiology-GPT, a large language model for radiology. Using an instruction tuning approach on an extensive dataset of radiology domain knowledge, Radiology-GPT demonstrates superior performance compared to general language models such as StableLM, Dolly and LLaMA. It exhibits significant versatility in radiological diagnosis, research, and communication. This work serves as a catalyst for future developments in clinical NLP. The successful implementation of Radiology-GPT is indicative of the potential of localizing generative large language models, specifically tailored for distinctive medical specialties, while ensuring adherence to privacy standards such as HIPAA. The prospect of developing individualized, large-scale language models that cater to specific needs of various hospitals presents a promising direction. The fusion of conversational competence and domain-specific knowledge in these models is set to foste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#22806;&#26816;&#27979;&#26041;&#27861;SRLOOD&#65292;&#36890;&#36807;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#24341;&#23548;&#30340;&#26041;&#24335;&#20174;&#21477;&#23376;&#30340;&#19981;&#21516;&#35770;&#28857;&#20013;&#20998;&#31163;&#12289;&#25552;&#21462;&#21644;&#23398;&#20064;&#23616;&#37096;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#36793;&#32536;&#20026;&#22522;&#30784;&#36827;&#34892;&#23545;&#27604;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2305.18026</link><description>&lt;p&gt;
&#21477;&#20041;&#35282;&#33394;&#26631;&#27880;&#24341;&#23548;&#30340;&#39046;&#22495;&#22806;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semantic Role Labeling Guided Out-of-distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.18026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#22806;&#26816;&#27979;&#26041;&#27861;SRLOOD&#65292;&#36890;&#36807;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#24341;&#23548;&#30340;&#26041;&#24335;&#20174;&#21477;&#23376;&#30340;&#19981;&#21516;&#35770;&#28857;&#20013;&#20998;&#31163;&#12289;&#25552;&#21462;&#21644;&#23398;&#20064;&#23616;&#37096;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#36793;&#32536;&#20026;&#22522;&#30784;&#36827;&#34892;&#23545;&#27604;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#35782;&#21035;&#24847;&#22806;&#30340;&#39046;&#22495;&#36716;&#31227;&#23454;&#20363;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#21033;&#29992;&#21333;&#19968;&#30340;&#20840;&#23616;&#29305;&#24449;&#23884;&#20837;&#26469;&#34920;&#31034;&#21477;&#23376;&#26469;&#35782;&#21035;&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#23454;&#20363;&#65292;&#20294;&#19981;&#33021;&#24456;&#22909;&#22320;&#21051;&#30011;&#24494;&#22937;&#30340;OOD&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#21363;&#21477;&#20041;&#35282;&#33394;&#26631;&#27880;&#24341;&#23548;&#30340;&#39046;&#22495;&#22806;&#26816;&#27979;&#65288;SRLOOD&#65289;&#65292;&#36890;&#36807;&#22522;&#20110;&#36793;&#32536;&#30340;&#23545;&#27604;&#25439;&#22833;&#23558;&#21477;&#23376;&#19981;&#21516;&#35770;&#28857;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#24341;&#23548;&#30340;&#32454;&#31890;&#24230;&#23616;&#37096;&#29305;&#24449;&#34920;&#31034;&#19982;&#21477;&#23376;&#30340;&#20840;&#23616;&#29305;&#24449;&#34920;&#31034;&#20998;&#31163;&#12289;&#25552;&#21462;&#24182;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.18026v2 Announce Type: replace  Abstract: Identifying unexpected domain-shifted instances in natural language processing is crucial in real-world applications. Previous works identify the out-of-distribution (OOD) instance by leveraging a single global feature embedding to represent the sentence, which cannot characterize subtle OOD patterns well. Another major challenge current OOD methods face is learning effective low-dimensional sentence representations to identify the hard OOD instances that are semantically similar to the in-distribution (ID) data. In this paper, we propose a new unsupervised OOD detection method, namely Semantic Role Labeling Guided Out-of-distribution Detection (SRLOOD), that separates, extracts, and learns the semantic role labeling (SRL) guided fine-grained local feature representations from different arguments of a sentence and the global feature representations of the full sentence using a margin-based contrastive loss. A novel self-supervised ap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#23398;&#20064;&#27169;&#22359;&#21644;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#30340;&#26631;&#31614;&#29983;&#25104;&#27169;&#22359;&#65292;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20013;&#23398;&#20064;&#20849;&#20139;&#21644;&#31169;&#26377;&#20449;&#24687;&#65292;&#21487;&#26681;&#25454;&#21442;&#25968;&#21270;&#35843;&#25972;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2305.08473</link><description>&lt;p&gt;
&#28145;&#24230;&#27169;&#24577;&#23545;&#40784;&#21644;&#33258;&#30417;&#30563;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#20849;&#20139;&#21644;&#31169;&#26377;&#20449;&#24687;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Shared and Private Information Learning in Multimodal Sentiment Analysis with Deep Modal Alignment and Self-supervised Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.08473
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#23398;&#20064;&#27169;&#22359;&#21644;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#30340;&#26631;&#31614;&#29983;&#25104;&#27169;&#22359;&#65292;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20013;&#23398;&#20064;&#20849;&#20139;&#21644;&#31169;&#26377;&#20449;&#24687;&#65292;&#21487;&#26681;&#25454;&#21442;&#25968;&#21270;&#35843;&#25972;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#35774;&#35745;&#19968;&#20010;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25361;&#25112;&#22312;&#20110;&#23398;&#20064;&#23436;&#25972;&#27169;&#24577;&#34920;&#31034;&#20013;&#30340;&#20849;&#20139;&#20449;&#24687;&#21644;&#31169;&#26377;&#20449;&#24687;&#65292;&#36825;&#22312;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#26631;&#31614;&#21644;&#21407;&#22987;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#20013;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#28145;&#24230;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#23398;&#20064;&#27169;&#22359;&#65292;&#29992;&#20110;&#25429;&#25417;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#30340;&#26631;&#31614;&#29983;&#25104;&#27169;&#22359;&#26469;&#25429;&#25417;&#27169;&#24577;&#30340;&#31169;&#26377;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#26159;&#21363;&#25554;&#21363;&#29992;&#30340;&#65292;&#36890;&#36807;&#25913;&#21464;&#21442;&#25968;&#21270;&#65292;&#21487;&#20197;&#35843;&#25972;&#27169;&#24577;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#20851;&#31995;&#65292;&#24182;&#23398;&#20064;&#25351;&#23450;&#27169;&#24335;&#20043;&#38388;&#30340;&#31169;&#26377;&#25110;&#20849;&#20139;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#65292;&#24110;&#21161;&#27169;&#22411;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#27169;&#24577;d&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.08473v2 Announce Type: replace  Abstract: Designing an effective representation learning method for multimodal sentiment analysis tasks is a crucial research direction. The challenge lies in learning both shared and private information in a complete modal representation, which is difficult with uniform multimodal labels and a raw feature fusion approach. In this work, we propose a deep modal shared information learning module based on the covariance matrix to capture the shared information between modalities. Additionally, we use a label generation module based on a self-supervised learning strategy to capture the private information of the modalities. Our module is plug-and-play in multimodal tasks, and by changing the parameterization, it can adjust the information exchange relationship between the modes and learn the private or shared information between the specified modes. We also employ a multi-task learning strategy to help the model focus its attention on the modal d
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#20102;&#19968;&#20010;LA-GAN&#31639;&#27861;&#65292;&#29992;&#20110;&#20998;&#31867;&#23398;&#29983;&#21442;&#19982;&#20013;&#30340;&#35821;&#35328;&#27665;&#26063;&#24535;&#29305;&#24449;&#65292;&#25506;&#35752;&#23569;&#25968;&#27665;&#26063;&#25945;&#32946;&#20013;&#30340;&#23398;&#20064;&#26041;&#24335;&#21644;&#23398;&#20064;&#26041;&#27861;</title><link>https://arxiv.org/abs/2301.13853</link><description>&lt;p&gt;
LAGAN: &#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#28145;&#24230;&#21322;&#30417;&#30563;&#35821;&#35328;&#20154;&#31867;&#23398;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LAGAN: Deep Semi-Supervised Linguistic-Anthropology Classification with Conditional Generative Adversarial Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13853
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#20102;&#19968;&#20010;LA-GAN&#31639;&#27861;&#65292;&#29992;&#20110;&#20998;&#31867;&#23398;&#29983;&#21442;&#19982;&#20013;&#30340;&#35821;&#35328;&#27665;&#26063;&#24535;&#29305;&#24449;&#65292;&#25506;&#35752;&#23569;&#25968;&#27665;&#26063;&#25945;&#32946;&#20013;&#30340;&#23398;&#20064;&#26041;&#24335;&#21644;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#26159;&#27599;&#20010;&#20154;&#30340;&#26435;&#21033;&#65292;&#28982;&#32780;&#65292;&#27599;&#20010;&#20010;&#20307;&#37117;&#19982;&#20854;&#20182;&#20154;&#19981;&#21516;&#12290;&#22312;&#21518;&#20849;&#20135;&#20027;&#20041;&#26102;&#20195;&#65292;&#25945;&#24072;&#21457;&#29616;&#22266;&#26377;&#30340;&#20010;&#20154;&#20027;&#20041;&#20197;&#21516;&#26679;&#30340;&#26041;&#24335;&#22521;&#35757;&#25152;&#26377;&#20154;&#26397;&#21521;&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;&#30340;&#24037;&#20316;&#24066;&#22330;&#12290;&#25105;&#20204;&#21487;&#20197;&#32771;&#34385;&#23398;&#26415;&#23454;&#36341;&#20013;&#30340;&#23569;&#25968;&#27665;&#26063;&#25945;&#32946;&#22330;&#26223;&#12290;&#23569;&#25968;&#27665;&#26063;&#32676;&#20307;&#22312;&#33258;&#24049;&#30340;&#25991;&#21270;&#20013;&#25104;&#38271;&#65292;&#24182;&#24076;&#26395;&#29992;&#33258;&#24049;&#30340;&#26041;&#24335;&#25945;&#25480;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#35821;&#35328;&#20154;&#31867;&#23398;&#65288;&#20154;&#20204;&#22914;&#20309;&#23398;&#20064;&#65289;&#20026;&#22522;&#30784;&#30340;&#21442;&#19982;&#24418;&#24335;&#24314;&#31435;&#20026;&#21322;&#30417;&#30563;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26465;&#20214;&#28145;&#24230;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31639;&#27861;&#65292;&#21363;LA-GAN&#65292;&#29992;&#20110;&#20998;&#31867;&#23398;&#29983;&#21442;&#19982;&#20013;&#30340;&#35821;&#35328;&#27665;&#26063;&#24535;&#29305;&#24449;&#12290;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#21322;&#30417;&#30563;&#23545;&#25239;&#27169;&#22411;&#30340;&#23458;&#35266;&#24615;&#12289;&#27491;&#21017;&#21270;&#21644;&#25439;&#22833;&#20989;&#25968;&#12290;&#20934;&#22791;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#20197;&#20102;&#35299;z&#20195;&#21644;&#23569;&#25968;&#26063;&#32676;&#30340;&#19968;&#20123;&#20551;&#35774;&#65292;&#21253;&#25324;&#20182;&#20204;&#30340;&#23398;&#20064;&#39118;&#26684;&#12289;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13853v2 Announce Type: replace  Abstract: Education is a right of all, however, every individual is different than others. Teachers in post-communism era discover inherent individualism to equally train all towards job market of fourth industrial revolution. We can consider scenario of ethnic minority education in academic practices. Ethnic minority group has grown in their own culture and would prefer to be taught in their native way. We have formulated such linguistic anthropology(how people learn)based engagement as semi-supervised problem. Then, we have developed an conditional deep generative adversarial network algorithm namely LA-GAN to classify linguistic ethnographic features in student engagement. Theoretical justification proves the objective, regularization and loss function of our semi-supervised adversarial model. Survey questions are prepared to reach some form of assumptions about z-generation and ethnic minority group, whose learning style, learning approach
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#65292;&#37325;&#35775;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#27169;&#22411;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#37096;&#20998;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#23545;&#22810;&#20010;LLM&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#30340;&#26032;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.01989</link><description>&lt;p&gt;
&#37325;&#35775;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#65292;&#20174;&#20301;&#32622;&#20559;&#35265;&#30340;&#35282;&#24230;&#20986;&#21457;
&lt;/p&gt;
&lt;p&gt;
Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias. (arXiv:2401.01989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01989
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#65292;&#37325;&#35775;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#27169;&#22411;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#37096;&#20998;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#23545;&#22810;&#20010;LLM&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#30340;&#26032;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#26469;&#34920;&#24449;&#21644;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#20808;&#21069;&#25991;&#29486;&#20013;&#30740;&#31350;&#36807;&#30340;&#26356;&#20026;&#38480;&#21046;&#24615;&#30340;&#24341;&#23548;&#20559;&#35265;&#29616;&#35937;&#30340;&#19968;&#33324;&#34920;&#36848;&#12290;&#20301;&#32622;&#20559;&#35265;&#25429;&#25417;&#21040;&#27169;&#22411;&#22312;&#36755;&#20837;&#25991;&#26412;&#30340;&#26576;&#20123;&#37096;&#20998;&#19978;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#20449;&#24687;&#65292;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;LLM&#27169;&#22411;&#22914;GPT 3.5-Turbo&#65292;Llama-2&#21644;Dolly-v2&#20013;&#30340;&#20301;&#32622;&#20559;&#35265;&#65292;&#20197;&#21450;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#22914;Pegasus&#21644;BART&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize and study zero-shot abstractive summarization in Large Language Models (LLMs) by measuring position bias, which we propose as a general formulation of the more restrictive lead bias phenomenon studied previously in the literature. Position bias captures the tendency of a model unfairly prioritizing information from certain parts of the input text over others, leading to undesirable behavior. Through numerous experiments on four diverse real-world datasets, we study position bias in multiple LLM models such as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained encoder-decoder abstractive summarization models such as Pegasus and BART. Our findings lead to novel insights and discussion on performance and position bias of models for zero-shot summarization tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#36830;&#32493; (TiC) &#22522;&#20934;&#65292;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#25490;&#32451;&#26041;&#27861;&#26469;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16226</link><description>&lt;p&gt;
TiC-CLIP: CLIP&#27169;&#22411;&#30340;&#25345;&#32493;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TiC-CLIP: Continual Training of CLIP Models. (arXiv:2310.16226v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16226
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#36830;&#32493; (TiC) &#22522;&#20934;&#65292;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#25490;&#32451;&#26041;&#27861;&#26469;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25345;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#19982;&#26368;&#26032;&#25968;&#25454;&#20445;&#25345;&#21516;&#27493;&#26412;&#36523;&#23601;&#26159;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#19981;&#26029;&#37325;&#26032;&#35757;&#32451;&#30340;&#39640;&#25104;&#26412;&#65292;&#25345;&#32493;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#32570;&#20047;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#25110;&#22522;&#32447;&#25152;&#21152;&#21095;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#31532;&#19968;&#25209; Web &#35268;&#27169;&#26102;&#38388;&#36830;&#32493;&#65288;TiC&#65289;&#22522;&#20934;&#65306;TiC-DataCompt&#12289;TiC-YFCC &#21644; TiC-RedCaps&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807; 127 &#20159;&#20010;&#26102;&#38388;&#25139;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#36328;&#36234;&#20102; 9 &#24180;&#30340;&#26102;&#38388;&#65288;2014-2022&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#26469;&#31574;&#21010;&#21508;&#31181;&#21160;&#24577;&#35780;&#20272;&#65292;&#20197;&#34913;&#37327;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; OpenAI &#30340; CLIP &#27169;&#22411;&#65288;&#20351;&#29992; 2020 &#24180;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65289;&#22312;&#25105;&#20204;&#31574;&#21010;&#30340;&#20174; 2021 &#24180;&#21040; 2022 &#24180;&#30340;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;&#22833;&#21435;&#20102;&#32422; 8% &#30340;&#38646;-shot&#20934;&#30830;&#29575;&#65292;&#32780;&#19982; OpenCLIP &#23384;&#20648;&#24211;&#20013;&#26368;&#36817;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#23545;&#26102;&#38388;&#36830;&#32493;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25490;&#32451;&#26041;&#27861;&#65292;&#20174;&#19978;&#27425;&#30340;&#35757;&#32451;&#20013;&#32487;&#32493;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with over 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses $\approx 8\%$ zero-shot accuracy on our curated retrieval task from 2021--2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the l
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#38646;&#23556;&#20132;&#20114;&#20010;&#24615;&#21270;&#23545;&#35937;&#23548;&#33322;&#65288;ZIPON&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#29992;&#25143;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#20010;&#24615;&#21270;&#30446;&#26631;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07968</link><description>&lt;p&gt;
&#24605;&#32771;&#12289;&#34892;&#21160;&#21644;&#38382;&#65306;&#24320;&#25918;&#19990;&#30028;&#20114;&#21160;&#20010;&#24615;&#21270;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation. (arXiv:2310.07968v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07968
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#38646;&#23556;&#20132;&#20114;&#20010;&#24615;&#21270;&#23545;&#35937;&#23548;&#33322;&#65288;ZIPON&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#29992;&#25143;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#20010;&#24615;&#21270;&#30446;&#26631;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#23556;&#21629;&#20196;&#23545;&#35937;&#23548;&#33322;&#65288;ZSON&#65289;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#24320;&#25918;&#35789;&#27719;&#23545;&#35937;&#12290;&#29616;&#26377;&#30340;ZSON&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#36981;&#24490;&#20010;&#21035;&#25351;&#20196;&#20197;&#23547;&#25214;&#36890;&#29992;&#23545;&#35937;&#31867;&#65292;&#24573;&#30053;&#20102;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#21033;&#29992;&#21644;&#35782;&#21035;&#29992;&#25143;&#29305;&#23450;&#23545;&#35937;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38646;&#23556;&#20132;&#20114;&#20010;&#24615;&#21270;&#23545;&#35937;&#23548;&#33322;&#65288;ZIPON&#65289;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#38656;&#35201;&#22312;&#19982;&#29992;&#25143;&#23545;&#35805;&#30340;&#21516;&#26102;&#23548;&#33322;&#21040;&#20010;&#24615;&#21270;&#30446;&#26631;&#23545;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;ZIPON&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#31216;&#20026;&#24320;&#25918;&#19990;&#30028;&#20114;&#21160;&#20010;&#24615;&#21270;&#23548;&#33322;&#65288;ORION&#65289;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#24207;&#21015;&#20915;&#31574;&#65292;&#20197;&#25805;&#20316;&#19981;&#21516;&#30340;&#24863;&#30693;&#12289;&#23548;&#33322;&#21644;&#36890;&#20449;&#27169;&#22359;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#33021;&#22815;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#30340;&#20114;&#21160;&#20195;&#29702;&#30340;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Object Navigation (ZSON) enables agents to navigate towards open-vocabulary objects in unknown environments. The existing works of ZSON mainly focus on following individual instructions to find generic object classes, neglecting the utilization of natural language interaction and the complexities of identifying user-specific objects. To address these limitations, we introduce Zero-shot Interactive Personalized Object Navigation (ZIPON), where robots need to navigate to personalized goal objects while engaging in conversations with users. To solve ZIPON, we propose a new framework termed Open-woRld Interactive persOnalized Navigation (ORION), which uses Large Language Models (LLMs) to make sequential decisions to manipulate different modules for perception, navigation and communication. Experimental results show that the performance of interactive agents that can leverage user feedback exhibits significant improvement. However, obtaining a good balance between task completion 
&lt;/p&gt;</description></item><item><title>$\mathcal{B}$-Coder&#26159;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#31243;&#24207;&#21512;&#25104;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#25191;&#34892;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03173</link><description>&lt;p&gt;
$\mathcal{B}$-Coder: &#22522;&#20110;&#20215;&#20540;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
$\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis. (arXiv:2310.03173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03173
&lt;/p&gt;
&lt;p&gt;
$\mathcal{B}$-Coder&#26159;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#31243;&#24207;&#21512;&#25104;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#25191;&#34892;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#21512;&#25104;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#21019;&#24314;&#20934;&#30830;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#12290;&#35813;&#39046;&#22495;&#32467;&#21512;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21147;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#35813;&#38598;&#25104;&#20027;&#35201;&#20851;&#27880;&#30452;&#25509;&#20248;&#21270;&#21151;&#33021;&#27491;&#30830;&#24615;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#25439;&#22833;&#12290;&#23613;&#31649;&#24403;&#21069;&#25991;&#29486;&#20027;&#35201;&#25903;&#25345;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#20294;&#31243;&#24207;&#21512;&#25104;&#30340;&#23646;&#24615;&#34920;&#26126;&#19982;&#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#33258;&#28982;&#20860;&#23481;&#12290;&#36825;&#28304;&#20110;&#20154;&#31867;&#31243;&#24207;&#21592;&#24320;&#21457;&#30340;&#20016;&#23500;&#31163;&#32447;&#31243;&#24207;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#21333;&#20803;&#27979;&#35797;&#23545;&#29983;&#25104;&#30340;&#31243;&#24207;&#36827;&#34892;&#30452;&#35266;&#39564;&#35777;&#65288;&#21363;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#23481;&#26131;&#33719;&#24471;&#22870;&#21169;&#30340;&#35821;&#35328;&#34920;&#36798;&#65289;&#12290;&#19982;&#20027;&#35201;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#25105;&#20204;&#30340;$\mathcal{B}$-Coder&#65288;&#21457;&#38899;&#20026;Bellman coder&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Program synthesis aims to create accurate, executable code from natural language descriptions. This field has leveraged the power of reinforcement learning (RL) in conjunction with large language models (LLMs), significantly enhancing code generation capabilities. This integration focuses on directly optimizing functional correctness, transcending conventional supervised losses. While current literature predominantly favors policy-based algorithms, attributes of program synthesis suggest a natural compatibility with value-based methods. This stems from rich collection of off-policy programs developed by human programmers, and the straightforward verification of generated programs through automated unit testing (i.e. easily obtainable rewards in RL language). Diverging from the predominant use of policy-based algorithms, our work explores the applicability of value-based approaches, leading to the development of our $\mathcal{B}$-Coder (pronounced Bellman coder). Yet, training value-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#28508;&#21147;&#21644;&#32570;&#38519;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#22312;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#21162;&#21147;&#24320;&#21457;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;LLMs&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.13173</link><description>&lt;p&gt;
BenLLMEval: &#19968;&#39033;&#23545;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28508;&#21147;&#21644;&#32570;&#38519;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP. (arXiv:2309.13173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#28508;&#21147;&#21644;&#32570;&#38519;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#22312;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#21162;&#21147;&#24320;&#21457;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;LLMs&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22312;&#35821;&#35328;&#29983;&#25104;&#21644;&#20854;&#20182;&#20855;&#20307;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#33021;&#21147;&#32780;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#37325;&#35201;&#30340;&#31361;&#30772;&#20043;&#19968;&#12290;&#23613;&#31649;LLMs&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#24471;&#21040;&#35780;&#20272;&#65292;&#20294;&#22823;&#37096;&#20998;&#35780;&#20272;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#23578;&#26410;&#23545;&#23391;&#21152;&#25289;&#35821;&#31561;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;LLMs&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#23391;&#21152;&#25289;&#35821;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#21508;&#31181;&#37325;&#35201;&#19988;&#22810;&#26679;&#30340;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22914;&#25277;&#35937;&#25688;&#35201;&#12289;&#38382;&#31572;&#12289;&#25913;&#20889;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#65292;&#20197;&#38646;-shot&#35780;&#20272;ChatGPT&#12289;LLaMA-2&#21644;Claude-2&#65292;&#24182;&#23558;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#22312;&#19981;&#21516;&#30340;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#21162;&#21147;&#24320;&#21457;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#23391;&#21152;&#25289;&#35821;&#65289;&#20013;LLMs&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as one of the most important breakthroughs in natural language processing (NLP) for their impressive skills in language generation and other language-specific tasks. Though LLMs have been evaluated in various tasks, mostly in English, they have not yet undergone thorough evaluation in under-resourced languages such as Bengali (Bangla). In this paper, we evaluate the performance of LLMs for the low-resourced Bangla language. We select various important and diverse Bangla NLP tasks, such as abstractive summarization, question answering, paraphrasing, natural language inference, text classification, and sentiment analysis for zero-shot evaluation with ChatGPT, LLaMA-2, and Claude-2 and compare the performance with state-of-the-art fine-tuned models. Our experimental results demonstrate an inferior performance of LLMs for different Bangla NLP tasks, calling for further effort to develop better understanding of LLMs in low-resource languages like Ba
&lt;/p&gt;</description></item><item><title>&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#26102;&#65292;&#20165;&#24378;&#35843;&#24110;&#21161;&#24615;&#32780;&#19981;&#32771;&#34385;&#23433;&#20840;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;LLaMA&#27169;&#22411;&#26102;&#28155;&#21152;&#23569;&#37327;&#23433;&#20840;&#31034;&#20363;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#23433;&#20840;&#24615;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#33021;&#21147;&#21644;&#24110;&#21161;&#24615;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#23433;&#20840;&#35843;&#20248;&#20250;&#20351;&#27169;&#22411;&#25298;&#32477;&#22238;&#24212;&#34920;&#38754;&#19978;&#31867;&#20284;&#20110;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#21512;&#29702;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.07875</link><description>&lt;p&gt;
&#23433;&#20840;&#35843;&#20248;&#30340;LLaMAs&#65306;&#20174;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#30340;&#23433;&#20840;&#24615;&#20013;&#23398;&#21040;&#30340;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions. (arXiv:2309.07875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07875
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#26102;&#65292;&#20165;&#24378;&#35843;&#24110;&#21161;&#24615;&#32780;&#19981;&#32771;&#34385;&#23433;&#20840;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;LLaMA&#27169;&#22411;&#26102;&#28155;&#21152;&#23569;&#37327;&#23433;&#20840;&#31034;&#20363;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#23433;&#20840;&#24615;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#33021;&#21147;&#21644;&#24110;&#21161;&#24615;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#23433;&#20840;&#35843;&#20248;&#20250;&#20351;&#27169;&#22411;&#25298;&#32477;&#22238;&#24212;&#34920;&#38754;&#19978;&#31867;&#20284;&#20110;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#21512;&#29702;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#21487;&#20197;&#20351;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#24471;&#26356;&#22909;&#65292;&#36890;&#24120;&#26356;&#20855;&#26377;&#24110;&#21161;&#24615;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#23436;&#20840;&#26377;&#29992;&#30340;&#27169;&#22411;&#20250;&#36981;&#24490;&#29978;&#33267;&#26368;&#24694;&#24847;&#30340;&#25351;&#20196;&#65292;&#24182;&#36731;&#26131;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#37027;&#20123;&#21482;&#24378;&#35843;&#24110;&#21161;&#24615;&#32780;&#19981;&#32771;&#34385;&#23433;&#20840;&#24615;&#30340;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#38750;&#24120;&#19981;&#23433;&#20840;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;fine-tuning&#31867;&#20284;LLaMA&#30340;&#27169;&#22411;&#26102;&#65292;&#21482;&#38656;&#23558;3%&#30340;&#23433;&#20840;&#31034;&#20363;&#65288;&#20960;&#30334;&#20010;&#28436;&#31034;&#65289;&#28155;&#21152;&#21040;&#35757;&#32451;&#38598;&#20013;&#65292;&#23601;&#33021;&#26174;&#33879;&#25552;&#39640;&#20854;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#23433;&#20840;&#35843;&#20248;&#24182;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#33021;&#21147;&#25110;&#24110;&#21161;&#24615;&#65292;&#36825;&#26159;&#36890;&#36807;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#26469;&#34913;&#37327;&#30340;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#36807;&#24230;&#23433;&#20840;&#30340;&#34892;&#20026;&#65292;&#21363;&#36807;&#24230;&#30340;&#23433;&#20840;&#35843;&#20248;&#20250;&#20351;&#24471;&#27169;&#22411;&#25298;&#32477;&#23545;&#34920;&#38754;&#19978;&#31867;&#20284;&#20110;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#21512;&#29702;&#25552;&#31034;&#20570;&#20986;&#22238;&#24212;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#35757;&#32451;LLM&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#26102;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models to follow instructions makes them perform better on a wide range of tasks, generally becoming more helpful. However, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content. In this paper, we raise concerns over the safety of models that only emphasize helpfulness, not safety, in their instruction-tuning. We show that several popular instruction-tuned models are highly unsafe. Moreover, we show that adding just 3% safety examples (a few hundred demonstrations) in the training set when fine-tuning a model like LLaMA can substantially improve their safety. Our safety-tuning does not make models significantly less capable or helpful as measured by standard benchmarks. However, we do find a behavior of exaggerated safety, where too much safety-tuning makes models refuse to respond to reasonable prompts that superficially resemble unsafe ones. Our study sheds light on trade-offs in training LLMs to follow
&lt;/p&gt;</description></item><item><title>T-MARS&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#36991;&#25991;&#26412;&#29305;&#24449;&#23398;&#20064;&#65292;&#25913;&#21892;&#20102;&#35270;&#35273;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#19982;&#22270;&#20687;&#37325;&#21472;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03132</link><description>&lt;p&gt;
T-MARS&#65306;&#36890;&#36807;&#35268;&#36991;&#25991;&#26412;&#29305;&#24449;&#23398;&#20064;&#26469;&#25913;&#21892;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
T-MARS: Improving Visual Representations by Circumventing Text Feature Learning. (arXiv:2307.03132v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03132
&lt;/p&gt;
&lt;p&gt;
T-MARS&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#36991;&#25991;&#26412;&#29305;&#24449;&#23398;&#20064;&#65292;&#25913;&#21892;&#20102;&#35270;&#35273;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#19982;&#22270;&#20687;&#37325;&#21472;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#32593;&#32476;&#26469;&#28304;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20026;&#23398;&#20064;&#36890;&#29992;&#35270;&#35273;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#25552;&#20379;&#20102;&#21160;&#21147;&#65292;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#24443;&#24213;&#25913;&#21464;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35782;&#21035;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#20915;&#31574;&#38382;&#39064;&#26159;&#22914;&#20309;&#31579;&#36873;&#36825;&#20123;&#26085;&#30410;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#31579;&#36873;&#26041;&#27861;&#65292;&#20854;&#21160;&#26426;&#26159;&#25105;&#20204;&#35266;&#23519;&#21040;&#36817;40%&#30340;LAION&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#19982;&#35828;&#26126;&#23384;&#22312;&#37325;&#21472;&#30340;&#25991;&#26412;&#12290;&#30452;&#35273;&#19978;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#21487;&#33021;&#20250;&#28010;&#36153;&#36164;&#28304;&#65292;&#22240;&#20026;&#23427;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#32780;&#19981;&#26159;&#23398;&#20064;&#35270;&#35273;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#23558;&#25152;&#26377;&#36825;&#20123;&#25968;&#25454;&#21435;&#38500;&#20063;&#21487;&#33021;&#28010;&#36153;&#65292;&#22240;&#20026;&#36825;&#20250;&#20002;&#24323;&#21253;&#21547;&#35270;&#35273;&#29305;&#24449;&#30340;&#22270;&#20687;&#65288;&#38500;&#20102;&#37325;&#21472;&#30340;&#25991;&#26412;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large web-sourced multimodal datasets have powered a slew of new methods for learning general-purpose visual representations, advancing the state of the art in computer vision and revolutionizing zero- and few-shot recognition. One crucial decision facing practitioners is how, if at all, to curate these ever-larger datasets. For example, the creators of the LAION-5B dataset chose to retain only image-caption pairs whose CLIP similarity score exceeded a designated threshold. In this paper, we propose a new state-of-the-art data filtering approach motivated by our observation that nearly 40% of LAION's images contain text that overlaps significantly with the caption. Intuitively, such data could be wasteful as it incentivizes models to perform optical character recognition rather than learning visual features. However, naively removing all such data could also be wasteful, as it throws away images that contain visual features (in addition to overlapping text). Our simple and scalable app
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DNABERT-2&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#31181;&#29289;&#31181;&#22522;&#22240;&#32452;&#30340;&#39640;&#25928;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32479;&#35745;&#30340;&#25968;&#25454;&#21387;&#32553;&#31639;&#27861;Byte Pair Encoding&#65288;BPE&#65289;&#26367;&#20195;&#20256;&#32479;&#30340;k-mer&#26631;&#35760;&#21270;&#65292;&#20811;&#26381;&#20102;k-mer&#26631;&#35760;&#21270;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.15006</link><description>&lt;p&gt;
DNABERT-2:&#22810;&#31181;&#29289;&#31181;&#22522;&#22240;&#32452;&#30340;&#39640;&#25928;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome. (arXiv:2306.15006v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DNABERT-2&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#31181;&#29289;&#31181;&#22522;&#22240;&#32452;&#30340;&#39640;&#25928;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32479;&#35745;&#30340;&#25968;&#25454;&#21387;&#32553;&#31639;&#27861;Byte Pair Encoding&#65288;BPE&#65289;&#26367;&#20195;&#20256;&#32479;&#30340;k-mer&#26631;&#35760;&#21270;&#65292;&#20811;&#26381;&#20102;k-mer&#26631;&#35760;&#21270;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#30721;&#22522;&#22240;&#32452;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#26159;&#29983;&#29289;&#23398;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#32780;DNABERT&#21644;Nucleotide Transformer&#31561;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#20381;&#36182;&#20110;k-mer&#20316;&#20026;&#22522;&#22240;&#32452;&#35821;&#35328;&#30340;&#26631;&#35760;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;k-mer&#26631;&#35760;&#21270;&#24341;&#20837;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#26159;&#21457;&#23637;&#22823;&#35268;&#27169;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#22240;&#32452;&#26631;&#35760;&#21270;&#30340;&#27010;&#24565;&#21644;&#32463;&#39564;&#35265;&#35299;&#65292;&#22522;&#20110;&#27492;&#25552;&#20986;&#29992;&#22522;&#20110;&#32479;&#35745;&#30340;&#25968;&#25454;&#21387;&#32553;&#31639;&#27861;Byte Pair Encoding&#65288;BPE&#65289;&#26367;&#20195;k-mer&#26631;&#35760;&#21270;&#65292;BPE&#36890;&#36807;&#36845;&#20195;&#21512;&#24182;&#35821;&#26009;&#24211;&#20013;&#26368;&#39057;&#32321;&#20849;&#21516;&#20986;&#29616;&#30340;&#22522;&#22240;&#32452;&#29255;&#27573;&#26469;&#26500;&#24314;&#26631;&#35760;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;BPE&#19981;&#20165;&#20811;&#26381;&#20102;k-mer&#26631;&#35760;&#21270;&#30340;&#23616;&#38480;&#24615;&#65292;&#36824;&#33021;&#20174;&#38750;&#37325;&#21472;&#26631;&#35760;&#21270;&#30340;&#35745;&#31639;&#25928;&#29575;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; GEMEL &#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340; LLMs &#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#23454;&#20307;&#21517;&#31216;&#65292;&#20165;&#35843;&#25972;&#20102;&#26497;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340; MEL &#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12725</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Generative Multimodal Entity Linking. (arXiv:2306.12725v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; GEMEL &#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340; LLMs &#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#23454;&#20307;&#21517;&#31216;&#65292;&#20165;&#35843;&#25972;&#20102;&#26497;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340; MEL &#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#26159;&#23558;&#24102;&#26377;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#25552;&#21450;&#26144;&#23556;&#21040;&#30693;&#35782;&#24211;&#65288;&#20363;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#20013;&#30340;&#24341;&#29992;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; GEMEL &#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340; LLMs &#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#23454;&#20307;&#21517;&#31216;&#12290;&#25105;&#20204;&#20445;&#25345;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20923;&#32467;&#65292;&#21482;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#23618;&#20197;&#21551;&#29992;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#20026;&#20102;&#23558; LLMs &#36866;&#24212; MEL &#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992; LLMs &#30340;&#26032;&#20852;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#36890;&#36807;&#26816;&#32034;&#22810;&#27169;&#24577;&#23454;&#20363;&#20316;&#20026;&#31034;&#33539;&#26469;&#36827;&#34892;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#35843;&#25972;&#20102;&#22823;&#32422;0.3&#65285;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;GEMEL &#23601;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Entity Linking (MEL) is the task of mapping mentions with multimodal contexts to the referent entities from a knowledge base (e.g., Wikipedia). Prior MEL methods mainly focus on designing complex multimodal interaction mechanisms and require fine-tuning all model parameters, which can be prohibitively costly and difficult to scale in the era of Large Language Models (LLMs). In this work, we propose GEMEL, a simple yet effective Generative Multimodal Entity Linking method, which leverages the capabilities of LLMs from large-scale pre-training to directly generate target entity names. We keep the vision and language model frozen and only train a linear layer to enable cross-modality interactions. To adapt LLMs to the MEL task, we take advantage of the emerging in-context learning (ICL) capability of LLMs by retrieving multimodal instances as demonstrations. Extensive experiments show that with only ~0.3% of the model parameters fine-tuned, GEMEL achieves state-of-the-art resul
&lt;/p&gt;</description></item><item><title>LeTI&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;LM&#29983;&#25104;&#30340;&#31243;&#24207;&#21644;&#38169;&#35823;&#28040;&#24687;&#36827;&#34892;&#20018;&#32852;&#36845;&#20195;&#24494;&#35843;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#33258;&#28982;&#21457;&#29983;&#30340;Python&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20808;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.10314</link><description>&lt;p&gt;
LeTI&#65306;&#20174;&#25991;&#26412;&#20132;&#20114;&#20013;&#23398;&#20064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LeTI: Learning to Generate from Textual Interactions. (arXiv:2305.10314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10314
&lt;/p&gt;
&lt;p&gt;
LeTI&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;LM&#29983;&#25104;&#30340;&#31243;&#24207;&#21644;&#38169;&#35823;&#28040;&#24687;&#36827;&#34892;&#20018;&#32852;&#36845;&#20195;&#24494;&#35843;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#33258;&#28982;&#21457;&#29983;&#30340;Python&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20808;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LM)&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#25216;&#26415;&#36890;&#36807;&#36755;&#20837;&#36755;&#20986;&#23545;&#65288;&#20363;&#22914;&#25351;&#20196;&#24494;&#35843;&#65289;&#25110;&#29992;&#35780;&#20272;&#36755;&#20986;&#36136;&#37327;&#30340;&#25968;&#23383;&#22870;&#21169;&#65288;&#20363;&#22914;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#23545;&#39044;&#35757;&#32451;&#30340;LM&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;LM&#20174;&#25991;&#26412;&#20132;&#20114;&#20013;&#23398;&#20064;&#30340;&#28508;&#21147;(LeTI)&#65292;&#36825;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#20108;&#36827;&#21046;&#26631;&#31614;&#26816;&#26597;&#20854;&#27491;&#30830;&#24615;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#21453;&#39304;&#25351;&#20986;&#21644;&#35299;&#37322;&#20854;&#36755;&#20986;&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#20195;&#30721;&#29255;&#27573;&#12290;&#36825;&#31181;&#35774;&#32622;&#21487;&#20197;&#33258;&#28982;&#19988;&#21487;&#25193;&#23637;&#22320;&#33719;&#21462;&#25991;&#26412;&#21453;&#39304;&#65306;&#20351;&#29992;Python&#35299;&#37322;&#22120;&#36827;&#34892;&#20195;&#30721;&#25191;&#34892;&#26102;&#30340;&#38169;&#35823;&#28040;&#24687;&#21644;&#22534;&#26632;&#36319;&#36394;&#12290; LeTI&#20351;&#29992;LM&#30446;&#26631;&#23545;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;LM&#29983;&#25104;&#30340;&#31243;&#24207;&#21644;&#25991;&#26412;&#21453;&#39304;&#36827;&#34892;&#20018;&#32852;&#30340;&#36845;&#20195;&#24494;&#35843;&#65292;&#21482;&#26377;&#22312;&#29983;&#25104;&#20195;&#30721;&#26080;&#27861;&#25191;&#34892;&#26102;&#25165;&#25552;&#20379;&#25991;&#26412;&#21453;&#39304;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;58k&#20010;&#33258;&#28982;&#21457;&#29983;&#30340;Python&#25351;&#20196;&#65292;&#22686;&#21152;&#20102;&#38169;&#35823;&#28040;&#24687;&#21644;&#22534;&#26632;&#36319;&#36394;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LeTI&#65292;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#19978;&#26174;&#33879;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning pre-trained language models (LMs) enhances the models' capabilities. Prior techniques fine-tune a pre-trained LM on input-output pairs (e.g., instruction fine-tuning), or with numerical rewards that gauge the quality of its outputs (e.g., reinforcement learning from human feedback). We explore LMs' potential to learn from textual interactions (LeTI) that not only check their correctness with binary labels, but also pinpoint and explain errors in their outputs through textual feedback. Our investigation focuses on the code generation task, where the model produces code pieces in response to natural language instructions. This setting invites a natural and scalable way to acquire the textual feedback: the error messages and stack traces from code execution using a Python interpreter. LeTI iteratively fine-tunes the model, using the LM objective, on a concatenation of natural language instructions, LM-generated programs, and textual feedback, which is only provided when the gen
&lt;/p&gt;</description></item></channel></rss>