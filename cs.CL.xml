<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>DUMA&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.18075</link><description>&lt;p&gt;
DUMA&#65306;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking. (arXiv:2310.18075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18075
&lt;/p&gt;
&lt;p&gt;
DUMA&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DUMA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29992;&#20110;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#30340;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20307;&#29616;&#20102;&#21452;&#37325;&#24605;&#32500;&#26426;&#21046;&#12290;&#24555;&#36895;&#24605;&#32771;&#27169;&#22411;&#20316;&#20026;&#20027;&#35201;&#25509;&#21475;&#29992;&#20110;&#22806;&#37096;&#20132;&#20114;&#21644;&#21021;&#22987;&#21709;&#24212;&#29983;&#25104;&#65292;&#26681;&#25454;&#23436;&#25972;&#21709;&#24212;&#30340;&#22797;&#26434;&#24615;&#35780;&#20272;&#26159;&#21542;&#38656;&#35201;&#35843;&#29992;&#24930;&#36895;&#24605;&#32771;&#27169;&#22411;&#12290;&#19968;&#26086;&#34987;&#35843;&#29992;&#65292;&#24930;&#36895;&#24605;&#32771;&#27169;&#22411;&#25509;&#31649;&#23545;&#35805;&#65292;&#22312;&#32454;&#33268;&#35268;&#21010;&#12289;&#25512;&#29702;&#21644;&#24037;&#20855;&#21033;&#29992;&#26041;&#38754;&#36827;&#34892;&#24037;&#20316;&#65292;&#25552;&#20379;&#32463;&#36807;&#20805;&#20998;&#20998;&#26512;&#30340;&#21709;&#24212;&#12290;&#36825;&#31181;&#21452;&#37325;&#24605;&#32500;&#37197;&#32622;&#20801;&#35768;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#25151;&#22320;&#20135;&#34892;&#19994;&#22312;&#32447;&#21672;&#35810;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the dual-process theory of human cognition, we introduce DUMA, a novel conversational agent framework that embodies a dual-mind mechanism through the utilization of two generative Large Language Models (LLMs) dedicated to fast and slow thinking respectively. The fast thinking model serves as the primary interface for external interactions and initial response generation, evaluating the necessity for engaging the slow thinking model based on the complexity of the complete response. When invoked, the slow thinking model takes over the conversation, engaging in meticulous planning, reasoning, and tool utilization to provide a well-analyzed response. This dual-mind configuration allows for a seamless transition between intuitive responses and deliberate problem-solving processes based on the situation. We have constructed a conversational agent to handle online inquiries in the real estate industry. The experiment proves that our method balances effectiveness and efficiency, an
&lt;/p&gt;</description></item><item><title>CrossCodeEval&#26159;&#19968;&#20010;&#22810;&#20803;&#21270;&#21644;&#22810;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#36328;&#25991;&#20214;&#20195;&#30721;&#34917;&#20840;&#65292;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#29702;&#35299;&#25165;&#33021;&#20934;&#30830;&#23436;&#25104;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.11248</link><description>&lt;p&gt;
CrossCodeEval: &#19968;&#20010;&#22810;&#20803;&#21270;&#21644;&#22810;&#35821;&#35328;&#30340;&#29992;&#20110;&#36328;&#25991;&#20214;&#20195;&#30721;&#34917;&#20840;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion. (arXiv:2310.11248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11248
&lt;/p&gt;
&lt;p&gt;
CrossCodeEval&#26159;&#19968;&#20010;&#22810;&#20803;&#21270;&#21644;&#22810;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#36328;&#25991;&#20214;&#20195;&#30721;&#34917;&#20840;&#65292;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#29702;&#35299;&#25165;&#33021;&#20934;&#30830;&#23436;&#25104;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#34917;&#20840;&#27169;&#22411;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#24403;&#21069;&#27969;&#34892;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#22914;HumanEval&#21644;MBPP&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#25991;&#20214;&#20869;&#30340;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#19978;&#12290;&#36825;&#31181;&#36807;&#20110;&#31616;&#21270;&#30340;&#35774;&#32622;&#26080;&#27861;&#20934;&#30830;&#22320;&#20195;&#34920;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#36719;&#20214;&#24320;&#21457;&#22330;&#26223;&#65292;&#20854;&#20013;&#23384;&#20648;&#24211;&#36328;&#36234;&#22810;&#20010;&#25991;&#20214;&#65292;&#23384;&#22312;&#22823;&#37327;&#30340;&#36328;&#25991;&#20214;&#20381;&#36182;&#20851;&#31995;&#65292;&#38656;&#35201;&#35775;&#38382;&#21644;&#29702;&#35299;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#25165;&#33021;&#27491;&#30830;&#23436;&#25104;&#20195;&#30721;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CrossCodeEval&#65292;&#19968;&#20010;&#22810;&#20803;&#21270;&#21644;&#22810;&#35821;&#35328;&#30340;&#20195;&#30721;&#34917;&#20840;&#22522;&#20934;&#27979;&#35797;&#65292;&#38656;&#35201;&#28145;&#20837;&#30340;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#29702;&#35299;&#25165;&#33021;&#20934;&#30830;&#23436;&#25104;&#20195;&#30721;&#12290;CrossCodeEval&#22522;&#20110;&#22235;&#31181;&#27969;&#34892;&#30340;&#32534;&#31243;&#35821;&#35328;&#65288;Python&#65292;Java&#65292;TypeScript&#21644;C#&#65289;&#20013;&#30340;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#12289;&#24320;&#28304;&#12289;&#26435;&#38480;&#35768;&#21487;&#30340;&#23384;&#20648;&#24211;&#38598;&#21512;&#26500;&#24314;&#12290;&#20026;&#20102;&#21019;&#24314;&#20005;&#26684;&#35201;&#27714;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#36827;&#34892;&#20934;&#30830;&#23436;&#25104;&#30340;&#31034;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#38745;&#24577;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly.  To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#38024;&#23545;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65288;TIDA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26469;&#22635;&#34917;&#20851;&#32852;&#32467;&#26500;&#24046;&#36317;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20154;&#31867;&#24863;&#30693;&#33021;&#21147;&#65292;&#22914;&#24615;&#21035;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;TIDA&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15991</link><description>&lt;p&gt;
&#26377;&#38024;&#23545;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#20102;&#22522;&#26412;&#25216;&#33021;&#23383;&#24149;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Targeted Image Data Augmentation Increases Basic Skills Captioning Robustness. (arXiv:2309.15991v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#38024;&#23545;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65288;TIDA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26469;&#22635;&#34917;&#20851;&#32852;&#32467;&#26500;&#24046;&#36317;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20154;&#31867;&#24863;&#30693;&#33021;&#21147;&#65292;&#22914;&#24615;&#21035;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;TIDA&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#22312;&#25512;&#24191;&#21040;&#36229;&#20986;&#19978;&#19979;&#25991;&#30340;&#31034;&#20363;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#36825;&#31181;&#38480;&#21046;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#26377;&#20851;&#19990;&#30028;&#28508;&#22312;&#20851;&#32852;&#32467;&#26500;&#30340;&#37096;&#20998;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TIDA&#65288;&#26377;&#38024;&#23545;&#24615;&#30340;&#22270;&#20687;&#32534;&#36753;&#25968;&#25454;&#22686;&#24378;&#65289;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#19987;&#27880;&#20110;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26469;&#22635;&#34917;&#20851;&#32852;&#32467;&#26500;&#24046;&#36317;&#20197;&#25552;&#39640;&#27169;&#22411;&#31867;&#20154;&#33021;&#21147;&#65288;&#20363;&#22914;&#24615;&#21035;&#35782;&#21035;&#65289;&#30340;&#26377;&#38024;&#23545;&#24615;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;TIDA&#35782;&#21035;&#25551;&#36848;&#22270;&#20687;&#30340;&#26631;&#39064;&#20013;&#30340;&#29305;&#23450;&#25216;&#33021;&#65288;&#20363;&#22914;&#22270;&#20013;&#29305;&#23450;&#24615;&#21035;&#30340;&#23384;&#22312;&#65289;&#65292;&#25913;&#21464;&#26631;&#39064;&#65288;&#20363;&#22914;&#23558;&#8220;&#22899;&#20154;&#8221;&#25913;&#20026;&#8220;&#30007;&#20154;&#8221;&#65289;&#65292;&#28982;&#21518;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#32534;&#36753;&#22270;&#20687;&#65292;&#20197;&#20351;&#20854;&#19982;&#26032;&#26631;&#39064;&#21305;&#37197;&#65288;&#20363;&#22914;&#21807;&#19968;&#22320;&#23558;&#19968;&#20010;&#22899;&#20154;&#25913;&#20026;&#19968;&#20010;&#30007;&#20154;&#21516;&#26102;&#20445;&#25345;&#19978;&#19979;&#25991;&#19981;&#21464;&#65289;&#12290;&#22522;&#20110;Flickr30K&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#20351;&#29992;TIDA&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks typically struggle in generalizing to out-of-context examples. One reason for this limitation is caused by having datasets that incorporate only partial information regarding the potential correlational structure of the world. In this work, we propose TIDA (Targeted Image-editing Data Augmentation), a targeted data augmentation method focused on improving models' human-like abilities (e.g., gender recognition) by filling the correlational structure gap using a text-to-image generative model. More specifically, TIDA identifies specific skills in captions describing images (e.g., the presence of a specific gender in the image), changes the caption (e.g., "woman" to "man"), and then uses a text-to-image model to edit the image in order to match the novel caption (e.g., uniquely changing a woman to a man while maintaining the context identical). Based on the Flickr30K benchmark, we show that, compared with the original data set, a TIDA-enhanced dataset related to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#39532;&#20811;&#30333;&#24422;&#30149;&#30740;&#31350;&#20013;&#23384;&#22312;&#30683;&#30462;&#32467;&#26524;&#30340;&#25253;&#21578;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#35805;&#39064;&#19982;&#26174;&#33879;&#32467;&#26524;&#30340;&#30456;&#20851;&#24615;&#65292;&#25214;&#21040;&#20102;&#19982;&#40644;&#26001;&#21464;&#24615;&#30740;&#31350;&#20013;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#30456;&#20851;&#30340;&#20843;&#31181;&#21270;&#21512;&#29289;&#12290;</title><link>http://arxiv.org/abs/2309.00312</link><description>&lt;p&gt;
&#29992;&#20110;&#39532;&#20811;&#30333;&#24422;&#30149;&#30740;&#31350;&#30340;&#19981;&#21516;&#25253;&#21578;&#32467;&#26524;&#30340;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Comparative Topic Modeling for Determinants of Divergent Report Results Applied to Macular Degeneration Studies. (arXiv:2309.00312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#39532;&#20811;&#30333;&#24422;&#30149;&#30740;&#31350;&#20013;&#23384;&#22312;&#30683;&#30462;&#32467;&#26524;&#30340;&#25253;&#21578;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#35805;&#39064;&#19982;&#26174;&#33879;&#32467;&#26524;&#30340;&#30456;&#20851;&#24615;&#65292;&#25214;&#21040;&#20102;&#19982;&#40644;&#26001;&#21464;&#24615;&#30740;&#31350;&#20013;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#30456;&#20851;&#30340;&#20843;&#31181;&#21270;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35805;&#39064;&#24314;&#27169;&#21644;&#25991;&#26412;&#25366;&#25496;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23376;&#38598;&#65292;&#36866;&#29992;&#20110;&#36827;&#34892;&#20803;&#20998;&#26512;&#21644;&#31995;&#32479;&#23457;&#26597;&#12290;&#23545;&#20110;&#35777;&#25454;&#32508;&#36848;&#65292;&#19978;&#36848;NLP&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#25991;&#29486;&#25628;&#32034;&#25110;&#20174;&#25253;&#21578;&#20013;&#25552;&#21462;&#20540;&#20197;&#33258;&#21160;&#21270;SR&#21644;MA&#30340;&#20851;&#38190;&#38454;&#27573;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#21516;&#19968;&#24191;&#20041;&#30740;&#31350;&#38382;&#39064;&#19978;&#23384;&#22312;&#30683;&#30462;&#32467;&#26524;&#30340;&#25253;&#21578;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#26681;&#25454;&#20854;&#27604;&#20363;&#21457;&#29983;&#21644;&#22312;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#20013;&#30340;&#19968;&#33268;&#24615;&#20998;&#24067;&#23545;&#20854;&#36827;&#34892;&#25490;&#21517;&#65292;&#25214;&#21040;&#19982;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#26174;&#33879;&#30456;&#20851;&#30340;&#35805;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#28041;&#21450;&#34917;&#20805;&#33829;&#20859;&#21270;&#21512;&#29289;&#26159;&#21542;&#26174;&#33879;&#26377;&#30410;&#20110;&#40644;&#26001;&#21464;&#24615;(MD)&#30340;&#24191;&#27867;&#33539;&#22260;&#30340;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#30830;&#23450;&#20102;&#20843;&#31181;&#21270;&#21512;&#29289;&#19982;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#30340;&#29305;&#23450;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic modeling and text mining are subsets of Natural Language Processing with relevance for conducting meta-analysis (MA) and systematic review (SR). For evidence synthesis, the above NLP methods are conventionally used for topic-specific literature searches or extracting values from reports to automate essential phases of SR and MA. Instead, this work proposes a comparative topic modeling approach to analyze reports of contradictory results on the same general research question. Specifically, the objective is to find topics exhibiting distinct associations with significant results for an outcome of interest by ranking them according to their proportional occurrence and consistency of distribution across reports of significant results. The proposed method was tested on broad-scope studies addressing whether supplemental nutritional compounds significantly benefit macular degeneration (MD). Eight compounds were identified as having a particular association with reports of significant r
&lt;/p&gt;</description></item><item><title>VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2308.06595</link><description>&lt;p&gt;
VisIT-Bench: &#19968;&#20010;&#21463;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#21551;&#21457;&#30340;&#35270;&#35273;&#35821;&#35328;&#25351;&#31034;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06595
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;VisIT-Bench&#65288;Visual InsTruction Benchmark&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20215;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#31034;&#36981;&#24490;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#36215;&#28857;&#26159;&#31574;&#21010;&#20102;70&#20010;&#8220;&#25351;&#31034;&#23478;&#26063;&#8221;&#65292;&#25105;&#20204;&#35748;&#20026;&#25351;&#31034;&#35843;&#20248;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#35299;&#20915;&#36825;&#20123;&#23478;&#26063;&#12290;&#20219;&#21153;&#19981;&#20165;&#38480;&#20110;VQAv2&#21644;COCO&#31561;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20174;&#22522;&#26412;&#35782;&#21035;&#21040;&#28216;&#25103;&#29609;&#27861;&#21644;&#21019;&#36896;&#24615;&#29983;&#25104;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#22312;&#31574;&#21010;&#20043;&#21518;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;592&#20010;&#27979;&#35797;&#26597;&#35810;&#65292;&#27599;&#20010;&#26597;&#35810;&#37117;&#24102;&#26377;&#19968;&#20010;&#20154;&#24037;&#32534;&#20889;&#30340;&#25351;&#31034;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#12290;&#36825;&#20123;&#25551;&#36848;&#23637;&#29616;&#20102;&#29305;&#23450;&#25351;&#31034;&#22240;&#32032;&#65292;&#20363;&#22914;&#23545;&#20110;&#35810;&#38382;&#24215;&#38754;&#23545;&#20110;&#36718;&#26885;&#29992;&#25143;&#30340;&#26131;&#35775;&#38382;&#24615;&#30340;&#25351;&#31034;&#65292;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#25551;&#36848;&#20102;&#26012;&#22369;/&#28508;&#22312;&#38556;&#30861;&#29289;&#12290;&#36825;&#20123;&#25551;&#36848;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#65306;1&#65289;&#25910;&#38598;&#27599;&#20010;&#23454;&#20363;&#30340;&#20154;&#24037;&#39564;&#35777;&#30340;&#21442;&#32771;&#36755;&#20986;&#65307;2&#65289;&#20351;&#29992;&#20165;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20505;&#36873;&#22810;&#27169;&#24577;&#29983;&#25104;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 'instruction families' that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;InteractiveIE&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#21160;&#38382;&#31572;&#29983;&#25104;&#27861;&#26469;&#24341;&#20986;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#27169;&#26495;&#25554;&#27133;&#65292;&#36890;&#36807;&#24494;&#23567;&#37327;&#20195;&#29702;&#20154;&#31867;&#30417;&#30563;&#65292;&#25552;&#39640;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#27861;&#24459;&#25991;&#26723;&#31561;&#39046;&#22495;&#20013;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14659</link><description>&lt;p&gt;
InteractiveIE&#65306;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#24378;&#24230;&#65292;&#25552;&#39640;&#20449;&#24687;&#25552;&#21462;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration in Improving the Performance of Information Extraction. (arXiv:2305.14659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;InteractiveIE&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#21160;&#38382;&#31572;&#29983;&#25104;&#27861;&#26469;&#24341;&#20986;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#27169;&#26495;&#25554;&#27133;&#65292;&#36890;&#36807;&#24494;&#23567;&#37327;&#20195;&#29702;&#20154;&#31867;&#30417;&#30563;&#65292;&#25552;&#39640;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#27861;&#24459;&#25991;&#26723;&#31561;&#39046;&#22495;&#20013;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#27169;&#26495;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#26159;&#19968;&#39033;&#20851;&#38190;&#19988;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#22522;&#20110;&#27169;&#26495;&#30340;&#20449;&#24687;&#25552;&#21462;&#26041;&#27861;&#20551;&#23450;&#20855;&#26377;&#39046;&#22495;&#27169;&#26495;&#30340;&#20808;&#39564;&#30693;&#35782;&#65307;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#20449;&#24687;&#25552;&#21462;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#27169;&#24335;&#65292;&#24182;&#19988;&#26159;&#19968;&#20010;&#21363;&#20852;&#21019;&#36896;&#30340;&#29616;&#35937;&#12290;&#20026;&#20102;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#24555;&#36895;&#24341;&#23548;&#27169;&#26495;&#65292;&#25105;&#20204;&#38656;&#35201;&#20174;&#25991;&#26723;&#20013;&#22312;&#38646;&#25110;&#26368;&#23567;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#24341;&#20986;&#27169;&#26495;&#25554;&#27133;&#12290;&#30001;&#20110;&#38382;&#31572;&#30340;&#30446;&#30340;&#19982;&#20449;&#24687;&#25552;&#21462;&#30340;&#30446;&#26631;&#20132;&#27719;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#38382;&#31572;&#29983;&#25104;&#20174;&#25991;&#26723;&#20013;&#24341;&#20986;&#27169;&#26495;&#25554;&#27133;&#65292;&#24182;&#30740;&#31350;&#24494;&#23567;&#37327;&#30340;&#20195;&#29702;&#20154;&#31867;&#30417;&#30563;&#65288;&#31216;&#20026;InteractiveIE&#65289;&#22914;&#20309;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#33719;&#21462;&#35757;&#32451;&#25968;&#25454;&#26114;&#36149;&#30340;&#29983;&#29289;&#21307;&#23398;&#21644;&#27861;&#24459;&#25991;&#26723;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#25581;&#31034;&#20102;&#20351;&#29992;InteractiveIE&#30456;&#23545;&#20110;&#20165;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#40723;&#33310;&#20154;&#24515;&#30340;&#24615;&#33021;&#25552;&#21319;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning template based information extraction from documents is a crucial yet difficult task. Prior template-based IE approaches assume foreknowledge of the domain templates; however, real-world IE do not have pre-defined schemas and it is a figure-out-as you go phenomena. To quickly bootstrap templates in a real-world setting, we need to induce template slots from documents with zero or minimal supervision. Since the purpose of question answering intersect with the goal of information extraction, we use automatic question generation to induce template slots from the documents and investigate how a tiny amount of a proxy human-supervision on-the-fly (termed as InteractiveIE) can further boost the performance. Extensive experiments on biomedical and legal documents, where obtaining training data is expensive, reveal encouraging trends of performance improvement using InteractiveIE over AI-only baseline.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HiCatGLR&#20219;&#21153;&#65292;&#33268;&#21147;&#20110;&#20026;&#25991;&#29486;&#32508;&#36848;&#29983;&#25104;&#20998;&#23618;&#30446;&#24405;&#65292;&#23427;&#21487;&#20197;&#20174;&#22810;&#31687;&#35770;&#25991;&#20013;&#25552;&#21462;&#21644;&#32452;&#32455;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#32570;&#23569;&#28165;&#26224;&#36923;&#36753;&#23618;&#27425;&#32467;&#26500;&#27010;&#36848;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#21019;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27492;&#39033;&#30740;&#31350;&#65292;&#21487;&#20197;&#26356;&#21152;&#20934;&#30830;&#22320;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#24182;&#39564;&#35777;&#25968;&#25454;&#38598;&#30340;&#39640;&#36136;&#37327;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03512</link><description>&lt;p&gt;
&#25991;&#29486;&#32508;&#36848;&#30340;&#20998;&#23618;&#30446;&#24405;&#29983;&#25104;&#65306;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Catalogue Generation for Literature Review: A Benchmark. (arXiv:2304.03512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03512
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HiCatGLR&#20219;&#21153;&#65292;&#33268;&#21147;&#20110;&#20026;&#25991;&#29486;&#32508;&#36848;&#29983;&#25104;&#20998;&#23618;&#30446;&#24405;&#65292;&#23427;&#21487;&#20197;&#20174;&#22810;&#31687;&#35770;&#25991;&#20013;&#25552;&#21462;&#21644;&#32452;&#32455;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#32570;&#23569;&#28165;&#26224;&#36923;&#36753;&#23618;&#27425;&#32467;&#26500;&#27010;&#36848;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#21019;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27492;&#39033;&#30740;&#31350;&#65292;&#21487;&#20197;&#26356;&#21152;&#20934;&#30830;&#22320;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#24182;&#39564;&#35777;&#25968;&#25454;&#38598;&#30340;&#39640;&#36136;&#37327;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25991;&#26723;&#31185;&#23398;&#25688;&#35201;&#21487;&#20197;&#20174;&#22823;&#37327;&#30340;&#35770;&#25991;&#20013;&#25552;&#21462;&#21644;&#32452;&#32455;&#37325;&#35201;&#20449;&#24687;&#65292;&#26368;&#36817;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20135;&#29983;&#32570;&#20047;&#28165;&#26224;&#21644;&#36923;&#36753;&#23618;&#27425;&#32467;&#26500;&#30340;&#20887;&#38271;&#27010;&#36848;&#19978;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Hierarchical Catalogue Generation for Literature Review (HiCatGLR)&#8221;&#30340;&#21407;&#23376;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#26681;&#25454;&#21508;&#31181;&#21442;&#32771;&#25991;&#29486;&#20026;&#32508;&#36848;&#35770;&#25991;&#29983;&#25104;&#20998;&#23618;&#30446;&#24405;&#12290;&#25105;&#20204;&#31934;&#24515;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#33521;&#25991;&#25991;&#29486;&#32508;&#36848;&#20998;&#23618;&#30446;&#24405;&#25968;&#25454;&#38598;(HiCaD)&#65292;&#20854;&#20013;&#21253;&#21547;13.8k&#31687;&#25991;&#29486;&#32508;&#36848;&#30446;&#24405;&#21644;120k&#31687;&#21442;&#32771;&#35770;&#25991;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#21644;&#27969;&#27700;&#32447;&#26041;&#27861;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20174;&#35821;&#20041;&#21644;&#32467;&#26500;&#19978;&#19982;&#21442;&#32771;&#26631;&#20934;&#30456;&#20284;&#24230;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24191;&#27867;&#20998;&#26512;&#39564;&#35777;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#39640;&#36136;&#37327;&#21644;&#25105;&#20204;&#35780;&#20272;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-document scientific summarization can extract and organize important information from an abundant collection of papers, arousing widespread attention recently. However, existing efforts focus on producing lengthy overviews lacking a clear and logical hierarchy. To alleviate this problem, we present an atomic and challenging task named Hierarchical Catalogue Generation for Literature Review (HiCatGLR), which aims to generate a hierarchical catalogue for a review paper given various references. We carefully construct a novel English Hierarchical Catalogues of Literature Reviews Dataset (HiCaD) with 13.8k literature review catalogues and 120k reference papers, where we benchmark diverse experiments via the end-to-end and pipeline methods. To accurately assess the model performance, we design evaluation metrics for similarity to ground truth from semantics and structure. Besides, our extensive analyses verify the high quality of our dataset and the effectiveness of our evaluation met
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24212;&#29992;&#20256;&#32479;&#38889;&#21307;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#24212;&#29992;&#38889;&#22269;&#22269;&#23478;&#20013;&#21307;&#21307;&#29983;&#25191;&#29031;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;57.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.17807</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20256;&#32479;&#38889;&#21307;&#20013;&#30340;&#28508;&#21147;&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#25991;&#21270;&#36866;&#24212;&#20445;&#20581;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language models in Traditional Korean Medicine: A Foundation Model Approach to Culturally-Adapted Healthcare. (arXiv:2303.17807v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24212;&#29992;&#20256;&#32479;&#38889;&#21307;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#24212;&#29992;&#38889;&#22269;&#22269;&#23478;&#20013;&#21307;&#21307;&#29983;&#25191;&#29031;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;57.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#38889;&#21307;&#27880;&#37325;&#20010;&#20307;&#21270;&#35786;&#26029;&#21644;&#27835;&#30103;&#65292;&#25968;&#25454;&#26377;&#38480;&#19988;&#36807;&#31243;&#38544;&#24615;&#65292;&#20351;AI&#24314;&#27169;&#22256;&#38590;&#12290;GPT-3.5&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23613;&#31649;&#32570;&#20047;&#21307;&#23398;&#19987;&#19994;&#22521;&#35757;&#65292;&#20294;&#24050;&#26174;&#31034;&#20986;&#20986;&#33394;&#30340;&#21307;&#30103;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;GPT-3.5&#21644;GPT-4&#22312;&#24212;&#29992;&#38889;&#22269;&#22269;&#23478;&#20013;&#21307;&#21307;&#29983;&#25191;&#29031;&#32771;&#35797;&#20013;&#30340;&#28508;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#21462;&#24471;&#20102;42.06%&#21644;57.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#20854;&#20013;GPT-4&#25509;&#36817;&#21450;&#26684;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduction: Traditional Korean medicine (TKM) emphasizes individualized diagnosis and treatment, making AI modeling difficult due to limited data and implicit processes. GPT-3.5 and GPT-4, large language models, have shown impressive medical knowledge despite lacking medicine-specific training. This study aimed to assess the capabilities of GPT-3.5 and GPT-4 for TKM using the Korean National Licensing Examination for Korean Medicine Doctors. Methods: GPT-3.5 (February 2023) and GPT-4 (March 2023) models answered 340 questions from the 2022 examination across 12 subjects. Each question was independently evaluated five times in an initialized session. Results: GPT-3.5 and GPT-4 achieved 42.06% and 57.29% accuracy, respectively, with GPT-4 nearing passing performance. There were significant differences in accuracy by subjects, with 83.75% accuracy for neuropsychiatry compared to 28.75% for internal medicine (2). Both models showed high accuracy in recall-based and diagnosis-based questi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17491</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#35745;&#31639;&#26426;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Language Models can Solve Computer Tasks. (arXiv:2303.17491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#22312;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#36890;&#29992;&#20219;&#21153;&#30340;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#21270;&#37325;&#22797;&#20219;&#21153;&#21644;&#21327;&#21161;&#22797;&#26434;&#38382;&#39064;&#30340;&#35299;&#20915;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#29983;&#20135;&#21147;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20195;&#29702;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#35299;&#20915;&#26032;&#30340;&#35745;&#31639;&#26426;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#19987;&#23478;&#31034;&#33539;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#26032;&#20219;&#21153;&#26469;&#35828;&#37117;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#65288;RCI&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#24182;&#22312;&#25209;&#35780;&#21644;&#25913;&#36827;&#36755;&#20986;&#30340;&#36807;&#31243;&#20013;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;RCI&#26041;&#27861;&#22312;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#20219;&#21153;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;LLM&#26041;&#27861;&#65292;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;RCI&#26041;&#27861;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#20165;&#26377;&#30340;&#23569;&#25968;&#31034;&#33539;&#65292;&#19982;&#26368;&#26032;&#30340;SL+RL&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent recursively criticizes and improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. RCI is competitive with the state-of-the-art SL+RL method, using only a handful of demonstrations per ta
&lt;/p&gt;</description></item></channel></rss>