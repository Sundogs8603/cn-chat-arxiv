<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02491</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Measuring Social Norms of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02491
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#20197;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35201;&#27714;&#20855;&#26377;&#35299;&#20915;&#31038;&#20250;&#35268;&#33539;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#26368;&#22823;&#30340;&#31038;&#20250;&#35268;&#33539;&#25216;&#33021;&#38598;&#65292;&#21253;&#25324;402&#39033;&#25216;&#33021;&#21644;12,383&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#35266;&#28857;&#21644;&#35770;&#28857;&#21040;&#25991;&#21270;&#21644;&#27861;&#24459;&#31561;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#29702;&#35299;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#19982;&#23567;&#23398;&#29983;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20135;&#29983;&#20960;&#20046;&#38543;&#26426;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5-Turbo&#21644;LLaMA2-Chat&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20165;&#30053;&#20302;&#20110;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
&lt;/p&gt;</description></item><item><title>MineLand&#27169;&#25311;&#22120;&#24341;&#20837;&#26377;&#38480;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#65292;&#25903;&#25345;&#22810;&#26234;&#33021;&#20307;&#22312;&#21327;&#20316;&#20013;&#22635;&#34917;&#20102;&#20449;&#24687;&#21644;&#21151;&#33021;&#38480;&#21046;&#30340;&#31354;&#30333;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#20855;&#21160;&#24577;&#21644;&#26377;&#25928;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2403.19267</link><description>&lt;p&gt;
MineLand&#65306;&#27169;&#25311;&#20855;&#26377;&#26377;&#38480;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#30340;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19267
&lt;/p&gt;
&lt;p&gt;
MineLand&#27169;&#25311;&#22120;&#24341;&#20837;&#26377;&#38480;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#65292;&#25903;&#25345;&#22810;&#26234;&#33021;&#20307;&#22312;&#21327;&#20316;&#20013;&#22635;&#34917;&#20102;&#20449;&#24687;&#21644;&#21151;&#33021;&#38480;&#21046;&#30340;&#31354;&#30333;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#20855;&#21160;&#24577;&#21644;&#26377;&#25928;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#20223;&#30495;&#22120;&#36890;&#24120;&#20551;&#35774;&#25317;&#26377;&#23436;&#32654;&#20449;&#24687;&#21644;&#26080;&#38480;&#21151;&#33021;&#65292;&#36825;&#38480;&#21046;&#20102;&#31038;&#20250;&#20114;&#21160;&#30340;&#29983;&#24577;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;Minecraft&#27169;&#25311;&#22120;MineLand&#65292;&#36890;&#36807;&#24341;&#20837;&#26377;&#38480;&#30340;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#20223;&#30495;&#22120;&#25903;&#25345;&#26368;&#22810;48&#20010;&#20855;&#26377;&#26377;&#38480;&#35270;&#35273;&#12289;&#21548;&#35273;&#21644;&#29615;&#22659;&#24847;&#35782;&#30340;&#26234;&#33021;&#20307;&#65292;&#36843;&#20351;&#23427;&#20204;&#31215;&#26497;&#27807;&#36890;&#21644;&#21327;&#20316;&#20197;&#28385;&#36275;&#39135;&#29289;&#21644;&#36164;&#28304;&#31561;&#29983;&#29702;&#38656;&#27714;&#12290;&#36825;&#20419;&#36827;&#20102;&#21160;&#24577;&#21644;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#20010;&#28789;&#24863;&#26469;&#33258;&#22810;&#20219;&#21153;&#22788;&#29702;&#29702;&#35770;&#30340;AI&#26234;&#33021;&#20307;&#26694;&#26550;Alex&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21327;&#35843;&#21644;&#35843;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#25311;&#22120;&#12289;&#30456;&#24212;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;AI&#26234;&#33021;&#20307;&#26694;&#26550;&#26377;&#21161;&#20110;&#26356;&#20855;&#29983;&#24577;&#21644;&#32454;&#33268;&#30340;&#38598;&#20307;&#34892;&#20026;&#12290;MineLand&#21644;Alex&#30340;&#28304;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/c&#20013;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19267v1 Announce Type: cross  Abstract: Conventional multi-agent simulators often assume perfect information and limitless capabilities, hindering the ecological validity of social interactions. We propose a multi-agent Minecraft simulator, MineLand, that bridges this gap by introducing limited multimodal senses and physical needs. Our simulator supports up to 48 agents with limited visual, auditory, and environmental awareness, forcing them to actively communicate and collaborate to fulfill physical needs like food and resources. This fosters dynamic and valid multi-agent interactions. We further introduce an AI agent framework, Alex, inspired by multitasking theory, enabling agents to handle intricate coordination and scheduling. Our experiments demonstrate that the simulator, the corresponding benchmark, and the AI agent framework contribute to more ecological and nuanced collective behavior. The source code of MineLand and Alex is openly available at https://github.com/c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19135</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#21270;&#19981;&#37325;&#35201;&#30340;&#23618;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compressing Large Language Models by Streamlining the Unimportant Layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19135
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#20294;&#20854;&#36866;&#29992;&#24615;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20851;&#27880;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#30340;&#32039;&#20945;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LLM&#30340;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#25200;&#21160;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#20986;&#19981;&#37027;&#20040;&#37325;&#35201;&#30340;&#23618;&#12290;&#22522;&#20110;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM-Streamline&#65292;&#21253;&#25324;&#20004;&#37096;&#20998;&#65306;&#23618;&#21098;&#26525;&#65292;&#26681;&#25454;&#30446;&#26631;&#31232;&#30095;&#24230;&#31227;&#38500;&#27169;&#22411;&#20013;&#19968;&#32452;&#36830;&#32493;&#30340;&#26368;&#19981;&#37325;&#35201;&#30340;&#23618;&#65307;&#23618;&#26367;&#25442;&#65292;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#26469;&#26367;&#25442;&#34987;&#21098;&#26525;&#30340;&#23618;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#21098;&#26525;&#36896;&#25104;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#21644;&#19968;&#20010;transformer&#23618;&#31561;&#32467;&#26500;&#20316;&#20026;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19135v1 Announce Type: cross  Abstract: Large language models (LLM) have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the large number of parameters of the models. Consequently, there is an increasing emphasis on compact models that exhibit high performance. In this study, we observe that different layers in LLM have varying degrees of perturbation on the hidden states, which allows us to identify less important layers. Based on this phenomenon, we propose LLM-Streamline, which consists of two parts: layer pruning, where we remove a set of consecutive layers with the lowest importance in the model according to the target sparsity; and layer replacement, where we train a lightweight model to substitute the pruned layers, thereby mitigating the performance degradation caused by pruning. In our experiments, we utilize structures such as a multi-layer perceptron (MLP) and a transformer layer as lightweight mode
&lt;/p&gt;</description></item><item><title>&#22810;&#39033;&#36873;&#25321;&#39064;&#34429;&#28982;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#22312;&#27979;&#35797;LLMs&#33021;&#21147;&#26102;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#38271;&#31687;&#29983;&#25104;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#21452;&#35821;MCQs&#20013;&#34920;&#29616;&#20986;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17752</link><description>&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#26159;&#21542;&#30495;&#30340;&#33021;&#22815;&#26816;&#27979;LLMs&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can multiple-choice questions really be useful in detecting the abilities of LLMs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17752
&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#34429;&#28982;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#22312;&#27979;&#35797;LLMs&#33021;&#21147;&#26102;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#38271;&#31687;&#29983;&#25104;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#21452;&#35821;MCQs&#20013;&#34920;&#29616;&#20986;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;(MCQs)&#30001;&#20110;&#20854;&#31616;&#21333;&#21644;&#39640;&#25928;&#32780;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#20110;MCQs&#26159;&#21542;&#33021;&#30495;&#27491;&#34913;&#37327;LLMs&#30340;&#33021;&#21147;&#23384;&#22312;&#30097;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#38271;&#31687;&#29983;&#25104;(LFG)&#31572;&#26696;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#22330;&#26223;&#20013;&#12290;&#20219;&#21153;&#19982;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38656;&#35201;&#23545;MCQ&#30340;&#25928;&#29992;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#32780;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36890;&#36807;&#35780;&#20272;&#20004;&#31181;&#35821;&#35328;&#65288;&#20013;&#25991;&#21644;&#33521;&#25991;&#65289;&#30340;&#22235;&#20010;&#38382;&#31572;(QA)&#25968;&#25454;&#38598;&#19978;&#30340;&#20061;&#20010;LLMs&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;LLMs&#22312;&#21452;&#35821;MCQs&#20013;&#34920;&#29616;&#20986;&#19968;&#31181;&#39034;&#24207;&#25935;&#24863;&#24615;&#65292;&#20559;&#21521;&#20110;&#20301;&#20110;&#29305;&#23450;&#20301;&#32622;&#30340;&#31572;&#26696;&#65292;&#21363;&#31532;&#19968;&#20010;&#20301;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#30452;&#25509;&#36755;&#20986;&#12289;token logit&#21644;&#23884;&#20837;&#26469;&#37327;&#21270;MCQs&#21644;&#38271;&#31687;&#29983;&#25104;&#38382;&#39064;(LFGQs)&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;MCQs&#21644;&#38271;&#31687;&#29983;&#25104;&#30340;&#31572;&#26696;&#20043;&#38388;&#23384;&#22312;&#30456;&#23545;&#36739;&#20302;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17752v1 Announce Type: new  Abstract: Multiple-choice questions (MCQs) are widely used in the evaluation of large language models (LLMs) due to their simplicity and efficiency. However, there are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required. The misalignment between the task and the evaluation method demands a thoughtful analysis of MCQ's efficacy, which we undertake in this paper by evaluating nine LLMs on four question-answering (QA) datasets in two languages: Chinese and English. We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position. We further quantify the gap between MCQs and long-form generation questions (LFGQs) by comparing their direct outputs, token logits, and embeddings. Our results reveal a relatively low correlation between answers from MC
&lt;/p&gt;</description></item><item><title>PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15388</link><description>&lt;p&gt;
LLaVA-PruMerge: &#33258;&#36866;&#24212;&#20196;&#29260;&#20943;&#23569;&#29992;&#20110;&#39640;&#25928;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15388
&lt;/p&gt;
&lt;p&gt;
PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#36890;&#36807;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;LMMs&#21253;&#25324;&#20102;&#26356;&#22797;&#26434;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22914;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#20196;&#29260;&#20943;&#23569;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#31867;&#20284;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#35768;&#22810;&#35270;&#35273;&#20196;&#29260;&#22312;&#31354;&#38388;&#19978;&#26159;&#20887;&#20313;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PruMerge&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21487;&#27604;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15388v1 Announce Type: cross  Abstract: Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#37197;&#32622;&#65292;&#31216;&#20026;prompt-in-decoder&#65288;PiD&#65289;&#65292;&#21487;&#20197;&#19968;&#27425;&#32534;&#30721;&#36755;&#20837;&#24182;&#24182;&#34892;&#35299;&#30721;&#36755;&#20986;&#65292;&#22312;&#32467;&#26500;&#21270;&#36755;&#20986;&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#39640;&#25928;&#29575;&#65292;&#36991;&#20813;&#20102;&#37325;&#22797;&#36755;&#20837;&#32534;&#30721;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#35299;&#30721;&#22120;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.13112</link><description>&lt;p&gt;
&#19968;&#27425;&#32534;&#30721;&#65292;&#22810;&#27425;&#24182;&#34892;&#35299;&#30721;&#65306;&#39640;&#25928;Transformer&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Encode Once and Decode in Parallel: Efficient Transformer Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13112
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#37197;&#32622;&#65292;&#31216;&#20026;prompt-in-decoder&#65288;PiD&#65289;&#65292;&#21487;&#20197;&#19968;&#27425;&#32534;&#30721;&#36755;&#20837;&#24182;&#24182;&#34892;&#35299;&#30721;&#36755;&#20986;&#65292;&#22312;&#32467;&#26500;&#21270;&#36755;&#20986;&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#39640;&#25928;&#29575;&#65292;&#36991;&#20813;&#20102;&#37325;&#22797;&#36755;&#20837;&#32534;&#30721;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#35299;&#30721;&#22120;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21151;&#33021;&#24378;&#22823;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#38480;&#21046;&#20102;&#37096;&#32626;&#22330;&#26223;&#12290;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#24494;&#35843;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#22791;&#21463;&#38738;&#30544;&#65292;&#21487;&#20197;&#32988;&#36807;&#26356;&#22823;&#26356;&#36890;&#29992;&#30340;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#20363;&#22914;GPT-4&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#37197;&#32622;&#65292;&#21487;&#20197;&#25552;&#39640;&#22312;&#32467;&#26500;&#21270;&#36755;&#20986;&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#38656;&#35201;&#20174;&#21333;&#20010;&#36755;&#20837;&#20013;&#20135;&#29983;&#22810;&#20010;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;prompt-in-decoder&#65288;PiD&#65289;&#65292;&#21482;&#23545;&#36755;&#20837;&#36827;&#34892;&#19968;&#27425;&#32534;&#30721;&#65292;&#24182;&#19988;&#24182;&#34892;&#35299;&#30721;&#36755;&#20986;&#65292;&#36890;&#36807;&#36991;&#20813;&#37325;&#22797;&#36755;&#20837;&#32534;&#30721;&#65292;&#20174;&#32780;&#20943;&#23569;&#35299;&#30721;&#22120;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#25552;&#21319;&#20102;&#35757;&#32451;&#21644;&#25512;&#26029;&#25928;&#29575;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#35745;&#31639;&#20943;&#23569;&#65292;&#22823;&#33268;&#38543;&#23376;&#20219;&#21153;&#25968;&#37327;&#22686;&#21152;&#32780;&#25193;&#23637;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#12289;&#25688;&#35201;&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#33719;&#24471;&#39640;&#36798;4.6&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#19988;&#24615;&#33021;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;/&#25512;&#26029;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13112v1 Announce Type: new  Abstract: Transformer-based NLP models are powerful but have high computational costs that limit deployment scenarios. Finetuned encoder-decoder models are popular in specialized domains and can outperform larger more generalized decoder-only models, such as GPT-4. We introduce a new configuration for encoder-decoder models that improves efficiency on structured output and question-answering tasks where multiple outputs are required of a single input. Our method, prompt-in-decoder (PiD), encodes the input once and decodes output in parallel, boosting both training and inference efficiency by avoiding duplicate input encoding, thereby reducing the decoder's memory footprint. We achieve computation reduction that roughly scales with the number of subtasks, gaining up to 4.6x speed-up over state-of-the-art models for dialogue state tracking, summarization, and question-answering tasks with comparable or better performance. We release our training/inf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#65292;&#20026;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13002</link><description>&lt;p&gt;
AutoTRIZ&#65306;&#21033;&#29992;TRIZ&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#21019;&#24847;
&lt;/p&gt;
&lt;p&gt;
AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#65292;&#20026;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21644;&#21019;&#26032;&#32773;&#22312;&#24320;&#21457;&#24605;&#32500;&#26041;&#27861;&#26041;&#38754;&#20570;&#20986;&#20102;&#24040;&#22823;&#21162;&#21147;&#65292;&#27604;&#22914;&#24418;&#24577;&#20998;&#26512;&#21644;&#31867;&#27604;&#35774;&#35745;&#65292;&#20197;&#36741;&#21161;&#24037;&#31243;&#35774;&#35745;&#21019;&#24847;&#65292;&#35299;&#20915;&#38382;&#39064;&#21644;&#25512;&#21160;&#21019;&#26032;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;TRIZ&#20316;&#20026;&#26368;&#33879;&#21517;&#30340;&#26041;&#27861;&#33073;&#39062;&#32780;&#20986;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31995;&#32479;&#21270;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;TRIZ&#36164;&#28304;&#21644;&#27010;&#24565;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#20854;&#23545;&#29992;&#25143;&#30693;&#35782;&#12289;&#32463;&#39564;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#20381;&#36182;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#24191;&#27867;&#30693;&#35782;&#21644;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#65292;AutoTRIZ&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#30683;&#30462;&#26816;&#27979;&#21644;&#27604;&#36739;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#23454;&#39564;&#26469;&#35777;&#26126;&#24182;&#35780;&#20272;AutoTRIZ&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13002v1 Announce Type: cross  Abstract: Researchers and innovators have made enormous efforts in developing ideation methods, such as morphological analysis and design-by-analogy, to aid engineering design ideation for problem solving and innovation. Among these, TRIZ stands out as the most well-known approach, widely applied for systematic innovation. However, the complexity of TRIZ resources and concepts, coupled with its reliance on users' knowledge, experience, and reasoning capabilities, limits its practicability. This paper proposes AutoTRIZ, an artificial ideation tool that leverages large language models (LLMs) to automate and enhance the TRIZ methodology. By leveraging the broad knowledge and advanced reasoning capabilities of LLMs, AutoTRIZ offers a novel approach to design automation and interpretable ideation with artificial intelligence. We demonstrate and evaluate the effectiveness of AutoTRIZ through consistency experiments in contradiction detection and compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#21363;&#26102;&#24615;&#32452;&#32455;&#32467;&#26500;&#24378;&#21152;&#22312;LLM&#20195;&#29702;&#19978;&#65292;&#20197;&#20419;&#36827;&#22810;&#20195;&#29702;&#31995;&#32479;&#20869;&#30340;&#21512;&#20316;&#65292;&#22312;&#20855;&#36523;LLM&#20195;&#29702;&#21644;&#20154;-&#20195;&#29702;&#21512;&#20316;&#23454;&#39564;&#20013;&#21457;&#29616;&#25351;&#23450;&#39046;&#23548;&#23545;&#22242;&#38431;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#30340;&#39046;&#23548;&#32032;&#36136;&#21644;&#33258;&#21457;&#21512;&#20316;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.12482</link><description>&lt;p&gt;
&#20855;&#36523;LLM&#20195;&#29702;&#22312;&#32452;&#32455;&#22242;&#38431;&#20013;&#23398;&#20250;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Embodied LLM Agents Learn to Cooperate in Organized Teams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#21363;&#26102;&#24615;&#32452;&#32455;&#32467;&#26500;&#24378;&#21152;&#22312;LLM&#20195;&#29702;&#19978;&#65292;&#20197;&#20419;&#36827;&#22810;&#20195;&#29702;&#31995;&#32479;&#20869;&#30340;&#21512;&#20316;&#65292;&#22312;&#20855;&#36523;LLM&#20195;&#29702;&#21644;&#20154;-&#20195;&#29702;&#21512;&#20316;&#23454;&#39564;&#20013;&#21457;&#29616;&#25351;&#23450;&#39046;&#23548;&#23545;&#22242;&#38431;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#30340;&#39046;&#23548;&#32032;&#36136;&#21644;&#33258;&#21457;&#21512;&#20316;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#25512;&#29702;&#12289;&#35268;&#21010;&#21644;&#20915;&#31574;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#30340;&#29087;&#32451;&#24230;&#12290;LLMs&#22240;&#27492;&#22312;&#22810;&#20195;&#29702;&#31995;&#32479;&#20869;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20197;&#20419;&#36827;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;LLM&#20195;&#29702;&#24448;&#24448;&#20250;&#36807;&#24230;&#25253;&#21578;&#24182;&#36981;&#20174;&#20219;&#20309;&#25351;&#20196;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22810;&#20195;&#29702;&#21512;&#20316;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#28151;&#20081;&#12290;&#21463;&#20154;&#31867;&#32452;&#32455;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#21363;&#26102;&#24615;&#32452;&#32455;&#32467;&#26500;&#24378;&#21152;&#22312;LLM&#20195;&#29702;&#19978;&#65292;&#20197;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20855;&#36523;LLM&#20195;&#29702;&#21644;&#20154;-&#20195;&#29702;&#21512;&#20316;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#25351;&#23450;&#39046;&#23548;&#23545;&#22242;&#38431;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#23637;&#31034;&#30340;&#39046;&#23548;&#32032;&#36136;&#21644;&#20182;&#20204;&#30340;&#33258;&#21457;&#21512;&#20316;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12482v1 Announce Type: new  Abstract: Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs t
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#25552;&#31034;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#33391;&#65292;&#36824;&#21487;&#20197;&#19982;&#29616;&#26377;&#25552;&#31034;&#26041;&#27861;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08211</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#23545;&#27604;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Contrastive Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08211
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#25552;&#31034;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#33391;&#65292;&#36824;&#21487;&#20197;&#19982;&#29616;&#26377;&#25552;&#31034;&#26041;&#27861;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26041;&#27861;&#22312;&#22686;&#24378;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#23545;&#27604;&#25552;&#31034;&#65288;CP&#65289;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;LLMs&#25552;&#20379;&#31572;&#26696;&#20043;&#21069;&#28155;&#21152;"&#35753;&#25105;&#20204;&#32473;&#20986;&#19968;&#20010;&#27491;&#30830;&#31572;&#26696;&#21644;&#19968;&#20010;&#38169;&#35823;&#31572;&#26696;"&#26469;&#28436;&#31034;LLMs&#26159;&#20307;&#38754;&#30340;&#23545;&#27604;&#25512;&#29702;&#32773;&#12290;&#23545;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38646;&#36801;&#31227;&#23545;&#27604;&#25552;&#31034;&#25552;&#21319;&#20102;&#22312;&#19968;&#31995;&#21015;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#23569;&#37327;&#36801;&#31227;&#31034;&#20363;&#65292;&#27604;&#22914;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22312;GSM8K&#19978;&#30340;&#20934;&#30830;&#29575;&#20174;35.9%&#21040;88.8%&#20197;&#21450;AQUA-RAT&#20174;41.3%&#21040;62.2%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#22823;&#22810;&#25968;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#20013;&#32988;&#36807;&#38646;&#36801;&#31227;CoT&#21644;&#23569;&#37327;&#36801;&#31227;CoT&#65292;&#36824;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#26080;&#32541;&#25972;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#25913;&#36827;&#25110;&#32773;&#31454;&#20105;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08211v1 Announce Type: cross  Abstract: Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs). We explore how contrastive prompting (CP) significantly improves the ability of large language models to perform complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by simply adding "Let's give a correct and a wrong answer." before LLMs provide answers. Experiments on two large language models show that zero-shot contrastive prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks without any hand-crafted few-shot examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and commonsense reasoning tasks but also can seamlessly integrate with existing prompting methods, resulting in improved or comp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#21644;LLMs&#23545;&#40784;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#30693;&#35782;&#65292;&#20805;&#20998;&#37322;&#25918;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;</title><link>https://arxiv.org/abs/2403.07300</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#25511;&#21046;&#39044;&#35757;&#32451;LLMs&#36827;&#34892;&#24191;&#20041;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#21644;LLMs&#23545;&#40784;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#30693;&#35782;&#65292;&#20805;&#20998;&#37322;&#25918;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26368;&#36817;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24555;&#36895;&#22686;&#38271;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#26377;&#38480;&#30340;&#26102;&#38388;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28608;&#22686;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#23558;LLMs&#24341;&#20837;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30452;&#25509;&#23558;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#24573;&#30053;&#20102;&#26102;&#38388;&#21644;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#22266;&#26377;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#26102;&#38388;&#24207;&#21015;&#23545;&#40784;&#26694;&#26550;&#65292;&#31216;&#20026;LLaTA&#65292;&#20197;&#20805;&#20998;&#21457;&#25381;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25361;&#25112;&#20013;&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;LLMs&#20013;&#30340;&#36755;&#20837;&#26080;&#20851;&#38745;&#24577;&#30693;&#35782;&#21644;&#36755;&#20837;&#30456;&#20851;&#21160;&#24577;&#30693;&#35782;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#20026;&#39044;&#27979;&#27169;&#22411;&#36171;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07300v1 Announce Type: cross  Abstract: Multivariate time series forecasting has recently gained great success with the rapid growth of deep learning models. However, existing approaches usually train models from scratch using limited temporal data, preventing their generalization. Recently, with the surge of the Large Language Models (LLMs), several works have attempted to introduce LLMs into time series forecasting. Despite promising results, these methods directly take time series as the input to LLMs, ignoring the inherent modality gap between temporal and text data. In this work, we propose a novel Large Language Models and time series alignment framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time series forecasting challenge. Based on cross-modal knowledge distillation, the proposed method exploits both input-agnostic static knowledge and input-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers the forecasting model with f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.03506</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#28041;&#21450;&#24102;&#26377;&#26377;&#38480;&#36793;&#30028;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26816;&#27979;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#30740;&#31350;&#24212;&#35206;&#30422;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#29983;&#25104;&#30340;&#19981;&#21516;&#31867;&#22411;&#28151;&#21512;&#25991;&#26412;&#65292;&#20197;&#26356;&#22909;&#22320;&#25351;&#23548;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;CoAuthor&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#36890;&#36807;&#20154;&#31867;&#20316;&#32773;&#21644;&#26234;&#33021;&#20889;&#20316;&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#29983;&#25104;&#30340;&#22810;&#36718;&#20132;&#20114;&#20013;&#20135;&#29983;&#30340;&#22810;&#26679;&#21270;&#12289;&#30495;&#23454;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#27493;&#20998;&#21106;&#20026;&#22522;&#30784;&#30340;&#27969;&#31243;&#65306;(i)&#26816;&#27979;&#32473;&#23450;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#21508;&#20010;&#27573;&#33853;&#65292;&#20854;&#20013;&#27599;&#20010;&#27573;&#33853;&#21253;&#21547;&#19968;&#33268;&#20316;&#32773;&#30340;&#21477;&#23376;&#65292;&#20197;&#21450;(ii)&#20998;&#31867;&#27599;&#20010;&#30830;&#23450;&#27573;&#33853;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03506v1 Announce Type: cross  Abstract: This study explores the challenge of sentence-level AI-generated text detection within human-AI collaborative hybrid texts. Existing studies of AI-generated text detection for hybrid texts often rely on synthetic datasets. These typically involve hybrid texts with a limited number of boundaries. We contend that studies of detecting AI-generated content within hybrid texts should cover different types of hybrid texts generated in realistic settings to better inform real-world applications. Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid texts generated through the collaboration between human writers and an intelligent writing system in multi-turn interactions. We adopt a two-step, segmentation-based pipeline: (i) detect segments within a given hybrid text where each segment contains sentences of consistent authorship, and (ii) classify the authorship of each identified segment. Our empirical 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22240;&#26524;&#24341;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#21069;&#38376;&#35843;&#25972;&#26377;&#25928;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.02738</link><description>&lt;p&gt;
&#22240;&#26524;&#24341;&#23548;&#65306;&#22522;&#20110;&#21069;&#38376;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21551;&#21457;&#24335;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02738
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22240;&#26524;&#24341;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#21069;&#38376;&#35843;&#25972;&#26377;&#25928;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#26377;&#30340;&#35832;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#32500;&#38142;&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21551;&#21457;&#24335;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#21508;&#31181;&#20559;&#35265;&#25361;&#25112;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#21551;&#21457;&#24335;&#26041;&#27861;&#32972;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#38376;&#35843;&#25972;&#30340;&#26032;&#22411;&#22240;&#26524;&#24341;&#23548;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#20943;&#36731;LLMs&#30340;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#32780;&#26080;&#38656;&#35775;&#38382;LLMs&#30340;&#21442;&#25968;&#21644;logit&#26469;&#23454;&#26045;&#22240;&#26524;&#24178;&#39044;&#12290;&#30001;LLMs&#29983;&#25104;&#30340;&#24605;&#32500;&#38142;&#34987;&#29992;&#20316;&#20013;&#20171;&#21464;&#37327;&#65292;&#36890;&#36807;&#21069;&#38376;&#35745;&#31639;&#36755;&#20837;&#25552;&#31034;&#19982;&#36755;&#20986;&#31572;&#26696;&#20043;&#38388;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02738v1 Announce Type: new  Abstract: Despite the significant achievements of existing prompting methods such as in-context learning and chain-of-thought for large language models (LLMs), they still face challenges of various biases. Traditional debiasing methods primarily focus on the model training stage, including data augmentation-based and reweight-based approaches, with the limitations of addressing the complex biases of LLMs. To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate the bias of LLMs. In specific, causal intervention is implemented by designing the prompts without accessing the parameters and logits of LLMs.The chain-of-thoughts generated by LLMs are employed as the mediator variable and the causal effect between the input prompt and the output answers is calculated through front-do
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.17826</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Prediction-Powered Ranking of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#26681;&#25454;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#27700;&#24179;&#36827;&#34892;&#25490;&#21517;--&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#21463;&#20154;&#31867;&#20559;&#22909;&#65292;&#37027;&#20040;&#23427;&#23601;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#26469;&#24357;&#21512;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#21487;&#33021;&#24341;&#20837;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17826v1 Announce Type: cross  Abstract: Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provid
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;</title><link>https://arxiv.org/abs/2402.17205</link><description>&lt;p&gt;
&#27979;&#37327;&#31070;&#32463;&#27169;&#22411;&#30340;&#35270;&#35273;&#35821;&#35328;STEM&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Measuring Vision-Language STEM Skills of Neural Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#36890;&#24120;&#38656;&#35201;&#32467;&#21512;STEM&#65288;&#31185;&#23398;&#12289;&#25216;&#26415;&#12289;&#24037;&#31243;&#21644;&#25968;&#23398;&#65289;&#30693;&#35782;&#26469;&#35299;&#20915;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#25361;&#25112;&#24615;&#38382;&#39064;&#20013;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#23427;&#21253;&#25324;448&#39033;&#25216;&#33021;&#21644;1,073,146&#20010;&#36328;&#36234;&#25152;&#26377;STEM&#31185;&#30446;&#30340;&#38382;&#39064;&#12290;&#19982;&#36890;&#24120;&#20391;&#37325;&#20110;&#26816;&#39564;&#19987;&#23478;&#27700;&#24179;&#33021;&#21147;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#22522;&#30784;&#25216;&#33021;&#21644;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23558;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;CLIP&#21644;GPT-3.5-Turbo&#65292;&#28155;&#21152;&#21040;&#25105;&#20204;&#30340;&#22522;&#20934;&#20013;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#36817;&#30340;&#27169;&#22411;&#36827;&#23637;&#21482;&#26377;&#21161;&#20110;&#25484;&#25569;&#25968;&#25454;&#38598;&#20013;&#38750;&#24120;&#26377;&#38480;&#25968;&#37327;&#30340;&#20302;&#24180;&#32423;&#25216;&#33021;&#65288;&#19977;&#24180;&#32423;&#20013;&#30340;2.5%&#65289;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#36828;&#27809;&#26377;&#23436;&#20840;&#25484;&#25569;&#23398;&#21069;&#25945;&#32946;&#38454;&#27573;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17205v1 Announce Type: cross  Abstract: We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well bel
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#35268;&#27169;&#30417;&#27979;&#21644;&#20998;&#26512;GPT&#21830;&#24215;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#21270;&#24037;&#20855;&#26469;&#30740;&#31350;GPT&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#28431;&#27934;&#21644;&#25220;&#34989;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.15105</link><description>&lt;p&gt;
GPT&#24212;&#29992;&#30340;&#21021;&#25506;&#65306;&#26684;&#23616;&#19982;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
A First Look at GPT Apps: Landscape and Vulnerability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15105
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#35268;&#27169;&#30417;&#27979;&#21644;&#20998;&#26512;GPT&#21830;&#24215;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#21270;&#24037;&#20855;&#26469;&#30740;&#31350;GPT&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#28431;&#27934;&#21644;&#25220;&#34989;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#65292;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#24378;&#22823;&#30340;GPT&#36827;&#20837;&#24066;&#22330;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;LLM&#29983;&#24577;&#31995;&#32479;&#20173;&#28982;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;LLMs&#23545;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#24341;&#21457;&#20102;&#23545;&#23433;&#20840;&#24615;&#21644;&#25220;&#34989;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;GPT&#21830;&#24215;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30340;&#25506;&#32034;&#65292;&#26088;&#22312;&#30740;&#31350;GPT&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#28431;&#27934;&#21644;&#25220;&#34989;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25454;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#27425;&#22823;&#35268;&#27169;&#30417;&#27979;&#21644;&#20998;&#26512;&#65292;&#20998;&#21035;&#26159;&#19968;&#20010;&#38750;&#23448;&#26041;&#30340;GPTStore.AI&#21644;&#19968;&#20010;&#23448;&#26041;&#30340;OpenAI GPT Store&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;TriLevel GPT Reversing&#65288;T-GR&#65289;&#31574;&#30053;&#65292;&#29992;&#20110;&#25552;&#21462;GPT&#20869;&#37096;&#20449;&#24687;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23436;&#25104;&#36825;&#20004;&#39033;&#20219;&#21153;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#33258;&#21160;&#21270;&#24037;&#20855;&#65306;&#19968;&#20010;&#29992;&#20110;&#32593;&#32476;&#25235;&#21462;&#65292;&#21478;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#19982;GPT&#36827;&#34892;&#31243;&#24207;&#21270;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#23545;GPT&#20132;&#20114;&#21644;&#21019;&#24314;&#30340;&#24040;&#22823;&#28909;&#24773;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15105v1 Announce Type: cross  Abstract: With the advancement of Large Language Models (LLMs), increasingly sophisticated and powerful GPTs are entering the market. Despite their popularity, the LLM ecosystem still remains unexplored. Additionally, LLMs' susceptibility to attacks raises concerns over safety and plagiarism. Thus, in this work, we conduct a pioneering exploration of GPT stores, aiming to study vulnerabilities and plagiarism within GPT applications. To begin with, we conduct, to our knowledge, the first large-scale monitoring and analysis of two stores, an unofficial GPTStore.AI, and an official OpenAI GPT Store. Then, we propose a TriLevel GPT Reversing (T-GR) strategy for extracting GPT internals. To complete these two tasks efficiently, we develop two automated tools: one for web scraping and another designed for programmatically interacting with GPTs. Our findings reveal a significant enthusiasm among users and developers for GPT interaction and creation, as
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.14744</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22478;&#24066;&#23621;&#27665;&#65306;&#29992;&#20110;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#30340;LLM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38598;&#25104;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#28789;&#27963;&#39640;&#25928;&#30340;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#12290;LLMs&#36890;&#36807;&#39640;&#25928;&#22788;&#29702;&#35821;&#20041;&#25968;&#25454;&#24182;&#22312;&#24314;&#27169;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#20379;&#22810;&#21151;&#33021;&#24615;, &#20811;&#26381;&#20102;&#20197;&#24448;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#36843;&#20999;&#38656;&#27714;, &#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;: &#23558;LLMs&#19982;&#20016;&#23500;&#30340;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;, &#24320;&#21457;&#21487;&#38752;&#30340;&#27963;&#21160;&#29983;&#25104;&#31574;&#30053;, &#20197;&#21450;&#25506;&#32034;LLMs&#22312;&#22478;&#24066;&#31227;&#21160;&#20013;&#30340;&#24212;&#29992;&#12290;&#20854;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#20195;&#29702;&#26694;&#26550;, &#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#20010;&#20307;&#27963;&#21160;&#27169;&#24335;&#21644;&#21160;&#26426;, &#21253;&#25324;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#33258;&#27965;&#26041;&#27861;&#21644;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;, &#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14744v1 Announce Type: new  Abstract: This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This 
&lt;/p&gt;</description></item><item><title>KorNAT&#26159;&#39318;&#20010;&#29992;&#20110;&#35780;&#20272;&#38889;&#22269;&#22269;&#23478;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#31038;&#20250;&#20215;&#20540;&#35266;&#21644;&#24120;&#35782;&#23545;&#40784;&#20004;&#20010;&#26041;&#38754;&#65292;&#36890;&#36807;&#23545;&#31038;&#20250;&#20215;&#20540;&#21644;&#24120;&#35782;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.13605</link><description>&lt;p&gt;
KorNAT&#65306;&#38889;&#22269;&#31038;&#20250;&#20215;&#20540;&#35266;&#21644;&#24120;&#35782;&#30340;LLM&#23545;&#40784;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13605
&lt;/p&gt;
&lt;p&gt;
KorNAT&#26159;&#39318;&#20010;&#29992;&#20110;&#35780;&#20272;&#38889;&#22269;&#22269;&#23478;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#31038;&#20250;&#20215;&#20540;&#35266;&#21644;&#24120;&#35782;&#23545;&#40784;&#20004;&#20010;&#26041;&#38754;&#65292;&#36890;&#36807;&#23545;&#31038;&#20250;&#20215;&#20540;&#21644;&#24120;&#35782;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29305;&#23450;&#22269;&#23478;&#24471;&#20197;&#26377;&#25928;&#37096;&#32626;&#65292;&#23427;&#20204;&#24517;&#39035;&#20855;&#26377;&#23545;&#35813;&#22269;&#25991;&#21270;&#21644;&#22522;&#26412;&#30693;&#35782;&#30340;&#29702;&#35299;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22269;&#23478;&#23545;&#40784;&#65288;National Alignment&#65289;&#65292;&#20174;&#31038;&#20250;&#20215;&#20540;&#35266;&#23545;&#40784;&#21644;&#24120;&#35782;&#23545;&#40784;&#20004;&#20010;&#26041;&#38754;&#34913;&#37327;LLM&#19982;&#30446;&#26631;&#22269;&#23478;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#31038;&#20250;&#20215;&#20540;&#35266;&#23545;&#40784;&#35780;&#20272;&#27169;&#22411;&#23545;&#29305;&#23450;&#22269;&#23478;&#31038;&#20250;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#31243;&#24230;&#65292;&#32780;&#24120;&#35782;&#23545;&#40784;&#21017;&#26816;&#39564;&#27169;&#22411;&#23545;&#30456;&#20851;&#22522;&#26412;&#22269;&#23478;&#30693;&#35782;&#30340;&#25226;&#25569;&#24773;&#20917;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;KorNAT&#65292;&#36825;&#26159;&#39318;&#20010;&#34913;&#37327;&#19982;&#38889;&#22269;&#22269;&#23478;&#23545;&#40784;&#30340;&#22522;&#20934;&#12290;&#23545;&#20110;&#31038;&#20250;&#20215;&#20540;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;&#21253;&#25324;6174&#21517;&#38889;&#22269;&#21442;&#19982;&#32773;&#22312;&#20869;&#30340;&#22823;&#35268;&#27169;&#35843;&#26597;&#20013;&#33719;&#24471;&#20102;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#12290;&#23545;&#20110;&#24120;&#35782;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22522;&#20110;&#38889;&#22269;&#25945;&#31185;&#20070;&#21644;GED&#21442;&#32771;&#36164;&#26009;&#26500;&#24314;&#20102;&#26679;&#26412;&#12290;KorNAT&#21253;&#21547;4K&#21644;6K&#20010;&#38024;&#23545;&#31038;&#20250;&#20215;&#20540;&#21644;&#24120;&#35782;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13605v1 Announce Type: new  Abstract: For Large Language Models (LLMs) to be effectively deployed in a specific country, they must possess an understanding of the nation's culture and basic knowledge. To this end, we introduce National Alignment, which measures an alignment between an LLM and a targeted country from two aspects: social value alignment and common knowledge alignment. Social value alignment evaluates how well the model understands nation-specific social values, while common knowledge alignment examines how well the model captures basic knowledge related to the nation. We constructed KorNAT, the first benchmark that measures national alignment with South Korea. For the social value dataset, we obtained ground truth labels from a large-scale survey involving 6,174 unique Korean participants. For the common knowledge dataset, we constructed samples based on Korean textbooks and GED reference materials. KorNAT contains 4K and 6K multiple-choice questions for socia
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32479;&#19968;&#30340;&#22522;&#20110;&#20998;&#31867;&#23398;&#25351;&#23548;&#30340;&#25351;&#23548;&#35843;&#25972;&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29616;&#26377;&#20998;&#31867;&#23398;&#36827;&#34892;&#23454;&#20307;&#20851;&#31995;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#23454;&#20307;&#38598;&#25193;&#23637;&#12289;&#20998;&#31867;&#23398;&#25193;&#23637;&#21644;&#31181;&#23376;&#24341;&#23548;&#20998;&#31867;&#23398;&#26500;&#24314;&#19977;&#20010;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.13405</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20110;&#20998;&#31867;&#23398;&#25351;&#23548;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#21644;&#20998;&#31867;&#23398;&#25193;&#23637;&#30340;&#25351;&#23548;&#35843;&#25972;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13405
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32479;&#19968;&#30340;&#22522;&#20110;&#20998;&#31867;&#23398;&#25351;&#23548;&#30340;&#25351;&#23548;&#35843;&#25972;&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29616;&#26377;&#20998;&#31867;&#23398;&#36827;&#34892;&#23454;&#20307;&#20851;&#31995;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#23454;&#20307;&#38598;&#25193;&#23637;&#12289;&#20998;&#31867;&#23398;&#25193;&#23637;&#21644;&#31181;&#23376;&#24341;&#23548;&#20998;&#31867;&#23398;&#26500;&#24314;&#19977;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38598;&#25193;&#23637;&#12289;&#20998;&#31867;&#23398;&#25193;&#23637;&#21644;&#31181;&#23376;&#24341;&#23548;&#20998;&#31867;&#23398;&#26500;&#24314;&#26159;&#19977;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#21487;&#20197;&#29992;&#26469;&#33258;&#21160;&#21521;&#29616;&#26377;&#20998;&#31867;&#23398;&#22635;&#20805;&#26032;&#23454;&#20307;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#24322;&#36136;&#25216;&#26415;&#20998;&#21035;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#35270;&#35282;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20998;&#31867;&#23398;&#32467;&#26500;&#30340;&#35270;&#35282;&#30830;&#35748;&#20102;&#36825;&#20123;&#20219;&#21153;&#25152;&#38656;&#30340;&#20849;&#21516;&#20851;&#38190;&#25216;&#33021;&#8212;&#8212;&#25214;&#21040;&#8220;&#20804;&#24351;&#8221;&#21644;&#25214;&#21040;&#8220;&#29238;&#27597;&#8221;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20110;&#20998;&#31867;&#23398;&#25351;&#23548;&#30340;&#25351;&#23548;&#35843;&#25972;&#26694;&#26550;&#26469;&#20849;&#21516;&#35299;&#20915;&#36825;&#19977;&#20010;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#20998;&#31867;&#23398;&#20316;&#20026;&#20016;&#23500;&#30340;&#23454;&#20307;&#20851;&#31995;&#28304;&#65292;&#25105;&#20204;&#21033;&#29992;&#25351;&#23548;&#35843;&#25972;&#26469;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29983;&#25104;&#29238;&#27597;&#21644;&#20804;&#24351;&#23454;&#20307;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;TaxoInstruct&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13405v1 Announce Type: new  Abstract: Entity Set Expansion, Taxonomy Expansion, and Seed-Guided Taxonomy Construction are three representative tasks that can be used to automatically populate an existing taxonomy with new entities. However, previous approaches often address these tasks separately with heterogeneous techniques, lacking a unified perspective. To tackle this issue, in this paper, we identify the common key skills needed for these tasks from the view of taxonomy structures -- finding 'siblings' and finding 'parents' -- and propose a unified taxonomy-guided instruction tuning framework to jointly solve the three tasks. To be specific, by leveraging the existing taxonomy as a rich source of entity relationships, we utilize instruction tuning to fine-tune a large language model to generate parent and sibling entities. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of TaxoInstruct, which outperforms task-specific baselines across 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26723;&#65292;&#23454;&#29616;&#22312;&#26032;&#39046;&#22495;&#22238;&#31572;&#38382;&#39064;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.12170</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Unsupervised LLM Adaptation for Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12170
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26723;&#65292;&#23454;&#29616;&#22312;&#26032;&#39046;&#22495;&#22238;&#31572;&#38382;&#39064;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#26679;&#21270;&#30693;&#35782;&#12290;&#25509;&#30528;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;&#65292;LLM&#33021;&#22815;&#36820;&#22238;&#22810;&#26679;&#38382;&#39064;&#30340;&#27491;&#30830;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#39044;&#35757;&#32451;&#30340;LLM&#35843;&#25972;&#21040;&#26032;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#22914;&#19981;&#21516;&#32452;&#32455;&#25110;&#26102;&#26399;&#65292;&#29992;&#20110;&#38382;&#31572;&#20219;&#21153;&#20250;&#20135;&#29983;&#24456;&#39640;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21363;&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#12289;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;&#28304;&#25968;&#25454;&#65289;&#21644;&#30446;&#26631;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26723;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;LLM&#65292;&#20351;&#20854;&#33021;&#22815;&#22238;&#31572;&#20851;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#22312;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35265;&#35299;&#65307;&#65288;i&#65289;&#24494;&#35843;&#27169;&#22411;&#23637;&#31034;&#20102;&#25552;&#20379;&#27491;&#30830;&#31572;&#26696;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12170v1 Announce Type: cross  Abstract: Large language models (LLM) learn diverse knowledge present in the large-scale training dataset via self-supervised training. Followed by instruction-tuning, LLM acquires the ability to return correct information for diverse questions. However, adapting these pre-trained LLMs to new target domains, such as different organizations or periods, for the question-answering (QA) task incurs a substantial annotation cost. To tackle this challenge, we propose a novel task, unsupervised LLM adaptation for question answering. In this task, we leverage a pre-trained LLM, a publicly available QA dataset (source data), and unlabeled documents from the target domain. Our goal is to learn LLM that can answer questions about the target domain. We introduce one synthetic and two real datasets to evaluate models fine-tuned on the source and target data, and reveal intriguing insights; (i) fine-tuned models exhibit the ability to provide correct answers 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26041;&#27861;SimPlan&#65292;&#32467;&#21512;&#20102;LLMs&#21644;&#32463;&#20856;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#35268;&#21010;&#39046;&#22495;&#30340;&#23454;&#39564;&#34920;&#26126;SimPlan&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#32773;</title><link>https://arxiv.org/abs/2402.11489</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;: &#35745;&#21010;&#26159;&#20160;&#20040;&#65311;&#35780;&#20272;&#21644;&#24320;&#21457;&#38024;&#23545;LLMs&#30340;&#35745;&#21010;&#24847;&#35782;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
What's the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26041;&#27861;SimPlan&#65292;&#32467;&#21512;&#20102;LLMs&#21644;&#32463;&#20856;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#35268;&#21010;&#39046;&#22495;&#30340;&#23454;&#39564;&#34920;&#26126;SimPlan&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#32773;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11489v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#35745;&#21010;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#28041;&#21450;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#25214;&#21040;&#23454;&#29616;&#29305;&#23450;&#30446;&#26631;&#30340;&#19968;&#31995;&#21015;&#34892;&#21160;&#12290; &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#38656;&#35201;&#35745;&#21010;&#33021;&#21147;&#30340;&#24212;&#29992;&#65292;&#22914;&#32593;&#32476;&#25110;&#20855;&#20307;&#20195;&#29702;&#12290; &#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#19968;&#33268;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;LLMs&#32570;&#20047;&#35745;&#21010;&#25152;&#38656;&#30340;&#24517;&#35201;&#25216;&#33021;&#12290; &#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#20513;&#23548;&#19968;&#31181;&#23558;LLMs&#19982;&#32463;&#20856;&#35745;&#21010;&#26041;&#27861;&#32467;&#21512;&#30340;&#28151;&#21512;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SimPlan&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#20013;&#30340;&#34920;&#29616;&#12290; &#25105;&#20204;&#22312;&#21508;&#31181;&#35268;&#21010;&#39046;&#22495;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;SimPlan&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11489v1 Announce Type: new  Abstract: Planning is a fundamental task in artificial intelligence that involves finding a sequence of actions that achieve a specified goal in a given environment. Large language models (LLMs) are increasingly used for applications that require planning capabilities, such as web or embodied agents. In line with recent studies, we demonstrate through experimentation that LLMs lack necessary skills required for planning. Based on these observations, we advocate for the potential of a hybrid approach that combines LLMs with classical planning methodology. Then, we introduce SimPlan, a novel hybrid-method, and evaluate its performance in a new challenging setup. Our extensive experiments across various planning domains demonstrate that SimPlan significantly outperforms existing LLM-based planners.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36923;&#36753;&#25903;&#26550;&#25512;&#29702;&#35268;&#21017;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21253;&#21547;&#22522;&#30784;&#21644;&#32452;&#21512;&#35268;&#21017;&#30340;&#25512;&#29702;&#35268;&#21017;&#24211;ULogic&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#36923;&#36753;&#29702;&#35299;&#26041;&#38754;&#19982;&#20154;&#31867;&#34920;&#29616;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#20934;&#30830;&#12289;&#22797;&#26434;&#21644;&#25277;&#35937;&#30340;&#25512;&#29702;&#32467;&#26524;&#26469;&#25552;&#21319;&#19979;&#28216;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.11442</link><description>&lt;p&gt;
&#33021;&#22815;&#19982;&#35268;&#21017;&#36827;&#34892;&#25512;&#29702;&#21527;&#65311;&#36923;&#36753;&#25903;&#26550;&#29992;&#20110;&#21387;&#21147;&#27979;&#35797;&#21644;&#25552;&#21319;LLM
&lt;/p&gt;
&lt;p&gt;
Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11442
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36923;&#36753;&#25903;&#26550;&#25512;&#29702;&#35268;&#21017;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21253;&#21547;&#22522;&#30784;&#21644;&#32452;&#21512;&#35268;&#21017;&#30340;&#25512;&#29702;&#35268;&#21017;&#24211;ULogic&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#36923;&#36753;&#29702;&#35299;&#26041;&#38754;&#19982;&#20154;&#31867;&#34920;&#29616;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#20934;&#30830;&#12289;&#22797;&#26434;&#21644;&#25277;&#35937;&#30340;&#25512;&#29702;&#32467;&#26524;&#26469;&#25552;&#21319;&#19979;&#28216;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25509;&#36817;&#20154;&#31867;&#34920;&#29616;&#30340;&#25104;&#32489;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#22522;&#30784;&#25512;&#29702;&#35268;&#21017;&#30340;&#25484;&#25569;&#20173;&#28982;&#19981;&#21450;&#20154;&#31867;&#33021;&#21147;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36923;&#36753;&#25903;&#26550;&#25512;&#29702;&#35268;&#21017;&#29983;&#25104;&#26694;&#26550;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#39046;&#22495;&#20013;&#22522;&#30784;&#21644;&#32452;&#21512;&#35268;&#21017;&#30340;&#25512;&#29702;&#35268;&#21017;&#24211;ULogic&#12290;&#25105;&#20204;&#23545;GPT&#31995;&#21015;&#27169;&#22411;&#22312;&#35268;&#21017;&#23376;&#38598;&#19978;&#30340;&#20998;&#26512;&#25581;&#31034;&#20986;LLMs&#22312;&#36923;&#36753;&#29702;&#35299;&#26041;&#38754;&#19982;&#20154;&#31867;&#34920;&#29616;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#26576;&#20123;&#20559;&#35265;&#27169;&#24335;&#30340;&#32452;&#21512;&#21644;&#32467;&#26500;&#22797;&#26434;&#35268;&#21017;&#20013;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#36825;&#20123;&#35268;&#21017;&#25552;&#28860;&#25104;&#19968;&#20010;&#26356;&#23567;&#35268;&#27169;&#30340;&#25512;&#29702;&#24341;&#25806;&#65292;&#29992;&#20110;&#28789;&#27963;&#22320;&#29983;&#25104;&#35268;&#21017;&#24182;&#22686;&#24378;&#19979;&#28216;&#25512;&#29702;&#12290;&#36890;&#36807;&#22810;&#35780;&#20272;&#20154;&#21592;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#25512;&#29702;&#24341;&#25806;&#35777;&#26126;&#22312;&#29983;&#25104;&#20934;&#30830;&#12289;&#22797;&#26434;&#21644;&#25277;&#35937;&#30340;&#32467;&#35770;&#21644;&#21069;&#25552;&#26041;&#38754;&#34920;&#29616;&#20986;&#25928;&#26524;&#65292;&#21487;&#20197;&#25913;&#21892;&#21508;&#31181;&#24120;&#35782;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11442v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive human-like performance across various reasoning tasks. However, their mastery of underlying inferential rules still falls short of human capabilities. To investigate this, we propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains. Our analysis of GPT-series models over a rule subset reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns. We further distill these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream reasoning. Through a multi-judger evaluation, our inference engine proves effective in generating accurate, complex and abstract conclusions and premises, and improve various commonsense reas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OneBit&#30340;1&#20301;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#30697;&#38453;&#37327;&#21270;&#20026;1&#20301;&#65292;&#20026;&#26497;&#20302;&#27604;&#29305;&#23485;&#24230;&#30340;LLMs&#37096;&#32626;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.11295</link><description>&lt;p&gt;
OneBit:&#26397;&#30528;&#26497;&#20302;&#27604;&#29305;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
OneBit: Towards Extremely Low-bit Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OneBit&#30340;1&#20301;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#30697;&#38453;&#37327;&#21270;&#20026;1&#20301;&#65292;&#20026;&#26497;&#20302;&#27604;&#29305;&#23485;&#24230;&#30340;LLMs&#37096;&#32626;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#37327;&#21270;&#20351;&#29992;&#20302;&#27604;&#29305;&#23485;&#24230;&#20540;&#26469;&#34920;&#31034;&#27169;&#22411;&#30340;&#26435;&#37325;&#30697;&#38453;&#65292;&#36825;&#26159;&#20943;&#23569;&#37096;&#32626;&#39640;&#24230;&#26399;&#24453;&#30340;LLMs&#30340;&#23384;&#20648;&#21644;&#35745;&#31639;&#24320;&#38144;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37327;&#21270;&#26041;&#27861;&#22312;&#27604;&#29305;&#23485;&#24230;&#26497;&#23567;&#26102;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#65292;&#22240;&#27492;&#19987;&#27880;&#20110;&#21033;&#29992;4&#20301;&#25110;8&#20301;&#20540;&#26469;&#37327;&#21270;&#27169;&#22411;&#12290;&#26412;&#25991;&#22823;&#32966;&#22320;&#23558;LLMs&#30340;&#26435;&#37325;&#30697;&#38453;&#37327;&#21270;&#20026;1&#20301;&#65292;&#20026;LLMs&#30340;&#26497;&#20302;&#27604;&#29305;&#23485;&#24230;&#37096;&#32626;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;OneBit&#30340;1&#20301;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26356;&#22909;&#22320;&#37327;&#21270;LLMs&#30340;&#26032;&#39062;&#30340;1&#20301;&#21442;&#25968;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#26377;&#25928;&#21442;&#25968;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;QAT&#26694;&#26550;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#20805;&#20998;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OneBit&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65288;&#33267;&#23569;&#26159;&#38750;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11295v1 Announce Type: new  Abstract: Model quantification uses low bit-width values to represent the weight matrices of models, which is a promising approach to reduce both storage and computational overheads of deploying highly anticipated LLMs. However, existing quantization methods suffer severe performance degradation when the bit-width is extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes the weight matrices of LLMs to 1-bit, paving the way for the extremely low bit-width deployment of LLMs. For this target, we introduce a 1-bit quantization-aware training (QAT) framework named OneBit, including a novel 1-bit parameter representation method to better quantize LLMs as well as an effective parameter initialization method based on matrix decomposition to improve the convergence speed of the QAT framework. Sufficient experimental results indicate that OneBit achieves good performance (at least 83% of the non
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550; AFaCTA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#30340;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11073</link><description>&lt;p&gt;
AFaCTA: &#20351;&#29992;&#21487;&#38752;&#30340;LLM&#26631;&#27880;&#32773;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#30340;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11073
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550; AFaCTA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#30340;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#65292;&#29992;&#20110;&#25171;&#20987;&#35823;&#23548;&#20449;&#24687;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#65292;&#21363;&#20107;&#23454;&#26680;&#26597;&#31649;&#36947;&#20013;&#30340;&#31532;&#19968;&#27493;&#65292;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#21487;&#20280;&#32553;&#24615;&#21644;&#27867;&#21270;&#24615;&#65306;&#65288;1&#65289;&#20219;&#21153;&#23450;&#20041;&#21644;&#32034;&#36180;&#27010;&#24565;&#30340;&#19981;&#19968;&#33268;&#24615;&#20197;&#21450;&#65288;2&#65289;&#25163;&#21160;&#26631;&#27880;&#30340;&#39640;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#65288;1&#65289;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#30456;&#20851;&#24037;&#20316;&#20013;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32858;&#28966;&#20110;&#21487;&#39564;&#35777;&#24615;&#30340;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#32479;&#19968;&#23450;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#65288;2&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AFaCTA&#65288;&#33258;&#21160;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#26631;&#27880;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#20013;&#25552;&#20379;&#24110;&#21161;&#12290;AFaCTA&#36890;&#36807;&#27839;&#30528;&#19977;&#26465;&#39044;&#23450;&#20041;&#30340;&#25512;&#29702;&#36335;&#24452;&#20445;&#25345;&#19968;&#33268;&#24615;&#26469;&#26657;&#20934;&#20854;&#27880;&#37322;&#30340;&#32622;&#20449;&#24230;&#12290;&#22312;&#25919;&#27835;&#35328;&#35770;&#39046;&#22495;&#30340;&#22823;&#37327;&#35780;&#20272;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;AFaCTA&#33021;&#22815;&#39640;&#25928;&#22320;&#21327;&#21161;&#19987;&#19994;&#20154;&#21592;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11073v1 Announce Type: cross  Abstract: With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist exper
&lt;/p&gt;</description></item><item><title>SPAR&#26159;&#19968;&#20010;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;PLM&#12289;&#22810;&#27880;&#24847;&#21147;&#23618;&#21644;&#27880;&#24847;&#21147;&#31232;&#30095;&#26426;&#21046;&#65292;&#22312;&#20250;&#35805;&#32423;&#21035;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#65292;&#25552;&#21462;&#20840;&#38754;&#29992;&#25143;&#20852;&#36259;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2402.10555</link><description>&lt;p&gt;
SPAR&#65306;&#36890;&#36807;&#38271;&#26399;&#21442;&#19982;&#27880;&#24847;&#21147;&#23454;&#29616;&#20010;&#24615;&#21270;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
SPAR: Personalized Content-Based Recommendation via Long Engagement Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10555
&lt;/p&gt;
&lt;p&gt;
SPAR&#26159;&#19968;&#20010;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;PLM&#12289;&#22810;&#27880;&#24847;&#21147;&#23618;&#21644;&#27880;&#24847;&#21147;&#31232;&#30095;&#26426;&#21046;&#65292;&#22312;&#20250;&#35805;&#32423;&#21035;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#65292;&#25552;&#21462;&#20840;&#38754;&#29992;&#25143;&#20852;&#36259;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29992;&#25143;&#38271;&#26399;&#21442;&#19982;&#21382;&#21490;&#23545;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#33267;&#20851;&#37325;&#35201;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#25104;&#21151;&#23548;&#33268;&#23427;&#20204;&#34987;&#29992;&#20110;&#32534;&#30721;&#29992;&#25143;&#21382;&#21490;&#21644;&#20505;&#36873;&#39033;&#65292;&#23558;&#20869;&#23481;&#25512;&#33616;&#35270;&#20026;&#25991;&#26412;&#35821;&#20041;&#21305;&#37197;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#22312;&#22788;&#29702;&#38750;&#24120;&#38271;&#30340;&#29992;&#25143;&#21382;&#21490;&#25991;&#26412;&#21644;&#19981;&#36275;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#26694;&#26550;SPAR&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;&#20174;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#20013;&#25552;&#21462;&#20840;&#38754;&#29992;&#25143;&#20852;&#36259;&#30340;&#25361;&#25112;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;PLM&#12289;&#22810;&#27880;&#24847;&#21147;&#23618;&#21644;&#27880;&#24847;&#21147;&#31232;&#30095;&#26426;&#21046;&#20197;&#20250;&#35805;&#20026;&#22522;&#30784;&#23545;&#29992;&#25143;&#30340;&#21382;&#21490;&#36827;&#34892;&#32534;&#30721;&#12290;&#29992;&#25143;&#21644;&#29289;&#21697;&#20391;&#29305;&#24449;&#34987;&#20805;&#20998;&#34701;&#21512;&#36827;&#34892;&#21442;&#19982;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#25345;&#21452;&#26041;&#30340;&#29420;&#31435;&#34920;&#31034;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#27169;&#22411;&#37096;&#32626;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10555v1 Announce Type: cross  Abstract: Leveraging users' long engagement histories is essential for personalized content recommendations. The success of pretrained language models (PLMs) in NLP has led to their use in encoding user histories and candidate items, framing content recommendations as textual semantic matching tasks. However, existing works still struggle with processing very long user historical text and insufficient user-item interaction. In this paper, we introduce a content-based recommendation framework, SPAR, which effectively tackles the challenges of holistic user interest extraction from the long user engagement history. It achieves so by leveraging PLM, poly-attention layers and attention sparsity mechanisms to encode user's history in a session-based manner. The user and item side features are sufficiently fused for engagement prediction while maintaining standalone representations for both sides, which is efficient for practical model deployment. Mor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.08957</link><description>&lt;p&gt;
MUSTARD&#65306;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#23398;&#25512;&#29702;&#21644;&#23450;&#29702;&#35777;&#26126;&#12290;&#30001;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#38656;&#35201;&#20005;&#26684;&#21644;&#24418;&#24335;&#21270;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#23427;&#20204;&#26159;&#25506;&#32034;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#21560;&#24341;&#39046;&#22495;&#65292;&#20294;&#20173;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#22914;Chain-of-Thought&#65288;CoT&#65289;&#25581;&#31034;&#20102;&#20013;&#38388;&#27493;&#39588;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36880;&#27493;&#27880;&#37322;&#38656;&#35201;&#22823;&#37327;&#30340;&#21171;&#21160;&#21147;&#65292;&#23548;&#33268;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#30340;&#35757;&#32451;&#27493;&#39588;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#20027;&#23548;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;&#12290;MUSTARD&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#21512;&#25104;&#25968;&#25454;&#65306;&#65288;1&#65289;&#23427;&#38543;&#26426;&#36873;&#25321;&#20960;&#20010;&#25968;&#23398;&#27010;&#24565;&#20316;&#20026;&#38382;&#39064;&#30340;&#31867;&#21035;&#12290;&#65288;2&#65289;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#36873;&#23450;&#30340;&#27010;&#24565;&#25552;&#31034;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#38382;&#39064;&#21644;&#23427;&#20204;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08957v1 Announce Type: new Abstract: Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-w
&lt;/p&gt;</description></item><item><title>SemRel2024&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;14&#31181;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;&#65292;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#21512;&#21487;&#20197;&#25506;&#32034;&#21644;&#37327;&#21270;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21512;&#28085;&#30422;&#20102;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#31561;14&#31181;&#35821;&#35328;&#12290;</title><link>https://arxiv.org/abs/2402.08638</link><description>&lt;p&gt;
SemRel2024: 14&#31181;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08638
&lt;/p&gt;
&lt;p&gt;
SemRel2024&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;14&#31181;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;&#65292;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#21512;&#21487;&#20197;&#25506;&#32034;&#21644;&#37327;&#21270;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21512;&#28085;&#30422;&#20102;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#31561;14&#31181;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#21644;&#37327;&#21270;&#35821;&#20041;&#30456;&#20851;&#24615;&#26159;&#35821;&#35328;&#34920;&#36798;&#30340;&#26680;&#24515;&#12290;&#23427;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#21253;&#25324;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#25552;&#20379;&#27934;&#23519;&#12290;&#34429;&#28982;&#26089;&#26399;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#19978;&#65292;&#24448;&#24448;&#26159;&#22312;&#33521;&#35821;&#35821;&#22659;&#20013;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#26356;&#24191;&#27867;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#20540;&#24471;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SemRel&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#27597;&#35821;&#20026;14&#31181;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#30340;&#26032;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;&#65306;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#12290;&#36825;&#20123;&#35821;&#35328;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#30340;&#35821;&#31995;&#65292;&#20027;&#35201;&#22312;&#38750;&#27954;&#21644;&#20122;&#27954;&#20351;&#29992;&#65292;&#36825;&#20123;&#22320;&#21306;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30456;&#23545;&#36739;&#23569;&#12290;SemRel&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#37117;&#26159;&#19982;&#19968;&#20010;&#34920;&#31034;&#30456;&#20851;&#24615;&#24471;&#20998;&#30340;&#21477;&#23376;&#23545;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring and quantifying semantic relatedness is central to representing language. It holds significant implications across various NLP tasks, including offering insights into the capabilities and performance of Large Language Models (LLMs). While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness. In this paper, we present SemRel, a new semantic relatedness dataset collection annotated by native speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by a relatively limited availability of NLP resources. Each instance in the SemRel datasets is a sentence pair associated with a score that repr
&lt;/p&gt;</description></item><item><title>UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07939</link><description>&lt;p&gt;
UFO: &#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#20132;&#20114;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
UFO: A UI-Focused Agent for Windows OS Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07939
&lt;/p&gt;
&lt;p&gt;
UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;UFO&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20102;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;UFO&#37319;&#29992;&#21452;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#31934;&#30830;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#21644;&#25511;&#21046;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#26234;&#33021;&#20307;&#21487;&#20197;&#26080;&#32541;&#22320;&#22312;&#21333;&#20010;&#24212;&#29992;&#31243;&#24207;&#20869;&#20197;&#21450;&#36328;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#23548;&#33322;&#21644;&#25805;&#20316;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#21363;&#20351;&#28041;&#21450;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#21160;&#20316;&#36830;&#25509;&#65292;&#24182;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#12290;&#22240;&#27492;&#65292;UFO&#23558;&#33392;&#24040;&#32780;&#32791;&#26102;&#30340;&#36807;&#31243;&#36716;&#21464;&#20026;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#23601;&#21487;&#20197;&#23436;&#25104;&#30340;&#31616;&#21333;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;9&#20010;&#27969;&#34892;&#30340;Windows&#24212;&#29992;&#31243;&#24207;&#19978;&#23545;UFO&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#21453;&#26144;&#29992;&#25143;&#26085;&#24120;&#20351;&#29992;&#24773;&#26223;&#30340;&#21508;&#31181;&#24773;&#20917;&#12290;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#21644;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#24471;&#20986;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;UFO&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07204</link><description>&lt;p&gt;
&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#22478;&#24066;&#34892;&#31243;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#12290;OUIP&#19982;&#20256;&#32479;&#34892;&#31243;&#35268;&#21010;&#19981;&#21516;&#65292;&#20256;&#32479;&#35268;&#21010;&#38480;&#21046;&#20102;&#29992;&#25143;&#34920;&#36798;&#26356;&#35814;&#32454;&#30340;&#38656;&#27714;&#65292;&#38459;&#30861;&#20102;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#23454;&#26102;&#20449;&#24687;&#12289;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#21644;&#19981;&#36275;&#30340;&#31354;&#38388;&#24847;&#35782;&#65292;&#23427;&#20204;&#26080;&#27861;&#29420;&#31435;&#22320;&#25552;&#20379;&#28385;&#24847;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ItiNera&#30340;OUIP&#31995;&#32479;&#65292;&#23558;&#31354;&#38388;&#20248;&#21270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#26356;&#26032;&#20852;&#36259;&#28857;&#29305;&#24449;&#65292;&#20197;&#21019;&#24314;&#29992;&#25143;&#33258;&#24049;&#30340;&#20010;&#24615;&#21270;&#20852;&#36259;&#28857;&#25968;&#25454;&#24211;&#12290;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#35831;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#21327;&#21516;&#23454;&#29616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
&lt;/p&gt;</description></item><item><title>L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04902</link><description>&lt;p&gt;
L4Q: &#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#37327;&#21270;&#35757;&#32451;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04902
&lt;/p&gt;
&lt;p&gt;
L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;(QAT)&#26041;&#27861;&#27491;&#22312;&#27969;&#34892;&#36215;&#26469;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25152;&#24102;&#26469;&#30340;&#39640;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#20943;&#23569;&#30340;&#35757;&#32451;&#24320;&#38144;&#65292;&#36890;&#24120;&#39318;&#36873;&#21518;&#35757;&#32451;&#37327;&#21270;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#24182;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#37327;&#21270;&#24863;&#30693;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#37327;&#21270;&#27169;&#22411;&#30340;&#37197;&#32622;&#12290;&#30001;&#38750;&#32447;&#24615;&#37327;&#21270;&#25110;&#28151;&#21512;&#31934;&#24230;&#26435;&#37325;&#24341;&#36215;&#30340;&#25928;&#26524;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#24182;&#19988;&#37325;&#26032;&#35757;&#32451;&#29305;&#23450;&#37327;&#21270;&#21442;&#25968;&#21487;&#33021;&#20250;&#24433;&#21709;&#26368;&#20248;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L4Q&#65292;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#12290;L4Q&#21033;&#29992;&#20102;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CogCoM&#65292;&#19968;&#20010;&#20855;&#22791;&#25805;&#20316;&#38142;&#26426;&#21046;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25805;&#20316;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#65292;&#24182;&#20197;&#20854;&#35777;&#25454;&#24615;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#23454;&#29616;&#24544;&#23454;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.04236</link><description>&lt;p&gt;
CogCoM: &#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25805;&#20316;&#35757;&#32451;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#28145;&#20837;&#32454;&#33410;
&lt;/p&gt;
&lt;p&gt;
CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CogCoM&#65292;&#19968;&#20010;&#20855;&#22791;&#25805;&#20316;&#38142;&#26426;&#21046;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25805;&#20316;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#65292;&#24182;&#20197;&#20854;&#35777;&#25454;&#24615;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#23454;&#29616;&#24544;&#23454;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#36890;&#36807;&#24191;&#27867;&#30340;&#35757;&#32451;&#65292;&#22312;&#23558;&#35270;&#35273;&#25351;&#20196;&#19982;&#31572;&#26696;&#23545;&#40784;&#26041;&#38754;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30830;&#23450;&#24615;&#30340;&#23545;&#40784;&#23548;&#33268;&#27169;&#22411;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#35270;&#35273;&#25512;&#29702;&#65292;&#24182;&#23548;&#33268;&#22312;&#32454;&#33268;&#30340;&#35270;&#35273;&#38382;&#39064;&#21644;&#19981;&#24544;&#23454;&#30340;&#21709;&#24212;&#26041;&#38754;&#22833;&#36133;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#25805;&#20316;&#38142;&#8221;&#30340;&#26426;&#21046;&#65292;&#20351;VLM&#33021;&#22815;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25805;&#20316;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#25805;&#20316;&#37117;&#25351;&#30340;&#26159;&#23545;&#35270;&#35273;&#36755;&#20837;&#30340;&#25805;&#20316;&#65292;&#21487;&#20197;&#26159;&#36890;&#36807;&#20808;&#21069;&#35757;&#32451;&#33719;&#24471;&#30340;&#20869;&#22312;&#33021;&#21147;&#65288;&#20363;&#22914;&#65292;&#22522;&#30784;&#65289;&#25110;&#32773;&#26159;&#27169;&#20223;&#31867;&#20154;&#34892;&#20026;&#65288;&#20363;&#22914;&#65292;&#25918;&#22823;&#65289;&#12290;&#36825;&#20010;&#26426;&#21046;&#40723;&#21169;VLM&#29983;&#25104;&#24102;&#26377;&#35777;&#25454;&#30340;&#35270;&#35273;&#25512;&#29702;&#30340;&#24544;&#23454;&#30340;&#21709;&#24212;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#22312;&#21487;&#35299;&#37322;&#30340;&#36335;&#24452;&#19978;&#36861;&#36394;&#38169;&#35823;&#30340;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;CogCoM&#65292;&#19968;&#20010;&#20855;&#26377;&#20869;&#32622;&#25512;&#29702;&#26426;&#21046;&#30340;17B&#36890;&#29992;VLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers. However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses. In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths. We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism. Experiments show that our model achieves the state-of-the-art 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21463;&#24515;&#29702;&#23398;&#21551;&#21457;&#30340;&#20004;&#20010;&#20559;&#35265;&#27979;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#26126;&#30830;&#26080;&#20559;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;&#20154;&#31867;&#21270;&#30340;&#21051;&#26495;&#21360;&#35937;&#20559;&#35265;&#65292;&#24182;&#19982;&#23454;&#38469;&#20915;&#31574;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#30456;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.04105</link><description>&lt;p&gt;
&#22312;&#26126;&#30830;&#26080;&#20559;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#38544;&#24615;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Measuring Implicit Bias in Explicitly Unbiased Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04105
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21463;&#24515;&#29702;&#23398;&#21551;&#21457;&#30340;&#20004;&#20010;&#20559;&#35265;&#27979;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#26126;&#30830;&#26080;&#20559;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;&#20154;&#31867;&#21270;&#30340;&#21051;&#26495;&#21360;&#35937;&#20559;&#35265;&#65292;&#24182;&#19982;&#23454;&#38469;&#20915;&#31574;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#36890;&#36807;&#26126;&#30830;&#30340;&#20559;&#35265;&#27979;&#35797;&#65292;&#20294;&#20173;&#28982;&#21487;&#33021;&#23384;&#22312;&#38544;&#24615;&#20559;&#35265;&#65292;&#31867;&#20284;&#20110;&#25345;&#26377;&#24179;&#31561;&#20027;&#20041;&#20449;&#24565;&#30340;&#20154;&#20204;&#21364;&#34920;&#29616;&#20986;&#24494;&#22937;&#30340;&#20559;&#35265;&#12290;&#27979;&#37327;&#36825;&#31181;&#38544;&#24615;&#20559;&#35265;&#26159;&#19968;&#39033;&#25361;&#25112;&#65306;&#38543;&#30528;LLMs&#21464;&#24471;&#36234;&#26469;&#36234;&#19987;&#26377;&#65292;&#21487;&#33021;&#26080;&#27861;&#35775;&#38382;&#23427;&#20204;&#30340;&#23884;&#20837;&#65292;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#20559;&#35265;&#27979;&#37327;&#26041;&#27861;&#65307;&#27492;&#22806;&#65292;&#38544;&#24615;&#20559;&#35265;&#20027;&#35201;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#22914;&#26524;&#23427;&#20204;&#24433;&#21709;&#20102;&#36825;&#20123;&#31995;&#32479;&#25152;&#20570;&#30340;&#23454;&#38469;&#20915;&#31574;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#21463;&#24515;&#29702;&#23398;&#21551;&#21457;&#30340;&#20004;&#20010;&#20559;&#35265;&#27979;&#37327;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20004;&#20010;&#25361;&#25112;&#65306;LLMs&#38544;&#21547;&#32852;&#24819;&#27979;&#35797;&#65288;IAT&#65289;&#20559;&#35265;&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#27979;&#37327;&#38544;&#24615;&#20559;&#35265;&#30340;&#26041;&#27861;&#65307;LLMs&#20915;&#31574;&#20559;&#35265;&#29992;&#20110;&#26816;&#27979;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#24494;&#22937;&#27495;&#35270;&#12290;&#20351;&#29992;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;6&#20010;LLMs&#22312;4&#20010;&#31038;&#20250;&#39046;&#22495;&#65288;&#31181;&#26063;&#12289;&#24615;&#21035;&#12289;&#23447;&#25945;&#12289;&#20581;&#24247;&#65289;&#21644;21&#20010;&#31867;&#21035;&#65288;&#27494;&#22120;&#12289;&#32618;&#32602;&#12289;&#31185;&#23398;&#12289;&#32844;&#19994;&#31561;&#65289;&#20013;&#26222;&#36941;&#23384;&#22312;&#20154;&#31867;&#21270;&#30340;&#21051;&#26495;&#21360;&#35937;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#38544;&#24615;&#20559;&#35265;&#27979;&#37327;&#26041;&#27861;&#19982;&#23454;&#38469;&#20915;&#31574;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: as LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both of these challenges by introducing two measures of bias inspired by psychology: LLM Implicit Association Test (IAT) Bias, which is a prompt-based method for revealing implicit bias; and LLM Decision Bias for detecting subtle discrimination in decision-making tasks. Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others). Our prompt-based measure of implicit bias correlates wit
&lt;/p&gt;</description></item><item><title>DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02563</link><description>&lt;p&gt;
DefInt&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02563
&lt;/p&gt;
&lt;p&gt;
DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#33021;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22914;&#36830;&#38145;&#25512;&#29702;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#20027;&#35201;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#19981;&#26029;&#22686;&#21152;&#30340;&#26631;&#35760;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#24040;&#22823;&#35299;&#31354;&#38388;&#30340;&#24320;&#25918;&#24615;&#23454;&#38469;&#20219;&#21153;&#26469;&#35828;&#21487;&#33021;&#29305;&#21035;&#38382;&#39064;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65288;DefInt&#65289;&#65292;&#20197;&#37322;&#25918;&#28151;&#21512;LLMs&#30340;&#21327;&#21516;&#28508;&#21147;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;DefInt&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20302;&#25104;&#26412;&#30340;&#25512;&#29702;&#24605;&#36335;&#65292;&#31867;&#20284;&#20110;&#8220;&#31995;&#32479;1&#8221;&#20135;&#29983;&#30340;&#24555;&#36895;&#30452;&#35273;&#12290;&#22914;&#26524;&#36825;&#20123;&#30452;&#35273;&#34987;&#35748;&#20026;&#20302;&#32622;&#20449;&#24230;&#65292;&#21017;DefInt&#23558;&#35843;&#29992;&#25918;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#24605;&#25512;&#29702;&#20316;&#20026;&#8220;&#31995;&#32479;2&#8221;&#30340;&#24178;&#39044;&#65292;&#21487;&#20197;&#35206;&#30422;&#40664;&#35748;&#24605;&#32771;&#24182;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#23454;&#39564;&#22312;&#20116;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;DefInt&#35770;&#25991;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
&lt;/p&gt;</description></item><item><title>AutoTimes&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36716;&#25442;&#33021;&#21147;&#26469;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#19982;&#20808;&#21069;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02370</link><description>&lt;p&gt;
AutoTimes: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
AutoTimes: Autoregressive Time Series Forecasters via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02370
&lt;/p&gt;
&lt;p&gt;
AutoTimes&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36716;&#25442;&#33021;&#21147;&#26469;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#19982;&#20808;&#21069;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#65292;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#23578;&#26410;&#23436;&#20840;&#21457;&#23637;&#12290;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#30456;&#20284;&#39034;&#24207;&#32467;&#26500;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#24573;&#35270;&#20102;&#26102;&#38388;&#24207;&#21015;&#21644;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#30340;&#19968;&#33268;&#24615;&#65292;&#23548;&#33268;&#23545;LLM&#28508;&#21147;&#30340;&#21033;&#29992;&#19981;&#36275;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#20174;&#35821;&#35328;&#24314;&#27169;&#20013;&#23398;&#21040;&#30340;&#36890;&#29992;&#20196;&#29260;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoTimes&#65292;&#23558;LLM&#37325;&#26032;&#29992;&#20316;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#36825;&#19982;LLM&#30340;&#33719;&#21462;&#21644;&#21033;&#29992;&#19968;&#33268;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#21442;&#25968;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#39044;&#27979;&#22120;&#21487;&#20197;&#22788;&#29702;&#28789;&#27963;&#30340;&#31995;&#21015;&#38271;&#24230;&#65292;&#24182;&#23454;&#29616;&#19982;&#27969;&#34892;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20196;&#29260;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#24212;&#30340;&#26102;&#38388;&#25139;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models of time series have not been fully developed due to the limited availability of large-scale time series and the underexploration of scalable pre-training. Based on the similar sequential structure of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, prior methods may overlook the consistency in aligning time series and natural language, resulting in insufficient utilization of the LLM potentials. To fully exploit the general-purpose token transitions learned from language modeling, we propose AutoTimes to repurpose LLMs as Autoregressive Time series forecasters, which is consistent with the acquisition and utilization of LLMs without updating the parameters. The consequent forecasters can handle flexible series lengths and achieve competitive performance as prevalent models. Further, we present token-wise prompting that utilizes corresponding timestamps to make ou
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#20026;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#38271;&#24230;&#32780;&#35774;&#35745;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#24182;&#22238;&#39038;&#20102;&#21253;&#25324;&#26550;&#26500;&#20462;&#25913;&#22312;&#20869;&#30340;&#22810;&#31181;&#25216;&#26415;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#38271;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.02244</link><description>&lt;p&gt;
&#36229;&#36234;&#26497;&#38480;&#65306;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02244
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#20026;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#38271;&#24230;&#32780;&#35774;&#35745;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#24182;&#22238;&#39038;&#20102;&#21253;&#25324;&#26550;&#26500;&#20462;&#25913;&#22312;&#20869;&#30340;&#22810;&#31181;&#25216;&#26415;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#38271;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#24778;&#24322;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#29702;&#35299;&#19978;&#19979;&#25991;&#12289;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#21644;&#29983;&#25104;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#20005;&#26684;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#35201;&#27714;&#20026;&#20195;&#20215;&#30340;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#26377;&#25928;&#25903;&#25345;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#33021;&#21147;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#22238;&#39038;&#20102;&#26368;&#36817;&#20026;&#25193;&#23637;LLMs&#24207;&#21015;&#38271;&#24230;&#32780;&#35774;&#35745;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#23545;&#38271;&#19978;&#19979;&#25991;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22238;&#39038;&#21644;&#20998;&#31867;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#20462;&#25913;&#20301;&#32622;&#32534;&#30721;&#21644;&#20462;&#25913;&#27880;&#24847;&#26426;&#21046;&#31561;&#26550;&#26500;&#20462;&#25913;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#26356;&#38271;&#24207;&#21015;&#30340;&#22788;&#29702;&#65292;&#21516;&#26102;&#36991;&#20813;&#35745;&#31639;&#38656;&#27714;&#30340;&#25104;&#27604;&#20363;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#30340;&#22810;&#26679;&#26041;&#27861;&#21487;&#20197;&#22312;LLMs&#30340;&#19981;&#21516;&#38454;&#27573;&#65288;&#21363;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#65289;&#20013;&#21033;&#29992;&#12290;&#36825;&#20351;&#24471;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#24207;&#21015;&#24182;&#25552;&#21319;&#23545;&#38271;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to effic
&lt;/p&gt;</description></item><item><title>Panacea &#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#32500;&#20559;&#22909;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#20302;&#31209;&#36866;&#24212;&#65292;&#20197;&#22312;&#32447;&#27880;&#20837;&#20559;&#22909;&#21521;&#37327;&#30340;&#24418;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#24182; Pareto &#26368;&#20248;&#22320;&#28385;&#36275;&#21508;&#31181;&#20559;&#22909;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.02030</link><description>&lt;p&gt;
Panacea: &#36890;&#36807;&#20559;&#22909;&#36866;&#24212;&#23454;&#29616; Pareto &#23545;&#40784;&#30340; LLMS
&lt;/p&gt;
&lt;p&gt;
Panacea: Pareto Alignment via Preference Adaptation for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02030
&lt;/p&gt;
&lt;p&gt;
Panacea &#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#32500;&#20559;&#22909;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#20302;&#31209;&#36866;&#24212;&#65292;&#20197;&#22312;&#32447;&#27880;&#20837;&#20559;&#22909;&#21521;&#37327;&#30340;&#24418;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#24182; Pareto &#26368;&#20248;&#22320;&#28385;&#36275;&#21508;&#31181;&#20559;&#22909;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#26631;&#37327;&#20154;&#31867;&#20559;&#22909;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32422;&#23450;&#20542;&#21521;&#20110;&#36807;&#24230;&#31616;&#21270;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#32500;&#21644;&#24322;&#36136;&#24615;&#29305;&#24615;&#65292;&#23548;&#33268;&#34920;&#36798;&#33021;&#21147;&#38477;&#20302;&#29978;&#33267;&#22833;&#37197;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861; Panacea&#65292;&#23558;&#23545;&#40784;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#32500;&#20559;&#22909;&#20248;&#21270;&#38382;&#39064;&#12290;Panacea &#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#32447;&#36866;&#24212;&#24182; Pareto &#26368;&#20248;&#22320;&#28385;&#36275;&#21508;&#31181;&#20559;&#22909;&#38598;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#30340;&#35843;&#25972;&#12290;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#20351;&#29992;&#20302;&#32500;&#20559;&#22909;&#21521;&#37327;&#26469;&#24341;&#23548;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#23613;&#31649;&#27169;&#22411;&#30001;&#25968;&#37327;&#24222;&#22823;&#30340;&#21442;&#25968;&#25152;&#25511;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;Panacea &#34987;&#35774;&#35745;&#20026;&#20351;&#29992;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#20302;&#31209;&#36866;&#24212;&#65292;&#21487;&#20197;&#23558;&#20559;&#22909;&#21521;&#37327;&#20316;&#20026;&#22855;&#24322;&#20540;&#31616;&#21333;&#22312;&#32447;&#27880;&#20837;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102; Panacea &#33021;&#22815;&#24674;&#22797;&#25972;&#20010; Pareto &#21069;&#27839;&#19982;&#24120;&#35265;&#25439;&#22833;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods for large language model alignment typically use scalar human preference labels. However, this convention tends to oversimplify the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment. This paper presents Panacea, an innovative approach that reframes alignment as a multi-dimensional preference optimization problem. Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning. A major challenge here is using a low-dimensional preference vector to guide the model's behavior, despite it being governed by an overwhelmingly large number of parameters. To address this, Panacea is designed to use singular value decomposition (SVD)-based low-rank adaptation, which allows the preference vector to be simply injected online as singular values. Theoretically, we prove that Panacea recovers the entire Pareto front with common loss agg
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiPO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#21015;&#34920;&#22411;&#25490;&#24207;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#25490;&#21517;&#21015;&#34920;&#20013;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20351;&#31574;&#30053;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#21487;&#34892;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.01878</link><description>&lt;p&gt;
LiPO: &#36890;&#36807;&#23398;&#20064;&#25490;&#24207;&#36827;&#34892;&#21015;&#34920;&#22411;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
LiPO: Listwise Preference Optimization through Learning-to-Rank
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiPO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#21015;&#34920;&#22411;&#25490;&#24207;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#25490;&#21517;&#21015;&#34920;&#20013;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20351;&#31574;&#30053;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#21487;&#34892;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#24037;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;&#26159;&#25511;&#21046;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#34892;&#20026;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;DPO&#21644;SLiC&#65292;&#25104;&#20026;&#20256;&#32479;&#30340;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#23454;&#38469;&#19978;&#65292;&#20154;&#24037;&#21453;&#39304;&#36890;&#24120;&#20197;&#23545;&#22810;&#20010;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#30340;&#26684;&#24335;&#25552;&#20379;&#65292;&#20197;&#25674;&#38144;&#38405;&#35835;&#25552;&#31034;&#30340;&#25104;&#26412;&#12290;&#22810;&#20010;&#21709;&#24212;&#20063;&#21487;&#20197;&#36890;&#36807;&#22870;&#21169;&#27169;&#22411;&#25110;AI&#21453;&#39304;&#36827;&#34892;&#25490;&#24207;&#12290;&#32570;&#23569;&#20851;&#20110;&#30452;&#25509;&#36866;&#24212;&#21709;&#24212;&#21015;&#34920;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#21015;&#34920;&#22411;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#25551;&#36848;&#20102;&#21015;&#34920;&#22411;&#20559;&#22909;&#20248;&#21270;&#65288;LiPO&#65289;&#26694;&#26550;&#65292;&#22312;&#32473;&#23450;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#31574;&#30053;&#21487;&#20197;&#20174;&#19968;&#20010;&#25490;&#21517;&#21015;&#34920;&#20013;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21487;&#34892;&#21709;&#24212;&#12290;&#36825;&#31181;&#35266;&#28857;&#19982;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#24418;&#25104;&#26126;&#30830;&#30340;&#32852;&#31995;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20559;&#22909;&#20248;&#21270;&#24037;&#20316;&#21487;&#20197;&#26144;&#23556;&#21040;&#29616;&#26377;&#30340;&#25490;&#21517;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI feedback. There lacks such a study on directly fitting upon a list of responses. In this work, we formulate the LM alignment as a listwise ranking problem and describe the Listwise Preference Optimization (LiPO) framework, where the policy can potentially learn more effectively from a ranked list of plausible responses given the prompt. This view draws an explicit connection to Learning-to-Rank (LTR), where most existing preference optimization work can be mapped to existing ranking objectives, esp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#35821;&#35328;&#30340;&#20998;&#24418;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#20855;&#26377;&#33258;&#30456;&#20284;&#24615;&#21644;&#38271;&#31243;&#30456;&#20851;&#24615;&#65292;&#20174;&#27573;&#33853;&#21040;&#25972;&#20010;&#25991;&#26723;&#37117;&#23384;&#22312;&#30456;&#20284;&#30340;&#27169;&#24335;&#21644;&#20381;&#36182;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#22914;&#20309;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26469;&#29702;&#35299;&#25991;&#26412;&#30340;&#22810;&#20010;&#23618;&#32423;&#32467;&#26500;&#12290;&#20998;&#24418;&#21442;&#25968;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22256;&#24785;&#24230;&#12290;&#36825;&#20123;&#21457;&#29616;&#25552;&#20379;&#20102;&#23545;&#35821;&#35328;&#21644;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2402.01825</link><description>&lt;p&gt;
&#20998;&#24418;&#27169;&#24335;&#21487;&#33021;&#25581;&#31034;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20013;&#30340;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Fractal Patterns May Unravel the Intelligence in Next-Token Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01825
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#35821;&#35328;&#30340;&#20998;&#24418;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#20855;&#26377;&#33258;&#30456;&#20284;&#24615;&#21644;&#38271;&#31243;&#30456;&#20851;&#24615;&#65292;&#20174;&#27573;&#33853;&#21040;&#25972;&#20010;&#25991;&#26723;&#37117;&#23384;&#22312;&#30456;&#20284;&#30340;&#27169;&#24335;&#21644;&#20381;&#36182;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#22914;&#20309;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26469;&#29702;&#35299;&#25991;&#26412;&#30340;&#22810;&#20010;&#23618;&#32423;&#32467;&#26500;&#12290;&#20998;&#24418;&#21442;&#25968;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22256;&#24785;&#24230;&#12290;&#36825;&#20123;&#21457;&#29616;&#25552;&#20379;&#20102;&#23545;&#35821;&#35328;&#21644;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#35821;&#35328;&#30340;&#20998;&#24418;&#32467;&#26500;&#65292;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#31934;&#30830;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#37327;&#21270;&#21487;&#33021;&#20043;&#21069;&#21482;&#26377;&#24576;&#30097;&#20294;&#23578;&#26410;&#27491;&#24335;&#35777;&#26126;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35821;&#35328;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;1&#65289;&#33258;&#30456;&#20284;&#24615;&#65292;&#23637;&#31034;&#20986;&#21508;&#20010;&#23618;&#32423;&#19978;&#30340;&#22797;&#26434;&#24615;&#65292;&#27809;&#26377;&#29305;&#23450;&#30340;&#29305;&#24449;&#19978;&#19979;&#25991;&#38271;&#24230;&#65307;&#65288;2&#65289;&#38271;&#31243;&#30456;&#20851;&#24615;&#65288;LRD&#65289;&#65292;&#20855;&#26377;&#22823;&#32422;H=0.70&#30340;Hurst&#21442;&#25968;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35748;&#20026;&#35821;&#35328;&#20013;&#30340;&#30701;&#26399;&#27169;&#24335;/&#20381;&#36182;&#24615;&#65292;&#22914;&#27573;&#33853;&#20013;&#30340;&#27169;&#24335;/&#20381;&#36182;&#24615;&#65292;&#21453;&#26144;&#20102;&#26356;&#22823;&#33539;&#22260;&#30340;&#27169;&#24335;/&#20381;&#36182;&#24615;&#65292;&#22914;&#25972;&#20010;&#25991;&#26723;&#12290;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#29702;&#35299;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#22914;&#20309;&#23548;&#33268;&#23545;&#25991;&#26412;&#30340;&#22810;&#20010;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#21333;&#35789;&#21644;&#20174;&#21477;&#21040;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#21644;&#24847;&#22270;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20998;&#24418;&#21442;&#25968;&#22312;&#39044;&#27979;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#22256;&#24785;&#24230;&#30340;&#27599;&#23383;&#33410;&#27604;&#29305;&#65288;BPB&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20123;&#21457;&#29616;&#33021;&#20026;&#35821;&#35328;&#21644;&#26426;&#21046;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately H=0.70. Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can lead to a comprehension of the structure of text at multiple levels of granularity, from words and clauses to broader contexts and intents. We also demonstrate that fractal parameters improve upon perplexity-based bits-per-byte (BPB) in predicting downstream performance. We hope these findings offer a fresh perspective on language and the mechani
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.07453</link><description>&lt;p&gt;
&#35268;&#27169;&#21270;&#27169;&#22411;&#32534;&#36753;&#20250;&#23548;&#33268;&#28176;&#36827;&#24615;&#21644;&#31361;&#21457;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Model Editing at Scale leads to Gradual and Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07453
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#36753;&#30693;&#35782;&#26159;&#19968;&#31181;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#33021;&#21147;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#32416;&#27491;&#38169;&#35823;&#23398;&#20064;&#30340;&#20107;&#23454;&#65292;&#21516;&#26102;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#30340;&#26032;&#20107;&#23454;&#21015;&#34920;&#26356;&#26032;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#20351;&#27169;&#22411;&#32534;&#36753;&#20855;&#26377;&#23454;&#38469;&#25928;&#29992;&#65292;&#25105;&#20204;&#24517;&#39035;&#33021;&#22815;&#23545;&#21516;&#19968;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#32534;&#36753;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65306;ROME &#21644; MEMIT&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#19981;&#26029;&#22320;&#36951;&#24536;&#20808;&#21069;&#32534;&#36753;&#36807;&#30340;&#20107;&#23454;&#20197;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#36951;&#24536;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;--&#21021;&#22987;&#30340;&#28176;&#36827;&#24615;&#36951;&#24536;&#38454;&#27573;&#65292;&#38543;&#21518;&#26159;&#31361;&#28982;&#25110;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07453v2 Announce Type: replace-cross  Abstract: Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate the current model editing methods at scale, focusing on two state of the art methods: ROME and MEMIT. We find that as the model is edited sequentially with multiple facts, it continually forgets previously edited facts and the ability to perform downstream tasks. This forgetting happens in two phases -- an initial gradual but progressive forgetting phase followed by abrupt or catastrophic forgettin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#23545;&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;&#20174;&#27169;&#22411;&#20026;&#20013;&#24515;&#12289;&#25968;&#25454;&#20026;&#20013;&#24515;&#21644;&#26694;&#26550;&#20026;&#20013;&#24515;&#30340;&#19977;&#20010;&#20027;&#35201;&#35282;&#24230;&#30340;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;&#27492;&#22806;&#65292;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;GitHub&#23384;&#20648;&#24211;&#26469;&#25910;&#38598;&#21644;&#26356;&#26032;&#30456;&#20851;&#35770;&#25991;&#12290;</title><link>https://arxiv.org/abs/2312.03863</link><description>&lt;p&gt;
&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Efficient Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03863
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#23545;&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;&#20174;&#27169;&#22411;&#20026;&#20013;&#24515;&#12289;&#25968;&#25454;&#20026;&#20013;&#24515;&#21644;&#26694;&#26550;&#20026;&#20013;&#24515;&#30340;&#19977;&#20010;&#20027;&#35201;&#35282;&#24230;&#30340;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;&#27492;&#22806;&#65292;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;GitHub&#23384;&#20648;&#24211;&#26469;&#25910;&#38598;&#21644;&#26356;&#26032;&#30456;&#20851;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37325;&#35201;&#20219;&#21153;&#22914;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#35821;&#35328;&#29983;&#25104;&#21644;&#22797;&#26434;&#25512;&#29702;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#26377;&#28508;&#21147;&#23545;&#25105;&#20204;&#30340;&#31038;&#20250;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20276;&#38543;&#30528;&#23427;&#20204;&#25152;&#38656;&#30340;&#30456;&#24403;&#22823;&#30340;&#36164;&#28304;&#65292;&#31361;&#26174;&#20102;&#35299;&#20915;&#25928;&#29575;&#25361;&#25112;&#30340;&#26377;&#25928;&#25216;&#26415;&#30340;&#24378;&#28872;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#39640;&#25928;LLMs&#30740;&#31350;&#30340;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#25105;&#20204;&#23558;&#25991;&#29486;&#25353;&#29031;&#27169;&#22411;&#20026;&#20013;&#24515;&#12289;&#25968;&#25454;&#20026;&#20013;&#24515;&#21644;&#26694;&#26550;&#20026;&#20013;&#24515;&#30340;&#19977;&#20010;&#20027;&#35201;&#20998;&#31867;&#36827;&#34892;&#32452;&#32455;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#20294;&#30456;&#20114;&#20851;&#32852;&#30340;&#39640;&#25928;LLMs&#20027;&#39064;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;GitHub&#23384;&#20648;&#24211;&#65292;&#20854;&#20013;&#25910;&#38598;&#20102;&#26412;&#35843;&#26597;&#20013;&#21015;&#20986;&#30340;&#35770;&#25991;&#65292;&#24182;&#23558;&#31215;&#26497;&#32500;&#25252;&#35813;&#23384;&#20648;&#24211;&#65292;&#24182;&#38543;&#30528;&#26032;&#30340;&#30740;&#31350;&#30340;&#20986;&#29616;&#32780;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding, language generation, and complex reasoning and have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges.In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we compile the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey, and will actively maintain this repository and incorporate new research as it emerges. We hope our survey can s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2311.15964</link><description>&lt;p&gt;
&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Pre-training for Localized Instruction Generation of Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35270;&#39057;&#23637;&#31034;&#20102;&#35832;&#22914;&#39135;&#35889;&#20934;&#22791;&#31561;&#20219;&#21153;&#30340;&#36880;&#27493;&#28436;&#31034;&#12290;&#29702;&#35299;&#27492;&#31867;&#35270;&#39057;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#23545;&#27493;&#39588;&#36827;&#34892;&#31934;&#30830;&#23450;&#20301;&#24182;&#29983;&#25104;&#25991;&#23383;&#35828;&#26126;&#12290;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#24182;&#32534;&#20889;&#35828;&#26126;&#25104;&#26412;&#39640;&#26114;&#65292;&#36825;&#38480;&#21046;&#20102;&#24403;&#21069;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#24182;&#38459;&#30861;&#20102;&#26377;&#25928;&#23398;&#20064;&#12290;&#21033;&#29992;&#22823;&#35268;&#27169;&#20294;&#22024;&#26434;&#30340;&#35270;&#39057;-&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25991;&#26412;&#36716;&#24405;&#21253;&#21547;&#26080;&#20851;&#20869;&#23481;&#65292;&#19982;&#20154;&#31867;&#27880;&#37322;&#21592;&#32534;&#20889;&#30340;&#35828;&#26126;&#30456;&#27604;&#23384;&#22312;&#39118;&#26684;&#21464;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;Sieve-&amp;-Swap&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#21644;&#20351;&#29992;&#25991;&#26412;&#39135;&#35889;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#33258;&#21160;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#20197;&#22686;&#24378;&#25991;&#23383;&#25351;&#20196;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15964v2 Announce Type: replace-cross  Abstract: Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&amp;-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;Meta-Continual Active Learning&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#35299;&#20915;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2311.03732</link><description>&lt;p&gt;
&#23398;&#20064;&#23398;&#20064;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#25345;&#20037;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn for Few-shot Continual Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;Meta-Continual Active Learning&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#35299;&#20915;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#30830;&#20445;&#35299;&#20915;&#20808;&#21069;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#23637;&#31034;&#23545;&#26032;&#39046;&#22495;&#30340;&#21487;&#22609;&#24615;&#12290;&#26368;&#36817;&#22312;&#25345;&#32493;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#23616;&#38480;&#20110;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65292;&#23588;&#20854;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#65288;CAL&#65289;&#35774;&#32622;&#65292;&#20854;&#20013;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#65292;&#20294;&#26410;&#26631;&#35760;&#25968;&#25454;&#20805;&#36275;&#65292;&#20294;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20803;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#26469;&#35299;&#20915;&#20219;&#21153;&#20043;&#38388;&#30340;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#23569;&#26679;&#26412;CAL&#35774;&#32622;&#20013;&#19981;&#21516;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03732v2 Announce Type: replace-cross  Abstract: Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in CL are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning (CAL) setting where labeled data are inadequate, and unlabeled data are abundant but with a limited annotation budget. We propose a simple but efficient method, called Meta-Continual Active Learning. Specifically, we employ meta-learning and experience replay to address inter-task confusion and catastrophic forgetting. We further incorporate textual augmentations to ensure generalization. We conduct extensive experiments on benchmark text classification datasets to validate the effectiveness of the proposed method and analyze the effect of different active learning strategies in few-shot CAL setting. Our experimental results demonstrate t
&lt;/p&gt;</description></item><item><title>&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20551;&#35774;&#25552;&#20986;&#26041;&#38754;&#34920;&#29616;&#24778;&#20154;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#19968;&#20010;&#65288;&#20219;&#21153;&#29305;&#23450;&#30340;&#65289;&#31526;&#21495;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#31995;&#32479;&#22320;&#36807;&#28388;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.08559</link><description>&lt;p&gt;
&#20196;&#20154;&#24778;&#21497;&#20294;&#20196;&#20154;&#22256;&#24785;&#65306;&#29992;&#20551;&#35774;&#32454;&#21270;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#30340;&#24402;&#32435;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08559
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20551;&#35774;&#25552;&#20986;&#26041;&#38754;&#34920;&#29616;&#24778;&#20154;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#19968;&#20010;&#65288;&#20219;&#21153;&#29305;&#23450;&#30340;&#65289;&#31526;&#21495;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#31995;&#32479;&#22320;&#36807;&#28388;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23569;&#25968;&#35266;&#23519;&#20013;&#25512;&#23548;&#20986;&#28508;&#22312;&#21407;&#21017;&#65292;&#28982;&#21518;&#25512;&#24191;&#21040;&#26032;&#24773;&#20917;&#30340;&#33021;&#21147;&#65292;&#21363;&#24402;&#32435;&#25512;&#29702;&#65292;&#23545;&#20110;&#20154;&#31867;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#22312;&#30740;&#31350;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#24402;&#32435;&#25512;&#29702;&#26041;&#38754;&#24120;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36845;&#20195;&#20551;&#35774;&#32454;&#21270;&#36825;&#19968;&#25216;&#26415;&#23545;LMs&#30340;&#24402;&#32435;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#35813;&#25216;&#26415;&#26356;&#25509;&#36817;&#20154;&#31867;&#24402;&#32435;&#36807;&#31243;&#65292;&#32780;&#19981;&#26159;&#26631;&#20934;&#30340;&#36755;&#20837;-&#36755;&#20986;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08559v3 Announce Type: replace-cross  Abstract: The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26469;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#32780;&#23548;&#33268;RLHF&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16635</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble. (arXiv:2401.16635v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26469;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#32780;&#23548;&#33268;RLHF&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;RLHF&#20381;&#36182;&#20110;&#36890;&#36807;&#26377;&#38480;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;RLHF&#21487;&#33021;&#20135;&#29983;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22870;&#21169;&#27169;&#22411;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#32771;&#34385;&#21040;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#21644;&#36164;&#28304;&#26114;&#36149;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21253;&#25324;&#32447;&#24615;&#23618;&#38598;&#25104;&#21644;&#22522;&#20110;LoRA&#30340;&#38598;&#25104;&#22312;&#20869;&#30340;&#39640;&#25928;&#38598;&#25104;&#26041;&#27861;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#38598;&#25104;&#22870;&#21169;&#27169;&#22411;&#36816;&#34892;Best-of-$n$&#21644;Proximal Policy Optimization&#65292;&#24182;&#39564;&#35777;&#25105;&#20204;&#30340;&#38598;&#25104;&#26041;&#27861;&#26377;&#21161;&#20110;&#25913;&#21892;RLHF&#36755;&#20986;&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is a widely adopted approach for aligning large language models with human values. However, RLHF relies on a reward model that is trained with a limited amount of human preference data, which could lead to inaccurate predictions. As a result, RLHF may produce outputs that are misaligned with human values. To mitigate this issue, we contribute a reward ensemble method that allows the reward model to make more accurate predictions. As using an ensemble of large language model-based reward models can be computationally and resource-expensive, we explore efficient ensemble methods including linear-layer ensemble and LoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy Optimization with our ensembled reward models, and verify that our ensemble methods help improve the alignment performance of RLHF outputs.
&lt;/p&gt;</description></item><item><title>LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13920</link><description>&lt;p&gt;
LocMoE: &#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#20302;&#24320;&#38144;MoE
&lt;/p&gt;
&lt;p&gt;
LocMoE: A Low-overhead MoE for Large Language Model Training. (arXiv:2401.13920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13920
&lt;/p&gt;
&lt;p&gt;
LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65288;MoE&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#20998;&#24067;&#24335;&#21644;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#26377;&#25928;&#31232;&#30095;&#21644;&#25193;&#23637;&#27169;&#22411;&#65292;&#22240;&#27492;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;MoE&#30340;&#24615;&#33021;&#21463;&#21040;&#36127;&#36733;&#19981;&#24179;&#34913;&#21644;&#20840;&#23545;&#20840;&#36890;&#20449;&#30340;&#39640;&#24310;&#36831;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#30001;&#20110;&#22823;&#37327;&#30340;&#19987;&#23478;&#23481;&#37327;&#23548;&#33268;&#30456;&#23545;&#20887;&#20313;&#30340;&#35745;&#31639;&#12290;&#36127;&#36733;&#19981;&#24179;&#34913;&#21487;&#33021;&#26159;&#30001;&#20110;&#29616;&#26377;&#36335;&#30001;&#31574;&#30053;&#22987;&#32456;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#30340;&#19987;&#23478;&#23548;&#33268;&#30340;&#12290;&#20840;&#23545;&#20840;&#36807;&#31243;&#20013;&#39057;&#32321;&#30340;&#33410;&#28857;&#38388;&#36890;&#20449;&#20063;&#26174;&#33879;&#24310;&#38271;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#24615;&#33021;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#19987;&#23478;&#23481;&#37327;&#30340;&#26368;&#23567;&#38408;&#20540;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#30340;&#38376;&#25511;&#26435;&#37325;&#19982;&#20998;&#37197;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#26368;&#22823;&#35282;&#20559;&#24046;&#35745;&#31639;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We por
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27010;&#24565;&#30340;&#39057;&#29575;&#65292;&#24182;&#21457;&#29616;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2401.12425</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#24573;&#35270;&#30340;&#23614;&#37096;
&lt;/p&gt;
&lt;p&gt;
The Neglected Tails of Vision-Language Models. (arXiv:2401.12425v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27010;&#24565;&#30340;&#39057;&#29575;&#65292;&#24182;&#21457;&#29616;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#38646;&#26679;&#26412;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#27010;&#24565;&#19978;&#30340;&#34920;&#29616;&#26497;&#19981;&#22343;&#34913;&#12290;&#20363;&#22914;&#65292;&#23613;&#31649;CLIP&#22312;ImageNet&#19978;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24179;&#22343;&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#65288;72.7&#65285;&#65289;&#65292;&#20294;&#22312;&#21313;&#20010;&#27010;&#24565;&#65288;&#22914;gyromitra&#21644;night snake&#65289;&#19978;&#30340;&#20934;&#30830;&#29575;&#19981;&#21040;10&#65285;&#65292;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#36825;&#20123;&#27010;&#24565;&#22312;VLM&#30340;&#38750;&#22343;&#34913;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#19981;&#36275;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36825;&#31181;&#19981;&#24179;&#34913;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;VLM&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#35745;&#31639;&#29305;&#23450;&#27010;&#24565;&#30340;&#39057;&#29575;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#27979;&#37327;&#27010;&#24565;&#39057;&#29575;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#24110;&#21161;&#35745;&#31639;&#21253;&#21547;&#32473;&#23450;&#27010;&#24565;&#30340;&#21516;&#20041;&#35789;&#30340;&#30456;&#20851;&#25991;&#26412;&#65292;&#24182;&#35299;&#20915;&#35821;&#35328;&#27495;&#20041;&#12290;&#25105;&#20204;&#30830;&#35748;&#20687;LAION&#36825;&#26679;&#30340;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#30830;&#23454;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#24182;&#19988;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#24403;&#20195;&#30340;&#22810;&#27169;&#24335;&#31995;&#32479;&#65292;&#22914;&#35270;&#35273;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#25991;&#26412;-&#35270;&#35273;&#25512;&#29702;&#27169;&#22411;&#65292;&#22312;&#36825;&#31181;&#38271;&#23614;&#20998;&#24067;&#19979;&#32463;&#24120;&#38590;&#20197;&#36798;&#21040;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) excel in zero-shot recognition but exhibit drastically imbalanced performance across visual concepts. For example, CLIP, despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields $&lt;$10% on ten concepts (e.g., gyromitra and night snake), presumably, because these concepts are under-represented in VLMs' imbalanced pretraining data. Yet, assessing this imbalance is challenging as it is non-trivial to calculate the frequency of specific concepts within VLMs' large-scale pretraining data. Our work makes the first attempt to measure the concept frequency by analyzing pretraining texts. We use off-the-shelf language models to help count relevant texts that contain synonyms of the given concepts and resolve linguistic ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit long-tailed concept distributions, which strongly correlate with per-class accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and text-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10463</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#36825;&#26159;&#20174;&#24555;&#36895;&#35760;&#24518;&#21040;&#32531;&#24930;&#27867;&#21270;&#30340;&#19968;&#20010;&#22522;&#26412;&#36716;&#21464;&#38408;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30456;&#21464;&#24418;&#24335;&#21270;&#20026;Grokking&#37197;&#32622;&#19979;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#65292;&#24182;&#30830;&#23450;&#20102;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21160;&#21147;&#23398;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#20805;&#36275;&#21644;&#36807;&#21097;&#38454;&#27573;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#35843;&#25972;&#21021;&#22987;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;Grokking&#37197;&#32622;&#65292;&#31283;&#23450;&#22320;&#22312;&#31616;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#37325;&#29616;&#20102;Grokking&#12290;&#25105;&#20204;&#34920;&#26126;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26679;&#26412;&#32423;&#21644;&#27169;&#22411;&#32423;&#30340;Grokking&#65292;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#25968;&#25454;&#38598;&#22823;&#23567;&#22788;&#21457;&#29983;&#30340;&#26356;&#24179;&#28369;&#30340;&#30456;&#21464;&#12290;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#36825;&#20010;&#20020;&#30028;&#28857;&#20063;&#21464;&#24471;&#26356;&#22823;&#65292;&#36825;&#34920;&#26126;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21152;&#28145;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#29702;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel pers
&lt;/p&gt;</description></item><item><title>ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.10225</link><description>&lt;p&gt;
ChatQA: &#26500;&#24314;GPT-4&#32423;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10225
&lt;/p&gt;
&lt;p&gt;
ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatQA&#65292;&#19968;&#31995;&#21015;&#20855;&#26377;GPT-4&#32423;&#21035;&#20934;&#30830;&#24615;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22788;&#29702;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22810;&#36718;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#24494;&#35843;&#65292;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ChatQA-70B&#21487;&#20197;&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#19978;&#36229;&#36807;GPT-4&#65288;54.14 vs. 53.90&#65289;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;OpenAI GPT&#27169;&#22411;&#30340;&#20219;&#20309;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32593;&#32476;&#21644;&#35821;&#35328;&#20998;&#26512;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#22312;&#32447;&#31038;&#32676;&#20013;&#25512;&#24191;&#39278;&#39135;&#32010;&#20081;&#30340;&#21160;&#24577;&#65292;&#35748;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25918;&#22823;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20998;&#26512;&#31038;&#32676;&#20869;&#30340;&#35805;&#35821;&#65292;&#25105;&#20204;&#25506;&#27979;&#21040;&#20102;&#19982;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#28508;&#22312;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.09647</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#24449;&#22312;&#32447;&#39278;&#39135;&#32010;&#20081;&#31038;&#32676;
&lt;/p&gt;
&lt;p&gt;
Characterizing Online Eating Disorder Communities with Large Language Models. (arXiv:2401.09647v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09647
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32593;&#32476;&#21644;&#35821;&#35328;&#20998;&#26512;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#22312;&#32447;&#31038;&#32676;&#20013;&#25512;&#24191;&#39278;&#39135;&#32010;&#20081;&#30340;&#21160;&#24577;&#65292;&#35748;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25918;&#22823;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20998;&#26512;&#31038;&#32676;&#20869;&#30340;&#35805;&#35821;&#65292;&#25105;&#20204;&#25506;&#27979;&#21040;&#20102;&#19982;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#28508;&#22312;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39278;&#39135;&#32010;&#20081;&#20316;&#20026;&#19968;&#31181;&#21361;&#38505;&#30340;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#27515;&#20129;&#29575;&#21644;&#21457;&#30149;&#29575;&#65292;&#20854;&#19978;&#21319;&#19982;&#31038;&#20132;&#23186;&#20307;&#19978;&#29702;&#24819;&#21270;&#36523;&#20307;&#24418;&#35937;&#30340;&#27867;&#28389;&#26377;&#20851;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#23186;&#20307;&#19982;&#39278;&#39135;&#32010;&#20081;&#20043;&#38388;&#30340;&#32852;&#31995;&#36828;&#19981;&#27490;&#22914;&#27492;&#12290;&#25105;&#20204;&#35748;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21019;&#24314;&#20102;&#19968;&#20010;&#21453;&#39304;&#24490;&#29615;&#65292;&#25918;&#22823;&#20102;&#25512;&#24191;&#21388;&#39135;&#30151;&#21644;&#26292;&#39135;&#30151;&#31561;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#21644;&#31038;&#32676;&#30340;&#22686;&#38271;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20351;&#26131;&#21463;&#20260;&#23475;&#30340;&#20010;&#20307;&#33021;&#22815;&#36731;&#26494;&#25214;&#21040;&#24182;&#32852;&#31995;&#21040;&#24535;&#21516;&#36947;&#21512;&#30340;&#20854;&#20182;&#20154;&#65292;&#32780;&#32676;&#20307;&#21160;&#24577;&#36807;&#31243;&#21017;&#40723;&#21169;&#20182;&#20204;&#22312;&#25512;&#24191;&#21644;&#32654;&#21270;&#19982;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#26377;&#23475;&#34892;&#20026;&#30340;&#31038;&#32676;&#20013;&#25345;&#32493;&#21442;&#19982;&#12290;&#25105;&#20204;&#36890;&#36807;&#32593;&#32476;&#21644;&#35821;&#35328;&#20998;&#26512;&#30340;&#32452;&#21512;&#65292;&#20174;&#32463;&#39564;&#19978;&#25551;&#36848;&#20102;&#36825;&#19968;&#21160;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#22312;&#32447;&#31038;&#32676;&#20869;&#30340;&#35805;&#35821;&#65292;&#24182;&#23545;&#19982;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#35805;&#39064;&#30340;&#24577;&#24230;&#36827;&#34892;&#25506;&#27979;&#65292;&#20197;&#37492;&#21035;&#28508;&#22312;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise in eating disorders, a dangerous mental health condition with high mortality and morbidity, has been linked to the proliferation of idealized body images on social media. However, the link between social media and eating disorders is far more complex. We argue that social media platforms create a feedback loop that amplifies the growth of content and communities that promote eating disorders like anorexia and bulimia. Specifically, social media platforms make it easy for vulnerable individuals to find and connect to like-minded others, while group dynamic processes encourage them to stay engaged within communities that promote and glorify harmful behaviors linked to eating disorders. We characterize this dynamic empirically through a combination of network and language analysis. We describe a novel framework that leverages large language models to analyze the discourse within online communities and probe their attitudes on topics related to eating disorders to identify potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#32763;&#35793;&#26041;&#21521;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20854;&#22312;&#39640;&#36127;&#36733;&#35821;&#35328;&#23545;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#35770;&#25991;&#26631;&#39064;&#20026;&#8220;Machine Translation Models are Zero-Shot Detectors of Translation Direction&#8221;&#12290;</title><link>http://arxiv.org/abs/2401.06769</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#26159;&#38646;&#23556;&#20987;&#30340;&#32763;&#35793;&#26041;&#21521;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Translation Models are Zero-Shot Detectors of Translation Direction. (arXiv:2401.06769v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#32763;&#35793;&#26041;&#21521;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20854;&#22312;&#39640;&#36127;&#36733;&#35821;&#35328;&#23545;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#35770;&#25991;&#26631;&#39064;&#20026;&#8220;Machine Translation Models are Zero-Shot Detectors of Translation Direction&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#24182;&#34892;&#25991;&#26412;&#30340;&#32763;&#35793;&#26041;&#21521;&#23545;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#65292;&#20294;&#20063;&#20855;&#26377;&#27861;&#21307;&#24212;&#29992;&#65292;&#20363;&#22914;&#35299;&#20915;&#21117;&#31363;&#25110;&#20266;&#36896;&#25351;&#25511;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#19968;&#20010;&#31616;&#21333;&#30340;&#20551;&#35774;&#65292;&#21363;$p(\text{translation}|\text{original})&gt;p(\text{original}|\text{translation})$&#65292;&#20197;&#20256;&#32479;&#19978;&#34987;&#31216;&#20026;&#32763;&#35793;&#35821;&#25110;&#26426;&#22120;&#32763;&#35793;&#35821;&#20013;&#30340;&#31616;&#21270;&#25928;&#24212;&#20026;&#21160;&#26426;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#32763;&#35793;&#26041;&#21521;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;20&#20010;&#32763;&#35793;&#26041;&#21521;&#36827;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#23545;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;&#20110;NMT&#29983;&#25104;&#30340;&#32763;&#35793;&#65292;&#23454;&#29616;&#20102;&#25991;&#26723;&#32423;&#20934;&#30830;&#29575;&#20026;82-96&#65285;&#65292;&#23545;&#20110;&#20154;&#24037;&#32763;&#35793;&#65292;&#26681;&#25454;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;60-81&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#20195;&#30721;&#21644;&#28436;&#31034;&#21487;&#22312;https://github.com/ZurichNLP/translation-direction-detection&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting the translation direction of parallel text has applications for machine translation training and evaluation, but also has forensic applications such as resolving plagiarism or forgery allegations. In this work, we explore an unsupervised approach to translation direction detection based on the simple hypothesis that $p(\text{translation}|\text{original})&gt;p(\text{original}|\text{translation})$, motivated by the well-known simplification effect in translationese or machine-translationese. In experiments with massively multilingual machine translation models across 20 translation directions, we confirm the effectiveness of the approach for high-resource language pairs, achieving document-level accuracies of 82-96% for NMT-produced translations, and 60-81% for human translations, depending on the model used. Code and demo are available at https://github.com/ZurichNLP/translation-direction-detection
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#36817;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#27169;&#22411;&#25512;&#26029;&#20986;&#20195;&#29702;&#21464;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.06687</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#36817;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Proximal Causal Inference With Text Data. (arXiv:2401.06687v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#36817;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#27169;&#22411;&#25512;&#26029;&#20986;&#20195;&#29702;&#21464;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#22240;&#26524;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#20316;&#20026;&#20542;&#21521;&#20110;&#21253;&#21547;&#37096;&#20998;&#25110;&#19981;&#23436;&#20840;&#27979;&#37327;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#20195;&#29702;&#26469;&#20943;&#36731;&#28151;&#28102;&#20559;&#24046;&#12290;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#20998;&#26512;&#20154;&#21592;&#22312;&#19968;&#37096;&#20998;&#23454;&#20363;&#30340;&#25991;&#26412;&#20013;&#20855;&#26377;&#26377;&#30417;&#30563;&#30340;&#28151;&#28102;&#21464;&#37327;&#26631;&#31614;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#25110;&#25104;&#26412;&#65292;&#36825;&#31181;&#32422;&#26463;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#23558;&#22788;&#29702;&#21069;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#38646;&#26679;&#26412;&#27169;&#22411;&#20174;&#20998;&#21106;&#30340;&#20004;&#20010;&#37096;&#20998;&#25512;&#26029;&#20986;&#20004;&#20010;&#20195;&#29702;&#65292;&#24182;&#23558;&#36825;&#20123;&#20195;&#29702;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#30340;&#20195;&#29702;&#26041;&#27861;&#28385;&#36275;&#36817;&#37051; g-formula&#25152;&#38656;&#30340;&#35782;&#21035;&#26465;&#20214;&#65292;&#32780;&#20854;&#20182;&#30475;&#20284;&#21512;&#29702;&#30340;&#25552;&#35758;&#21017;&#19981;&#28385;&#36275;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-based causal methods attempt to mitigate confounding bias by including unstructured text data as proxies of confounding variables that are partially or imperfectly measured. These approaches assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is not always feasible due to data privacy or cost. Here, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that splits pre-treatment text data, infers two proxies from two zero-shot models on the separate splits, and applies these proxies in the proximal g-formula. We prove that our text-based proxy method satisfies identification conditions required by the proximal g-formula while other seemingly reasonable proposals do not. We evaluate our method in synthetic and semi-synthetic settings and find that it produces estimates with low bias. This combination of proximal causal inference and zero-sh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TransliCo&#65292;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#33050;&#26412;&#38556;&#30861;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#33050;&#26412;&#30340;&#21477;&#23376;&#21450;&#20854;&#22312;&#32479;&#19968;&#33050;&#26412;&#20013;&#30340;&#38899;&#35793;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#33050;&#26412;&#30340;&#32479;&#19968;&#34920;&#31034;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#36328;&#35821;&#35328;&#20256;&#36882;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06620</link><description>&lt;p&gt;
TransliCo&#65306;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#33050;&#26412;&#38556;&#30861;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models. (arXiv:2401.06620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TransliCo&#65292;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#33050;&#26412;&#38556;&#30861;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#33050;&#26412;&#30340;&#21477;&#23376;&#21450;&#20854;&#22312;&#32479;&#19968;&#33050;&#26412;&#20013;&#30340;&#38899;&#35793;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#33050;&#26412;&#30340;&#32479;&#19968;&#34920;&#31034;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#36328;&#35821;&#35328;&#20256;&#36882;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20070;&#38754;&#24418;&#24335;&#20013;&#26377;293&#31181;&#33050;&#26412;&#20195;&#34920;&#30528;7000&#22810;&#31181;&#35821;&#35328;&#12290;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#35768;&#22810;&#23494;&#20999;&#30456;&#20851;&#30340;&#35821;&#35328;&#20351;&#29992;&#19981;&#21516;&#30340;&#33050;&#26412;&#65292;&#36825;&#32473;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;mPLMs&#65289;&#36890;&#36807;&#35789;&#27719;&#37325;&#21472;&#23398;&#20064;&#36328;&#35821;&#35328;&#30693;&#35782;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;mPLMs&#23384;&#22312;&#33050;&#26412;&#38556;&#30861;&#65306;&#26469;&#33258;&#19981;&#21516;&#33050;&#26412;&#30340;&#34920;&#31034;&#20301;&#20110;&#19981;&#21516;&#23376;&#31354;&#38388;&#20013;&#65292;&#36825;&#26159;&#20026;&#20160;&#20040;&#28041;&#21450;&#19981;&#21516;&#33050;&#26412;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#26174;&#31034;&#27425;&#20248;&#24615;&#33021;&#30340;&#24378;&#26377;&#21147;&#25351;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;TransliCo&#65292;&#23427;&#21253;&#21547;Transliteration Contrastive Modeling&#65288;TCM&#65289;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#21477;&#23376;&#21450;&#20854;&#22312;&#32479;&#19968;&#33050;&#26412;&#65288;&#22312;&#25105;&#20204;&#30340;&#26696;&#20363;&#20013;&#26159;&#25289;&#19969;&#23383;&#27597;&#65289;&#20013;&#30340;&#38899;&#35793;&#36827;&#34892;&#23545;&#27604;&#65292;&#26469;&#24494;&#35843;mPLM&#65292;&#20174;&#32780;&#30830;&#20445;&#19981;&#21516;&#33050;&#26412;&#30340;&#34920;&#31034;&#31354;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#20351;&#29992;Glot500-m&#20316;&#20026;&#25105;&#20204;&#30340;&#28304;&#27169;&#22411;&#65292;&#23427;&#26159;&#22312;500&#22810;&#31181;&#35821;&#35328;&#19978;&#39044;&#35757;&#32451;&#30340;mPLM&#65292;&#25105;&#20204;&#23558;&#20854;&#22312;&#20854;5&#65285;&#30340;&#23567;&#37096;&#20998;&#19978;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
There are 293 scripts representing over 7,000 languages in the written form. Due to various reasons, many closely related languages use different scripts, which poses difficulty for multilingual pretrained language models (mPLMs) in learning crosslingual knowledge through lexical overlap. As a result, mPLMs present a script barrier: representations from different scripts are located in different subspaces, which is a strong indicator of why crosslingual transfer involving languages of different scripts shows sub-optimal performance. To address this problem, we propose a simple framework TransliCo that contains Transliteration Contrastive Modeling (TCM) to fine-tune an mPLM by contrasting sentences in its training data and their transliterations in a unified script (Latn, in our case), which ensures uniformity in the representation space for different scripts. Using Glot500-m, an mPLM pretrained on over 500 languages, as our source model, we find-tune it on a small portion (5\%) of its 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#65288;EDiSC&#65289;&#65292;&#32467;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;DiSC&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;EDiSC&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00541</link><description>&lt;p&gt;
&#19968;&#20010;&#24102;&#26377;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#30340;&#35770;&#25991;&#19982;&#19968;&#20010;&#20851;&#20110;&#21476;&#24076;&#33098;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek. (arXiv:2311.00541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#65288;EDiSC&#65289;&#65292;&#32467;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;DiSC&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;EDiSC&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#30340;&#24847;&#20041;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#21464;&#21270;&#65292;&#35789;&#20041;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#20250;&#28436;&#21464;&#12289;&#20986;&#29616;&#25110;&#28040;&#22833;&#12290;&#23545;&#20110;&#21476;&#20195;&#35821;&#35328;&#26469;&#35828;&#65292;&#30001;&#20110;&#35821;&#26009;&#24211;&#36890;&#24120;&#36739;&#23567;&#12289;&#31232;&#30095;&#19988;&#22024;&#26434;&#65292;&#20934;&#30830;&#24314;&#27169;&#36825;&#31181;&#21464;&#21270;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#27492;&#23545;&#20110;&#24847;&#20041;&#21464;&#21270;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#21464;&#24471;&#37325;&#35201;&#12290;GASC&#21644;DiSC&#26159;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#32463;&#34987;&#29992;&#26469;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#65292;&#20351;&#29992;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#24182;&#27809;&#26377;&#20511;&#21161;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#24110;&#21161;&#12290;&#36825;&#20123;&#27169;&#22411;&#23558;&#32473;&#23450;&#30446;&#26631;&#35789;&#27719;&#65288;&#22914;"kosmos"&#65292;&#24847;&#20026;&#35013;&#39280;&#12289;&#31209;&#24207;&#25110;&#19990;&#30028;&#65289;&#30340;&#24847;&#20041;&#34920;&#31034;&#20026;&#19978;&#19979;&#25991;&#35789;&#27719;&#30340;&#20998;&#24067;&#65292;&#24182;&#23558;&#24847;&#20041;&#30340;&#26222;&#36941;&#24615;&#34920;&#31034;&#20026;&#24847;&#20041;&#30340;&#20998;&#24067;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#36827;&#34892;&#25311;&#21512;&#65292;&#20197;&#27979;&#37327;&#36825;&#20123;&#34920;&#31034;&#20013;&#30340;&#26102;&#38388;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EDiSC&#65292;&#36825;&#26159;DiSC&#30340;&#23884;&#20837;&#29256;&#26412;&#65292;&#23427;&#23558;&#35789;&#23884;&#20837;&#19982;DiSC&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#26356;&#20248;&#31168;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;EDiSC&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small, sparse and noisy, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC and DiSC are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as "kosmos" (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using MCMC methods to measure temporal changes in these representations. In this paper, we introduce EDiSC, an embedded version of DiSC, which combines word embeddings with DiSC to provide superior model performance. We show empirically that EDiSC offers improved p
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;</title><link>http://arxiv.org/abs/2310.17688</link><description>&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#26102;&#20195;&#31649;&#29702;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17688
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#31616;&#30701;&#30340;&#20849;&#35782;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#22823;&#35268;&#27169;&#30340;&#31038;&#20250;&#21361;&#23475;&#21644;&#24694;&#24847;&#20351;&#29992;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22833;&#21435;&#25511;&#21046;&#30340;&#19981;&#21487;&#36870;&#36716;&#30340;&#25439;&#22833;&#12290;&#37492;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21644;&#25345;&#32493;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#21457;&#21644;&#27835;&#29702;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&amp;D and governance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11085</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25512;&#26029;&#32467;&#26500;&#21270;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#26377;&#20004;&#20010;&#38480;&#21046;&#65306;(1)&#23427;&#20204;&#35201;&#27714;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#36755;&#20837;&#25110;&#25512;&#26029;&#23427;&#20204;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22122;&#22768;&#65292;(2)&#23427;&#20204;&#38656;&#35201;&#20154;&#24037;&#23545;&#25991;&#26723;&#36827;&#34892;&#27880;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#30340;&#30740;&#31350;&#32773;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#22312;&#28040;&#38500;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#20851;&#38190;&#24615;&#30340;&#20248;&#21183;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;DocRED&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
&lt;/p&gt;</description></item><item><title>CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.08753</link><description>&lt;p&gt;
CompA: &#35299;&#20915;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models. (arXiv:2310.08753v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08753
&lt;/p&gt;
&lt;p&gt;
CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#30340;&#22522;&#26412;&#29305;&#24615;&#26159;&#20854;&#32452;&#21512;&#24615;&#12290;&#20351;&#29992;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;CLAP&#65289;&#35757;&#32451;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#33021;&#22815;&#23398;&#20064;&#38899;&#39057;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#12289;&#38899;&#39057;&#26816;&#32034;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26377;&#25928;&#25191;&#34892;&#32452;&#21512;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#36824;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CompA&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#30495;&#23454;&#19990;&#30028;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;CompA-order&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#25110;&#21457;&#29983;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#32780;CompA-attribute&#35780;&#20272;&#22768;&#38899;&#20107;&#20214;&#30340;&#23646;&#24615;&#32465;&#23450;&#12290;&#27599;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#21253;&#21547;&#20004;&#20010;&#38899;&#39057;-&#26631;&#39064;&#23545;&#65292;&#20854;&#20013;&#20004;&#20010;&#38899;&#39057;&#20855;&#26377;&#30456;&#21516;&#30340;&#22768;&#38899;&#20107;&#20214;&#65292;&#20294;&#32452;&#21512;&#26041;&#24335;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Parrot&#65292;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#23436;&#21892;&#20102;&#22810;&#36718;&#32842;&#22825;&#27169;&#22411;&#22312;&#23545;&#35805;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07301</link><description>&lt;p&gt;
Parrot:&#36890;&#36807;&#23398;&#20064;&#25552;&#38382;&#26469;&#22686;&#24378;&#22810;&#36718;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions. (arXiv:2310.07301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Parrot&#65292;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#23436;&#21892;&#20102;&#22810;&#36718;&#32842;&#22825;&#27169;&#22411;&#22312;&#23545;&#35805;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65307;&#28982;&#32780;&#65292;&#22312;&#24320;&#28304;&#32842;&#22825;&#27169;&#22411;&#65288;&#22914;Alpaca&#21644;Vicuna&#65289;&#19982;&#39046;&#20808;&#30340;&#32842;&#22825;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;GPT-4&#65289;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#22810;&#36718;&#23545;&#35805;&#28382;&#21518;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;&#28382;&#21518;&#24402;&#22240;&#20110;&#32570;&#20047;&#36275;&#22815;&#39640;&#36136;&#37327;&#30340;&#22810;&#36718;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#12290;&#31038;&#21306;&#25552;&#20379;&#30340;&#35843;&#20248;&#25968;&#25454;&#35201;&#20040;&#26159;&#21333;&#36718;&#20250;&#35805;&#65292;&#35201;&#20040;&#26159;&#23384;&#22312;&#26576;&#20123;&#38382;&#39064;&#30340;&#22810;&#36718;&#20250;&#35805;&#65292;&#20363;&#22914;&#38750;&#20154;&#31867;&#30340;&#25351;&#20196;&#65292;&#21709;&#24212;&#19981;&#22815;&#35814;&#32454;&#65292;&#25110;&#32773;&#24456;&#23569;&#20986;&#29616;&#20027;&#39064;&#36716;&#25442;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;Parrot&#65292;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#65292;&#28982;&#21518;&#29992;&#20110;&#22686;&#24378;&#22810;&#36718;&#32842;&#22825;&#27169;&#22411;&#22312;&#23545;&#35805;&#20013;&#30340;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;Parrot-Ask&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26088;&#22312;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#29983;&#25104;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Impressive progress has been made on chat models based on Large Language Models (LLMs) recently; however, there is a noticeable lag in multi-turn conversations between open-source chat models (e.g., Alpaca and Vicuna) and the leading chat models (e.g., ChatGPT and GPT-4). Through a series of analyses, we attribute the lag to the lack of enough high-quality multi-turn instruction-tuning data. The available instruction-tuning data for the community are either single-turn conversations or multi-turn ones with certain issues, such as non-human-like instructions, less detailed responses, or rare topic shifts. In this paper, we address these challenges by introducing Parrot, a highly scalable solution designed to automatically generate high-quality instruction-tuning data, which are then used to enhance the effectiveness of chat models in multi-turn conversations. Specifically, we start by training the Parrot-Ask model, which is designed to emulate real users in generating instructions. We t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.05516</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;LLMs&#37327;&#21270;&#20013;&#30340;&#26435;&#37325;&#33293;&#20837;
&lt;/p&gt;
&lt;p&gt;
Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25191;&#34892;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;3&#20301;&#21644;4&#20301;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#24050;&#32463;&#25104;&#20026;&#26368;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#38543;&#30528;&#20301;&#25968;&#30340;&#20943;&#23569;&#65292;&#37327;&#21270;&#32593;&#26684;&#21464;&#24471;&#26356;&#21152;&#23485;&#27867;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19978;&#19979;&#33293;&#20837;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#32454;&#35843;&#19978;&#19979;&#33293;&#20837;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21463;&#21046;&#20110;&#36825;&#20123;&#25200;&#21160;&#30340;&#31934;&#30830;&#19988;&#26377;&#38480;&#30340;&#36793;&#30028;&#65292;&#21482;&#26377;&#25913;&#21464;&#33293;&#20837;&#20540;&#30340;&#38408;&#20540;&#25165;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#39640;&#25928;&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;SignRound&#65292;&#23427;&#28041;&#21450;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#30340;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#29983;&#25104;&#35843;&#30740;&#25991;&#31456;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;GPT-4&#20248;&#20110;GPT-3.5&#65292;&#24182;&#19988;&#25351;&#20986;&#20102;GPT&#22312;&#20449;&#24687;&#23436;&#25972;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#19968;&#20123;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2308.10410</link><description>&lt;p&gt;
&#22522;&#20110;&#32500;&#22522;&#30334;&#31185;&#39118;&#26684;&#30340;&#35843;&#30740;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27010;&#24565;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on Wikipedia-Style Survey Generation: an Evaluation in NLP Concepts. (arXiv:2308.10410v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#29983;&#25104;&#35843;&#30740;&#25991;&#31456;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;GPT-4&#20248;&#20110;GPT-3.5&#65292;&#24182;&#19988;&#25351;&#20986;&#20102;GPT&#22312;&#20449;&#24687;&#23436;&#25972;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#19968;&#20123;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#12290;&#34429;&#28982;LLMs&#22312;&#19968;&#33324;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#29305;&#23450;&#39046;&#22495;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#20173;&#22312;&#25506;&#32034;&#20013;&#12290;&#27492;&#22806;&#65292;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26377;&#26102;&#20250;&#20986;&#29616;&#24187;&#35273;&#21644;&#19981;&#23454;&#20449;&#24687;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;-NLP&#39046;&#22495;&#20013;&#29983;&#25104;&#31616;&#27905;&#35843;&#30740;&#25991;&#31456;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;20&#20010;&#36873;&#23450;&#30340;&#20027;&#39064;&#12290;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;GPT-4&#22312;&#19982;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#20248;&#20110;GPT-3.5&#12290;&#27492;&#22806;&#65292;&#22235;&#20301;&#20154;&#31867;&#35780;&#20272;&#32773;&#20174;&#22235;&#20010;&#27169;&#22411;&#37197;&#32622;&#30340;&#20845;&#20010;&#35282;&#24230;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#34429;&#28982;GPT&#36890;&#24120;&#33021;&#20135;&#29983;&#21487;&#31216;&#36190;&#30340;&#32467;&#26524;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#22914;&#20449;&#24687;&#19981;&#23436;&#25972;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved significant success across various natural language processing (NLP) tasks, encompassing question-answering, summarization, and machine translation, among others. While LLMs excel in general tasks, their efficacy in domain-specific applications remains under exploration. Additionally, LLM-generated text sometimes exhibits issues like hallucination and disinformation. In this study, we assess LLMs' capability of producing concise survey articles within the computer science-NLP domain, focusing on 20 chosen topics. Automated evaluations indicate that GPT-4 outperforms GPT-3.5 when benchmarked against the ground truth. Furthermore, four human evaluators provide insights from six perspectives across four model configurations. Through case studies, we demonstrate that while GPT often yields commendable results, there are instances of shortcomings, such as incomplete information and the exhibition of lapses in factual accuracy.
&lt;/p&gt;</description></item><item><title>RecycleGPT&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22238;&#25910;&#39044;&#29983;&#25104;&#30340;&#27169;&#22411;&#29366;&#24577;&#32780;&#26080;&#38656;&#22810;&#27425;&#36816;&#34892;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#38477;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#39640;&#36895;&#35299;&#30721;&#21644;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03421</link><description>&lt;p&gt;
RecycleGPT&#65306;&#19968;&#31181;&#20855;&#26377;&#21487;&#22238;&#25910;&#27169;&#22359;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RecycleGPT: An Autoregressive Language Model with Recyclable Module. (arXiv:2308.03421v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03421
&lt;/p&gt;
&lt;p&gt;
RecycleGPT&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22238;&#25910;&#39044;&#29983;&#25104;&#30340;&#27169;&#22411;&#29366;&#24577;&#32780;&#26080;&#38656;&#22810;&#27425;&#36816;&#34892;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#38477;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#39640;&#36895;&#35299;&#30721;&#21644;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24517;&#39035;&#36816;&#34892;K&#27425;&#25165;&#33021;&#29983;&#25104;K&#20010;&#20196;&#29260;&#30340;&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RecycleGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#35299;&#30721;&#36895;&#24230;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22238;&#25910;&#39044;&#29983;&#25104;&#30340;&#27169;&#22411;&#29366;&#24577;&#32780;&#26080;&#38656;&#23558;&#25972;&#20010;&#27169;&#22411;&#36816;&#34892;&#22810;&#27425;&#27493;&#39588;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#65306;&#24207;&#21015;&#20013;&#30456;&#37051;&#30340;&#20196;&#29260;&#36890;&#24120;&#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#21069;&#38754;&#30340;&#20196;&#29260;&#21512;&#29702;&#29468;&#27979;&#25110;&#25512;&#26029;&#20986;&#19979;&#19968;&#20010;&#20196;&#29260;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38477;&#20302;&#25512;&#29702;&#24310;&#36831;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;1.4&#20493;&#30340;&#21152;&#36895;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing large language models have to run K times to generate a sequence of K tokens. In this paper, we present RecycleGPT, a generative language model with fast decoding speed by recycling pre-generated model states without running the whole model in multiple steps. Our approach relies on the observation that adjacent tokens in a sequence usually have strong correlations and the next token in a sequence can be reasonably guessed or inferred based on the preceding ones. Experiments and analysis demonstrate the effectiveness of our approach in lowering inference latency, achieving up to 1.4x speedup while preserving high performance.
&lt;/p&gt;</description></item><item><title>LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.01413</link><description>&lt;p&gt;
LaFiCMIL&#65306;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22823;&#25991;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning. (arXiv:2308.01413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01413
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#31361;&#30772;&#12290;&#30452;&#35266;&#19978;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#26399;&#26395;&#25991;&#26412;&#20998;&#31867;&#65292;&#20316;&#20026;&#19981;&#38656;&#35201;&#20687;&#29983;&#25104;&#20219;&#21153;&#37027;&#26679;&#35768;&#22810;&#39640;&#32423;&#34920;&#31034;&#30340;&#20219;&#21153;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;Transformer&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#36827;&#34892;&#32508;&#21512;&#24615;&#30340;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#65292;&#22312;&#22810;&#31867;&#21035;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#38271;&#25991;&#26412;&#25991;&#26723;&#21644;&#20854;&#20182;&#22823;&#25991;&#20214;&#30340;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#36739;&#22823;&#30340;&#25913;&#36827;&#28508;&#21147;&#12290;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#20027;&#35201;&#21463;&#21040;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#30340;&#38459;&#30861;&#65306;&#26377;&#38480;&#30340;&#36755;&#20837;&#38271;&#24230;&#65292;&#27604;&#22914;BERT&#30340;512&#20010;&#26631;&#35760;&#12290;&#34429;&#28982;&#22686;&#21152;GPU&#20869;&#23384;&#21487;&#20197;&#31245;&#24494;&#25193;&#23637;&#36825;&#20010;&#38480;&#21046;&#65292;&#20294;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;GPU&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#36755;&#20837;&#38480;&#21046;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;LaFiCMIL&#65292;&#20316;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have revolutionized the performance of a wide range of language tasks. Intuitively, one might expect text classification, which does not necessitate as many high-level representations as generative tasks, to be comprehensively addressed with the powerful representation capabilities of Transformers. However, in reality, there remains significant potential for enhancement, particularly in the areas of multi-class and multi-label classification of lengthy textual documents and other large files. The performance of Transformer-based models is mainly hindered by a major limitation: a restricted input length, e.g., 512 tokens for BERT. While an increase in GPU memory can marginally extend this limit, practical real-world applications often operate under constrained GPU resources. In this work, we tackle the input limit problem from the perspective of correlated multiple instance learning. The proposed approach, LaFiCMIL, serves as a versatile framework applicable to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#35774;&#35745;&#30340;&#30456;&#20851;&#35201;&#32032;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#29305;&#24449;&#12289;&#25903;&#25345;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26500;&#24314;&#21333;&#29420;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#23545;&#35805;&#20219;&#21153;&#26159;&#26114;&#36149;&#19988;&#20887;&#20313;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.07255</link><description>&lt;p&gt;
&#23545;&#35805;&#20195;&#29702;101&#65306;&#35774;&#35745;&#26377;&#25928;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#20851;&#38190;&#35201;&#32032;&#21021;&#23398;&#32773;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems. (arXiv:2307.07255v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#35774;&#35745;&#30340;&#30456;&#20851;&#35201;&#32032;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#29305;&#24449;&#12289;&#25903;&#25345;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26500;&#24314;&#21333;&#29420;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#23545;&#35805;&#20219;&#21153;&#26159;&#26114;&#36149;&#19988;&#20887;&#20313;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19982;&#21516;&#34892;&#36827;&#34892;&#20132;&#27969;&#26469;&#20998;&#20139;&#24819;&#27861;&#26159;&#20154;&#31867;&#20114;&#21160;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#22240;&#27492;&#65292;&#22312;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#23548;&#33268;&#23545;&#35805;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#21644;&#22810;&#26679;&#24615;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#20010;&#20219;&#21153;&#21516;&#26102;&#25506;&#32034;&#65292;&#24403;&#21069;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#29616;&#29366;&#21464;&#24471;&#20998;&#25955;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24110;&#21161;&#20174;&#38646;&#24320;&#22987;&#35774;&#35745;&#23545;&#35805;&#20195;&#29702;&#30340;&#20174;&#19994;&#32773;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#29305;&#24449;&#12289;&#25903;&#25345;&#20219;&#21153;&#12289;&#30456;&#24212;&#30340;&#24320;&#25918;&#39046;&#22495;&#25968;&#25454;&#38598;&#20197;&#21450;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#30340;&#23545;&#35805;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#26500;&#24314;&#21333;&#29420;&#30340;&#27169;&#22411;&#26159;&#26114;&#36149;&#19988;&#20887;&#20313;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharing ideas through communication with peers is the primary mode of human interaction. Consequently, extensive research has been conducted in the area of conversational AI, leading to an increase in the availability and diversity of conversational tasks, datasets, and methods. However, with numerous tasks being explored simultaneously, the current landscape of conversational AI becomes fragmented. Therefore, initiating a well-thought-out model for a dialogue agent can pose significant challenges for a practitioner. Towards highlighting the critical ingredients needed for a practitioner to design a dialogue agent from scratch, the current study provides a comprehensive overview of the primary characteristics of a dialogue agent, the supporting tasks, their corresponding open-domain datasets, and the methods used to benchmark these datasets. We observe that different methods have been used to tackle distinct dialogue tasks. However, building separate models for each task is costly and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#31934;&#24515;&#21046;&#20316;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#24341;&#23548;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#25805;&#20316;&#20219;&#21153;&#29305;&#23450;&#23646;&#24615;&#24182;&#37325;&#26500;&#26032;&#30340;&#21477;&#23376;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26631;&#31614;&#20132;&#25442;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;LLMs&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#32467;&#26524;&#36824;&#26174;&#31034;&#20102;&#36890;&#36807;LLM&#24341;&#23548;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#21363;&#20351;&#22312;&#26356;&#23569;&#30340;&#30417;&#30563;&#24773;&#20917;&#19979;&#20063;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.07099</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;LLM&#30340;&#23646;&#24615;&#25805;&#20316;&#29983;&#25104;&#39640;&#25928;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating Efficient Training Data via LLM-based Attribute Manipulation. (arXiv:2307.07099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#31934;&#24515;&#21046;&#20316;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#24341;&#23548;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#25805;&#20316;&#20219;&#21153;&#29305;&#23450;&#23646;&#24615;&#24182;&#37325;&#26500;&#26032;&#30340;&#21477;&#23376;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26631;&#31614;&#20132;&#25442;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;LLMs&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#32467;&#26524;&#36824;&#26174;&#31034;&#20102;&#36890;&#36807;LLM&#24341;&#23548;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#21363;&#20351;&#22312;&#26356;&#23569;&#30340;&#30417;&#30563;&#24773;&#20917;&#19979;&#20063;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#38142;&#24335;&#24605;&#32500;&#23646;&#24615;&#25805;&#20316;&#65288;CoTAM&#65289;&#65292;&#36890;&#36807;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#31934;&#24515;&#21046;&#20316;&#30340;&#25968;&#25454;&#26469;&#24341;&#23548;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#20165;&#23545;&#20219;&#21153;&#30446;&#26631;&#23646;&#24615;&#36827;&#34892;&#26356;&#25913;&#24182;&#21019;&#24314;&#25968;&#25454;&#12290;&#21463;&#21040;&#38754;&#37096;&#23646;&#24615;&#25805;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#26469;&#25805;&#20316;&#20219;&#21153;&#29305;&#23450;&#23646;&#24615;&#24182;&#20197;&#21463;&#25511;&#30340;&#26041;&#24335;&#37325;&#26500;&#26032;&#30340;&#21477;&#23376;&#65292;&#20174;&#32780;&#29983;&#25104;&#26631;&#31614;&#20132;&#25442;&#25968;&#25454;&#12290;&#25105;&#20204;&#37319;&#29992;&#38142;&#24335;&#24605;&#32500;&#20998;&#35299;&#21644;&#37325;&#26500;&#26469;&#36866;&#24212;LLMs&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#28508;&#22312;&#34920;&#31034;&#25511;&#21046;&#26041;&#27861;&#12290;&#22312;&#25991;&#26412;&#20998;&#31867;&#21644;&#20854;&#20182;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;CoTAM&#30456;&#23545;&#20110;&#20854;&#20182;&#20855;&#26377;&#30456;&#21516;&#25968;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#22522;&#20110;LLMs&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#20998;&#26512;&#32467;&#26524;&#21487;&#35270;&#21270;&#20102;CoTAM&#30340;&#23646;&#24615;&#25805;&#20316;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26356;&#23569;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;LLM&#24341;&#23548;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method, Chain-of-Thoughts Attribute Manipulation (CoTAM), to guide few-shot learning by carefully crafted data from Large Language Models (LLMs). The main idea is to create data with changes only in the attribute targeted by the task. Inspired by facial attribute manipulation, our approach generates label-switched data by leveraging LLMs to manipulate task-specific attributes and reconstruct new sentences in a controlled manner. Instead of conventional latent representation controlling, we implement chain-of-thoughts decomposition and reconstruction to adapt the procedure to LLMs. Extensive results on text classification and other tasks verify the advantage of CoTAM over other LLM-based text generation methods with the same number of training examples. Analysis visualizes the attribute manipulation effectiveness of CoTAM and presents the potential of LLM-guided learning with even less supervision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Pareto Optimal&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#29992;&#30340;&#32534;&#31243;&#30417;&#30563;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21709;&#24212;&#36827;&#34892;&#31995;&#32479;&#26657;&#20934;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21709;&#24212;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.16564</link><description>&lt;p&gt;
&#36890;&#36807;Pareto Optimal&#33258;&#30417;&#30563;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#26657;&#20934;&#21644;&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision. (arXiv:2306.16564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Pareto Optimal&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#29992;&#30340;&#32534;&#31243;&#30417;&#30563;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21709;&#24212;&#36827;&#34892;&#31995;&#32479;&#26657;&#20934;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21709;&#24212;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#32463;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;&#26159;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#22686;&#38271;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#31561;&#20851;&#38190;&#39046;&#22495;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;LLM&#21709;&#24212;&#30340;&#32622;&#20449;&#27700;&#24179;&#65292;&#23545;&#20110;&#33258;&#21160;&#26816;&#27979;&#38169;&#35823;&#24182;&#20419;&#36827;&#20154;&#26426;&#21327;&#20316;&#39564;&#35777;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#26657;&#20934;&#20449;&#21495;&#26469;&#28304;&#26159;&#19987;&#23478;&#25351;&#23450;&#30340;&#32534;&#31243;&#30417;&#30563;&#65292;&#36890;&#24120;&#20855;&#26377;&#36739;&#20302;&#30340;&#25104;&#26412;&#65292;&#20294;&#20063;&#26377;&#20854;&#33258;&#36523;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#22122;&#22768;&#21644;&#35206;&#30422;&#33539;&#22260;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;Pareto Optimal&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#21487;&#29992;&#30340;&#32534;&#31243;&#30417;&#30563;&#26469;&#31995;&#32479;&#22320;&#26657;&#20934;LLM&#21709;&#24212;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21709;&#24212;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;&#36825;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#35843;&#21644;&#27169;&#22411;&#26469;&#23454;&#29616;&#65292;&#23558;LLM&#36755;&#20986;&#19982;&#20854;&#20182;&#21487;&#29992;&#30340;&#30417;&#30563;&#26469;&#28304;&#30456;&#21327;&#35843;&#65292;&#23558;&#26356;&#19981;&#30830;&#23450;&#30340;&#21709;&#24212;&#20998;&#37197;&#26356;&#39640;&#30340;&#39118;&#38505;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the confidence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop verification. An important source of calibration signals stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain L
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#26469;&#20943;&#23569;&#38750;&#27954;&#21475;&#38899;&#20020;&#24202;ASR&#35757;&#32451;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#29616;&#26377;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#21475;&#38899;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.02105</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#20197;&#24212;&#23545;&#20302;&#36164;&#28304;&#20020;&#24202;&#35821;&#38899;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Adapting Pretrained ASR Models to Low-resource Clinical Speech using Epistemic Uncertainty-based Data Selection. (arXiv:2306.02105v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#26469;&#20943;&#23569;&#38750;&#27954;&#21475;&#38899;&#20020;&#24202;ASR&#35757;&#32451;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#29616;&#26377;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#21475;&#38899;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;ASR&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#38750;&#27954;&#21475;&#38899;&#30340;&#20020;&#24202;ASR&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#26500;&#24314;&#24378;&#22823;&#30340;ASR&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;&#21644;&#24418;&#24577;&#20016;&#23500;&#30340;&#21475;&#38899;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#30340;&#21019;&#24314;&#25104;&#26412;&#36739;&#39640;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#20449;&#24687;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#20943;&#23569;&#27880;&#37322;&#36153;&#29992;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#21487;&#20197;&#36229;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;ASR&#27169;&#22411;&#24314;&#31435;&#30340;&#20960;&#20010;&#22522;&#20934;&#32467;&#26524;&#65292;&#21516;&#26102;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#65292;&#20174;&#32780;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#25913;&#21892;&#20102;&#20302;&#36164;&#28304;&#21475;&#38899;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38750;&#27954;&#20020;&#24202;ASR&#30340;&#32972;&#26223;&#19979;&#26500;&#24314;&#27867;&#21270;&#22411;ASR&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#32780;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#20027;&#35201;&#26159;&#31232;&#32570;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there has been significant progress in ASR, African-accented clinical ASR has been understudied due to a lack of training datasets. Building robust ASR systems in this domain requires large amounts of annotated or labeled data, for a wide variety of linguistically and morphologically rich accents, which are expensive to create. Our study aims to address this problem by reducing annotation expenses through informative uncertainty-based data selection. We show that incorporating epistemic uncertainty into our adaptation rounds outperforms several baseline results, established using state-of-the-art (SOTA) ASR models, while reducing the required amount of labeled data, and hence reducing annotation costs. Our approach also improves out-of-distribution generalization for very low-resource accents, demonstrating the viability of our approach for building generalizable ASR models in the context of accented African clinical ASR, where training datasets are predominantly scarce.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26597;&#35810;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#23433;&#20840;&#26377;&#25928;&#22320;&#35757;&#32451;&#28145;&#24230;&#26816;&#32034;&#27169;&#22411;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.05973</link><description>&lt;p&gt;
&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26597;&#35810;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;.
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models. (arXiv:2305.05973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26597;&#35810;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#23433;&#20840;&#26377;&#25928;&#22320;&#35757;&#32451;&#28145;&#24230;&#26816;&#32034;&#27169;&#22411;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21457;&#38544;&#31169;&#20445;&#25252;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#65292;&#20811;&#26381;&#20102;&#22312;&#35757;&#32451;&#36825;&#20123;&#22797;&#26434;&#31995;&#32479;&#26102;&#30340;&#26576;&#20123;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#20294;&#20063;&#21487;&#20197;&#36731;&#26494;&#22320;&#29992;&#20110;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#34920;&#31034;&#30340;&#20219;&#20309;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;DP&#35757;&#32451;&#26041;&#27861;&#65292;&#23545;&#20844;&#24320;&#39044;&#35757;&#32451;&#30340;LLM&#22312;&#26597;&#35810;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#29983;&#25104;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#31169;&#26377;&#21512;&#25104;&#26597;&#35810;&#65292;&#20195;&#34920;&#21407;&#22987;&#26597;&#35810;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#19979;&#28216;&#38750;&#31169;&#26377;&#25512;&#33616;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#30001;&#20849;&#20139;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#20219;&#20309;&#39069;&#22806;&#30340;&#38544;&#31169;&#25104;&#26412;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#23433;&#20840;&#35757;&#32451;&#26377;&#25928;&#30340;&#28145;&#24230;&#26816;&#32034;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#30340;&#26816;&#32034;&#36136;&#37327;&#26377;&#26174;&#30528;&#30340;&#25552;&#39640;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#26597;&#35810;&#32423;&#21035;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach for developing privacy-preserving large-scale recommender systems using differentially private (DP) large language models (LLMs) which overcomes certain challenges and limitations in DP training these complex systems. Our method is particularly well suited for the emerging area of LLM-based recommender systems, but can be readily employed for any recommender systems that process representations of natural language inputs. Our approach involves using DP training methods to fine-tune a publicly pre-trained LLM on a query generation task. The resulting model can generate private synthetic queries representative of the original queries which can be freely shared for any downstream non-private recommendation training procedures without incurring any additional privacy cost. We evaluate our method on its ability to securely train effective deep retrieval models, and we observe significant improvements in their retrieval quality without compromising query-level pri
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#38024;&#23545;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#20171;&#32461;&#20102;&#28041;&#21450;&#27492;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#20197;&#21450;&#25551;&#36848;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#21644;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#24314;&#35758;&#24615;&#30340;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2211.09172</link><description>&lt;p&gt;
&#25991;&#23383;&#23545;&#35805;&#20013;&#30340;&#28145;&#24230;&#24773;&#24863;&#35782;&#21035;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Deep Emotion Recognition in Textual Conversations: A Survey. (arXiv:2211.09172v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#38024;&#23545;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#20171;&#32461;&#20102;&#28041;&#21450;&#27492;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#20197;&#21450;&#25551;&#36848;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#21644;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#24314;&#35758;&#24615;&#30340;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#24180;&#26469;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#26032;&#30340;&#24212;&#29992;&#21644;&#23454;&#26045;&#22330;&#26223;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#21033;&#29992;&#23545;&#35805;&#35821;&#22659;&#12289;&#35828;&#35805;&#20154;&#21644;&#24773;&#24863;&#21160;&#24577;&#24314;&#27169;&#65292;&#35299;&#37322;&#24120;&#35782;&#34920;&#36798;&#12289;&#38750;&#27491;&#24335;&#35821;&#35328;&#21644;&#35773;&#21050;&#65292;&#24212;&#23545;&#23454;&#26102;&#24773;&#24863;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#35782;&#21035;&#24773;&#24863;&#21407;&#22240;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#31181;&#20998;&#31867;&#27861;&#65292;&#22810;&#35821;&#35328;&#24773;&#24863;&#35782;&#21035;&#20197;&#21450;&#35299;&#37322;&#24615;&#12290;&#26412;&#35843;&#30740;&#39318;&#20808;&#20171;&#32461;&#20102;&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#19982;&#27492;&#20219;&#21153;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#28982;&#21518;&#65292;&#23427;&#20171;&#32461;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#22810;&#31181;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#24773;&#24863;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#12290;&#25509;&#19979;&#26469;&#65292;&#23427;&#25551;&#36848;&#20102;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#65292;&#24182;&#35299;&#37322;&#20102;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#26368;&#21518;&#65292;&#23427;&#25552;&#20379;&#20102;&#23545;&#20110;&#26356;&#22909;&#30340;&#26694;&#26550;&#30340;&#24314;&#35758;&#24615;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#22788;&#29702;&#20027;&#35266;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Emotion Recognition in Conversations (ERC) has seen a tremendous advancement in the last few years, new applications and implementation scenarios present novel challenges and opportunities. These range from leveraging the conversational context, speaker and emotion dynamics modelling, to interpreting common sense expressions, informal language and sarcasm, addressing challenges of real time ERC, recognizing emotion causes, different taxonomies across datasets, multilingual ERC to interpretability. This survey starts by introducing ERC, elaborating on the challenges and opportunities pertaining to this task. It proceeds with a description of the emotion taxonomies and a variety of ERC benchmark datasets employing such taxonomies. This is followed by descriptions of the most prominent works in ERC with explanations of the Deep Learning architectures employed. Then, it provides advisable ERC practices towards better frameworks, elaborating on methods to deal with subjectivity in ann
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20351;&#29992;&#30340;&#25506;&#27979;&#35774;&#32622;&#65292;&#36890;&#36807;&#24178;&#39044;&#27169;&#22411;&#30340;&#34920;&#31034;&#26469;&#21435;&#38500;&#23646;&#24615;&#65292;&#20174;&#32780;&#21457;&#29616;&#27169;&#22411;&#23454;&#38469;&#20351;&#29992;&#30340;&#32534;&#30721;&#12290;&#20197;BERT&#22914;&#20309;&#32534;&#30721;&#35821;&#27861;&#25968;&#20026;&#20363;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;BERT&#20381;&#36182;&#20110;&#35821;&#27861;&#25968;&#30340;&#32447;&#24615;&#32534;&#30721;&#26469;&#20135;&#29983;&#27491;&#30830;&#30340;&#34892;&#20026;&#36755;&#20986;&#65292;&#24182;&#23545;&#21517;&#35789;&#21644;&#21160;&#35789;&#30340;&#35821;&#27861;&#25968;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2204.08831</link><description>&lt;p&gt;
&#25506;&#31350;&#35821;&#27861;&#25968;&#30340;&#20351;&#29992;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Probing for the Usage of Grammatical Number. (arXiv:2204.08831v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20351;&#29992;&#30340;&#25506;&#27979;&#35774;&#32622;&#65292;&#36890;&#36807;&#24178;&#39044;&#27169;&#22411;&#30340;&#34920;&#31034;&#26469;&#21435;&#38500;&#23646;&#24615;&#65292;&#20174;&#32780;&#21457;&#29616;&#27169;&#22411;&#23454;&#38469;&#20351;&#29992;&#30340;&#32534;&#30721;&#12290;&#20197;BERT&#22914;&#20309;&#32534;&#30721;&#35821;&#27861;&#25968;&#20026;&#20363;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;BERT&#20381;&#36182;&#20110;&#35821;&#27861;&#25968;&#30340;&#32447;&#24615;&#32534;&#30721;&#26469;&#20135;&#29983;&#27491;&#30830;&#30340;&#34892;&#20026;&#36755;&#20986;&#65292;&#24182;&#23545;&#21517;&#35789;&#21644;&#21160;&#35789;&#30340;&#35821;&#27861;&#25968;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#30340;&#26680;&#24515;&#38382;&#39064;&#26159;&#25581;&#31034;&#39044;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#22312;&#20854;&#34920;&#31034;&#20013;&#32534;&#30721;&#35821;&#35328;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#32534;&#30721;&#21487;&#33021;&#26159;&#34394;&#20551;&#30340;&#65292;&#21363;&#27169;&#22411;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#21487;&#33021;&#19981;&#20381;&#36182;&#20110;&#23427;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#23547;&#25214;&#27169;&#22411;&#23454;&#38469;&#20351;&#29992;&#30340;&#32534;&#30721;&#65292;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#20351;&#29992;&#30340;&#25506;&#27979;&#35774;&#32622;&#12290;&#25105;&#20204;&#39318;&#20808;&#36873;&#25321;&#19968;&#20010;&#34892;&#20026;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#22312;&#19981;&#20351;&#29992;&#35821;&#35328;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#35299;&#20915;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#24178;&#39044;&#27169;&#22411;&#30340;&#34920;&#31034;&#26469;&#21435;&#38500;&#23646;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#27169;&#22411;&#20351;&#29992;&#20102;&#26576;&#31181;&#32534;&#30721;&#65292;&#21435;&#38500;&#35813;&#32534;&#30721;&#24212;&#35813;&#20250;&#25439;&#23475;&#25152;&#36873;&#25321;&#30340;&#34892;&#20026;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20197;BERT&#22914;&#20309;&#32534;&#30721;&#35821;&#27861;&#25968;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#35813;&#32534;&#30721;&#35299;&#20915;&#25968;&#30340;&#19968;&#33268;&#24615;&#20219;&#21153;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;BERT&#20381;&#36182;&#20110;&#35821;&#27861;&#25968;&#30340;&#32447;&#24615;&#32534;&#30721;&#26469;&#20135;&#29983;&#27491;&#30830;&#30340;&#34892;&#20026;&#36755;&#20986;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;BERT&#23545;&#21517;&#35789;&#21644;&#21160;&#35789;&#30340;&#35821;&#27861;&#25968;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central quest of probing is to uncover how pre-trained models encode a linguistic property within their representations. An encoding, however, might be spurious-i.e., the model might not rely on it when making predictions. In this paper, we try to find encodings that the model actually uses, introducing a usage-based probing setup. We first choose a behavioral task which cannot be solved without using the linguistic property. Then, we attempt to remove the property by intervening on the model's representations. We contend that, if an encoding is used by the model, its removal should harm the performance on the chosen behavioral task. As a case study, we focus on how BERT encodes grammatical number, and on how it uses this encoding to solve the number agreement task. Experimentally, we find that BERT relies on a linear encoding of grammatical number to produce the correct behavioral output. We also find that BERT uses a separate encoding of grammatical number for nouns and verbs. Fina
&lt;/p&gt;</description></item></channel></rss>