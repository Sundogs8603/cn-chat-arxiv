<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.19923</link><description>&lt;p&gt;
Jina Embeddings 2: &#38754;&#21521;&#38271;&#31687;&#25991;&#26723;&#30340;8192-Token&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19923
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23558;&#21477;&#23376;&#36716;&#21270;&#20026;&#22266;&#23450;&#22823;&#23567;&#29305;&#24449;&#21521;&#37327;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36825;&#20123;&#21521;&#37327;&#21253;&#21547;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#25991;&#26412;&#37325;&#25490;&#24207;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;BERT&#31561;&#26550;&#26500;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#38590;&#20197;&#34920;&#31034;&#38271;&#31687;&#25991;&#26723;&#65292;&#24182;&#19988;&#24120;&#24120;&#20250;&#36827;&#34892;&#25130;&#26029;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#27573;&#33853;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#21521;&#37327;&#38598;&#21512;&#65292;&#36827;&#32780;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#19988;&#22312;&#21521;&#37327;&#25628;&#32034;&#26102;&#20250;&#20986;&#29616;&#35745;&#31639;&#23494;&#38598;&#21644;&#24310;&#36831;&#21319;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jina Embeddings 2&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#32435;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#31361;&#30772;&#20256;&#32479;&#30340;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2310.10477</link><description>&lt;p&gt;
&#20174;&#25387;&#25240;&#20013;&#33719;&#24471;&#26234;&#24935;&#65306;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#21644;&#26377;&#27602;&#22238;&#24212;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#23545;&#40784;&#26041;&#27861;&#33268;&#21147;&#20110;&#24341;&#23548;LLMs&#26397;&#30528;&#26399;&#26395;&#30340;&#24615;&#33021;&#21457;&#23637;&#24182;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#20840;&#26032;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26377;&#24847;&#26292;&#38706;LLMs&#30340;&#32570;&#38519;&#36755;&#20986;&#24182;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#23436;&#20840;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;LLMs&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#22238;&#24212;&#65292;&#36824;&#21487;&#20197;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#65292;&#21457;&#25381;&#20854;&#36776;&#21035;&#26377;&#27602;&#20869;&#23481;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Reward-Augmented Decoding (RAD)&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#30340;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#26469;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25991;&#26412;&#12290;RAD&#22312;&#29983;&#25104;&#38750;&#26377;&#23475;&#21644;&#24773;&#24863;&#21463;&#25511;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20063;&#24456;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.09520</link><description>&lt;p&gt;
Reward-Augmented Decoding: &#20351;&#29992;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#21463;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model. (arXiv:2310.09520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Reward-Augmented Decoding (RAD)&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#30340;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#26469;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25991;&#26412;&#12290;RAD&#22312;&#29983;&#25104;&#38750;&#26377;&#23475;&#21644;&#24773;&#24863;&#21463;&#25511;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20063;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#29983;&#25104;&#30340;&#25991;&#26412;&#23384;&#22312;&#38382;&#39064;&#25110;&#32773;&#32570;&#20047;&#25152;&#38656;&#30340;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reward-Augmented Decoding (RAD)&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#19968;&#20010;&#23567;&#22411;&#30340;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#26469;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25991;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;RAD&#21033;&#29992;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#35843;&#25972;&#37319;&#26679;&#27010;&#29575;&#26469;&#26356;&#20542;&#21521;&#20110;&#39640;&#22870;&#21169;&#30340;&#26631;&#35760;&#12290;&#36890;&#36807;&#20351;&#29992;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#65292;RAD&#33021;&#22815;&#32531;&#23384;&#20808;&#21069;&#29983;&#25104;&#27493;&#39588;&#30340;&#28608;&#27963;&#20540;&#65292;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#38750;&#26377;&#23475;&#21644;&#24773;&#24863;&#21463;&#25511;&#25991;&#26412;&#26041;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;RAD&#22312;&#20165;&#25913;&#21464;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#27861;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#19982;&#28041;&#21450;&#37325;&#26032;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;RAD&#22312;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models w
&lt;/p&gt;</description></item><item><title>&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;&#33521;&#22269;&#30340;&#36229;&#36807;250,000&#20010;&#27861;&#24237;&#26696;&#20363;&#12290;&#22312;&#35813;&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26696;&#20363;&#32467;&#26524;&#30340;&#19987;&#23478;&#27880;&#35299;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#20102;&#26696;&#20363;&#32467;&#26524;&#25552;&#21462;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.12269</link><description>&lt;p&gt;
&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#65306;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
The Cambridge Law Corpus: A Corpus for Legal AI Research. (arXiv:2309.12269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12269
&lt;/p&gt;
&lt;p&gt;
&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;&#33521;&#22269;&#30340;&#36229;&#36807;250,000&#20010;&#27861;&#24237;&#26696;&#20363;&#12290;&#22312;&#35813;&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26696;&#20363;&#32467;&#26524;&#30340;&#19987;&#23478;&#27880;&#35299;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#20102;&#26696;&#20363;&#32467;&#26524;&#25552;&#21462;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#65288;CLC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#12290;&#23427;&#21253;&#21547;&#20102;&#26469;&#33258;&#33521;&#22269;&#30340;&#36229;&#36807;250,000&#20010;&#27861;&#24237;&#26696;&#20363;&#12290;&#22823;&#37096;&#20998;&#26696;&#20363;&#26469;&#33258;21&#19990;&#32426;&#65292;&#20294;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;&#20102;16&#19990;&#32426;&#20197;&#26469;&#30340;&#26696;&#20363;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35813;&#35821;&#26009;&#24211;&#30340;&#39318;&#27425;&#21457;&#24067;&#65292;&#21253;&#25324;&#21407;&#22987;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#12290;&#22312;&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;638&#20010;&#26696;&#20363;&#30340;&#27861;&#24459;&#19987;&#23478;&#23545;&#26696;&#20363;&#32467;&#26524;&#30340;&#27880;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;GPT-3&#12289;GPT-4&#21644;RoBERTa&#27169;&#22411;&#36827;&#34892;&#26696;&#20363;&#32467;&#26524;&#25552;&#21462;&#65292;&#20197;&#25552;&#20379;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#35752;&#35770;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#26448;&#26009;&#21487;&#33021;&#20855;&#26377;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#35813;&#35821;&#26009;&#24211;&#21482;&#20250;&#22312;&#19968;&#23450;&#38480;&#21046;&#19979;&#29992;&#20110;&#30740;&#31350;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research. It consists of over 250 000 court cases from the UK. Most cases are from the 21st century, but the corpus includes cases as old as the 16th century. This paper presents the first release of the corpus, containing the raw text and meta-data. Together with the corpus, we provide annotations on case outcomes for 638 cases, done by legal experts. Using our annotated data, we have trained and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to provide benchmarks. We include an extensive legal and ethical discussion to address the potentially sensitive nature of this material. As a consequence, the corpus will only be released for research purposes under certain restrictions.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06035</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#26399;&#38388;&#34920;&#29616;&#20986;&#20154;&#31867;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing. (arXiv:2308.06035v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06035
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#22797;&#21046;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30340;&#20105;&#35758;&#12290;LLMs&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#19968;&#20010;&#21306;&#21035;&#22312;&#20110;&#65292;&#35821;&#35328;&#36755;&#20837;&#36890;&#24120;&#24314;&#31435;&#22312;&#22810;&#20010;&#30693;&#35273;&#27169;&#24577;&#19978;&#65292;&#32780;&#22823;&#22810;&#25968;LLMs&#20165;&#22788;&#29702;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#20351;&#20154;&#31867;&#33021;&#22815;&#25972;&#21512;&#35270;&#35273;&#32972;&#26223;&#19982;&#35821;&#35328;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#21363;&#23558;&#20986;&#29616;&#30340;&#21333;&#35789;&#30340;&#31354;&#38388;&#26045;&#21152;&#38480;&#21046;&#65292;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;LLMs&#65288;mLLMs&#65289;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#21464;&#21387;&#22120;&#31867;&#22411;&#30340;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#19979;&#19968;&#20010;&#21333;&#35789;&#30340;&#39044;&#27979;&#12290;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#65292;&#22522;&#20110;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#22312;mLLMs&#21644;&#20154;&#31867;&#20013;&#21563;&#21512;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;200&#21517;&#34987;&#35797;&#35266;&#30475;&#20102;&#30701;&#30340;&#35270;&#21548;&#21098;&#36753;&#65292;&#24182;&#20272;&#35745;&#20102;&#21363;&#23558;&#20986;&#29616;&#30340;&#21160;&#35789;&#25110;&#21517;&#35789;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advanced language processing abilities of large language models (LLMs) have stimulated debate over their capacity to replicate human-like cognitive processes. One differentiating factor between language processing in LLMs and humans is that language input is often grounded in more than one perceptual modality, whereas most LLMs process solely text-based information. Multimodal grounding allows humans to integrate - e.g. visual context with linguistic information and thereby place constraints on the space of upcoming words, reducing cognitive load and improving perception and comprehension. Recent multimodal LLMs (mLLMs) combine visual and linguistic embedding spaces with a transformer type attention mechanism for next-word prediction. To what extent does predictive language processing based on multimodal input align in mLLMs and humans? To answer this question, 200 human participants watched short audio-visual clips and estimated the predictability of an upcoming verb or noun. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11300</link><description>&lt;p&gt;
RS5M&#65306;&#29992;&#20110;&#36965;&#24863;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model. (arXiv:2306.11300v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#21033;&#29992;&#24050;&#26377;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#22495;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#36801;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#24357;&#21512;&#20102;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#65288;RS&#65289;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;500&#19975;&#24352;&#24102;&#26377;&#33521;&#25991;&#25551;&#36848;&#30340;RS&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#20165;&#24102;&#26631;&#31614;&#30340;RS&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Vision-Language Foundation Models utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain Foundation Model (DFM), bridging the gap between the General Foundation Model (GFM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.17760</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;
&lt;/p&gt;
&lt;p&gt;
Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Ouyang&#31561;&#20154;&#65292;2022&#65289;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#65288;Kahneman&#65292;2011&#65289;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24515;&#29702;&#23398;&#23478;&#20204;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#36825;&#20010;&#26694;&#26550;&#30340;&#36884;&#24452;&#12290;&#26412;&#30740;&#31350;&#23454;&#36136;&#19978;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#26469;&#33719;&#24471;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#26041;&#38754;&#30340;&#28145;&#21051;&#35265;&#35299;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
&lt;/p&gt;</description></item><item><title>ArtGPT-4&#26159;&#19968;&#31181;&#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#35299;&#20915;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#20986;&#20855;&#22791;&#33391;&#22909;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.07490</link><description>&lt;p&gt;
ArtGPT-4: &#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#30340;&#33402;&#26415;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4. (arXiv:2305.07490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07490
&lt;/p&gt;
&lt;p&gt;
ArtGPT-4&#26159;&#19968;&#31181;&#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#35299;&#20915;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#20986;&#20855;&#22791;&#33391;&#22909;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#27604;&#22914;ChatGPT&#21644;GPT-4&#31561;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#23545;&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#25214;&#21040;&#19982;&#27169;&#22411;&#35268;&#27169;&#21305;&#37197;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#20063;&#24456;&#22256;&#38590;&#12290;&#24494;&#35843;&#21644;&#20351;&#29992;&#26032;&#26041;&#27861;&#35757;&#32451;&#21442;&#25968;&#36739;&#23569;&#30340;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;MiniGPT-4&#27169;&#22411;&#20415;&#26159;&#20854;&#20013;&#20043;&#19968;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#36816;&#29992;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38761;&#26032;&#24615;&#30340;&#22521;&#35757;&#31574;&#30053;&#23454;&#29616;&#20102;&#19982;GPT-4&#30456;&#24403;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#35813;&#27169;&#22411;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33402;&#26415;&#22270;&#29255;&#26041;&#38754;&#12290;ArtGPT-4&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#26088;&#22312;&#24212;&#23545;&#36825;&#20123;&#23616;&#38480;&#12290;ArtGPT-4&#20351;&#29992;Tesla A100&#35774;&#22791;&#23545;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#20165;&#29992;&#20102;&#32422;200GB&#30340;&#25968;&#25454;&#65292;&#22312;2&#23567;&#26102;&#20869;&#23601;&#33021;&#23637;&#31034;&#20986;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have made significant progress in natural language processing (NLP), with models like ChatGPT and GPT-4 achieving impressive capabilities in various linguistic tasks. However, training models on such a large scale is challenging, and finding datasets that match the model's scale is often difficult. Fine-tuning and training models with fewer parameters using novel methods have emerged as promising approaches to overcome these challenges. One such model is MiniGPT-4, which achieves comparable vision-language understanding to GPT-4 by leveraging novel pre-training models and innovative training strategies. However, the model still faces some challenges in image understanding, particularly in artistic pictures. A novel multimodal model called ArtGPT-4 has been proposed to address these limitations. ArtGPT-4 was trained on image-text pairs using a Tesla A100 device in just 2 hours, using only about 200 GB of data. The model can depict images wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;Essential Element Network (EEN)&#31639;&#27861;&#23558;&#38899;&#39057;&#32534;&#30721;&#25104;&#25991;&#26412;&#24182;&#36827;&#34892;&#30456;&#20851;&#24615;&#35745;&#31639;&#21644;&#20248;&#21270;&#24212;&#29992;&#20110;&#32858;&#31867;&#31995;&#25968;&#30340;&#39057;&#29575;&#21644;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#38899;&#20048;&#30340;&#28145;&#23618;&#32467;&#26500;&#20449;&#24687;&#65292;&#20026;&#21400;&#28165;&#38899;&#20048;&#32467;&#26500;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13631</link><description>&lt;p&gt;
&#38899;&#20048;&#32467;&#26500;&#30340;&#33258;&#32452;&#32455;&#32593;&#32476;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
In-depth analysis of music structure as a self-organized network. (arXiv:2303.13631v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;Essential Element Network (EEN)&#31639;&#27861;&#23558;&#38899;&#39057;&#32534;&#30721;&#25104;&#25991;&#26412;&#24182;&#36827;&#34892;&#30456;&#20851;&#24615;&#35745;&#31639;&#21644;&#20248;&#21270;&#24212;&#29992;&#20110;&#32858;&#31867;&#31995;&#25968;&#30340;&#39057;&#29575;&#21644;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#38899;&#20048;&#30340;&#28145;&#23618;&#32467;&#26500;&#20449;&#24687;&#65292;&#20026;&#21400;&#28165;&#38899;&#20048;&#32467;&#26500;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#35789;&#27719;&#19981;&#20165;&#20256;&#36882;&#20449;&#24687;&#65292;&#36824;&#38543;&#30528;&#25991;&#26126;&#21644;&#20154;&#31867;&#36801;&#31227;&#32780;&#28436;&#21464;&#12290;&#38899;&#20048;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#29702;&#35299;&#38899;&#20048;&#32972;&#21518;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21483;&#20570;Essential Element Network (EEN)&#30340;&#31639;&#27861;&#23558;&#38899;&#39057;&#32534;&#30721;&#25104;&#25991;&#26412;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#35745;&#31639;&#38899;&#35843;&#12289;&#26102;&#38388;&#21644;&#38899;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24471;&#21040;&#65292;&#36890;&#36807;&#20248;&#21270;EEN&#31639;&#27861;&#20197;&#29983;&#25104;Zipf&#23450;&#24459;&#24212;&#29992;&#20110;&#32858;&#31867;&#31995;&#25968;&#30340;&#39057;&#29575;&#21644;&#25490;&#21517;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#35821;&#20041;&#20851;&#31995;&#35270;&#20026;&#35789;&#27719;&#24182;&#29983;&#25104;&#23427;&#20204;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32534;&#30721;&#21518;&#30340;&#35789;&#27719;&#26144;&#23556;&#21040;&#38899;&#35843;-&#26102;&#38388;&#31354;&#38388;&#20013;&#65292;&#26377;&#21161;&#20110;&#25105;&#20204;&#31995;&#32479;&#22320;&#32452;&#32455;&#38899;&#20048;&#28145;&#23618;&#32467;&#26500;&#20013;&#30340;&#21477;&#27861;&#12290;&#30456;&#27604;&#20110;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#40657;&#30418;&#23376;&#29305;&#24615;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#23545;&#38899;&#20048;&#32972;&#21518;&#22797;&#26434;&#32593;&#32476;&#30340;&#31934;&#30830;&#25551;&#36848;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#36807;&#31243;&#31215;&#32047;&#30340;&#32463;&#39564;&#21644;&#23646;&#24615;&#19981;&#20165;&#20026;&#27492;&#31867;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#20026;&#35768;&#22810;&#20854;&#20182;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25506;&#32034;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Words in a natural language not only transmit information but also evolve with the development of civilization and human migration. The same is true for music. To understand the complex structure behind the music, we introduced an algorithm called the Essential Element Network (EEN) to encode the audio into text. The network is obtained by calculating the correlations between scales, time, and volume. Optimizing EEN to generate Zipfs law for the frequency and rank of the clustering coefficient enables us to generate and regard the semantic relationships as words. We map these encoded words into the scale-temporal space, which helps us organize systematically the syntax in the deep structure of music. Our algorithm provides precise descriptions of the complex network behind the music, as opposed to the black-box nature of other deep learning approaches. As a result, the experience and properties accumulated through these processes can offer not only a new approach to the applications of
&lt;/p&gt;</description></item></channel></rss>