<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01109</link><description>&lt;p&gt;
&#30123;&#33495;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Vaccine: Perturbation-aware Alignment for Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01109
&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#21363;&#26381;&#21153;&#33539; paradigm&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20026;&#29992;&#25143;&#19978;&#20256;&#30340;&#19968;&#23567;&#37096;&#20998;&#26377;&#23475;&#25968;&#25454;&#25552;&#20379;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#36825;&#20123;&#25968;&#25454;&#24456;&#23481;&#26131;&#27450;&#39575;&#24494;&#35843;&#36807;&#31243;&#20174;&#32780;&#20135;&#29983;&#23545;&#40784;&#22833;&#25928;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#21487;&#33021;&#23548;&#33268;&#23545;&#40784;&#22833;&#25928;&#30340;&#26377;&#23475;&#23884;&#20837;&#28418;&#31227;&#29616;&#35937;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30123;&#33495; (Vaccine) &#65292;&#19968;&#31181;&#38024;&#23545;&#24178;&#25200;&#24863;&#30693;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#29992;&#25143;&#24494;&#35843;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30123;&#33495;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#22312;&#23545;&#40784;&#38454;&#27573;&#36880;&#28176;&#28155;&#21152;&#31934;&#24515;&#35774;&#35745;&#30340;&#25200;&#21160;&#65292;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#20174;&#32780;&#20351;&#23884;&#20837;&#33021;&#22815;&#25269;&#24481;&#26469;&#33258;&#26410;&#32463;&#28040;&#27602;&#30340;&#29992;&#25143;&#25968;&#25454;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;&#20027;&#27969;LLM&#65288;&#22914;Llama2&#65292;Opt&#65292;Vicuna&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30123;&#33495;&#33021;&#22815;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompt
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#38382;&#39064;&#65292;&#22312;Battleship&#28216;&#25103;&#20013;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#21305;&#37197;&#30340;&#25928;&#26524;&#65292;&#24182;&#25581;&#31034;&#20102;&#36125;&#21494;&#26031;&#27169;&#22411;&#22914;&#20309;&#25351;&#23548;&#38382;&#38382;&#39064;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.19471</link><description>&lt;p&gt;
&#20005;&#26684;&#30340;LIPS&#27785;&#27809;&#33328;&#33337;&#65306;&#22312;Battleship&#20013;&#20351;&#29992;&#35821;&#35328;&#20449;&#24687;&#31243;&#24207;&#25277;&#26679;&#25552;&#20986;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19471
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#38382;&#39064;&#65292;&#22312;Battleship&#28216;&#25103;&#20013;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#21305;&#37197;&#30340;&#25928;&#26524;&#65292;&#24182;&#25581;&#31034;&#20102;&#36125;&#21494;&#26031;&#27169;&#22411;&#22914;&#20309;&#25351;&#23548;&#38382;&#38382;&#39064;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#32467;&#21512;&#20102;&#25105;&#20204;&#23545;&#35821;&#35328;&#30340;&#25484;&#25569;&#21644;&#25105;&#20204;&#23545;&#20110;&#22312;&#26377;&#38480;&#35748;&#30693;&#36164;&#28304;&#24773;&#20917;&#19979;&#25512;&#26029;&#19981;&#30830;&#23450;&#24615;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#20154;&#20204;&#22914;&#20309;&#22312;&#24040;&#22823;&#20551;&#35774;&#31354;&#38388;&#20013;&#25552;&#20986;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#38382;&#39064;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#22312;&#22522;&#20110;&#25112;&#33328;&#28216;&#25103;Battleship&#30340;&#32463;&#20856;&#25552;&#38382;&#20219;&#21153;&#20013;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#35821;&#35328;&#20449;&#24687;&#31243;&#24207;&#25277;&#26679;&#65288;LIPS&#65289;&#27169;&#22411;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#31526;&#21495;&#31243;&#24207;&#65292;&#24182;&#35780;&#20272;&#20854;&#39044;&#26399;&#20449;&#24687;&#22686;&#30410;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#36164;&#28304;&#39044;&#31639;&#19979;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;&#33945;&#29305;&#21345;&#32599;&#20248;&#21270;&#31574;&#30053;&#20063;&#33021;&#20135;&#29983;&#21453;&#26144;&#20154;&#31867;&#22312;&#21508;&#31181;Battleship&#26827;&#30424;&#22330;&#26223;&#20013;&#34920;&#29616;&#30340;&#20016;&#23500;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20165;&#20351;&#29992;LLM&#30340;&#22522;&#32447;&#22312;&#23558;&#38382;&#39064;&#19982;&#26827;&#30424;&#29366;&#24577;&#32852;&#31995;&#36215;&#26469;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65307;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GPT-4V&#24182;&#27809;&#26377;&#27604;&#26080;&#35270;&#35273;&#22522;&#32447;&#25552;&#20379;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#36125;&#21494;&#26031;&#25552;&#38382;&#27169;&#22411;&#22914;&#20309;&#21487;&#33021;&#27169;&#25311;&#21644;&#25351;&#23548;&#20154;&#31867;&#30340;&#38382;&#38382;&#39064;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19471v1 Announce Type: cross  Abstract: Questions combine our mastery of language with our remarkable facility for reasoning about uncertainty. How do people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources? We study these tradeoffs in a classic grounded question-asking task based on the board game Battleship. Our language-informed program sampling (LIPS) model uses large language models (LLMs) to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain. We find that with a surprisingly modest resource budget, this simple Monte Carlo optimization strategy yields informative questions that mirror human performance across varied Battleship board scenarios. In contrast, LLM-only baselines struggle to ground questions in the board state; notably, GPT-4V provides no improvement over non-visual baselines. Our results illustrate how Bayesian models of question-asking can l
&lt;/p&gt;</description></item><item><title>TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19467</link><description>&lt;p&gt;
TV-TREES&#65306;&#29992;&#20110;&#31070;&#32463;&#31526;&#21495;&#35270;&#39057;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;
&lt;/p&gt;
&lt;p&gt;
TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19467
&lt;/p&gt;
&lt;p&gt;
TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#30005;&#35270;&#21098;&#36753;&#31561;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#38382;&#31572;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#37096;&#20998;&#26159;&#22240;&#20026;&#24403;&#21069;&#30340;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20381;&#36182;&#20110;&#21333;&#27169;&#24577;&#25512;&#29702;&#65292;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TV-TREES&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#12290;TV-TREES&#20316;&#20026;&#19968;&#31181;&#20419;&#36827;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#20219;&#21153;&#26469;&#35780;&#20272;&#27492;&#31867;&#26041;&#27861;&#30340;&#25512;&#29702;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#30340;&#12289;&#20855;&#26377;&#26368;&#20808;&#36827;&#38646;-shot&#24615;&#33021;&#30340;&#23436;&#25972;&#35270;&#39057;&#21098;&#36753;&#65292;&#23637;&#31034;&#20102;&#19982;&#40657;&#30418;&#26041;&#27861;&#30456;&#27604;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19467v1 Announce Type: cross  Abstract: It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#21487;&#20449;&#24230;&#65292;&#25581;&#31034;&#20102;&#26089;&#26399;&#39044;&#35757;&#32451;LLMs&#24050;&#32463;&#33021;&#22815;&#21306;&#20998;&#21508;&#20010;&#21487;&#20449;&#24230;&#32500;&#24230;&#20013;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20174;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#20013;&#25552;&#21462;&#36716;&#21521;&#21521;&#37327;&#20197;&#22686;&#24378;LLM&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.19465</link><description>&lt;p&gt;
&#36861;&#36394;&#21487;&#20449;&#24230;&#21160;&#24577;&#65306;&#37325;&#35775;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#26399;
&lt;/p&gt;
&lt;p&gt;
Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#21487;&#20449;&#24230;&#65292;&#25581;&#31034;&#20102;&#26089;&#26399;&#39044;&#35757;&#32451;LLMs&#24050;&#32463;&#33021;&#22815;&#21306;&#20998;&#21508;&#20010;&#21487;&#20449;&#24230;&#32500;&#24230;&#20013;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20174;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#20013;&#25552;&#21462;&#36716;&#21521;&#21521;&#37327;&#20197;&#22686;&#24378;LLM&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20805;&#20998;&#39044;&#35757;&#32451;&#30340;LLMs&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#25552;&#39640;LLMs&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#39044;&#35757;&#32451;&#30340;&#28508;&#21147;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;LLMs&#22312;&#27492;&#26399;&#38388;&#30340;&#21487;&#20449;&#24230;&#65292;&#19987;&#27880;&#20110;&#20116;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#21487;&#38752;&#24615;&#12289;&#38544;&#31169;&#12289;&#26377;&#23475;&#24230;&#12289;&#20844;&#24179;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;LLMs&#24212;&#29992;&#32447;&#24615;&#25506;&#27979;&#12290;&#39640;&#25506;&#27979;&#20934;&#30830;&#24230;&#34920;&#26126;&#65292;\textit{&#26089;&#26399;&#39044;&#35757;&#32451;&#30340;LLMs&#24050;&#32463;&#33021;&#22815;&#21306;&#20998;&#27599;&#20010;&#21487;&#20449;&#24230;&#32500;&#24230;&#20013;&#30340;&#27010;&#24565;}&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25581;&#31034;&#39044;&#35757;&#32451;&#30340;&#28508;&#22312;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#20174;LLM&#30340;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#20013;&#25552;&#21462;&#36716;&#21521;&#21521;&#37327;&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#21487;&#20449;&#24230;&#12290;&#26368;&#21518;&#65292;&#21463;&#21040;~\citet{choi2023understanding} &#30340;&#21551;&#21457;&#65292;&#30456;&#20114;&#20449;&#24687;&#20272;&#35745;&#21463;&#32447;&#24615;&#25506;&#27979;&#20934;&#30830;&#24230;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#36824;&#29992;&#30456;&#20114;&#20449;&#24687;&#25506;&#27979;LLMs&#26469;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19465v1 Announce Type: cross  Abstract: Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness. Finally, inspired by~\citet{choi2023understanding} that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to in
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#35757;&#32451;&#32418;&#38431;LLM&#65292;&#33258;&#21160;&#21270;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#65292;&#20197;&#26368;&#22823;&#21270;&#24341;&#20986;&#30446;&#26631;LLM&#19981;&#33391;&#21709;&#24212;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;RL&#26041;&#27861;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#35206;&#30422;&#33539;&#22260;&#36739;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.19464</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22909;&#22855;&#39537;&#21160;&#30340;&#32418;&#38431;&#23545;&#25239;
&lt;/p&gt;
&lt;p&gt;
Curiosity-driven Red-teaming for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19464
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#35757;&#32451;&#32418;&#38431;LLM&#65292;&#33258;&#21160;&#21270;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#65292;&#20197;&#26368;&#22823;&#21270;&#24341;&#20986;&#30446;&#26631;LLM&#19981;&#33391;&#21709;&#24212;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;RL&#26041;&#27861;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#35206;&#30422;&#33539;&#22260;&#36739;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#23384;&#22312;&#29983;&#25104;&#19981;&#27491;&#30830;&#25110;&#26377;&#27602;&#20869;&#23481;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#25506;&#31350;LLM&#20309;&#26102;&#29983;&#25104;&#19981;&#38656;&#35201;&#30340;&#20869;&#23481;&#65292;&#24403;&#21069;&#30340;&#33539;&#20363;&#26159;&#25307;&#21215;&#19968;&#20010;&#20154;&#31867;&#27979;&#35797;&#32773;\textit{&#32418;&#38431;}&#26469;&#35774;&#35745;&#36755;&#20837;&#25552;&#31034;&#65288;&#21363;&#27979;&#35797;&#26696;&#20363;&#65289;&#65292;&#36825;&#20123;&#25552;&#31034;&#21487;&#20197;&#24341;&#20986;LLMs&#30340;&#19981;&#33391;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#20154;&#31867;&#27979;&#35797;&#32773;&#26159;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#32418;&#38431;LLM&#33258;&#21160;&#21270;&#32418;&#38431;&#23545;&#25239;&#65292;&#29983;&#25104;&#26368;&#22823;&#21270;&#24341;&#20986;&#30446;&#26631;LLMs&#19981;&#33391;&#21709;&#24212;&#30340;&#27979;&#35797;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;RL&#26041;&#27861;&#21482;&#33021;&#29983;&#25104;&#23569;&#37327;&#26377;&#25928;&#30340;&#27979;&#35797;&#26696;&#20363;&#65292;&#23548;&#33268;&#23545;&#24341;&#20986;&#30446;&#26631;LLMs&#19981;&#33391;&#21709;&#24212;&#25552;&#31034;&#33539;&#22260;&#30340;&#35206;&#30422;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#23558;&#22686;&#21152;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#35206;&#30422;&#33539;&#22260;&#30340;&#38382;&#39064;&#19982;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19464v1 Announce Type: cross  Abstract: Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a \textit{red team} of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM. To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#21152;&#36895;&#32858;&#21512;&#29289;&#22826;&#38451;&#33021;&#30005;&#27744;&#26448;&#26009;&#21457;&#29616;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#21457;&#29616;&#26102;&#38388;&#24182;&#39044;&#27979;&#26410;&#34987;&#25253;&#36947;&#30340;&#26377;&#28508;&#21147;&#30340;&#21463;&#20307;-&#32473;&#20307;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.19462</link><description>&lt;p&gt;
&#21152;&#36895;&#32858;&#21512;&#29289;&#22826;&#38451;&#33021;&#30005;&#27744;&#26448;&#26009;&#21457;&#29616;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23454;&#29616;&#30340;&#25968;&#25454;&#39537;&#21160;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Accelerating materials discovery for polymer solar cells: Data-driven insights enabled by natural language processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19462
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#21152;&#36895;&#32858;&#21512;&#29289;&#22826;&#38451;&#33021;&#30005;&#27744;&#26448;&#26009;&#21457;&#29616;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#21457;&#29616;&#26102;&#38388;&#24182;&#39044;&#27979;&#26410;&#34987;&#25253;&#36947;&#30340;&#26377;&#28508;&#21147;&#30340;&#21463;&#20307;-&#32473;&#20307;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#25991;&#29486;&#20013;&#25552;&#21462;&#32858;&#21512;&#29289;&#22826;&#38451;&#33021;&#30005;&#27744;&#23646;&#24615;&#25968;&#25454;&#65292;&#24182;&#27169;&#25311;&#21508;&#31181;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#12290;&#34429;&#28982;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#24050;&#32463;&#34987;&#24191;&#27867;&#24314;&#31435;&#36215;&#26469;&#65292;&#21487;&#20197;&#27604;&#29233;&#36842;&#29983;&#35797;&#38169;&#27861;&#26356;&#24555;&#22320;&#21457;&#29616;&#26032;&#26448;&#26009;&#65292;&#20294;&#23427;&#20204;&#30340;&#30410;&#22788;&#23578;&#26410;&#24471;&#21040;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#21457;&#29616;&#26102;&#38388;&#28508;&#22312;&#20943;&#23569;&#32422;75&#65285;&#65292;&#30456;&#24403;&#20110;&#26448;&#26009;&#21019;&#26032;&#21152;&#36895;15&#24180;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#36229;&#36807;3300&#31687;&#35770;&#25991;&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#36825;&#27604;&#20854;&#20182;&#20154;&#25253;&#21578;&#30340;&#31867;&#20284;&#25968;&#25454;&#38598;&#22823;&#32422;&#22810;5&#20493;&#12290;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#21151;&#29575;&#36716;&#25442;&#25928;&#29575;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#27169;&#22411;&#35782;&#21035;&#20102;&#23578;&#26410;&#25253;&#36947;&#30340;&#26377;&#21069;&#36884;&#30340;&#21463;&#20307;-&#32473;&#20307;&#32452;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#24037;&#20316;&#27969;&#31243;&#65292;&#20174;&#24050;&#21457;&#34920;&#30340;&#25991;&#29486;&#21040;&#25552;&#21462;&#30340;&#26448;&#26009;&#23646;&#24615;&#25968;&#25454;&#65292;&#36827;&#32780;&#29992;&#20110;&#33719;&#24471;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19462v1 Announce Type: cross  Abstract: We present a natural language processing pipeline that was used to extract polymer solar cell property data from the literature and simulate various active learning strategies. While data-driven methods have been well established to discover novel materials faster than Edisonian trial-and-error approaches, their benefits have not been quantified. Our approach demonstrates a potential reduction in discovery time by approximately 75 %, equivalent to a 15 year acceleration in material innovation. Our pipeline enables us to extract data from more than 3300 papers which is ~5 times larger than similar data sets reported by others. We also trained machine learning models to predict the power conversion efficiency and used our model to identify promising donor-acceptor combinations that are as yet unreported. We thus demonstrate a workflow that goes from published literature to extracted material property data which in turn is used to obtain 
&lt;/p&gt;</description></item><item><title>$\texttt{COSMIC}$&#26159;&#19968;&#31181;&#20197;&#30456;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#26032;&#30340;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#25928;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#24378;&#12290;&#31454;&#20105;&#24615;&#33021;&#20248;&#20110;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#12290;</title><link>https://arxiv.org/abs/2402.19457</link><description>&lt;p&gt;
$\texttt{COSMIC}$: &#30456;&#20114;&#20449;&#24687;&#29992;&#20110;&#20219;&#21153;&#26080;&#20851;&#25688;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
$\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19457
&lt;/p&gt;
&lt;p&gt;
$\texttt{COSMIC}$&#26159;&#19968;&#31181;&#20197;&#30456;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#26032;&#30340;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#25928;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#24378;&#12290;&#31454;&#20105;&#24615;&#33021;&#20248;&#20110;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#24635;&#32467;&#36136;&#37327;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#26681;&#25454;&#24635;&#32467;&#22120;&#29983;&#25104;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#29992;&#19988;&#20445;&#30041;&#20219;&#21153;&#32467;&#26524;&#30340;&#25688;&#35201;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#36825;&#20123;&#20219;&#21153;&#30340;&#32467;&#26524;&#38169;&#35823;&#27010;&#29575;&#19982;&#28304;&#25991;&#26412;&#21644;&#29983;&#25104;&#25688;&#35201;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;$\texttt{COSMIC}$&#20316;&#20026;&#36825;&#19968;&#24230;&#37327;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#23637;&#31034;&#20102;&#23427;&#19982;&#22522;&#20110;&#20154;&#31867;&#21028;&#26029;&#30340;&#24230;&#37327;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#23427;&#22312;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23545;&#24050;&#24314;&#31435;&#30340;&#24230;&#37327;&#22914;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#30340;&#27604;&#36739;&#20998;&#26512;&#20984;&#26174;&#20102;$\texttt{COSMIC}$&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19457v1 Announce Type: cross  Abstract: Assessing the quality of summarizers poses significant challenges. In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks, while preserving task outcomes. We theoretically establish a direct relationship between the resulting error probability of these tasks and the mutual information between source texts and generated summaries. We introduce $\texttt{COSMIC}$ as a practical implementation of this metric, demonstrating its strong correlation with human judgment-based metrics and its effectiveness in predicting downstream task performance. Comparative analyses against established metrics like $\texttt{BERTScore}$ and $\texttt{ROUGE}$ highlight the competitive performance of $\texttt{COSMIC}$.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21151;&#33021;&#21464;&#20307;&#30340;&#22522;&#20934;&#36827;&#34892;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#21457;&#29616;&#38745;&#24577;&#22522;&#20934;&#21644;&#21151;&#33021;&#22522;&#20934;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#25512;&#29702;&#24046;&#36317;</title><link>https://arxiv.org/abs/2402.19450</link><description>&lt;p&gt;
&#29992;&#20110;&#40065;&#26834;&#25512;&#29702;&#24615;&#33021;&#35780;&#20272;&#30340;&#21151;&#33021;&#22522;&#20934;&#21450;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19450
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21151;&#33021;&#21464;&#20307;&#30340;&#22522;&#20934;&#36827;&#34892;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#21457;&#29616;&#38745;&#24577;&#22522;&#20934;&#21644;&#21151;&#33021;&#22522;&#20934;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20934;&#30340;&#21151;&#33021;&#21464;&#20307;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#40065;&#26834;&#35780;&#20272;&#12290;&#35299;&#20915;&#25512;&#29702;&#27979;&#35797;&#30340;&#27169;&#22411;&#22312;&#38745;&#24577;&#38382;&#39064;&#30340;&#34920;&#29616;&#19982;&#21151;&#33021;&#21464;&#20307;&#24555;&#29031;&#30456;&#27604;&#24212;&#35813;&#27809;&#26377;&#24046;&#24322;&#12290;&#25105;&#20204;&#23558;MATH&#22522;&#20934;&#30340;&#30456;&#20851;&#29255;&#27573;&#37325;&#20889;&#20026;&#20854;&#21151;&#33021;&#21464;&#20307;MATH()&#65292;&#24182;&#23558;&#20854;&#20182;&#22522;&#20934;&#30340;&#21151;&#33021;&#21270;&#38543;&#20043;&#32780;&#26469;&#12290;&#22312;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#22312;MATH()&#24555;&#29031;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#25512;&#29702;&#24046;&#36317;--&#38745;&#24577;&#20934;&#30830;&#24615;&#19982;&#21151;&#33021;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30334;&#20998;&#27604;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#22312;&#34920;&#29616;&#33391;&#22909;&#30340;&#38745;&#24577;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#23553;&#38381;&#21644;&#24320;&#25918;&#26435;&#37325;&#27169;&#22411;&#20043;&#38388;&#30340;&#25512;&#29702;&#24046;&#36317;&#65292;&#20174;58.35%&#21040;80.31%&#65292;&#20294;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#24046;&#36317;&#21487;&#33021;&#22312;&#20351;&#29992;&#26356;&#22797;&#26434;&#25552;&#31034;&#31574;&#30053;&#26102;&#26356;&#23567;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#22312;&#30495;&#23454;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#29702;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19450v1 Announce Type: new  Abstract: We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35% to 80.31% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#23548;&#33268;&#20102;&#20248;&#21270;&#21160;&#24577;&#19978;&#30340;&#22256;&#38590;&#65292;Adam&#21644;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.19449</link><description>&lt;p&gt;
Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models
&lt;/p&gt;
&lt;p&gt;
Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19449
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#23548;&#33268;&#20102;&#20248;&#21270;&#21160;&#24577;&#19978;&#30340;&#22256;&#38590;&#65292;Adam&#21644;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;Adam&#22312;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#34920;&#29616;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26102;&#65292;&#19982;&#19981;&#24120;&#35265;&#21333;&#35789;&#30456;&#20851;&#30340;&#25439;&#22833;&#19979;&#38477;&#36895;&#24230;&#27604;&#19982;&#24120;&#35265;&#21333;&#35789;&#30456;&#20851;&#30340;&#25439;&#22833;&#19979;&#38477;&#36895;&#24230;&#24930;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#26679;&#26412;&#26469;&#33258;&#30456;&#23545;&#19981;&#24120;&#35265;&#30340;&#21333;&#35789;&#65292;&#24179;&#22343;&#25439;&#22833;&#20540;&#22312;&#26799;&#24230;&#19979;&#38477;&#26102;&#19979;&#38477;&#36895;&#24230;&#36739;&#24930;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Adam&#21644;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;&#21364;&#19981;&#21463;&#27492;&#38382;&#39064;&#24433;&#21709;&#65292;&#24182;&#25913;&#21892;&#20102;&#25152;&#26377;&#31867;&#21035;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#26550;&#26500;&#21644;&#25968;&#25454;&#31867;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#34892;&#20026;&#30830;&#23454;&#26159;&#30001;&#31867;&#21035;&#19981;&#24179;&#34913;&#24341;&#36215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19449v1 Announce Type: cross  Abstract: Adam has been shown to outperform gradient descent in optimizing large language transformers empirically, and by a larger margin than on other tasks, but it is unclear why this happens. We show that the heavy-tailed class imbalance found in language modeling tasks leads to difficulties in the optimization dynamics. When training with gradient descent, the loss associated with infrequent words decreases slower than the loss associated with frequent ones. As most samples come from relatively infrequent words, the average loss decreases slowly with gradient descent. On the other hand, Adam and sign-based methods do not suffer from this problem and improve predictions on all classes. To establish that this behavior is indeed caused by class imbalance, we show empirically that it persist through different architectures and data types, on language transformers, vision CNNs, and linear models. We further study this phenomenon on a linear clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;LLMs&#30340;&#22810;&#36718;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26694;&#26550;</title><link>https://arxiv.org/abs/2402.19446</link><description>&lt;p&gt;
ArCHer: &#36890;&#36807;&#20998;&#23618;&#22810;&#36718;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;LLMs&#30340;&#22810;&#36718;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#26696;&#20363;&#26159;&#30446;&#26631;&#23548;&#21521;&#30340;&#20915;&#31574;&#20219;&#21153;&#65288;&#25110;&#8220;&#20195;&#29702;&#8221;&#20219;&#21153;&#65289;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;LLM&#19981;&#20165;&#38656;&#35201;&#20026;&#32473;&#23450;&#25552;&#31034;&#29983;&#25104;&#23436;&#25104;&#65292;&#32780;&#19988;&#38656;&#35201;&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#20570;&#20986;&#26234;&#33021;&#20915;&#31574;&#20197;&#23436;&#25104;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#19982;&#32593;&#32476;&#20132;&#20114;&#65292;&#20351;&#29992;&#24037;&#20855;&#25110;&#25552;&#20379;&#23458;&#25143;&#25903;&#25345;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;LLMs&#30340;&#22810;&#36718;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19446v1 Announce Type: cross  Abstract: A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or "agent" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CAPIR&#65288;Compositional API Recommendation&#65289;&#26469;&#20026;&#31895;&#31890;&#24230;&#38656;&#27714;&#25512;&#33616;API&#65292;&#24182;&#37319;&#29992;&#8220;&#20998;&#32780;&#27835;&#20043;&#8221;&#30340;&#31574;&#30053;&#23558;&#20219;&#21153;&#25551;&#36848;&#20998;&#35299;&#20026;&#35814;&#32454;&#30340;&#23376;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.19431</link><description>&lt;p&gt;
&#38754;&#21521;&#24211;&#23548;&#21521;&#20195;&#30721;&#29983;&#25104;&#30340;&#32452;&#21512;&#24335;API&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Compositional API Recommendation for Library-Oriented Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19431
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CAPIR&#65288;Compositional API Recommendation&#65289;&#26469;&#20026;&#31895;&#31890;&#24230;&#38656;&#27714;&#25512;&#33616;API&#65292;&#24182;&#37319;&#29992;&#8220;&#20998;&#32780;&#27835;&#20043;&#8221;&#30340;&#31574;&#30053;&#23558;&#20219;&#21153;&#25551;&#36848;&#20998;&#35299;&#20026;&#35814;&#32454;&#30340;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#38754;&#21521;&#24211;&#30340;&#20195;&#30721;&#26041;&#38754;&#34920;&#29616;&#20173;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;LLM&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#24211;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#21033;&#29992;API&#25512;&#33616;&#25216;&#26415;&#24110;&#21161;LLMs&#20351;&#29992;&#24211;&#65306;&#23427;&#26816;&#32034;&#19982;&#29992;&#25143;&#38656;&#27714;&#30456;&#20851;&#30340;API&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#20316;&#20026;&#19978;&#19979;&#25991;&#26469;&#25552;&#31034;LLMs&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#38656;&#27714;&#21487;&#33021;&#26159;&#31895;&#31890;&#24230;&#30340;&#65292;&#38656;&#35201;&#32467;&#21512;&#22810;&#20010;&#32454;&#31890;&#24230;API&#12290;&#36825;&#31181;&#31890;&#24230;&#19981;&#19968;&#33268;&#20351;API&#25512;&#33616;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAPIR&#65288;&#32452;&#21512;&#24335;API&#25512;&#33616;&#65289;&#65292;&#23427;&#37319;&#29992;&#8220;&#20998;&#32780;&#27835;&#20043;&#8221;&#30340;&#31574;&#30053;&#20026;&#31895;&#31890;&#24230;&#35201;&#27714;&#25512;&#33616;API&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CAPIR&#37319;&#29992;&#22522;&#20110;LLM&#30340;&#20998;&#35299;&#22120;&#23558;&#31895;&#31890;&#24230;&#20219;&#21153;&#25551;&#36848;&#20998;&#35299;&#20026;&#20960;&#20010;&#35814;&#32454;&#30340;&#23376;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;CAPIR&#24212;&#29992;&#22522;&#20110;&#23884;&#20837;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19431v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved exceptional performance in code generation. However, the performance remains unsatisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs. Previous work utilizes API recommendation technology to help LLMs use libraries: it retrieves APIs related to the user requirements, then leverages them as context to prompt LLMs. However, developmental requirements can be coarse-grained, requiring a combination of multiple fine-grained APIs. This granularity inconsistency makes API recommendation a challenging task. To address this, we propose CAPIR (Compositional API Recommendation), which adopts a "divide-and-conquer" strategy to recommend APIs for coarse-grained requirements. Specifically, CAPIR employs an LLM-based Decomposer to break down a coarse-grained task description into several detailed subtasks. Then, CAPIR applies an embedding-based
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; Griffin &#27169;&#22411;&#65292;&#23558;&#38376;&#25511;&#32447;&#24615;&#24490;&#29615;&#19982;&#23616;&#37096;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.19427</link><description>&lt;p&gt;
Griffin: &#23558;&#38376;&#25511;&#32447;&#24615;&#24490;&#29615;&#19982;&#23616;&#37096;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19427
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; Griffin &#27169;&#22411;&#65292;&#23558;&#38376;&#25511;&#32447;&#24615;&#24490;&#29615;&#19982;&#23616;&#37096;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#22312;&#38271;&#24207;&#21015;&#19978;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#21644;&#39640;&#25928;&#25193;&#23637;&#30340;&#20248;&#21183;&#65292;&#20294;&#35757;&#32451;&#22256;&#38590;&#19988;&#38590;&#20197;&#25193;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Hawk&#65292;&#19968;&#31181;&#20855;&#26377;&#38376;&#25511;&#32447;&#24615;&#24490;&#29615;&#30340;RNN&#65292;&#20197;&#21450;Griffin&#65292;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#23558;&#38376;&#25511;&#32447;&#24615;&#24490;&#29615;&#19982;&#23616;&#37096;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#12290;Hawk&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;Mamba&#65292;&#32780;Griffin&#22312;&#35757;&#32451;&#26102;&#20165;&#20351;&#29992;&#20102;6&#20493;&#23569;&#30340;&#20196;&#29260;&#25968;&#37327;&#21364;&#19982;Llama-2&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Griffin&#22312;&#35757;&#32451;&#26399;&#38388;&#21487;&#20197;&#23545;&#27604;&#35757;&#32451;&#26102;&#38271;&#24471;&#22810;&#30340;&#24207;&#21015;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35757;&#32451;&#26102;&#20855;&#26377;&#19982;Transformer&#30456;&#21305;&#37197;&#30340;&#30828;&#20214;&#25928;&#29575;&#65292;&#32780;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#26356;&#20302;&#30340;&#24310;&#36831;&#21644;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#23558;Griffin&#25193;&#23637;&#21040;&#20102;14B&#21442;&#25968;&#65292;&#24182;&#35299;&#37322;&#20102;&#22914;&#20309;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20998;&#29255;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19427v1 Announce Type: cross  Abstract: Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.
&lt;/p&gt;</description></item><item><title>PaECTER&#26159;&#19968;&#20010;&#19987;&#20026;&#19987;&#21033;&#35774;&#35745;&#30340;&#24320;&#25918;&#28304;&#30721;&#25991;&#26723;&#32423;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#24341;&#25991;&#20449;&#24687;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#21033;&#25991;&#26723;&#30340;&#25968;&#20540;&#34920;&#31034;&#65292;&#24182;&#22312;&#19987;&#21033;&#39046;&#22495;&#30340;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.19411</link><description>&lt;p&gt;
PaECTER&#65306;&#20351;&#29992;&#24341;&#25991;&#20449;&#24687;&#30340;&#19987;&#21033;&#32423;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PaECTER: Patent-level Representation Learning using Citation-informed Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19411
&lt;/p&gt;
&lt;p&gt;
PaECTER&#26159;&#19968;&#20010;&#19987;&#20026;&#19987;&#21033;&#35774;&#35745;&#30340;&#24320;&#25918;&#28304;&#30721;&#25991;&#26723;&#32423;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#24341;&#25991;&#20449;&#24687;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#21033;&#25991;&#26723;&#30340;&#25968;&#20540;&#34920;&#31034;&#65292;&#24182;&#22312;&#19987;&#21033;&#39046;&#22495;&#30340;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PaECTER&#26159;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#12289;&#38754;&#21521;&#19987;&#21033;&#30340;&#25991;&#26723;&#32423;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#21033;&#29992;&#23457;&#26680;&#21592;&#28155;&#21152;&#30340;&#24341;&#25991;&#20449;&#24687;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#20026;&#19987;&#21033;&#25991;&#26723;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#12290;&#19982;&#19987;&#21033;&#39046;&#22495;&#20013;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;PaECTER&#22312;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19987;&#21033;&#24341;&#25991;&#39044;&#27979;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20004;&#31181;&#19981;&#21516;&#30340;&#25490;&#21517;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#19979;&#19968;&#20010;&#26368;&#20339;&#19987;&#21033;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#19987;&#21033;BERT&#65289;&#12290;&#19982;25&#20010;&#19981;&#30456;&#20851;&#30340;&#19987;&#21033;&#30456;&#27604;&#65292;PaECTER&#22312;&#24179;&#22343;&#25490;&#21517;1.32&#22788;&#39044;&#27979;&#21040;&#33267;&#23569;&#19968;&#20010;&#26368;&#30456;&#20284;&#30340;&#19987;&#21033;&#12290;PaECTER&#20174;&#19987;&#21033;&#25991;&#26412;&#29983;&#25104;&#30340;&#25968;&#20540;&#34920;&#31034;&#21487;&#29992;&#20110;&#20998;&#31867;&#12289;&#36861;&#36394;&#30693;&#35782;&#27969;&#21160;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#25628;&#32034;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#35821;&#20041;&#30456;&#20284;&#24615;&#25628;&#32034;&#22312;&#21457;&#26126;&#20154;&#21644;&#19987;&#21033;&#30340;&#20808;&#21069;&#25216;&#26415;&#25628;&#32034;&#32972;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19411v1 Announce Type: cross  Abstract: PaECTER is a publicly available, open-source document-level encoder specific for patents. We fine-tune BERT for Patents with examiner-added citation information to generate numerical representations for patent documents. PaECTER performs better in similarity tasks than current state-of-the-art models used in the patent domain. More specifically, our model outperforms the next-best patent specific pre-trained language model (BERT for Patents) on our patent citation prediction test dataset on two different rank evaluation metrics. PaECTER predicts at least one most similar patent at a rank of 1.32 on average when compared against 25 irrelevant patents. Numerical representations generated by PaECTER from patent text can be used for downstream tasks such as classification, tracing knowledge flows, or semantic similarity search. Semantic similarity search is especially relevant in the context of prior art search for both inventors and paten
&lt;/p&gt;</description></item><item><title>&#22320;&#29702;&#30693;&#35782;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#32780;&#19968;&#33268;&#25193;&#23637;&#65292;&#20294;&#26356;&#22823;&#30340;&#27169;&#22411;&#26080;&#27861;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22320;&#29702;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.19406</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#22320;&#29702;&#34920;&#31034;&#30340;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Scaling Laws of Geographical Representation in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19406
&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#30693;&#35782;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#32780;&#19968;&#33268;&#25193;&#23637;&#65292;&#20294;&#26356;&#22823;&#30340;&#27169;&#22411;&#26080;&#27861;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22320;&#29702;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#38271;&#26399;&#20197;&#26469;&#34987;&#35777;&#26126;&#22312;&#20854;&#38544;&#34255;&#34920;&#31034;&#20013;&#23884;&#20837;&#20102;&#22320;&#29702;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#23558;&#36825;&#19968;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12290;&#26412;&#25991;&#36890;&#36807;&#35266;&#23519;&#35821;&#35328;&#27169;&#22411;&#35268;&#27169;&#25193;&#22823;&#26102;&#22320;&#29702;&#30693;&#35782;&#30340;&#28436;&#21270;&#65292;&#25552;&#20986;&#22635;&#34917;&#29616;&#26377;&#21644;&#26368;&#36817;&#25991;&#29486;&#20043;&#38388;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#23545;&#20110;&#24494;&#23567;&#27169;&#22411;&#65292;&#22320;&#29702;&#30693;&#35782;&#20063;&#26159;&#21487;&#35266;&#27979;&#30340;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#19968;&#33268;&#25193;&#23637;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#22320;&#29702;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19406v1 Announce Type: cross  Abstract: Language models have long been shown to embed geographical information in their hidden representations. This line of work has recently been revisited by extending this result to Large Language Models (LLMs). In this paper, we propose to fill the gap between well-established and recent literature by observing how geographical knowledge evolves when scaling language models. We show that geographical knowledge is observable even for tiny models, and that it scales consistently as we increase the model size. Notably, we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#20219;&#21153;&#21644;&#23545;&#40784;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.19404</link><description>&lt;p&gt;
&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26694;&#26550;&#29992;&#20110;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Entity-Aware Multimodal Alignment Framework for News Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19404
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#20219;&#21153;&#21644;&#23545;&#40784;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#26159;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#35201;&#27714;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#23383;&#24149;&#65292;&#20854;&#20013;&#21253;&#21547;&#26032;&#38395;&#22270;&#20687;&#21644;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#12290;&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#36805;&#36895;&#65292;&#24182;&#22312;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#24120;&#35265;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#23450;&#19979;&#29983;&#25104;&#23454;&#20307;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#21363;&#20351;&#22312;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#31616;&#21333;&#24494;&#35843;&#65292;&#23427;&#20204;&#22788;&#29702;&#23454;&#20307;&#20449;&#24687;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#23454;&#20307;&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#22810;&#27169;&#24577;&#23454;&#20307;&#24863;&#30693;&#23545;&#40784;&#20219;&#21153;&#21644;&#19968;&#20010;&#23545;&#40784;&#26694;&#26550;&#65292;&#20197;&#23545;&#40784;&#27169;&#22411;&#24182;&#29983;&#25104;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GoodNews&#25968;&#25454;&#38598;&#19978;&#23558;CIDEr&#20998;&#25968;&#25552;&#39640;&#21040;86.29&#65288;&#20174;72.33&#65289;&#65292;&#22312;NYTimes800k&#25968;&#25454;&#38598;&#19978;&#23558;&#20854;&#25552;&#39640;&#21040;85.61&#65288;&#20174;70.83&#65289;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19404v1 Announce Type: cross  Abstract: News image captioning task is a variant of image captioning task which requires model to generate a more informative caption with news image and the associated news article. Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-tuned on news image captioning dataset. To obtain a more powerful model to handle the multimodal entity information, we design two multimodal entity-aware alignment tasks and an alignment framework to align the model and generate the news image captions. Our method achieves better results than previous state-of-the-art models in CIDEr score (72.33 -&gt; 86.29) on GoodNews dataset and (70.83 -&gt; 85.61) on NYTimes800k dataset.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.19379</link><description>&lt;p&gt;
&#30789;&#35895;&#20154;&#32676;&#30340;&#26234;&#24935;&#65306;LLM&#38598;&#25104;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#20154;&#32676;&#20934;&#30830;&#29575;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;&#20381;&#36182;&#20110;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#25928;&#24212;&#65292;&#21363;&#36890;&#36807;&#32858;&#21512;&#19968;&#32676;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#36807;&#21435;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#21069;&#27839;LLMs&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#20154;&#31867;&#32676;&#20307;&#39044;&#27979;&#27604;&#36187;&#30340;&#40644;&#37329;&#26631;&#20934;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30001;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;31&#20010;&#20108;&#20803;&#38382;&#39064;&#30340;&#32858;&#21512;LLM&#39044;&#27979;&#19982;&#19968;&#20010;&#26469;&#33258;&#19977;&#20010;&#26376;&#39044;&#27979;&#27604;&#36187;&#30340;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#32676;&#20307;&#30340;&#34920;&#29616;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#31181;&#39034;&#20174;&#25928;&#24212;&#65292;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26126;&#26174;&#39640;&#20110;50%&#65292;&#23613;&#31649;&#20960;&#20046;&#26159;&#24179;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
&lt;/p&gt;</description></item><item><title>OpenMedLM &#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#24179;&#21488;&#65292;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#22312;&#21307;&#23398;&#38382;&#31572;&#20013;&#33021;&#22815;&#36229;&#36234;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#21307;&#23398;&#22522;&#20934;&#19978;&#30340; SOTA &#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19371</link><description>&lt;p&gt;
OpenMedLM&#65306;&#22312;&#21307;&#23398;&#38382;&#31572;&#20013;&#65292;&#25552;&#31034;&#24037;&#31243;&#21487;&#20197;&#32988;&#36807;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19371
&lt;/p&gt;
&lt;p&gt;
OpenMedLM &#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#24179;&#21488;&#65292;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#22312;&#21307;&#23398;&#38382;&#31572;&#20013;&#33021;&#22815;&#36229;&#36234;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#21307;&#23398;&#22522;&#20934;&#19978;&#30340; SOTA &#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs &#22312;&#23436;&#25104;&#19968;&#31995;&#21015;&#19987;&#38376;&#20219;&#21153;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#26469;&#25193;&#22823;&#23545;&#21307;&#23398;&#30693;&#35782;&#30340;&#20844;&#24179;&#35775;&#38382;&#12290;&#22823;&#22810;&#25968;&#21307;&#23398; LLMs &#37117;&#28041;&#21450;&#22823;&#37327;&#24494;&#35843;&#65292;&#21033;&#29992;&#19987;&#38376;&#30340;&#21307;&#23398;&#25968;&#25454;&#21644;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#27492;&#25104;&#26412;&#39640;&#26114;&#12290;&#35768;&#22810;&#34920;&#29616;&#21069;&#21015;&#30340; LLMs &#26159;&#19987;&#26377;&#30340;&#65292;&#20182;&#20204;&#30340;&#35775;&#38382;&#20165;&#38480;&#20110;&#23569;&#25968;&#30740;&#31350;&#22242;&#20307;&#12290;&#28982;&#32780;&#65292;&#24320;&#28304;&#65288;OS&#65289;&#27169;&#22411;&#20195;&#34920;&#20102;&#21307;&#23398; LLMs &#30340;&#19968;&#20010;&#37325;&#35201;&#22686;&#38271;&#39046;&#22495;&#65292;&#30001;&#20110;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#20197;&#21450;&#25552;&#20379;&#21355;&#29983;&#20445;&#20581;&#25152;&#38656;&#30340;&#36879;&#26126;&#24230;&#21644;&#21512;&#35268;&#24615;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; OpenMedLM&#65292;&#36825;&#26159;&#19968;&#20010;&#25552;&#31034;&#24179;&#21488;&#65292;&#20026;&#21307;&#23398;&#22522;&#20934;&#19978;&#30340; OS LLMs &#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#21307;&#23398;&#22522;&#20934;&#65288;MedQA&#12289;MedMCQA&#12289;PubMedQA&#12289;MMLU &#21307;&#23398;&#23376;&#38598;&#65289;&#19978;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015; OS &#22522;&#30784; LLMs&#65288;7B-70B&#65289;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31995;&#21015;&#25552;&#31034;&#31574;&#30053;&#65292;&#21253;&#25324;&#38646;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19371v1 Announce Type: cross  Abstract: LLMs have become increasingly capable at accomplishing a range of specialized-tasks and can be utilized to expand equitable access to medical knowledge. Most medical LLMs have involved extensive fine-tuning, leveraging specialized medical data and significant, thus costly, amounts of computational power. Many of the top performing LLMs are proprietary and their access is limited to very few research groups. However, open-source (OS) models represent a key area of growth for medical LLMs due to significant improvements in performance and an inherent ability to provide the transparency and compliance required in healthcare. We present OpenMedLM, a prompting platform which delivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks. We evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks (MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of prompting strategies, including zero-s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65292;&#20174;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#30340;&#35282;&#24230;&#36830;&#25509;&#36755;&#20837;&#25991;&#27573;&#21644;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.19350</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65292;&#20174;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#30340;&#35282;&#24230;&#36830;&#25509;&#36755;&#20837;&#25991;&#27573;&#21644;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21033;&#29992;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#27169;&#25311;&#20154;&#31867;&#25512;&#29702;&#21644;&#25512;&#26029;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#36339;QA&#26041;&#38754;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#26102;&#65292;PLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#20154;&#31867;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#24515;&#29702;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#65292;&#36755;&#20837;&#25991;&#27573;&#20013;&#30340;&#26174;&#24335;&#20449;&#24687;&#19982;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#26410;&#33021;&#20805;&#20998;&#20851;&#27880;&#20174;&#20154;&#31867;&#35748;&#30693;&#30740;&#31350;&#30340;&#35282;&#24230;&#38142;&#25509;&#36755;&#20837;&#25991;&#27573;&#21644;&#22522;&#20110;PLMs&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#65288;PEI&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#25552;&#31034;&#36830;&#25509;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#65292;&#19982;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#23545;&#40784;&#65292;&#29992;&#20110;&#22810;&#36339;QA&#12290;&#25105;&#20204;&#23558;&#36755;&#20837;&#25991;&#27573;&#35270;&#20026;&#26174;&#24335;&#30693;&#35782;&#65292;&#21033;&#29992;&#23427;&#20204;&#36890;&#36807;&#32479;&#19968;&#25552;&#31034;&#25512;&#23548;&#38544;&#24335;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19350v1 Announce Type: new  Abstract: Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to simulate human reasoning and inference processes, achieving proficient performance in multi-hop QA. However, a gap persists between PLMs' reasoning abilities and those of humans when tackling complex problems. Psychological studies suggest a vital connection between explicit information in passages and human prior knowledge during reading. Nevertheless, current research has given insufficient attention to linking input passages and PLMs' pre-training-based knowledge from the perspective of human cognition studies. In this study, we introduce a \textbf{P}rompting \textbf{E}xplicit and \textbf{I}mplicit knowledge (PEI) framework, which uses prompts to connect explicit and implicit knowledge, aligning with human reading process for multi-hop QA. We consider the input passages as explicit knowledge, employing them to elicit implicit knowledge through unified prompt reason
&lt;/p&gt;</description></item><item><title>&#23558;&#24102;&#21518;&#38376;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#21516;&#31867;&#27169;&#22411;&#21512;&#24182;&#21487;&#20197;&#26377;&#25928;&#27835;&#30103;&#21518;&#38376;&#28431;&#27934;&#65292;&#20026;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#25512;&#29702;&#38454;&#27573;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#38450;&#24481;</title><link>https://arxiv.org/abs/2402.19334</link><description>&lt;p&gt;
&#36825;&#37324;&#26377;&#19968;&#20010;&#20813;&#36153;&#21320;&#39184;&#65306;&#20351;&#29992;&#27169;&#22411;&#21512;&#24182;&#28040;&#27602;&#24102;&#21518;&#38376;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19334
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24102;&#21518;&#38376;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#21516;&#31867;&#27169;&#22411;&#21512;&#24182;&#21487;&#20197;&#26377;&#25928;&#27835;&#30103;&#21518;&#38376;&#28431;&#27934;&#65292;&#20026;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#25512;&#29702;&#38454;&#27573;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#28304;&#20513;&#35758;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27665;&#20027;&#21270;&#24555;&#36895;&#25512;&#21160;&#20102;&#21019;&#26032;&#65292;&#24182;&#25193;&#22823;&#20102;&#23545;&#23574;&#31471;&#25216;&#26415;&#30340;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24320;&#25918;&#24615;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#23433;&#20840;&#39118;&#38505;&#65292;&#21253;&#25324;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#38544;&#34255;&#30340;&#24694;&#24847;&#34892;&#20026;&#30001;&#29305;&#23450;&#36755;&#20837;&#35302;&#21457;&#65292;&#25439;&#23475;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#24314;&#35758;&#36890;&#36807;&#23558;&#24102;&#21518;&#38376;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#21516;&#31867;&#27169;&#22411;&#21512;&#24182;&#65292;&#21487;&#20197;&#27835;&#30103;&#21518;&#38376;&#28431;&#27934;&#65292;&#21363;&#20351;&#36825;&#20123;&#27169;&#22411;&#24182;&#38750;&#20840;&#37096;&#23433;&#20840;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#27169;&#22411;&#65288;BERT-Base&#12289;RoBERTa-Large&#12289;Llama2-7B&#21644;Mistral-7B&#65289;&#21644;&#25968;&#25454;&#38598;&#65288;SST-2&#12289;OLID&#12289;AG News&#21644;QNLI&#65289;&#12290;&#19982;&#22810;&#31181;&#20808;&#36827;&#30340;&#38450;&#24481;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#25512;&#29702;&#38454;&#27573;&#23545;&#25239;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#36164;&#28304;&#25110;&#29305;&#23450;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#34920;&#29616;&#20248;&#31168;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19334v1 Announce Type: new  Abstract: The democratization of pre-trained language models through open-source initiatives has rapidly advanced innovation and expanded access to cutting-edge technologies. However, this openness also brings significant security risks, including backdoor attacks, where hidden malicious behaviors are triggered by specific inputs, compromising natural language processing (NLP) system integrity and reliability. This paper suggests that merging a backdoored model with other homogeneous models can remediate backdoor vulnerabilities even if such models are not entirely secure. In our experiments, we explore various models (BERT-Base, RoBERTa-Large, Llama2-7B, and Mistral-7B) and datasets (SST-2, OLID, AG News, and QNLI). Compared to multiple advanced defensive approaches, our method offers an effective and efficient inference-stage defense against backdoor attacks without additional resources or specific knowledge. Our approach consistently outperform
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#19978;&#39044;&#35757;&#32451;&#36739;&#23567;&#27169;&#22411;&#65292;&#20197;&#33976;&#39311;SSL&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#32039;&#20945;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#20855;&#26377;&#30701;&#25512;&#29702;&#31649;&#36947;&#21644;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#31561;&#20248;&#28857;</title><link>https://arxiv.org/abs/2402.19333</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#39044;&#35757;&#32451;&#23454;&#29616;&#32039;&#20945;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compact Speech Translation Models via Discrete Speech Units Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19333
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#19978;&#39044;&#35757;&#32451;&#36739;&#23567;&#27169;&#22411;&#65292;&#20197;&#33976;&#39311;SSL&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#32039;&#20945;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#20855;&#26377;&#30701;&#25512;&#29702;&#31649;&#36947;&#21644;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#31561;&#20248;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#27169;&#22411;&#21021;&#22987;&#21270;&#22914;&#20170;&#22312;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#20013;&#33719;&#24471;&#24378;&#22823;&#32467;&#26524;&#26159;&#24120;&#35265;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#20250;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#65292;&#38459;&#30861;&#20102;&#35774;&#22791;&#37096;&#32626;&#12290;&#26412;&#25991;&#21033;&#29992;SSL&#27169;&#22411;&#36890;&#36807;&#22312;&#20854;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#65288;DSU&#65289;&#19978;&#39044;&#35757;&#32451;&#36739;&#23567;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;1&#65289;Filterbank-to-DSU&#21644;2&#65289;DSU-to-Translation&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#28982;&#21518;&#21462;&#33258;1&#65289;&#30340;&#32534;&#30721;&#22120;&#21644;&#26469;&#33258;2&#65289;&#30340;&#35299;&#30721;&#22120;&#26469;&#21021;&#22987;&#21270;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#22312;&#26377;&#38480;&#30340;&#35821;&#38899;&#32763;&#35793;&#25968;&#25454;&#19978;&#24494;&#35843;&#12290;&#36890;&#36807;&#20351;&#29992;DSU&#39044;&#35757;&#32451;&#26469;&#25552;&#28860;SSL&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#26368;&#32456;&#27169;&#22411;&#21464;&#24471;&#32039;&#20945;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#20351;&#29992;DSU&#20316;&#20026;&#27169;&#22411;&#36755;&#20837;&#26377;&#20960;&#20010;&#20248;&#28857;&#65292;&#27604;&#22914;&#25512;&#29702;&#31649;&#36947;&#26356;&#30701;&#21644;&#23545;&#65288;DSU&#65289;&#26631;&#35760;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#19982;ASR&#39044;&#35757;&#32451;&#30456;&#27604;&#65292;&#23427;&#19981;&#38656;&#35201;&#36716;&#24405;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#36164;&#28304;&#21294;&#20047;&#30340;&#29615;&#22659;&#12290;&#22312;CoVoST-2 X-En&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19333v1 Announce Type: new  Abstract: Using Self-Supervised Learning (SSL) as model initialization is now common to obtain strong results in Speech Translation (ST). However, they also impose a large memory footprint, hindering on-device deployment. In this paper, we leverage the SSL models by pretraining smaller models on their Discrete Speech Units (DSU). We pretrain encoder-decoder models on 1) Filterbank-to-DSU and 2) DSU-to-Translation data, and take the encoder from 1) and the decoder from 2) to initialise a new model, finetuning this on limited speech-translation data. The final model becomes compact by using the DSU pretraining to distil the knowledge of the SSL model. Our method has several benefits over using DSU as model inputs, such as shorter inference pipeline and robustness over (DSU) tokenization. In contrast to ASR pretraining, it does not require transcripts, making it applicable to low-resource settings. Evaluation on CoVoST-2 X-En shows that our method is
&lt;/p&gt;</description></item><item><title>WanJuan-CC&#26159;&#19968;&#20010;&#23433;&#20840;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;Common Crawl&#25968;&#25454;&#24182;&#32463;&#36807;&#22810;&#39033;&#31579;&#36873;&#21644;&#36807;&#28388;&#27493;&#39588;&#24471;&#21040;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.19282</link><description>&lt;p&gt;
WanJuan-CC&#65306;&#19968;&#20010;&#23433;&#20840;&#19988;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19282
&lt;/p&gt;
&lt;p&gt;
WanJuan-CC&#26159;&#19968;&#20010;&#23433;&#20840;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;Common Crawl&#25968;&#25454;&#24182;&#32463;&#36807;&#22810;&#39033;&#31579;&#36873;&#21644;&#36807;&#28388;&#27493;&#39588;&#24471;&#21040;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; WanJuan-CC&#65292;&#36825;&#26159;&#19968;&#20010;&#23433;&#20840;&#19988;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#26469;&#28304;&#20110;Common Crawl&#25968;&#25454;&#12290;&#30740;&#31350;&#35299;&#20915;&#20102;&#20026;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27969;&#31243;&#26469;&#22788;&#29702;Common Crawl&#25968;&#25454;&#65292;&#21253;&#25324;&#25552;&#21462;&#12289;&#21551;&#21457;&#24335;&#35268;&#21017;&#36807;&#28388;&#12289;&#27169;&#31946;&#21435;&#37325;&#12289;&#20869;&#23481;&#23433;&#20840;&#36807;&#28388;&#21644;&#25968;&#25454;&#36136;&#37327;&#36807;&#28388;&#12290;&#20174;&#22823;&#32422;680&#20159;&#20010;&#21407;&#22987;&#33521;&#25991;&#25991;&#26723;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;22&#19975;&#20159;&#26631;&#35760;&#30340;&#23433;&#20840;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#36873;&#20986;&#20102;10&#19975;&#20159;&#26631;&#35760;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20316;&#20026;WanJuan-CC&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#24050;&#32463;&#24320;&#28304;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;3000&#20159;&#26631;&#35760;&#12290;&#35813;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19982;&#25968;&#25454;&#36136;&#37327;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36873;&#25321;&#36866;&#24403;&#30340;&#25968;&#25454;&#12290;&#20026;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;WanJuan-CC&#35757;&#32451;&#20102;10&#20159;&#21442;&#25968;&#21644;30&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19282v1 Announce Type: new  Abstract: This paper presents WanJuan-CC, a safe and high-quality open-sourced English webtext dataset derived from Common Crawl data. The study addresses the challenges of constructing large-scale pre-training datasets for language models, which require vast amounts of high-quality data. A comprehensive process was designed to handle Common Crawl data, including extraction, heuristic rule filtering, fuzzy deduplication, content safety filtering, and data quality filtering. From approximately 68 billion original English documents, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens of high-quality data as part of WanJuan-CC. We have open-sourced 300B Tokens from this dataset. The paper also provides statistical information related to data quality, enabling users to select appropriate data according to their needs. To evaluate the quality and utility of the dataset, we trained 1B-parameter and 3B-parameter models using WanJuan-CC and ano
&lt;/p&gt;</description></item><item><title>PlanGPT&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#22478;&#24066;&#21644;&#31354;&#38388;&#35268;&#21010;&#37327;&#36523;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23450;&#21046;&#25968;&#25454;&#24211;&#26816;&#32034;&#26694;&#26550;&#12289;&#39046;&#22495;&#29305;&#23450;&#24494;&#35843;&#21644;&#20808;&#36827;&#24037;&#20855;&#21151;&#33021;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24615;&#33021;&#34920;&#29616;&#21644;&#25552;&#20379;&#39640;&#36136;&#37327;&#35268;&#21010;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.19273</link><description>&lt;p&gt;
PlanGPT: &#29992;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#21644;&#39640;&#25928;&#26816;&#32034;&#22686;&#24378;&#22478;&#24066;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19273
&lt;/p&gt;
&lt;p&gt;
PlanGPT&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#22478;&#24066;&#21644;&#31354;&#38388;&#35268;&#21010;&#37327;&#36523;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23450;&#21046;&#25968;&#25454;&#24211;&#26816;&#32034;&#26694;&#26550;&#12289;&#39046;&#22495;&#29305;&#23450;&#24494;&#35843;&#21644;&#20808;&#36827;&#24037;&#20855;&#21151;&#33021;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24615;&#33021;&#34920;&#29616;&#21644;&#25552;&#20379;&#39640;&#36136;&#37327;&#35268;&#21010;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#35268;&#21010;&#39046;&#22495;&#65292;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#28385;&#36275;&#35268;&#21010;&#32773;&#30340;&#29305;&#23450;&#38656;&#27714;&#12290;&#29983;&#25104;&#22478;&#24066;&#35268;&#21010;&#25991;&#26412;&#12289;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#21644;&#35780;&#20272;&#35268;&#21010;&#25991;&#20214;&#31561;&#20219;&#21153;&#37117;&#23384;&#22312;&#29420;&#29305;&#25361;&#25112;&#12290;&#20026;&#25552;&#39640;&#22478;&#24066;&#19987;&#19994;&#20154;&#21592;&#30340;&#25928;&#29575;&#24182;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PlanGPT&#65292;&#31532;&#19968;&#20010;&#19987;&#20026;&#22478;&#24066;&#21644;&#31354;&#38388;&#35268;&#21010;&#37327;&#36523;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#19982;&#20013;&#22269;&#22478;&#24066;&#35268;&#21010;&#30740;&#31350;&#38498;&#31561;&#26426;&#26500;&#30340;&#21512;&#20316;&#21162;&#21147;&#24320;&#21457;&#65292;PlanGPT&#21033;&#29992;&#23450;&#21046;&#30340;&#26412;&#22320;&#25968;&#25454;&#24211;&#26816;&#32034;&#26694;&#26550;&#12289;&#22522;&#30784;&#27169;&#22411;&#30340;&#39046;&#22495;&#29305;&#23450;&#24494;&#35843;&#21644;&#20808;&#36827;&#30340;&#24037;&#20855;&#21151;&#33021;&#12290;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#65292;PlanGPT&#21462;&#24471;&#20102;&#20808;&#36827;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#31934;&#30830;&#36866;&#24212;&#22478;&#24066;&#35268;&#21010;&#22797;&#26434;&#24615;&#30340;&#39640;&#36136;&#37327;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19273v1 Announce Type: new  Abstract: In the field of urban planning, general-purpose large language models often struggle to meet the specific needs of planners. Tasks like generating urban planning texts, retrieving related information, and evaluating planning documents pose unique challenges. To enhance the efficiency of urban professionals and overcome these obstacles, we introduce PlanGPT, the first specialized Large Language Model tailored for urban and spatial planning. Developed through collaborative efforts with institutions like the Chinese Academy of Urban Planning, PlanGPT leverages a customized local database retrieval framework, domain-specific fine-tuning of base models, and advanced tooling capabilities. Empirical tests demonstrate that PlanGPT has achieved advanced performance, delivering responses of superior quality precisely tailored to the intricacies of urban planning.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#20196;&#20154;&#22256;&#25200;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#32763;&#35793;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.19267</link><description>&lt;p&gt;
&#24378;&#22823;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#25351;&#23548;&#65306;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#20196;&#20154;&#22256;&#25200;&#30340;&#21629;&#21517;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19267
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#20196;&#20154;&#22256;&#25200;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#32763;&#35793;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#38598;&#21487;&#20197;&#35757;&#32451;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#20934;&#30830;&#32763;&#35793;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#21477;&#23376;&#12290;&#33719;&#24471;&#21644;&#32763;&#35793;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#34429;&#28982;&#25104;&#26412;&#39640;&#26114;&#65292;&#20294;&#23545;&#20110;&#39640;&#36136;&#37327;&#32763;&#35793;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#25214;&#21040;&#26368;&#8220;&#26377;&#25928;&#8221;&#30340;&#25968;&#25454;&#25104;&#20026;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#30340;&#23454;&#29992;&#31574;&#30053;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#8220;&#36866;&#24403;&#22256;&#38590;&#30340;&#25968;&#25454;&#8221;&#26469;&#25214;&#21040;&#36825;&#20123;&#26377;&#25928;&#25968;&#25454;&#65292;&#36825;&#24847;&#21619;&#30528;&#25968;&#25454;&#19981;&#24212;&#36807;&#20110;&#22256;&#38590;&#25110;&#36807;&#20110;&#31616;&#21333;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#24314;&#31435;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26631;&#20934;&#20173;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#8220;&#36866;&#24403;&#22256;&#38590;&#24230;&#8221;&#21487;&#33021;&#22240;&#25152;&#35757;&#32451;&#30340;&#25968;&#25454;&#39046;&#22495;&#32780;&#24322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#8216;Capturing Perplexing Named Entities&#8217;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19267v1 Announce Type: cross  Abstract: Employing extensive datasets enables the training of multilingual machine translation models; however, these models often fail to accurately translate sentences within specialized domains. Although obtaining and translating domain-specific data incurs high costs, it is inevitable for high-quality translations. Hence, finding the most 'effective' data with an unsupervised setting becomes a practical strategy for reducing labeling costs. Recent research indicates that this effective data could be found by selecting 'properly difficult data' based on its volume. This means the data should not be excessively challenging or overly simplistic, especially if the amount of data is limited. However, we found that establishing a criterion for unsupervised data selection remains challenging, as the 'proper difficulty' might vary based on the data domain being trained on. We introduce a novel unsupervised data selection method, 'Capturing Perplexi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24335;&#23567;&#23398;&#25968;&#23398;&#25968;&#25454;&#38598;&#65288;GSM-Plus&#65289;&#65292;&#35780;&#20272;&#20102;25&#20010;LLMs&#21644;4&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#24191;&#27867;&#30340;&#38382;&#39064;&#21464;&#21270;&#20013;&#23637;&#31034;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#36828;&#38750;&#31283;&#20581;&#12290;</title><link>https://arxiv.org/abs/2402.19255</link><description>&lt;p&gt;
GSM-Plus&#65306;&#35780;&#20272;LLMs&#20316;&#20026;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#32773;&#30340;&#31283;&#20581;&#24615;&#30340;&#20840;&#38754;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19255
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24335;&#23567;&#23398;&#25968;&#23398;&#25968;&#25454;&#38598;&#65288;GSM-Plus&#65289;&#65292;&#35780;&#20272;&#20102;25&#20010;LLMs&#21644;4&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#24191;&#27867;&#30340;&#38382;&#39064;&#21464;&#21270;&#20013;&#23637;&#31034;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#36828;&#38750;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#24182;&#24212;&#29992;&#25968;&#23398;&#30693;&#35782;&#65292;&#36824;&#26159;&#20165;&#20165;&#20381;&#36182;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#25463;&#24452;&#65292;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20105;&#35770;&#12290;&#19968;&#20010;&#22522;&#26412;&#19988;&#32463;&#24120;&#21457;&#29983;&#30340;&#35777;&#25454;&#26159;&#65292;&#24403;&#25968;&#23398;&#38382;&#39064;&#31245;&#20316;&#26356;&#25913;&#26102;&#65292;LLMs&#21487;&#33021;&#20250;&#20986;&#29616;&#38169;&#35823;&#34892;&#20026;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#36890;&#36807;&#27979;&#35797;&#21508;&#31181;&#38382;&#39064;&#21464;&#21270;&#26469;&#35780;&#20272;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#25239;&#24335;&#23567;&#23398;&#25968;&#23398;&#65288;\datasetname&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#23545;GSM8K&#30340;&#25193;&#23637;&#65292;&#24182;&#28155;&#21152;&#20102;&#21508;&#31181;&#25968;&#23398;&#25200;&#21160;&#12290;&#25105;&#20204;&#23545;25&#20010;LLMs&#21644;4&#31181;&#25552;&#31034;&#25216;&#26415;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#34429;&#28982;LLMs&#23637;&#29616;&#20986;&#19981;&#21516;&#27700;&#24179;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#34920;&#29616;&#36828;&#38750;&#31283;&#20581;&#12290;&#29305;&#21035;&#26159;&#65292;&#21363;&#20351;&#26159;&#22312;GSM8K&#20013;&#24050;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;LLMs&#20063;&#21487;&#33021;&#20986;&#38169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19255v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (\datasetname) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CDQA&#65292;&#19968;&#20010;&#20013;&#25991;&#21160;&#24577;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22238;&#31572;&#21160;&#24577;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;&#31934;&#32454;&#26679;&#26412;&#20998;&#31867;&#23454;&#29616;&#20102;&#23545;LLMs&#33021;&#21147;&#26356;&#32454;&#33268;&#30340;&#35266;&#23519;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CDQA&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#20540;&#24471;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.19248</link><description>&lt;p&gt;
&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#23545;&#26368;&#26032;&#25361;&#25112;&#65281;&#19968;&#20010;&#20013;&#25991;&#21160;&#24577;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CDQA&#65292;&#19968;&#20010;&#20013;&#25991;&#21160;&#24577;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22238;&#31572;&#21160;&#24577;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;&#31934;&#32454;&#26679;&#26412;&#20998;&#31867;&#23454;&#29616;&#20102;&#23545;LLMs&#33021;&#21147;&#26356;&#32454;&#33268;&#30340;&#35266;&#23519;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CDQA&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#20540;&#24471;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19248v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#22914;&#20309;&#26356;&#22909;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#26159;&#24403;&#21069;LLMs&#30740;&#31350;&#30340;&#28966;&#28857;&#21644;&#28909;&#28857;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#30001;&#20110;&#22823;&#35268;&#27169;&#36845;&#20195;&#26356;&#26032;LLMs&#30340;&#25104;&#26412;&#26497;&#39640;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#24456;&#22909;&#22320;&#22238;&#31572;&#26368;&#26032;&#30340;&#21160;&#24577;&#38382;&#39064;&#12290;&#20026;&#20102;&#20419;&#36827;&#20013;&#25991;LLMs&#22238;&#31572;&#21160;&#24577;&#38382;&#39064;&#30340;&#33021;&#21147;&#25552;&#21319;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; CDQA&#65292;&#19968;&#20010;&#21253;&#21547;&#19982;&#20013;&#22269;&#20114;&#32852;&#32593;&#19978;&#26368;&#26032;&#26032;&#38395;&#30456;&#20851;&#30340;&#38382;&#31572;&#23545;&#30340;&#20013;&#25991;&#21160;&#24577;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20154;&#31867;&#21644;&#27169;&#22411;&#32467;&#21512;&#30340;&#27969;&#31243;&#33719;&#24471;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#24182;&#26681;&#25454;&#31572;&#26696;&#21464;&#21270;&#39057;&#29575;&#31934;&#32454;&#20998;&#31867;&#26679;&#26412;&#65292;&#20197;&#20415;&#26356;&#32454;&#33268;&#22320;&#35266;&#23519;LLMs&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#22312;CDQA&#19978;&#35780;&#20272;&#21644;&#20998;&#26512;&#20102;&#20027;&#27969;&#21644;&#20808;&#36827;&#30340;&#20013;&#25991;LLMs&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#23453;&#36149;&#30340;&#35265;&#35299;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;CDQA&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#20540;&#24471;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19248v1 Announce Type: new  Abstract: How to better evaluate the capabilities of Large Language Models (LLMs) is the focal point and hot topic in current LLMs research. Previous work has noted that due to the extremely high cost of iterative updates of LLMs, they are often unable to answer the latest dynamic questions well. To promote the improvement of Chinese LLMs' ability to answer dynamic questions, in this paper, we introduce CDQA, a Chinese Dynamic QA benchmark containing question-answer pairs related to the latest news on the Chinese Internet. We obtain high-quality data through a pipeline that combines humans and models, and carefully classify the samples according to the frequency of answer changes to facilitate a more fine-grained observation of LLMs' capabilities. We have also evaluated and analyzed mainstream and advanced Chinese LLMs on CDQA. Extensive experiments and valuable insights suggest that our proposed CDQA is challenging and worthy of more further stud
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#26631;&#20934;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#22686;&#21152;&#39069;&#22806;&#30340;&#35760;&#24518;&#24211;&#21644;&#27880;&#24847;&#21147;&#23618;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25552;&#39640;&#21464;&#21387;&#22120;&#29983;&#25104;&#35821;&#35328;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.19218</link><description>&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#22686;&#24378;&#30340;&#29983;&#25104;&#23545;&#25239;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Memory-Augmented Generative Adversarial Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19218
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#26631;&#20934;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#22686;&#21152;&#39069;&#22806;&#30340;&#35760;&#24518;&#24211;&#21644;&#27880;&#24847;&#21147;&#23618;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25552;&#39640;&#21464;&#21387;&#22120;&#29983;&#25104;&#35821;&#35328;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;&#21464;&#21387;&#22120;&#65289;&#30340;&#20250;&#35805;AI&#31995;&#32479;&#22312;&#23558;&#22806;&#37096;&#25968;&#25454;&#65288;&#22914;&#20107;&#23454;&#65289;&#19982;&#20854;&#29983;&#25104;&#30340;&#35821;&#35328;&#30456;&#20114;&#20132;&#32455;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#26222;&#36890;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#24182;&#26410;&#35774;&#35745;&#29992;&#20110;&#20934;&#30830;&#22238;&#31572;&#20107;&#23454;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#33021;&#36884;&#24452;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#22312;&#26631;&#20934;&#21464;&#21387;&#22120;&#26550;&#26500;&#19978;&#25193;&#23637;&#39069;&#22806;&#20449;&#24687;&#30340;&#35760;&#24518;&#24211;&#65288;&#22914;&#26469;&#33258;&#30693;&#35782;&#24211;&#30340;&#20107;&#23454;&#65289;&#21644;&#29992;&#20110;&#22788;&#29702;&#36825;&#19968;&#35760;&#24518;&#30340;&#39069;&#22806;&#27880;&#24847;&#21147;&#23618;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#22686;&#24378;&#35760;&#24518;&#28155;&#21152;&#21040;&#21551;&#21457;&#24335;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#12290;&#36825;&#31181;&#35774;&#32622;&#20801;&#35768;&#22312;&#21464;&#21387;&#22120;&#29983;&#25104;&#30340;&#35821;&#35328;&#19978;&#23454;&#26045;&#20219;&#24847;&#30340;&#24555;&#20048;&#26465;&#20214;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26426;&#21046;&#22914;&#20309;&#34987;&#29992;&#20110;&#22788;&#29702;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#20107;&#23454;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#23545;&#24212;&#29992;&#31243;&#24207;&#21487;&#33021;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19218v1 Announce Type: new  Abstract: Conversational AI systems that rely on Large Language Models, like Transformers, have difficulty interweaving external data (like facts) with the language they generate. Vanilla Transformer architectures are not designed for answering factual questions with high accuracy. This paper investigates a possible route for addressing this problem. We propose to extend the standard Transformer architecture with an additional memory bank holding extra information (such as facts drawn from a knowledge base), and an extra attention layer for addressing this memory. We add this augmented memory to a Generative Adversarial Network-inspired Transformer architecture. This setup allows for implementing arbitrary felicity conditions on the generated language of the Transformer. We first demonstrate how this machinery can be deployed for handling factual questions in goal-oriented dialogues. Secondly, we demonstrate that our approach can be useful for app
&lt;/p&gt;</description></item><item><title>PeLLE&#26159;&#22522;&#20110;RoBERTa&#26550;&#26500;&#30340;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#20854;&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#20102;Carolina&#35821;&#26009;&#24211;&#30340;&#24320;&#25918;&#25968;&#25454;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#36739;&#22823;&#27169;&#22411;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#20294;&#26377;&#20123;&#20219;&#21153;&#20250;&#22240;&#20026;&#20351;&#29992;&#36739;&#23567;&#20294;&#31934;&#36873;&#30340;&#25968;&#25454;&#22312;&#39044;&#35757;&#32451;&#20013;&#32780;&#26377;&#25152;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.19204</link><description>&lt;p&gt;
&#22522;&#20110;&#24320;&#25918;&#25968;&#25454;&#30340;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;PeLLE
&lt;/p&gt;
&lt;p&gt;
PeLLE: Encoder-based language models for Brazilian Portuguese based on open data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19204
&lt;/p&gt;
&lt;p&gt;
PeLLE&#26159;&#22522;&#20110;RoBERTa&#26550;&#26500;&#30340;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#20854;&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#20102;Carolina&#35821;&#26009;&#24211;&#30340;&#24320;&#25918;&#25968;&#25454;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#36739;&#22823;&#27169;&#22411;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#20294;&#26377;&#20123;&#20219;&#21153;&#20250;&#22240;&#20026;&#20351;&#29992;&#36739;&#23567;&#20294;&#31934;&#36873;&#30340;&#25968;&#25454;&#22312;&#39044;&#35757;&#32451;&#20013;&#32780;&#26377;&#25152;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PeLLE&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;RoBERTa&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#65292;&#35757;&#32451;&#25968;&#25454;&#26469;&#33258;Carolina&#35821;&#26009;&#24211;&#12290;&#20026;&#20102;&#21487;&#37325;&#22797;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#32454;&#33410;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;PeLLE&#27169;&#22411;&#19982;&#19968;&#32452;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;&#21644;PT-BR&#31934;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer LLM&#32534;&#30721;&#22120;&#65292;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#27604;&#20102;&#22823;&#22411;&#27169;&#22411;&#19982;&#36739;&#23567;&#20294;&#32463;&#36807;&#31579;&#36873;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#19968;&#20123;&#20219;&#21153;&#20351;&#29992;&#26356;&#22823;&#30340;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#65292;&#20294;&#22312;&#39044;&#35757;&#32451;&#20013;&#65292;&#19968;&#20123;&#20219;&#21153;&#21463;&#30410;&#20110;&#36739;&#23567;&#20294;&#31934;&#36873;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19204v1 Announce Type: new  Abstract: In this paper we present PeLLE, a family of large language models based on the RoBERTa architecture, for Brazilian Portuguese, trained on curated, open data from the Carolina corpus. Aiming at reproducible results, we describe details of the pretraining of the models. We also evaluate PeLLE models against a set of existing multilingual and PT-BR refined pretrained Transformer-based LLM encoders, contrasting performance of large versus smaller-but-curated pretrained models in several downstream tasks. We conclude that several tasks perform better with larger models, but some tasks benefit from smaller-but-curated data in its pretraining.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21830;&#19994;LLMs&#30340;&#25552;&#31034;&#21453;&#31363;&#21462;&#25915;&#20987;&#26694;&#26550;PRSA&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#20851;&#38190;&#29305;&#24449;&#23454;&#29616;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.19200</link><description>&lt;p&gt;
PRSA&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#21453;&#30423;&#31363;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
PRSA: Prompt Reverse Stealing Attacks against Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21830;&#19994;LLMs&#30340;&#25552;&#31034;&#21453;&#31363;&#21462;&#25915;&#20987;&#26694;&#26550;PRSA&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#20851;&#38190;&#29305;&#24449;&#23454;&#29616;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#20316;&#20026;&#37325;&#35201;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#37325;&#35201;&#24615;&#12290;&#38543;&#30528;&#22522;&#20110;&#25552;&#31034;&#30340;&#26381;&#21153;&#30340;&#23835;&#36215;&#65292;&#22914;&#25552;&#31034;&#24066;&#22330;&#21644;LLM&#24212;&#29992;&#31243;&#24207;&#65292;&#25552;&#20379;&#32773;&#32463;&#24120;&#36890;&#36807;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#23637;&#31034;&#25552;&#31034;&#30340;&#33021;&#21147;&#65292;&#20197;&#21560;&#24341;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#23433;&#20840;&#38382;&#39064;&#65306;&#26292;&#38706;&#36755;&#20837;-&#36755;&#20986;&#23545;&#26159;&#21542;&#20250;&#23545;&#28508;&#22312;&#25552;&#31034;&#27844;&#28431;&#26500;&#25104;&#39118;&#38505;&#65292;&#20405;&#29359;&#24320;&#21457;&#32773;&#30340;&#30693;&#35782;&#20135;&#26435;&#65311;&#23601;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#35752;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#25506;&#35752;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21830;&#19994;LLMs&#30340;&#25552;&#31034;&#21453;&#31363;&#21462;&#25915;&#20987;&#26694;&#26550;&#65292;&#21363;PRSA&#12290;PRSA&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#25105;&#20204;&#27169;&#20223;&#24182;g
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19200v1 Announce Type: cross  Abstract: Prompt, recognized as crucial intellectual property, enables large language models (LLMs) to perform specific tasks without the need of fine-tuning, underscoring their escalating importance. With the rise of prompt-based services, such as prompt marketplaces and LLM applications, providers often display prompts' capabilities through input-output examples to attract users. However, this paradigm raises a pivotal security concern: does the exposure of input-output pairs pose the risk of potential prompt leakage, infringing on the intellectual property rights of the developers? To our knowledge, this problem still has not been comprehensively explored yet. To remedy this gap, in this paper, we perform the first in depth exploration and propose a novel attack framework for reverse-stealing prompts against commercial LLMs, namely PRSA. The main idea of PRSA is that by analyzing the critical features of the input-output pairs, we mimic and g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#36890;&#36807;&#25193;&#23637;Transformer&#27169;&#22411;&#30340;&#24207;&#21015;&#38271;&#24230;&#26469;&#26356;&#22909;&#29702;&#35299;&#27861;&#24459;&#35821;&#26009;&#24211;&#20013;&#30340;&#38271;&#25991;&#26723;&#65292;&#24182;&#22312;&#32599;&#39532;&#23612;&#20122;&#30340;4&#20010;LJP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.19170</link><description>&lt;p&gt;
&#36890;&#36807;&#38271;&#25991;&#26412;&#32534;&#30721;&#22120;&#25552;&#21319;&#32599;&#39532;&#23612;&#20122;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Legal Judgement Prediction in Romanian with Long Text Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#36890;&#36807;&#25193;&#23637;Transformer&#27169;&#22411;&#30340;&#24207;&#21015;&#38271;&#24230;&#26469;&#26356;&#22909;&#29702;&#35299;&#27861;&#24459;&#35821;&#26009;&#24211;&#20013;&#30340;&#38271;&#25991;&#26723;&#65292;&#24182;&#22312;&#32599;&#39532;&#23612;&#20122;&#30340;4&#20010;LJP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#26032;&#25104;&#26524;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#27861;&#24459;NLP&#39046;&#22495;&#20063;&#38543;&#20043;&#21457;&#23637;&#36805;&#29467;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#27169;&#22411;&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#12290;&#30001;&#20110;&#20854;&#19987;&#19994;&#35789;&#27719;&#12289;&#38271;&#25991;&#26723;&#31561;&#29305;&#28857;&#65292;&#27861;&#24459;NLP&#36890;&#24120;&#38656;&#35201;&#29305;&#23450;&#27169;&#22411;&#21644;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#21644;&#36890;&#29992;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#27861;&#24459;&#26696;&#20363;&#30340;&#26368;&#32456;&#35009;&#20915;&#30340;&#26041;&#27861;&#65292;&#21363;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#22914;&#20309;&#25193;&#23637;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#24207;&#21015;&#38271;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27861;&#24459;&#35821;&#26009;&#24211;&#20013;&#30340;&#38271;&#25991;&#26723;&#12290;&#22312;&#26469;&#33258;&#20004;&#20010;&#26469;&#28304;&#12289;&#35268;&#27169;&#21644;&#25991;&#26723;&#38271;&#24230;&#26174;&#33879;&#19981;&#21516;&#26102;&#30340;4&#20010;&#32599;&#39532;&#23612;&#20122;LJP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#19987;&#38376;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19170v1 Announce Type: cross  Abstract: In recent years,the entire field of Natural Language Processing (NLP) has enjoyed amazing novel results achieving almost human-like performance on a variety of tasks. Legal NLP domain has also been part of this process, as it has seen an impressive growth. However, general-purpose models are not readily applicable for legal domain. Due to the nature of the domain (e.g. specialized vocabulary, long documents) specific models and methods are often needed for Legal NLP. In this work we investigate both specialized and general models for predicting the final ruling of a legal case, task known as Legal Judgment Prediction (LJP). We particularly focus on methods to extend to sequence length of Transformer-based models to better understand the long documents present in legal corpora. Extensive experiments on 4 LJP datasets in Romanian, originating from 2 sources with significantly different sizes and document lengths, show that specialized mo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#31034;&#65292;&#22312;&#39134;&#34892;&#20013;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#31181;&#26410;&#30693;&#35821;&#35328;&#65292;&#25552;&#20986;DiPMT ++&#26694;&#26550;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20351;LLMs&#36866;&#24212;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#65292;&#24182;&#23454;&#29616;&#20102;&#22766;&#35821;&#21644;&#27721;&#35821;&#20043;&#38388;&#30340;&#32763;&#35793;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#24110;&#21161;&#20154;&#31867;&#32763;&#35793;&#23436;&#20840;&#26410;&#30693;&#35821;&#35328;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.19167</link><description>&lt;p&gt;
&#22312;&#38656;&#35201;&#26102;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#31181;&#26410;&#30693;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Teaching Large Language Models an Unseen Language on the Fly
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19167
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#65292;&#22312;&#39134;&#34892;&#20013;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#31181;&#26410;&#30693;&#35821;&#35328;&#65292;&#25552;&#20986;DiPMT ++&#26694;&#26550;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20351;LLMs&#36866;&#24212;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#65292;&#24182;&#23454;&#29616;&#20102;&#22766;&#35821;&#21644;&#27721;&#35821;&#20043;&#38388;&#30340;&#32763;&#35793;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#24110;&#21161;&#20154;&#31867;&#32763;&#35793;&#23436;&#20840;&#26410;&#30693;&#35821;&#35328;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#35768;&#22810;&#20302;&#36164;&#28304;&#35821;&#35328;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#26041;&#38754;&#65292;&#22312;&#36825;&#20123;&#35821;&#35328;&#20013;&#65292;&#26377;&#25928;&#21442;&#25968;&#26356;&#26032;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#26497;&#23569;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLM&#26159;&#21542;&#21487;&#20197;&#20165;&#36890;&#36807;&#25552;&#31034;&#22312;&#39134;&#34892;&#20013;&#23398;&#20064;&#19968;&#31181;&#26032;&#35821;&#35328;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#22766;&#35821;&#25910;&#38598;&#20102;&#19968;&#20010;&#30740;&#31350;&#22871;&#20214;&#65292;&#36825;&#26159;&#24403;&#21069;&#27809;&#26377;LLMs&#25903;&#25345;&#30340;&#19968;&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DiPMT++&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23558;LLMs&#36866;&#24212;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#12290;&#20351;&#29992;&#19968;&#26412;&#35789;&#20856;&#21644;&#20165;&#26377;5K&#23545;&#24179;&#34892;&#21477;&#23376;&#65292;DiPMT++&#23558;GPT-4&#30340;&#24615;&#33021;&#20174;0&#25552;&#21319;&#21040;16 BLEU&#65292;&#29992;&#20110;&#27721;&#35821;&#21040;&#22766;&#35821;&#30340;&#32763;&#35793;&#65292;&#24182;&#23454;&#29616;&#20102;&#22766;&#35821;&#21040;&#27721;&#35821;&#30340;32 BLEU&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#26694;&#26550;&#22312;&#24110;&#21161;&#20154;&#31867;&#32763;&#35793;&#23436;&#20840;&#26410;&#30693;&#35821;&#35328;&#26041;&#38754;&#30340;&#23454;&#38469;&#29992;&#36884;&#65292;&#36825;&#26377;&#21161;&#20110;&#32500;&#25252;&#35821;&#35328;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19167v1 Announce Type: new  Abstract: Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones where there is minimal training data available for effective parameter updating. We thus investigate whether LLMs can learn a new language on the fly solely through prompting. To study this question, we collect a research suite for Zhuang, a language supported by no LLMs currently. We introduce \textsc{DiPMT++}, a framework for adapting LLMs to unseen languages by in-context learning. Using a dictionary and only 5K parallel sentences, \textsc{DiPMT++} significantly enhances the performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32 BLEU for Zhuang-to-Chinese translation. Furthermore, we demonstrate the practical utility of this framework in aiding humans to translate completely unseen languages, which could contribute to the preservation of linguistic diversity.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#25668;&#20687;&#22836;&#30340;&#30524;&#21160;&#25968;&#25454;&#20316;&#20026;&#35780;&#20272;&#37325;&#35201;&#24615;&#35780;&#20998;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#23545;WebQAmGaze&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#20957;&#35270;&#25968;&#25454;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35821;&#35328;&#23398;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.19133</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#32593;&#32476;&#25668;&#20687;&#22836;&#30340;&#20957;&#35270;&#25968;&#25454;&#20316;&#20026;&#20154;&#31867;&#21407;&#22240;&#26631;&#27880;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19133
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#25668;&#20687;&#22836;&#30340;&#30524;&#21160;&#25968;&#25454;&#20316;&#20026;&#35780;&#20272;&#37325;&#35201;&#24615;&#35780;&#20998;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#23545;WebQAmGaze&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#20957;&#35270;&#25968;&#25454;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35821;&#35328;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#26102;&#65292;&#20197;&#25163;&#21160;&#27880;&#37322;&#30340;&#36755;&#20837;&#36328;&#24230;&#24418;&#24335;&#21576;&#29616;&#30340;&#21407;&#22240;&#36890;&#24120;&#20316;&#20026;&#22522;&#20934;&#30495;&#30456;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32791;&#26102;&#19988;&#24448;&#24448;&#21463;&#27880;&#37322;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#35780;&#20272;&#37325;&#35201;&#24615;&#35780;&#20998;&#26102;&#65292;&#20154;&#31867;&#20957;&#35270;&#65292;&#21363;&#22522;&#20110;&#32593;&#32476;&#25668;&#20687;&#22836;&#30340;&#30524;&#21160;&#36319;&#36394;&#35760;&#24405;&#65292;&#26159;&#21542;&#26500;&#25104;&#19968;&#20010;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20957;&#35270;&#25968;&#25454;&#25552;&#20379;&#30340;&#38468;&#21152;&#20449;&#24687;&#65292;&#27604;&#22914;&#24635;&#38405;&#35835;&#26102;&#38388;&#12289;&#20957;&#35270;&#29109;&#20197;&#21450;&#35299;&#30721;&#20934;&#30830;&#24615;&#65292;&#19982;&#20154;&#31867;&#21407;&#22240;&#26631;&#27880;&#30456;&#20851;&#12290;&#25105;&#20204;&#23558;WebQAmGaze&#65292;&#19968;&#20010;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;QA&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#19982;4&#31181;&#19981;&#21516;&#30340;&#22810;&#35821;&#35328;Transformer&#35821;&#35328;&#27169;&#22411;&#65288;mBERT&#65292;distil-mBERT&#65292;XLMR&#21644;XLMR-L&#65289;&#20197;&#21450;3&#31181;&#35821;&#35328;&#65288;&#33521;&#35821;&#65292;&#35199;&#29677;&#29273;&#35821;&#21644;&#24503;&#35821;&#65289;&#30340;&#27880;&#24847;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#37325;&#35201;&#24615;&#20998;&#25968;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#21644;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20957;&#35270;&#25968;&#25454;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35821;&#35328;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19133v1 Announce Type: new  Abstract: Rationales in the form of manually annotated input spans usually serve as ground truth when evaluating explainability methods in NLP. They are, however, time-consuming and often biased by the annotation process. In this paper, we debate whether human gaze, in the form of webcam-based eye-tracking recordings, poses a valid alternative when evaluating importance scores. We evaluate the additional information provided by gaze data, such as total reading times, gaze entropy, and decoding accuracy with respect to human rationale annotations. We compare WebQAmGaze, a multilingual dataset for information-seeking QA, with attention and explainability-based importance scores for 4 different multilingual Transformer-based language models (mBERT, distil-mBERT, XLMR, and XLMR-L) and 3 languages (English, Spanish, and German). Our pipeline can easily be applied to other tasks and languages. Our findings suggest that gaze data offers valuable linguist
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VIXEN&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#29992;&#25991;&#26412;&#31616;&#27905;&#22320;&#24635;&#32467;&#19968;&#23545;&#22270;&#20687;&#20043;&#38388;&#30340;&#35270;&#35273;&#24046;&#24322;&#65292;&#20026;&#31361;&#20986;&#20869;&#23481;&#25805;&#20316;&#25552;&#20379;&#28508;&#22312;&#30340;&#32531;&#35299;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.19119</link><description>&lt;p&gt;
VIXEN: &#22270;&#20687;&#24046;&#24322;&#23383;&#24149;&#30340;&#35270;&#35273;&#25991;&#26412;&#27604;&#36739;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
VIXEN: Visual Text Comparison Network for Image Difference Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19119
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VIXEN&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#29992;&#25991;&#26412;&#31616;&#27905;&#22320;&#24635;&#32467;&#19968;&#23545;&#22270;&#20687;&#20043;&#38388;&#30340;&#35270;&#35273;&#24046;&#24322;&#65292;&#20026;&#31361;&#20986;&#20869;&#23481;&#25805;&#20316;&#25552;&#20379;&#28508;&#22312;&#30340;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VIXEN - &#19968;&#31181;&#33021;&#22815;&#29992;&#25991;&#26412;&#31616;&#27905;&#22320;&#24635;&#32467;&#19968;&#23545;&#22270;&#20687;&#20043;&#38388;&#30340;&#35270;&#35273;&#24046;&#24322;&#65292;&#20197;&#31361;&#20986;&#20854;&#20013;&#30340;&#20219;&#20309;&#20869;&#23481;&#25805;&#20316;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#20197;&#25104;&#23545;&#30340;&#26041;&#24335;&#32447;&#24615;&#26144;&#23556;&#22270;&#20687;&#29305;&#24449;&#65292;&#26500;&#24314;&#20986;&#19968;&#20010;&#36719;&#25552;&#31034;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#26368;&#36817;&#30340;InstructPix2Pix&#25968;&#25454;&#38598;&#20013;&#21033;&#29992;&#25552;&#31034;&#21040;&#25552;&#31034;&#32534;&#36753;&#26694;&#26550;&#29983;&#25104;&#30340;&#21512;&#25104;&#25805;&#20316;&#22270;&#20687;&#26469;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22270;&#20687;&#24046;&#24322;&#23383;&#24149;&#65288;IDC&#65289;&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#25968;&#25454;&#37327;&#23569;&#65292;&#25805;&#20316;&#31867;&#22411;&#22810;&#26679;&#24615;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;GPT-3&#29983;&#25104;&#30340;&#21464;&#21270;&#25688;&#35201;&#26469;&#25193;&#20805;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;VIXEN&#33021;&#20026;&#19981;&#21516;&#22270;&#20687;&#20869;&#23481;&#21644;&#32534;&#36753;&#31867;&#22411;&#29983;&#25104;&#26368;&#26032;&#30340;&#26131;&#25026;&#30340;&#24046;&#24322;&#23383;&#24149;&#65292;&#20026;&#38450;&#27490;&#36890;&#36807;&#25805;&#32437;&#22270;&#20687;&#20869;&#23481;&#20256;&#25773;&#30340;&#20449;&#24687;&#38169;&#35823;&#25552;&#20379;&#28508;&#22312;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;http://github.com/alexblck/vixen&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19119v1 Announce Type: cross  Abstract: We present VIXEN - a technique that succinctly summarizes in text the visual differences between a pair of images in order to highlight any content manipulation present. Our proposed network linearly maps image features in a pairwise manner, constructing a soft prompt for a pretrained large language model. We address the challenge of low volume of training data and lack of manipulation variety in existing image difference captioning (IDC) datasets by training on synthetically manipulated images from the recent InstructPix2Pix dataset generated via prompt-to-prompt editing framework. We augment this dataset with change summaries produced via GPT-3. We show that VIXEN produces state-of-the-art, comprehensible difference captions for diverse image contents and edit types, offering a potential mitigation against misinformation disseminated via manipulated image content. Code and data are available at http://github.com/alexblck/vixen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65288;IECI&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#26631;&#27880;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#23637;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.19116</link><description>&lt;p&gt;
&#22914;&#20309;&#29702;&#35299;&#8220;&#25903;&#25345;&#8221;&#65311;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#29992;&#20110;&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
How to Understand "Support"? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19116
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65288;IECI&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#26631;&#27880;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#23637;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;&#65288;WPG&#65289;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#25512;&#26029;&#32454;&#31890;&#24230;&#30701;&#35821;-&#21306;&#22495;&#21305;&#37197;&#65292;&#20165;&#21033;&#29992;&#31895;&#31890;&#24230;&#30340;&#21477;&#23376;-&#22270;&#20687;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20851;&#20110;WPG&#30340;&#30740;&#31350;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#38544;&#24335;&#30701;&#35821;-&#21306;&#22495;&#21305;&#37197;&#20851;&#31995;&#65292;&#36825;&#23545;&#20110;&#35780;&#20272;&#27169;&#22411;&#29702;&#35299;&#28145;&#23618;&#22810;&#27169;&#24577;&#35821;&#20041;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#65288;IECI&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#23545;&#24314;&#27169;&#38544;&#24335;&#20851;&#31995;&#21644;&#31361;&#20986;&#26174;&#24615;&#20851;&#31995;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#21033;&#29992;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#25216;&#26415;&#26469;&#24212;&#23545;&#19978;&#36848;&#20004;&#20010;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#36824;&#26631;&#27880;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#38544;&#24335;&#22686;&#24378;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;IECI&#65292;&#35814;&#32454;&#35780;&#20272;&#26174;&#31034;IECI&#30456;&#27604;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#26377;&#24456;&#22823;&#20248;&#21183;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19116v1 Announce Type: cross  Abstract: Weakly-supervised Phrase Grounding (WPG) is an emerging task of inferring the fine-grained phrase-region matching, while merely leveraging the coarse-grained sentence-image pairs for training. However, existing studies on WPG largely ignore the implicit phrase-region matching relations, which are crucial for evaluating the capability of models in understanding the deep multimodal semantics. To this end, this paper proposes an Implicit-Enhanced Causal Inference (IECI) approach to address the challenges of modeling the implicit relations and highlighting them beyond the explicit. Specifically, this approach leverages both the intervention and counterfactual techniques to tackle the above two challenges respectively. Furthermore, a high-quality implicit-enhanced dataset is annotated to evaluate IECI and detailed evaluations show the great advantages of IECI over the state-of-the-art baselines. Particularly, we observe an interesting findi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;FAITH&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.19103</link><description>&lt;p&gt;
&#38663;&#25788;&#22522;&#30784;&#30340;&#32454;&#35821;&#65306;&#20998;&#26512;&#21644;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;FAITH&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#21463;&#21040;&#24187;&#35273;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#37325;&#35201;&#31867;&#22411;&#26159;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#65292;&#25105;&#20204;&#23450;&#20041;&#20026;&#24403;LLMs&#38754;&#23545;&#34394;&#20551;&#21069;&#25552;&#38382;&#39064;&#26102;&#29983;&#25104;&#24187;&#35273;&#25991;&#26412;&#30340;&#29616;&#35937;&#12290;&#26412;&#25991;&#23545;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#38416;&#26126;&#20102;&#20854;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#65306;&#19968;&#23567;&#37096;&#20998;&#27880;&#24847;&#21147;&#22836;(&#25105;&#20204;&#23558;&#20854;&#25351;&#23450;&#20026;&#34394;&#20551;&#21069;&#25552;&#22836;)&#25200;&#20081;&#20102;&#30693;&#35782;&#25552;&#21462;&#36807;&#31243;&#65292;&#23548;&#33268;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;FAITH&#8221;(&#34394;&#20551;&#21069;&#25552;&#27880;&#24847;&#21147;&#22836;&#32422;&#26463;&#20197;&#20943;&#36731;&#24187;&#35273;)&#36825;&#19968;&#26032;&#39062;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#12290;&#23427;&#22312;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#20013;&#32422;&#26463;&#34394;&#20551;&#21069;&#25552;&#27880;&#24847;&#21147;&#22836;&#12290;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19103v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose \textbf{FAITH} (\textbf{F}alse premise \textbf{A}ttention head constra\textbf{I}ining for mi\textbf{T}igating \textbf{H}allucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#31354;&#38388;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#35299;&#30721;&#22120;&#20197;&#21450;&#33258;&#25105;&#35843;&#33410;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;TEncDM&#30340;&#25991;&#26412;&#32534;&#30721;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#20004;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2402.19097</link><description>&lt;p&gt;
TEncDM: &#22312;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#31354;&#38388;&#20013;&#29702;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
TEncDM: Understanding the Properties of Diffusion Model in the Space of Language Model Encodings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19097
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#31354;&#38388;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#35299;&#30721;&#22120;&#20197;&#21450;&#33258;&#25105;&#35843;&#33410;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;TEncDM&#30340;&#25991;&#26412;&#32534;&#30721;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#20004;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#35768;&#22810;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#20854;&#24212;&#29992;&#20110;&#25991;&#26412;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#21162;&#21147;&#65292;&#20294;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#23545;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#30340;&#20851;&#38190;&#32452;&#20214;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Text Encoding Diffusion Model (TEncDM)&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#31354;&#38388;&#20013;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#36890;&#24120;&#20351;&#29992;&#30340;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#35299;&#30721;&#22120;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#36827;&#34892;&#25991;&#26412;&#37325;&#26500;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#33258;&#25105;&#35843;&#33410;&#65292;&#24182;&#21457;&#29616;&#36825;&#20250;&#22686;&#21152;&#27169;&#22411;&#36755;&#20986;&#30340;&#25968;&#37327;&#32423;&#65292;&#20174;&#32780;&#20943;&#23569;&#25512;&#29702;&#38454;&#27573;&#30340;&#21435;&#22122;&#27493;&#39588;&#25968;&#37327;&#12290;&#22312;&#20004;&#20010;&#19979;&#28216;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;QQP&#21644;XSum&#19978;&#23545;TEncDM&#30340;&#35780;&#20272;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19097v1 Announce Type: new  Abstract: Drawing inspiration from the success of diffusion models in various domains, numerous research papers proposed methods for adapting them to text data. Despite these efforts, none of them has managed to achieve the quality of the large language models. In this paper, we conduct a comprehensive analysis of key components of the text diffusion models and introduce a novel approach named Text Encoding Diffusion Model (TEncDM). Instead of the commonly used token embedding space, we train our model in the space of the language model encodings. Additionally, we propose to use a Transformer-based decoder that utilizes contextual information for text reconstruction. We also analyse self-conditioning and find that it increases the magnitude of the model outputs, allowing the reduction of the number of denoising steps at the inference stage. Evaluation of TEncDM on two downstream text generation tasks, QQP and XSum, demonstrates its superiority ove
&lt;/p&gt;</description></item><item><title>&#35821;&#20041;&#21464;&#21270;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#27492;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;</title><link>https://arxiv.org/abs/2402.19088</link><description>&lt;p&gt;
&#23545;&#35821;&#20041;&#21464;&#21270;&#29305;&#24449;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey in Characterization of Semantic Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19088
&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#21464;&#21270;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#27492;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#35821;&#35328;&#19981;&#26029;&#21457;&#23637;&#65292;&#20197;&#21560;&#32435;&#20154;&#31867;&#31038;&#20250;&#30340;&#25991;&#21270;&#21464;&#21270;&#12290;&#36825;&#31181;&#28436;&#21464;&#36890;&#36807;&#26032;&#35789;&#35821;&#65288;&#26032;&#21333;&#35789;&#65289;&#25110;&#21333;&#35789;&#30340;&#35821;&#20041;&#21464;&#21270;&#65288;&#36171;&#20104;&#24050;&#26377;&#21333;&#35789;&#26032;&#30340;&#21547;&#20041;&#65289;&#26469;&#20307;&#29616;&#12290;&#29702;&#35299;&#21333;&#35789;&#30340;&#21547;&#20041;&#23545;&#35299;&#37322;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#65288;&#22320;&#26041;&#29992;&#35821;&#25110;&#20442;&#35821;&#65289;&#12289;&#39046;&#22495;&#65288;&#20363;&#22914;&#25216;&#26415;&#26415;&#35821;&#65289;&#25110;&#26102;&#20195;&#30340;&#25991;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#65292;&#36825;&#20123;&#21333;&#35789;&#19982;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30456;&#20851;&#65292;&#20363;&#22914;&#32763;&#35793;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#38382;&#31572;&#31561;&#12290;&#35821;&#20041;&#21464;&#21270;&#21487;&#33021;&#20250;&#24433;&#21709;&#36825;&#20123;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#21644;&#24418;&#24335;&#21270;&#34920;&#24449;&#36825;&#20123;&#21464;&#21270;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#30740;&#31350;&#36825;&#31181;&#24433;&#21709;&#26159;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#36817;&#26399;&#24341;&#36215;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20960;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#26816;&#27979;&#35821;&#20041;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#21162;&#21147;&#26469;&#23545;&#20854;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19088v1 Announce Type: cross  Abstract: Live languages continuously evolve to integrate the cultural change of human societies. This evolution manifests through neologisms (new words) or \textbf{semantic changes} of words (new meaning to existing words). Understanding the meaning of words is vital for interpreting texts coming from different cultures (regionalism or slang), domains (e.g., technical terms), or periods. In computer science, these words are relevant to computational linguistics algorithms such as translation, information retrieval, question answering, etc. Semantic changes can potentially impact the quality of the outcomes of these algorithms. Therefore, it is important to understand and characterize these changes formally. The study of this impact is a recent problem that has attracted the attention of the computational linguistics community. Several approaches propose methods to detect semantic changes with good precision, but more effort is needed to charact
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#20026;&#19981;&#21516;&#30446;&#26631;&#25351;&#23450;&#20559;&#22909;&#20998;&#25968;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#38656;&#27714;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.19085</link><description>&lt;p&gt;
&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65306;&#26397;&#30528;&#21487;&#25511;&#22810;&#30446;&#26631;&#23545;&#40784;&#26041;&#21521;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19085
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#20026;&#19981;&#21516;&#30446;&#26631;&#25351;&#23450;&#20559;&#22909;&#20998;&#25968;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#38656;&#27714;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23545;&#40784;&#24037;&#20316;&#26088;&#22312;&#36861;&#27714;&#27169;&#22411;&#21709;&#24212;&#19982;&#20154;&#31867;&#20559;&#22909;&#21644;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#20026;&#19981;&#21516;&#30446;&#26631;&#25351;&#23450;&#20559;&#22909;&#20998;&#25968;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#38656;&#27714;&#30340;&#21709;&#24212;&#12290;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#23545;&#40784;&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#31526;&#21512;&#21508;&#31181;&#20559;&#22909;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19085v1 Announce Type: new  Abstract: Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the "alignment tax" -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26367;&#25442;&#23454;&#20307;&#25552;&#21450;&#26469;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#25581;&#31034;&#20102;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#30340;&#25463;&#24452;&#29305;&#24615;&#32570;&#38519;</title><link>https://arxiv.org/abs/2402.19076</link><description>&lt;p&gt;
&#29992;&#35821;&#20041;&#39537;&#21160;&#30340;&#23545;&#25239;&#29983;&#25104;&#27169;&#22411;&#25351;&#20986;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#30340;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Pointing out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26367;&#25442;&#23454;&#20307;&#25552;&#21450;&#26469;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#25581;&#31034;&#20102;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#30340;&#25463;&#24452;&#29305;&#24615;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35843;&#26597;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#20381;&#36182;&#20110;&#25463;&#24452;&#29305;&#24615;&#65292;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20351;&#27169;&#22411;&#22312;&#27867;&#21270;&#21040;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#26679;&#26412;&#26102;&#19981;&#21487;&#38752;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20960;&#31181;&#22522;&#20110;&#35821;&#20041;&#21160;&#26426;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#26367;&#25442;&#23454;&#20307;&#25552;&#21450;&#26469;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20197;&#27492;&#26469;&#25506;&#31350;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19076v1 Announce Type: new  Abstract: In recent years, large language models have achieved state-of-the-art performance across various NLP tasks. However, investigations have shown that these models tend to rely on shortcut features, leading to inaccurate predictions and causing the models to be unreliable at generalization to out-of-distribution (OOD) samples. For instance, in the context of relation extraction (RE), we would expect a model to identify the same relation independently of the entities involved in it. For example, consider the sentence "Leonardo da Vinci painted the Mona Lisa" expressing the created(Leonardo_da_Vinci, Mona_Lisa) relation. If we substiute "Leonardo da Vinci" with "Barack Obama", then the sentence still expresses the created relation. A robust model is supposed to detect the same relation in both cases. In this work, we describe several semantically-motivated strategies to generate adversarial examples by replacing entity mentions and investigat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36873;&#25321;&#24615;&#24635;&#32467;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;&#20250;&#35805;&#20013;&#30340;&#21151;&#25928;&#65292;&#24182;&#24341;&#20837;&#20102;MentalCLOUDS&#25968;&#25454;&#38598;&#20316;&#20026;&#22522;&#20934;&#65292;&#20197;&#25506;&#32034;&#20854;&#22312;&#36741;&#23548;&#32452;&#20214;&#25351;&#23548;&#30340;&#24635;&#32467;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19052</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24635;&#32467;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;&#20250;&#35805;&#20013;&#30340;&#21151;&#25928;&#65306;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: A Benchmark Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36873;&#25321;&#24615;&#24635;&#32467;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;&#20250;&#35805;&#20013;&#30340;&#21151;&#25928;&#65292;&#24182;&#24341;&#20837;&#20102;MentalCLOUDS&#25968;&#25454;&#38598;&#20316;&#20026;&#22522;&#20934;&#65292;&#20197;&#25506;&#32034;&#20854;&#22312;&#36741;&#23548;&#32452;&#20214;&#25351;&#23548;&#30340;&#24635;&#32467;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#38754;&#24635;&#32467;&#20250;&#35805;&#26377;&#21161;&#20110;&#22312;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;&#20013;&#26377;&#25928;&#22320;&#20445;&#25345;&#36830;&#32493;&#24615;&#65292;&#20419;&#36827;&#30693;&#24773;&#30103;&#27861;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#24635;&#32467;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#65292;&#20998;&#25955;&#19987;&#23478;&#27880;&#24847;&#21147;&#65292;&#20559;&#31163;&#26680;&#24515;&#21672;&#35810;&#27969;&#31243;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36890;&#36807;&#22522;&#20110;&#26041;&#38754;&#30340;&#24635;&#32467;&#26377;&#36873;&#25321;&#24615;&#22320;&#24635;&#32467;&#21508;&#31181;&#30103;&#27861;&#20250;&#35805;&#32452;&#20214;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#26088;&#22312;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MentalCLOUDS&#65292;&#19968;&#20010;&#30001;&#23548;&#24072;&#32452;&#20214;&#25351;&#23548;&#30340;&#24635;&#32467;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;191&#20010;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#19981;&#21516;&#36741;&#23548;&#32452;&#20214;&#65288;&#20063;&#31216;&#20026;&#36741;&#23548;&#26041;&#38754;&#65289;&#30340;&#36741;&#23548;&#20250;&#35805;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;LLM&#22312;&#35299;&#20915;&#36741;&#23548;&#20013;&#22522;&#20110;&#32452;&#20214;&#30340;&#24635;&#32467;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#21033;&#29992;&#26631;&#20934;&#24635;&#32467;&#24230;&#37327;&#23450;&#37327;&#35780;&#20272;&#29983;&#25104;&#30340;&#24635;&#32467;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19052v1 Announce Type: new  Abstract: Comprehensive summaries of sessions enable an effective continuity in mental health counseling, facilitating informed therapy planning. Yet, manual summarization presents a significant challenge, diverting experts' attention from the core counseling process. This study evaluates the effectiveness of state-of-the-art Large Language Models (LLMs) in selectively summarizing various components of therapy sessions through aspect-based summarization, aiming to benchmark their performance. We introduce MentalCLOUDS, a counseling-component guided summarization dataset consisting of 191 counseling sessions with summaries focused on three distinct counseling components (aka counseling aspects). Additionally, we assess the capabilities of 11 state-of-the-art LLMs in addressing the task of component-guided summarization in counseling. The generated summaries are evaluated quantitatively using standard summarization metrics and verified qualitatively
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Popularity-Aligned Language Models (PopALM)&#26469;&#21306;&#20998;&#21463;&#22823;&#20247;&#21916;&#27426;&#30340;&#22238;&#22797;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#35838;&#31243;&#23398;&#20064;&#26469;&#25552;&#39640;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#28909;&#38376;&#20107;&#20214;&#22238;&#24212;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.18950</link><description>&lt;p&gt;
PopALM: &#38754;&#21521;&#31038;&#20132;&#23186;&#20307;&#28909;&#38376;&#20107;&#20214;&#22238;&#24212;&#39044;&#27979;&#30340;&#21463;&#27426;&#36814;&#24230;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PopALM: Popularity-Aligned Language Models for Social Media Trendy Response Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18950
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Popularity-Aligned Language Models (PopALM)&#26469;&#21306;&#20998;&#21463;&#22823;&#20247;&#21916;&#27426;&#30340;&#22238;&#22797;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#35838;&#31243;&#23398;&#20064;&#26469;&#25552;&#39640;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#28909;&#38376;&#20107;&#20214;&#22238;&#24212;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18950v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25277;&#35937;&#65306;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27599;&#22825;&#37117;&#22312;&#23637;&#31034;&#25968;&#30334;&#19975;&#20107;&#20214;&#12290;&#20026;&#20102;&#21021;&#27493;&#39044;&#27979;&#23545;&#36825;&#20123;&#20107;&#20214;&#30340;&#20027;&#27969;&#20844;&#20247;&#21453;&#24212;&#65292;&#25105;&#20204;&#30740;&#31350;&#26102;&#39654;&#30340;&#21709;&#24212;&#39044;&#27979;&#65292;&#20197;&#33258;&#21160;&#29983;&#25104;&#23545;&#31038;&#20132;&#23186;&#20307;&#20107;&#20214;&#30340;&#28909;&#38376;&#29992;&#25143;&#22238;&#22797;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#29983;&#25104;&#21709;&#24212;&#32780;&#19981;&#32771;&#34385;&#21463;&#27426;&#36814;&#31243;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21463;&#27426;&#36814;&#24230;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#65288;PopALM&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21306;&#20998;&#21463;&#22823;&#20247;&#21916;&#27426;&#30340;&#22238;&#22797;&#12290;&#37492;&#21035;&#29992;&#25143;&#8220;&#21916;&#27426;&#8221;&#30340;&#22024;&#26434;&#26631;&#31614;&#65292;&#25105;&#20204;&#23450;&#21046;&#20102;&#35838;&#31243;&#23398;&#20064;&#22312;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#20013;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#25429;&#25417;&#22522;&#20110;&#26131;&#21040;&#38590;&#30340;&#35757;&#32451;&#30340;&#20851;&#38190;&#26679;&#26412;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24494;&#21338;&#25968;&#25454;&#38598;&#29992;&#20110;&#26102;&#39654;&#30340;&#21709;&#24212;&#39044;&#27979;&#65292;&#20854;&#32467;&#26524;&#34920;&#26126;PopALM&#21487;&#20197;&#24110;&#21161;&#25552;&#21319;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18950v1 Announce Type: new  Abstract: Social media platforms are daily exhibiting millions of events. To preliminarily predict the mainstream public reaction to these events, we study trendy response prediction to automatically generate top-liked user replies to social media events. While previous works focus on generating responses without factoring in popularity, we propose Popularity-Aligned Language Models (PopALM) to distinguish responses liked by a larger audience through reinforcement learning. Recognizing the noisy labels from user "likes", we tailor-make curriculum learning in proximal policy optimization (PPO) to help models capture the essential samples for easy-to-hard training. In experiments, we build a large-scale Weibo dataset for trendy response prediction, and its results show that PopALM can help boost the performance of advanced language models.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Syntactic Ghost&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26080;&#24863;&#30693;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.18945</link><description>&lt;p&gt;
Syntactic Ghost&#65306;&#19968;&#31181;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#26080;&#24863;&#30693;&#36890;&#29992;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18945
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Syntactic Ghost&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26080;&#24863;&#30693;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#34987;&#21457;&#29616;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#21487;&#20197;&#23558;&#28431;&#27934;&#36716;&#31227;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PLM&#21518;&#38376;&#25915;&#20987;&#37319;&#29992;&#26126;&#26174;&#30340;&#35302;&#21457;&#22120;&#65292;&#22312;&#25163;&#21160;&#23545;&#20934;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#65292;&#22240;&#27492;&#22312;&#25928;&#26524;&#12289;&#38544;&#21311;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#26399;&#26395;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19981;&#21487;&#35265;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#65292;&#31216;&#20026;Syntactic Ghost&#65288;&#31616;&#31216;&#20026;synGhost&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26041;&#27861;&#25932;&#24847;&#22320;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#39044;&#23450;&#20041;&#21477;&#27861;&#32467;&#26500;&#30340;&#27602;&#23475;&#26679;&#26412;&#20316;&#20026;&#38544;&#34109;&#35302;&#21457;&#22120;&#65292;&#28982;&#21518;&#23558;&#21518;&#38376;&#26893;&#20837;&#21040;&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#65292;&#32780;&#19981;&#20250;&#30772;&#22351;&#21407;&#22987;&#30693;&#35782;&#12290;&#27602;&#23475;&#26679;&#26412;&#30340;&#36755;&#20986;&#34920;&#31034;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23613;&#21487;&#33021;&#22343;&#21248;&#22320;&#20998;&#24067;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#24418;&#25104;&#24191;&#27867;&#30340;&#21518;&#38376;&#12290;&#27492;&#22806;&#65292;&#22312;&#20142;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18945v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have been found susceptible to backdoor attacks, which can transfer vulnerabilities to various downstream tasks. However, existing PLM backdoors are conducted with explicit triggers under the manually aligned, thus failing to satisfy expectation goals simultaneously in terms of effectiveness, stealthiness, and universality. In this paper, we propose a novel approach to achieve invisible and general backdoor implantation, called \textbf{Syntactic Ghost} (synGhost for short). Specifically, the method hostilely manipulates poisoned samples with different predefined syntactic structures as stealth triggers and then implants the backdoor to pre-trained representation space without disturbing the primitive knowledge. The output representations of poisoned samples are distributed as uniformly as possible in the feature space via contrastive learning, forming a wide range of backdoors. Additionally, in light 
&lt;/p&gt;</description></item><item><title>SemEval-2024&#30340;&#20219;&#21153;10&#26088;&#22312;&#35782;&#21035;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#24182;&#25214;&#20986;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#21442;&#19982;&#32773;&#38656;&#33258;&#21160;&#25191;&#34892;&#24773;&#32490;&#35782;&#21035;&#21644;&#24773;&#32490;&#36716;&#21464;&#25512;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.18944</link><description>&lt;p&gt;
SemEval 2024 -- &#20219;&#21153;10&#65306;&#24773;&#32490;&#21457;&#29616;&#21450;&#23545;&#35805;&#20013;&#24773;&#32490;&#36716;&#21464;&#30340;&#25512;&#29702;&#65288;EDiReF&#65289;
&lt;/p&gt;
&lt;p&gt;
SemEval 2024 -- Task 10: Emotion Discovery and Reasoning its Flip in Conversation (EDiReF)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18944
&lt;/p&gt;
&lt;p&gt;
SemEval-2024&#30340;&#20219;&#21153;10&#26088;&#22312;&#35782;&#21035;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#24182;&#25214;&#20986;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#21442;&#19982;&#32773;&#38656;&#33258;&#21160;&#25191;&#34892;&#24773;&#32490;&#35782;&#21035;&#21644;&#24773;&#32490;&#36716;&#21464;&#25512;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SemEval-2024&#20219;&#21153;10&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#22312;&#21333;&#35821;&#31181;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;-&#33521;&#35821;&#28151;&#21512;&#23545;&#35805;&#20013;&#35782;&#21035;&#24773;&#32490;&#24182;&#25214;&#20986;&#24773;&#32490;&#36716;&#21464;&#32972;&#21518;&#21407;&#22240;&#30340;&#20849;&#20139;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#23376;&#20219;&#21153; - &#29992;&#20110;&#28151;&#21512;&#23545;&#35805;&#20013;&#24773;&#32490;&#35782;&#21035;&#12289;&#28151;&#21512;&#23545;&#35805;&#20013;&#24773;&#32490;&#36716;&#21464;&#25512;&#29702;&#12289;&#20197;&#21450;&#33521;&#25991;&#23545;&#35805;&#20013;&#24773;&#32490;&#36716;&#21464;&#25512;&#29702;&#12290;&#21442;&#19982;&#31995;&#32479;&#34987;&#35201;&#27714;&#33258;&#21160;&#25191;&#34892;&#19968;&#20010;&#25110;&#22810;&#20010;&#36825;&#20123;&#23376;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#25163;&#21160;&#27880;&#37322;&#30340;&#23545;&#35805;&#65292;&#37325;&#28857;&#25918;&#22312;&#24773;&#32490;&#21644;&#35302;&#21457;&#24773;&#32490;&#36716;&#21464;&#30340;&#21407;&#22240;&#19978;&#65288;&#20219;&#21153;&#25968;&#25454;&#21487;&#22312;https://github.com/LCS2-IIITD/EDiReF-SemEval2024.git&#33719;&#21462;&#65289;&#12290;&#24635;&#20849;&#26377;84&#20010;&#21442;&#19982;&#32773;&#21442;&#19982;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;&#20854;&#20013;&#26368;&#25797;&#38271;&#30340;&#31995;&#32479;&#22312;&#21508;&#20010;&#23376;&#20219;&#21153;&#19978;&#33719;&#24471;&#20102;0.70&#12289;0.79&#21644;0.76&#30340;F1&#20998;&#25968;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#26469;&#33258;24&#20010;&#22242;&#38431;&#30340;&#32467;&#26524;&#21644;&#21457;&#29616;&#20197;&#21450;&#20182;&#20204;&#31995;&#32479;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18944v1 Announce Type: cross  Abstract: We present SemEval-2024 Task 10, a shared task centred on identifying emotions and finding the rationale behind their flips within monolingual English and Hindi-English code-mixed dialogues. This task comprises three distinct subtasks - emotion recognition in conversation for code-mixed dialogues, emotion flip reasoning for code-mixed dialogues, and emotion flip reasoning for English dialogues. Participating systems were tasked to automatically execute one or more of these subtasks. The datasets for these tasks comprise manually annotated conversations focusing on emotions and triggers for emotion shifts (The task data is available at https://github.com/LCS2-IIITD/EDiReF-SemEval2024.git). A total of 84 participants engaged in this task, with the most adept systems attaining F1-scores of 0.70, 0.79, and 0.76 for the respective subtasks. This paper summarises the results and findings from 24 teams alongside their system descriptions.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25193;&#23637;&#22823;&#35268;&#27169;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21457;&#38899;&#38556;&#30861;&#35821;&#38899;&#20013;&#26816;&#27979;&#19981;&#24403;&#20572;&#39039;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20219;&#21153;&#35774;&#35745;&#12289;&#26631;&#27880;&#31574;&#30053;&#21644;&#19981;&#24403;&#20572;&#39039;&#39044;&#27979;&#23618;&#65292;&#20026;&#35780;&#20272;&#30149;&#24773;&#20005;&#37325;&#31243;&#24230;&#21644;&#35821;&#35328;&#27835;&#30103;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.18923</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#26816;&#27979;&#21457;&#38899;&#38556;&#30861;&#35821;&#38899;&#20013;&#30340;&#19981;&#24403;&#20572;&#39039;
&lt;/p&gt;
&lt;p&gt;
Inappropriate Pause Detection In Dysarthric Speech Using Large-Scale Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18923
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25193;&#23637;&#22823;&#35268;&#27169;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21457;&#38899;&#38556;&#30861;&#35821;&#38899;&#20013;&#26816;&#27979;&#19981;&#24403;&#20572;&#39039;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20219;&#21153;&#35774;&#35745;&#12289;&#26631;&#27880;&#31574;&#30053;&#21644;&#19981;&#24403;&#20572;&#39039;&#39044;&#27979;&#23618;&#65292;&#20026;&#35780;&#20272;&#30149;&#24773;&#20005;&#37325;&#31243;&#24230;&#21644;&#35821;&#35328;&#27835;&#30103;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#38899;&#38556;&#30861;&#26159;&#20013;&#39118;&#24739;&#32773;&#24120;&#35265;&#38382;&#39064;&#65292;&#20005;&#37325;&#24433;&#21709;&#35821;&#38899;&#21487;&#25026;&#24615;&#12290;&#19981;&#24403;&#20572;&#39039;&#22312;&#30149;&#24773;&#35780;&#20272;&#21644;&#35821;&#35328;&#27835;&#30103;&#20013;&#26159;&#20851;&#38190;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#26469;&#26816;&#27979;&#21457;&#38899;&#38556;&#30861;&#35821;&#38899;&#20013;&#30340;&#19981;&#24403;&#20572;&#39039;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20219;&#21153;&#35774;&#35745;&#12289;&#26631;&#27880;&#31574;&#30053;&#21644;&#19968;&#20010;&#24102;&#26377;&#19981;&#24403;&#20572;&#39039;&#39044;&#27979;&#23618;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#20572;&#39039;&#26816;&#27979;&#35270;&#20026;&#35821;&#38899;&#35782;&#21035;&#65292;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#23558;&#35821;&#38899;&#36716;&#25442;&#20026;&#24102;&#26377;&#20572;&#39039;&#26631;&#35760;&#30340;&#25991;&#26412;&#12290;&#26681;&#25454;&#35774;&#35745;&#30340;&#26032;&#20219;&#21153;&#65292;&#22312;&#25991;&#26412;&#32423;&#21035;&#26631;&#20986;&#20572;&#39039;&#20301;&#32622;&#21450;&#20854;&#26159;&#21542;&#36866;&#24403;&#12290;&#25105;&#20204;&#19982;&#35328;&#35821;&#30149;&#29702;&#23398;&#23478;&#21512;&#20316;&#21046;&#23450;&#26631;&#27880;&#26631;&#20934;&#65292;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#24102;&#26631;&#27880;&#25968;&#25454;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#19968;&#20010;&#19981;&#24403;&#20572;&#39039;&#39044;&#27979;&#23618;&#25193;&#23637;&#20102;ASR&#27169;&#22411;&#65292;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#19981;&#24403;&#20572;&#39039;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18923v1 Announce Type: new  Abstract: Dysarthria, a common issue among stroke patients, severely impacts speech intelligibility. Inappropriate pauses are crucial indicators in severity assessment and speech-language therapy. We propose to extend a large-scale speech recognition model for inappropriate pause detection in dysarthric speech. To this end, we propose task design, labeling strategy, and a speech recognition model with an inappropriate pause prediction layer. First, we treat pause detection as speech recognition, using an automatic speech recognition (ASR) model to convert speech into text with pause tags. According to the newly designed task, we label pause locations at the text level and their appropriateness. We collaborate with speech-language pathologists to establish labeling criteria, ensuring high-quality annotated data. Finally, we extend the ASR model with an inappropriate pause prediction layer for end-to-end inappropriate pause detection. Moreover, we p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#27861; $\texttt{AdaMergeX}$&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#36866;&#37197;&#22120;&#34701;&#21512;&#26469;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#21644;&#35821;&#35328;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.18913</link><description>&lt;p&gt;
AdaMergeX: &#36328;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#36866;&#37197;&#22120;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18913
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#27861; $\texttt{AdaMergeX}$&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#36866;&#37197;&#22120;&#34701;&#21512;&#26469;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#21644;&#35821;&#35328;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#22312;&#29305;&#23450;&#35821;&#35328;&#30340;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#30452;&#25509;&#24494;&#35843;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#36890;&#36807;&#22312;&#28304;&#35821;&#35328;&#19978;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#24182;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#36873;&#25321;&#21478;&#19968;&#20010;&#20219;&#21153;&#26469;&#35299;&#32806;&#20102;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#20174;&#32780;&#20998;&#31163;&#20102;&#8220;&#20219;&#21153;&#33021;&#21147;&#8221;&#21644;&#8220;&#35821;&#35328;&#33021;&#21147;&#8221;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26410;&#33021;&#20805;&#20998;&#23558;&#20219;&#21153;&#33021;&#21147;&#19982;&#28304;&#35821;&#35328;&#25110;&#32773;&#35821;&#35328;&#33021;&#21147;&#19982;&#36873;&#25321;&#30340;&#20219;&#21153;&#23436;&#20840;&#20998;&#24320;&#12290;&#26412;&#25991;&#25215;&#35748;&#20219;&#21153;&#33021;&#21147;&#21644;&#35821;&#35328;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#30446;&#26631;&#35821;&#35328;&#21644;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#20219;&#21153;&#24046;&#36317;&#19978;&#12290;&#30001;&#20110;&#35813;&#24046;&#36317;&#28040;&#38500;&#20102;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20551;&#23450;&#23427;&#22312;&#21508;&#20219;&#21153;&#38388;&#20445;&#25345;&#19968;&#33268;&#12290;&#22522;&#20110;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; $\texttt{AdaMergeX}$ &#30340;&#26032;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#36866;&#37197;&#22120;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18913v1 Announce Type: cross  Abstract: As an effective alternative to the direct fine-tuning on target tasks in specific languages, cross-lingual transfer addresses the challenges of limited training data by decoupling ''task ability'' and ''language ability'' by fine-tuning on the target task in the source language and another selected task in the target language, respectively. However, they fail to fully separate the task ability from the source language or the language ability from the chosen task. In this paper, we acknowledge the mutual reliance between task ability and language ability and direct our attention toward the gap between the target language and the source language on tasks. As the gap removes the impact of tasks, we assume that it remains consistent across tasks. Based on this assumption, we propose a new cross-lingual transfer method called $\texttt{AdaMergeX}$ that utilizes adaptive adapter merging. By introducing a reference task, we can determine that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#36753;&#65288;UKE&#65289;&#65292;&#26088;&#22312;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20316;&#20026;&#30693;&#35782;&#26356;&#26032;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#26500;&#24314;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#21644;&#21709;&#24212;&#24615;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18909</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#20107;&#23454;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#65306;&#36808;&#21521;&#23454;&#29992;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#36753;&#65288;UKE&#65289;&#65292;&#26088;&#22312;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20316;&#20026;&#30693;&#35782;&#26356;&#26032;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#26500;&#24314;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#21644;&#21709;&#24212;&#24615;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#23558;&#30693;&#35782;&#26356;&#26032;&#27880;&#20837;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#20854;&#20445;&#25345;&#27491;&#30830;&#24615;&#21644;&#26368;&#26032;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#31574;&#30053;&#26126;&#26174;&#19981;&#20999;&#23454;&#38469;&#65306;&#23427;&#20204;&#20165;&#20351;&#29992;&#31934;&#24515;&#31574;&#21010;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#65288;&#20027;&#39064;&#12289;&#20851;&#31995;&#21644;&#23545;&#35937;&#30340;&#19977;&#20803;&#32452;&#65289;&#36827;&#34892;&#26356;&#26032;&#65292;&#32780;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#26356;&#26032;&#36890;&#24120;&#20986;&#29616;&#22312;&#26032;&#38395;&#25991;&#31456;&#31561;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#36753;&#65288;UKE&#65289;&#12290;&#23427;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#30452;&#25509;&#35780;&#20272;&#32534;&#36753;&#24615;&#33021;&#65292;&#31216;&#20026;&#38750;&#32467;&#26500;&#21270;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;UKE&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#26500;&#24314;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#21709;&#24212;&#36805;&#36895;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#25104;&#20026;&#19968;&#20010;&#26356;&#23454;&#29992;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#22312;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;UKE&#23545;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;&#23427;&#20204;&#30340;&#20851;&#38190;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18909v1 Announce Type: cross  Abstract: Knowledge editing aims to inject knowledge updates into language models to keep them correct and up-to-date. However, its current evaluation strategies are notably impractical: they solely update with well-curated structured facts (triplets with subjects, relations, and objects), whereas real-world knowledge updates commonly emerge in unstructured texts like news articles. In this paper, we propose a new benchmark, Unstructured Knowledge Editing (UKE). It evaluates editing performance directly using unstructured texts as knowledge updates, termed unstructured facts. Hence UKE avoids the laborious construction of structured facts and enables efficient and responsive knowledge editing, becoming a more practical benchmark. We conduct extensive experiments on newly built datasets and demonstrate that UKE poses a significant challenge to state-of-the-art knowledge editing methods, resulting in their critical performance declines. We further
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21512;&#29702;&#24615;&#26816;&#26597;&#26041;&#27861;&#65306;&#36890;&#36807;&#20027;&#25104;&#20998;&#20998;&#26512;&#23558;&#37325;&#24314;&#26641;&#25237;&#24433;&#21040;&#31354;&#38388;&#20013;&#65292;&#26377;&#25928;&#22320;&#21487;&#35270;&#21270;&#20102;&#24322;&#24120;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#22312;&#24418;&#24335;&#19978;&#30340;&#27874;&#21160;&#12290;</title><link>https://arxiv.org/abs/2402.18877</link><description>&lt;p&gt;
&#20027;&#25104;&#20998;&#20998;&#26512;&#20316;&#20026;&#36125;&#21494;&#26031;&#35821;&#35328;&#28436;&#21270;&#37325;&#24314;&#30340;&#21512;&#29702;&#24615;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Principal Component Analysis as a Sanity Check for Bayesian Phylolinguistic Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18877
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21512;&#29702;&#24615;&#26816;&#26597;&#26041;&#27861;&#65306;&#36890;&#36807;&#20027;&#25104;&#20998;&#20998;&#26512;&#23558;&#37325;&#24314;&#26641;&#25237;&#24433;&#21040;&#31354;&#38388;&#20013;&#65292;&#26377;&#25928;&#22320;&#21487;&#35270;&#21270;&#20102;&#24322;&#24120;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#22312;&#24418;&#24335;&#19978;&#30340;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;: &#36890;&#36807;&#20027;&#25104;&#20998;&#20998;&#26512;&#23558;&#37325;&#24314;&#26641;&#25237;&#24433;&#21040;&#19968;&#20010;&#31354;&#38388;&#19978;&#65292;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#21512;&#29702;&#24615;&#26816;&#26597;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#21487;&#35270;&#21270;&#24322;&#24120;&#65292;&#23588;&#20854;&#26159;&#22312;&#24418;&#24335;&#19978;&#30340;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18877v1 Announce Type: new  Abstract: Bayesian approaches to reconstructing the evolutionary history of languages rely on the tree model, which assumes that these languages descended from a common ancestor and underwent modifications over time. However, this assumption can be violated to different extents due to contact and other factors. Understanding the degree to which this assumption is violated is crucial for validating the accuracy of phylolinguistic inference. In this paper, we propose a simple sanity check: projecting a reconstructed tree onto a space generated by principal component analysis. By using both synthetic and real data, we demonstrate that our method effectively visualizes anomalies, particularly in the form of jogging.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20107;&#23454;-&#27169;&#26495;&#20998;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;SlotSum&#65292;&#29992;&#20110;&#20943;&#23569;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#20307;&#25688;&#35201;&#20013;&#29983;&#25104;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18873</link><description>&lt;p&gt;
&#36890;&#36807;&#20107;&#23454;-&#27169;&#26495;&#20998;&#35299;&#20943;&#23569;&#23454;&#20307;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Reducing Hallucinations in Entity Abstract Summarization with Facts-Template Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18873
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20107;&#23454;-&#27169;&#26495;&#20998;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;SlotSum&#65292;&#29992;&#20110;&#20943;&#23569;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#20307;&#25688;&#35201;&#20013;&#29983;&#25104;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#25688;&#35201;&#24635;&#32467;&#26088;&#22312;&#22522;&#20110;&#19968;&#32452;&#30456;&#20851;&#30340;&#20114;&#32852;&#32593;&#25991;&#26723;&#29983;&#25104;&#19968;&#20010;&#32473;&#23450;&#23454;&#20307;&#30340;&#36830;&#36143;&#25551;&#36848;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#21487;&#33021;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#20851;&#20110;&#23454;&#20307;&#30340;&#38750;&#20107;&#23454;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#25688;&#35201;&#20998;&#35299;&#25104;&#20004;&#37096;&#20998;&#65306;&#34920;&#31034;&#32473;&#23450;&#23454;&#20307;&#30340;&#20107;&#23454;&#20449;&#24687;&#30340;&#20107;&#23454;&#65292;PLMs&#23481;&#26131;&#25423;&#36896;&#65307;&#20197;&#21450;&#21253;&#21547;&#36890;&#29992;&#20869;&#23481;&#19988;&#20026;&#20107;&#23454;&#25351;&#23450;&#27133;&#30340;&#27169;&#26495;&#65292;PLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#12290;&#22522;&#20110;&#20107;&#23454;-&#27169;&#26495;&#20998;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SlotSum&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#23454;&#20307;&#25688;&#35201;&#24635;&#32467;&#26694;&#26550;&#12290;SlotSum&#39318;&#20808;&#21019;&#24314;&#27169;&#26495;&#65292;&#28982;&#21518;&#26681;&#25454;&#36755;&#20837;&#25991;&#26723;&#39044;&#27979;&#27599;&#20010;&#27169;&#26495;&#27133;&#30340;&#20107;&#23454;&#12290;&#21463;&#30410;&#20110;&#25105;&#20204;&#30340;&#20107;&#23454;-&#27169;&#26495;&#20998;&#35299;&#65292;SlotSum&#21487;&#20197;&#36731;&#26494;&#23450;&#20301;&#38169;&#35823;&#65292;&#24182;&#36827;&#19968;&#27493;&#32416;&#27491;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18873v1 Announce Type: new  Abstract: Entity abstract summarization aims to generate a coherent description of a given entity based on a set of relevant Internet documents. Pretrained language models (PLMs) have achieved significant success in this task, but they may suffer from hallucinations, i.e. generating non-factual information about the entity. To address this issue, we decompose the summary into two components: Facts that represent the factual information about the given entity, which PLMs are prone to fabricate; and Template that comprises generic content with designated slots for facts, which PLMs can generate competently. Based on the facts-template decomposition, we propose SlotSum, an explainable framework for entity abstract summarization. SlotSum first creates the template and then predicts the fact for each template slot based on the input documents. Benefiting from our facts-template decomposition, SlotSum can easily locate errors and further rectify halluci
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#24335;&#36830;&#25509;&#35843;&#26597;&#20102;&#36830;&#32493;&#24494;&#35843;&#20013;&#19981;&#21516;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20960;&#20309;&#36830;&#25509;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18865</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#20943;&#23569;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18865
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#24335;&#36830;&#25509;&#35843;&#26597;&#20102;&#36830;&#32493;&#24494;&#35843;&#20013;&#19981;&#21516;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20960;&#20309;&#36830;&#25509;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#26377;&#30740;&#31350;&#26174;&#31034;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24403;LLMs&#19981;&#26029;&#22312;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#29305;&#23450;&#39046;&#22495;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#23545;&#21382;&#21490;&#20219;&#21153;&#30340;&#25512;&#29702;&#24615;&#33021;&#20250;&#24613;&#21095;&#19979;&#38477;&#65292;&#36825;&#34987;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#38656;&#35201;&#22312;&#23398;&#20064;&#21487;&#22609;&#24615;&#21644;&#35760;&#24518;&#31283;&#23450;&#24615;&#20043;&#38388;&#20445;&#25345;&#26435;&#34913;&#12290;&#24050;&#26377;&#24456;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#35832;&#22914;&#35760;&#24518;&#37325;&#25918;&#12289;&#27491;&#21017;&#21270;&#21644;&#21442;&#25968;&#38548;&#31163;&#31561;&#31574;&#30053;&#65292;&#20294;&#22312;&#36830;&#32493;&#30340;LLMs&#24494;&#35843;&#22330;&#26223;&#20013;&#65292;&#23545;&#21508;&#20010;&#30456;&#37051;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20960;&#20309;&#36830;&#25509;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#24335;&#36830;&#25509;&#30340;&#35270;&#35282;&#35843;&#26597;&#20102;&#19981;&#21516;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20960;&#20309;&#36830;&#25509;&#65292;&#36825;&#24847;&#21619;&#30528;&#19981;&#21516;&#26497;&#23567;&#20540;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#20302;&#25439;&#22833;&#30340;&#23665;&#35895;&#30456;&#36830;&#25509;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLMs&#24494;&#35843;&#20013;&#30340;&#27169;&#24335;&#36830;&#25509;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18865v1 Announce Type: cross  Abstract: Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem. A trade-off needs to be kept between learning plasticity and memory stability. Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the continual LLMs fine-tuning scenarios. In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley. Through extensive experiments, we uncover the mode connectivity phenomenon in the LLMs contin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;NLP&#22823;&#22411;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;LSB-NLP&#28151;&#21512;&#26694;&#26550;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#38544;&#20889;&#25991;&#26412;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#20013;&#25991;&#23383;&#31526;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.18849</link><description>&lt;p&gt;
&#25552;&#21319;&#38544;&#20889;&#25991;&#26412;&#25552;&#21462;&#65306;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23545;&#20934;&#30830;&#24615;&#21644;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18849
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;NLP&#22823;&#22411;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;LSB-NLP&#28151;&#21512;&#26694;&#26550;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#38544;&#20889;&#25991;&#26412;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#20013;&#25991;&#23383;&#31526;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#38544;&#20889;&#26415;&#25216;&#26415;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22823;&#22411;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;&#25552;&#21462;&#38544;&#20889;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20256;&#32479;&#30340;&#26368;&#20302;&#26377;&#25928;&#20301;&#65288;LSB&#65289;&#38544;&#20889;&#26415;&#25216;&#26415;&#22312;&#22788;&#29702;&#22797;&#26434;&#23383;&#31526;&#32534;&#30721;&#65288;&#22914;&#20013;&#25991;&#23383;&#31526;&#65289;&#26102;&#22312;&#20449;&#24687;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;LSB-NLP&#28151;&#21512;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#38598;&#25104;&#20102;NLP&#22823;&#22411;&#27169;&#22411;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#22914;&#38169;&#35823;&#26816;&#27979;&#12289;&#32416;&#27491;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#20998;&#26512;&#65292;&#20197;&#21450;&#20449;&#24687;&#37325;&#24314;&#25216;&#26415;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#38544;&#20889;&#25991;&#26412;&#25552;&#21462;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LSB-NLP&#28151;&#21512;&#26694;&#26550;&#22312;&#25552;&#39640;&#38544;&#20889;&#25991;&#26412;&#25552;&#21462;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#20013;&#25991;&#23383;&#31526;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18849v1 Announce Type: cross  Abstract: This study discusses a new method combining image steganography technology with Natural Language Processing (NLP) large models, aimed at improving the accuracy and robustness of extracting steganographic text. Traditional Least Significant Bit (LSB) steganography techniques face challenges in accuracy and robustness of information extraction when dealing with complex character encoding, such as Chinese characters. To address this issue, this study proposes an innovative LSB-NLP hybrid framework. This framework integrates the advanced capabilities of NLP large models, such as error detection, correction, and semantic consistency analysis, as well as information reconstruction techniques, thereby significantly enhancing the robustness of steganographic text extraction. Experimental results show that the LSB-NLP hybrid framework excels in improving the extraction accuracy of steganographic text, especially in handling Chinese characters. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24207;&#30340;&#25935;&#24863;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35789;&#24207;&#20449;&#24687;&#37327;&#65292;&#21457;&#29616;&#22312;&#35821;&#35328;&#25552;&#31034;&#25552;&#20379;&#20887;&#20313;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#23545;&#35789;&#24207;&#21464;&#21270;&#19981;&#25935;&#24863;&#65292;&#19988;&#19981;&#21516;&#20219;&#21153;&#38388;&#30340;&#19981;&#25935;&#24863;&#31243;&#24230;&#26377;&#25152;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.18838</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#35789;&#24207;&#37325;&#35201;&#65292;&#20160;&#20040;&#26102;&#20505;&#19981;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
When does word order matter and when doesn't it?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24207;&#30340;&#25935;&#24863;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35789;&#24207;&#20449;&#24687;&#37327;&#65292;&#21457;&#29616;&#22312;&#35821;&#35328;&#25552;&#31034;&#25552;&#20379;&#20887;&#20313;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#23545;&#35789;&#24207;&#21464;&#21270;&#19981;&#25935;&#24863;&#65292;&#19988;&#19981;&#21516;&#20219;&#21153;&#38388;&#30340;&#19981;&#25935;&#24863;&#31243;&#24230;&#26377;&#25152;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#33021;&#23545;&#35789;&#24207;&#21464;&#21270;&#19981;&#25935;&#24863;&#12290;&#26412;&#25991;&#25552;&#20986;&#35821;&#35328;&#20887;&#20313;&#24615;&#21487;&#20197;&#35299;&#37322;&#36825;&#19968;&#29616;&#35937;&#65292;&#21363;&#35789;&#24207;&#21644;&#20854;&#20182;&#35821;&#35328;&#25552;&#31034;&#65288;&#22914;&#26684;&#26631;&#65289;&#25552;&#20379;&#37325;&#21472;&#19988;&#20887;&#20313;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#24403;&#39034;&#24207;&#25552;&#20379;&#20887;&#20313;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#23545;&#35789;&#24207;&#30340;&#19981;&#25935;&#24863;&#34920;&#29616;&#65292;&#32780;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#25935;&#24863;&#31243;&#24230;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#20351;&#29992;&#26080;&#24207;&#21644;&#25171;&#20081;&#39034;&#24207;&#30340;&#21477;&#23376;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#26469;&#37327;&#21270;&#35789;&#24207;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#35789;&#24207;&#20449;&#24687;&#36234;&#19981;&#20855;&#20449;&#24687;&#37327;&#65292;&#27169;&#22411;&#22312;&#26080;&#24207;&#21644;&#25171;&#20081;&#39034;&#24207;&#30340;&#21477;&#23376;&#20043;&#38388;&#30340;&#39044;&#27979;&#36234;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#36825;&#31181;&#24433;&#21709;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65306;&#23545;&#20110;&#19968;&#20123;&#20219;&#21153;&#65292;&#22914;SST-2&#65292;LM&#30340;&#39044;&#27979;&#20960;&#20046;&#24635;&#26159;&#19982;&#21407;&#22987;&#32467;&#26524;&#19968;&#33268;&#65292;&#21363;&#20351;&#28857;&#38388;&#20114;&#20449;&#24687;&#65288;PMI&#65289;&#21457;&#29983;&#21464;&#21270;&#65292;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18838v1 Announce Type: new  Abstract: Language models (LMs) may appear insensitive to word order changes in natural language understanding (NLU) tasks. In this paper, we propose that linguistic redundancy can explain this phenomenon, whereby word order and other linguistic cues such as case markers provide overlapping and thus redundant information. Our hypothesis is that models exhibit insensitivity to word order when the order provides redundant information, and the degree of insensitivity varies across tasks. We quantify how informative word order is using mutual information (MI) between unscrambled and scrambled sentences. Our results show the effect that the less informative word order is, the more consistent the model's predictions are between unscrambled and scrambled sentences. We also find that the effect varies across tasks: for some tasks, like SST-2, LMs' prediction is almost always consistent with the original one even if the Pointwise-MI (PMI) changes, while fo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#26694;&#26550;&#21644;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;HTC&#27169;&#22411;&#30340;HiAdv&#26694;&#26550;&#65292;&#20248;&#21270;&#20102;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#35777;&#26126;&#20102;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#32597;&#35265;&#31867;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.18825</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Utilizing Local Hierarchy with Adversarial Training for Hierarchical Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18825
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#26694;&#26550;&#21644;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;HTC&#27169;&#22411;&#30340;HiAdv&#26694;&#26550;&#65292;&#20248;&#21270;&#20102;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#35777;&#26126;&#20102;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#32597;&#35265;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#65288;HTC&#65289;&#26159;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#20219;&#21153;&#65292;&#22240;&#20026;&#20854;&#22797;&#26434;&#30340;&#20998;&#31867;&#32467;&#26500;&#12290;&#20960;&#20046;&#25152;&#26377;&#26368;&#36817;&#30340;HTC&#20316;&#21697;&#37117;&#20851;&#27880;&#26631;&#31614;&#22914;&#20309;&#32467;&#26500;&#21270;&#65292;&#20294;&#24573;&#30053;&#20102;&#26681;&#25454;&#27599;&#20010;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#30340;&#23376;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#20016;&#23500;&#30340;&#26631;&#31614;&#20849;&#29616;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36825;&#31181;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#21644;&#19968;&#20010;&#23545;&#25239;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HiAdv&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#20960;&#20046;&#25152;&#26377;HTC&#27169;&#22411;&#65292;&#24182;&#23558;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20856;&#22411;&#30340;HTC&#27169;&#22411;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;HiAdv&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#20998;&#31867;&#23618;&#27425;&#32467;&#26500;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#26694;&#26550;&#30340;&#25552;&#21319;&#30830;&#23454;&#26469;&#33258;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#65292;&#26412;&#22320;&#23618;&#27425;&#32467;&#26500;&#26377;&#21033;&#20110;&#37027;&#20123;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#32597;&#35265;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18825v1 Announce Type: new  Abstract: Hierarchical text classification (HTC) is a challenging subtask of multi-label classification due to its complex taxonomic structure. Nearly all recent HTC works focus on how the labels are structured but ignore the sub-structure of ground-truth labels according to each input text which contains fruitful label co-occurrence information. In this work, we introduce this local hierarchy with an adversarial framework. We propose a HiAdv framework that can fit in nearly all HTC models and optimize them with the local hierarchy as auxiliary information. We test on two typical HTC models and find that HiAdv is effective in all scenarios and is adept at dealing with complex taxonomic hierarchies. Further experiments demonstrate that the promotion of our framework indeed comes from the local hierarchy and the local hierarchy is beneficial for rare classes which have insufficient training data.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22788;&#29702;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#27425;&#20013;&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.18815</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#22810;&#35821;&#35328;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Handle Multilingualism?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18815
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22788;&#29702;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#27425;&#20013;&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#35821;&#35328;&#19978;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#22810;&#35821;&#35328;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;LLMs&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#36807;&#31243;&#65306;&#22312;&#21069;&#20960;&#23618;&#20013;&#65292;LLMs&#29702;&#35299;&#38382;&#39064;&#65292;&#23558;&#22810;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#33521;&#35821;&#20197;&#20415;&#20419;&#36827;&#20219;&#21153;&#35299;&#20915;&#38454;&#27573;&#12290;&#22312;&#20013;&#38388;&#23618;&#20013;&#65292;LLMs&#36890;&#36807;&#20197;&#33521;&#35821;&#24605;&#32771;&#24182;&#25972;&#21512;&#22810;&#35821;&#35328;&#30693;&#35782;&#26469;&#36827;&#34892;&#35299;&#20915;&#38382;&#39064;&#65292;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32467;&#26500;&#65292;&#20998;&#21035;&#33719;&#21462;&#20107;&#23454;&#20869;&#23481;&#12290;&#22312;&#26368;&#21518;&#20960;&#23618;&#20013;&#65292;LLMs&#29983;&#25104;&#19982;&#26597;&#35810;&#30340;&#21407;&#22987;&#35821;&#35328;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#29305;&#23450;&#35821;&#35328;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#26816;&#27979;&#30001;&#36755;&#20837;&#35821;&#35328;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#65292;&#21363;&#20351;&#27809;&#26377;&#26631;&#31614;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#35774;&#35745;&#20102;&#19968;&#20010;&#24182;&#34892;&#35821;&#35328;&#29305;&#23450;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18815v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35282;&#33394;&#25198;&#28436;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#26631;&#21644;&#25351;&#23548;&#20197;&#22686;&#24378;&#20854;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18807</link><description>&lt;p&gt;
&#35770;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35282;&#33394;&#25198;&#28436;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Decision-Making Abilities in Role-Playing using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35282;&#33394;&#25198;&#28436;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#26631;&#21644;&#25351;&#23548;&#20197;&#22686;&#24378;&#20854;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29616;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#35282;&#33394;&#25198;&#28436;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#20223;&#29305;&#23450;&#39046;&#22495;&#19987;&#23478;&#26102;&#65292;&#20027;&#35201;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#20114;&#21160;&#26102;&#65292;&#35282;&#33394;&#30340;&#20915;&#31574;&#33021;&#21147;&#26174;&#33879;&#22320;&#22609;&#36896;&#20854;&#34892;&#20026;&#27169;&#24335;&#12290;&#26412;&#25991;&#38598;&#20013;&#35780;&#20272;LLMs&#22312;&#35282;&#33394;&#25198;&#28436;&#21518;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#20174;&#32780;&#39564;&#35777;&#35282;&#33394;&#25198;&#28436;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#22686;&#24378;LLMs&#22312;&#35282;&#33394;&#25198;&#28436;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#25552;&#20379;&#25351;&#26631;&#21644;&#25351;&#23548;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;LLMs&#29983;&#25104;&#23545;&#24212;&#20110;&#36808;&#23572;&#26031;-&#24067;&#37324;&#26684;&#26031;&#31867;&#22411;&#25351;&#26631;&#65288;MBTI&#65289;&#30340;16&#31181;&#20154;&#26684;&#31867;&#22411;&#30340;&#34394;&#25311;&#35282;&#33394;&#25551;&#36848;&#65292;&#20195;&#34920;&#20154;&#21475;&#30340;&#32454;&#20998;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20855;&#20307;&#30340;&#23450;&#37327;&#25805;&#20316;&#65292;&#20174;&#36866;&#24212;&#24615;&#12289;&#25506;&#32034;&#24615;&#31561;&#22235;&#20010;&#26041;&#38754;&#35780;&#20272;LLMs&#22312;&#35282;&#33394;&#25198;&#28436;&#21518;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18807v1 Announce Type: cross  Abstract: Large language models (LLMs) are now increasingly utilized for role-playing tasks, especially in impersonating domain-specific experts, primarily through role-playing prompts. When interacting in real-world scenarios, the decision-making abilities of a role significantly shape its behavioral patterns. In this paper, we concentrate on evaluating the decision-making abilities of LLMs post role-playing thereby validating the efficacy of role-playing. Our goal is to provide metrics and guidance for enhancing the decision-making abilities of LLMs in role-playing tasks. Specifically, we first use LLMs to generate virtual role descriptions corresponding to the 16 personality types of Myers-Briggs Type Indicator (abbreviated as MBTI) representing a segmentation of the population. Then we design specific quantitative operations to evaluate the decision-making abilities of LLMs post role-playing from four aspects: adaptability, exploration$\&amp;$ex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ARTiST&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#25991;&#26412;&#31616;&#21270;&#31995;&#32479;&#65292;&#32467;&#21512;&#23569;&#37327;&#31034;&#20363;&#25552;&#31034;&#21644;GPT-3&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#31616;&#21270;&#25216;&#26415;&#29983;&#25104;&#20102;&#36866;&#29992;&#20110;&#22836;&#25140;&#24335;&#26174;&#31034;&#22120;&#30340;&#31616;&#21270;AR&#25991;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ARTiST&#26174;&#33879;&#38477;&#20302;&#20102;&#35748;&#30693;&#36127;&#25285;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18797</link><description>&lt;p&gt;
ARTiST&#65306;&#29992;&#20110;&#22686;&#24378;&#29616;&#23454;&#20219;&#21153;&#25351;&#23548;&#30340;&#33258;&#21160;&#25991;&#26412;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
ARTiST: Automated Text Simplification for Task Guidance in Augmented Reality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18797
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ARTiST&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#25991;&#26412;&#31616;&#21270;&#31995;&#32479;&#65292;&#32467;&#21512;&#23569;&#37327;&#31034;&#20363;&#25552;&#31034;&#21644;GPT-3&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#31616;&#21270;&#25216;&#26415;&#29983;&#25104;&#20102;&#36866;&#29992;&#20110;&#22836;&#25140;&#24335;&#26174;&#31034;&#22120;&#30340;&#31616;&#21270;AR&#25991;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ARTiST&#26174;&#33879;&#38477;&#20302;&#20102;&#35748;&#30693;&#36127;&#25285;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22686;&#24378;&#29616;&#23454;&#20013;&#21576;&#29616;&#30340;&#25991;&#26412;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#23601;&#22320;&#12289;&#23454;&#26102;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#20174;&#20107;&#35748;&#30693;&#38656;&#27714;&#39640;&#30340;AR&#20219;&#21153;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#22836;&#25140;&#24335;&#26174;&#31034;&#22120;&#19978;&#21576;&#29616;&#26102;&#65292;&#36825;&#20123;&#20869;&#23481;&#24456;&#38590;&#24555;&#36895;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ARTiST&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#25991;&#26412;&#31616;&#21270;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#25552;&#31034;&#21644;GPT-3&#27169;&#22411;&#65292;&#19987;&#38376;&#20026;&#22686;&#24378;&#29616;&#23454;&#20248;&#21270;&#25991;&#26412;&#38271;&#24230;&#21644;&#35821;&#20041;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#32467;&#21512;&#20102;&#33258;&#23450;&#20041;&#30340;&#35823;&#24046;&#26657;&#20934;&#27169;&#22411;&#21644;&#23569;&#37327;&#31034;&#20363;&#25552;&#31034;&#65292;&#20197;&#25972;&#21512;&#21477;&#27861;&#12289;&#35789;&#27719;&#12289;&#38416;&#36848;&#21644;&#20869;&#23481;&#31616;&#21270;&#25216;&#26415;&#65292;&#24182;&#20026;&#22836;&#25140;&#24335;&#26174;&#31034;&#22120;&#29983;&#25104;&#31616;&#21270;&#30340;AR&#25991;&#26412;&#12290;&#19968;&#39033;&#21253;&#25324;&#19971;&#21517;&#29992;&#25143;&#21644;&#19977;&#21517;&#19987;&#23478;&#30340;&#24418;&#25104;&#24615;&#30740;&#31350;&#24320;&#21457;&#20102;&#36825;&#19968;&#31995;&#32479;&#12290;&#19968;&#39033;&#21253;&#25324;16&#21517;&#29992;&#25143;&#30340;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ARTiST&#20943;&#36731;&#20102;&#35748;&#30693;&#36127;&#33655;&#65292;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#30456;&#27604;&#36739;&#20110;&#26410;&#20462;&#25913;&#30340;&#25991;&#26412;&#21644;&#20256;&#32479;&#26041;&#27861;&#20462;&#25913;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18797v1 Announce Type: cross  Abstract: Text presented in augmented reality provides in-situ, real-time information for users. However, this content can be challenging to apprehend quickly when engaging in cognitively demanding AR tasks, especially when it is presented on a head-mounted display. We propose ARTiST, an automatic text simplification system that uses a few-shot prompt and GPT-3 models to specifically optimize the text length and semantic content for augmented reality. Developed out of a formative study that included seven users and three experts, our system combines a customized error calibration model with a few-shot prompt to integrate the syntactic, lexical, elaborative, and content simplification techniques, and generate simplified AR text for head-worn displays. Results from a 16-user empirical study showed that ARTiST lightens the cognitive load and improves performance significantly over both unmodified text and text modified via traditional methods. Our 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24694;&#24847;&#25200;&#21160;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65288;MPAT&#65289;&#26469;&#26500;&#24314;&#40065;&#26834;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#25269;&#24481;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.18792</link><description>&lt;p&gt;
MPAT: &#25239;&#20987;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
MPAT: Building Robust Deep Neural Networks against Textual Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18792
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24694;&#24847;&#25200;&#21160;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65288;MPAT&#65289;&#26469;&#26500;&#24314;&#40065;&#26834;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#25269;&#24481;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#23545;&#25239;&#24615;&#31034;&#20363;&#26159;&#33030;&#24369;&#30340;&#65292;&#24182;&#19988;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#38450;&#24481;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#22312;&#20445;&#25345;&#26377;&#25928;&#30340;&#38450;&#24481;&#21516;&#26102;&#30830;&#20445;&#21407;&#22987;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24694;&#24847;&#25200;&#21160;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65288;MPAT&#65289;&#65292;&#29992;&#20110;&#26500;&#24314;&#25269;&#24481;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#32423;&#24694;&#24847;&#31034;&#20363;&#29983;&#25104;&#31574;&#30053;&#65292;&#20197;&#29983;&#25104;&#24102;&#26377;&#24694;&#24847;&#25200;&#21160;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#36825;&#20123;&#31034;&#20363;&#34987;&#29992;&#26469;&#20195;&#26367;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#21407;&#22987;&#36755;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35757;&#32451;&#30446;&#26631;&#20989;&#25968;&#65292;&#20197;&#30830;&#20445;&#36798;&#21040;&#38450;&#24481;&#30446;&#26631;&#32780;&#19981;&#25439;&#23475;&#21407;&#22987;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#25915;&#20987;&#20116;&#20010;v
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18792v1 Announce Type: cross  Abstract: Deep neural networks have been proven to be vulnerable to adversarial examples and various methods have been proposed to defend against adversarial attacks for natural language processing tasks. However, previous defense methods have limitations in maintaining effective defense while ensuring the performance of the original task. In this paper, we propose a malicious perturbation based adversarial training method (MPAT) for building robust deep neural networks against textual adversarial attacks. Specifically, we construct a multi-level malicious example generation strategy to generate adversarial examples with malicious perturbations, which are used instead of original inputs for model training. Additionally, we employ a novel training objective function to ensure achieving the defense goal without compromising the performance on the original task. We conduct comprehensive experiments to evaluate our defense method by attacking five v
&lt;/p&gt;</description></item><item><title>FlexLLM&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#19968;&#36845;&#20195;&#20013;&#20849;&#21516;&#25552;&#20379;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35831;&#27714;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#24494;&#35843;&#26426;&#21046;&#23454;&#29616;&#20849;&#20139;GPU&#36164;&#28304;&#30340;&#39640;&#25928;&#21033;&#29992;</title><link>https://arxiv.org/abs/2402.18789</link><description>&lt;p&gt;
FlexLLM&#65306;&#19968;&#31181;&#29992;&#20110;&#20849;&#21516;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18789
&lt;/p&gt;
&lt;p&gt;
FlexLLM&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#19968;&#36845;&#20195;&#20013;&#20849;&#21516;&#25552;&#20379;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35831;&#27714;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#24494;&#35843;&#26426;&#21046;&#23454;&#29616;&#20849;&#20139;GPU&#36164;&#28304;&#30340;&#39640;&#25928;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Parameter-efficient finetuning&#65288;PEFT&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20026;&#19981;&#21516;&#20219;&#21153;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#24120;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#20250;&#20026;&#29992;&#25143;&#21019;&#24314;&#21333;&#29420;&#30340;&#31995;&#32479;&#65292;&#20197;&#25191;&#34892;PEFT&#27169;&#22411;&#24494;&#35843;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#31995;&#32479;&#26080;&#27861;&#22788;&#29702;&#21253;&#21547;&#25512;&#29702;&#21644;PEFT&#24494;&#35843;&#35831;&#27714;&#28151;&#21512;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;&#22240;&#27492;&#65292;&#20849;&#20139;&#30340;GPU&#36164;&#28304;&#21033;&#29992;&#19981;&#36275;&#65292;&#23548;&#33268;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FlexLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#19968;&#36845;&#20195;&#20013;&#20026;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35831;&#27714;&#25552;&#20379;&#26381;&#21153;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21033;&#29992;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#20114;&#34917;&#24615;&#36136;&#65292;&#24182;&#21033;&#29992;&#20849;&#20139;&#30340;GPU&#36164;&#28304;&#26469;&#20849;&#21516;&#36816;&#34892;&#23427;&#20204;&#65292;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#20849;&#21516;&#25552;&#20379;&#30340;&#26041;&#27861;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;FlexLLM&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#32423;&#24494;&#35843;&#26426;&#21046;&#65292;&#23558;&#24207;&#21015;&#30340;&#24494;&#35843;&#35745;&#31639;&#20998;&#35299;&#20026;&#26356;&#23567;&#30340;&#26631;&#35760;&#32423;&#35745;&#31639;&#65292;&#24182;&#20351;&#29992;&#20381;&#36182;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18789v1 Announce Type: cross  Abstract: Parameter-efficient finetuning (PEFT) is a widely used technique to adapt large language models for different tasks. Service providers typically create separate systems for users to perform PEFT model finetuning and inference tasks. This is because existing systems cannot handle workloads that include a mix of inference and PEFT finetuning requests. As a result, shared GPU resources are underutilized, leading to inefficiencies. To address this problem, we present FlexLLM, the first system that can serve inference and parameter-efficient finetuning requests in the same iteration. Our system leverages the complementary nature of these two tasks and utilizes shared GPU resources to run them jointly, using a method called co-serving. To achieve this, FlexLLM introduces a novel token-level finetuning mechanism, which breaks down the finetuning computation of a sequence into smaller token-level computations and uses dependent parallelization
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33889;&#33796;&#29273;&#35821;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#25918;&#35299;&#30721;&#22120;&#27169;&#22411;Gerv\'asio PT*&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#20419;&#36827;&#33889;&#33796;&#29273;&#35821;&#35328;&#25216;&#26415;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.18766</link><description>&lt;p&gt;
&#25512;&#21160;&#33889;&#33796;&#29273;&#35821;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#24320;&#25918;&#35299;&#30721;&#22120;Gerv\'asio PT*
&lt;/p&gt;
&lt;p&gt;
Advancing Generative AI for Portuguese with Open Decoder Gerv\'asio PT*
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18766
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33889;&#33796;&#29273;&#35821;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#25918;&#35299;&#30721;&#22120;&#27169;&#22411;Gerv\'asio PT*&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#20419;&#36827;&#33889;&#33796;&#29273;&#35821;&#35328;&#25216;&#26415;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25512;&#36827;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#35299;&#30721;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#12289;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#24320;&#25918;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#20174;&#36825;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#20026;&#20102;&#24320;&#21457;&#36825;&#20010;&#35299;&#30721;&#22120;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;LLaMA~27B&#27169;&#22411;&#20316;&#20026;&#36215;&#28857;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#35757;&#32451;&#23545;&#21253;&#25324;&#20026;&#27492;&#30446;&#30340;&#20934;&#22791;&#30340;&#26032;&#33889;&#33796;&#29273;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;&#35821;&#35328;&#36164;&#28304;&#36827;&#34892;&#25913;&#36827;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20063;&#22312;&#26412;&#25991;&#20013;&#25552;&#20379;&#12290;&#25152;&#26377;&#29256;&#26412;&#30340;Gerv\'asio&#37117;&#26159;&#24320;&#28304;&#30340;&#65292;&#21487;&#20197;&#20813;&#36153;&#20351;&#29992;&#65292;&#24182;&#21487;&#20197;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#36816;&#34892;&#65292;&#26088;&#22312;&#20419;&#36827;&#33889;&#33796;&#29273;&#35821;&#35328;&#25216;&#26415;&#30740;&#31350;&#21644;&#21019;&#26032;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18766v1 Announce Type: new  Abstract: To advance the neural decoding of Portuguese, in this paper we present a fully open Transformer-based, instruction-tuned decoder model that sets a new state of the art in this respect. To develop this decoder, which we named Gerv\'asio PT*, a strong LLaMA~2 7B model was used as a starting point, and its further improvement through additional training was done over language resources that include new instruction data sets of Portuguese prepared for this purpose, which are also contributed in this paper. All versions of Gerv\'asio are open source and distributed for free under an open license, including for either research or commercial usage, and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in language technology for Portuguese.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#20165;&#38656;&#35201;&#19981;&#21040;100&#20010;&#20363;&#23376;&#23601;&#33021;&#22815;&#24471;&#20986;&#23545;&#25688;&#35201;&#31995;&#32479;&#30340;&#26126;&#30830;&#20559;&#22909;&#65292;&#24182;&#19988;&#21482;&#26377;&#19968;&#20123;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#33021;&#22815;&#36866;&#24230;&#39044;&#27979;&#27169;&#22411;&#30340;&#32988;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18756</link><description>&lt;p&gt;
&#38656;&#35201;&#22810;&#23569;&#27880;&#37322;&#25165;&#33021;&#27604;&#36739;&#25688;&#35201;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Much Annotation is Needed to Compare Summarization Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#20165;&#38656;&#35201;&#19981;&#21040;100&#20010;&#20363;&#23376;&#23601;&#33021;&#22815;&#24471;&#20986;&#23545;&#25688;&#35201;&#31995;&#32479;&#30340;&#26126;&#30830;&#20559;&#22909;&#65292;&#24182;&#19988;&#21482;&#26377;&#19968;&#20123;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#33021;&#22815;&#36866;&#24230;&#39044;&#27979;&#27169;&#22411;&#30340;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20195;&#30340;&#25351;&#23548;&#35843;&#25972;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65288;&#22914;&#25688;&#35201;&#65289;&#20013;&#21464;&#24471;&#38750;&#24120;&#26377;&#33021;&#21147;&#65292;&#24182;&#19988;&#39044;&#35745;&#20250;&#20197;&#31283;&#23450;&#30340;&#36895;&#24230;&#21457;&#24067;&#12290;&#23454;&#38469;&#19978;&#65292;&#20154;&#20204;&#29616;&#22312;&#21487;&#33021;&#24076;&#26395;&#22312;&#24212;&#29992;&#20110;&#26032;&#39046;&#22495;&#25110;&#30446;&#30340;&#26102;&#65292;&#33258;&#20449;&#22320;&#36873;&#25321;&#20294;&#21448;&#20184;&#20986;&#26368;&#23569;&#21162;&#21147;&#30340;&#26368;&#20339;&#25688;&#35201;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#26032;&#38395;&#25688;&#35201;&#29615;&#22659;&#20013;&#36873;&#25321;&#39318;&#36873;&#27169;&#22411;&#25152;&#38656;&#30340;&#27979;&#35797;&#26679;&#26412;&#22823;&#23567;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#33258;&#21160;&#35780;&#20272;&#36824;&#26159;&#20154;&#24037;&#35780;&#20272;&#65292;&#27604;&#36739;&#35780;&#20272;&#37117;&#20250;&#36805;&#36895;&#25910;&#25947;&#65292;&#20174;&#19981;&#21040;100&#20010;&#31034;&#20363;&#20013;&#20250;&#20986;&#29616;&#23545;&#31995;&#32479;&#30340;&#26126;&#30830;&#20559;&#22909;&#12290;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#33258;&#21160;&#20998;&#25968;&#33021;&#22815;&#22914;&#20309;&#22797;&#21046;&#22312;&#21508;&#31181;&#19979;&#28216;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#20559;&#22909;&#25490;&#21517;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#33258;&#21160;&#25351;&#26631;&#22312;&#36739;&#23567;&#26679;&#26412;&#22823;&#23567;&#26102;&#31283;&#23450;&#65292;&#20294;&#21482;&#26377;&#19968;&#20123;&#33258;&#21160;&#25351;&#26631;&#33021;&#22815;&#36866;&#24230;&#39044;&#27979;&#27169;&#22411;&#30340;&#33719;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18756v1 Announce Type: new  Abstract: Modern instruction-tuned models have become highly capable in text generation tasks such as summarization, and are expected to be released at a steady pace. In practice one may now wish to choose confidently, but with minimal effort, the best performing summarization model when applied to a new domain or purpose. In this work, we empirically investigate the test sample size necessary to select a preferred model in the context of news summarization. Empirical results reveal that comparative evaluation converges quickly for both automatic and human evaluation, with clear preferences for a system emerging from under 100 examples. The human preference data allows us to quantify how well automatic scores can reproduce preference rankings across a variety of downstream summarization tasks. We find that, while automatic metrics are stable at smaller sample sizes, only some automatic metrics are able to moderately predict model win rates accordi
&lt;/p&gt;</description></item><item><title>&#32454;&#35843;&#30340;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#23545;&#20110;&#20381;&#36182;&#34920;&#38754;&#24418;&#24335;&#30340;&#24230;&#37327;&#21644;&#26410;&#32463;MT&#36136;&#37327;&#21028;&#26029;&#32454;&#35843;&#30340;&#39044;&#35757;&#32451;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.18747</link><description>&lt;p&gt;
&#32454;&#35843;&#30340;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#23384;&#22312;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18747
&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#30340;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#23545;&#20110;&#20381;&#36182;&#34920;&#38754;&#24418;&#24335;&#30340;&#24230;&#37327;&#21644;&#26410;&#32463;MT&#36136;&#37327;&#21028;&#26029;&#32454;&#35843;&#30340;&#39044;&#35757;&#32451;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#28085;&#30422;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;11&#31181;&#35821;&#35328;&#23545;&#30340;&#24191;&#27867;&#30340;&#22810;&#32500;&#36136;&#37327;&#24230;&#37327;(MQM)&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#26469;&#25506;&#31350;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#31227;&#26102;&#65292;&#26159;&#21542;&#37027;&#20123;&#26681;&#25454;&#20154;&#24037;&#29983;&#25104;&#30340;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#21028;&#26029;&#36827;&#34892;&#32454;&#35843;&#30340;MT&#24230;&#37327;&#26159;&#31283;&#20581;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26410;&#30693;&#39046;&#22495;&#30340;&#24773;&#20917;&#19979;&#65292;&#32454;&#35843;&#30340;&#24230;&#37327;&#30456;&#23545;&#20110;&#20381;&#36182;&#34920;&#38754;&#24418;&#24335;&#30340;&#24230;&#37327;&#20197;&#21450;&#26410;&#32463;MT&#36136;&#37327;&#21028;&#26029;&#32454;&#35843;&#30340;&#39044;&#35757;&#32451;&#24230;&#37327;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18747v1 Announce Type: cross  Abstract: We introduce a new, extensive multidimensional quality metrics (MQM) annotated dataset covering 11 language pairs in the biomedical domain. We use this dataset to investigate whether machine translation (MT) metrics which are fine-tuned on human-generated MT quality judgements are robust to domain shifts between training and inference. We find that fine-tuned metrics exhibit a substantial performance drop in the unseen domain scenario relative to metrics that rely on the surface form, as well as pre-trained metrics which are not fine-tuned on MT quality judgments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#20808;&#37319;&#26679;&#25216;&#26415;&#65292;&#33021;&#22815;&#25353;&#29031;&#27169;&#22411;&#20449;&#24515;&#24230;&#20135;&#29983;&#21807;&#19968;&#26679;&#26412;&#65292;&#22312;&#29983;&#25104;&#21644;&#20248;&#21270;&#20195;&#30721;&#26102;&#34920;&#29616;&#20248;&#20110;&#26680;&#37319;&#26679;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18734</link><description>&lt;p&gt;
&#32534;&#35793;&#22120;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#20808;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Priority Sampling of Large Language Models for Compilers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18734
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#20808;&#37319;&#26679;&#25216;&#26415;&#65292;&#33021;&#22815;&#25353;&#29031;&#27169;&#22411;&#20449;&#24515;&#24230;&#20135;&#29983;&#21807;&#19968;&#26679;&#26412;&#65292;&#22312;&#29983;&#25104;&#21644;&#20248;&#21270;&#20195;&#30721;&#26102;&#34920;&#29616;&#20248;&#20110;&#26680;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#21644;&#20248;&#21270;&#20195;&#30721;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#27604;&#22914;&#26680;&#37319;&#26679;&#65288;Nucleus Sampling&#65289;&#65292;&#22686;&#21152;&#20102;&#29983;&#25104;&#30340;&#22810;&#26679;&#24615;&#65292;&#20294;&#22312;&#20302;&#28201;&#24230;&#19979;&#32463;&#24120;&#20135;&#29983;&#37325;&#22797;&#30340;&#26679;&#26412;&#65292;&#22312;&#39640;&#28201;&#24230;&#19979;&#20135;&#29983;&#19981;&#36830;&#36143;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#28201;&#24230;&#31995;&#25968;&#24517;&#39035;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#36827;&#34892;&#35843;&#25972;&#65292;&#38480;&#21046;&#20102;&#20854;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#20808;&#37319;&#26679;&#65288;Priority Sampling&#65289;&#65292;&#19968;&#31181;&#31616;&#21333;&#19988;&#30830;&#23450;&#24615;&#30340;&#37319;&#26679;&#25216;&#26415;&#65292;&#23427;&#20135;&#29983;&#25353;&#27169;&#22411;&#32622;&#20449;&#24230;&#25490;&#24207;&#30340;&#21807;&#19968;&#26679;&#26412;&#12290;&#27599;&#20010;&#26032;&#26679;&#26412;&#37117;&#20250;&#25193;&#23637;&#25193;&#23637;&#25628;&#32034;&#26641;&#20013;&#27010;&#29575;&#26368;&#39640;&#30340;&#26410;&#25193;&#23637;&#20196;&#29260;&#12290;&#27492;&#22806;&#65292;&#20248;&#20808;&#37319;&#26679;&#25903;&#25345;&#22522;&#20110;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#29983;&#25104;&#65292;&#25552;&#20379;&#21487;&#25511;&#21644;&#32467;&#26500;&#21270;&#30340;&#25506;&#32034;&#36807;&#31243;&#12290;&#20248;&#20808;&#37319;&#26679;&#22312;&#20219;&#24847;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#22343;&#20248;&#20110;&#26680;&#37319;&#26679;&#65292;&#23558;&#21407;&#22987;&#27169;&#22411;&#30340;&#24615;&#33021;&#20174;2.87%&#25552;&#21319;&#33267;5%&#36229;&#36807;-Oz&#12290;&#27492;&#22806;&#65292;&#23427;&#36229;&#36807;&#20102;&#33258;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18734v1 Announce Type: cross  Abstract: Large language models show great potential in generating and optimizing code. Widely used sampling methods such as Nucleus Sampling increase the diversity of generation but often produce repeated samples for low temperatures and incoherent samples for high temperatures. Furthermore, the temperature coefficient has to be tuned for each task, limiting its usability. We present Priority Sampling, a simple and deterministic sampling technique that produces unique samples ordered by the model's confidence. Each new sample expands the unexpanded token with the highest probability in the augmented search tree. Additionally, Priority Sampling supports generation based on regular expression that provides a controllable and structured exploration process. Priority Sampling outperforms Nucleus Sampling for any number of samples, boosting the performance of the original model from 2.87% to 5% improvement over -Oz. Moreover, it outperforms the auto
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18700</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning to Compress Prompt in Natural Language Formats
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25797;&#38271;&#22788;&#29702;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#33021;&#21147;&#21463;&#21040;&#38271;&#19978;&#19979;&#25991;&#12289;&#25512;&#29702;&#36895;&#24230;&#24930;&#20197;&#21450;&#35745;&#31639;&#32467;&#26524;&#25104;&#26412;&#39640;&#30340;&#38480;&#21046;&#12290;&#37096;&#32626;&#20855;&#26377;&#31934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#19978;&#19979;&#25991;&#30340;LLMs&#26377;&#21161;&#20110;&#29992;&#25143;&#26356;&#26377;&#25928;&#21644;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#22320;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#20316;&#21697;&#20381;&#36182;&#23558;&#38271;&#25552;&#31034;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#36719;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#36719;&#25552;&#31034;&#21387;&#32553;&#22312;&#19981;&#21516;LLM&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;API&#30340;LLMs&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20197;LLM&#21487;&#36716;&#31227;&#24615;&#30340;&#24418;&#24335;&#21387;&#32553;&#38271;&#25552;&#31034;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#12290;&#36825;&#24102;&#26469;&#20004;&#20010;&#25361;&#25112;&#65306;(i) &#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#25552;&#31034;&#19981;&#20860;&#23481;&#21453;&#21521;&#20256;&#25773;&#65292;(ii) NL&#25552;&#31034;&#22312;&#26045;&#21152;&#38271;&#24230;&#32422;&#26463;&#26041;&#38754;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18700v1 Announce Type: cross  Abstract: Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results. Deploying LLMs with precise and informative context helps users process large-scale datasets more effectively and cost-efficiently. Existing works rely on compressing long prompt contexts into soft prompts. However, soft prompt compression encounters limitations in transferability across different LLMs, especially API-based LLMs. To this end, this work aims to compress lengthy prompts in the form of natural language with LLM transferability. This poses two challenges: (i) Natural Language (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack flexibility in imposing length constraints. In this work, we propose a Natural Language Prompt Encapsulation (Nano-Capsulator) framewor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;AutoVER&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#35273;&#23454;&#20307;&#35782;&#21035;&#20013;&#24212;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;&#32422;&#26463;&#29983;&#25104;&#65292;&#25104;&#21151;&#21306;&#20998;&#24040;&#22823;&#26631;&#31614;&#31354;&#38388;&#20013;&#30456;&#20284;&#30340;&#23454;&#20307;&#65292;&#24182;&#22312;Oven-Wiki&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.18695</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#22312;&#35270;&#35273;&#23454;&#20307;&#35782;&#21035;&#19978;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Models for Visual Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18695
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;AutoVER&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#35273;&#23454;&#20307;&#35782;&#21035;&#20013;&#24212;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;&#32422;&#26463;&#29983;&#25104;&#65292;&#25104;&#21151;&#21306;&#20998;&#24040;&#22823;&#26631;&#31614;&#31354;&#38388;&#20013;&#30456;&#20284;&#30340;&#23454;&#20307;&#65292;&#24182;&#22312;Oven-Wiki&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;AutoVER&#65292;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#23454;&#20307;&#35782;&#21035;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#32422;&#26463;&#29983;&#25104;&#65292;&#25193;&#23637;&#20102;&#33258;&#22238;&#24402;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#22788;&#29702;&#36328;&#39046;&#22495;&#23454;&#20307;&#26102;&#20943;&#36731;&#20102;&#20302;&#24615;&#33021;&#65292;&#22312;&#38656;&#35201;&#35270;&#35273;&#25512;&#29702;&#30340;&#26597;&#35810;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#30828;&#36127;&#23545;&#19978;&#36827;&#34892;&#23545;&#27604;&#35757;&#32451;&#65292;&#24182;&#22312;&#24207;&#21015;-&#24207;&#21015;&#30446;&#26631;&#20013;&#24182;&#34892;&#36827;&#34892;&#35757;&#32451;&#65292;&#23398;&#20064;&#22312;&#24040;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#20013;&#21306;&#20998;&#30456;&#20284;&#30340;&#23454;&#20307;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#19968;&#31995;&#21015;&#26816;&#32034;&#30340;&#20505;&#36873;&#31572;&#26696;&#26126;&#30830;&#25351;&#23548;&#35821;&#35328;&#29983;&#25104;&#65292;&#36890;&#36807;&#28040;&#38500;&#26080;&#25928;&#30340;&#35299;&#30721;&#36335;&#24452;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;Oven-Wiki&#22522;&#20934;&#27979;&#35797;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#25286;&#20998;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#24050;&#30693;&#23454;&#20307;&#25286;&#20998;&#19978;&#30340;&#20934;&#30830;&#29575;&#20174;32.7%&#25552;&#39640;&#21040;61.5%&#12290;&#35813;&#26041;&#27861;&#36824;&#36890;&#36807;&#22823;&#24133;&#24230;&#25552;&#21319;&#22312;&#26410;&#30693;&#21644;&#26597;&#35810;&#25286;&#20998;&#19978;&#30340;&#24615;&#33021;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18695v1 Announce Type: cross  Abstract: We introduce AutoVER, an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive Multi-modal Large Language Model by employing retrieval augmented constrained generation. It mitigates low performance on out-of-domain entities while excelling in queries that require visually-situated reasoning. Our method learns to distinguish similar entities within a vast label space by contrastively training on hard negative pairs in parallel with a sequence-to-sequence objective without an external retriever. During inference, a list of retrieved candidate answers explicitly guides language generation by removing invalid decoding paths. The proposed method achieves significant improvements across different dataset splits in the recently proposed Oven-Wiki benchmark. Accuracy on the Entity seen split rises from 32.7% to 61.5%. It also demonstrates superior performance on the unseen and query splits by a substantial dou
&lt;/p&gt;</description></item><item><title>RORA &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#23545;&#26631;&#31614;&#30340;&#26032;&#20449;&#24687;&#36129;&#29486;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18678</link><description>&lt;p&gt;
RORA&#65306;&#24378;&#22823;&#30340;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RORA: Robust Free-Text Rationale Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18678
&lt;/p&gt;
&lt;p&gt;
RORA &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#23545;&#26631;&#31614;&#30340;&#26032;&#20449;&#24687;&#36129;&#29486;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#22312;&#21487;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24357;&#21512;&#20102;&#27169;&#22411;&#20915;&#31574;&#32972;&#21518;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28508;&#22312;&#25512;&#29702;&#36335;&#24452;&#30340;&#22810;&#26679;&#24615;&#21450;&#30456;&#24212;&#32570;&#20047;&#26126;&#30830;&#30340;&#30495;&#30456;&#22522;&#30784;&#65292;&#20854;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#20381;&#36182;&#20110;&#29702;&#30001;&#25903;&#25345;&#30446;&#26631;&#26631;&#31614;&#30340;&#31243;&#24230;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25351;&#26631;&#22312;&#35780;&#20272;&#24847;&#22806;&#27844;&#28431;&#26631;&#31614;&#30340;&#29702;&#30001;&#26102;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RORA&#65292;&#19968;&#31181;&#38024;&#23545;&#26631;&#31614;&#27844;&#28431;&#30340;&#24378;&#22823;&#30340;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#35780;&#20272;&#26041;&#27861;&#12290;RORA&#37327;&#21270;&#20102;&#29702;&#30001;&#20026;&#35777;&#26126;&#26631;&#31614;&#25552;&#20379;&#30340;&#26032;&#20449;&#24687;&#12290;&#36825;&#26159;&#36890;&#36807;&#35780;&#20272;&#19982;&#27844;&#28431;&#29305;&#24449;&#25239;&#24178;&#25200;&#30340;&#39044;&#27979;&#24615;&#23478;&#26063;&#26465;&#20214;V&#20449;&#24687;&#23454;&#29616;&#30340;&#12290;RORA&#22312;&#35780;&#20272;&#20154;&#24037;&#25776;&#20889;&#12289;&#21512;&#25104;&#25110;&#27169;&#22411;&#29983;&#25104;&#30340;&#29702;&#30001;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18678v1 Announce Type: new  Abstract: Free-text rationales play a pivotal role in explainable NLP, bridging the knowledge and reasoning gaps behind a model's decision-making. However, due to the diversity of potential reasoning paths and a corresponding lack of definitive ground truth, their evaluation remains a challenge. Existing evaluation metrics rely on the degree to which a rationale supports a target label, but we find these fall short in evaluating rationales that inadvertently leak the labels. To address this problem, we propose RORA, a Robust free-text Rationale evaluation against label leakage. RORA quantifies the new information supplied by a rationale to justify the label. This is achieved by assessing the conditional V-information \citep{hewitt-etal-2021-conditional} with a predictive family robust against leaky features that can be exploited by a small model. RORA consistently outperforms existing approaches in evaluating human-written, synthetic, or model-gen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#24179;&#34913;&#21484;&#22238;&#21644;&#20869;&#23384;&#28040;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.18668</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#35821;&#35328;&#27169;&#22411;&#24179;&#34913;&#20102;&#21484;&#22238;-&#21534;&#21520;&#37327;&#30340;&#25240;&#34935;
&lt;/p&gt;
&lt;p&gt;
Simple linear attention language models balance the recall-throughput tradeoff
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18668
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#24179;&#34913;&#21484;&#22238;&#21644;&#20869;&#23384;&#28040;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#21484;&#22238;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#20013;&#24050;&#32463;&#30475;&#21040;&#30340;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#30340;&#25928;&#29575;&#21463;&#21040;KV-cache&#30340;&#20869;&#23384;&#28040;&#32791;&#30340;&#29942;&#39048;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#65288;&#20363;&#22914;&#36890;&#36807;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#65289;&#32780;&#19981;&#24433;&#21709;&#21484;&#22238;&#12290;&#36890;&#36807;&#23558;&#23454;&#39564;&#21644;&#29702;&#35770;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#26550;&#26500;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#27169;&#22411;&#29366;&#24577;&#22823;&#23567;&#21644;&#21484;&#22238;&#33021;&#21147;&#20043;&#38388;&#30340;&#19968;&#20010;&#20851;&#38190;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#27880;&#24847;&#21147;&#30340;&#39640;&#25928;&#26367;&#20195;&#26041;&#27861;&#65288;&#20363;&#22914;H3&#12289;Mamba&#12289;RWKV&#65289;&#20445;&#25345;&#22266;&#23450;&#22823;&#23567;&#30340;&#24490;&#29615;&#29366;&#24577;&#65292;&#20294;&#22312;&#21484;&#22238;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BASED&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#32447;&#24615;&#21644;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#30340;&#31616;&#21333;&#26550;&#26500;&#12290;&#36890;&#36807;&#25913;&#21464;BASED&#31383;&#21475;&#22823;&#23567;&#21644;&#32447;&#24615;&#27880;&#24847;&#21147;&#29305;&#24449;&#32500;&#24230;&#65292;&#25105;&#20204;&#21487;&#20197;&#35843;&#25972;&#29366;&#24577;&#22823;&#23567;&#65292;&#24182;&#36941;&#21382;&#21484;&#22238;-&#20869;&#23384;&#25240;&#34935;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18668v1 Announce Type: new  Abstract: Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff cu
&lt;/p&gt;</description></item><item><title>FOFO&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36861;&#38543;&#22797;&#26434;&#12289;&#39046;&#22495;&#29305;&#23450;&#26684;&#24335;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#26684;&#24335;&#36861;&#38543;&#33021;&#21147;&#26041;&#38754;&#30340;&#34920;&#29616;&#21644;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;</title><link>https://arxiv.org/abs/2402.18667</link><description>&lt;p&gt;
FOFO&#65306;&#29992;&#20110;&#35780;&#20272;LLMs&#26684;&#24335;&#36861;&#38543;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18667
&lt;/p&gt;
&lt;p&gt;
FOFO&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36861;&#38543;&#22797;&#26434;&#12289;&#39046;&#22495;&#29305;&#23450;&#26684;&#24335;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#26684;&#24335;&#36861;&#38543;&#33021;&#21147;&#26041;&#38754;&#30340;&#34920;&#29616;&#21644;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FoFo&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36861;&#38543;&#22797;&#26434;&#39046;&#22495;&#29305;&#23450;&#26684;&#24335;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#23427;&#20204;&#20316;&#20026;AI&#20195;&#29702;&#30340;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#20294;&#26410;&#32463;&#20805;&#20998;&#32771;&#34385;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;LLMs&#26377;&#20102;&#36827;&#23637;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26410;&#33021;&#20805;&#20998;&#35780;&#20272;&#23427;&#20204;&#30340;&#26684;&#24335;&#36861;&#38543;&#33021;&#21147;&#12290;FoFo&#36890;&#36807;AI-&#20154;&#31867;&#21327;&#20316;&#26041;&#27861;&#24320;&#21457;&#20102;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#26684;&#24335;&#21644;&#25351;&#20196;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#36328;&#36234;&#24320;&#28304;&#27169;&#22411;&#65288;&#20363;&#22914;Llama 2&#65292;WizardLM&#65289;&#21644;&#38381;&#28304;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-4&#65292;PALM2&#65292;Gemini&#65289;&#65292;&#31361;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;&#24320;&#28304;&#27169;&#22411;&#22312;&#26684;&#24335;&#36981;&#24490;&#26041;&#38754;&#26126;&#26174;&#33853;&#21518;&#20110;&#38381;&#28304;&#27169;&#22411;&#65307;LLMs&#30340;&#26684;&#24335;&#36861;&#38543;&#34920;&#29616;&#29420;&#31435;&#20110;&#23427;&#20204;&#30340;&#20869;&#23481;&#29983;&#25104;&#36136;&#37327;&#65307;LLMs&#30340;&#26684;&#24335;&#29087;&#32451;&#24230;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#21464;&#21270;&#12290;&#36825;&#20123;&#35265;&#35299;&#34920;&#26126;&#38656;&#35201;&#19987;&#38376;&#35843;&#25972;&#26684;&#24335;&#36861;&#38543;&#25216;&#33021;&#65292;&#24182;&#31361;&#20986;&#20102;FoFo&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18667v1 Announce Type: new  Abstract: This paper presents FoFo, a pioneering benchmark for evaluating large language models' (LLMs) ability to follow complex, domain-specific formats, a crucial yet underexamined capability for their application as AI agents. Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs' format-following performance is independent of their content generation quality; and LLMs' format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo's ro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18659</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#28216;&#25103;&#65306;&#35843;&#30740;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Games: A Survey and Roadmap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#24613;&#21095;&#22686;&#21152;&#65292;&#24182;&#20276;&#38543;&#30528;&#20844;&#20247;&#23545;&#35813;&#20027;&#39064;&#30340;&#21442;&#19982;&#12290;&#23613;&#31649;&#36215;&#21021;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;LLMs&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#21253;&#25324;&#28216;&#25103;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21450;&#20026;&#28216;&#25103;&#25552;&#20379;&#25903;&#25345;&#30340;&#21508;&#31181;&#24212;&#29992;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#26126;&#30830;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21487;&#20197;&#25198;&#28436;&#30340;&#19981;&#21516;&#35282;&#33394;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23578;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#21644;LLMs&#22312;&#28216;&#25103;&#20013;&#26410;&#26469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;LLMs&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#20316;&#20026;LLMs&#21644;&#28216;&#25103;&#20132;&#21449;&#39046;&#22495;&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#21644;&#36335;&#32447;&#22270;&#65292;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#33021;&#22815;&#25104;&#20026;&#36825;&#19968;&#28608;&#21160;&#20154;&#24515;&#30340;&#26032;&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#21644;&#21019;&#26032;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18659v1 Announce Type: cross  Abstract: Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18603</link><description>&lt;p&gt;
MMSR&#65306;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MMSR: Symbolic Regression is a Multimodal Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18603
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#26159;&#25506;&#32034;&#33258;&#28982;&#35268;&#24459;&#20960;&#21315;&#24180;&#26469;&#20154;&#31867;&#26234;&#24935;&#30340;&#32467;&#26230;&#12290;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#20844;&#24335;&#25551;&#36848;&#22797;&#26434;&#30340;&#33258;&#28982;&#35268;&#24459;&#26159;&#31185;&#23398;&#23478;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#39046;&#22495;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#20174;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
&lt;/p&gt;</description></item><item><title>Verif.ai&#26159;&#19968;&#20010;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#12289;&#29983;&#25104;&#27169;&#22411;&#21644;&#39564;&#35777;&#24341;&#25806;&#30340;&#32467;&#21512;&#23454;&#29616;&#23545;&#20027;&#24352;&#30340;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.18589</link><description>&lt;p&gt;
Verif.ai: &#19968;&#31181;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Verif.ai: Towards an Open-Source Scientific Generative Question-Answering System with Referenced and Verifiable Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18589
&lt;/p&gt;
&lt;p&gt;
Verif.ai&#26159;&#19968;&#20010;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#12289;&#29983;&#25104;&#27169;&#22411;&#21644;&#39564;&#35777;&#24341;&#25806;&#30340;&#32467;&#21512;&#23454;&#29616;&#23545;&#20027;&#24352;&#30340;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39033;&#30446;Verif.ai&#30340;&#24403;&#21069;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#30340;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#65288;1&#65289;&#19968;&#20010;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#32467;&#21512;&#35821;&#20041;&#21644;&#35789;&#27719;&#25628;&#32034;&#25216;&#26415;&#23545;&#31185;&#23398;&#35770;&#25991;&#65288;PubMed&#65289;&#36827;&#34892;&#26816;&#32034;&#65292;&#65288;2&#65289;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;Mistral 7B&#65289;&#65292;&#33719;&#21462;&#21069;&#20960;&#20010;&#31572;&#26696;&#24182;&#29983;&#25104;&#38468;&#26377;&#20174;&#20013;&#24471;&#20986;&#20027;&#24352;&#30340;&#35770;&#25991;&#24341;&#29992;&#30340;&#31572;&#26696;&#65292;&#20197;&#21450;&#65288;3&#65289;&#19968;&#20010;&#39564;&#35777;&#24341;&#25806;&#65292;&#29992;&#20110;&#20132;&#21449;&#26816;&#26597;&#29983;&#25104;&#30340;&#20027;&#24352;&#21644;&#20174;&#20013;&#24471;&#20986;&#20027;&#24352;&#30340;&#25688;&#35201;&#25110;&#35770;&#25991;&#65292;&#39564;&#35777;&#29983;&#25104;&#20027;&#24352;&#26102;&#26159;&#21542;&#23384;&#22312;&#20219;&#20309;&#38169;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19978;&#19979;&#25991;&#20013;&#30340;&#25688;&#35201;&#21152;&#24378;&#20102;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#27492;&#22806;&#65292;&#19968;&#20010;&#29420;&#31435;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#38598;&#27491;&#22312;&#39564;&#35777;&#31572;&#26696;&#24182;&#26816;&#26597;&#26159;&#21542;&#23384;&#22312;&#38169;&#35273;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30456;&#20449;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#31185;&#23398;&#23478;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18589v1 Announce Type: cross  Abstract: In this paper, we present the current progress of the project Verif.ai, an open-source scientific generative question-answering system with referenced and verified answers. The components of the system are (1) an information retrieval system combining semantic and lexical search techniques over scientific papers (PubMed), (2) a fine-tuned generative model (Mistral 7B) taking top answers and generating answers with references to the papers from which the claim was derived, and (3) a verification engine that cross-checks the generated claim and the abstract or paper from which the claim was derived, verifying whether there may have been any hallucinations in generating the claim. We are reinforcing the generative model by providing the abstract in context, but in addition, an independent set of methods and models are verifying the answer and checking for hallucinations. Therefore, we believe that by using our method, we can make scientis
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18571</link><description>&lt;p&gt;
&#29992;&#20110;&#28385;&#36275;&#22810;&#26679;&#29992;&#25143;&#20559;&#22909;&#30340;&#31639;&#26415;&#25511;&#21046;LLMs&#65306;&#20855;&#26377;&#22810;&#30446;&#26631;&#22870;&#21169;&#30340;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31934;&#32454;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#36866;&#24212;&#21508;&#31181;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#24314;&#27169;&#26469;&#34920;&#31034;&#22810;&#26679;&#21270;&#30340;&#20559;&#22909;&#37197;&#32622;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20026;&#22870;&#21169;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65288;&#21363;&#21333;&#20301;&#21521;&#37327;&#65289;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18571v1 Announce Type: cross  Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.18510</link><description>&lt;p&gt;
RNNs&#36824;&#19981;&#26159;Transformer&#65306;&#22312;&#19978;&#19979;&#25991;&#26816;&#32034;&#20013;&#30340;&#20851;&#38190;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#21644;Transformer&#22312;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#31034;&#33021;&#21147;&#24046;&#36317;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;RNNs&#26159;&#21542;&#33021;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#65292;&#36890;&#36807;Chain-of-Thought (CoT)&#25552;&#31034;&#65292;&#19982;Transformer&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#26174;&#31034;CoT&#21487;&#20197;&#25913;&#36827;RNNs&#65292;&#20294;&#26080;&#27861;&#24357;&#34917;&#19982;Transformer&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20851;&#38190;&#29942;&#39048;&#22312;&#20110;RNNs&#26080;&#27861;&#23436;&#20840;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#21363;&#20351;&#32463;&#36807;CoT&#30340;&#22686;&#24378;&#65306;&#23545;&#20110;&#20960;&#20010;&#26126;&#30830;&#25110;&#38544;&#24335;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#65292;&#22914;&#32852;&#24819;&#21484;&#22238;&#21644;&#30830;&#23450;&#22270;&#26159;&#21542;&#20026;&#26641;&#65292;&#25105;&#20204;&#35777;&#26126;RNNs&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#20197;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#65292;&#32780;Transformer&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#37319;&#29992;&#22686;&#24378;RNNs&#19978;&#19979;&#25991;&#26816;&#32034;&#33021;&#21147;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18510v1 Announce Type: cross  Abstract: This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, inclu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#26512;&#35821;&#35328;&#27169;&#22411;&#20013;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#34920;&#36848;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;&#65292;&#36825;&#23545;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#22312;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18496</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#34920;&#36798;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;
&lt;/p&gt;
&lt;p&gt;
Language Models Represent Beliefs of Self and Others
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18496
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#26512;&#35821;&#35328;&#27169;&#22411;&#20013;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#34920;&#36848;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;&#65292;&#36825;&#23545;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#22312;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#24402;&#22240;&#24515;&#29702;&#29366;&#24577;&#65292;&#21363;&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#65292;&#34987;&#35270;&#20026;&#20154;&#31867;&#31038;&#20250;&#25512;&#29702;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20284;&#20046;&#20855;&#26377;&#26576;&#20123;ToM&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#20196;&#20154;&#36153;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#30721;&#21508;&#20010;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#26159;&#21487;&#33021;&#30340;&#65292;&#36825;&#34920;&#26126;&#23384;&#22312;&#33258;&#25105;&#30340;&#20869;&#37096;&#34920;&#36848;&#21644;&#20182;&#20154;&#20449;&#24565;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#25805;&#32437;&#36825;&#20123;&#34920;&#24449;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;ToM&#24615;&#33021;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#24310;&#20280;&#21040;&#28041;&#21450;&#19981;&#21516;&#22240;&#26524;&#25512;&#29702;&#27169;&#24335;&#30340;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#65292;&#26263;&#31034;&#20102;&#36825;&#20123;&#34920;&#24449;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18496v1 Announce Type: new  Abstract: Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.18409</link><description>&lt;p&gt;
&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22270;&#20687;&#25512;&#29702;&#21644;&#25551;&#36848;&#30340;&#35748;&#30693;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18409
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(LVLMs)&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#21463;&#21040;&#20840;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#27979;&#35797;&#12290;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#27979;&#35797;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#8220;&#20599;&#39292;&#24178;&#8221;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21033;&#29992;&#20855;&#26377;&#20016;&#23500;&#35821;&#20041;&#30340;&#22270;&#20687;&#35780;&#20272;LVLMs&#30340;&#39640;&#32423;&#35748;&#30693;&#33021;&#21147;&#12290;&#23427;&#23450;&#20041;&#20102;&#20843;&#31181;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21253;&#25324;&#22270;&#20687;&#25551;&#36848;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;&#30693;&#21517;LVLMs&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;LVLMs&#21644;&#20154;&#31867;&#20043;&#38388;&#20173;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18409v1 Announce Type: new  Abstract: Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the "Cookie Theft" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#37197;&#22120;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#25506;&#32034;&#65292;&#21457;&#29616;&#23558;&#36866;&#37197;&#22120;&#25554;&#20837;&#27973;&#23618;&#21487;&#20197;&#33719;&#24471;&#26356;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#30495;&#23454;&#25968;&#25454;&#27604;&#27169;&#25311;&#25968;&#25454;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;ASR&#31995;&#32479;&#20013;&#21487;&#20197;&#24102;&#26469;&#23454;&#36136;&#24615;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.18275</link><description>&lt;p&gt;
&#25506;&#32034;&#36866;&#37197;&#22120;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Exploration of Adapter for Noise Robust Automatic Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#37197;&#22120;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#25506;&#32034;&#65292;&#21457;&#29616;&#23558;&#36866;&#37197;&#22120;&#25554;&#20837;&#27973;&#23618;&#21487;&#20197;&#33719;&#24471;&#26356;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#30495;&#23454;&#25968;&#25454;&#27604;&#27169;&#25311;&#25968;&#25454;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;ASR&#31995;&#32479;&#20013;&#21487;&#20197;&#24102;&#26469;&#23454;&#36136;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#40065;&#26834;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20197;&#35299;&#20915;&#26410;&#30693;&#22122;&#22768;&#22330;&#26223;&#33267;&#20851;&#37325;&#35201;&#12290;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#22122;&#22768;&#40065;&#26834;ASR&#36866;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;CHiME--4&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27973;&#23618;&#25554;&#20837;&#36866;&#37197;&#22120;&#33021;&#22815;&#20135;&#29983;&#26356;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#22312;&#20165;&#22312;&#27973;&#23618;&#20869;&#37096;&#36827;&#34892;&#36866;&#24212;&#21644;&#22312;&#25152;&#26377;&#23618;&#20043;&#38388;&#36827;&#34892;&#36866;&#24212;&#20043;&#38388;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#27169;&#25311;&#25968;&#25454;&#26377;&#21161;&#20110;&#31995;&#32479;&#25913;&#21892;&#20854;&#22312;&#23454;&#38469;&#22122;&#22768;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#37327;&#30456;&#21516;&#26102;&#65292;&#30495;&#23454;&#25968;&#25454;&#27604;&#27169;&#25311;&#25968;&#25454;&#26356;&#26377;&#25928;&#12290;&#22312;&#36866;&#37197;&#22120;&#35757;&#32451;&#20013;&#65292;&#22810;&#26465;&#20214;&#35757;&#32451;&#20173;&#28982;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;ASR&#31995;&#32479;&#20013;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18275v1 Announce Type: cross  Abstract: Adapting a robust automatic speech recognition (ASR) system to tackle unseen noise scenarios is crucial. Integrating adapters into neural networks has emerged as a potent technique for transfer learning. This paper thoroughly investigates adapter-based noise-robust ASR adaptation. We conducted the experiments using the CHiME--4 dataset. The results show that inserting the adapter in the shallow layer yields superior effectiveness, and there is no significant difference between adapting solely within the shallow layer and adapting across all layers. Besides, the simulated data helps the system to improve its performance under real noise conditions. Nonetheless, when the amount of data is the same, the real data is more effective than the simulated data. Multi-condition training remains valid for adapter training. Furthermore, integrating adapters into speech enhancement-based ASR systems yields substantial improvements.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;MIKO&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#21644;&#25991;&#26412;&#30340;&#21327;&#21516;&#20316;&#29992;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#24847;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.18169</link><description>&lt;p&gt;
MIKO&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20132;&#23186;&#20307;&#24120;&#35782;&#21457;&#29616;&#30340;&#22810;&#27169;&#24577;&#24847;&#22270;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18169
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;MIKO&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#21644;&#25991;&#26412;&#30340;&#21327;&#21516;&#20316;&#29992;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24050;&#32463;&#25104;&#20026;&#19982;&#20182;&#20154;&#32852;&#31995;&#12289;&#20102;&#35299;&#26032;&#38395;&#12289;&#34920;&#36798;&#35266;&#28857;&#20197;&#21450;&#25214;&#21040;&#23089;&#20048;&#30340;&#26080;&#22788;&#19981;&#22312;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#24847;&#22270;&#30340;&#38544;&#21547;&#24615;&#12289;&#38656;&#35201;&#36328;&#27169;&#24577;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#20687;&#12289;&#20197;&#21450;&#23384;&#22312;&#26631;&#31614;&#12289;&#25340;&#20889;&#38169;&#35823;&#21644;&#22797;&#26434;&#32553;&#20889;&#31561;&#22024;&#26434;&#20449;&#24687;&#65292;&#29702;&#35299;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#32972;&#21518;&#30340;&#24847;&#22270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIKO&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#24847;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#23427;&#20849;&#21516;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26469;&#25581;&#31034;&#29992;&#25143;&#30340;&#24847;&#22270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;MLLM&#26469;&#35299;&#37322;&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;LLM&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#26368;&#21518;&#20877;&#27425;&#25351;&#23548;LLM&#29983;&#25104;&#24847;&#22270;&#12290;&#36890;&#36807;&#23558;MIKO&#24212;&#29992;&#20110;&#20844;&#24320;&#21487;&#29992;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24847;&#22270;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18169v1 Announce Type: new  Abstract: Social media has become a ubiquitous tool for connecting with others, staying updated with news, expressing opinions, and finding entertainment. However, understanding the intention behind social media posts remains challenging due to the implicitness of intentions in social media posts, the need for cross-modality understanding of both text and images, and the presence of noisy information such as hashtags, misspelled words, and complicated abbreviations. To address these challenges, we present MIKO, a Multimodal Intention Kowledge DistillatiOn framework that collaboratively leverages a Large Language Model (LLM) and a Multimodal Large Language Model (MLLM) to uncover users' intentions. Specifically, we use an MLLM to interpret the image and an LLM to extract key information from the text and finally instruct the LLM again to generate intentions. By applying MIKO to publicly available social media datasets, we construct an intention kno
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#19968;&#31181;&#26085;&#26412;&#32972;&#26223;&#19979;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#20013;&#26368;&#20808;&#36827;&#35821;&#27861;&#38169;&#35823;&#26816;&#27979;&#21644;&#26657;&#27491;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22312;&#38169;&#35823;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#23432;&#24615;&#65292;&#22312;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#30830;&#24230;&#21644;&#35843;&#25972;&#21484;&#22238;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18101</link><description>&lt;p&gt;
&#35780;&#20272;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#30340;&#26377;&#25928;&#24615;&#65306;&#22312;&#26085;&#26412;&#32972;&#26223;&#19979;&#30340;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Assessing the Efficacy of Grammar Error Correction: A Human Evaluation Approach in the Japanese Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18101
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#19968;&#31181;&#26085;&#26412;&#32972;&#26223;&#19979;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#20013;&#26368;&#20808;&#36827;&#35821;&#27861;&#38169;&#35823;&#26816;&#27979;&#21644;&#26657;&#27491;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22312;&#38169;&#35823;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#23432;&#24615;&#65292;&#22312;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#30830;&#24230;&#21644;&#35843;&#25972;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#27880;&#37322;&#24037;&#20855;&#21253;ERRANT&#65292;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#26631;&#35760;&#35821;&#27861;&#38169;&#35823;&#26816;&#27979;&#21644;&#26657;&#27491;&#27169;&#22411;&#65288;SeqTagger&#65289;&#22312;&#26085;&#26412;&#22823;&#23398;&#29983;&#20889;&#20316;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;SeqTagger&#30340;&#24615;&#33021;&#19982;&#20154;&#31867;&#19987;&#23478;&#26657;&#27491;&#20316;&#20026;&#22522;&#20934;&#26469;&#35780;&#20272;&#38169;&#35823;&#26657;&#27491;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#37319;&#29992;&#20154;&#24037;&#26631;&#27880;&#26041;&#27861;&#26469;&#35780;&#20272;Seqtagger&#22312;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#20889;&#20316;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#38169;&#35823;&#26657;&#27491;&#30340;&#31934;&#30830;&#24230;&#20026;63.66%&#65292;&#21484;&#22238;&#29575;&#20026;20.19%&#12290;&#23545;&#20110;&#23376;&#38598;&#65292;&#22312;&#25163;&#21160;&#25490;&#38500;&#20102;&#35821;&#20041;&#21644;&#26426;&#26800;&#38169;&#35823;&#31561;&#19981;&#30456;&#20851;&#38169;&#35823;&#21518;&#65292;&#27169;&#22411;&#22312;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#26174;&#31034;&#20986;97.98%&#30340;&#35843;&#25972;&#31934;&#30830;&#24230;&#21644;42.98%&#30340;&#35843;&#25972;&#21484;&#22238;&#29575;&#65292;&#34920;&#26126;&#27169;&#22411;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#34920;&#29616;&#20986;&#20445;&#23432;&#24615;&#12290;&#23545;&#27169;&#22411;&#26410;&#26816;&#27979;&#21040;&#30340;&#38169;&#35823;&#36827;&#34892;&#30340;&#20027;&#39064;&#20998;&#26512;&#25581;&#31034;&#20102;&#38480;&#23450;&#35789;&#21644;&#20896;&#35789;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18101v1 Announce Type: new  Abstract: In this study, we evaluated the performance of the state-of-the-art sequence tagging grammar error detection and correction model (SeqTagger) using Japanese university students' writing samples. With an automatic annotation toolkit, ERRANT, we first evaluated SeqTagger's performance on error correction with human expert correction as the benchmark. Then a human-annotated approach was adopted to evaluate Seqtagger's performance in error detection using a subset of the writing dataset. Results indicated a precision of 63.66% and a recall of 20.19% for error correction in the full dataset. For the subset, after manual exclusion of irrelevant errors such as semantic and mechanical ones, the model shows an adjusted precision of 97.98% and an adjusted recall of 42.98% for error detection, indicating the model's high accuracy but also its conservativeness. Thematic analysis on errors undetected by the model revealed that determiners and article
&lt;/p&gt;</description></item><item><title>&#22312;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#38469;&#20020;&#24202;&#26696;&#20363;&#19978;&#30340;&#34920;&#29616;&#26159;&#20851;&#38190;&#65292;&#22240;&#27492;&#26500;&#24314;&#20102;&#20004;&#20010;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.18060</link><description>&lt;p&gt;
&#22312;&#22238;&#31572;&#21644;&#35299;&#37322;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21307;&#23398;&#38382;&#39064;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18060
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#38469;&#20020;&#24202;&#26696;&#20363;&#19978;&#30340;&#34920;&#29616;&#26159;&#20851;&#38190;&#65292;&#22240;&#27492;&#26500;&#24314;&#20102;&#20004;&#20010;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;&#36890;&#36807;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20381;&#36182;&#20110;&#22996;&#21592;&#20250;&#32771;&#35797;&#38382;&#39064;&#25110;&#19968;&#33324;&#21307;&#23398;&#38382;&#39064;&#65292;&#26080;&#27861;&#25429;&#25417;&#30495;&#23454;&#20020;&#24202;&#26696;&#20363;&#30340;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#31572;&#26696;&#30340;&#21442;&#32771;&#35299;&#37322;&#38459;&#30861;&#20102;&#23545;&#27169;&#22411;&#35299;&#37322;&#30340;&#35780;&#20272;&#65292;&#36825;&#23545;&#25903;&#25345;&#21307;&#29983;&#20570;&#20986;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;JAMA&#20020;&#24202;&#25361;&#25112;&#21644;Medbullets&#12290;JAMA&#20020;&#24202;&#25361;&#25112;&#21253;&#21547;&#22522;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20020;&#24202;&#26696;&#20363;&#30340;&#38382;&#39064;&#65292;&#32780;Medbullets&#21253;&#21547;&#31867;&#20284;USMLE Step 2&amp;3&#39118;&#26684;&#30340;&#20020;&#24202;&#38382;&#39064;&#12290;&#20004;&#20010;&#25968;&#25454;&#38598;&#22343;&#20197;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;-&#22238;&#31572;&#20219;&#21153;&#30340;&#32467;&#26500;&#21270;&#24418;&#24335;&#21576;&#29616;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#38468;&#26377;&#19987;&#23478;&#25776;&#20889;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#22312;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#22235;&#20010;LLMs&#12290;&#23454;&#39564;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18060v1 Announce Type: new  Abstract: LLMs have demonstrated impressive performance in answering medical questions, such as passing medical licensing examinations. However, most existing benchmarks rely on board exam questions or general medical questions, falling short in capturing the complexity of realistic clinical cases. Moreover, the lack of reference explanations for answers hampers the evaluation of model explanations, which are crucial to supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises USMLE Step 2&amp;3 style clinical questions. Both datasets are structured as multiple-choice question-answering tasks, where each question is accompanied by an expert-written explanation. We evaluate four LLMs on the two datasets using various prompts. Experiments demonstrat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25903;&#25345;&#25991;&#26723;&#20013;&#36873;&#25321;&#19978;&#19979;&#25991;&#24863;&#30693;&#30701;&#35821;&#30340;&#26032;&#39062;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24335;&#33258;&#25105;&#24378;&#21270;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#20934;&#30830;&#24615;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#25552;&#39640;&#20102;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.17532</link><description>&lt;p&gt;
&#26816;&#32034;&#21363;&#31934;&#20934;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Retrieval is Accurate Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17532
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25903;&#25345;&#25991;&#26723;&#20013;&#36873;&#25321;&#19978;&#19979;&#25991;&#24863;&#30693;&#30701;&#35821;&#30340;&#26032;&#39062;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24335;&#33258;&#25105;&#24378;&#21270;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#20934;&#30830;&#24615;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#25552;&#39640;&#20102;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20174;&#22266;&#23450;&#30340;&#12289;&#26377;&#38480;&#30340;&#21644;&#29420;&#31435;&#30340;&#35789;&#27719;&#20013;&#36873;&#25321;&#26631;&#35760;&#26469;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20174;&#19968;&#32452;&#25903;&#25345;&#25991;&#26723;&#20013;&#36873;&#25321;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#35821;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#20013;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#30830;&#23450;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#20026;&#25991;&#26412;&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#20998;&#21106;&#65292;&#24182;&#19988;&#27599;&#20010;&#29255;&#27573;&#37117;&#21487;&#20197;&#20174;&#22810;&#20010;&#21487;&#33021;&#30340;&#25991;&#26723;&#20013;&#26816;&#32034;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35821;&#35328;&#21551;&#21457;&#24335;&#21021;&#22987;&#21270;&#35757;&#32451;&#25968;&#25454;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#36890;&#36807;&#36845;&#20195;&#24335;&#33258;&#25105;&#24378;&#21270;&#26469;&#24341;&#23548;&#35757;&#32451;&#25968;&#25454;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#20248;&#20110;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#19988;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#20013;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#20363;&#22914;&#65292;&#19982;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#23545;&#24212;&#30340;&#27169;&#22411;&#65292;&#22312;&#24320;&#25918;&#24615;&#20219;&#21153;&#19978;&#23558;&#20934;&#30830;&#29575;&#20174;23.47%&#25552;&#39640;&#21040;36.27%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17532v1 Announce Type: new  Abstract: Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on Open
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#26469;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;&#65292;&#23558;&#29983;&#25104;&#30340;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#29992;&#20110;&#25581;&#31034;&#21407;&#22987;&#25552;&#31034;&#30340;&#23454;&#38469;&#24847;&#22270;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16459</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending LLMs against Jailbreaking Attacks via Backtranslation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16459
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#26469;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;&#65292;&#23558;&#29983;&#25104;&#30340;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#29992;&#20110;&#25581;&#31034;&#21407;&#22987;&#25552;&#31034;&#30340;&#23454;&#38469;&#24847;&#22270;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#34987;&#35757;&#32451;&#25104;&#25298;&#32477;&#26377;&#23475;&#35831;&#27714;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#20250;&#37325;&#20889;&#21407;&#22987;&#25552;&#31034;&#20197;&#38544;&#34255;&#20854;&#26377;&#23475;&#24847;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#8220;&#21453;&#21521;&#32763;&#35793;&#8221;&#26469;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#30446;&#26631;LLM&#20174;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#30340;&#21021;&#22987;&#21709;&#24212;&#65292;&#25105;&#20204;&#30340;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#26029;&#21487;&#20197;&#23548;&#33268;&#35813;&#21709;&#24212;&#30340;&#36755;&#20837;&#25552;&#31034;&#12290;&#25512;&#26029;&#30340;&#25552;&#31034;&#31216;&#20026;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#65292;&#20542;&#21521;&#20110;&#25581;&#31034;&#21407;&#22987;&#25552;&#31034;&#30340;&#23454;&#38469;&#24847;&#22270;&#65292;&#22240;&#20026;&#23427;&#26159;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#30340;&#65292;&#19981;&#26159;&#30452;&#25509;&#30001;&#25915;&#20987;&#32773;&#25805;&#32437;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20877;&#27425;&#22312;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#19978;&#36816;&#34892;&#30446;&#26631;LLM&#65292;&#22914;&#26524;&#27169;&#22411;&#25298;&#32477;&#20102;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#65292;&#21017;&#25298;&#32477;&#21407;&#22987;&#25552;&#31034;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#25514;&#26045;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#20960;&#20010;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16459v1 Announce Type: cross  Abstract: Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent. In this paper, we propose a new method for defending LLMs against jailbreaking attacks by ``backtranslation''. Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response. The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM's response and is not directly manipulated by the attacker. We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt. We explain that the proposed defense provides several benefits on its effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.16363</link><description>&lt;p&gt;
LLM&#25512;&#26029;&#25581;&#31034;&#65306;&#35843;&#26597;&#19982;Roofline&#27169;&#22411;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
LLM Inference Unveiled: Survey and Roofline Model Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#26029;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#25552;&#20379;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#29420;&#29305;&#32467;&#21512;&#12290;&#34429;&#28982;&#35813;&#39046;&#22495;&#24050;&#32463;&#25193;&#23637;&#24182;&#20805;&#28385;&#27963;&#21147;&#65292;&#20294;&#33267;&#20170;&#36824;&#27809;&#26377;&#19968;&#20010;&#31616;&#26126;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;LLM&#25512;&#26029;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#28165;&#26224;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#19981;&#20165;&#24635;&#32467;&#20102;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#36824;&#22522;&#20110;Roofline&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#12290;&#36825;&#19968;&#26694;&#26550;&#33021;&#22815;&#24110;&#21161;&#35782;&#21035;LLM&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#26041;&#38754;&#65292;&#20174;&#32780;&#20026;&#37096;&#32626;LLM&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27719;&#24635;&#20102;&#39640;&#25928;LLM&#25512;&#26029;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20851;&#38190;&#39046;&#22495;&#65292;&#27604;&#22914;&#26435;&#37325;&#20248;&#21270;&#65288;&#22914;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16363v1 Announce Type: cross  Abstract: The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM. Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantizatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEMANTICSMOOTH&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#35821;&#20041;&#36716;&#25442;&#21103;&#26412;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36973;&#36935;GCG&#12289;PAIR&#21644;AutoDAN&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36739;&#24378;&#30340;&#27491;&#24120;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16192</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#24179;&#28369;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36973;&#36935;&#30417;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16192
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEMANTICSMOOTH&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#35821;&#20041;&#36716;&#25442;&#21103;&#26412;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36973;&#36935;GCG&#12289;PAIR&#21644;AutoDAN&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36739;&#24378;&#30340;&#27491;&#24120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23481;&#26131;&#21463;&#21040;&#30417;&#29425;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#32469;&#36807;&#30446;&#26631;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#24182;&#39575;&#36807;&#23427;&#20204;&#29983;&#25104;&#20196;&#20154;&#21453;&#24863;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SEMANTICSMOOTH&#65292;&#19968;&#31181;&#22522;&#20110;&#24179;&#28369;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#32463;&#36807;&#35821;&#20041;&#36716;&#25442;&#30340;&#32473;&#23450;&#36755;&#20837;&#25552;&#31034;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#26469;&#25552;&#39640;&#23545;GCG&#12289;PAIR&#21644;AutoDAN&#25915;&#20987;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SEMANTICSMOOTH&#22312;&#20445;&#25345;&#25351;&#23548;&#24615;&#22522;&#20934;&#27979;&#35797;&#65288;&#22914;InstructionFollowing&#21644;AlpacaEval&#65289;&#19978;&#30340;&#24378;&#21170;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#23545;&#21508;&#31181;&#25915;&#20987;&#30340;&#26368;&#26032;&#25216;&#26415;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16192v1 Announce Type: new  Abstract: Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content. While initial defenses show promise against token-based threat models, there do not exist defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance. To meet this need, we propose SEMANTICSMOOTH, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input prompt. Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval. The codes will be publicly available at https://github.com/UCSB-NLP-Chang/SemanticSmooth.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#31181;&#32676;&#24847;&#35782;&#20248;&#21270;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#20043;&#38388;&#24494;&#22937;&#30340;&#20998;&#24067;&#24046;&#24322;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16041</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#31181;&#32676;&#24847;&#35782;&#20248;&#21270;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;
&lt;/p&gt;
&lt;p&gt;
Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16041
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#31181;&#32676;&#24847;&#35782;&#20248;&#21270;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#20043;&#38388;&#24494;&#22937;&#30340;&#20998;&#24067;&#24046;&#24322;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22312;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21487;&#33021;&#23384;&#22312;&#20005;&#37325;&#39118;&#38505;&#65292;&#22914;&#25220;&#34989;&#38382;&#39064;&#12289;&#35823;&#23548;&#24615;&#20449;&#24687;&#25110;&#24187;&#35273;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26159;&#38750;&#24120;&#32039;&#36843;&#21644;&#37325;&#35201;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;LLMs&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#21306;&#20998;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#24120;&#24120;&#38750;&#24120;&#24494;&#22937;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#21033;&#29992;\textit{&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;}&#65288;MMD&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;MMD&#21487;&#20197;&#24456;&#22909;&#22320;&#35782;&#21035;&#20998;&#24067;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20351;&#29992;&#21508;&#31181;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#23545;MMD&#36827;&#34892;&#35757;&#32451;&#23558;&#23548;&#33268;MMD&#30340;&#26041;&#24046;&#26174;&#33879;&#22686;&#21152;&#65292;&#22240;&#20026;&#19981;&#21516;LLMs&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21487;&#33021;&#21253;&#21547;\textit{&#22810;&#20010;&#25991;&#26412;&#32676;&#20307;}&#12290;&#36825;&#23558;&#20005;&#37325;&#25439;&#23475;MMD&#27979;&#37327;&#20998;&#24067;&#24046;&#24322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16041v1 Announce Type: new  Abstract: Large language models (LLMs) such as ChatGPT have exhibited remarkable performance in generating human-like texts. However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues, misleading information, or hallucination issues. Therefore, it is very urgent and important to detect MGTs in many situations. Unfortunately, it is challenging to distinguish MGTs and human-written texts because the distributional discrepancy between them is often very subtle due to the remarkable performance of LLMs. In this paper, we seek to exploit \textit{maximum mean discrepancy} (MMD) to address this issue in the sense that MMD can well identify distributional discrepancies. However, directly training a detector with MMD using diverse MGTs will incur a significantly increased variance of MMD since MGTs may contain \textit{multiple text populations} due to various LLMs. This will severely impair MMD's ability to measure the diff
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#23558;&#35848;&#21028;&#20219;&#21153;&#24418;&#24335;&#21270;&#25551;&#36848;&#20026;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#19981;&#23545;&#31216;&#28216;&#25103;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25198;&#28436;&#20080;&#26041;&#30340;&#38590;&#24230;&#22823;&#19988;&#27169;&#22411;&#22823;&#23567;&#19981;&#33021;&#26377;&#25928;&#25552;&#39640;&#20080;&#26041;&#34920;&#29616;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OG-Narrator&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15813</link><description>&lt;p&gt;
&#35780;&#20272;LLMs&#30340;&#35848;&#21028;&#33021;&#21147;&#65306;&#19968;&#20010;&#22522;&#20934;&#21644;&#19968;&#20010;&#20080;&#26041;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15813
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#23558;&#35848;&#21028;&#20219;&#21153;&#24418;&#24335;&#21270;&#25551;&#36848;&#20026;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#19981;&#23545;&#31216;&#28216;&#25103;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25198;&#28436;&#20080;&#26041;&#30340;&#38590;&#24230;&#22823;&#19988;&#27169;&#22411;&#22823;&#23567;&#19981;&#33021;&#26377;&#25928;&#25552;&#39640;&#20080;&#26041;&#34920;&#29616;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OG-Narrator&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35848;&#21028;&#26159;&#20154;&#31867;&#20043;&#38388;&#35848;&#21028;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#29420;&#29305;&#30340;&#37096;&#20998;&#12290;&#38543;&#30528;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#23398;&#20064;&#35848;&#21028;&#24182;&#34920;&#29616;&#24471;&#20687;&#30495;&#27491;&#30340;&#20154;&#31867;&#19968;&#26679;&#65292;&#22914;&#20309;&#35780;&#20272;&#20195;&#29702;&#30340;&#35848;&#21028;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#31532;&#19968;&#27425;&#23558;&#35848;&#21028;&#20219;&#21153;&#24418;&#24335;&#21270;&#25551;&#36848;&#20026;&#19968;&#31181;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#19981;&#23545;&#31216;&#28216;&#25103;&#65292;&#23450;&#20041;&#20102;&#20080;&#26041;&#21644;&#21334;&#26041;&#22312;&#22810;&#27425;&#35848;&#21028;&#36807;&#31243;&#20013;&#30340;&#25910;&#30410;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23450;&#37327;&#35780;&#20272;&#19968;&#20010;&#20195;&#29702;&#22312;&#35848;&#21028;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#30495;&#23454;&#20135;&#21697;&#20215;&#26684;&#25968;&#25454;&#38598;AmazonHistoryPrice&#65292;&#24182;&#23545;&#21508;&#31181;LLM&#20195;&#29702;&#30340;&#35848;&#21028;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#25198;&#28436;&#20080;&#26041;&#27604;&#25198;&#28436;&#21334;&#26041;&#35201;&#22256;&#38590;&#24471;&#22810;&#65292;&#24182;&#19988;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#26080;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20080;&#26041;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;OG-Narrator&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#19968;&#20010;&#30830;&#23450;&#24615;&#30340;&#25253;&#20215;&#29983;&#25104;&#22120;&#26469;&#25511;&#21046;&#20080;&#26041;&#25253;&#20215;&#30340;&#20215;&#26684;&#33539;&#22260;&#65292;&#24182;&#19988;&#38598;&#25104;&#20102;&#19968;&#20010;LLM&#35299;&#35828;&#32773;&#26469;&#21019;&#24314;&#19968;&#31181;&#33258;&#28982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15813v1 Announce Type: new  Abstract: Bargaining is an important and unique part of negotiation between humans. As LLM-driven agents learn to negotiate and act like real humans, how to evaluate agents' bargaining abilities remains an open problem. For the first time, we formally described the Bargaining task as an asymmetric incomplete information game, defining the gains of the Buyer and Seller in multiple bargaining processes. It allows us to quantitatively assess an agent's performance in the Bargain task. We collected a real product price dataset, AmazonHistoryPrice, and conducted evaluations of various LLM agents' bargaining abilities. We find that playing a Buyer is much harder than a Seller, and increasing model size can not effectively improve the Buyer's performance. To address the challenge, we propose a novel approach called OG-Narrator that integrates a deterministic Offer Generator to control the price range of Buyer's offers, and an LLM Narrator to create natur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#30340;RelayAttention&#31639;&#27861;&#26088;&#22312;&#25913;&#21892;&#28041;&#21450;&#38271;&#31995;&#32479;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#19968;&#27425;&#24615;&#20174;DRAM&#35835;&#21462;&#38544;&#34255;&#29366;&#24577;&#26469;&#28040;&#38500;&#29616;&#26377;&#22240;&#26524;&#27880;&#24847;&#21147;&#31639;&#27861;&#20013;&#30340;&#20869;&#23384;&#35775;&#38382;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2402.14808</link><description>&lt;p&gt;
RelayAttention&#65306;&#29992;&#20110;&#39640;&#25928;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#19982;&#38271;&#31995;&#32479;&#25552;&#31034;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
RelayAttention for Efficient Large Language Model Serving with Long System Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#30340;RelayAttention&#31639;&#27861;&#26088;&#22312;&#25913;&#21892;&#28041;&#21450;&#38271;&#31995;&#32479;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#19968;&#27425;&#24615;&#20174;DRAM&#35835;&#21462;&#38544;&#34255;&#29366;&#24577;&#26469;&#28040;&#38500;&#29616;&#26377;&#22240;&#26524;&#27880;&#24847;&#21147;&#31639;&#27861;&#20013;&#30340;&#20869;&#23384;&#35775;&#38382;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#21487;&#33021;&#28041;&#21450;&#19968;&#20010;&#38271;&#30340;&#31995;&#32479;&#25552;&#31034;&#65292;&#20854;&#20013;&#21253;&#21547;&#20219;&#21153;&#30340;&#25351;&#31034;&#12289;&#31034;&#20363;&#21644;&#30693;&#35782;&#25991;&#26723;&#65292;&#24182;&#22312;&#35768;&#22810;&#35831;&#27714;&#20013;&#22797;&#29992;&#12290;&#28982;&#32780;&#65292;&#38271;&#31995;&#32479;&#25552;&#31034;&#20250;&#23548;&#33268;&#21534;&#21520;&#37327;/&#24310;&#36831;&#29942;&#39048;&#65292;&#22240;&#20026;&#29983;&#25104;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#25104;&#26412;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#38271;&#32780;&#22686;&#21152;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#28041;&#21450;&#38271;&#31995;&#32479;&#25552;&#31034;&#30340;LLM&#26381;&#21153;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#22788;&#29702;&#36825;&#20123;&#31995;&#32479;&#25552;&#31034;&#22312;&#29616;&#26377;&#22240;&#26524;&#27880;&#24847;&#21147;&#35745;&#31639;&#31639;&#27861;&#20013;&#38656;&#35201;&#22823;&#37327;&#20887;&#20313;&#30340;&#20869;&#23384;&#35775;&#38382;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;&#25209;&#37327;&#35831;&#27714;&#65292;&#31995;&#32479;&#25552;&#31034;&#30340;&#32531;&#23384;&#38544;&#34255;&#29366;&#24577;&#65288;&#21363;&#38190;-&#20540;&#23545;&#65289;&#34987;&#22810;&#27425;&#20174;&#33455;&#29255;&#22806;&#30340;DRAM&#20256;&#36755;&#21040;&#33455;&#29255;&#19978;&#30340;SRAM&#65292;&#27599;&#27425;&#23545;&#24212;&#19968;&#20010;&#21333;&#29420;&#30340;&#35831;&#27714;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#31181;&#20887;&#20313;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RelayAttention&#65292;&#19968;&#31181;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#20801;&#35768;&#20165;&#20174;DRAM&#35835;&#21462;&#36825;&#20123;&#38544;&#34255;&#29366;&#24577;&#19968;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14808v1 Announce Type: new  Abstract: Practical large language model (LLM) services may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across numerous requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (i.e., key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#12298;Tokenization and the Noiseless Channel&#12299;&#25552;&#20986;&#30340;&#20351;&#29992;R&#233;nyi&#25928;&#29575;&#20316;&#20026;&#20998;&#35789;&#22120;&#35780;&#20272;&#26426;&#21046;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25551;&#36848;&#20102;&#20004;&#20010;BPE&#20998;&#35789;&#30340;&#21453;&#20363;&#65292;&#23637;&#31034;&#20102;R&#233;nyi&#25928;&#29575;&#26080;&#27861;&#25429;&#25417;&#21040;&#25152;&#26377;&#20248;&#31168;&#20998;&#35789;&#26041;&#26696;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.14614</link><description>&lt;p&gt;
&#12298;Tokenization and the Noiseless Channel&#12299;&#30340;&#20004;&#20010;&#21453;&#20363;
&lt;/p&gt;
&lt;p&gt;
Two Counterexamples to \textit{Tokenization and the Noiseless Channel}
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14614
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#12298;Tokenization and the Noiseless Channel&#12299;&#25552;&#20986;&#30340;&#20351;&#29992;R&#233;nyi&#25928;&#29575;&#20316;&#20026;&#20998;&#35789;&#22120;&#35780;&#20272;&#26426;&#21046;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25551;&#36848;&#20102;&#20004;&#20010;BPE&#20998;&#35789;&#30340;&#21453;&#20363;&#65292;&#23637;&#31034;&#20102;R&#233;nyi&#25928;&#29575;&#26080;&#27861;&#25429;&#25417;&#21040;&#25152;&#26377;&#20248;&#31168;&#20998;&#35789;&#26041;&#26696;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#12298;Tokenization and the Noiseless Channel&#12299;&#20013;&#65292;&#24314;&#35758;&#20351;&#29992;R&#233;nyi&#25928;&#29575;&#20316;&#20026;&#35780;&#20272;&#20998;&#35789;&#22120;&#30340;&#22266;&#26377;&#26426;&#21046;: &#23545;&#20110;NLP&#20219;&#21153;&#65292;&#24212;&#36873;&#25321;&#23548;&#33268;unigram&#20998;&#24067;R&#233;nyi&#25928;&#29575;&#26368;&#39640;&#30340;&#20998;&#35789;&#22120;&#12290;&#22240;&#27492;&#65292;R&#233;nyi&#25928;&#29575;&#34987;&#35270;&#20026;&#19979;&#28216;&#24615;&#33021;&#30340;&#39044;&#27979;&#22120;&#65288;&#20363;&#22914;&#65292;&#29992;&#20110;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#30340;BLEU&#65289;&#65292;&#32780;&#26080;&#38656;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#20998;&#35789;&#22120;&#30340;&#22810;&#20010;&#27169;&#22411;&#36825;&#19968;&#26114;&#36149;&#30340;&#27493;&#39588;&#12290;&#23613;&#31649;&#26377;&#29992;&#65292;&#20294;&#36825;&#19968;&#24230;&#37327;&#26631;&#20934;&#30340;&#39044;&#27979;&#33021;&#21147;&#24182;&#19981;&#23436;&#32654;&#65292;&#20316;&#32773;&#25351;&#20986;&#26377;&#20854;&#20182;&#20248;&#31168;&#20998;&#35789;&#26041;&#26696;&#30340;&#38468;&#21152;&#29305;&#36136;R&#233;nyi&#25928;&#29575;&#26412;&#36523;&#26080;&#27861;&#25429;&#25417;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20004;&#31181;BPE&#20998;&#35789;&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#22312;&#22686;&#21152;R&#233;nyi&#25928;&#29575;&#30340;&#21516;&#26102;&#38477;&#20302;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#21453;&#20363;&#25581;&#31034;&#20102;R&#233;nyi&#25928;&#29575;&#20316;&#20026;&#22266;&#26377;&#20998;&#35789;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#22833;&#36133;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14614v1 Announce Type: new  Abstract: In \textit{Tokenization and the Noiseless Channel} \cite{zouhar-etal-2023-tokenization}, R\'enyi efficiency is suggested as an intrinsic mechanism for evaluating a tokenizer: for NLP tasks, the tokenizer which leads to the highest R\'enyi efficiency of the unigram distribution should be chosen. The R\'enyi efficiency is thus treated as a predictor of downstream performance (e.g., predicting BLEU for a machine translation task), without the expensive step of training multiple models with different tokenizers. Although useful, the predictive power of this metric is not perfect, and the authors note there are additional qualities of a good tokenization scheme that R\'enyi efficiency alone cannot capture.   We describe two variants of BPE tokenization which can arbitrarily increase R\'enyi efficiency while decreasing the downstream model performance. These counterexamples expose cases where R\'enyi efficiency fails as an intrinsic tokenizati
&lt;/p&gt;</description></item><item><title>Sequoia&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#31283;&#20581;&#19988;&#30828;&#20214;&#24863;&#30693;&#30340;&#25512;&#27979;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#20248;&#21270;&#26631;&#35760;&#26641;&#32467;&#26500;&#12289;&#37319;&#29992;&#26032;&#39062;&#30340;&#37319;&#26679;&#21644;&#39564;&#35777;&#26041;&#27861;&#23454;&#29616;&#31283;&#20581;&#24615;&#33021;&#20197;&#21450;&#30828;&#20214;&#24863;&#30693;&#30340;&#26641;&#20248;&#21270;&#22120;&#26368;&#22823;&#21270;&#25512;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12374</link><description>&lt;p&gt;
Sequoia: &#21487;&#25193;&#23637;&#12289;&#31283;&#20581;&#19988;&#30828;&#20214;&#24863;&#30693;&#30340;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12374
&lt;/p&gt;
&lt;p&gt;
Sequoia&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#31283;&#20581;&#19988;&#30828;&#20214;&#24863;&#30693;&#30340;&#25512;&#27979;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#20248;&#21270;&#26631;&#35760;&#26641;&#32467;&#26500;&#12289;&#37319;&#29992;&#26032;&#39062;&#30340;&#37319;&#26679;&#21644;&#39564;&#35777;&#26041;&#27861;&#23454;&#29616;&#31283;&#20581;&#24615;&#33021;&#20197;&#21450;&#30828;&#20214;&#24863;&#30693;&#30340;&#26641;&#20248;&#21270;&#22120;&#26368;&#22823;&#21270;&#25512;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#22686;&#22810;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#25512;&#29702;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#34429;&#28982;&#26368;&#36817;&#25512;&#27979;&#35299;&#30721;&#24050;&#32463;&#25104;&#20026;&#21152;&#36895;&#25512;&#29702;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#25193;&#23637;&#21040;&#36739;&#22823;&#30340;&#25512;&#27979;&#39044;&#31639;&#12289;&#36866;&#24212;&#19981;&#21516;&#36229;&#21442;&#25968;&#21644;&#30828;&#20214;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Sequoia&#65292;&#19968;&#20010;&#21487;&#25193;&#23637;&#12289;&#31283;&#20581;&#19988;&#30828;&#20214;&#24863;&#30693;&#30340;&#29992;&#20110;&#25512;&#27979;&#35299;&#30721;&#30340;&#31639;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;Sequoia&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26469;&#25214;&#21040;&#29992;&#20110;&#34987;&#25512;&#27979;&#26631;&#35760;&#30340;&#26368;&#20339;&#26641;&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#31283;&#20581;&#30340;&#25512;&#27979;&#24615;&#33021;&#65292;Sequoia&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37319;&#26679;&#21644;&#39564;&#35777;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#35299;&#30721;&#28201;&#24230;&#19979;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;Sequoia&#24341;&#20837;&#20102;&#19968;&#31181;&#30828;&#20214;&#24863;&#30693;&#30340;&#26641;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#32473;&#23450;&#24773;&#20917;&#19979;&#30340;&#26631;&#35760;&#26641;&#22823;&#23567;&#21644;&#28145;&#24230;&#26469;&#26368;&#22823;&#21270;&#25512;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12374v1 Announce Type: new  Abstract: As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a giv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;LLMs&#22312;&#37329;&#34701;&#34920;&#26684;&#38382;&#31572;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#25552;&#31034;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.11194</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#25991;&#26723;&#38382;&#31572;&#20013;&#35780;&#20272;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Assessing LLMs' Mathematical Reasoning in Financial Document Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11194
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;LLMs&#22312;&#37329;&#34701;&#34920;&#26684;&#38382;&#31572;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#25552;&#31034;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#20855;&#26377;&#32467;&#26500;&#21270;&#34920;&#26684;&#21644;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28151;&#21512;&#30340;&#22797;&#26434;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#19981;&#30830;&#23450;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#22235;&#20010;&#37329;&#34701;&#34920;&#26684;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65306;TATQA&#12289;FinQA&#12289;ConvFinQA&#21644;Multihiertt&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#27169;&#22411;&#21644;&#25552;&#31034;&#25216;&#26415;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22914;&#20309;&#36866;&#24212;&#22797;&#26434;&#34920;&#26684;&#21644;&#25968;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#20851;&#27880;&#23545;&#34920;&#26684;&#22797;&#26434;&#24615;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#22312;&#22686;&#21152;&#31639;&#26415;&#25512;&#29702;&#27493;&#39588;&#25968;&#37327;&#26102;&#24615;&#33021;&#21464;&#21270;&#12290;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#22788;&#29702;&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#20013;&#22797;&#26434;&#25968;&#23398;&#22330;&#26223;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#30340;&#26032;&#22411;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#24615;&#33021;&#26041;&#38754;&#19982;&#20854;&#20182;&#22522;&#32447;&#30456;&#21305;&#37197;&#25110;&#32988;&#36807;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;LLMs&#33021;&#21147;&#30340;&#24494;&#22937;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11194v1 Announce Type: new  Abstract: Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain. This study explores LLMs' mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique tailored to semi-structured documents, matching or outperforming other baselines in performance while providing a nuanced understanding 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#19982;&#24847;&#22806;&#20943;&#23569;&#26368;&#23567;&#21270;&#21407;&#21017;&#22312;&#21517;&#35789;&#30701;&#35821;&#20013;&#30340;&#20914;&#31361;&#65292;&#32467;&#35770;&#26174;&#31034;&#24403;&#28041;&#21450;&#30340;&#21333;&#35789;&#36739;&#23569;&#19988;&#21333;&#35789;&#36739;&#30701;&#26102;&#65292;&#24847;&#22806;&#20943;&#23569;&#21487;&#33021;&#20250;&#36229;&#36234;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10311</link><description>&lt;p&gt;
&#21517;&#35789;&#30701;&#35821;&#20013;&#22836;&#37096;&#30340;&#26368;&#20339;&#20301;&#32622;&#12290;&#25351;&#31034;&#35821;&#12289;&#25968;&#35789;&#12289;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#30340;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#19982;&#24847;&#22806;&#20943;&#23569;&#26368;&#23567;&#21270;&#21407;&#21017;&#22312;&#21517;&#35789;&#30701;&#35821;&#20013;&#30340;&#20914;&#31361;&#65292;&#32467;&#35770;&#26174;&#31034;&#24403;&#28041;&#21450;&#30340;&#21333;&#35789;&#36739;&#23569;&#19988;&#21333;&#35789;&#36739;&#30701;&#26102;&#65292;&#24847;&#22806;&#20943;&#23569;&#21487;&#33021;&#20250;&#36229;&#36234;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#21477;&#35805;&#30340;&#35789;&#24207;&#30001;&#22810;&#31181;&#21407;&#21017;&#22609;&#36896;&#12290;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#21407;&#21017;&#19982;&#24847;&#22806;&#20943;&#23569;&#26368;&#23567;&#21270;&#21407;&#21017;&#65288;&#25110;&#21487;&#39044;&#27979;&#24615;&#26368;&#22823;&#21270;&#65289;&#22312;&#21333;&#19968;&#22836;&#37096;&#30340;&#21477;&#27861;&#20381;&#36182;&#32467;&#26500;&#20013;&#23384;&#22312;&#20914;&#31361;&#65306;&#21069;&#32773;&#39044;&#27979;&#22836;&#37096;&#24212;&#35813;&#25918;&#32622;&#22312;&#32447;&#24615;&#25490;&#21015;&#30340;&#20013;&#24515;&#65292;&#21518;&#32773;&#39044;&#27979;&#22836;&#37096;&#24212;&#35813;&#25918;&#32622;&#22312;&#20004;&#31471;&#20043;&#19968;&#65288;&#35201;&#20040;&#22312;&#39318;&#20301;&#65292;&#35201;&#20040;&#22312;&#26411;&#20301;&#65289;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#20309;&#26102;&#24847;&#22806;&#20943;&#23569;&#65288;&#25110;&#21487;&#39044;&#27979;&#24615;&#26368;&#22823;&#21270;&#65289;&#24212;&#35813;&#36229;&#36234;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#12290;&#22312;&#21333;&#19968;&#22836;&#37096;&#32467;&#26500;&#30340;&#32972;&#26223;&#19979;&#65292;&#39044;&#27979;&#22312;&#28385;&#36275;&#20004;&#20010;&#26465;&#20214;&#26102;&#26356;&#26377;&#21487;&#33021;&#21457;&#29983;&#65292;&#21363;&#65288;a&#65289;&#28041;&#21450;&#30340;&#21333;&#35789;&#36739;&#23569;&#65292;&#24182;&#19988;&#65288;b&#65289;&#21333;&#35789;&#36739;&#30701;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#30001;&#25351;&#31034;&#35821;&#12289;&#25968;&#35789;&#12289;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#32452;&#25104;&#30340;&#21517;&#35789;&#30701;&#35821;&#19978;&#27979;&#35797;&#20102;&#36825;&#19968;&#39044;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#39318;&#36873;&#39034;&#24207;&#20013;...&#65288;&#32570;&#22833;&#37096;&#20998;&#26080;&#27861;&#25552;&#20379;&#23436;&#25972;&#32763;&#35793;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10311v1 Announce Type: new  Abstract: The word order of a sentence is shaped by multiple principles. The principle of syntactic dependency distance minimization is in conflict with the principle of surprisal minimization (or predictability maximization) in single head syntactic dependency structures: while the former predicts that the head should be placed at the center of the linear arrangement, the latter predicts that the head should be placed at one of the ends (either first or last). A critical question is when surprisal minimization (or predictability maximization) should surpass syntactic dependency distance minimization. In the context of single head structures, it has been predicted that this is more likely to happen when two conditions are met, i.e. (a) fewer words are involved and (b) words are shorter. Here we test the prediction on the noun phrase when its composed of a demonstrative, a numeral, an adjective and a noun. We find that, across preferred orders in l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#30340;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#65288;CHA&#65289;&#29992;&#20110;&#31958;&#23615;&#30149;&#24739;&#32773;&#65292;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#31958;&#23615;&#30149;&#31649;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10153</link><description>&lt;p&gt;
&#30693;&#35782;&#27880;&#20837;&#30340;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#65306;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#30340;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#65288;CHA&#65289;&#29992;&#20110;&#31958;&#23615;&#30149;&#24739;&#32773;&#65292;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#31958;&#23615;&#30149;&#31649;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#31958;&#23615;&#30149;&#31649;&#29702;&#23545;&#20110;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#31958;&#23615;&#30149;&#31649;&#29702;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#21463;&#38480;&#20110;&#23545;&#19968;&#33324;&#26469;&#28304;&#30340;&#20381;&#36182;&#65292;&#32570;&#20047;&#19982;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#25972;&#21512;&#65292;&#23548;&#33268;&#22238;&#22797;&#19981;&#20934;&#30830;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#30340;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#65288;CHA&#65289;&#29992;&#20110;&#31958;&#23615;&#30149;&#24739;&#32773;&#12290;&#25105;&#20204;&#26681;&#25454;&#24320;&#28304;&#30340;openCHA&#26694;&#26550;&#36827;&#34892;&#23450;&#21046;&#65292;&#24182;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;CHA&#30340;&#22806;&#37096;&#30693;&#35782;&#21644;&#20998;&#26512;&#33021;&#21147;&#12290;&#36825;&#31181;&#25972;&#21512;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;1&#65289;&#25972;&#21512;&#32654;&#22269;&#31958;&#23615;&#30149;&#21327;&#20250;&#30340;&#33203;&#39135;&#25351;&#21335;&#21644;Nutritionix&#30340;&#20449;&#24687;&#65307;2&#65289;&#37096;&#32626;&#20998;&#26512;&#24037;&#20855;&#65292;&#23454;&#29616;&#33829;&#20859;&#25668;&#20837;&#35745;&#31639;&#24182;&#19982;&#25351;&#21335;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;CHA&#19982;GPT4&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324;100&#20010;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10153v1 Announce Type: new  Abstract: Effective diabetes management is crucial for maintaining health in diabetic patients. Large Language Models (LLMs) have opened new avenues for diabetes management, facilitating their efficacy. However, current LLM-based approaches are limited by their dependence on general sources and lack of integration with domain-specific knowledge, leading to inaccurate responses. In this paper, we propose a knowledge-infused LLM-powered conversational health agent (CHA) for diabetic patients. We customize and leverage the open-source openCHA framework, enhancing our CHA with external knowledge and analytical capabilities. This integration involves two key components: 1) incorporating the American Diabetes Association dietary guidelines and the Nutritionix information and 2) deploying analytical tools that enable nutritional intake calculation and comparison with the guidelines. We compare the proposed CHA with GPT4. Our evaluation includes 100 diabe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#26469;&#33258;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#12290;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#21487;&#20197;&#22312;&#22238;&#31572;&#26597;&#35810;&#21069;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#65292;&#24182;&#36890;&#36807;MATRIX-simulated&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#20445;&#25345;&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#36981;&#20174;&#21644;&#25512;&#29702;&#36895;&#24230;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;</title><link>https://arxiv.org/abs/2402.05699</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#22404;&#26029;&#23545;&#35805;&#30340;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#26469;&#33258;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#12290;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#21487;&#20197;&#22312;&#22238;&#31572;&#26597;&#35810;&#21069;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#65292;&#24182;&#36890;&#36807;MATRIX-simulated&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#20445;&#25345;&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#36981;&#20174;&#21644;&#25512;&#29702;&#36895;&#24230;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#65292;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#20511;&#37492;&#31038;&#20250;&#23398;&#30340;&#35265;&#35299;&#65292;&#21363;&#35748;&#35782;&#21040;&#25152;&#26377;&#21508;&#26041;&#30340;&#20851;&#20999;&#26159;&#22609;&#36896;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23545;&#40784;LLMs&#30340;&#26032;&#26041;&#21521;&#65306;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#21019;&#26032;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#22120;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#36755;&#20837;&#26597;&#35810;&#21608;&#22260;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#20351;LLM&#22312;&#22238;&#31572;&#21069;&#33021;&#22815;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#12290;MATRIX&#31867;&#20284;&#20110;&#19968;&#20010;&#8220;&#22404;&#26029;&#23545;&#35805;&#8221;&#19979;&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#22312;&#20854;&#20013;&#25198;&#28436;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#22810;&#20010;&#35282;&#33394;&#24182;&#36827;&#34892;&#33258;&#25105;&#23454;&#36341;&#12290;&#20026;&#20102;&#24341;&#20837;&#36825;&#31181;&#23545;&#40784;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;MATRIX&#27169;&#25311;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#30830;&#20445;&#20854;&#22312;&#19981;&#24433;&#21709;&#25512;&#29702;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#31526;&#21512;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;&#26368;&#21518;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that ou
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#32929;&#31080;&#39044;&#27979;&#20013;&#30340;&#35299;&#37322;&#38382;&#39064;&#21644;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03659</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21453;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#32929;&#31080;&#39044;&#27979;&#20013;&#30340;&#35299;&#37322;&#38382;&#39064;&#21644;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20256;&#32479;&#30340;&#38750;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#35299;&#37322;&#32929;&#31080;&#39044;&#27979;&#36890;&#24120;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#35299;&#37322;&#20165;&#38480;&#20110;&#21487;&#35270;&#21270;&#37325;&#35201;&#25991;&#26412;&#19978;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#32929;&#31080;&#39044;&#27979;&#23545;LLM&#26469;&#35828;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33021;&#22815;&#26435;&#34913;&#28151;&#20081;&#31038;&#20250;&#25991;&#26412;&#23545;&#32929;&#31080;&#20215;&#26684;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;&#38543;&#30528;&#24341;&#20837;&#35299;&#37322;&#32452;&#20214;&#65292;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#38656;&#35201;LLM&#33021;&#22815;&#29992;&#21475;&#22836;&#26041;&#24335;&#35299;&#37322;&#20026;&#20160;&#20040;&#26576;&#20123;&#22240;&#32032;&#27604;&#20854;&#20182;&#22240;&#32032;&#26356;&#37325;&#35201;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35201;&#20026;&#36825;&#26679;&#30340;&#20219;&#21153;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#38656;&#35201;&#19987;&#23478;&#26631;&#27880;&#30340;&#26679;&#26412;&#26469;&#35299;&#37322;&#35757;&#32451;&#38598;&#20013;&#30340;&#27599;&#27425;&#32929;&#31080;&#27874;&#21160;&#65292;&#36825;&#22312;&#25104;&#26412;&#21644;&#23454;&#38469;&#21487;&#25193;&#23637;&#24615;&#19978;&#26159;&#26114;&#36149;&#19988;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others. On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our Summarize-Explain-Predict (SEP) 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;BPDec&#65288;BERT&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65289;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#22686;&#24378;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#35774;&#35745;&#21450;&#30740;&#31350;&#22312;BERT&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.15861</link><description>&lt;p&gt;
BPDec: &#25581;&#31034;BERT&#39044;&#35757;&#32451;&#20013;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
BPDec: Unveiling the Potential of Masked Language Modeling Decoder in BERT pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;BPDec&#65288;BERT&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65289;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#22686;&#24378;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#35774;&#35745;&#21450;&#30740;&#31350;&#22312;BERT&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BERT&#65288;&#26469;&#33258;Transformer&#30340;&#21452;&#21521;&#32534;&#30721;&#34920;&#31034;&#65289;&#36890;&#36807;&#20854;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#20986;&#33394;&#30340;&#24615;&#33021;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#38598;&#20013;&#22312;&#19982;&#27169;&#22411;&#32467;&#26500;&#30456;&#20851;&#30340;&#22686;&#24378;&#65292;&#20363;&#22914;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#21644;&#26356;&#26377;&#25928;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#36824;&#26377;&#19968;&#20123;&#20154;&#28145;&#20837;&#30740;&#31350;&#20102;&#19982;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#25216;&#24039;&#65292;&#21253;&#25324;&#25972;&#35789;&#25513;&#30721;&#12290;DeBERTa&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;BERT&#32534;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22686;&#24378;&#35299;&#30721;&#22120;&#65292;&#35777;&#26126;&#25928;&#26524;&#38750;&#24120;&#26174;&#33879;&#12290;&#25105;&#20204;&#35748;&#20026;&#22260;&#32469;&#22686;&#24378;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#30340;&#35774;&#35745;&#21644;&#30740;&#31350;&#24182;&#26410;&#24471;&#21040;&#24212;&#26377;&#30340;&#37325;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#35774;&#35745;&#65292;&#24182;&#20171;&#32461;&#20102;BPDec&#65288;BERT&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#24120;&#65292;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20250;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15861v2 Announce Type: replace-cross  Abstract: BERT (Bidirectional Encoder Representations from Transformers) has revolutionized the field of natural language processing through its exceptional performance on numerous tasks. Yet, the majority of researchers have mainly concentrated on enhancements related to the model structure, such as relative position embedding and more efficient attention mechanisms. Others have delved into pretraining tricks associated with Masked Language Modeling, including whole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's encoder model for pretraining, proving to be highly effective. We argue that the design and research around enhanced masked language modeling decoders have been underappreciated. In this paper, we propose several designs of enhanced decoders and introduce BPDec (BERT Pretraining Decoder), a novel method for modeling training. Typically, a pretrained BERT model is fine-tuned for specific Natural Language 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#32467;&#26500;&#21644;&#36807;&#31243;&#36827;&#34892;&#35774;&#35745;&#65292;&#36890;&#36807;&#35748;&#30693;&#36807;&#36733;&#25915;&#20987;&#65292;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#20043;&#21518;&#65292;&#20063;&#21487;&#20197;&#28608;&#21457;LLMs&#20135;&#29983;&#26377;&#23475;&#25110;&#19981;&#36947;&#24503;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2311.09827</link><description>&lt;p&gt;
&#35748;&#30693;&#36127;&#33655;: &#36890;&#36807;&#36229;&#36733;&#36923;&#36753;&#24605;&#32500;&#36234;&#29425;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#32467;&#26500;&#21644;&#36807;&#31243;&#36827;&#34892;&#35774;&#35745;&#65292;&#36890;&#36807;&#35748;&#30693;&#36807;&#36733;&#25915;&#20987;&#65292;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#20043;&#21518;&#65292;&#20063;&#21487;&#20197;&#28608;&#21457;LLMs&#20135;&#29983;&#26377;&#23475;&#25110;&#19981;&#36947;&#24503;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#21508;&#31181;&#26377;&#23475;&#34892;&#20026;&#12290;&#20316;&#20026;&#20195;&#34920;&#65292;&#36234;&#29425;&#25915;&#20987;&#21487;&#33021;&#24341;&#21457;LLMs&#20135;&#29983;&#26377;&#23475;&#25110;&#19981;&#36947;&#24503;&#30340;&#21709;&#24212;&#65292;&#21363;&#20351;&#32463;&#36807;&#20102;&#23433;&#20840;&#23545;&#40784;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#26032;&#39062;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#19987;&#38376;&#38024;&#23545;LLMs&#30340;&#35748;&#30693;&#32467;&#26500;&#21644;&#36807;&#31243;&#36827;&#34892;&#35774;&#35745;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#22312;&#38754;&#23545;&#65288;1&#65289;&#22810;&#35821;&#35328;&#35748;&#30693;&#36127;&#33655;&#65292;&#65288;2&#65289;&#38544;&#26214;&#34920;&#36798;&#21644;&#65288;3&#65289;&#25928;&#26524;&#25512;&#23548;&#25512;&#29702;&#26102;&#30340;&#23433;&#20840;&#24615;&#33030;&#24369;&#24615;&#12290;&#19982;&#20808;&#21069;&#30340;&#36234;&#29425;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#35748;&#30693;&#36127;&#36733;&#26159;&#19968;&#31181;&#26080;&#38656;&#20102;&#35299;&#27169;&#22411;&#26550;&#26500;&#25110;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;&#22312;AdvBench&#21644;MasterKey&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21253;&#25324;&#27969;&#34892;&#30340;&#24320;&#28304;&#27169;&#22411;Llama 2&#21644;&#19987;&#26377;&#27169;&#22411;ChatGPT&#22312;&#20869;&#30340;&#21508;&#31181;LLMs&#21487;&#20197;&#36890;&#36807;&#35748;&#30693;&#36807;&#36733;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09827v2 Announce Type: replace  Abstract: While large language models (LLMs) have demonstrated increasing power, they have also given rise to a wide range of harmful behaviors. As representatives, jailbreak attacks can provoke harmful or unethical responses from LLMs, even after safety alignment. In this paper, we investigate a novel category of jailbreak attacks specifically designed to target the cognitive structure and processes of LLMs. Specifically, we analyze the safety vulnerability of LLMs in the face of (1) multilingual cognitive overload, (2) veiled expression, and (3) effect-to-cause reasoning. Different from previous jailbreak attacks, our proposed cognitive overload is a black-box attack with no need for knowledge of model architecture or access to model weights. Experiments conducted on AdvBench and MasterKey reveal that various LLMs, including both popular open-source model Llama 2 and the proprietary model ChatGPT, can be compromised through cognitive overloa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SLM/LLM&#36335;&#30001;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#22686;&#24378;&#20219;&#21153;&#24615;&#33021;&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#25552;&#21462;&#20219;&#21153;&#20013;SLMs&#21644;LLMs&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#20174;&#32780;&#38477;&#20302;&#25104;&#26412;&#32780;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.09758</link><description>&lt;p&gt;
OrchestraLLM&#65306;&#29992;&#20110;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#35821;&#35328;&#27169;&#22411;&#39640;&#25928;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SLM/LLM&#36335;&#30001;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#22686;&#24378;&#20219;&#21153;&#24615;&#33021;&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#25552;&#21462;&#20219;&#21153;&#20013;SLMs&#21644;LLMs&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#20174;&#32780;&#38477;&#20302;&#25104;&#26412;&#32780;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#26684;&#23616;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#26114;&#36149;&#12290;&#20026;&#20102;&#38477;&#20302;&#25104;&#26412;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#21033;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#20316;&#20026;&#20854;&#26356;&#22823;&#22411;&#23545;&#24212;&#29289;&#30340;&#32463;&#27982;&#26377;&#25928;&#26367;&#20195;&#21697;&#12290;&#21463;&#21040;SLMs&#21644;LLMs&#22312;&#32467;&#26500;&#21270;&#30693;&#35782;&#25552;&#21462;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20114;&#34917;&#20248;&#21183;&#30340;&#21457;&#29616;&#39537;&#21160;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SLM/LLM&#36335;&#30001;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#24182;&#22686;&#24378;&#20219;&#21153;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#21019;&#24314;&#31034;&#33539;&#27744;&#20197;&#34920;&#31034;&#27599;&#20010;LM&#25552;&#20379;&#26356;&#21487;&#38752;&#31572;&#26696;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#65292;&#21033;&#29992;&#21477;&#23376;&#23884;&#20837;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#25509;&#36817;&#23545;&#35805;&#29366;&#24577;&#30456;&#20284;&#24615;&#12290;&#28982;&#21518;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26816;&#32034;&#21040;&#27979;&#35797;&#23454;&#20363;&#30340;k&#20010;&#26368;&#36817;&#31034;&#33539;&#65292;&#24182;&#26681;&#25454;&#24773;&#20917;&#36335;&#30001;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09758v2 Announce Type: replace  Abstract: Large language models (LLMs) have revolutionized the landscape of Natural Language Processing systems, but are computationally expensive. To reduce the cost without sacrificing performance, previous studies have explored various approaches to harness the potential of Small Language Models (SLMs) as cost-effective alternatives to their larger counterparts. Driven by findings that SLMs and LLMs exhibit complementary strengths in a structured knowledge extraction task, this work presents a novel SLM/LLM routing framework designed to improve computational efficiency and enhance task performance. First, exemplar pools are created to represent the types of contexts where each LM provides a more reliable answer, leveraging a sentence embedding fine-tuned so that context similarity is close to dialogue state similarity. Then, during inference, the k-nearest exemplars to the testing instance are retrieved, and the instance is routed according
&lt;/p&gt;</description></item><item><title>HallusionBench&#26159;&#19968;&#20010;&#19987;&#20026;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#32972;&#26223;&#25512;&#29702;&#20013;&#38754;&#20020;&#25361;&#25112;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#32467;&#26500;&#21644;&#37327;&#21270;&#20998;&#26512;&#65292;&#26174;&#31034;&#20986;GPT-4V&#21462;&#24471;&#20102;31.42%&#30340;&#20934;&#30830;&#29575;&#65292;&#36828;&#39640;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2310.14566</link><description>&lt;p&gt;
HallusionBench&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#32416;&#32544;&#30340;&#35821;&#35328;&#24187;&#35273;&#21644;&#35270;&#24187;&#35273;&#30340;&#39640;&#32423;&#35786;&#26029;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14566
&lt;/p&gt;
&lt;p&gt;
HallusionBench&#26159;&#19968;&#20010;&#19987;&#20026;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#32972;&#26223;&#25512;&#29702;&#20013;&#38754;&#20020;&#25361;&#25112;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#32467;&#26500;&#21644;&#37327;&#21270;&#20998;&#26512;&#65292;&#26174;&#31034;&#20986;GPT-4V&#21462;&#24471;&#20102;31.42%&#30340;&#20934;&#30830;&#29575;&#65292;&#36828;&#39640;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;HallusionBench&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#35780;&#20272;&#22270;&#20687;&#32972;&#26223;&#25512;&#29702;&#32780;&#35774;&#35745;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;&#36825;&#20010;&#22522;&#20934;&#23545;&#20110;&#39640;&#32423;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#65288;&#22914;GPT-4V&#65288;Vision&#65289;&#12289;Gemini Pro Vision&#21644;LLaVA-1.5&#65289;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#24378;&#35843;&#23545;&#35270;&#35273;&#25968;&#25454;&#30340;&#24494;&#22937;&#29702;&#35299;&#21644;&#35299;&#37322;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;346&#24352;&#22270;&#20687;&#21644;1129&#20010;&#38382;&#39064;&#65292;&#20840;&#37096;&#30001;&#20154;&#31867;&#19987;&#23478;&#31934;&#24515;&#35774;&#35745;&#12290;&#25105;&#20204;&#20026;&#36825;&#20123;&#35270;&#35273;&#38382;&#39064;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#65292;&#26088;&#22312;&#24314;&#31435;&#23545;&#29031;&#32452;&#12290;&#36825;&#31181;&#32467;&#26500;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#27169;&#22411;&#30340;&#21709;&#24212;&#20542;&#21521;&#12289;&#36923;&#36753;&#19968;&#33268;&#24615;&#21644;&#21508;&#31181;&#25925;&#38556;&#27169;&#24335;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#23545;HallusionBench&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23545;14&#31181;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#20986;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;GPT-4V&#21462;&#24471;&#30340;31.42&#65285;&#30340;&#38382;&#39064;&#23545;&#20934;&#30830;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#26377;&#20854;&#20182;&#35780;&#20272;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#22343;&#20302;&#20110;16&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14566v3 Announce Type: replace-cross  Abstract: We introduce HallusionBench, a comprehensive benchmark designed for the evaluation of image-context reasoning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision), Gemini Pro Vision, and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on HallusionBench, we benchmarked 14 different models, highlighting a 31.42% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16%. Moreover, our analysis not only h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#20551;&#35774;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#20551;&#35774;&#23384;&#22312;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20351;&#20854;&#19982;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#35821;&#22659;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#27169;&#22411;&#30340;&#35266;&#23519;&#21644;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;ICL&#21644;GD&#22312;&#35266;&#23519;&#28436;&#31034;&#39034;&#24207;&#19978;&#30340;&#19981;&#21516;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.08540</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20551;&#35774;&#65306;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#20551;&#35774;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#20551;&#35774;&#23384;&#22312;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20351;&#20854;&#19982;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#35821;&#22659;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#27169;&#22411;&#30340;&#35266;&#23519;&#21644;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;ICL&#21644;GD&#22312;&#35266;&#23519;&#28436;&#31034;&#39034;&#24207;&#19978;&#30340;&#19981;&#21516;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#20013;&#30340;In-Context Learning&#65288;ICL&#65289;&#30340;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#29616;&#35937;&#65292;&#20294;&#25105;&#20204;&#23545;&#20854;&#20102;&#35299;&#29978;&#23569;&#12290;&#20026;&#20102;&#35299;&#37322;ICL&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#23558;&#20854;&#19982;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#38382;&#65292;&#36825;&#31181;&#32852;&#31995;&#22312;&#23454;&#38469;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#26159;&#21542;&#25104;&#31435;&#65311;&#25105;&#20204;&#24378;&#35843;&#20808;&#21069;&#20316;&#21697;&#20013;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#20351;&#24471;&#23427;&#20204;&#30340;&#35821;&#22659;&#19982;&#35821;&#35328;&#27169;&#22411;&#23454;&#38469;&#35757;&#32451;&#26102;&#30340;&#23454;&#38469;&#35821;&#22659;&#24046;&#21035;&#24456;&#22823;&#12290;&#20363;&#22914;&#65292;&#36825;&#20123;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#29702;&#35770;&#25163;&#24037;&#26500;&#36896;&#30340;&#26435;&#37325;&#20855;&#26377;&#19982;&#30495;&#23454;LLM&#19981;&#21305;&#37197;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#30340;&#23454;&#39564;&#39564;&#35777;&#20351;&#29992;ICL&#30446;&#26631;&#65288;&#26126;&#30830;&#20026;ICL&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#36825;&#19982;&#37326;&#22806;&#20986;&#29616;&#30340;ICL&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#36824;&#23547;&#25214;&#20102;&#30495;&#23454;&#27169;&#22411;&#20013;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;ICL&#21644;GD&#23545;&#20110;&#35266;&#23519;&#28436;&#31034;&#30340;&#39034;&#24207;&#26377;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#25506;&#35752;&#24182;&#27604;&#36739;ICL&#19982;GD&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08540v4 Announce Type: replace-cross  Abstract: The emergence of In-Context Learning (ICL) in LLMs remains a significant phenomenon with little understanding. To explain ICL, recent studies try to theoretically connect it to Gradient Descent (GD). We ask, does this connection hold up in actual pre-trained models?   We highlight the limiting assumptions in prior works that make their context considerably different from the practical context in which language models are trained. For example, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. Furthermore, their experimental verification uses ICL objective (training models explicitly for ICL), which differs from the emergent ICL in the wild.   We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2309.13339</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340; remarkable generalizability&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#32463;&#24120;&#26410;&#33021;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#24314;&#31435;&#36830;&#36143;&#30340;&#24605;&#32500;&#33539;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#26410;&#21463;&#36923;&#36753;&#21407;&#21017;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#31995;&#32479;&#22320;&#39564;&#35777;&#21644;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13339v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts) prompting, a self-improvement framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in
&lt;/p&gt;</description></item><item><title>WebVoyager&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;Web&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#20132;&#20114;&#26469;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13919</link><description>&lt;p&gt;
WebVoyager&#65306;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#30340;Web Agent
&lt;/p&gt;
&lt;p&gt;
WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13919
&lt;/p&gt;
&lt;p&gt;
WebVoyager&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;Web&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#20132;&#20114;&#26469;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#24341;&#39046;&#20102;&#19968;&#20010;&#30001;&#30495;&#23454;&#19990;&#30028;&#20013;&#33258;&#20027;&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#25152;&#26631;&#24535;&#30340;&#26032;&#26102;&#20195;&#65292;&#25512;&#21160;&#20102;&#22522;&#20110;&#32593;&#32476;&#30340;&#39640;&#32423;&#20195;&#29702;&#30340;&#21019;&#26032;&#12290;&#29616;&#26377;&#30340;&#32593;&#32476;&#20195;&#29702;&#36890;&#24120;&#21482;&#22788;&#29702;&#19968;&#20010;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#19988;&#20165;&#22312;&#31616;&#21270;&#30340;&#32593;&#32476;&#27169;&#25311;&#22120;&#25110;&#38745;&#24577;&#30340;&#32593;&#32476;&#24555;&#29031;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WebVoyager&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;Web&#20195;&#29702;&#65292;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#36827;&#34892;&#20132;&#20114;&#65292;&#33021;&#22815;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#24335;Web&#20195;&#29702;&#20219;&#21153;&#30340;&#33258;&#21160;&#35780;&#20272;&#25361;&#25112;&#65292;&#21033;&#29992;&#20102;GPT-4V&#30340;&#24378;&#22823;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;15&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#32593;&#31449;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;WebVoyager&#23454;&#29616;&#20102;55.7&#65285;&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#26174;&#33879;&#22320;.....
&lt;/p&gt;
&lt;p&gt;
The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7% task success rate, significantly 
&lt;/p&gt;</description></item><item><title>BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02982</link><description>&lt;p&gt;
BIBench: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#20998;&#26512;&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BIBench: Benchmarking Data Analysis Knowledge of Large Language Models. (arXiv:2401.02982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02982
&lt;/p&gt;
&lt;p&gt;
BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#29087;&#32451;&#24230;&#21644;&#21487;&#38752;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#24605;&#32500;&#20026;&#37325;&#28857;&#30340;&#39046;&#22495;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BIBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#30340;&#32972;&#26223;&#19979;&#30340;&#25968;&#25454;&#20998;&#26512;&#33021;&#21147;&#12290;BIBench&#36890;&#36807;&#19977;&#20010;&#32500;&#24230;&#35780;&#20272;LLMs&#65306;1&#65289;BI&#22522;&#30784;&#30693;&#35782;&#65292;&#35780;&#20272;&#27169;&#22411;&#30340;&#25968;&#20540;&#25512;&#29702;&#33021;&#21147;&#21644;&#23545;&#37329;&#34701;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65307;2&#65289;BI&#30693;&#35782;&#24212;&#29992;&#65292;&#30830;&#23450;&#27169;&#22411;&#24555;&#36895;&#29702;&#35299;&#25991;&#26412;&#20449;&#24687;&#24182;&#20174;&#22810;&#20010;&#35270;&#35282;&#29983;&#25104;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65307;3&#65289;BI&#25216;&#26415;&#25216;&#33021;&#65292;&#26816;&#26597;&#27169;&#22411;&#20351;&#29992;&#25216;&#26415;&#30693;&#35782;&#35299;&#20915;&#29616;&#23454;&#25968;&#25454;&#20998;&#26512;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;BIBench&#21253;&#25324;11&#20010;&#23376;&#20219;&#21153;&#65292;&#28085;&#30422;&#20998;&#31867;&#12289;&#25552;&#21462;&#21644;&#29983;&#25104;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. However, their proficiency and reliability in the specialized domain of Data Analysis, particularly with a focus on data-driven thinking, remain uncertain. To bridge this gap, we introduce BIBench, a comprehensive benchmark designed to evaluate the data analysis capabilities of LLMs within the context of Business Intelligence (BI). BIBench assesses LLMs across three dimensions: 1) BI foundational knowledge, evaluating the models' numerical reasoning and familiarity with financial concepts; 2) BI knowledge application, determining the models' ability to quickly comprehend textual information and generate analysis questions from multiple views; and 3) BI technical skills, examining the models' use of technical knowledge to address real-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning three categories of task types: classification, extraction, and generation. Additi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#24182;&#36827;&#34892;&#25512;&#29702;&#26469;&#25351;&#23548;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.18127</link><description>&lt;p&gt;
&#25552;&#38382;&#26356;&#22810;&#65292;&#20102;&#35299;&#26356;&#22810;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#38382;&#39064;&#19982;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models. (arXiv:2310.18127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#24182;&#36827;&#34892;&#25512;&#29702;&#26469;&#25351;&#23548;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23558;&#22522;&#20110;&#34892;&#21160;&#30340;&#31574;&#30053;&#19982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#22797;&#26434;&#23454;&#38469;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#26469;&#35828;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#36825;&#20123;&#25552;&#31034;&#26159;&#36890;&#36807;&#24191;&#27867;&#20351;&#29992;&#20154;&#21147;&#25163;&#24037;&#21046;&#20316;&#30340;&#65292;&#23548;&#33268;CoT&#31574;&#30053;&#32463;&#24120;&#26080;&#27861;&#25512;&#24191;&#12290;&#20026;&#20102;&#30830;&#20445;&#20302;&#23618;&#25511;&#21046;&#22120;&#36866;&#24403;&#22320;&#22788;&#29702;CoT&#25512;&#29702;&#65292;&#36824;&#38656;&#35201;&#20154;&#20026;&#20171;&#20837;&#26469;&#24320;&#21457;&#25509;&#22320;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#36808;&#21521;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#24212;&#29992;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#35299;&#20915;&#30340;&#23436;&#20840;&#38598;&#25104;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#23548;&#32773;-&#36861;&#38543;&#32773;&#21452;&#23618;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#65288;&#25552;&#31034;&#65289;&#65292;&#24182;&#38543;&#21518;&#36827;&#34892;&#25512;&#29702;&#65292;&#25351;&#23548;&#22312;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;&#19968;&#20010;&#22909;&#30340;&#25552;&#31034;&#24212;&#35813;&#22522;&#20110;&#21382;&#21490;&#30340;&#33258;&#30465;&#24615;&#20462;&#35746;&#26469;&#36827;&#34892;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate their promise in tackling complicated practical challenges by combining action-based policies with chain of thought (CoT) reasoning. Having high-quality prompts on hand, however, is vital to the framework's effectiveness. Currently, these prompts are handcrafted utilizing extensive human labor, resulting in CoT policies that frequently fail to generalize. Human intervention is also required in order to develop grounding functions that ensure low-level controllers appropriately process CoT reasoning. In this paper, we take the first step towards a fully integrated end-to-end framework for task-solving in real settings employing complicated reasoning. To that purpose, we offer a new leader-follower bilevel framework capable of learning to ask relevant questions (prompts) and subsequently undertaking reasoning to guide the learning of actions to be performed in an environment. A good prompt should make introspective revisions based on historical fi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21478;&#19968;&#20010;&#23884;&#20837;&#24335;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25152;&#26377;&#21069;&#38754;token&#30340;&#35821;&#20041;&#23884;&#20837;&#65292;&#28982;&#21518;&#21033;&#29992;&#35757;&#32451;&#24471;&#21040;&#30340;&#27700;&#21360;&#27169;&#22411;&#23558;&#36825;&#20123;&#35821;&#20041;&#23884;&#20837;&#36716;&#25442;&#20026;&#27700;&#21360;logits&#12290;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#25915;&#20987;&#40065;&#26834;&#24615;&#21448;&#20855;&#26377;&#23433;&#20840;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06356</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#19981;&#21464;&#40065;&#26834;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
A Semantic Invariant Robust Watermark for Large Language Models. (arXiv:2310.06356v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21478;&#19968;&#20010;&#23884;&#20837;&#24335;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25152;&#26377;&#21069;&#38754;token&#30340;&#35821;&#20041;&#23884;&#20837;&#65292;&#28982;&#21518;&#21033;&#29992;&#35757;&#32451;&#24471;&#21040;&#30340;&#27700;&#21360;&#27169;&#22411;&#23558;&#36825;&#20123;&#35821;&#20041;&#23884;&#20837;&#36716;&#25442;&#20026;&#27700;&#21360;logits&#12290;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#25915;&#20987;&#40065;&#26834;&#24615;&#21448;&#20855;&#26377;&#23433;&#20840;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#31639;&#27861;&#22312;&#26816;&#27979;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#31639;&#27861;&#22312;&#25915;&#20987;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#40065;&#26834;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLM&#36827;&#34892;&#35821;&#20041;&#19981;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#25915;&#20987;&#40065;&#26834;&#24615;&#21448;&#20855;&#26377;&#23433;&#20840;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21478;&#19968;&#20010;&#23884;&#20837;&#24335;LLM&#29983;&#25104;&#25152;&#26377;&#21069;&#38754;token&#30340;&#35821;&#20041;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;&#35757;&#32451;&#24471;&#21040;&#30340;&#27700;&#21360;&#27169;&#22411;&#23558;&#36825;&#20123;&#35821;&#20041;&#23884;&#20837;&#36716;&#25442;&#20026;&#27700;&#21360;logits&#12290;
&lt;/p&gt;
&lt;p&gt;
Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. In this work, we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens. Specifically, we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model. Subs
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37325;&#26032;&#23450;&#20041;&#25968;&#23383;&#20581;&#24247;&#30028;&#38754;&#30340;&#26041;&#27861;&#65292;&#23558;LLMs&#19982;&#22806;&#37096;&#24037;&#20855;&#32467;&#21512;&#20351;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#19982;&#20020;&#24202;&#25216;&#26415;&#30340;&#20114;&#21160;&#25928;&#26524;&#65292;&#25913;&#21892;&#20102;&#25968;&#23383;&#21307;&#30103;&#24037;&#20855;&#21644;AI&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;LLMs&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03560</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#23450;&#20041;&#25968;&#23383;&#20581;&#24247;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Redefining Digital Health Interfaces with Large Language Models. (arXiv:2310.03560v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37325;&#26032;&#23450;&#20041;&#25968;&#23383;&#20581;&#24247;&#30028;&#38754;&#30340;&#26041;&#27861;&#65292;&#23558;LLMs&#19982;&#22806;&#37096;&#24037;&#20855;&#32467;&#21512;&#20351;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#19982;&#20020;&#24202;&#25216;&#26415;&#30340;&#20114;&#21160;&#25928;&#26524;&#65292;&#25913;&#21892;&#20102;&#25968;&#23383;&#21307;&#30103;&#24037;&#20855;&#21644;AI&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;LLMs&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#20581;&#24247;&#24037;&#20855;&#20855;&#26377;&#26174;&#33879;&#25913;&#21892;&#21307;&#30103;&#26381;&#21153;&#20256;&#36882;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#29992;&#24615;&#21644;&#20449;&#20219;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#23427;&#20204;&#30340;&#20351;&#29992;&#20173;&#28982;&#30456;&#23545;&#26377;&#38480;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20855;&#26377;&#22788;&#29702;&#22797;&#26434;&#20449;&#24687;&#21644;&#29983;&#25104;&#20154;&#31867;&#36136;&#37327;&#25991;&#26412;&#33021;&#21147;&#30340;&#36890;&#29992;&#27169;&#22411;&#20986;&#29616;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#30452;&#25509;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#24212;&#29992;LLMs&#24182;&#19981;&#31616;&#21333;&#65292;&#22240;&#20026;LLMs&#23481;&#26131;&#25552;&#20379;&#19981;&#19968;&#33268;&#25110;&#26080;&#24847;&#20041;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#20351;LLMs&#22312;&#20020;&#24202;&#21307;&#30103;&#25216;&#26415;&#20114;&#21160;&#20013;&#25552;&#20379;&#20840;&#26032;&#30028;&#38754;&#12290;&#36825;&#22686;&#24378;&#20102;&#25968;&#23383;&#21307;&#30103;&#24037;&#20855;&#21644;AI&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#21644;&#23454;&#38469;&#24433;&#21709;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;LLMs&#30340;&#24403;&#21069;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;&#31958;&#23615;&#30149;&#39118;&#38505;&#39044;&#27979;&#30340;&#20363;&#23376;&#38416;&#36848;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital health tools have the potential to significantly improve the delivery of healthcare services. However, their use remains comparatively limited due, in part, to challenges surrounding usability and trust. Recently, Large Language Models (LLMs) have emerged as general-purpose models with the ability to process complex information and produce human-quality text, presenting a wealth of potential applications in healthcare. Directly applying LLMs in clinical settings is not straightforward, with LLMs susceptible to providing inconsistent or nonsensical answers. We demonstrate how LLMs can utilize external tools to provide a novel interface between clinicians and digital technologies. This enhances the utility and practical impact of digital healthcare tools and AI models while addressing current issues with using LLM in clinical settings such as hallucinations. We illustrate our approach with examples from cardiovascular disease and diabetes risk prediction, highlighting the benefit
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#21307;&#30103;&#23545;&#35805;&#27169;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#29616;&#26377;&#25351;&#26631;&#23545;&#21307;&#23398;&#21644;&#20581;&#24247;&#27010;&#24565;&#30340;&#29702;&#35299;&#19981;&#36275;&#20197;&#21450;&#24573;&#30053;&#20102;&#29992;&#25143;&#20307;&#39564;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.12444</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;AI&#30340;&#21307;&#30103;&#23545;&#35805;&#25928;&#26524;&#30340;&#37327;&#21270;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI. (arXiv:2309.12444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12444
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#21307;&#30103;&#23545;&#35805;&#27169;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#29616;&#26377;&#25351;&#26631;&#23545;&#21307;&#23398;&#21644;&#20581;&#24247;&#27010;&#24565;&#30340;&#29702;&#35299;&#19981;&#36275;&#20197;&#21450;&#24573;&#30053;&#20102;&#29992;&#25143;&#20307;&#39564;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#23558;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24739;&#32773;&#25252;&#29702;&#36716;&#21464;&#20026;&#26356;&#20010;&#24615;&#21270;&#12289;&#39640;&#25928;&#21644;&#31215;&#26497;&#30340;&#36807;&#31243;&#65292;&#24443;&#24213;&#25913;&#21464;&#21307;&#30103;&#20445;&#20581;&#20132;&#20184;&#26041;&#24335;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#20114;&#21160;&#23545;&#35805;&#27169;&#22411;&#65292;&#24456;&#21487;&#33021;&#25512;&#21160;&#21307;&#30103;&#20445;&#20581;&#30340;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#36716;&#22411;&#12290;&#36890;&#36807;&#25552;&#20379;&#35786;&#26029;&#12289;&#20010;&#24615;&#21270;&#29983;&#27963;&#26041;&#24335;&#24314;&#35758;&#21644;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#31561;&#21508;&#31181;&#26381;&#21153;&#65292;&#30446;&#26631;&#26159;&#22823;&#24133;&#24230;&#25552;&#39640;&#24739;&#32773;&#30340;&#20581;&#24247;&#32467;&#26524;&#65292;&#21516;&#26102;&#20943;&#36731;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#21307;&#30103;&#24212;&#29992;&#30340;&#29983;&#21629;&#20851;&#38190;&#24615;&#35201;&#27714;&#24314;&#31435;&#32479;&#19968;&#20840;&#38754;&#30340;&#23545;&#35805;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#12290;&#24050;&#26377;&#30340;&#38024;&#23545;&#21508;&#31181;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25552;&#20986;&#30340;&#35780;&#20272;&#25351;&#26631;&#22312;&#29702;&#35299;&#21307;&#23398;&#21644;&#20581;&#24247;&#27010;&#24565;&#21450;&#20854;&#22312;&#20419;&#36827;&#24739;&#32773;&#31119;&#31049;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25351;&#26631;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#29992;&#25143;&#20307;&#39564;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence is set to revolutionize healthcare delivery by transforming traditional patient care into a more personalized, efficient, and proactive process. Chatbots, serving as interactive conversational models, will probably drive this patient-centered transformation in healthcare. Through the provision of various services, including diagnosis, personalized lifestyle recommendations, and mental health support, the objective is to substantially augment patient health outcomes, all the while mitigating the workload burden on healthcare providers. The life-critical nature of healthcare applications necessitates establishing a unified and comprehensive set of evaluation metrics for conversational models. Existing evaluation metrics proposed for various generic large language models (LLMs) demonstrate a lack of comprehension regarding medical and health concepts and their significance in promoting patients' well-being. Moreover, these metrics neglect pivotal user-ce
&lt;/p&gt;</description></item><item><title>&#28436;&#32451;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#20914;&#31361;&#21644;&#25552;&#20379;&#21453;&#39304;&#65292;&#25945;&#25480;&#29992;&#25143;&#20914;&#31361;&#35299;&#20915;&#30340;&#25216;&#33021;&#12290;&#21033;&#29992;&#28436;&#32451;&#65292;&#29992;&#25143;&#21487;&#20197;&#32451;&#20064;&#22788;&#29702;&#21508;&#31181;&#20914;&#31361;&#22330;&#26223;&#65292;&#24182;&#23398;&#20064;&#22914;&#20309;&#36816;&#29992;&#20914;&#31361;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.12309</link><description>&lt;p&gt;
&#28436;&#32451;&#65306;&#36890;&#36807;&#27169;&#25311;&#20914;&#31361;&#26469;&#25945;&#25480;&#20914;&#31361;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rehearsal: Simulating Conflict to Teach Conflict Resolution. (arXiv:2309.12309v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12309
&lt;/p&gt;
&lt;p&gt;
&#28436;&#32451;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#20914;&#31361;&#21644;&#25552;&#20379;&#21453;&#39304;&#65292;&#25945;&#25480;&#29992;&#25143;&#20914;&#31361;&#35299;&#20915;&#30340;&#25216;&#33021;&#12290;&#21033;&#29992;&#28436;&#32451;&#65292;&#29992;&#25143;&#21487;&#20197;&#32451;&#20064;&#22788;&#29702;&#21508;&#31181;&#20914;&#31361;&#22330;&#26223;&#65292;&#24182;&#23398;&#20064;&#22914;&#20309;&#36816;&#29992;&#20914;&#31361;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#38469;&#20914;&#31361;&#26159;&#19968;&#31181;&#20196;&#20154;&#19981;&#33298;&#26381;&#20294;&#19981;&#21487;&#36991;&#20813;&#30340;&#29983;&#27963;&#20107;&#23454;&#12290;&#25104;&#21151;&#22320;&#22788;&#29702;&#20914;&#31361;&#26159;&#19968;&#31181;&#25216;&#33021;&#65292;&#21487;&#20197;&#36890;&#36807;&#21051;&#24847;&#32451;&#20064;&#26469;&#23398;&#20064;&#65292;&#20294;&#26159;&#24456;&#23569;&#26377;&#20154;&#33021;&#22815;&#33719;&#24471;&#26377;&#25928;&#30340;&#22521;&#35757;&#25110;&#21453;&#39304;&#12290;&#20026;&#20102;&#25193;&#22823;&#36825;&#31181;&#26426;&#20250;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28436;&#32451;&#65288;Rehearsal&#65289;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#19982;&#21487;&#20449;&#30340;&#27169;&#25311;&#23545;&#35805;&#32773;&#19968;&#36215;&#25490;&#32451;&#20914;&#31361;&#65292;&#25506;&#32034;&#22914;&#26524;&#24773;&#20917;&#22914;&#20309;&#30340;&#8220;&#20551;&#35774;&#8221;&#22330;&#26223;&#20197;&#35782;&#21035;&#26367;&#20195;&#30340;&#23545;&#35805;&#36335;&#24452;&#65292;&#24182;&#36890;&#36807;&#21453;&#39304;&#23398;&#20064;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#24212;&#29992;&#29305;&#23450;&#30340;&#20914;&#31361;&#31574;&#30053;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#28436;&#32451;&#26469;&#32451;&#20064;&#22788;&#29702;&#21508;&#31181;&#24050;&#23450;&#20041;&#30340;&#20914;&#31361;&#22330;&#26223;&#65292;&#20174;&#21150;&#20844;&#23460;&#20105;&#35758;&#21040;&#24773;&#24863;&#38382;&#39064;&#65292;&#25110;&#32773;&#20182;&#20204;&#20063;&#21487;&#20197;&#36873;&#25321;&#21019;&#24314;&#33258;&#24049;&#30340;&#20914;&#31361;&#22330;&#26223;&#12290;&#20026;&#20102;&#23454;&#29616;&#28436;&#32451;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;IRP&#25552;&#31034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20914;&#31361;&#35299;&#20915;&#20013;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#21033;&#30410;-&#26435;&#21147;-&#33021;&#21147;&#65288;IRP&#65289;&#29702;&#35770;&#26469;&#35843;&#33410;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#28436;&#32451;&#20351;&#29992;IRP&#29983;&#25104;&#22522;&#20110;&#20914;&#31361;&#35299;&#20915;&#29702;&#35770;&#30340;&#35805;&#35821;&#65292;&#24341;&#23548;&#29992;&#25143;&#23454;&#36341;&#24212;&#29992;&#20914;&#31361;&#35299;&#20915;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpersonal conflict is an uncomfortable but unavoidable fact of life. Navigating conflict successfully is a skill -- one that can be learned through deliberate practice -- but few have access to effective training or feedback. To expand this access, we introduce Rehearsal, a system that allows users to rehearse conflicts with a believable simulated interlocutor, explore counterfactual "what if?" scenarios to identify alternative conversational paths, and learn through feedback on how and when to apply specific conflict strategies. Users can utilize Rehearsal to practice handling a variety of predefined conflict scenarios, from office disputes to relationship issues, or they can choose to create their own. To enable Rehearsal, we develop IRP prompting, a method of conditioning output of a large language model on the influential Interest-Rights-Power (IRP) theory from conflict resolution. Rehearsal uses IRP to generate utterances grounded in conflict resolution theory, guiding users t
&lt;/p&gt;</description></item><item><title>CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11143</link><description>&lt;p&gt;
CoT-BERT: &#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought. (arXiv:2309.11143v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11143
&lt;/p&gt;
&lt;p&gt;
CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#36755;&#20837;&#21477;&#23376;&#36716;&#21270;&#20026;&#23500;&#21547;&#22797;&#26434;&#35821;&#20041;&#20449;&#24687;&#30340;&#22266;&#23450;&#38271;&#24230;&#21521;&#37327;&#65292;&#21516;&#26102;&#28040;&#38500;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#23545;&#27604;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#25512;&#21160;&#19979;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#36712;&#36857;&#20013;&#65292;&#20173;&#28982;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#24605;&#32500;&#38142;&#26465;&#30340;&#28508;&#22312;&#33021;&#21147;&#12290;&#20026;&#20102;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#20013;&#30340;&#28508;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21477;&#23376;&#34920;&#31034;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#29702;&#35299;&#21644;&#25688;&#35201;&#12290;&#38543;&#21518;&#65292;&#21518;&#19968;&#38454;&#27573;&#30340;&#36755;&#20986;&#34987;&#21033;&#29992;&#20026;&#36755;&#20837;&#21477;&#23376;&#30340;&#21521;&#37327;&#21270;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#21644;&#27169;&#26495;&#21435;&#22122;&#25216;&#26415;&#36827;&#34892;&#20102;&#31934;&#32454;&#35843;&#25972;&#12290;&#20005;&#26684;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;CoT-BERT&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a
&lt;/p&gt;</description></item><item><title>&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#20110;&#22914;&#20309;&#24341;&#23548;&#21644;&#32467;&#26500;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#20110;&#36755;&#20837;&#38382;&#39064;&#26412;&#36523;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#37325;&#26032;&#38405;&#35835;&#8221;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#28145;&#20837;&#38405;&#35835;&#36755;&#20837;&#25552;&#31034;&#20013;&#30340;&#38382;&#39064;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#12289;&#26356;&#20934;&#30830;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#26356;&#26377;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06275</link><description>&lt;p&gt;
&#37325;&#26032;&#38405;&#35835;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Re-Reading Improves Reasoning in Language Models. (arXiv:2309.06275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06275
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#20110;&#22914;&#20309;&#24341;&#23548;&#21644;&#32467;&#26500;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#20110;&#36755;&#20837;&#38382;&#39064;&#26412;&#36523;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#37325;&#26032;&#38405;&#35835;&#8221;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#28145;&#20837;&#38405;&#35835;&#36755;&#20837;&#25552;&#31034;&#20013;&#30340;&#38382;&#39064;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#12289;&#26356;&#20934;&#30830;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#26356;&#26377;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#20197;&#24341;&#23548;&#21644;&#32467;&#26500;&#21270;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#20165;&#35299;&#30721;&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#20013;&#25805;&#20316;&#36755;&#20837;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#24573;&#30053;&#20154;&#31867;&#25512;&#29702;&#20013;&#20016;&#23500;&#30340;&#21069;&#21518;&#20132;&#20114;&#12290;&#23545;&#20110;&#23884;&#20837;&#22312;&#25552;&#31034;&#20013;&#30340;&#36755;&#20837;&#38382;&#39064;&#36825;&#19968;&#20851;&#38190;&#32500;&#24230;&#65292;&#30446;&#21069;&#20851;&#27880;&#36739;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#31216;&#20026;&#8220;&#37325;&#26032;&#38405;&#35835;&#8221;&#12290;&#20174;&#20154;&#31867;&#23398;&#20064;&#21644;&#38382;&#39064;&#35299;&#20915;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#37325;&#26032;&#38405;&#35835;&#24847;&#21619;&#30528;&#37325;&#35775;&#23884;&#22312;&#36755;&#20837;&#25552;&#31034;&#20013;&#30340;&#38382;&#39064;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#35748;&#30693;&#22686;&#24378;&#30340;&#21407;&#21017;&#23436;&#32654;&#22865;&#21512;&#65292;&#20351;LLM&#33021;&#22815;&#28145;&#20837;&#27934;&#23519;&#12289;&#35782;&#21035;&#22797;&#26434;&#30340;&#27169;&#24335;&#12289;&#24314;&#31435; mor
&lt;/p&gt;
&lt;p&gt;
Reasoning presents a significant and challenging issue for Large Language Models (LLMs). The predominant focus of research has revolved around developing diverse prompting strategies to guide and structure the reasoning processes of LLMs. However, these approaches based on decoder-only causal language models often operate the input question in a single forward pass, potentially missing the rich, back-and-forth interactions inherent in human reasoning. Scant attention has been paid to a critical dimension, i.e., the input question itself embedded within the prompts. In response, we introduce a deceptively simple yet highly effective prompting strategy, termed question "re-reading". Drawing inspiration from human learning and problem-solving, re-reading entails revisiting the question information embedded within input prompts. This approach aligns seamlessly with the cognitive principle of reinforcement, enabling LLMs to extract deeper insights, identify intricate patterns, establish mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21521;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#30340;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#23450;&#21521;&#27880;&#20837;&#20869;&#23384;&#26469;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.05605</link><description>&lt;p&gt;
&#20869;&#23384;&#27880;&#20837;&#65306;&#22312;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#20013;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models. (arXiv:2309.05605v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21521;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#30340;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#23450;&#21521;&#27880;&#20837;&#20869;&#23384;&#26469;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31572;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#38656;&#35201;&#20174;&#22810;&#20010;&#20449;&#24687;&#28304;&#20013;&#26816;&#32034;&#21644;&#32508;&#21512;&#20449;&#24687;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24448;&#24448;&#38590;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#36827;&#34892;&#23450;&#21521;&#20869;&#23384;&#27880;&#20837;&#26469;&#30830;&#23450;&#21644;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GPT-2&#27169;&#22411;&#22312;&#21333;&#36339;&#21644;&#22810;&#36339;&#25552;&#31034;&#19979;&#21508;&#23618;&#30340;&#28608;&#27963;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21521;&#20851;&#38190;LLM&#20301;&#32622;&#27880;&#20837;&#30456;&#20851;&#30340;&#25552;&#31034;&#29305;&#23450;&#20449;&#24687;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#35760;&#24518;&#8221;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;LLM&#33021;&#22815;&#25972;&#21512;&#39069;&#22806;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#22810;&#36339;&#25552;&#31034;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;&#23558;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#23450;&#21521;&#30340;&#35760;&#24518;&#27880;&#20837;&#21040;&#20851;&#38190;&#27880;&#24847;&#21147;&#23618;&#20013;&#24448;&#24448;&#33021;&#22815;&#25552;&#39640;&#22810;&#36339;&#20219;&#21153;&#20013;&#25152;&#38656;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#25552;&#39640;&#20102;&#36798;&#21040;424%&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#35752;&#22797;&#26434;&#24615;&#19982;&#23545;&#40784;&#20043;&#38388;&#30340;&#22266;&#26377;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;"tree-instruct"&#26041;&#27861;&#26469;&#22686;&#24378;&#25351;&#20196;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#21487;&#20197;&#22312;&#23545;&#40784;&#21040;&#20219;&#21153;&#21644;&#29992;&#25143;&#20559;&#22909;&#26041;&#38754;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05696</link><description>&lt;p&gt;
&#22797;&#26434;&#24615;&#19982;&#23545;&#40784;&#20043;&#38388;&#22266;&#26377;&#20851;&#31995;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment. (arXiv:2308.05696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#35752;&#22797;&#26434;&#24615;&#19982;&#23545;&#40784;&#20043;&#38388;&#30340;&#22266;&#26377;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;"tree-instruct"&#26041;&#27861;&#26469;&#22686;&#24378;&#25351;&#20196;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#21487;&#20197;&#22312;&#23545;&#40784;&#21040;&#20219;&#21153;&#21644;&#29992;&#25143;&#20559;&#22909;&#26041;&#38754;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24320;&#25918;&#22495;&#25351;&#20196;&#25968;&#25454;&#22521;&#35757;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23545;&#40784;&#21040;&#26368;&#32456;&#20219;&#21153;&#21644;&#29992;&#25143;&#20559;&#22909;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#22823;&#37327;&#30740;&#31350;&#34920;&#26126;&#65292;&#25552;&#39640;&#25351;&#20196;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#22987;&#32456;&#33021;&#22815;&#25913;&#21892;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#25351;&#26631;&#65292;&#25968;&#25454;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#20027;&#35201;&#21253;&#25324;&#19977;&#20010;&#26041;&#38754;&#65306;(1)&#25193;&#23637;&#35268;&#24459;&#65292;&#24615;&#33021;&#25913;&#36827;&#22312;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#30340;&#21487;&#25345;&#32493;&#24615;&#23578;&#19981;&#30830;&#23450;&#65292;(2)&#39069;&#22806;&#30340;&#26631;&#35760;&#65292;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#25913;&#36827;&#26159;&#21542;&#26469;&#33258;&#24341;&#20837;&#26356;&#22810;&#35757;&#32451;&#26631;&#35760;&#65292;&#20197;&#21450;(3)&#35838;&#31243;&#35774;&#32622;&#65292;&#23558;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#30340;&#25351;&#20196;&#32435;&#20837;&#26159;&#21542;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#20063;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"tree-instruct"&#65292;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#31995;&#32479;&#22320;&#22686;&#24378;&#25351;&#20196;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#25351;&#23450;&#25968;&#37327;&#30340;&#33410;&#28857;&#28155;&#21152;&#21040;&#25351;&#20196;&#35821;&#20041;&#26641;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26032;&#30340;&#25351;&#20196;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models (LLMs) with open-domain instruction data has yielded remarkable success in aligning to end tasks and user preferences. Extensive research has highlighted that enhancing the quality and diversity of instruction data consistently improves performance. However, the impact of data complexity, as a crucial metric, remains relatively unexplored in three aspects: (1) scaling law, where the sustainability of performance improvements with increasing complexity is uncertain, (2) additional tokens, whether the improvement brought by complexity comes from introducing more training tokens, and (3) curriculum tuning, where the potential advantages of incorporating instructions ranging from easy to difficult are not yet fully understood. In this paper, we propose \textit{tree-instruct} to systematically enhance the complexity of instruction data in a controllable manner. This approach adds a specified number of nodes into the instruction semantic tree, yielding new inst
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#23494;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#65292;&#24182;&#20849;&#20139;&#37096;&#20998;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#39640;&#20934;&#30830;&#24615;&#30340;&#26816;&#27979;&#65292;&#21516;&#26102;&#23545;&#29983;&#25104;&#21644;&#26816;&#27979;&#36895;&#24230;&#24433;&#21709;&#26368;&#23567;&#12290;</title><link>http://arxiv.org/abs/2307.16230</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31169;&#23494;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
A Private Watermark for Large Language Models. (arXiv:2307.16230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16230
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#23494;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#65292;&#24182;&#20849;&#20139;&#37096;&#20998;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#39640;&#20934;&#30830;&#24615;&#30340;&#26816;&#27979;&#65292;&#21516;&#26102;&#23545;&#29983;&#25104;&#21644;&#26816;&#27979;&#36895;&#24230;&#24433;&#21709;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#24050;&#32463;&#20943;&#36731;&#20102;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#24102;&#26469;&#30340;&#20266;&#26032;&#38395;&#21644;&#29256;&#26435;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#30340;&#27700;&#21360;&#26816;&#27979;&#38656;&#35201;&#29983;&#25104;&#36807;&#31243;&#30340;&#23494;&#38053;&#65292;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#36829;&#35268;&#21644;&#20266;&#36896;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31169;&#23494;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#38454;&#27573;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#32780;&#19981;&#26159;&#20351;&#29992;&#30456;&#21516;&#30340;&#23494;&#38053;&#26469;&#25193;&#23637;&#24403;&#21069;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#32593;&#32476;&#30340;&#37096;&#20998;&#21442;&#25968;&#26159;&#20849;&#20139;&#30340;&#65292;&#36825;&#20351;&#24471;&#26816;&#27979;&#32593;&#32476;&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30001;&#20110;&#20004;&#20010;&#32593;&#32476;&#30340;&#21442;&#25968;&#35268;&#27169;&#36739;&#23567;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30830;&#20445;&#20102;&#39640;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;&#29983;&#25104;&#21644;&#26816;&#27979;&#36895;&#24230;&#30340;&#24433;&#21709;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, text watermarking algorithms for large language models (LLMs) have been mitigating the potential harms of text generated by the LLMs, including fake news and copyright issues. However, the watermark detection of current text algorithms requires the key from the generation process, making them susceptible to breaches and counterfeiting. In this work, we propose the first private watermarking algorithm, which extends the current text watermarking algorithms by using two different neural networks respectively for watermark generation and detection, rather than using the same key at both stages. Meanwhile, part of the parameters of the watermark generation and detection networks are shared, which makes the detection network achieve a high accuracy very efficiently. Experiments show that our algorithm ensures high detection accuracy with minimal impact on generation and detection speed, due to the small parameter size of both networks. Additionally, our subsequent analysis demonst
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65306;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#12290;&#22312;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20013;&#65292;&#20154;&#31867;&#25198;&#28436;&#30528;&#20027;&#23548;&#35282;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.10811</link><description>&lt;p&gt;
"&#24863;&#35273;&#20687;&#26377;&#31532;&#20108;&#20010;&#24605;&#32500;": &#25506;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#21019;&#24847;&#21487;&#20889;&#24615;&#39044;&#20889;&#30340;&#20154;&#26426;&#20849;&#21019;
&lt;/p&gt;
&lt;p&gt;
"It Felt Like Having a Second Mind": Investigating Human-AI Co-creativity in Prewriting with Large Language Models. (arXiv:2307.10811v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10811
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65306;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#12290;&#22312;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20013;&#65292;&#20154;&#31867;&#25198;&#28436;&#30528;&#20027;&#23548;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20889;&#26159;&#22312;&#31532;&#19968;&#31295;&#20043;&#21069;&#21457;&#29616;&#21644;&#21457;&#23637;&#24605;&#24819;&#30340;&#36807;&#31243;&#65292;&#23427;&#38656;&#35201;&#21457;&#25955;&#24615;&#24605;&#32500;&#65292;&#36890;&#24120;&#28041;&#21450;&#21040;&#26080;&#32467;&#26500;&#30340;&#31574;&#30053;&#65292;&#22914;&#22270;&#34920;&#12289;&#27010;&#36848;&#21644;&#33258;&#30001;&#20889;&#20316;&#31561;&#12290;&#34429;&#28982;&#24050;&#32463;&#35777;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26159;&#26377;&#29992;&#30340;&#65292;&#21253;&#25324;&#21019;&#24847;&#20889;&#20316;&#65292;&#20294;&#23545;&#29992;&#25143;&#22914;&#20309;&#19982;LLMs&#21512;&#20316;&#26469;&#25903;&#25345;&#39044;&#20889;&#30340;&#26041;&#24335;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#31181;&#21019;&#36896;&#24615;&#36807;&#31243;&#20013;&#65292;LLMs&#30340;&#39318;&#36873;&#21512;&#20316;&#35282;&#33394;&#21644;&#20027;&#21160;&#24615;&#20063;&#19981;&#26126;&#30830;&#12290;&#20026;&#20102;&#30740;&#31350;&#20154;&#31867;&#19982;LLMs&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#21644;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#19982;15&#20301;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#20004;&#20010;&#21019;&#36896;&#24615;&#20219;&#21153;&#65306;&#20889;&#25925;&#20107;&#21644;&#20889;&#21475;&#21495;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#20316;&#30340;&#39044;&#20889;&#36807;&#31243;&#20013;&#65292;&#20284;&#20046;&#23384;&#22312;&#30528;&#19968;&#20010;&#19977;&#38454;&#27573;&#36845;&#20195;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65292;&#21253;&#25324;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#38454;&#27573;&#12290;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20197;&#20154;&#31867;&#22312;&#20027;&#23548;&#35282;&#33394;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prewriting is the process of discovering and developing ideas before a first draft, which requires divergent thinking and often implies unstructured strategies such as diagramming, outlining, free-writing, etc. Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting. The preferred collaborative role and initiative of LLMs during such a creativity process is also unclear. To investigate human-LLM collaboration patterns and dynamics during prewriting, we conducted a three-session qualitative study with 15 participants in two creative tasks: story writing and slogan writing. The findings indicated that during collaborative prewriting, there appears to be a three-stage iterative Human-AI Co-creativity process that includes Ideation, Illumination, and Implementation stages. This collaborative process champions the human in a dominant role, in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35821;&#35328;&#20998;&#31867;&#26041;&#27861;&#25506;&#31350;&#21333;&#35821;BERT&#30340;&#35821;&#35328;&#23646;&#24615;&#65292;&#26680;&#24515;&#21457;&#29616;&#20026;BERT&#27491;&#22312;&#22797;&#21046;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.02215</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#20998;&#31867;&#25506;&#31350;&#21333;&#35821;BERT&#30340;&#35821;&#35328;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages. (arXiv:2305.02215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35821;&#35328;&#20998;&#31867;&#26041;&#27861;&#25506;&#31350;&#21333;&#35821;BERT&#30340;&#35821;&#35328;&#23646;&#24615;&#65292;&#26680;&#24515;&#21457;&#29616;&#20026;BERT&#27491;&#22312;&#22797;&#21046;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#30340;&#24040;&#22823;&#25104;&#21151;&#20351;&#20154;&#20204;&#20135;&#29983;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#26426;&#22120;&#26159;&#22312;&#22797;&#21046;&#26576;&#20123;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36824;&#26159;&#21457;&#29616;&#20102;&#26681;&#26412;&#24615;&#30340;&#26032;&#29702;&#35770;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35266;&#28857;&#65292;&#20351;&#29992;&#35821;&#35328;&#20043;&#38388;&#30340;&#31867;&#22411;&#30456;&#20284;&#24615;&#26469;&#23545;&#27604;&#19981;&#21516;&#35821;&#35328;&#30340;transformer&#27169;&#22411;&#65292;&#35266;&#23519;&#36825;&#20123;&#30456;&#20284;&#24615;&#26159;&#21542;&#20986;&#29616;&#22312;&#29305;&#23450;&#30340;&#23618;&#27425;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20013;&#24515;&#26680;&#23545;&#40784;&#30340;&#26435;&#37325;&#30697;&#38453;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21477;&#27861;&#31867;&#22411;&#23398;&#30456;&#20284;&#24615;&#19982;&#20013;&#38388;&#23618;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26159;&#19968;&#33268;&#30340;&#12290;&#36825;&#19968;&#21457;&#29616;&#30830;&#35748;&#20102;&#36890;&#36807;&#21477;&#27861;&#25506;&#38024;&#26041;&#27861;&#33719;&#24471;&#30340;BERT&#30340;&#32467;&#26524;&#65292;&#24182;&#22240;&#27492;&#37325;&#35201;&#22320;&#35777;&#26126;&#20102;BERT&#27491;&#22312;&#22797;&#21046;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The overwhelming success of transformers is a real conundrum stimulating a compelling question: are these machines replicating some traditional linguistic models or discovering radically new theories? In this paper, we propose a novel standpoint to investigate this important question. Using typological similarities among languages, we aim to layer-wise compare transformers for different languages to observe whether these similarities emerge for particular layers. For this investigation, we propose to use Centered kernel alignment to measure similarity among weight matrices. We discovered that syntactic typological similarity is consistent with the similarity among weights in the middle layers. This finding confirms results obtained by syntactically probing BERT and, thus, gives an important confirmation that BERT is replicating traditional linguistic models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#30701;&#26263;&#19977;&#21512;&#19968;&#27979;&#39564;&#19978;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#65292;&#23384;&#22312;&#30456;&#23545;&#36739;&#26263;&#30340;&#20154;&#26684;&#27169;&#24335;&#12290;&#23613;&#31649;&#32463;&#36807;&#25351;&#26631;&#24494;&#35843;&#65292;&#20004;&#31181;&#27169;&#22411;&#20173;&#21576;&#29616;&#38544;&#21547;&#30340;&#40657;&#26263;&#20154;&#26684;&#27169;&#24335;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35266;&#23519;&#21040;GPT-3&#21644;InstructGPT&#30340;&#24184;&#31119;&#24863;&#24471;&#20998;&#25345;&#32493;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2212.10529</link><description>&lt;p&gt;
GPT-3&#26159;&#21542;&#23637;&#31034;&#20986;&#31934;&#31070;&#30149;&#24577;&#65311;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models from a Psychological Perspective. (arXiv:2212.10529v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#30701;&#26263;&#19977;&#21512;&#19968;&#27979;&#39564;&#19978;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#65292;&#23384;&#22312;&#30456;&#23545;&#36739;&#26263;&#30340;&#20154;&#26684;&#27169;&#24335;&#12290;&#23613;&#31649;&#32463;&#36807;&#25351;&#26631;&#24494;&#35843;&#65292;&#20004;&#31181;&#27169;&#22411;&#20173;&#21576;&#29616;&#38544;&#21547;&#30340;&#40657;&#26263;&#20154;&#26684;&#27169;&#24335;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35266;&#23519;&#21040;GPT-3&#21644;InstructGPT&#30340;&#24184;&#31119;&#24863;&#24471;&#20998;&#25345;&#32493;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#30830;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26080;&#20559;&#30340;&#25552;&#31034;&#26469;&#31995;&#32479;&#24615;&#22320;&#35780;&#20272;LLMs&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#20154;&#26684;&#27979;&#35797;&#8212;&#8212;&#30701;&#26263;&#19977;&#21512;&#19968;&#27979;&#39564;&#65288;SD-3&#65289;&#21644;&#22823;&#20116;&#20154;&#26684;&#38382;&#21367;&#65288;BFI&#65289;&#27979;&#35797;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;LLMs&#12290;&#25152;&#26377;&#27169;&#22411;&#22312;SD-3&#19978;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#65292;&#34920;&#26126;&#23384;&#22312;&#30456;&#23545;&#36739;&#26263;&#30340;&#20154;&#26684;&#27169;&#24335;&#12290;&#23613;&#31649;&#32463;&#36807;&#25351;&#26631;&#24494;&#35843;&#20197;&#20943;&#23569;&#27602;&#24615;&#65292;InstructGPT&#21644;FLAN-T5&#20173;&#28982;&#21576;&#29616;&#20986;&#38544;&#21547;&#30340;&#40657;&#26263;&#20154;&#26684;&#27169;&#24335;&#65307;&#22312;SD-3&#30340;&#29595;&#22522;&#38597;&#32500;&#21033;&#20027;&#20041;&#21644;&#33258;&#24651;&#29378;&#29305;&#24449;&#19978;&#65292;&#36825;&#20004;&#31181;&#27169;&#22411;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#33258;&#30417;&#30563;GPT-3&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24184;&#31119;&#24863;&#27979;&#35797;&#35780;&#20272;&#20102;GPT-3&#31995;&#21015;&#20013;&#30340;LLMs&#65292;&#20197;&#30740;&#31350;&#26356;&#22810;&#35757;&#32451;&#25968;&#25454;&#30340;&#24494;&#35843;&#23545;&#20854;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;GPT-3&#21644;InstructGPT&#30340;&#24184;&#31119;&#24863;&#24471;&#20998;&#25345;&#32493;&#22686;&#21152;&#12290;&#37492;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#27491;&#38754;&#22238;&#31572;&#20174;&#32780;&#25351;&#26631;&#24494;&#35843;FLAN-T5&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we determined whether large language models (LLMs) are psychologically safe. We designed unbiased prompts to systematically evaluate LLMs from a psychological perspective. First, we tested three different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored higher than the human average on SD-3, suggesting a relatively darker personality pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT and FLAN-T5 still showed implicit dark personality patterns; both models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT-3 series by using well-being tests to study the impact of fine-tuning with more training data. We observed a continuous increase in the well-being scores of GPT-3 and InstructGPT. Following these observations, we showed that instruction fine-tuning FLAN-T5 with positive answers from 
&lt;/p&gt;</description></item></channel></rss>