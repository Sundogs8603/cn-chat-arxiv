<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22235;&#20010;&#27969;&#34892;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#30340;&#21487;&#39564;&#35777;&#24615;&#65292;&#21457;&#29616;&#29616;&#26377;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#21709;&#24212;&#27969;&#30021;&#20294;&#20165;&#26377;51.5%&#30340;&#29983;&#25104;&#21477;&#23376;&#24471;&#21040;&#20102;&#23436;&#25972;&#30340;&#24341;&#29992;&#25903;&#25345;&#65292;&#20165;&#26377;74.5%&#30340;&#24341;&#29992;&#25903;&#25345;&#20854;&#30456;&#20851;&#35821;&#21477;&#12290;</title><link>http://arxiv.org/abs/2304.09848</link><description>&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#21487;&#39564;&#35777;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Verifiability in Generative Search Engines. (arXiv:2304.09848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22235;&#20010;&#27969;&#34892;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#30340;&#21487;&#39564;&#35777;&#24615;&#65292;&#21457;&#29616;&#29616;&#26377;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#21709;&#24212;&#27969;&#30021;&#20294;&#20165;&#26377;51.5%&#30340;&#29983;&#25104;&#21477;&#23376;&#24471;&#21040;&#20102;&#23436;&#25972;&#30340;&#24341;&#29992;&#25903;&#25345;&#65292;&#20165;&#26377;74.5%&#30340;&#24341;&#29992;&#25903;&#25345;&#20854;&#30456;&#20851;&#35821;&#21477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#30452;&#25509;&#20026;&#29992;&#25143;&#26597;&#35810;&#29983;&#25104;&#21709;&#24212;&#65292;&#24182;&#25552;&#20379;&#20869;&#32852;&#24341;&#29992;&#12290;&#19968;&#20010;&#20540;&#24471;&#20449;&#36182;&#30340;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#30340;&#20808;&#20915;&#26465;&#20214;&#26159;&#21487;&#39564;&#35777;&#24615;&#65292;&#21363;&#31995;&#32479;&#24212;&#20840;&#38754;&#24341;&#29992;&#65288;&#39640;&#24341;&#29992;&#22238;&#24518;&#29575;&#65292;&#25152;&#26377;&#35821;&#21477;&#37117;&#26377;&#23436;&#25972;&#30340;&#24341;&#29992;&#25903;&#25345;&#65289;&#21644;&#20934;&#30830;&#65288;&#39640;&#24341;&#29992;&#31934;&#24230;&#65292;&#27599;&#20010;&#24341;&#29992;&#37117;&#25903;&#25345;&#20854;&#30456;&#20851;&#35821;&#21477;&#65289;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#27969;&#34892;&#30340;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#8212;&#8212;Bing Chat&#12289;NeevaAI&#12289;perplexity.ai&#21644;YouChat&#8212;&#8212;&#36827;&#34892;&#20102;&#20154;&#31867;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26469;&#28304;&#30340;&#22810;&#26679;&#21270;&#26597;&#35810;&#65288;&#20363;&#22914;&#21382;&#21490;&#19978;&#30340;Google&#29992;&#25143;&#26597;&#35810;&#12289;Reddit&#19978;&#21160;&#24577;&#25910;&#38598;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#31561;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#21709;&#24212;&#27969;&#30021;&#19988;&#20449;&#24687;&#20016;&#23500;&#65292;&#20294;&#24120;&#24120;&#21253;&#21547;&#19981;&#25903;&#25345;&#30340;&#35821;&#21477;&#21644;&#19981;&#20934;&#30830;&#30340;&#24341;&#29992;&#65306;&#24179;&#22343;&#32780;&#35328;&#65292;&#20165;&#26377;51.5%&#30340;&#29983;&#25104;&#21477;&#23376;&#24471;&#21040;&#20102;&#23436;&#25972;&#30340;&#24341;&#29992;&#25903;&#25345;&#65292;&#21482;&#26377;74.5%&#30340;&#24341;&#29992;&#25903;&#25345;&#20854;&#30456;&#20851;&#35821;&#21477;&#12290;&#25105;&#20204;&#35748;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and accurately (high citation precision; every cite supports its associated statement). We conduct human evaluation to audit four popular generative search engines -- Bing Chat, NeevaAI, perplexity.ai, and YouChat -- across a diverse set of queries from a variety of sources (e.g., historical Google user queries, dynamically-collected open-ended questions on Reddit, etc.). We find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5% of generated sentences are fully supported by citations and only 74.5% of citations support their associated sentence. We believe that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#20250;&#21152;&#28145;&#20559;&#35265;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#65292;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#23547;&#27714;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.09826</link><description>&lt;p&gt;
AI&#30340;&#20844;&#24179;&#24615;&#21450;&#20854;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Fairness in AI and Its Long-Term Implications on Society. (arXiv:2304.09826v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#20250;&#21152;&#28145;&#20559;&#35265;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#65292;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#23547;&#27714;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#30340;&#25104;&#21151;&#37096;&#32626;&#24050;&#32463;&#20026;&#20010;&#20154;&#21644;&#31038;&#20250;&#24102;&#26469;&#20102;&#35768;&#22810;&#31215;&#26497;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#27979;&#30340;&#20559;&#35265;&#65292;AI&#31995;&#32479;&#20063;&#34987;&#35777;&#26126;&#23545;&#37096;&#20998;&#20154;&#21475;&#36896;&#25104;&#20102;&#20260;&#23475;&#12290;&#25105;&#20204;&#30528;&#30524;&#20110;AI&#30340;&#20844;&#24179;&#24615;&#65292;&#20998;&#26512;&#20102;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#26102;&#22914;&#20309;&#23548;&#33268;&#20559;&#35265;&#38543;&#30528;&#26102;&#38388;&#30340;&#21152;&#28145;&#32780;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#12290;&#22914;&#26524;&#38382;&#39064;&#25345;&#32493;&#23384;&#22312;&#65292;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#30340;&#38271;&#26399;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#19982;&#20854;&#20182;&#39118;&#38505;&#30340;&#20132;&#20114;&#26469;&#21152;&#24378;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#25552;&#39640;AI&#20844;&#24179;&#24615;&#30340;&#24403;&#21069;&#31574;&#30053;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#23454;&#38469;&#37096;&#32626;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#30830;&#20445;&#25105;&#20204;&#22312;&#19981;&#25439;&#23475;&#31038;&#20250;&#37325;&#35201;&#37096;&#20998;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;AI&#30340;&#22909;&#22788;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful deployment of artificial intelligence (AI) in various settings has led to numerous positive outcomes for individuals and society. However, AI systems have also been shown to harm parts of the population due to biased predictions. We take a closer look at AI fairness and analyse how lack of AI fairness can lead to deepening of biases over time and act as a social stressor. If the issues persist, it could have undesirable long-term implications on society, reinforced by interactions with other risks. We examine current strategies for improving AI fairness, assess their limitations in terms of real-world deployment, and explore potential paths forward to ensure we reap AI's benefits without harming significant parts of the society.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#21644;&#26469;&#33258;&#19981;&#21516;&#20294;&#30456;&#20851;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#23436;&#25104;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#65292;&#21462;&#24471;&#22312;&#21333;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#19978;&#30340;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09820</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#30417;&#30563;&#33976;&#39311;&#30340;&#21452;&#38454;&#27573;&#26694;&#26550;&#29992;&#20110;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Framework with Self-Supervised Distillation For Cross-Domain Text Classification. (arXiv:2304.09820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#21644;&#26469;&#33258;&#19981;&#21516;&#20294;&#30456;&#20851;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#23436;&#25104;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#65292;&#21462;&#24471;&#22312;&#21333;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#19978;&#30340;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#32570;&#23569;&#26631;&#35760;&#25968;&#25454;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#23427;&#21033;&#29992;&#25110;&#37325;&#29992;&#19981;&#21516;&#20294;&#30456;&#20851;&#28304;&#39046;&#22495;&#30340;&#20016;&#23500;&#26631;&#35760;&#25968;&#25454;&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#19987;&#27880;&#20110;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#35201;&#20040;&#24573;&#30053;&#21487;&#33021;&#23384;&#22312;&#20110;&#30446;&#26631;&#39046;&#22495;&#20013;&#24182;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#29992;&#30340;&#39046;&#22495;&#24863;&#30693;&#29305;&#24449;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#25513;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#21644;&#26469;&#33258;&#28304;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#24494;&#35843;&#27169;&#22411;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#65288;SSD&#65289;&#21644;&#26469;&#33258;&#30446;&#26631;&#22495;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#22522;&#20110;&#20844;&#20849;&#30340;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20854;&#24615;&#33021;&#65292;&#24182;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#19978;&#22343;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain text classification aims to adapt models to a target domain that lacks labeled data. It leverages or reuses rich labeled data from the different but related source domain(s) and unlabeled data from the target domain. To this end, previous work focuses on either extracting domain-invariant features or task-agnostic features, ignoring domain-aware features that may be present in the target domain and could be useful for the downstream task. In this paper, we propose a two-stage framework for cross-domain text classification. In the first stage, we finetune the model with mask language modeling (MLM) and labeled data from the source domain. In the second stage, we further fine-tune the model with self-supervised distillation (SSD) and unlabeled data from the target domain. We evaluate its performance on a public cross-domain text classification benchmark and the experiment results show that our method achieves new state-of-the-art results for both single-source domain adaptat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#24503;&#35821;&#20302;&#36164;&#28304;&#35821;&#35328;&#21464;&#20307;&#65292;&#36890;&#36807;&#31995;&#32479;&#30340;&#35843;&#26597;&#35821;&#26009;&#24211;&#65292;&#21457;&#29616;&#25163;&#24037;&#27880;&#37322;&#30340;&#35821;&#35328;&#36164;&#28304;&#31232;&#23569;&#65292;&#20027;&#35201;&#28085;&#30422;&#24418;&#24577;&#21477;&#27861;&#65292;&#21516;&#26102;&#35266;&#23519;&#21040;&#27492;&#31867;&#30740;&#31350;&#30340;&#20852;&#36259;&#27491;&#22312;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2304.09805</link><description>&lt;p&gt;
&#19968;&#20221;&#20851;&#20110;&#26085;&#32819;&#26364;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;&#26041;&#35328;&#35821;&#26009;&#24211;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Corpora for Germanic Low-Resource Languages and Dialects. (arXiv:2304.09805v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24503;&#35821;&#20302;&#36164;&#28304;&#35821;&#35328;&#21464;&#20307;&#65292;&#36890;&#36807;&#31995;&#32479;&#30340;&#35843;&#26597;&#35821;&#26009;&#24211;&#65292;&#21457;&#29616;&#25163;&#24037;&#27880;&#37322;&#30340;&#35821;&#35328;&#36164;&#28304;&#31232;&#23569;&#65292;&#20027;&#35201;&#28085;&#30422;&#24418;&#24577;&#21477;&#27861;&#65292;&#21516;&#26102;&#35266;&#23519;&#21040;&#27492;&#31867;&#30740;&#31350;&#30340;&#20852;&#36259;&#27491;&#22312;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#26159;&#38024;&#23545;&#20351;&#29992;&#24191;&#27867;&#30340;&#26631;&#20934;&#35821;&#35328;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#38750;&#26631;&#20934;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#12290;&#21363;&#20351;&#22312;&#34987;&#35748;&#20026;&#24050;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#30340;&#20027;&#35201;&#35821;&#31995;&#20013;&#65292;&#23545;&#20110;&#36825;&#20123;&#35821;&#35328;&#21464;&#20307;&#21487;&#29992;&#36164;&#28304;&#30340;&#31867;&#22411;&#21644;&#33539;&#22260;&#20197;&#21450;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#30340;&#31532;&#19968;&#27493;&#26159;&#31995;&#32479;&#22320;&#35843;&#26597;&#21487;&#29992;&#30340;&#35821;&#26009;&#24211;&#65288;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25163;&#24037;&#27880;&#37322;&#30340;&#35821;&#26009;&#24211;&#65292;&#36825;&#23545;NLP&#30740;&#31350;&#29305;&#21035;&#26377;&#20215;&#20540;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#24503;&#35821;&#20302;&#36164;&#28304;&#35821;&#35328;&#21464;&#20307;&#25552;&#20379;&#20102;&#36825;&#26679;&#30340;&#35843;&#26597;&#12290;&#38500;&#20102;&#22320;&#29702;&#20301;&#32622;&#65288;&#35828;&#35805;&#32773;&#25110;&#25991;&#26723;&#30340;&#26469;&#28304;&#65289;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25163;&#24037;&#27880;&#37322;&#30340;&#35821;&#35328;&#36164;&#28304;&#31232;&#23569;&#65292;&#22914;&#26524;&#23384;&#22312;&#65292;&#20027;&#35201;&#28085;&#30422;&#24418;&#24577;&#21477;&#27861;&#12290;&#23613;&#31649;&#32570;&#20047;&#36164;&#28304;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#19968;&#39046;&#22495;&#30340;&#20852;&#36259;&#27491;&#22312;&#22686;&#21152;&#65306;
&lt;/p&gt;
&lt;p&gt;
Despite much progress in recent years, the vast majority of work in natural language processing (NLP) is on standard languages with many speakers. In this work, we instead focus on low-resource languages and in particular non-standardized low-resource languages. Even within branches of major language families, often considered well-researched, little is known about the extent and type of available resources and what the major NLP challenges are for these language varieties. The first step to address this situation is a systematic survey of available corpora (most importantly, annotated corpora, which are particularly valuable for NLP research). Focusing on Germanic low-resource language varieties, we provide such a survey in this paper. Except for geolocation (origin of speaker or document), we find that manually annotated linguistic resources are sparse and, if they exist, mostly cover morphosyntax. Despite this lack of resources, we observe that interest in this area is increasing: t
&lt;/p&gt;</description></item><item><title>GeneGPT&#36890;&#36807;&#23569;&#37327;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#28436;&#31034;&#65292;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#65292;&#24182;&#22312;GeneTuring&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09667</link><description>&lt;p&gt;
GeneGPT: &#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API
&lt;/p&gt;
&lt;p&gt;
GeneGPT: Teaching Large Language Models to Use NCBI Web APIs. (arXiv:2304.09667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09667
&lt;/p&gt;
&lt;p&gt;
GeneGPT&#36890;&#36807;&#23569;&#37327;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#28436;&#31034;&#65292;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#65292;&#24182;&#22312;GeneTuring&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GeneGPT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20351;&#29992;&#22269;&#23478;&#29983;&#29289;&#25216;&#26415;&#20449;&#24687;&#20013;&#24515;&#65288;NCBI&#65289;&#30340;Web&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;API&#65289;&#65292;&#24182;&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23569;&#37327;&#30340;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#65292;&#21551;&#21457;Codex&#65288;code-davinci-002&#65289;&#35299;&#20915;GeneTuring&#27979;&#35797;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#19968;&#26086;&#26816;&#27979;&#21040;&#35843;&#29992;&#35831;&#27714;&#65292;&#25105;&#20204;&#23601;&#20572;&#27490;&#35299;&#30721;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;URL&#36827;&#34892;API&#35843;&#29992;&#12290;&#25105;&#20204;&#28982;&#21518;&#23558;NCBI API&#36820;&#22238;&#30340;&#21407;&#22987;&#25191;&#34892;&#32467;&#26524;&#38468;&#21152;&#21040;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#24182;&#32487;&#32493;&#29983;&#25104;&#30452;&#21040;&#25214;&#21040;&#31572;&#26696;&#25110;&#26816;&#27979;&#21040;&#21478;&#19968;&#20010;API&#35843;&#29992;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;GeneGPT&#22312;GeneTuring&#25968;&#25454;&#38598;&#30340;&#22235;&#20010;One-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;&#20116;&#20010;Zero-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;GeneGPT&#30340;&#23439;&#24179;&#22343;&#20998;&#25968;&#20026;0.76&#65292;&#36828;&#39640;&#20110;&#26816;&#32034;&#22686;&#24378;LLM&#65292;&#22914;New Bin&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present GeneGPT, a novel method for teaching large language models (LLMs) to use the Web Application Programming Interfaces (APIs) of the National Center for Biotechnology Information (NCBI) and answer genomics questions. Specifically, we prompt Codex (code-davinci-002) to solve the GeneTuring tests with few-shot URL requests of NCBI API calls as demonstrations for in-context learning. During inference, we stop the decoding once a call request is detected and make the API call with the generated URL. We then append the raw execution results returned by NCBI APIs to the generated texts and continue the generation until the answer is found or another API call is detected. Our preliminary results show that GeneGPT achieves state-of-the-art results on three out of four one-shot tasks and four out of five zero-shot tasks in the GeneTuring dataset. Overall, GeneGPT achieves a macro-average score of 0.76, which is much higher than retrieval-augmented LLMs such as the New Bin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20135;&#21697;&#25163;&#20876;&#38382;&#31572;&#65288;MPMQA&#65289;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;PM209&#26469;&#25903;&#25345;&#35813;&#20219;&#21153;&#65292;&#35201;&#27714;&#27169;&#22411;&#19981;&#20165;&#22788;&#29702;&#22810;&#27169;&#24577;&#20869;&#23481;&#65292;&#36824;&#25552;&#20379;&#22810;&#27169;&#24577;&#31572;&#26696;&#65292;&#26377;&#26395;&#25552;&#39640;&#20135;&#21697;&#25163;&#20876;&#29702;&#35299;&#25928;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09660</link><description>&lt;p&gt;
MPMQA&#65306;&#22522;&#20110;&#20135;&#21697;&#25163;&#20876;&#30340;&#22810;&#27169;&#24577;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
MPMQA: Multimodal Question Answering on Product Manuals. (arXiv:2304.09660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20135;&#21697;&#25163;&#20876;&#38382;&#31572;&#65288;MPMQA&#65289;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;PM209&#26469;&#25903;&#25345;&#35813;&#20219;&#21153;&#65292;&#35201;&#27714;&#27169;&#22411;&#19981;&#20165;&#22788;&#29702;&#22810;&#27169;&#24577;&#20869;&#23481;&#65292;&#36824;&#25552;&#20379;&#22810;&#27169;&#24577;&#31572;&#26696;&#65292;&#26377;&#26395;&#25552;&#39640;&#20135;&#21697;&#25163;&#20876;&#29702;&#35299;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20869;&#23481;&#65292;&#22312;&#20135;&#21697;&#25163;&#20876;&#29702;&#35299;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#29616;&#26377;&#30340;&#20135;&#21697;&#25163;&#20876;&#38382;&#31572;&#65288;PMQA&#65289;&#25968;&#25454;&#38598;&#24448;&#24448;&#24573;&#35270;&#20102;&#35270;&#35273;&#20869;&#23481;&#65292;&#20165;&#20445;&#30041;&#25991;&#26412;&#37096;&#20998;&#12290;&#20026;&#20102;&#24378;&#35843;&#22810;&#27169;&#24577;&#20869;&#23481;&#30340;&#37325;&#35201;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20135;&#21697;&#25163;&#20876;&#38382;&#31572;&#65288;MPMQA&#65289;&#20219;&#21153;&#12290;&#23545;&#20110;&#27599;&#20010;&#38382;&#39064;&#65292;MPMQA&#35201;&#27714;&#27169;&#22411;&#19981;&#20165;&#35201;&#22788;&#29702;&#22810;&#27169;&#24577;&#20869;&#23481;&#65292;&#36824;&#35201;&#25552;&#20379;&#22810;&#27169;&#24577;&#31572;&#26696;&#12290;&#20026;&#20102;&#25903;&#25345;MPMQA&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;PM209&#20316;&#20026;&#20154;&#31867;&#27880;&#37322;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;27&#20010;&#30693;&#21517;&#28040;&#36153;&#30005;&#23376;&#21697;&#29260;&#30340;209&#20010;&#20135;&#21697;&#25163;&#20876;&#12290;&#20154;&#31867;&#27880;&#37322;&#21253;&#25324;&#25163;&#20876;&#20869;&#23481;&#30340;6&#31181;&#35821;&#20041;&#21306;&#22495;&#21644;22,021&#23545;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#29305;&#21035;&#22320;&#65292;&#27599;&#20010;&#31572;&#26696;&#37117;&#21253;&#21547;&#19968;&#20010;&#25991;&#26412;&#21477;&#23376;&#21644;&#30456;&#20851;&#30340;&#25163;&#20876;&#35270;&#35273;&#21306;&#22495;&#12290;&#32771;&#34385;&#21040;&#20135;&#21697;&#25163;&#20876;&#30340;&#38271;&#24230;&#21644;&#19968;&#20010;&#38382;&#39064;&#24635;&#26159;&#19982;&#23569;&#25968;&#39029;&#38754;&#30456;&#20851;&#65292;MPMQA&#30340;&#24615;&#33021;&#21487;&#20197;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual contents, such as illustrations and images, play a big role in product manual understanding. Existing Product Manual Question Answering (PMQA) datasets tend to ignore visual contents and only retain textual parts. In this work, to emphasize the importance of multimodal contents, we propose a Multimodal Product Manual Question Answering (MPMQA) task. For each question, MPMQA requires the model not only to process multimodal contents but also to provide multimodal answers. To support MPMQA, a large-scale dataset PM209 is constructed with human annotations, which contains 209 product manuals from 27 well-known consumer electronic brands. Human annotations include 6 types of semantic regions for manual contents and 22,021 pairs of question and answer. Especially, each answer consists of a textual sentence and related visual regions from manuals. Taking into account the length of product manuals and the fact that a question is always related to a small number of pages, MPMQA can be n
&lt;/p&gt;</description></item><item><title>BRENT&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#21452;&#21521;&#26816;&#32034;&#25552;&#39640;&#25386;&#23041;&#35821;&#30340;&#26816;&#32034;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#35835;&#32773;&#22312;&#25552;&#21462;&#38382;&#31572;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.09649</link><description>&lt;p&gt;
BRENT: &#21452;&#21521;&#26816;&#32034;&#22686;&#24378;&#30340;&#25386;&#23041;&#35821;Transformer
&lt;/p&gt;
&lt;p&gt;
BRENT: Bidirectional Retrieval Enhanced Norwegian Transformer. (arXiv:2304.09649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09649
&lt;/p&gt;
&lt;p&gt;
BRENT&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#21452;&#21521;&#26816;&#32034;&#25552;&#39640;&#25386;&#23041;&#35821;&#30340;&#26816;&#32034;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#35835;&#32773;&#22312;&#25552;&#21462;&#38382;&#31572;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#37325;&#35270;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#35821;&#26009;&#24211;&#20013;&#25628;&#32034;&#30456;&#20851;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#23558;&#25152;&#26377;&#20107;&#23454;&#24615;&#30693;&#35782;&#23384;&#20648;&#22312;&#23427;&#30340;&#21442;&#25968;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#12289;&#36879;&#26126;&#24230;&#21644;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35843;&#25972;REALM&#26694;&#26550;&#26469;&#24320;&#21457;&#31532;&#19968;&#20010;&#25386;&#23041;&#35821;&#26816;&#32034;&#24335;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#22312;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#36824;&#23558;&#26816;&#32034;&#32452;&#20214;&#19982;&#35821;&#35328;&#27169;&#22411;&#65288;&#31216;&#20026;&#35835;&#32773;&#65289;&#20998;&#31163;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#21487;&#20197;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#24314;&#27169;&#25552;&#39640;&#20102;&#35835;&#32773;&#22312;&#25552;&#21462;&#38382;&#31572;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#36825;&#34920;&#26126;&#36825;&#31181;&#31867;&#22411;&#30340;&#35757;&#32451;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#19978;&#19979;&#25991;&#30340;&#19968;&#33324;&#33021;&#21147;&#65292;&#32780;&#36825;&#24182;&#19981;&#20250;&#29306;&#29298;&#20854;&#20182;&#33021;&#21147;&#65292;&#20363;&#22914;&#35789;&#24615;&#26631;&#27880;&#12289;&#20381;&#36182;&#20998;&#26512;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#35789;&#24418;&#24402;&#24182;&#12290;&#20195;&#30721;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/salaniz/BRENT&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-based language models are increasingly employed in question-answering tasks. These models search in a corpus of documents for relevant information instead of having all factual knowledge stored in its parameters, thereby enhancing efficiency, transparency, and adaptability. We develop the first Norwegian retrieval-based model by adapting the REALM framework and evaluating it on various tasks. After training, we also separate the language model, which we call the reader, from the retriever components, and show that this can be fine-tuned on a range of downstream tasks. Results show that retrieval augmented language modeling improves the reader's performance on extractive question-answering, suggesting that this type of training improves language models' general ability to use context and that this does not happen at the expense of other abilities such as part-of-speech tagging, dependency parsing, named entity recognition, and lemmatization. Code, trained models, and data are 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35745;&#31639;&#30340;&#35789;&#35821;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24357;&#34917;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#65292;&#36890;&#36807;&#25552;&#20379;&#22823;&#37327;&#25511;&#21046;&#20102;&#22312;&#35789;&#27719;&#22788;&#29702;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#21464;&#37327;&#30340;&#21517;&#35789;&#23545;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#37327;&#21270;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;&#24052;&#26031;&#20811;&#35821;&#21644;&#27431;&#27954;&#35199;&#29677;&#29273;&#35821;&#30340;&#21517;&#35789;&#23545;&#20449;&#24687;&#65292;&#20294;&#36827;&#19968;&#27493;&#30340;&#24037;&#20316;&#24847;&#22270;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#22810;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2304.09616</link><description>&lt;p&gt;
&#36830;&#25509;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;&#24515;&#29702;&#35821;&#35328;&#23398;&#65306;&#38754;&#21521;&#24052;&#26031;&#20811;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#22522;&#20110;&#35821;&#26009;&#24211;&#21644;&#30693;&#35782;&#24211;&#30340;&#35745;&#31639;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Bridging Natural Language Processing and Psycholinguistics: computationally grounded semantic similarity and relatedness datasets for Basque and Spanish. (arXiv:2304.09616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09616
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35745;&#31639;&#30340;&#35789;&#35821;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24357;&#34917;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#65292;&#36890;&#36807;&#25552;&#20379;&#22823;&#37327;&#25511;&#21046;&#20102;&#22312;&#35789;&#27719;&#22788;&#29702;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#21464;&#37327;&#30340;&#21517;&#35789;&#23545;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#37327;&#21270;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;&#24052;&#26031;&#20811;&#35821;&#21644;&#27431;&#27954;&#35199;&#29677;&#29273;&#35821;&#30340;&#21517;&#35789;&#23545;&#20449;&#24687;&#65292;&#20294;&#36827;&#19968;&#27493;&#30340;&#24037;&#20316;&#24847;&#22270;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#22810;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22522;&#20110;&#20004;&#20010;&#33879;&#21517;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#65306;&#25991;&#26412;&#35821;&#26009;&#24211;&#21644;&#30693;&#35782;&#24211;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35745;&#31639;&#30340;&#35789;&#35821;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#24357;&#34917;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#65292;&#36890;&#36807;&#25552;&#20379;&#22823;&#37327;&#25511;&#21046;&#20102;&#22312;&#35789;&#27719;&#22788;&#29702;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#21464;&#37327;&#30340;&#21517;&#35789;&#23545;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#37327;&#21270;&#12290;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;1&#65289;&#20026;&#27599;&#20010;&#21517;&#35789;&#35745;&#31639;&#22235;&#20010;&#20851;&#38190;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24449;&#65306;&#20855;&#20307;&#24615;&#12289;&#39057;&#29575;&#12289;&#35821;&#20041;&#21644;&#38899;&#20301;&#37051;&#36817;&#23494;&#24230;&#65307;2&#65289;&#22312;&#36825;&#20123;&#22235;&#20010;&#21464;&#37327;&#19979;&#23545;&#21517;&#35789;&#36827;&#34892;&#37197;&#23545;&#65307;3&#65289;&#23545;&#20110;&#27599;&#20010;&#21517;&#35789;&#23545;&#65292;&#20998;&#37197;&#19977;&#31181;&#31867;&#22411;&#30340;&#21333;&#35789;&#30456;&#20284;&#24230;&#27979;&#37327;&#65292;&#35745;&#31639;&#20986;&#25991;&#26412;&#12289;Wordnet&#21644;&#28151;&#21512;&#23884;&#20837;&#12290;&#30446;&#21069;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#24052;&#26031;&#20811;&#35821;&#21644;&#27431;&#27954;&#35199;&#29677;&#29273;&#35821;&#30340;&#21517;&#35789;&#23545;&#20449;&#24687;&#65292;&#20294;&#36827;&#19968;&#27493;&#30340;&#24037;&#20316;&#24847;&#22270;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#22810;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a computationally-grounded word similarity dataset based on two well-known Natural Language Processing resources; text corpora and knowledge bases. This dataset aims to fulfil a gap in psycholinguistic research by providing a variety of quantifications of semantic similarity in an extensive set of noun pairs controlled by variables that play a significant role in lexical processing. The dataset creation has consisted in three steps, 1) computing four key psycholinguistic features for each noun; concreteness, frequency, semantic and phonological neighbourhood density; 2) pairing nouns across these four variables; 3) for each noun pair, assigning three types of word similarity measurements, computed out of text, Wordnet and hybrid embeddings. The present dataset includes noun pairs' information in Basque and European Spanish, but further work intends to extend it to more languages.
&lt;/p&gt;</description></item><item><title>CB-Conformer &#26159;&#19968;&#31181;&#20026;&#20102;&#25552;&#39640;&#26377;&#20559;&#24046;&#35789;&#35782;&#21035;&#32780;&#25552;&#20986;&#30340; Conformer&#65292;&#24341;&#20837;&#20102; Contextual Biasing Module &#21644; Self-Adaptive Language Model &#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#21333;&#35789;&#20559;&#24046;&#20449;&#24687;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.09607</link><description>&lt;p&gt;
CB-Conformer: &#38024;&#23545;&#20559;&#24046;&#35789;&#35782;&#21035;&#30340;&#35821;&#22659;&#20559;&#24046;Conformer
&lt;/p&gt;
&lt;p&gt;
CB-Conformer: Contextual biasing Conformer for biased word recognition. (arXiv:2304.09607v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09607
&lt;/p&gt;
&lt;p&gt;
CB-Conformer &#26159;&#19968;&#31181;&#20026;&#20102;&#25552;&#39640;&#26377;&#20559;&#24046;&#35789;&#35782;&#21035;&#32780;&#25552;&#20986;&#30340; Conformer&#65292;&#24341;&#20837;&#20102; Contextual Biasing Module &#21644; Self-Adaptive Language Model &#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#21333;&#35789;&#20559;&#24046;&#20449;&#24687;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22914;&#20309;&#26356;&#22909;&#22320;&#21033;&#29992;&#26377;&#20559;&#24046;&#30340;&#21333;&#35789;&#20449;&#24687;&#26469;&#25552;&#39640;&#30446;&#26631;&#39046;&#22495;&#20013;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#35805;&#39064;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#22266;&#23450;&#30340;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#30721;&#65292;&#35201;&#20040;&#24341;&#20837;&#19968;&#20010;&#24222;&#22823;&#30340;&#20559;&#24046;&#27169;&#22359;&#65292;&#23548;&#33268;&#36866;&#24212;&#24615;&#24046;&#65292;&#25512;&#29702;&#36895;&#24230;&#24930;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CB-Conformer&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#20559;&#24046;&#27169;&#22359;&#21644;&#33258;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#26377;&#20559;&#24046;&#30340;&#21333;&#35789;&#35782;&#21035;&#12290;&#19978;&#19979;&#25991;&#20559;&#24046;&#27169;&#22359;&#23558;&#38899;&#39057;&#29255;&#27573;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#32452;&#21512;&#36215;&#26469;&#65292;&#20165;&#26377;&#21407;&#22987;Conformer&#27169;&#22411;&#21442;&#25968;&#30340;0.2&#65285;&#12290;&#33258;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#26377;&#20559;&#24046;&#30340;&#21333;&#35789;&#21484;&#22238;&#29575;&#21644;&#31934;&#30830;&#24230;&#20462;&#25913;&#20854;&#20869;&#37096;&#26435;&#37325;&#65292;&#20351;&#24471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#26356;&#20851;&#27880;&#26377;&#20559;&#24046;&#30340;&#21333;&#35789;&#65292;&#24182;&#27604;&#26631;&#20934;&#30340;&#22266;&#23450;&#35821;&#35328;&#27169;&#22411;&#26356;&#25104;&#21151;&#22320;&#38598;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#35821;&#38899;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#27979;&#35797;&#38598;&#65292;&#21457;&#29616;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the mismatch between the source and target domains, how to better utilize the biased word information to improve the performance of the automatic speech recognition model in the target domain becomes a hot research topic. Previous approaches either decode with a fixed external language model or introduce a sizeable biasing module, which leads to poor adaptability and slow inference. In this work, we propose CB-Conformer to improve biased word recognition by introducing the Contextual Biasing Module and the Self-Adaptive Language Model to vanilla Conformer. The Contextual Biasing Module combines audio fragments and contextual information, with only 0.2% model parameters of the original Conformer. The Self-Adaptive Language Model modifies the internal weights of biased words based on their recall and precision, resulting in a greater focus on biased words and more successful integration with the automatic speech recognition model than the standard fixed language model. In addition
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#21457;&#32773;OpenAI&#30340;ChatGPT&#24773;&#24863;&#23545;&#35805;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#24773;&#24863;&#22238;&#22797;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#24773;&#24863;&#23545;&#35805;&#29702;&#35299;&#26041;&#38754;&#33853;&#21518;&#20110;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.09582</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#20855;&#22791;&#24773;&#24863;&#23545;&#35805;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT Equipped with Emotional Dialogue Capabilities?. (arXiv:2304.09582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#21457;&#32773;OpenAI&#30340;ChatGPT&#24773;&#24863;&#23545;&#35805;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#24773;&#24863;&#22238;&#22797;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#24773;&#24863;&#23545;&#35805;&#29702;&#35299;&#26041;&#38754;&#33853;&#21518;&#20110;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;OpenAI&#24320;&#21457;&#30340;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24773;&#24863;&#23545;&#35805;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;ChatGPT&#22312;&#24773;&#24863;&#23545;&#35805;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;ChatGPT&#22312;&#24773;&#24863;&#23545;&#35805;&#29702;&#35299;&#26041;&#38754;&#30340;&#34920;&#29616;&#20173;&#28982;&#33853;&#21518;&#20110;&#30417;&#30563;&#27169;&#22411;&#65292;&#20294;&#22312;&#29983;&#25104;&#24773;&#24863;&#22238;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report presents a study on the emotional dialogue capability of ChatGPT, an advanced language model developed by OpenAI. The study evaluates the performance of ChatGPT on emotional dialogue understanding and generation through a series of experiments on several downstream tasks. Our findings indicate that while ChatGPT's performance on emotional dialogue understanding may still lag behind that of supervised models, it exhibits promising results in generating emotional responses. Furthermore, the study suggests potential avenues for future research directions.
&lt;/p&gt;</description></item><item><title>SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.09548</link><description>&lt;p&gt;
SemEval 2023 &#20219;&#21153;6: LegalEval -- &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09548
&lt;/p&gt;
&lt;p&gt;
SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#65292;&#24453;&#22788;&#29702;&#30340;&#27861;&#24459;&#26696;&#20214;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#26377;&#24517;&#35201;&#24320;&#21457;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25216;&#26415;&#65292;&#23545;&#27861;&#24459;&#25991;&#20214;&#36827;&#34892;&#22788;&#29702;&#21644;&#33258;&#21160;&#29702;&#35299;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#22312; SemEval 2023 &#19978;&#32452;&#32455;&#20102;&#20849;&#20139;&#20219;&#21153; LegalEval - &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#12290;LegalEval &#20219;&#21153;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;Task-A&#65288;&#20462;&#36766;&#35282;&#33394;&#26631;&#35760;&#65289;&#26159;&#33258;&#21160;&#23558;&#27861;&#24459;&#25991;&#20214;&#32467;&#26500;&#21270;&#20026;&#35821;&#20041;&#36830;&#36143;&#30340;&#21333;&#20803;&#65292;Task-B&#65288;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65289;&#22788;&#29702;&#22312;&#27861;&#24459;&#25991;&#20214;&#20013;&#35782;&#21035;&#30456;&#20851;&#23454;&#20307;&#65292;&#32780; Task-C&#65288;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#19982;&#35299;&#37322;&#65289;&#25506;&#32034;&#20102;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20197;&#21450;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;&#20849;&#26377;26&#20010;&#22242;&#38431;&#65288;&#20998;&#24067;&#22312;&#20840;&#29699;&#30340;&#32422;100&#21517;&#21442;&#19982;&#32773;&#65289;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#12290;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#37117;&#20248;&#20110;&#22522;&#20934;&#32447;&#65307;&#20294;&#26159;&#65292;&#20173;&#28982;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; LegalEval &#20219;&#21153;&#30340;&#32452;&#32455;&#21644;&#32454;&#33410;&#65292;&#24182;&#27010;&#36848;&#20102;&#21442;&#19982;&#31995;&#32479;&#21450;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In populous countries, pending legal cases have been growing exponentially. There is a need for developing NLP-based techniques for processing and automatically understanding legal documents. To promote research in the area of Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about automatically structuring legal documents into semantically coherent units, Task-B (Legal Named Entity Recognition) deals with identifying relevant entities in a legal document and Task-C (Court Judgement Prediction with Explanation) explores the possibility of automatically predicting the outcome of a legal case along with providing an explanation for the prediction. In total 26 teams (approx. 100 participants spread across the world) submitted systems paper. In each of the sub-tasks, the proposed systems outperformed the baselines; however, there is a lot of scope for improvement. This pape
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24615;LLMs&#65292;&#22914;ChatGPT&#21644;GPT-4&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#30456;&#20851;&#24615;&#25490;&#21517;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#36866;&#24403;&#25351;&#23548;&#21518;&#34920;&#29616;&#20248;&#24322;&#65292;&#26377;&#26102;&#29978;&#33267;&#20248;&#20110;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#23558;ChatGPT&#30340;&#25490;&#21517;&#33021;&#21147;&#25552;&#28860;&#20026;&#19987;&#38376;&#27169;&#22411;&#22312;BEIR&#19978;&#30340;&#25928;&#26524;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2304.09542</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#25490;&#21517;&#33021;&#21147;&#30740;&#31350;&#8212;&#8212;&#20197;ChatGPT&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. (arXiv:2304.09542v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24615;LLMs&#65292;&#22914;ChatGPT&#21644;GPT-4&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#30456;&#20851;&#24615;&#25490;&#21517;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#36866;&#24403;&#25351;&#23548;&#21518;&#34920;&#29616;&#20248;&#24322;&#65292;&#26377;&#26102;&#29978;&#33267;&#20248;&#20110;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#23558;ChatGPT&#30340;&#25490;&#21517;&#33021;&#21147;&#25552;&#28860;&#20026;&#19987;&#38376;&#27169;&#22411;&#22312;BEIR&#19978;&#30340;&#25928;&#26524;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;remarkable&#33021;&#21147;&#65292;&#33021;&#22815;&#23558;&#19968;&#20123;&#38646;&#26679;&#26412;&#35821;&#35328;&#20219;&#21153;&#25512;&#24191;&#33267;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#21644;GPT-4&#31561;&#29983;&#25104;&#24615;LLMs&#30340;&#30456;&#20851;&#24615;&#25490;&#21517;&#22312;&#20449;&#24687;&#26816;&#32034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#32463;&#36807;&#36866;&#24403;&#30340;&#25351;&#23548;&#65292;ChatGPT&#21644;GPT-4&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#19978;&#21462;&#24471;&#31454;&#20105;&#20248;&#21183;&#65292;&#29978;&#33267;&#26377;&#26102;&#20248;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;GPT-4&#22312;TREC&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;nDCG&#19978;&#34920;&#29616;&#20248;&#20110;&#23436;&#20840;&#24494;&#35843;&#30340;monoT5-3B&#65292;BEIR&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;nDCG&#19978;&#20248;&#20110;monoT5-3B 2.3&#20010;&#28857;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;Mr.TyDi&#19978;&#30340;&#24179;&#22343;nDCG&#19978;&#20248;&#20110;monoT5-3B 2.7&#20010;&#28857;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;ChatGPT&#30340;&#25490;&#21517;&#33021;&#21147;&#25552;&#28860;&#20026;&#19987;&#38376;&#30340;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#23567;&#22411;&#19987;&#38376;&#27169;&#22411;&#65288;&#35757;&#32451;&#20110;10K&#20010;ChatGPT&#29983;&#25104;&#30340;&#25968;&#25454;&#65289;&#22312;BEIR&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#22312;400K&#20010;MS MARCO&#27880;&#37322;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;monoT5&#12290;&#20195;&#30721;&#21487;&#22312;www.github.com/sunnwe&#19978;&#22797;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated a remarkable ability to generalize zero-shot to various language-related tasks. This paper focuses on the study of exploring generative LLMs such as ChatGPT and GPT-4 for relevance ranking in Information Retrieval (IR). Surprisingly, our experiments reveal that properly instructed ChatGPT and GPT-4 can deliver competitive, even superior results than supervised methods on popular IR benchmarks. Notably, GPT-4 outperforms the fully fine-tuned monoT5-3B on MS MARCO by an average of 2.7 nDCG on TREC datasets, an average of 2.3 nDCG on eight BEIR datasets, and an average of 2.7 nDCG on ten low-resource languages Mr.TyDi. Subsequently, we delve into the potential for distilling the ranking capabilities of ChatGPT into a specialized model. Our small specialized model that trained on 10K ChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO data on BEIR. The code to reproduce our results is available at www.github.com/sunnwe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#20013;&#20851;&#38190;&#35789;&#21450;&#20854;&#20301;&#32622;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#26631;&#35760;&#25511;&#21046;&#20851;&#38190;&#35789;&#30456;&#23545;&#20301;&#32622;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#29992;&#25143;&#24847;&#22270;&#30340;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.09516</link><description>&lt;p&gt;
&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#20851;&#38190;&#35789;&#21644;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
Controlling keywords and their positions in text generation. (arXiv:2304.09516v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#20013;&#20851;&#38190;&#35789;&#21450;&#20854;&#20301;&#32622;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#26631;&#35760;&#25511;&#21046;&#20851;&#38190;&#35789;&#30456;&#23545;&#20301;&#32622;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#29992;&#25143;&#24847;&#22270;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#25353;&#29031;&#29992;&#25143;&#24847;&#22270;&#25511;&#21046;&#29983;&#25104;&#32467;&#26524;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#25351;&#23450;&#24212;&#21253;&#21547;&#22312;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20851;&#38190;&#35789;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#36824;&#19981;&#36275;&#20197;&#29983;&#25104;&#21453;&#26144;&#29992;&#25143;&#24847;&#22270;&#30340;&#25991;&#26412;&#12290;&#20363;&#22914;&#65292;&#23558;&#37325;&#35201;&#20851;&#38190;&#35789;&#25918;&#22312;&#25991;&#26412;&#24320;&#22836;&#26377;&#21161;&#20110;&#21560;&#24341;&#35835;&#32773;&#30340;&#27880;&#24847;&#21147;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#19981;&#20801;&#35768;&#36825;&#31181;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#20013;&#20851;&#38190;&#35789;&#21644;&#20854;&#20301;&#32622;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#20351;&#29992;&#29305;&#27530;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#20851;&#38190;&#35789;&#30340;&#30456;&#23545;&#20301;&#32622;&#12290;&#25688;&#35201;&#29983;&#25104;&#21644;&#25925;&#20107;&#29983;&#25104;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25511;&#21046;&#20851;&#38190;&#35789;&#21450;&#20854;&#20301;&#32622;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#25511;&#21046;&#20851;&#38190;&#35789;&#20301;&#32622;&#21487;&#20197;&#29983;&#25104;&#27604;&#22522;&#32447;&#26356;&#31526;&#21512;&#29992;&#25143;&#24847;&#22270;&#30340;&#25688;&#35201;&#25991;&#26412;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges in text generation is to control generation as intended by a user. Previous studies have proposed to specify the keywords that should be included in the generated text. However, this is insufficient to generate text which reflect the user intent. For example, placing the important keyword beginning of the text would helps attract the reader's attention, but existing methods do not enable such flexible control. In this paper, we tackle a novel task of controlling not only keywords but also the position of each keyword in the text generation. To this end, we show that a method using special tokens can control the relative position of keywords. Experimental results on summarization and story generation tasks show that the proposed method can control keywords and their positions. We also demonstrate that controlling the keyword positions can generate summary texts that are closer to the user's intent than baseline. We release our code.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#20840;&#38754;&#32508;&#36848;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#31456;&#26469;&#20351;&#29992;&#24773;&#24863;&#20449;&#24687;&#36827;&#34892;&#24515;&#29702;&#30142;&#30149;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25991;&#31456;&#22238;&#39038;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#31574;&#30053;&#21450;&#20854;&#20248;&#32570;&#28857;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30740;&#31350;&#20154;&#21592;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.09493</link><description>&lt;p&gt;
&#24773;&#24863;&#34701;&#21512;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24212;&#29992;&#65306;&#29992;&#20110;&#24515;&#29702;&#30142;&#30149;&#26816;&#27979;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Emotion fusion for mental illness detection from social media: A survey. (arXiv:2304.09493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20840;&#38754;&#32508;&#36848;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#31456;&#26469;&#20351;&#29992;&#24773;&#24863;&#20449;&#24687;&#36827;&#34892;&#24515;&#29702;&#30142;&#30149;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25991;&#31456;&#22238;&#39038;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#31574;&#30053;&#21450;&#20854;&#20248;&#32570;&#28857;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30740;&#31350;&#20154;&#21592;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#30142;&#30149;&#26159;&#20840;&#29699;&#26368;&#26222;&#36941;&#30340;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#20043;&#19968;&#65292;&#23545;&#20154;&#20204;&#30340;&#29983;&#27963;&#21644;&#31038;&#20250;&#20581;&#24247;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#65292;&#23545;&#20110;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#31456;&#26469;&#26089;&#26399;&#21457;&#29616;&#24515;&#29702;&#30142;&#30149;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#26681;&#25454;&#24773;&#32490;&#21644;&#24515;&#29702;&#30142;&#30149;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#21644;&#34701;&#21512;&#24773;&#24863;&#20449;&#24687;&#24050;&#25104;&#20026;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#20027;&#39064;&#12290;&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#32467;&#21512;&#24773;&#24863;&#34701;&#21512;&#30340;&#26041;&#27861;&#29992;&#20110;&#24515;&#29702;&#30142;&#30149;&#26816;&#27979;&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#31574;&#30053;&#21450;&#20854;&#20248;&#32570;&#28857;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#20851;&#25968;&#25454;&#38598;&#21487;&#29992;&#24615;&#21644;&#36136;&#37327;&#12289;&#31639;&#27861;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mental illnesses are one of the most prevalent public health problems worldwide, which negatively influence people's lives and society's health. With the increasing popularity of social media, there has been a growing research interest in the early detection of mental illness by analysing user-generated posts on social media. According to the correlation between emotions and mental illness, leveraging and fusing emotion information has developed into a valuable research topic. In this article, we provide a comprehensive survey of approaches to mental illness detection in social media that incorporate emotion fusion. We begin by reviewing different fusion strategies, along with their advantages and disadvantages. Subsequently, we discuss the major challenges faced by researchers working in this area, including issues surrounding the availability and quality of datasets, the performance of algorithms and interpretability. We additionally suggest some potential directions for future resea
&lt;/p&gt;</description></item><item><title>EC^2 &#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#32039;&#24613;&#36890;&#20449;&#26041;&#26696;&#65292;&#29992;&#20110;&#35270;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#36523;&#20307;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312; EmbodiedAI &#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09448</link><description>&lt;p&gt;
EC^2: &#22522;&#20110;&#36523;&#20307;&#25511;&#21046;&#30340;&#26032;&#22411;&#32039;&#24613;&#20132;&#27969;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
EC^2: Emergent Communication for Embodied Control. (arXiv:2304.09448v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09448
&lt;/p&gt;
&lt;p&gt;
EC^2 &#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#32039;&#24613;&#36890;&#20449;&#26041;&#26696;&#65292;&#29992;&#20110;&#35270;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#36523;&#20307;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312; EmbodiedAI &#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#25511;&#21046;&#38656;&#35201;&#20195;&#29702;&#36890;&#36807;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#24555;&#36895;&#23398;&#20064;&#22914;&#20309;&#22312;&#26032;&#29615;&#22659;&#20013;&#34892;&#21160;&#65292;&#20854;&#20013;&#35270;&#39057;&#28436;&#31034;&#21253;&#21547;&#25152;&#38656;&#30340;&#35270;&#35273;&#21644;&#36816;&#21160;&#32454;&#33410;&#20197;&#36827;&#34892;&#20302;&#32423;&#21035;&#30693;&#35273;&#21644;&#25511;&#21046;&#65292;&#35821;&#35328;&#25351;&#20196;&#25903;&#25345;&#36890;&#36807;&#25277;&#35937;&#31526;&#21495;&#32467;&#26500;&#36827;&#34892;&#27867;&#21270;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#26041;&#27861;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#24378;&#21046;&#20004;&#31181;&#27169;&#24335;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#20294;&#25105;&#20204;&#20551;&#35774;&#26356;&#22909;&#22320;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20114;&#34917;&#24046;&#24322;&#21487;&#20197;&#24102;&#26469;&#26356;&#20840;&#38754;&#30340;&#34920;&#31034;&#20197;&#36827;&#34892;&#19979;&#28216;&#36866;&#24212;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Emergent Communication for Embodied Control (EC^2)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#35270;&#39057;&#35821;&#35328;&#34920;&#31034;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#36523;&#20307;&#25511;&#21046;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#32039;&#24613;&#36890;&#20449;&#23398;&#20064;&#35270;&#39057;&#30340;&#8220;&#35821;&#35328;&#8221;&#65292;&#23427;&#26725;&#25509;&#20102;&#35270;&#39057;&#32454;&#33410;&#30340;&#35821;&#20041;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35270;&#39057;&#36712;&#36857;&#65292;&#32039;&#24613;&#35821;&#35328;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#36523;&#20307;&#34920;&#31034;&#65292;&#28982;&#21518;&#23545;&#23427;&#20204;&#36827;&#34892;&#24494;&#35843;&#20197;&#36827;&#34892;&#36523;&#20307;&#25511;&#21046;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312; EmbodiedAI &#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272; EC^2&#65292;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied control requires agents to leverage multi-modal pre-training to quickly learn how to act in new environments, where video demonstrations contain visual and motion details needed for low-level perception and control, and language instructions support generalization with abstract, symbolic structures. While recent approaches apply contrastive learning to force alignment between the two modalities, we hypothesize better modeling their complementary differences can lead to more holistic representations for downstream adaption. To this end, we propose Emergent Communication for Embodied Control (EC^2), a novel scheme to pre-train video-language representations for few-shot embodied control. The key idea is to learn an unsupervised "language" of videos via emergent communication, which bridges the semantics of video details and structures of natural language. We learn embodied representations of video trajectories, emergent language, and natural language using a language model, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#30340;&#35757;&#32451;&#21644;&#23450;&#21046;&#19979;&#23454;&#29616;&#29983;&#25104;&#21487;&#26597;&#35810;&#34920;&#26684;&#30340;&#31616;&#21333;&#31995;&#32479;&#65292;&#24182;&#32473;&#20986;&#20102;&#20004;&#31181;&#23454;&#29616;&#31574;&#30053;&#65292;&#22312;&#19981;&#21516;&#36136;&#37327;&#21644;&#25104;&#26412;&#20013;&#24179;&#34913;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31995;&#32479;&#23545;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#29983;&#25104;&#30340;&#34920;&#26684;&#22343;&#39640;&#36136;&#37327;&#65292;&#19988;&#26080;&#38656;&#25991;&#26723;&#29305;&#23450;&#30340;&#23450;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.09433</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24322;&#26500;&#25968;&#25454;&#28246;&#32467;&#26500;&#21270;&#35270;&#22270;&#29983;&#25104;&#30340;&#31616;&#21333;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes. (arXiv:2304.09433v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#30340;&#35757;&#32451;&#21644;&#23450;&#21046;&#19979;&#23454;&#29616;&#29983;&#25104;&#21487;&#26597;&#35810;&#34920;&#26684;&#30340;&#31616;&#21333;&#31995;&#32479;&#65292;&#24182;&#32473;&#20986;&#20102;&#20004;&#31181;&#23454;&#29616;&#31574;&#30053;&#65292;&#22312;&#19981;&#21516;&#36136;&#37327;&#21644;&#25104;&#26412;&#20013;&#24179;&#34913;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31995;&#32479;&#23545;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#29983;&#25104;&#30340;&#34920;&#26684;&#22343;&#39640;&#36136;&#37327;&#65292;&#19988;&#26080;&#38656;&#25991;&#26723;&#29305;&#23450;&#30340;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31649;&#29702;&#30028;&#38271;&#26399;&#20197;&#26469;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#20986;&#36890;&#29992;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20154;&#21147;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#23450;&#21046;&#24773;&#20917;&#19979;&#25668;&#21462;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#24182;&#36755;&#20986;&#21487;&#26597;&#35810;&#30340;&#34920;&#26684;&#12290;&#37492;&#20110;&#28508;&#22312;&#25991;&#26723;&#30340;&#22810;&#26679;&#24615;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#36827;&#34892;&#31616;&#21270;&#30340;&#20551;&#35774;&#24182;&#20351;&#29992;&#29305;&#23450;&#39046;&#22495;&#30340;&#35757;&#32451;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35810;&#38382;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20445;&#25345;&#24191;&#27867;&#24615;&#12290;&#22312;&#24191;&#27867;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;LLMs&#21487;&#20165;&#38480;&#20110;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#25191;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#30001;LLMs&#39537;&#21160;&#30340;&#31616;&#21333;&#21407;&#22411;&#31995;&#32479;EVAPORATE&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#23454;&#29616;&#35813;&#31995;&#32479;&#30340;&#20004;&#31181;&#22522;&#26412;&#19981;&#21516;&#31574;&#30053;&#65306;&#25552;&#31034;LLM&#30452;&#25509;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#20540;&#25110;&#25552;&#31034;LLM&#21512;&#25104;&#25191;&#34892;&#25552;&#21462;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#25104;&#26412;-&#36136;&#37327;&#26435;&#34913;&#12290;&#20195;&#30721;&#21512;&#25104;&#20415;&#23452;&#65292;&#20294;&#27604;&#30452;&#25509;&#25277;&#21462;&#36828;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#23545;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EVAPORATE&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#25991;&#26723;&#29305;&#23450;&#30340;&#23450;&#21046;&#24773;&#20917;&#19979;&#20026;&#21508;&#31181;&#25991;&#26723;&#31867;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34920;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
A long standing goal of the data management community is to develop general, automated systems that ingest semi-structured documents and output queryable tables without human effort or domain specific customization. Given the sheer variety of potential documents, state-of-the art systems make simplifying assumptions and use domain specific training. In this work, we ask whether we can maintain generality by using large language models (LLMs). LLMs, which are pretrained on broad data, can perform diverse downstream tasks simply conditioned on natural language task descriptions.  We propose and evaluate EVAPORATE, a simple, prototype system powered by LLMs. We identify two fundamentally different strategies for implementing this system: prompt the LLM to directly extract values from documents or prompt the LLM to synthesize code that performs the extraction. Our evaluations show a cost-quality tradeoff between these two approaches. Code synthesis is cheap, but far less accurate than dire
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;TieFake&#65292;&#36890;&#36807;&#26816;&#27979;&#26032;&#38395;&#26631;&#39064;&#21644;&#27491;&#25991;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#21644;&#24773;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#36890;&#36807;BERT&#21644;ResNeSt&#23398;&#20064;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#25991;&#26412;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.09421</link><description>&lt;p&gt;
TieFake: &#26631;&#39064;-&#27491;&#25991;&#30456;&#20284;&#24230;&#21644;&#24773;&#24863;&#24863;&#30693;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TieFake: Title-Text Similarity and Emotion-Aware Fake News Detection. (arXiv:2304.09421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09421
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;TieFake&#65292;&#36890;&#36807;&#26816;&#27979;&#26032;&#38395;&#26631;&#39064;&#21644;&#27491;&#25991;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#21644;&#24773;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#36890;&#36807;BERT&#21644;ResNeSt&#23398;&#20064;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#25991;&#26412;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26816;&#27979;&#26088;&#22312;&#26816;&#27979;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#20256;&#25773;&#30340;&#34394;&#20551;&#26032;&#38395;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#20844;&#20247;&#21644;&#25919;&#24220;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#65292;&#20197;&#20174;&#26032;&#38395;&#22270;&#20687;&#12289;&#25991;&#26412;&#25110;&#35270;&#39057;&#20013;&#33719;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#38754;&#20020;&#20197;&#19979;&#38480;&#21046;&#65306;&#65288;1&#65289;&#24573;&#30053;&#26032;&#38395;&#20013;&#22266;&#26377;&#30340;&#24773;&#24863;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#26377;&#30410;&#65292;&#22240;&#20026;&#23427;&#21253;&#21547;&#20316;&#32773;&#30340;&#20027;&#35266;&#24847;&#22270;&#65307;&#65288;2&#65289;&#23545;&#20110;&#26032;&#38395;&#25991;&#31456;&#20013;&#26631;&#39064;&#19982;&#27491;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#65288;&#30456;&#20284;&#24230;&#65289;&#32473;&#20104;&#24456;&#23569;&#20851;&#27880;&#65292;&#36825;&#32463;&#24120;&#20351;&#29992;&#26080;&#20851;&#26631;&#39064;&#26469;&#21560;&#24341;&#35835;&#32773;&#30340;&#27880;&#24847;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#39064;-&#27491;&#25991;&#30456;&#20284;&#24230;&#21644;&#24773;&#24863;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#65288;TieFake&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#20849;&#21516;&#24314;&#27169;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#20316;&#32773;&#24773;&#24863;&#26469;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#21035;&#20351;&#29992;BERT&#21644;ResNeSt&#26469;&#23398;&#20064;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21457;&#34892;&#26041;&#24773;&#24863;&#25552;&#21462;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fake news detection aims to detect fake news widely spreading on social media platforms, which can negatively influence the public and the government. Many approaches have been developed to exploit relevant information from news images, text, or videos. However, these methods may suffer from the following limitations: (1) ignore the inherent emotional information of the news, which could be beneficial since it contains the subjective intentions of the authors; (2) pay little attention to the relation (similarity) between the title and textual information in news articles, which often use irrelevant title to attract reader' attention. To this end, we propose a novel Title-Text similarity and emotion-aware Fake news detection (TieFake) method by jointly modeling the multi-modal context information and the author sentiment in a unified framework. Specifically, we respectively employ BERT and ResNeSt to learn the representations for text and images, and utilize publisher emotion extractor 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#21462;GPT-2&#30340;&#34920;&#31034;&#26144;&#23556;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#27979;&#35797;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#20195;&#30721;&#36827;&#34892;&#20851;&#38190;&#20195;&#30721;&#30740;&#31350;&#30340;&#28508;&#21147;&#24182;&#35777;&#26126;&#20102;&#20195;&#30721;&#26159;&#20851;&#38190;&#20154;&#24037;&#26234;&#33021;&#21644;&#20851;&#38190;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#23376;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#20998;&#26512;&#37325;&#28857;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#35753;&#20154;&#20204;&#20851;&#27880;&#26222;&#36890;&#29992;&#25143;&#22914;&#20309;&#19982;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20132;&#20114;&#65292;&#24182;&#25351;&#23548;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.09406</link><description>&lt;p&gt;
&#22914;&#20309;&#22788;&#29702;&#28145;&#24230;&#23398;&#20064;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
How to Do Things with Deep Learning Code. (arXiv:2304.09406v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#21462;GPT-2&#30340;&#34920;&#31034;&#26144;&#23556;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#27979;&#35797;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#20195;&#30721;&#36827;&#34892;&#20851;&#38190;&#20195;&#30721;&#30740;&#31350;&#30340;&#28508;&#21147;&#24182;&#35777;&#26126;&#20102;&#20195;&#30721;&#26159;&#20851;&#38190;&#20154;&#24037;&#26234;&#33021;&#21644;&#20851;&#38190;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#23376;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#20998;&#26512;&#37325;&#28857;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#35753;&#20154;&#20204;&#20851;&#27880;&#26222;&#36890;&#29992;&#25143;&#22914;&#20309;&#19982;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20132;&#20114;&#65292;&#24182;&#25351;&#23548;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#21069;&#25552;&#35266;&#28857;&#26159;&#65292;&#22522;&#26412;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#25104;&#21644;&#21151;&#33021;&#33267;&#20851;&#32039;&#24613;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;OpenAI&#30340;GPT-2&#30340;&#34920;&#31034;&#26144;&#23556;&#65292;&#20854;&#20013;&#21253;&#25324;&#28041;&#21450;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#20195;&#30721;&#21644;&#26500;&#24314;&#22312;&#27169;&#22411;&#21608;&#22260;&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#22522;&#30784;&#20195;&#30721;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#21463;&#27426;&#36814;&#30340;GPT-2&#24212;&#29992;&#31243;&#24207;&#65288;&#25991;&#26412;&#20882;&#38505;&#28216;&#25103;AI Dungeon&#21644;&#35821;&#35328;&#33402;&#26415;&#39033;&#30446;This Word Does Not Exist&#65289;&#30340;&#26696;&#20363;&#30740;&#31350;&#26469;&#39564;&#35777;&#36825;&#20010;&#26144;&#23556;&#12290;&#36825;&#26679;&#30340;&#32451;&#20064;&#20351;&#25105;&#20204;&#33021;&#22815;&#27979;&#35797;&#23545;&#28145;&#24230;&#23398;&#20064;&#20195;&#30721;&#36827;&#34892;&#20851;&#38190;&#20195;&#30721;&#30740;&#31350;&#30340;&#28508;&#21147;&#65292;&#24182;&#35777;&#26126;&#20195;&#30721;&#26159;&#20851;&#38190;&#20154;&#24037;&#26234;&#33021;&#21644;&#20851;&#38190;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#23376;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#20998;&#26512;&#37325;&#28857;&#30340;&#26377;&#25928;&#24615;&#12290;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#26222;&#36890;&#29992;&#25143;&#22914;&#20309;&#19982;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20132;&#20114;&#65292;&#29978;&#33267;&#25351;&#23548;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#34920;&#36798;&#20195;&#29702;&#26426;&#26500;&#65292;&#25269;&#21046;&#20182;&#20204;&#21457;&#29616;&#26377;&#23475;&#25110;&#19981;&#20844;&#27491;&#30340;&#31995;&#32479;&#30340;&#24320;&#21457;&#30340;&#27880;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
The premise of this article is that a basic understanding of the composition and functioning of large language models is critically urgent. To that end, we extract a representational map of OpenAI's GPT-2 with what we articulate as two classes of deep learning code, that which pertains to the model and that which underwrites applications built around the model. We then verify this map through case studies of two popular GPT-2 applications: the text adventure game, AI Dungeon, and the language art project, This Word Does Not Exist. Such an exercise allows us to test the potential of Critical Code Studies when the object of study is deep learning code and to demonstrate the validity of code as an analytical focus for researchers in the subfields of Critical Artificial Intelligence and Critical Machine Learning Studies. More broadly, however, our work draws attention to the means by which ordinary users might interact with, and even direct, the behavior of deep learning systems, and by ex
&lt;/p&gt;</description></item><item><title>MixPro&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21407;&#22987;&#36755;&#20837;&#21644;&#27169;&#26495;&#36827;&#34892;&#28151;&#21512;&#26469;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;5.08%&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09402</link><description>&lt;p&gt;
MixPro&#65306;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning. (arXiv:2304.09402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09402
&lt;/p&gt;
&lt;p&gt;
MixPro&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21407;&#22987;&#36755;&#20837;&#21644;&#27169;&#26495;&#36827;&#34892;&#28151;&#21512;&#26469;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;5.08%&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#36890;&#36807;&#23558;&#36755;&#20837;&#19982;&#27169;&#26495;&#32452;&#21512;&#36215;&#26469;&#65292;&#23558;&#19979;&#28216;&#20219;&#21153;&#37325;&#26500;&#20026;&#22635;&#31354;&#38382;&#39064;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#29305;&#21035;&#26377;&#29992;&#65292;&#28982;&#32780;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#27169;&#26495;&#21644;&#25991;&#26412;&#20173;&#28982;&#23384;&#22312;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20351;&#29992;&#27169;&#22411;&#38598;&#25104;&#30340;&#26041;&#27861;&#21487;&#20197;&#38480;&#21046;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MixPro&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26631;&#35760;&#32423;&#12289;&#21477;&#23376;&#32423;&#21644;&#26102;&#20195;&#32423;&#30340;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#21407;&#22987;&#36755;&#20837;&#25991;&#26412;&#21644;&#27169;&#26495;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;MixPro&#20248;&#20110;&#20854;&#20182;&#22686;&#24378;&#22522;&#32447;&#65292;&#30456;&#27604;&#22686;&#24378;&#21069;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;5.08%&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning reformulates downstream tasks as cloze problems by combining the original input with a template. This technique is particularly useful in few-shot learning, where a model is trained on a limited amount of data. However, the limited templates and text used in few-shot prompt-based learning still leave significant room for performance improvement. Additionally, existing methods using model ensembles can constrain the model efficiency. To address these issues, we propose an augmentation method called MixPro, which augments both the vanilla input text and the templates through token-level, sentence-level, and epoch-level Mixup strategies. We conduct experiments on five few-shot datasets, and the results show that MixPro outperforms other augmentation baselines, improving model performance by an average of 5.08% compared to before augmentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21387;&#32553;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#23454;&#35777;&#25928;&#26524;&#65292;&#24182;&#20197;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#20026;&#26696;&#20363;&#23637;&#31034;&#20102;&#33976;&#39311;&#26041;&#27861;&#23545;&#27169;&#22411;&#22823;&#23567;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#23618;&#32039;&#20945;&#27169;&#22411;&#24448;&#24448;&#19982;&#27973;&#23618;&#38750;&#32039;&#20945;&#27169;&#22411;&#19968;&#26679;&#22909;&#65292;&#23558;&#33976;&#39311;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#23376;&#38598;&#19978;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.09388</link><description>&lt;p&gt;
&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21387;&#32553;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Leveraging Knowledge Distillation for Compressing Multilingual Neural Machine Translation Models. (arXiv:2304.09388v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21387;&#32553;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#23454;&#35777;&#25928;&#26524;&#65292;&#24182;&#20197;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#20026;&#26696;&#20363;&#23637;&#31034;&#20102;&#33976;&#39311;&#26041;&#27861;&#23545;&#27169;&#22411;&#22823;&#23567;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#23618;&#32039;&#20945;&#27169;&#22411;&#24448;&#24448;&#19982;&#27973;&#23618;&#38750;&#32039;&#20945;&#27169;&#22411;&#19968;&#26679;&#22909;&#65292;&#23558;&#33976;&#39311;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#23376;&#38598;&#19978;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#21387;&#32553;&#31070;&#32463;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;MNMT&#65288;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65289;&#30340;&#26222;&#21450;&#21644;&#20248;&#36234;&#24615;&#65292;&#20294;&#20174;&#22823;&#22411;MNMT&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#30740;&#31350;&#23454;&#38469;&#19978;&#24182;&#19981;&#23384;&#22312;&#12290;&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21387;&#32553;MNMT&#27169;&#22411;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#20197;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#35777;&#26126;&#20102;&#24120;&#29992;&#30340;&#35821;&#35328;&#26080;&#20851;&#21644;&#35821;&#35328;&#24863;&#30693;&#30340;&#33976;&#39311;&#26041;&#27861;&#21487;&#20197;&#20351;&#27169;&#22411;&#21387;&#32553;4-5&#20493;&#65292;&#20294;&#24615;&#33021;&#19979;&#38477;&#22810;&#36798;3.5 BLEU&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#35774;&#35745;&#19978;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#28145;&#23618;&#27169;&#22411;&#21644;&#27973;&#23618;&#27169;&#22411;&#12289;&#21442;&#25968;&#20849;&#20139;&#12289;&#22810;&#38454;&#27573;&#35757;&#32451;&#21644;&#36866;&#37197;&#22120;&#31561;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#28145;&#23618;&#32039;&#20945;&#27169;&#22411;&#24448;&#24448;&#19982;&#27973;&#23618;&#38750;&#32039;&#20945;&#27169;&#22411;&#19968;&#26679;&#22909;&#65292;&#23558;&#33976;&#39311;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#23376;&#38598;&#19978;&#24494;&#35843;&#21487;&#20197;&#31245;&#24494;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is a well-known method for compressing neural models. However, works focusing on distilling knowledge from large multilingual neural machine translation (MNMT) models into smaller ones are practically nonexistent, despite the popularity and superiority of MNMT. This paper bridges this gap by presenting an empirical investigation of knowledge distillation for compressing MNMT models. We take Indic to English translation as a case study and demonstrate that commonly used language-agnostic and language-aware KD approaches yield models that are 4-5x smaller but also suffer from performance drops of up to 3.5 BLEU. To mitigate this, we then experiment with design considerations such as shallower versus deeper models, heavy parameter sharing, multi-stage training, and adapters. We observe that deeper compact models tend to be as good as shallower non-compact ones, and that fine-tuning a distilled model on a High-Quality subset slightly boosts translation quality. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#8220;&#27927;&#29260;&#21644;&#20999;&#21106;&#8221;&#31639;&#27861;&#23545;&#38271;&#25991;&#26412;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#25552;&#21462;BERT&#23884;&#20837;&#65292;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#25552;&#39640;20.94%&#12290;</title><link>http://arxiv.org/abs/2304.09374</link><description>&lt;p&gt;
&#27927;&#29260;&#21644;&#20999;&#21106;&#65306;&#38271;&#25991;&#26412;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Shuffle &amp; Divide: Contrastive Learning for Long Text. (arXiv:2304.09374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#8220;&#27927;&#29260;&#21644;&#20999;&#21106;&#8221;&#31639;&#27861;&#23545;&#38271;&#25991;&#26412;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#25552;&#21462;BERT&#23884;&#20837;&#65292;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#25552;&#39640;20.94%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#38271;&#25991;&#26412;&#25991;&#26723;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#8220;&#27927;&#29260;&#21644;&#20999;&#21106;&#8221;&#65288;SaD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25991;&#26412;&#22686;&#24191;&#31639;&#27861;&#65292;&#21487;&#20026;&#22522;&#20110;BERT&#30340;&#25991;&#26723;&#23884;&#20837;&#25152;&#38656;&#30340;&#23545;&#27604;&#26356;&#26032;&#35774;&#32622;&#19968;&#20010;&#21069;&#32622;&#20219;&#21153;&#12290;SaD&#23558;&#25991;&#26723;&#25286;&#20998;&#20026;&#20004;&#20010;&#23376;&#25991;&#26723;&#65292;&#20854;&#20013;&#21253;&#21547;&#38543;&#26426;&#27927;&#29260;&#30340;&#21333;&#35789;&#12290;&#36825;&#20123;&#23376;&#25991;&#26723;&#34987;&#35270;&#20026;&#27491;&#26679;&#26412;&#65292;&#23558;&#25152;&#26377;&#20854;&#20182;&#25991;&#26723;&#35270;&#20026;&#36127;&#26679;&#26412;&#12290;&#22312;SaD&#20043;&#21518;&#65292;&#25105;&#20204;&#37325;&#22797;&#23545;&#27604;&#26356;&#26032;&#21644;&#32858;&#31867;&#38454;&#27573;&#65292;&#30452;&#33267;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20943;&#36731;&#20154;&#21147;&#36164;&#28304;&#30340;&#24037;&#20316;&#37327;&#65292;&#20174;&#32780;&#33410;&#30465;&#26114;&#36149;&#30340;AI&#36164;&#28304;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;20 Newsgroups&#12289;Reuters-21578&#12289;BBC&#21644;BBCSport&#25968;&#25454;&#38598;&#36827;&#34892;&#26080;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#32463;&#39564;&#35780;&#20272;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;20 Newsgroups&#19978;&#23558;&#24403;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;SS-SB-MT&#25552;&#39640;&#20102;20.94&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a self-supervised learning method for long text documents based on contrastive learning. A key to our method is Shuffle and Divide (SaD), a simple text augmentation algorithm that sets up a pretext task required for contrastive updates to BERT-based document embedding. SaD splits a document into two sub-documents containing randomly shuffled words in the entire documents. The sub-documents are considered positive examples, leaving all other documents in the corpus as negatives. After SaD, we repeat the contrastive update and clustering phases until convergence. It is naturally a time-consuming, cumbersome task to label text documents, and our method can help alleviate human efforts, which are most expensive resources in AI. We have empirically evaluated our method by performing unsupervised text classification on the 20 Newsgroups, Reuters-21578, BBC, and BBCSport datasets. In particular, our method pushes the current state-of-the-art, SS-SB-MT, on 20 Newsgroups by 20.94% in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.09349</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#22823;&#33041;&#65306;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#24863;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#22791;&#29289;&#29702;&#25110;&#34394;&#25311;&#23454;&#20307;&#65288;&#21363;&#26426;&#22120;&#20154;&#65289;&#24182;&#33021;&#22815;&#19982;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#35760;&#24518;&#21644;&#25511;&#21046;&#26159;&#20307;&#24863;&#31995;&#32479;&#30340;&#20004;&#20010;&#22522;&#26412;&#37096;&#20998;&#65292;&#36890;&#24120;&#38656;&#35201;&#20998;&#21035;&#20351;&#29992;&#26694;&#26550;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;LLM-Brain&#65306;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#65292;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#12290;LLM-Brain&#26694;&#26550;&#38598;&#25104;&#20102;&#22810;&#20010;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#21033;&#29992;&#38646;-shot&#23398;&#20064;&#26041;&#27861;&#12290;LLM-Brain&#20013;&#30340;&#25152;&#26377;&#32452;&#20214;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#12290;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#20855;&#22791;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#23454;&#20307;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#65306;&#20027;&#21160;&#25506;&#32034;&#21644;&#23454;&#20307;&#38382;&#31572;&#26469;&#28436;&#31034;LLM-Brain&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The
&lt;/p&gt;</description></item><item><title>&#24847;&#22823;&#21033;&#31105;&#27490;ChatGPT&#20135;&#29983;&#20102;&#30701;&#26399;&#30340;&#29983;&#20135;&#21147;&#24178;&#25200;&#65292;&#20294;&#20063;&#23548;&#33268;&#20102;&#23457;&#26597;&#32469;&#36807;&#24037;&#20855;&#30340;&#26174;&#33879;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2304.09339</link><description>&lt;p&gt;
&#31105;&#27490;&#25968;&#23383;&#25216;&#26415;&#30340;&#24847;&#22806;&#21518;&#26524;&#8212;&#8212;&#20197;&#24847;&#22823;&#21033;ChatGPT&#31105;&#20196;&#20026;&#20363;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
The Unintended Consequences of Censoring Digital Technology -- Evidence from Italy's ChatGPT Ban. (arXiv:2304.09339v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09339
&lt;/p&gt;
&lt;p&gt;
&#24847;&#22823;&#21033;&#31105;&#27490;ChatGPT&#20135;&#29983;&#20102;&#30701;&#26399;&#30340;&#29983;&#20135;&#21147;&#24178;&#25200;&#65292;&#20294;&#20063;&#23548;&#33268;&#20102;&#23457;&#26597;&#32469;&#36807;&#24037;&#20855;&#30340;&#26174;&#33879;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;ChatGPT&#65292;&#19968;&#31181;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#32842;&#22825;&#26426;&#22120;&#20154;&#34987;&#31105;&#27490;&#23545;&#20010;&#20154;&#29983;&#20135;&#29575;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#36229;&#36807;8,000&#21517;&#19987;&#19994;GitHub&#29992;&#25143;&#22312;&#24847;&#22823;&#21033;&#21644;&#20854;&#20182;&#27431;&#27954;&#22269;&#23478;&#30340;&#27599;&#23567;&#26102;&#32534;&#30721;&#20135;&#20986;&#25968;&#25454;&#65292;&#20197;&#20998;&#26512;&#31105;&#20196;&#23545;&#20010;&#20154;&#29983;&#20135;&#21147;&#30340;&#24433;&#21709;&#12290;&#23558;&#39640;&#39057;&#29575;&#25968;&#25454;&#19982;&#31105;&#20196;&#30340;&#31361;&#28982;&#23459;&#24067;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#36816;&#29992;&#24046;&#24322;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#31105;&#20196;&#21518;&#30340;&#21069;&#20004;&#20010;&#24037;&#20316;&#26085;&#65292;&#24847;&#22823;&#21033;&#24320;&#21457;&#32773;&#30340;&#20135;&#20986;&#20943;&#23569;&#20102;&#32422;50&#65285;&#65292;&#20043;&#21518;&#36880;&#28176;&#24674;&#22797;&#12290;&#36816;&#29992;&#21512;&#25104;&#25511;&#21046;&#26041;&#27861;&#26469;&#20998;&#26512;&#27599;&#26085;Google&#25628;&#32034;&#21644;Tor&#20351;&#29992;&#25968;&#25454;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#31105;&#20196;&#23548;&#33268;&#20102;&#32469;&#36807;&#23457;&#26597;&#30340;&#24037;&#20855;&#30340;&#20351;&#29992;&#37327;&#26174;&#33879;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29992;&#25143;&#24456;&#24555;&#37319;&#21462;&#32469;&#36807;&#20114;&#32852;&#32593;&#38480;&#21046;&#30340;&#31574;&#30053;&#65292;&#20294;&#36825;&#31181;&#36866;&#24212;&#27963;&#21160;&#20250;&#36896;&#25104;&#30701;&#26399;&#30340;&#24178;&#25200;&#21644;&#24433;&#21709;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyse the effects of the ban of ChatGPT, a generative pre-trained transformer chatbot, on individual productivity. We first compile data on the hourly coding output of over 8,000 professional GitHub users in Italy and other European countries to analyse the impact of the ban on individual productivity. Combining the high-frequency data with the sudden announcement of the ban in a difference-in-differences framework, we find that the output of Italian developers decreased by around 50% in the first two business days after the ban and recovered after that. Applying a synthetic control approach to daily Google search and Tor usage data shows that the ban led to a significant increase in the use of censorship bypassing tools. Our findings show that users swiftly implement strategies to bypass Internet restrictions but this adaptation activity creates short-term disruptions and hampers productivity.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#34394;&#25311;&#21161;&#25163;&#26694;&#26550;&#8212;&#8212;BIM-GPT&#65292;&#36890;&#36807;&#38598;&#25104;BIM&#21644;GPT&#25216;&#26415;&#65292;&#25903;&#25345;&#33258;&#28982;&#35821;&#35328;&#26816;&#32034;&#65292;&#24182;&#22312;BIM IR&#25968;&#25454;&#38598;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#39640;&#31934;&#24230;&#12290;&#21516;&#26102;&#65292;&#23545;&#21307;&#38498;&#24314;&#31569;&#30340;VA&#21407;&#22411;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#21151;&#33021;&#65292;&#20026;&#24314;&#31569;&#19994;&#20013;BIM IR&#30340;&#22810;&#21151;&#33021;VA&#24320;&#21457;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.09333</link><description>&lt;p&gt;
BIM&#20449;&#24687;&#26816;&#32034;&#30340;Prompt-Based&#34394;&#25311;&#21161;&#25163;&#26694;&#26550;&#65306;BIM-GPT
&lt;/p&gt;
&lt;p&gt;
BIM-GPT: a Prompt-Based Virtual Assistant Framework for BIM Information Retrieval. (arXiv:2304.09333v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09333
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#34394;&#25311;&#21161;&#25163;&#26694;&#26550;&#8212;&#8212;BIM-GPT&#65292;&#36890;&#36807;&#38598;&#25104;BIM&#21644;GPT&#25216;&#26415;&#65292;&#25903;&#25345;&#33258;&#28982;&#35821;&#35328;&#26816;&#32034;&#65292;&#24182;&#22312;BIM IR&#25968;&#25454;&#38598;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#39640;&#31934;&#24230;&#12290;&#21516;&#26102;&#65292;&#23545;&#21307;&#38498;&#24314;&#31569;&#30340;VA&#21407;&#22411;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#21151;&#33021;&#65292;&#20026;&#24314;&#31569;&#19994;&#20013;BIM IR&#30340;&#22810;&#21151;&#33021;VA&#24320;&#21457;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#38656;&#35201;&#28145;&#20837;&#30340;BIM&#30693;&#35782;&#25110;&#22823;&#37327;&#24037;&#31243;&#33258;&#21160;&#21270;&#65292;&#20174;&#24314;&#31569;&#20449;&#24687;&#27169;&#22411;&#65288;BIM&#65289;&#20013;&#39640;&#25928;&#22320;&#26816;&#32034;&#20449;&#24687;&#65288;IR&#65289;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;BIM-GPT&#65292;&#19968;&#20010;&#22522;&#20110;&#25552;&#31034;&#30340;&#34394;&#25311;&#21161;&#25163;&#65288;VA&#65289;&#26694;&#26550;&#65292;&#38598;&#25104;BIM&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#65288;GPT&#65289;&#25216;&#26415;&#65292;&#25903;&#25345;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;IR&#12290;&#25552;&#31034;&#31649;&#29702;&#22120;&#21644;&#21160;&#24577;&#27169;&#26495;&#20026;GPT&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#37322;NL&#26597;&#35810;&#65292;&#24635;&#32467;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#20197;&#21450;&#22238;&#31572;&#19982;BIM&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#22312;&#23545;BIM IR&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27809;&#26377;&#25968;&#25454;&#21644;2&#65285;&#25968;&#25454;&#21512;&#24182;&#22312;&#25552;&#31034;&#20013;&#30340;&#24773;&#20917;&#19979;&#65292;&#20998;&#21035;&#23454;&#29616;&#20102;83.5&#65285;&#21644;99.5&#65285;&#30340;NL&#26597;&#35810;&#20998;&#31867;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#38024;&#23545;&#21307;&#38498;&#24314;&#31569;&#30340;VA&#21407;&#22411;&#39564;&#35777;&#20102;BIM-GPT&#30340;&#21151;&#33021;&#12290;&#26412;&#30740;&#31350;&#20026;&#24314;&#31569;&#19994;&#20013;BIM IR&#30340;&#26377;&#25928;&#21644;&#22810;&#21151;&#33021;VA&#30340;&#24320;&#21457;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;BIM&#30340;&#21487;&#35775;&#38382;&#24615;&#24182;&#20943;&#23569;&#20102;&#24037;&#31243;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient information retrieval (IR) from building information models (BIMs) poses significant challenges due to the necessity for deep BIM knowledge or extensive engineering efforts for automation. We introduce BIM-GPT, a prompt-based virtual assistant (VA) framework integrating BIM and generative pre-trained transformer (GPT) technologies to support NL-based IR. A prompt manager and dynamic template generate prompts for GPT models, enabling interpretation of NL queries, summarization of retrieved information, and answering BIM-related questions. In tests on a BIM IR dataset, our approach achieved 83.5% and 99.5% accuracy rates for classifying NL queries with no data and 2% data incorporated in prompts, respectively. Additionally, we validated the functionality of BIM-GPT through a VA prototype for a hospital building. This research contributes to the development of effective and versatile VAs for BIM IR in the construction industry, significantly enhancing BIM accessibility and reduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#955;&#28436;&#31639;&#27861;&#65292;&#20351;&#29992;&#955;&#35821;&#35328;&#32534;&#31243;&#65292;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#25191;&#34892;&#25972;&#20010;&#31243;&#24207;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25299;&#23637;&#31070;&#32463;&#32593;&#32476;&#22312;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.09276</link><description>&lt;p&gt;
&#19968;&#31181;&#31070;&#32463;&#955;&#28436;&#31639;&#27861;&#65306;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#36935;&#35265;&#35745;&#31639;&#21644;&#20989;&#25968;&#24335;&#32534;&#31243;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Neural Lambda Calculus: Neurosymbolic AI meets the foundations of computing and functional programming. (arXiv:2304.09276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#955;&#28436;&#31639;&#27861;&#65292;&#20351;&#29992;&#955;&#35821;&#35328;&#32534;&#31243;&#65292;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#25191;&#34892;&#25972;&#20010;&#31243;&#24207;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25299;&#23637;&#31070;&#32463;&#32593;&#32476;&#22312;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#25104;&#20026;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20027;&#23548;&#33539;&#24335;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#35748;&#20026;&#22312;&#31526;&#21495;&#23398;&#20064;&#20013;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26159;&#36234;&#26469;&#36234;&#30456;&#20851;&#30340;&#12290;&#20026;&#20102;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25968;&#23398;&#26500;&#36896;&#65288;&#22914;&#21152;&#27861;&#21644;&#20056;&#27861;&#65289;&#12289;&#36923;&#36753;&#25512;&#29702;&#65288;&#22914;&#23450;&#29702;&#35777;&#26126;&#22120;&#65289;&#29978;&#33267;&#25191;&#34892;&#35745;&#31639;&#26426;&#31243;&#24207;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21518;&#32773;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#26159;&#22826;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#24182;&#19981;&#24635;&#26159;&#25104;&#21151;&#30340;&#65292;&#24182;&#19988;&#24448;&#24448;&#38656;&#35201;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#26377;&#20559;&#35265;&#30340;&#20803;&#32032;&#65292;&#20197;&#38480;&#21046;&#21487;&#33021;&#35201;&#25191;&#34892;&#30340;&#31243;&#24207;&#30340;&#33539;&#22260;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22914;&#20309;&#25191;&#34892;&#25972;&#20010;&#31243;&#24207;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19981;&#20351;&#29992;&#21629;&#20196;&#24335;&#32534;&#31243;&#35821;&#35328;&#65292;&#32780;&#26159;&#37319;&#29992;&#955;&#35821;&#35328;&#36827;&#34892;&#32534;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decades, deep neural networks based-models became the dominant paradigm in machine learning. Further, the use of artificial neural networks in symbolic learning has been seen as increasingly relevant recently. To study the capabilities of neural networks in the symbolic AI domain, researchers have explored the ability of deep neural networks to learn mathematical constructions, such as addition and multiplication, logic inference, such as theorem provers, and even the execution of computer programs. The latter is known to be too complex a task for neural networks. Therefore, the results were not always successful, and often required the introduction of biased elements in the learning process, in addition to restricting the scope of possible programs to be executed. In this work, we will analyze the ability of neural networks to learn how to execute programs as a whole. To do so, we propose a different approach. Instead of using an imperative programming language, with com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20339;&#21453;&#39539;&#26816;&#32034;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20998;&#27169;&#22411;&#65292;&#20351;&#29992;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#24230;&#37327;&#65292;&#36798;&#21040;&#20102;88.9&#65285;&#30340;accuracy@1&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.08807</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#22312;&#26368;&#20339;&#21453;&#39539;&#26816;&#32034;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Role of Similarity and Dissimilarity inBest Counter Argument Retrieval. (arXiv:2304.08807v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20339;&#21453;&#39539;&#26816;&#32034;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20998;&#27169;&#22411;&#65292;&#20351;&#29992;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#24230;&#37327;&#65292;&#36798;&#21040;&#20102;88.9&#65285;&#30340;accuracy@1&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32473;&#23450;&#36755;&#20837;&#35770;&#28857;&#24773;&#20917;&#19979;&#30340;&#26368;&#20339;&#21453;&#39539;&#26816;&#32034;&#20219;&#21153;&#12290;&#26681;&#25454;&#26368;&#20339;&#21453;&#39539;&#23450;&#20041;&#65292;&#26368;&#20339;&#21453;&#39539;&#24212;&#19982;&#36755;&#20837;&#35770;&#28857;&#22312;&#32454;&#33410;&#26041;&#38754;&#30456;&#20284;&#65292;&#20294;&#31435;&#22330;&#30456;&#21453;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#24230;&#37327;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#27169;&#22411;&#26469;&#23545;&#21453;&#39539;&#36827;&#34892;&#35780;&#20998;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#35780;&#20998;&#26041;&#27861;&#65288;&#21253;&#25324;&#20256;&#32479;&#30340;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#21644;&#26368;&#36817;&#30340;&#31070;&#32463;&#35780;&#20998;&#27169;&#22411;&#65289;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bipolar-encoder&#65292;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#21516;&#26102;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#30340;&#26368;&#20248;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;88.9&#65285;&#30340;accuracy@1&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;&#24403;&#19982;&#36866;&#24403;&#30340;&#32531;&#23384;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;Bipolar-encoder&#22312;&#39044;&#27979;&#26102;&#38388;&#19978;&#20063;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the task of best counter-argument retrieval given an input argument. Following the definition that the best counter-argument addresses the same aspects as the input argument while having the opposite stance, we aim to develop an efficient and effective model for scoring counter-arguments based on similarity and dissimilarity metrics. We first conduct an experimental study on the effectiveness of available scoring methods, including traditional Learning-To-Rank (LTR) and recent neural scoring models. We then propose Bipolar-encoder, a novel BERT-based model to learn an optimal representation for simultaneous similarity and dissimilarity. Experimental results show that our proposed method can achieve the accuracy@1 of 88.9\%, which significantly outperforms other baselines by a large margin. When combined with an appropriate caching technique, Bipolar-encoder is comparably efficient at prediction time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPC&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#23545;&#35805;&#20013;&#27599;&#20010;&#21457;&#35328;&#32773;&#29983;&#25104;&#20010;&#20154;&#29305;&#24449;&#25688;&#35201;&#12290;&#20219;&#21153;&#34987;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#20010;&#20154;&#29305;&#24449;&#21457;&#29616;&#12289;&#20010;&#20154;&#29305;&#24449;&#31867;&#22411;&#35782;&#21035;&#21644;&#20010;&#20154;&#29305;&#24449;&#20215;&#20540;&#25552;&#21462;&#12290;&#20219;&#21153;&#23545;&#20110;&#38134;&#34892;&#12289;&#37202;&#24215;&#39044;&#35746;&#21644;&#33322;&#31354;&#39044;&#35746;&#31561;&#34892;&#19994;&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38750;&#24120;&#37325;&#35201;&#65292;&#21487;&#20197;&#20351;&#32842;&#22825;&#26426;&#22120;&#20154;&#26356;&#22909;&#22320;&#20102;&#35299;&#21644;&#22238;&#24212;&#27599;&#20010;&#21457;&#35328;&#20154;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.08801</link><description>&lt;p&gt;
&#22810;&#26041;&#20250;&#35805;&#20013;&#30340;&#21457;&#35328;&#20154;&#20010;&#20154;&#29305;&#24449;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Speaker Profiling in Multiparty Conversations. (arXiv:2304.08801v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPC&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#23545;&#35805;&#20013;&#27599;&#20010;&#21457;&#35328;&#32773;&#29983;&#25104;&#20010;&#20154;&#29305;&#24449;&#25688;&#35201;&#12290;&#20219;&#21153;&#34987;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#20010;&#20154;&#29305;&#24449;&#21457;&#29616;&#12289;&#20010;&#20154;&#29305;&#24449;&#31867;&#22411;&#35782;&#21035;&#21644;&#20010;&#20154;&#29305;&#24449;&#20215;&#20540;&#25552;&#21462;&#12290;&#20219;&#21153;&#23545;&#20110;&#38134;&#34892;&#12289;&#37202;&#24215;&#39044;&#35746;&#21644;&#33322;&#31354;&#39044;&#35746;&#31561;&#34892;&#19994;&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38750;&#24120;&#37325;&#35201;&#65292;&#21487;&#20197;&#20351;&#32842;&#22825;&#26426;&#22120;&#20154;&#26356;&#22909;&#22320;&#20102;&#35299;&#21644;&#22238;&#24212;&#27599;&#20010;&#21457;&#35328;&#20154;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23637;&#29616;&#20986;&#29420;&#29305;&#30340;&#34892;&#20026;&#65292;&#20351;&#24471;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#26041;&#27861;&#19981;&#36275;&#20197;&#20026;&#23545;&#35805;&#20195;&#29702;&#29983;&#25104;&#22238;&#24212;&#12290;&#34429;&#28982;&#36807;&#21435;&#30340;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#21457;&#35328;&#20154;&#20010;&#20154;&#20449;&#24687;&#21019;&#24314;&#20010;&#24615;&#21270;&#23545;&#35805;&#20195;&#29702;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#21069;&#25552;&#65292;&#21363;&#21457;&#35328;&#20154;&#20010;&#20154;&#29305;&#24449;&#24050;&#32463;&#34987;&#25552;&#20379;&#12290;&#28982;&#32780;&#65292;&#22312;&#20687;&#38134;&#34892;&#12289;&#37202;&#24215;&#39044;&#35746;&#21644;&#33322;&#31354;&#39044;&#35746;&#31561;&#34892;&#19994;&#20013;&#20351;&#29992;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26041;&#38754;&#65292;&#36825;&#19968;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#27491;&#30830;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#23545;&#35805;&#20013;&#30340;&#21457;&#35328;&#20154;&#20010;&#20154;&#29305;&#24449;&#20998;&#26512; (SPC)&#20219;&#21153;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#12290;SPC&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#23545;&#35805;&#20013;&#27599;&#20010;&#21457;&#35328;&#20154;&#20135;&#29983;&#20010;&#20154;&#29305;&#24449;&#25688;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#20010;&#20154;&#29305;&#24449;&#21457;&#29616;&#12289;&#20010;&#20154;&#29305;&#24449;&#31867;&#22411;&#35782;&#21035;&#21644;&#20010;&#20154;&#29305;&#24449;&#20215;&#20540;&#25552;&#21462;&#12290;&#22312;&#32473;&#23450;&#23545;&#35805;&#30340;&#24773;&#20917;&#19979;&#65292;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#26088;&#22312;&#35782;&#21035;&#21253;&#21547;&#20010;&#20154;&#20449;&#24687;&#30340;&#25152;&#26377;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational settings, individuals exhibit unique behaviors, rendering a one-size-fits-all approach insufficient for generating responses by dialogue agents. Although past studies have aimed to create personalized dialogue agents using speaker persona information, they have relied on the assumption that the speaker's persona is already provided. However, this assumption is not always valid, especially when it comes to chatbots utilized in industries like banking, hotel reservations, and airline bookings. This research paper aims to fill this gap by exploring the task of Speaker Profiling in Conversations (SPC). The primary objective of SPC is to produce a summary of persona characteristics for each individual speaker present in a dialogue. To accomplish this, we have divided the task into three subtasks: persona discovery, persona-type identification, and persona-value extraction. Given a dialogue, the first subtask aims to identify all utterances that contain persona information.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36827;&#34892;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#19968;&#31995;&#21015;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#21644;&#22810;&#35821;&#35328;&#30340;&#23545;&#25163;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;GPT-3.5-turbo&#25345;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.07880</link><description>&lt;p&gt;
Sabi&#225;: &#33889;&#33796;&#29273;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sabi\'a: Portuguese Large Language Models. (arXiv:2304.07880v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07880
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36827;&#34892;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#19968;&#31995;&#21015;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#21644;&#22810;&#35821;&#35328;&#30340;&#23545;&#25163;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;GPT-3.5-turbo&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#26029;&#25552;&#39640;&#65292;&#8221;&#19968;&#20992;&#20999;&#8220;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#20027;&#27969;&#12290;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#20840;&#29699;&#20351;&#29992;&#30340;&#35821;&#35328;&#25968;&#37327;&#38750;&#24120;&#24222;&#22823;&#65292;&#24182;&#19988;&#20854;&#20013;&#24456;&#22810;&#35821;&#35328;&#37117;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#20027;&#35201;&#30340;&#20570;&#27861;&#26159;&#23545;&#22810;&#31181;&#35821;&#35328;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#20570;&#27861;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#35777;&#26126;&#20102;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#29992;3%&#25110;&#26356;&#23569;&#30340;&#21407;&#22987;&#39044;&#35757;&#32451;&#39044;&#31639;&#22312;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#19978;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;GPT-J&#21644;LLaMA&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Poeta&#65288;&#19968;&#22871;&#30001;14&#20010;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#22871;&#20214;&#65289;&#19978;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#34920;&#29616;&#19978;&#36828;&#20248;&#20110;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#21644;&#22810;&#35821;&#35328;&#30340;&#23545;&#25163;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;Sabi&#225;-65B&#30340;&#34920;&#29616;&#19982;GPT-3.5-turbo&#25345;&#24179;&#12290;&#25105;&#20204;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#24050;&#32463;&#35774;&#24819;&#20102;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#32463;&#36807;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the capabilities of language models continue to advance, it is conceivable that "one-size-fits-all" model will remain as the main paradigm. For instance, given the vast number of languages worldwide, many of which are low-resource, the prevalent practice is to pretrain a single model on multiple languages. In this paper, we add to the growing body of evidence that challenges this practice, demonstrating that monolingual pretraining on the target language significantly improves models already extensively trained on diverse corpora. More specifically, we further pretrain GPT-J and LLaMA models on Portuguese texts using 3% or less of their original pretraining budget. Few-shot evaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models outperform English-centric and multilingual counterparts by a significant margin. Our best model, Sabi\'a-65B, performs on par with GPT-3.5-turbo. By evaluating on datasets originally conceived in the target language as well as transl
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;VQA&#25968;&#25454;&#38598;PDF-VQA&#65292;&#20197;&#22810;&#20010;&#39029;&#38754;&#30340;&#23436;&#25972;&#25991;&#26723;&#20316;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#19982;&#22788;&#29702;&#25991;&#26723;&#20803;&#32032;&#12289;&#32467;&#26500;&#21644;&#20869;&#23481;&#31561;&#26041;&#38754;&#65292;&#20026;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2304.06447</link><description>&lt;p&gt;
PDF-VQA: &#19968;&#20010;&#26032;&#30340;&#29992;&#20110;PDF&#25991;&#20214;&#30495;&#23454;&#19990;&#30028;VQA&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PDF-VQA: A New Dataset for Real-World VQA on PDF Documents. (arXiv:2304.06447v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;VQA&#25968;&#25454;&#38598;PDF-VQA&#65292;&#20197;&#22810;&#20010;&#39029;&#38754;&#30340;&#23436;&#25972;&#25991;&#26723;&#20316;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#19982;&#22788;&#29702;&#25991;&#26723;&#20803;&#32032;&#12289;&#32467;&#26500;&#21644;&#20869;&#23481;&#31561;&#26041;&#38754;&#65292;&#20026;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26723;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#30740;&#31350;&#25991;&#26723;&#22270;&#20687;&#30340;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26723;&#30340;VQA&#25968;&#25454;&#38598;PDF-VQA&#65292;&#20174;&#25991;&#26723;&#20803;&#32032;&#35782;&#21035;&#12289;&#25991;&#26723;&#24067;&#23616;&#32467;&#26500;&#29702;&#35299;&#20197;&#21450;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#31561;&#21508;&#20010;&#26041;&#38754;&#20840;&#38754;&#25506;&#35752;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;PDF-VQA&#25968;&#25454;&#38598;&#23558;&#25991;&#26723;&#29702;&#35299;&#30340;&#35268;&#27169;&#20174;&#21333;&#20010;&#25991;&#26723;&#39029;&#38754;&#25193;&#23637;&#21040;&#35810;&#38382;&#22810;&#20010;&#39029;&#38754;&#30340;&#23436;&#25972;&#25991;&#26723;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;VQA&#27169;&#22411;&#65292;&#26126;&#30830;&#22320;&#38598;&#25104;&#20102;&#19981;&#21516;&#25991;&#26723;&#20803;&#32032;&#20043;&#38388;&#30340;&#31354;&#38388;&#21644;&#23618;&#27425;&#32467;&#26500;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#25991;&#26723;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#35813;&#24615;&#33021;&#19982;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#36739;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#38382;&#39064;&#31867;&#22411;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-based Visual Question Answering examines the document understanding of document images in conditions of natural language questions. We proposed a new document-based VQA dataset, PDF-VQA, to comprehensively examine the document understanding from various aspects, including document element recognition, document layout structural understanding as well as contextual understanding and key information extraction. Our PDF-VQA dataset extends the current scale of document understanding that limits on the single document page to the new scale that asks questions over the full document of multiple pages. We also propose a new graph-based VQA model that explicitly integrates the spatial and hierarchically structural relationships between different document elements to boost the document structural understanding. The performances are compared with several baselines over different question types and tasks\footnote{The full dataset will be released after paper acceptance.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;</title><link>http://arxiv.org/abs/2304.02819</link><description>&lt;p&gt;
GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#25512;&#24191;&#24102;&#26469;&#20102;&#25968;&#23383;&#36890;&#20449;&#26041;&#38754;&#30340;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26041;&#27861;&#26469;&#21306;&#20998;AI&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#33521;&#35821;&#27597;&#35821;&#21644;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#30340;&#20889;&#20316;&#26679;&#26412;&#35780;&#20272;&#20102;&#20960;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;GPT&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#25345;&#32493;&#23558;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20889;&#20316;&#26679;&#26412;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#32780;&#21407;&#29983;&#20889;&#20316;&#26679;&#26412;&#21017;&#33021;&#22815;&#34987;&#20934;&#30830;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#26080;&#24847;&#20013;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21628;&#21505;&#36827;&#34892;&#26356;&#24191;&#27867;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of AI-generated content. Although numerous detection methods have been proposed to differentiate between AI and human-generated content, the fairness and robustness of these detectors remain underexplored. In this study, we evaluate the performance of several widely-used GPT detectors using writing samples from native and non-native English writers. Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors, suggesting that GPT detectors may unintentionally penalize writers with constrained linguistic expressions. Our results call for a broader conversati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00215</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#24402;&#32435;&#20851;&#31995;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#20851;&#31995;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#29616;&#26377;&#30340;&#23884;&#20837;&#24335;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36716;&#23548;&#35774;&#32622;&#65292;&#32570;&#20047;&#24402;&#32435;&#33021;&#21147;&#65292;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#23454;&#20307;&#19978;&#36827;&#34892;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#20998;&#23618;Transformer&#26694;&#26550;&#65292;&#21363;REPORT&#65292;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;REPORT&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#20004;&#20010;&#23436;&#20840;&#24402;&#32435;&#30340;&#25968;&#25454;&#38598;&#30340;&#20843;&#20010;&#29256;&#26412;&#23376;&#38598;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;REPORT&#33021;&#22815;&#23558;&#25512;&#29702;&#25512;&#24191;&#21040;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#27809;&#26377;&#20844;&#20849;&#23454;&#20307;&#30340;&#26032;&#23454;&#20307;&#19978;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation prediction on knowledge graphs (KGs) is a key research topic. Dominant embedding-based methods mainly focus on the transductive setting and lack the inductive ability to generalize to new entities for inference. Existing methods for inductive reasoning mostly mine the connections between entities, i.e., relational paths, without considering the nature of head and tail entities contained in the relational context. This paper proposes a novel method that captures both connections between entities and the intrinsic nature of entities, by simultaneously aggregating RElational Paths and cOntext with a unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely on relation semantics and can naturally generalize to the fully-inductive setting, where KGs for training and inference have no common entities. In the experiments, REPORT performs consistently better than all baselines on almost all the eight version subsets of two fully-inductive datasets. Moreover. REPO
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#39046;&#22495;&#21033;&#29992;LLaMA&#27169;&#22411;&#24494;&#35843;&#30340;&#21307;&#30103;&#32842;&#22825;&#27169;&#22411;ChatDoctor&#12290;&#32463;&#36807;700&#22810;&#31181;&#30142;&#30149;&#21644;&#20854;&#30456;&#24212;&#30151;&#29366;&#12289;&#33647;&#21697;&#21644;&#21307;&#30103;&#26816;&#26597;&#30340;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#29702;&#35299;&#24739;&#32773;&#38656;&#27714;&#12289;&#25552;&#20379;&#24314;&#35758;&#21644;&#24110;&#21161;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#30340;&#27807;&#36890;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.14070</link><description>&lt;p&gt;
ChatDoctor&#65306;&#20351;&#29992;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#22312;LLaMA&#27169;&#22411;&#19978;&#24494;&#35843;&#30340;&#21307;&#30103;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. (arXiv:2303.14070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#39046;&#22495;&#21033;&#29992;LLaMA&#27169;&#22411;&#24494;&#35843;&#30340;&#21307;&#30103;&#32842;&#22825;&#27169;&#22411;ChatDoctor&#12290;&#32463;&#36807;700&#22810;&#31181;&#30142;&#30149;&#21644;&#20854;&#30456;&#24212;&#30151;&#29366;&#12289;&#33647;&#21697;&#21644;&#21307;&#30103;&#26816;&#26597;&#30340;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#29702;&#35299;&#24739;&#32773;&#38656;&#27714;&#12289;&#25552;&#20379;&#24314;&#35758;&#21644;&#24110;&#21161;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#30340;&#27807;&#36890;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#19968;&#33324;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20363;&#22914;ChatGPT&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#20223;&#20315;&#26159;&#20154;&#31867;&#35762;&#35805;&#33324;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#24182;&#27809;&#26377;&#32463;&#36807;&#20010;&#21035;&#19988;&#20180;&#32454;&#20026;&#21307;&#23398;&#39046;&#22495;&#23398;&#20064;&#65292;&#23548;&#33268;&#35786;&#26029;&#20934;&#30830;&#24230;&#20302;&#19988;&#19981;&#33021;&#32473;&#20986;&#27491;&#30830;&#30340;&#21307;&#30103;&#35786;&#26029;&#12289;&#33647;&#21697;&#31561;&#24314;&#35758;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;700&#22810;&#31181;&#30142;&#30149;&#21450;&#20854;&#30456;&#24212;&#30151;&#29366;&#12289;&#25512;&#33616;&#33647;&#21697;&#21644;&#25152;&#38656;&#21307;&#30103;&#26816;&#26597;&#65292;&#28982;&#21518;&#29983;&#25104;&#20102;5K&#21517;&#21307;&#24739;&#30340;&#23545;&#35805;&#12290;&#36890;&#36807;&#24494;&#35843;&#21307;&#24739;&#23545;&#35805;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20102;&#29702;&#35299;&#24739;&#32773;&#38656;&#27714;&#12289;&#25552;&#20379;&#26126;&#26234;&#24314;&#35758;&#24182;&#22312;&#21508;&#31181;&#21307;&#30103;&#30456;&#20851;&#39046;&#22495;&#25552;&#20379;&#23453;&#36149;&#24110;&#21161;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#23558;&#36825;&#20123;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21487;&#20197;&#24443;&#24213;&#25913;&#21464;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#30340;&#27807;&#36890;&#26041;&#24335;&#65292;&#26368;&#32456;&#25913;&#21892;&#25972;&#20307;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) in the general domain, such as ChatGPT, have shown remarkable success in following instructions and producing human-like responses. However, such language models have not been learned individually and carefully for the medical domain, resulting in poor diagnostic accuracy and inability to give correct recommendations for medical diagnosis, medications, etc. To address this issue, we collected more than 700 diseases and their corresponding symptoms, recommended medications, and required medical tests, and then generated 5K doctor-patient conversations. By fine-tuning models of doctor-patient conversations, these models emerge with great potential to understand patients' needs, provide informed advice, and offer valuable assistance in a variety of medical-related fields. The integration of these advanced language models into healthcare can revolutionize the way healthcare professionals and patients communicate, ultimately improving the overall quality 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#26029;&#23545;&#35805;&#20013;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#26469;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#24847;&#22270;&#34701;&#21512;&#27169;&#22359;&#30340;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;InferEM&#12290;&#27169;&#22411;&#21516;&#26102;&#21033;&#29992;&#21069;&#20960;&#27425;&#21457;&#35328;&#39044;&#27979;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.06373</link><description>&lt;p&gt;
InferEM: &#25512;&#26029;&#35828;&#35805;&#32773;&#24847;&#22270;&#30340;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06373
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#26029;&#23545;&#35805;&#20013;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#26469;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#24847;&#22270;&#34701;&#21512;&#27169;&#22359;&#30340;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;InferEM&#12290;&#27169;&#22411;&#21516;&#26102;&#21033;&#29992;&#21069;&#20960;&#27425;&#21457;&#35328;&#39044;&#27979;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20849;&#24773;&#22238;&#22797;&#29983;&#25104;&#30340;&#26041;&#27861;&#19968;&#33324;&#30452;&#25509;&#32534;&#30721;&#25972;&#20010;&#23545;&#35805;&#21382;&#21490;&#65292;&#28982;&#21518;&#36890;&#36807;&#35299;&#30721;&#22120;&#29983;&#25104;&#21451;&#22909;&#30340;&#21453;&#39304;&#12290;&#36825;&#20123;&#26041;&#27861;&#24378;&#35843;&#24314;&#27169;&#24773;&#22659;&#20449;&#24687;&#65292;&#20294;&#24573;&#35270;&#20102;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#30452;&#25509;&#24847;&#22270;&#12290;&#25105;&#20204;&#35748;&#20026;&#23545;&#35805;&#20013;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#34920;&#36798;&#20102;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InferEM&#30340;&#26032;&#27169;&#22411;&#29992;&#20110;&#20849;&#24773;&#22238;&#22797;&#29983;&#25104;&#12290;&#25105;&#20204;&#23558;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#21333;&#29420;&#32534;&#30721;&#65292;&#36890;&#36807;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#24847;&#22270;&#34701;&#21512;&#27169;&#22359;&#19982;&#25972;&#20010;&#23545;&#35805;&#34701;&#21512;&#20197;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#21069;&#20960;&#27425;&#21457;&#35328;&#39044;&#27979;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#30340;&#24515;&#29702;&#65292;&#29468;&#27979;&#23545;&#35805;&#32773;&#21487;&#33021;&#25552;&#21069;&#35828;&#20123;&#20160;&#20040;&#12290;&#20026;&#24179;&#34913;&#21457;&#35328;&#39044;&#27979;&#21644;&#22238;&#22797;&#29983;&#25104;&#30340;&#20248;&#21270;&#36895;&#29575;&#65292;InferEM&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches to empathetic response generation typically encode the entire dialogue history directly and put the output into a decoder to generate friendly feedback. These methods focus on modelling contextual information but neglect capturing the direct intention of the speaker. We argue that the last utterance in the dialogue empirically conveys the intention of the speaker. Consequently, we propose a novel model named InferEM for empathetic response generation. We separately encode the last utterance and fuse it with the entire dialogue through the multi-head attention based intention fusion module to capture the speaker's intention. Besides, we utilize previous utterances to predict the last utterance, which simulates human's psychology to guess what the interlocutor may speak in advance. To balance the optimizing rates of the utterance prediction and response generation, a multi-task learning strategy is designed for InferEM. Experimental results demonstrate the plausibility
&lt;/p&gt;</description></item><item><title>&#25991;&#26412;&#21040;&#35270;&#39057;&#26816;&#32034;&#22256;&#38590;&#65292;&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#34987;&#29992;&#20316;&#26816;&#27979;&#22522;&#20934;&#65292;&#20294;&#23384;&#22312;&#26681;&#26412;&#32570;&#38519;&#23548;&#33268;&#20551;&#38452;&#24615;&#21305;&#37197;&#12290;&#36890;&#36807;&#32416;&#27491;&#20551;&#38452;&#24615;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#25552;&#39640;&#20102;25%&#30340;&#21484;&#22238;&#29575;&#65292;&#38656;&#35201;&#27880;&#37322;&#26356;&#22810;&#25968;&#25454;&#65292;&#37325;&#26032;&#35745;&#31639;&#26377;&#25928;&#24615;&#20998;&#25968;&#65292;&#20197;&#24471;&#20986;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.05038</link><description>&lt;p&gt;
&#20351;&#29992;FIRE&#23545;&#25239;FIRe&#65306;&#35780;&#20272;&#25991;&#26412;&#21040;&#35270;&#39057;&#26816;&#32034;&#22522;&#20934;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video Retrieval Benchmarks. (arXiv:2210.05038v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05038
&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#35270;&#39057;&#26816;&#32034;&#22256;&#38590;&#65292;&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#34987;&#29992;&#20316;&#26816;&#27979;&#22522;&#20934;&#65292;&#20294;&#23384;&#22312;&#26681;&#26412;&#32570;&#38519;&#23548;&#33268;&#20551;&#38452;&#24615;&#21305;&#37197;&#12290;&#36890;&#36807;&#32416;&#27491;&#20551;&#38452;&#24615;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#25552;&#39640;&#20102;25%&#30340;&#21484;&#22238;&#29575;&#65292;&#38656;&#35201;&#27880;&#37322;&#26356;&#22810;&#25968;&#25454;&#65292;&#37325;&#26032;&#35745;&#31639;&#26377;&#25928;&#24615;&#20998;&#25968;&#65292;&#20197;&#24471;&#20986;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#20855;&#26377;&#25991;&#26412;&#25551;&#36848;&#30340;&#35270;&#39057;&#26159;&#19968;&#39033;&#26680;&#24515;&#30340;&#22810;&#27169;&#24335;&#26816;&#32034;&#20219;&#21153;&#12290;&#30001;&#20110;&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#25991;&#26412;&#21040;&#35270;&#39057;&#26816;&#32034;&#30340;&#25968;&#25454;&#38598;&#65292;&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#34987;&#37325;&#26032;&#29992;&#20110;&#36890;&#36807;(1)&#23558;&#23383;&#24149;&#35270;&#20026;&#20854;&#21508;&#33258;&#35270;&#39057;&#30340;&#27491;&#21305;&#37197;&#39033;&#21644;(2)&#20551;&#23450;&#25152;&#26377;&#20854;&#20182;&#35270;&#39057;&#20026;&#36127;&#21305;&#37197;&#39033;&#26469;&#35780;&#20272;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#23384;&#22312;&#19968;&#20010;&#26681;&#26412;&#32570;&#38519;&#65306;&#30001;&#20110;&#21482;&#26377;&#21407;&#22987;&#35270;&#39057;&#26631;&#35760;&#20026;&#30456;&#20851;&#23383;&#24149;&#65292;&#35768;&#22810;&#26367;&#20195;&#35270;&#39057;&#20063;&#21305;&#37197;&#35813;&#23383;&#24149;&#65292;&#23548;&#33268;&#20102;&#20551;&#38452;&#24615;&#30340;&#23383;&#24149;-&#35270;&#39057;&#23545;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#36825;&#20123;&#20551;&#38452;&#24615;&#24471;&#21040;&#32416;&#27491;&#26102;&#65292;&#26368;&#36817;&#30340;&#19968;&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#23558;&#33719;&#24471;25\%&#30340;&#21484;&#22238;&#29575;&#25552;&#21319; - &#36825;&#31181;&#24046;&#24322;&#23041;&#32961;&#20102;&#22522;&#20934;&#26412;&#36523;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35786;&#26029;&#21644;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#27880;&#37322;&#24182;&#21457;&#24067;&#20102;683K&#20010;&#39069;&#22806;&#30340;&#23383;&#24149;-&#35270;&#39057;&#23545;&#12290;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;MSR-VTT&#21644;MSVD&#65289;&#19978;&#37325;&#26032;&#35745;&#31639;&#20102;&#19977;&#20010;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Searching troves of videos with textual descriptions is a core multimodal retrieval task. Owing to the lack of a purpose-built dataset for text-to-video retrieval, video captioning datasets have been re-purposed to evaluate models by (1) treating captions as positive matches to their respective videos and (2) assuming all other videos to be negatives. However, this methodology leads to a fundamental flaw during evaluation: since captions are marked as relevant only to their original video, many alternate videos also match the caption, which introduces false-negative caption-video pairs. We show that when these false negatives are corrected, a recent state-of-the-art model gains 25\% recall points -- a difference that threatens the validity of the benchmark itself. To diagnose and mitigate this issue, we annotate and release 683K additional caption-video pairs. Using these, we recompute effectiveness scores for three models on two standard benchmarks (MSR-VTT and MSVD). We find that (1)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#19968;&#38454;&#25688;&#35201;&#37325;&#25490;&#24207;&#26694;&#26550;COLO&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#23427;&#33021;&#22815;&#25552;&#39640;CNN/DailyMail&#22522;&#20934;&#19978;&#19968;&#38454;&#31995;&#32479;&#30340;&#25277;&#21462;&#21644;&#29983;&#25104;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#21644;&#25512;&#29702;&#25928;&#29575;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12289;&#20197;&#21450;&#21487;&#27604;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14569</link><description>&lt;p&gt;
COLO&#65306;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#19968;&#38454;&#25688;&#35201;&#37325;&#25490;&#24207;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
COLO: A Contrastive Learning based Re-ranking Framework for One-Stage Summarization. (arXiv:2209.14569v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#19968;&#38454;&#25688;&#35201;&#37325;&#25490;&#24207;&#26694;&#26550;COLO&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#23427;&#33021;&#22815;&#25552;&#39640;CNN/DailyMail&#22522;&#20934;&#19978;&#19968;&#38454;&#31995;&#32479;&#30340;&#25277;&#21462;&#21644;&#29983;&#25104;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#21644;&#25512;&#29702;&#25928;&#29575;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12289;&#20197;&#21450;&#21487;&#27604;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#25688;&#35201;&#31995;&#32479;&#30340;&#35757;&#32451;&#33539;&#24335;&#36890;&#24120;&#21482;&#20351;&#29992;&#20196;&#29260;&#32423;&#21035;&#25110;&#21477;&#23376;&#32423;&#21035;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#20294;&#26159;&#65292;&#24635;&#32467;&#36755;&#20986;&#24635;&#26159;&#20174;&#25688;&#35201;&#32423;&#21035;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#23548;&#33268;&#35757;&#32451;&#21644;&#35780;&#20272;&#19981;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#19968;&#38454;&#25688;&#35201;&#37325;&#25490;&#24207;&#26694;&#26550;COLO&#12290;&#36890;&#36807;&#24314;&#27169;&#23545;&#27604;&#30446;&#26631;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25688;&#35201;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#25688;&#35201;&#32423;&#21035;&#24471;&#20998;&#30452;&#25509;&#29983;&#25104;&#25688;&#35201;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27169;&#22359;&#21644;&#21442;&#25968;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;COLO&#25552;&#39640;&#20102;CNN/DailyMail&#22522;&#20934;&#19978;&#19968;&#38454;&#31995;&#32479;&#30340;&#25277;&#21462;&#21644;&#29983;&#25104;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21442;&#25968;&#25928;&#29575;&#21644;&#25512;&#29702;&#25928;&#29575;&#65292;ROUGE-1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;44.58&#21644;46.33&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#26102;&#33410;&#30465;&#20102;100&#22810;&#20010;GPU&#23567;&#26102;&#65292;&#22312;&#25512;&#29702;&#26399;&#38388;&#33719;&#24471;&#20102;3~8&#20493;&#30340;&#21152;&#36895;&#27604;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21487;&#27604;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional training paradigms for extractive and abstractive summarization systems always only use token-level or sentence-level training objectives. However, the output summary is always evaluated from summary-level which leads to the inconsistency in training and evaluation. In this paper, we propose a Contrastive Learning based re-ranking framework for one-stage summarization called COLO. By modeling a contrastive objective, we show that the summarization model is able to directly generate summaries according to the summary-level score without additional modules and parameters. Extensive experiments demonstrate that COLO boosts the extractive and abstractive results of one-stage systems on CNN/DailyMail benchmark to 44.58 and 46.33 ROUGE-1 score while preserving the parameter efficiency and inference efficiency. Compared with state-of-the-art multi-stage systems, we save more than 100 GPU training hours and obtaining 3~8 speed-up ratio during inference while maintaining comparable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#19982;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20195;&#30721;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#30721;&#32763;&#35793;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.03578</link><description>&lt;p&gt;
&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20195;&#30721;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Code Translation with Compiler Representations. (arXiv:2207.03578v4 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#19982;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20195;&#30721;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#30721;&#32763;&#35793;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20302;&#32423;&#21035;&#30340;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#65288;IR&#65289;&#26469;&#25913;&#36827;&#20195;&#30721;&#32763;&#35793;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#21644;IR&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20195;&#30721;&#30340;&#35821;&#20041;&#65292;&#36991;&#20813;&#20197;&#24448;&#26041;&#27861;&#23384;&#22312;&#30340;&#24120;&#35265;&#38169;&#35823;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32763;&#35793;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we leverage low-level compiler intermediate representations (IR) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of corr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36923;&#36753;&#32467;&#26500;&#32422;&#26463;&#24314;&#27169;&#21644;&#35805;&#35821;&#24863;&#30693;&#22270;&#32593;&#32476;&#65288;DAGNs&#65289;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;&#36923;&#36753;&#25512;&#29702;&#38382;&#31572;&#38382;&#39064;&#12290;DAGNs&#21487;&#20197;&#26500;&#24314;&#36923;&#36753;&#22270;&#24182;&#36890;&#36807;&#36793;&#32536;&#25512;&#29702;&#26426;&#21046;&#21644;&#22270;&#29305;&#24449;&#26356;&#26032;&#26469;&#23398;&#20064;&#36923;&#36753;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.01450</link><description>&lt;p&gt;
&#38754;&#21521;&#25991;&#26412;&#36923;&#36753;&#25512;&#29702;&#30340;&#35805;&#35821;&#24863;&#30693;&#22270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Discourse-Aware Graph Networks for Textual Logical Reasoning. (arXiv:2207.01450v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36923;&#36753;&#32467;&#26500;&#32422;&#26463;&#24314;&#27169;&#21644;&#35805;&#35821;&#24863;&#30693;&#22270;&#32593;&#32476;&#65288;DAGNs&#65289;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;&#36923;&#36753;&#25512;&#29702;&#38382;&#31572;&#38382;&#39064;&#12290;DAGNs&#21487;&#20197;&#26500;&#24314;&#36923;&#36753;&#22270;&#24182;&#36890;&#36807;&#36793;&#32536;&#25512;&#29702;&#26426;&#21046;&#21644;&#22270;&#29305;&#24449;&#26356;&#26032;&#26469;&#23398;&#20064;&#36923;&#36753;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#36923;&#36753;&#25512;&#29702;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#21040;&#36923;&#36753;&#25512;&#29702;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#38656;&#35201;&#24847;&#35782;&#21040;&#29305;&#23450;&#30340;&#36923;&#36753;&#32467;&#26500;&#12290;&#27573;&#33853;&#32423;&#36923;&#36753;&#20851;&#31995;&#20195;&#34920;&#20102;&#21629;&#39064;&#21333;&#20803;&#20043;&#38388;&#30340;&#34164;&#28085;&#25110;&#30683;&#30462;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#32467;&#35770;&#21477;&#65289;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#30340;&#38382;&#31572;&#31995;&#32479;&#20391;&#37325;&#20110;&#22522;&#20110;&#23454;&#20307;&#30340;&#20851;&#31995;&#65292;&#36825;&#31181;&#32467;&#26500;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36923;&#36753;&#32467;&#26500;&#32422;&#26463;&#24314;&#27169;&#26469;&#35299;&#20915;&#36923;&#36753;&#25512;&#29702;&#38382;&#31572;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#35805;&#35821;&#24863;&#30693;&#22270;&#32593;&#32476;&#65288;DAGNs&#65289;&#12290;&#35813;&#32593;&#32476;&#39318;&#20808;&#21033;&#29992;&#34892;&#38388;&#35805;&#35821;&#36830;&#25509;&#35789;&#21644;&#36890;&#29992;&#36923;&#36753;&#29702;&#35770;&#26500;&#24314;&#36923;&#36753;&#22270;&#65292;&#28982;&#21518;&#36890;&#36807;&#31471;&#21040;&#31471;&#30340;&#36793;&#32536;&#25512;&#29702;&#26426;&#21046;&#21644;&#22270;&#29305;&#24449;&#26356;&#26032;&#26469;&#23398;&#20064;&#36923;&#36753;&#34920;&#31034;&#12290;&#36825;&#20010;&#27969;&#31243;&#24212;&#29992;&#20110;&#19968;&#20010;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;&#65292;&#20854;&#22522;&#26412;&#29305;&#24449;&#19982;&#39640;&#23618;&#36923;&#36753;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#31572;&#26696;&#39044;&#27979;&#12290;&#22312;&#19977;&#20010;&#25991;&#26412;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual logical reasoning, especially question-answering (QA) tasks with logical reasoning, requires awareness of particular logical structures. The passage-level logical relations represent entailment or contradiction between propositional units (e.g., a concluding sentence). However, such structures are unexplored as current QA systems focus on entity-based relations. In this work, we propose logic structural-constraint modeling to solve the logical reasoning QA and introduce discourse-aware graph networks (DAGNs). The networks first construct logic graphs leveraging in-line discourse connectives and generic logic theories, then learn logic representations by end-to-end evolving the logic relations with an edge-reasoning mechanism and updating the graph features. This pipeline is applied to a general encoder, whose fundamental features are joined with the high-level logic features for answer prediction. Experiments on three textual logical reasoning datasets demonstrate the reasonabi
&lt;/p&gt;</description></item><item><title>CodeAttack&#26159;&#19968;&#20010;&#22522;&#20110;&#20195;&#30721;&#32467;&#26500;&#30340;&#40657;&#30418;&#25915;&#20987;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#26377;&#25928;&#12289;&#39640;&#25928;&#21644;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26679;&#26412;&#65292;&#24182;&#35777;&#26126;&#39044;&#35757;&#32451;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26131;&#21463;&#20195;&#30721;&#29305;&#23450;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2206.00052</link><description>&lt;p&gt;
CodeAttack&#65306;&#38754;&#21521;&#39044;&#35757;&#32451;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#20195;&#30721;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
CodeAttack: Code-Based Adversarial Attacks for Pre-trained Programming Language Models. (arXiv:2206.00052v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00052
&lt;/p&gt;
&lt;p&gt;
CodeAttack&#26159;&#19968;&#20010;&#22522;&#20110;&#20195;&#30721;&#32467;&#26500;&#30340;&#40657;&#30418;&#25915;&#20987;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#26377;&#25928;&#12289;&#39640;&#25928;&#21644;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26679;&#26412;&#65292;&#24182;&#35777;&#26126;&#39044;&#35757;&#32451;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26131;&#21463;&#20195;&#30721;&#29305;&#23450;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;CodeT5&#65292;CodeBERT&#65292;GraphCodeBERT&#31561;&#65289;&#26377;&#28508;&#21147;&#33258;&#21160;&#21270;&#28041;&#21450;&#20195;&#30721;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#30340;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20195;&#30721;&#30340;&#33258;&#28982;&#36890;&#36947;&#20013;&#36816;&#34892;&#65292;&#21363;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#20154;&#23545;&#20195;&#30721;&#30340;&#29702;&#35299;&#12290;&#23427;&#20204;&#23545;&#36755;&#20837;&#30340;&#21464;&#21270;&#19981;&#22815;&#40065;&#26834;&#65292;&#22240;&#27492;&#28508;&#22312;&#26131;&#21463;&#33258;&#28982;&#36890;&#36947;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CodeAttack&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#40657;&#30418;&#25915;&#20987;&#27169;&#22411;&#65292;&#21033;&#29992;&#20195;&#30721;&#32467;&#26500;&#29983;&#25104;&#26377;&#25928;&#12289;&#39640;&#25928;&#21644;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;PL&#27169;&#22411;&#22312;&#20195;&#30721;&#29305;&#23450;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;CodeAttack&#22312;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#30340;&#22810;&#20010;&#20195;&#30721;-&#20195;&#30721;&#65288;&#32763;&#35793;&#21644;&#20462;&#22797;&#65289;&#21644;&#20195;&#30721;-NL&#65288;&#25688;&#35201;&#65289;&#20219;&#21153;&#19978;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;CodeAttack&#22312;&#20840;&#38754;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained programming language (PL) models (such as CodeT5, CodeBERT, GraphCodeBERT, etc.,) have the potential to automate software engineering tasks involving code understanding and code generation. However, these models operate in the natural channel of code, i.e., they are primarily concerned with the human understanding of the code. They are not robust to changes in the input and thus, are potentially susceptible to adversarial attacks in the natural channel. We propose, CodeAttack, a simple yet effective black-box attack model that uses code structure to generate effective, efficient, and imperceptible adversarial code samples and demonstrates the vulnerabilities of the state-of-the-art PL models to code-specific adversarial attacks. We evaluate the transferability of CodeAttack on several code-code (translation and repair) and code-NL (summarization) tasks across different programming languages. CodeAttack outperforms state-of-the-art adversarial NLP attack models to achieve th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#26102;&#23578;&#34892;&#19994;&#30340;CLIP-like&#27169;&#22411;&#8212;&#8212; FashionCLIP&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#20135;&#21697;&#30340;&#26816;&#32034;&#12289;&#20998;&#31867;&#21644;&#23450;&#20301;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#20855;&#21487;&#36716;&#31227;&#24615;&#30340;&#20135;&#21697;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2204.03972</link><description>&lt;p&gt;
&#23545;&#20110;&#26222;&#36941;&#26102;&#23578;&#27010;&#24565;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive language and vision learning of general fashion concepts. (arXiv:2204.03972v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#26102;&#23578;&#34892;&#19994;&#30340;CLIP-like&#27169;&#22411;&#8212;&#8212; FashionCLIP&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#20135;&#21697;&#30340;&#26816;&#32034;&#12289;&#20998;&#31867;&#21644;&#23450;&#20301;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#20855;&#21487;&#36716;&#31227;&#24615;&#30340;&#20135;&#21697;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#36141;&#29289;&#19981;&#26029;&#23835;&#36215;&#65292;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#21457;&#23637;&#32039;&#38543;&#20854;&#21518;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29992;&#20363;&#37117;&#34987;&#35270;&#20026;&#19987;&#19994;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#20174;&#26356;&#20855;&#21487;&#36716;&#31227;&#24615;&#30340;&#20135;&#21697;&#34920;&#24449;&#20013;&#21463;&#30410;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35757;&#32451;&#20986;&#20102;FashionCLIP&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#26102;&#23578;&#34892;&#19994;&#30340;CLIP-like&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#26816;&#32034;&#12289;&#20998;&#31867;&#21644;&#23450;&#20301;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#20195;&#30721;&#21457;&#24067;&#32473;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
The steady rise of online shopping goes hand in hand with the development of increasingly complex ML and NLP models. While most use cases are cast as specialized supervised learning problems, we argue that practitioners would greatly benefit from more transferable representations of products. In this work, we build on recent developments in contrastive learning to train FashionCLIP, a CLIP-like model for the fashion industry. We showcase its capabilities for retrieval, classification and grounding, and release our model and code to the community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;PMC-Patients&#8221;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23450;&#20041;&#21644;&#27979;&#35797;&#30149;&#24739;&#21040;&#25991;&#31456;&#30340;&#26816;&#32034;&#65288;ReCDS-PAR&#65289;&#21644;&#30149;&#24739;&#21040;&#30149;&#24739;&#30340;&#26816;&#32034;&#65288;ReCDS-PPR&#65289;&#65292;&#20197;&#35780;&#20272;&#22522;&#20110;&#21484;&#22238;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;ReCDS&#65289;&#30340;&#24615;&#33021;&#12290;PMC-Patients&#25968;&#25454;&#38598;&#28085;&#30422;&#36926;10,000&#21517;&#30149;&#24739;&#20449;&#24687;&#21644;27,000&#31687;PubMed Central&#25991;&#31456;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#31181;ReCDS&#31995;&#32479;&#30340;&#25928;&#26524;&#20998;&#26512;&#21644;20&#20010;&#26696;&#20363;&#30340;&#23454;&#29992;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2202.13876</link><description>&lt;p&gt;
PMC-Patients: &#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#21484;&#22238;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#30149;&#24739;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PMC-Patients: A Large-scale Dataset of Patient Summaries and Relations for Benchmarking Retrieval-based Clinical Decision Support Systems. (arXiv:2202.13876v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;PMC-Patients&#8221;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23450;&#20041;&#21644;&#27979;&#35797;&#30149;&#24739;&#21040;&#25991;&#31456;&#30340;&#26816;&#32034;&#65288;ReCDS-PAR&#65289;&#21644;&#30149;&#24739;&#21040;&#30149;&#24739;&#30340;&#26816;&#32034;&#65288;ReCDS-PPR&#65289;&#65292;&#20197;&#35780;&#20272;&#22522;&#20110;&#21484;&#22238;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;ReCDS&#65289;&#30340;&#24615;&#33021;&#12290;PMC-Patients&#25968;&#25454;&#38598;&#28085;&#30422;&#36926;10,000&#21517;&#30149;&#24739;&#20449;&#24687;&#21644;27,000&#31687;PubMed Central&#25991;&#31456;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#31181;ReCDS&#31995;&#32479;&#30340;&#25928;&#26524;&#20998;&#26512;&#21644;20&#20010;&#26696;&#20363;&#30340;&#23454;&#29992;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#22522;&#20110;&#21484;&#22238;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;ReCDS&#65289;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#30456;&#20851;&#25991;&#29486;&#21644;&#31867;&#20284;&#30149;&#24739;&#30340;&#20449;&#24687;&#26469;&#24110;&#21161;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#22810;&#26679;&#30340;&#30149;&#24739;&#25910;&#38598;&#21644;&#20844;&#24320;&#30340;&#22823;&#35268;&#27169;&#30149;&#24739;&#23618;&#38754;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;ReCDS&#31995;&#32479;&#30340;&#21457;&#23637;&#21463;&#21040;&#20102;&#20005;&#37325;&#38459;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20351;&#29992;&#21517;&#20026;PMC-Patients&#30340;&#26032;&#25968;&#25454;&#38598;&#23450;&#20041;&#21644;&#27979;&#35797;&#20004;&#20010;ReCDS&#20219;&#21153;&#65306;&#30149;&#24739;&#21040;&#25991;&#31456;&#30340;&#26816;&#32034;&#65288;ReCDS-PAR&#65289;&#21644;&#30149;&#24739;&#21040;&#30149;&#24739;&#30340;&#26816;&#32034;&#65288;ReCDS-PPR&#65289;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20351;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20174;PubMed Central&#25991;&#31456;&#20013;&#25552;&#21462;&#30149;&#24739;&#24635;&#32467;&#65292;&#24182;&#21033;&#29992;PubMed&#24341;&#25991;&#20851;&#31995;&#22270;&#26469;&#23450;&#20041;&#30149;&#24739;-&#25991;&#31456;&#30456;&#20851;&#24615;&#21644;&#30149;&#24739;-&#30149;&#24739;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#36824;&#22312;PMC-Patients&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;&#20960;&#31181;ReCDS&#31995;&#32479;&#65292;&#21253;&#25324;&#31232;&#30095;&#26816;&#32034;&#22120;&#12289;&#23494;&#38598;&#26816;&#32034;&#22120;&#21644;&#26368;&#36817;&#37051;&#26816;&#32034;&#22120;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;PMC-Patients&#30340;&#20020;&#24202;&#25928;&#29992;&#12290;&#32467;&#26524;&#65306;PMC-Patients&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;10,186&#21517;&#30149;&#24739;&#30340;&#20449;&#24687;&#65292;&#28085;&#30422;&#20102;&#36926;27,000&#31687;PubMed Central&#25991;&#31456;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;ReCDS&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#32467;&#26524;&#21644;&#22810;&#31181;&#31995;&#32479;&#30340;&#25928;&#26524;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#20219;&#21153;&#30340;20&#20010;&#26696;&#20363;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;PMC-Patients&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Retrieval-based Clinical Decision Support (ReCDS) can aid clinical workflow by providing relevant literature and similar patients for a given patient. However, the development of ReCDS systems has been severely obstructed by the lack of diverse patient collections and publicly available large-scale patient-level annotation datasets. In this paper, we aim to define and benchmark two ReCDS tasks: Patient-to-Article Retrieval (ReCDS-PAR) and Patient-to-Patient Retrieval (ReCDS-PPR) using a novel dataset called PMC-Patients.  Methods: We extract patient summaries from PubMed Central articles using simple heuristics and utilize the PubMed citation graph to define patient-article relevance and patient-patient similarity. We also implement and evaluate several ReCDS systems on the PMC-Patients benchmarks, including sparse retrievers, dense retrievers, and nearest neighbor retrievers. We conduct several case studies to show the clinical utility of PMC-Patients.  Results: PMC-Patient
&lt;/p&gt;</description></item></channel></rss>