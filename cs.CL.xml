<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20070;&#38754;&#35821;&#19982;&#32553;&#20889;&#23450;&#24459;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21457;&#29616;&#36825;&#19968;&#23450;&#24459;&#20063;&#36866;&#29992;&#20110;&#21475;&#35821;&#12290;&#32467;&#26524;&#25552;&#20379;&#20102;&#21387;&#32553;&#35821;&#35328;&#30340;&#38388;&#25509;&#35777;&#25454;&#65292;&#21363;&#32553;&#20889;&#23450;&#24459;&#26159;&#26368;&#20248;&#32534;&#30721;&#30340;&#39044;&#27979;&#65292;&#32780;&#36890;&#36807;&#33521;&#35821;&#30340;&#21382;&#21490;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#20154;&#20204;&#22312;&#35821;&#35328;&#20013;&#23454;&#38469;&#20351;&#29992;&#30340;&#35789;&#27719;&#25968;&#37327;&#27491;&#22312;&#32553;&#20943;&#12290;</title><link>http://arxiv.org/abs/2303.10128</link><description>&lt;p&gt;
&#30452;&#25509;&#21644;&#38388;&#25509;&#35777;&#25454;&#34920;&#26126;&#35789;&#27719;&#38271;&#24230;&#34987;&#21387;&#32553;. &#23545; Zipf&#30340;&#32553;&#20889;&#23450;&#24459;&#30340;&#37325;&#26032;&#23457;&#35270;.
&lt;/p&gt;
&lt;p&gt;
Direct and indirect evidence of compression of word lengths. Zipf's law of abbreviation revisited. (arXiv:2303.10128v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10128
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20070;&#38754;&#35821;&#19982;&#32553;&#20889;&#23450;&#24459;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21457;&#29616;&#36825;&#19968;&#23450;&#24459;&#20063;&#36866;&#29992;&#20110;&#21475;&#35821;&#12290;&#32467;&#26524;&#25552;&#20379;&#20102;&#21387;&#32553;&#35821;&#35328;&#30340;&#38388;&#25509;&#35777;&#25454;&#65292;&#21363;&#32553;&#20889;&#23450;&#24459;&#26159;&#26368;&#20248;&#32534;&#30721;&#30340;&#39044;&#27979;&#65292;&#32780;&#36890;&#36807;&#33521;&#35821;&#30340;&#21382;&#21490;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#20154;&#20204;&#22312;&#35821;&#35328;&#20013;&#23454;&#38469;&#20351;&#29992;&#30340;&#35789;&#27719;&#25968;&#37327;&#27491;&#22312;&#32553;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Zipf&#32553;&#20889;&#23450;&#24459;&#25351;&#30340;&#26159;&#26356;&#24120;&#35265;&#30340;&#21333;&#35789;&#26356;&#30701;&#65292;&#26159;&#35821;&#35328;&#26222;&#36941;&#24615;&#30340;&#26368;&#22362;&#23454;&#30340;&#20505;&#36873;&#32773;&#65292;&#23427;&#26377;&#21487;&#33021;&#26159;&#27809;&#26377;&#20363;&#22806;&#25110;&#32773;&#20363;&#22806;&#38750;&#24120;&#23569;&#30340;&#35821;&#35328;&#26222;&#36941;&#24615;&#12290;&#33258;&#20174;Zipf&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#20197;&#26469;&#65292;&#36825;&#19968;&#23450;&#24459;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#36890;&#20449;&#30340;&#26222;&#36941;&#21407;&#29702;&#30340;&#34920;&#29616;&#65292;&#21363;&#36890;&#36807;&#32553;&#30701;&#35789;&#27719;&#38271;&#24230;&#26469;&#20943;&#23569;&#36890;&#20449;&#30340;&#21162;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20070;&#38754;&#35821;&#19982;&#32553;&#20889;&#23450;&#24459;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#20851;&#38190;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#26356;&#24191;&#27867;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#36825;&#19968;&#23450;&#24459;&#20063;&#36866;&#29992;&#20110;&#21475;&#35821;&#65288;&#24403;&#29992;&#26102;&#38388;&#26469;&#27979;&#37327;&#35789;&#27719;&#38271;&#24230;&#26102;&#65289;&#65292;&#29305;&#21035;&#26159;&#36866;&#29992;&#20110;&#26469;&#33258;14&#20010;&#35821;&#35328;&#23478;&#26063;&#30340;46&#31181;&#35821;&#35328;&#12290;&#19982;&#32553;&#20889;&#23450;&#24459;&#30340;&#19968;&#33268;&#24615;&#25552;&#20379;&#20102;&#21387;&#32553;&#35821;&#35328;&#30340;&#38388;&#25509;&#35777;&#25454;&#65292;&#36825;&#26159;&#36890;&#36807;&#29702;&#35770;&#35770;&#35777;&#24471;&#20986;&#30340;&#65292;&#21363;&#32553;&#20889;&#23450;&#24459;&#26159;&#26368;&#20248;&#32534;&#30721;&#30340;&#39044;&#27979;&#12290;&#37492;&#20110;&#38656;&#35201;&#30452;&#25509;&#35777;&#25454;&#26469;&#35777;&#26126;&#21387;&#32553;&#65292;&#25105;&#20204;&#36890;&#36807;&#33521;&#35821;&#30340;&#21382;&#21490;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#20204;&#22312;&#35821;&#35328;&#20013;&#23454;&#38469;&#20351;&#29992;&#30340;&#35789;&#27719;&#25968;&#37327;&#27491;&#22312;&#32553;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zipf's law of abbreviation, the tendency of more frequent words to be shorter, is one of the most solid candidates for a linguistic universal, in the sense that it has the potential for being exceptionless or with a number of exceptions that is vanishingly small compared to the number of languages on Earth. Since Zipf's pioneering research, this law has been viewed as a manifestation of a universal principle of communication, i.e. the minimization of word lengths, to reduce the effort of communication. Here we revisit the concordance of written language with the law of abbreviation. Crucially, we provide wider evidence that the law holds also in speech (when word length is measured in time), in particular in 46 languages from 14 linguistic families. Agreement with the law of abbreviation provides indirect evidence of compression of languages via the theoretical argument that the law of abbreviation is a prediction of optimal coding. Motivated by the need of direct evidence of compressi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.10093</link><description>&lt;p&gt;
&#25552;&#39640;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Role of Context in Region-Word Alignment for Object Detection. (arXiv:2303.10093v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23398;&#20064;&#22270;&#20687;-&#26631;&#27880;&#37197;&#23545;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21306;&#22495;-&#35789;&#23545;&#40784;&#65292;&#25512;&#21160;&#20102;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21306;&#22495;-&#35789;&#23545;&#40784;&#26041;&#27861;&#36890;&#24120;&#20165;&#38024;&#23545;&#30446;&#26631;&#21517;&#35789;&#22312;&#26816;&#27979;&#20013;&#20351;&#29992;&#65292;&#20854;&#20182;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23646;&#24615;&#65292;&#23545;&#26816;&#27979;&#30340;&#24433;&#21709;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#30446;&#26631;&#26816;&#27979;&#65292;&#24182;&#25552;&#35758;&#22686;&#24378;&#19978;&#19979;&#25991;&#30340;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#31574;&#30053;&#24615;&#22320;&#23558;&#25509;&#22320;&#39044;&#35757;&#32451;&#30446;&#26631;&#24773;&#22659;&#21270;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23646;&#24615;&#20316;&#20026;&#29305;&#21035;&#26377;&#29992;&#30340;&#30446;&#26631;&#19978;&#19979;&#25991;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#22686;&#21152;&#23545;&#23427;&#20204;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20851;&#27880;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#19982;&#21306;&#22495;-&#35789;&#39044;&#35757;&#32451;&#30340;&#26368;&#26032;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21319;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25991;&#26412;-&#21306;&#22495;&#21487;&#35270;&#21270;&#26174;&#31034;&#23646;&#24615;&#25935;&#24863;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language pretraining to learn a fine-grained, region-word alignment between image-caption pairs has propelled progress in open-vocabulary object detection. We observe that region-word alignment methods are typically used in detection with respect to only object nouns, and the impact of other rich context in captions, such as attributes, is unclear. In this study, we explore how language context affects downstream object detection and propose to enhance the role of context. In particular, we show how to strategically contextualize the grounding pretraining objective for improved alignment. We further hone in on attributes as especially useful object context and propose a novel adjective and noun-based negative sampling strategy for increasing their focus in contrastive learning. Overall, our methods enhance object detection when compared to the state-of-the-art in region-word pretraining. We also highlight the fine-grained utility of an attribute-sensitive model through text-regi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26641;&#24418;&#35821;&#20041;&#21305;&#37197;&#30340;&#27169;&#24335;&#24341;&#23548;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#27169;&#24335;&#24182;&#32467;&#21512;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#29616;&#22312;SGD-X&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#21319;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#21644;&#27169;&#24335;&#28789;&#25935;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.09905</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#24418;&#35821;&#20041;&#21305;&#37197;&#30340;&#27169;&#24335;&#24341;&#23548;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#33021;&#21147;&#26356;&#24378;&#22823;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
More Robust Schema-Guided Dialogue State Tracking via Tree-Based Paraphrase Ranking. (arXiv:2303.09905v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26641;&#24418;&#35821;&#20041;&#21305;&#37197;&#30340;&#27169;&#24335;&#24341;&#23548;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#27169;&#24335;&#24182;&#32467;&#21512;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#29616;&#22312;SGD-X&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#21319;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#21644;&#27169;&#24335;&#28789;&#25935;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#24335;&#24341;&#23548;&#33539;&#24335;&#36890;&#36807;&#21253;&#21547;&#20219;&#21153;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#23618;&#27425;&#32467;&#26500;&#27169;&#24335;&#65292;&#20811;&#26381;&#20102;&#20351;&#29992;&#38745;&#24577;&#26412;&#20307;&#26500;&#24314;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#65288;TOD&#65289;&#20195;&#29702;&#26102;&#23384;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#31934;&#32454;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#24335;&#24341;&#23548;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#65288;DST&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#27169;&#24335;&#30340;&#20070;&#20889;&#39118;&#26684;&#24456;&#25935;&#24863;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25552;&#39640;DST&#27169;&#22411;&#31283;&#20581;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#29983;&#25104;&#20351;&#29992;&#22522;&#20110;&#26641;&#24418;&#25490;&#21517;&#30340;&#21512;&#25104;&#27169;&#24335;&#65292;&#20197;&#20849;&#21516;&#20248;&#21270;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#35821;&#20041;&#24544;&#23454;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;SGD-X&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#29983;&#25104;&#25552;&#31034;&#26469;&#22686;&#21152;&#20854;&#35757;&#32451;&#25968;&#25454;&#21518;&#65292;&#24378;&#22522;&#32447;&#30340;&#27867;&#21270;&#33021;&#21147;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#24179;&#22343;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#65288;JGA&#65289;&#21644;&#27169;&#24335;&#28789;&#25935;&#24230;&#65288;SS&#65289;&#22343;&#26377;&#26174;&#30528;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The schema-guided paradigm overcomes scalability issues inherent in building task-oriented dialogue (TOD) agents with static ontologies. Instead of operating on dialogue context alone, agents have access to hierarchical schemas containing task-relevant natural language descriptions. Fine-tuned language models excel at schema-guided dialogue state tracking (DST) but are sensitive to the writing style of the schemas. We explore methods for improving the robustness of DST models. We propose a framework for generating synthetic schemas which uses tree-based ranking to jointly optimise lexical diversity and semantic faithfulness. The generalisation of strong baselines is improved when augmenting their training data with prompts generated by our framework, as demonstrated by marked improvements in average joint goal accuracy (JGA) and schema sensitivity (SS) on the SGD-X benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.09901</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;3&#19978;&#30340;mCPT&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26694;&#26550;&#26816;&#27979;&#30340;&#22810;&#35821;&#35328;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38646;&#26679;&#26412;&#30340;&#35199;&#29677;&#29273;&#35821;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#33719;&#32988;&#31995;&#32479;&#65292;&#24182;&#22312;&#21478;&#22806;&#20843;&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#25361;&#25112;&#22312;&#20110;&#22312;&#21482;&#26377;&#23569;&#37327;&#25110;&#38646;&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#19968;&#32452;14&#20010;&#26694;&#26550;&#65292;&#21363;&#22810;&#35821;&#35328;&#22810;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;&#38500;&#20102;&#25551;&#36848;&#31995;&#32479;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23884;&#20837;&#31354;&#38388;&#20998;&#26512;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#22914;&#20309;&#25903;&#25345;&#26694;&#26550;&#26816;&#27979;&#20197;&#25512;&#36827;&#35745;&#31639;&#26694;&#26550;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.
&lt;/p&gt;</description></item><item><title>Memotion 3&#26159;&#19968;&#20010;&#21253;&#21547;10,000&#20010;&#24050;&#27880;&#37322;&#27169;&#22240;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;&#21360;&#24230;-&#33521;&#35821;&#28151;&#21512;&#27169;&#22240;&#65292;&#20351;&#20854;&#25104;&#20026;&#35813;&#39046;&#22495;&#20869;&#39318;&#20010;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#65292;&#24182;&#21487;&#29992;&#20110;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#34394;&#20551;&#20449;&#24687;&#25110;&#20167;&#24680;&#20869;&#23481;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.09892</link><description>&lt;p&gt;
Memotion 3: &#20195;&#34920;&#21360;&#24230;-&#33521;&#35821;&#28151;&#21512;&#30721;&#30340;&#24773;&#24863;&#19982;&#24773;&#32490;&#20998;&#26512;&#30340;&#20114;&#32852;&#32593;&#27169;&#22240;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Memotion 3: Dataset on sentiment and emotion analysis of codemixed Hindi-English Memes. (arXiv:2303.09892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09892
&lt;/p&gt;
&lt;p&gt;
Memotion 3&#26159;&#19968;&#20010;&#21253;&#21547;10,000&#20010;&#24050;&#27880;&#37322;&#27169;&#22240;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;&#21360;&#24230;-&#33521;&#35821;&#28151;&#21512;&#27169;&#22240;&#65292;&#20351;&#20854;&#25104;&#20026;&#35813;&#39046;&#22495;&#20869;&#39318;&#20010;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#65292;&#24182;&#21487;&#29992;&#20110;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#34394;&#20551;&#20449;&#24687;&#25110;&#20167;&#24680;&#20869;&#23481;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22240;&#26159;&#29616;&#20170;&#31038;&#20132;&#23186;&#20307;&#19978;&#20256;&#36798;&#24189;&#40664;&#30340;&#26032;&#22411;&#26426;&#21046;&#12290;&#27169;&#22240;&#36890;&#24120;&#21253;&#21547;&#22270;&#29255;&#21644;&#19968;&#20123;&#25991;&#26412;&#12290;&#27169;&#22240;&#21487;&#34987;&#29992;&#20110;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#25110;&#20167;&#24680;&#65292;&#22240;&#27492;&#23545;&#20854;&#36827;&#34892;&#35814;&#32454;&#30340;&#30740;&#31350;&#38750;&#24120;&#20851;&#38190;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Memotion 3&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;10,000&#20010;&#24050;&#27880;&#37322;&#27169;&#22240;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#19982;&#39046;&#22495;&#20869;&#20854;&#20182;&#26222;&#36941;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#21253;&#25324;&#20043;&#21069;&#30340;Memotion&#65292;Memotion 3&#24341;&#20837;&#20102;&#21360;&#24230;-&#33521;&#35821;&#28151;&#21512;&#27169;&#22240;&#65292;&#32780;&#20043;&#21069;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#33521;&#35821;&#27169;&#22240;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;Memotion&#20219;&#21153;&#12289;&#25968;&#25454;&#25910;&#38598;&#21644;&#25968;&#25454;&#38598;&#21019;&#24314;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#20026;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;&#22522;&#20934;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#23558;&#22312; https://github.com/Shreyashm16/Memotion-3.0 &#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memes are the new-age conveyance mechanism for humor on social media sites. Memes often include an image and some text. Memes can be used to promote disinformation or hatred, thus it is crucial to investigate in details. We introduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other prevalent datasets in the domain, including prior iterations of Memotion, Memotion 3 introduces Hindi-English Codemixed memes while prior works in the area were limited to only the English memes. We describe the Memotion task, the data collection and the dataset creation methodologies. We also provide a baseline for the task. The baseline code and dataset will be made available at https://github.com/Shreyashm16/Memotion-3.0
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33521;&#22269;&#22269;&#23478;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#27604;&#21407;&#22987;BERT&#27169;&#22411;&#36798;&#21040;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#22312;&#20844;&#24179;&#12289;&#21487;&#37325;&#22797;&#19988;&#25968;&#25454;&#26377;&#25928;&#30340;&#27604;&#36739;&#30740;&#31350;&#20013;&#65292;&#20182;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#35821;&#26009;&#24211;&#26377;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;LM&#20307;&#31995;&#32467;&#26500;&#31216;&#20026;LTG-BERT&#12290;</title><link>http://arxiv.org/abs/2303.09859</link><description>&lt;p&gt;
&#32463;&#36807;&#35757;&#32451;&#30340;1&#20159;&#21333;&#35789;&#20173;&#28982;&#20445;&#25345;&#29366;&#24577;&#65306;BERT&#32467;&#21512;&#33521;&#22269;&#22269;&#23478;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Trained on 100 million words and still in shape: BERT meets British National Corpus. (arXiv:2303.09859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33521;&#22269;&#22269;&#23478;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#27604;&#21407;&#22987;BERT&#27169;&#22411;&#36798;&#21040;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#22312;&#20844;&#24179;&#12289;&#21487;&#37325;&#22797;&#19988;&#25968;&#25454;&#26377;&#25928;&#30340;&#27604;&#36739;&#30740;&#31350;&#20013;&#65292;&#20182;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#35821;&#26009;&#24211;&#26377;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;LM&#20307;&#31995;&#32467;&#26500;&#31216;&#20026;LTG-BERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#29616;&#20195;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35757;&#32451;&#30340;&#35821;&#26009;&#24211;&#35268;&#27169;&#36234;&#26469;&#36234;&#22823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#32553;&#23567;&#35757;&#32451;&#35268;&#27169;&#21040;&#19968;&#20010;&#35268;&#27169;&#36866;&#20013;&#12289;&#20195;&#34920;&#24615;&#22909;&#12289;&#24179;&#34913;&#24615;&#22909;&#19988;&#20844;&#24320;&#21487;&#29992;&#30340;&#33521;&#25991;&#25991;&#26412;&#28304;-&#33521;&#22269;&#22269;&#23478;&#35821;&#26009;&#24211;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#27604;&#21407;&#22987;BERT&#27169;&#22411;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#31867;&#22411;&#30340;&#35821;&#26009;&#24211;&#20855;&#26377;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#28508;&#21147;&#65292;&#25105;&#20204;&#20197;&#20844;&#24179;&#12289;&#21487;&#37325;&#22797;&#21644;&#25968;&#25454;&#26377;&#25928;&#30340;&#27604;&#36739;&#30740;&#31350;&#20026;&#29305;&#33394;&#65292;&#22312;&#20854;&#20013;&#35780;&#20272;&#20102;&#20960;&#20010;&#35757;&#32451;&#30446;&#26631;&#21644;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#20197;&#31995;&#32479;&#24615;&#30340;&#26041;&#24335;&#22797;&#21046;&#20102;&#20808;&#21069;&#30340;&#32463;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;LM&#20307;&#31995;&#32467;&#26500;&#31216;&#20026;LTG-BERT&#12290;
&lt;/p&gt;
&lt;p&gt;
While modern masked language models (LMs) are trained on ever larger corpora, we here explore the effects of down-scaling training to a modestly-sized but representative, well-balanced, and publicly available English text source -the British National Corpus. We show that pre-training on this carefully curated corpus can reach better performance than the original BERT model. We argue that this type of corpora has great potential as a language modeling benchmark. To showcase this potential, we present fair, reproducible and data-efficient comparative studies of LMs, in which we evaluate several training objectives and model architectures and replicate previous empirical results in a systematic way. We propose an optimized LM architecture called LTG-BERT.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORIC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#21462;&#21160;&#35789;-&#23486;&#35821;&#23545;&#20197;&#36798;&#21040;&#28040;&#38500;&#19981;&#24517;&#35201;&#20449;&#24687;&#30340;&#30446;&#30340;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;&#39640;&#31934;&#24230;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.09827</link><description>&lt;p&gt;
DORIC: &#36890;&#36807;&#20381;&#36182;&#35299;&#26512;&#36827;&#34892;&#39046;&#22495;&#40065;&#26834;&#24494;&#35843;&#30340;&#24320;&#25918;&#24847;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
DORIC : Domain Robust Fine-Tuning for Open Intent Clustering through Dependency Parsing. (arXiv:2303.09827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09827
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORIC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#21462;&#21160;&#35789;-&#23486;&#35821;&#23545;&#20197;&#36798;&#21040;&#28040;&#38500;&#19981;&#24517;&#35201;&#20449;&#24687;&#30340;&#30446;&#30340;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;&#39640;&#31934;&#24230;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;Dialog System Technology Challenges 11&#65288;DSTC11&#65289;&#30340;&#31532;2&#36712;&#36947;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#12290;DSTC11-Track2&#26088;&#22312;&#20026;0-shot&#65292;&#36328;&#39046;&#22495;&#30340;&#24847;&#22270;&#38598;&#24402;&#32435;&#25552;&#20379;&#22522;&#20934;&#12290;&#22312;&#27809;&#26377;&#39046;&#22495;&#20869;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#24378;&#22823;&#30340;&#35805;&#35821;&#34920;&#31034;&#65292;&#21487;&#29992;&#20110;&#36328;&#39046;&#22495;&#24402;&#32435;&#29992;&#25143;&#24847;&#22270;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#25552;&#21462;&#21160;&#35789;-&#23486;&#35821;&#23545;&#20197;&#28040;&#38500;&#19981;&#24517;&#35201;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20026;&#32858;&#31867;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#29983;&#25104;&#27599;&#20010;&#32676;&#38598;&#30340;&#21517;&#31216;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#20102;&#20248;&#31168;&#30340;&#20934;&#30830;&#24615;&#21644;&#26631;&#20934;&#21270;&#20114;&#20449;&#24687;&#65288;NMI&#65289;&#24471;&#20998;&#65292;&#30456;&#36739;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#31934;&#24230;&#24471;&#20998;&#19978;&#33719;&#24471;&#20102;&#31532;&#19977;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present our work on Track 2 in the Dialog System Technology Challenges 11 (DSTC11). DSTC11-Track2 aims to provide a benchmark for zero-shot, cross-domain, intent-set induction. In the absence of in-domain training dataset, robust utterance representation that can be used across domains is necessary to induce users' intentions. To achieve this, we leveraged a multi-domain dialogue dataset to fine-tune the language model and proposed extracting Verb-Object pairs to remove the artifacts of unnecessary information. Furthermore, we devised the method that generates each cluster's name for the explainability of clustered results. Our approach achieved 3rd place in the precision score and showed superior accuracy and normalized mutual information (NMI) score than the baseline model on various domain datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;Ensemble&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38463;&#35821;&#24694;&#24847;&#35328;&#35770;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#22810;&#25968;&#34920;&#20915;&#30340;&#38598;&#25104;&#26041;&#27861;&#20855;&#26377;&#26368;&#20339;&#25928;&#26524;&#65292;&#20854;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;0.86&#65292;F1&#20998;&#25968;&#20026;0.60&#12290;</title><link>http://arxiv.org/abs/2303.09823</link><description>&lt;p&gt;
Transformers&#21644;Ensemble&#26041;&#27861;&#65306;&#38463;&#35821;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Transformers and Ensemble methods: A solution for Hate Speech Detection in Arabic languages. (arXiv:2303.09823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;Ensemble&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38463;&#35821;&#24694;&#24847;&#35328;&#35770;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#22810;&#25968;&#34920;&#20915;&#30340;&#38598;&#25104;&#26041;&#27861;&#20855;&#26377;&#26368;&#20339;&#25928;&#26524;&#65292;&#20854;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;0.86&#65292;F1&#20998;&#25968;&#20026;0.60&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#21442;&#21152;CERIST NLP&#25361;&#25112;&#36187;2022&#20013;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#20849;&#20139;&#20219;&#21153;&#30340;&#23454;&#39564;&#36807;&#31243;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;6&#20010;Transformer&#27169;&#22411;&#21450;&#20854;&#32452;&#21512;&#30340;&#24615;&#33021;&#65292;&#24182;&#20351;&#29992;&#20102;2&#31181;&#38598;&#25104;&#26041;&#27861;&#12290;&#22312;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#30340;&#35757;&#32451;&#38598;&#19978;&#65292;&#22522;&#20110;&#22810;&#25968;&#34920;&#20915;&#30340;&#38598;&#25104;&#26041;&#27861;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#20026;F1&#20998;&#25968;&#20026;0.60&#65292;&#20934;&#30830;&#24615;&#20026;0.86&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our participation in the shared task of hate speech detection, which is one of the subtasks of the CERIST NLP Challenge 2022. Our experiments evaluate the performance of six transformer models and their combination using 2 ensemble approaches. The best results on the training set, in a five-fold cross validation scenario, were obtained by using the ensemble approach based on the majority vote. The evaluation of this approach on the test set resulted in an F1-score of 0.60 and an Accuracy of 0.86.
&lt;/p&gt;</description></item><item><title>CoLT5&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#37325;&#35201;&#26631;&#35760;&#26469;&#21152;&#36895;&#38271;&#36317;&#31163;&#36755;&#20837;&#30340;&#22788;&#29702;&#12290;CoLT5&#22312;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#26368;&#22909;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.09752</link><description>&lt;p&gt;
CoLT5: &#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;&#24555;&#36895;&#38271;&#36317;&#31163;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09752
&lt;/p&gt;
&lt;p&gt;
CoLT5&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#37325;&#35201;&#26631;&#35760;&#26469;&#21152;&#36895;&#38271;&#36317;&#31163;&#36755;&#20837;&#30340;&#22788;&#29702;&#12290;CoLT5&#22312;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#26368;&#22909;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38656;&#35201;&#22788;&#29702;&#38271;&#36755;&#20837;&#65292;&#20294;&#20351;&#29992;Transformer&#22788;&#29702;&#38271;&#25991;&#26723;&#24456;&#26114;&#36149;&#8212;&#8212;&#36825;&#19981;&#20165;&#26159;&#22240;&#20026;&#20108;&#27425;&#27880;&#24847;&#22797;&#26434;&#24615;&#65292;&#36824;&#22240;&#20026;&#23545;&#27599;&#20010;&#26631;&#35760;&#24212;&#29992;&#21069;&#39304;&#21644;&#25237;&#24433;&#23618;&#12290;&#28982;&#32780;&#65292;&#19981;&#26159;&#25152;&#26377;&#26631;&#35760;&#37117;&#21516;&#26679;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36739;&#38271;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CoLT5&#65292;&#19968;&#31181;&#38271;&#36755;&#20837;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#35745;&#31639;&#26469;&#21033;&#29992;&#27492;&#30452;&#35273;&#65292;&#22312;&#21069;&#39304;&#21644;&#27880;&#24847;&#23618;&#20013;&#20026;&#37325;&#35201;&#26631;&#35760;&#25552;&#20379;&#26356;&#22810;&#36164;&#28304;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CoLT5&#27604;LongT5&#34920;&#29616;&#26356;&#24378;&#65292;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#26356;&#24555;&#65292;&#22312;&#38271;&#36755;&#20837;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;SOTA&#12290;&#27492;&#22806;&#65292;CoLT5&#33021;&#22815;&#26377;&#25928;&#19988;&#21487;&#25511;&#22320;&#21033;&#29992;&#26497;&#38271;&#30340;&#36755;&#20837;&#65292;&#23637;&#31034;&#20102;&#39640;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#30340;&#24378;&#22823;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35805;&#29983;&#25104;&#30340;&#26377;&#36873;&#25321;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65288;SDA&#65289;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22686;&#24378;&#20302;&#36136;&#37327;&#21644;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#26696;&#20363;&#65292;&#21487;&#20197;&#22312;&#31070;&#32463;&#23545;&#35805;&#27169;&#22411;&#35757;&#32451;&#20013;&#22823;&#24133;&#25552;&#21319;&#21709;&#24212;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09719</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#35805;&#29983;&#25104;&#30340;&#26377;&#36873;&#25321;&#25968;&#25454;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning towards Selective Data Augmentation for Dialogue Generation. (arXiv:2303.09719v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35805;&#29983;&#25104;&#30340;&#26377;&#36873;&#25321;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65288;SDA&#65289;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22686;&#24378;&#20302;&#36136;&#37327;&#21644;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#26696;&#20363;&#65292;&#21487;&#20197;&#22312;&#31070;&#32463;&#23545;&#35805;&#27169;&#22411;&#35757;&#32451;&#20013;&#22823;&#24133;&#25552;&#21319;&#21709;&#24212;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#31070;&#32463;&#23545;&#35805;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#29616;&#26377;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22312;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22823;&#22810;&#25968;&#26159;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#25152;&#26377;&#26696;&#20363;&#36827;&#34892;&#22686;&#24378;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#19981;&#21516;&#26696;&#20363;&#20043;&#38388;&#30340;&#20869;&#22312;&#23646;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24182;&#38750;&#25152;&#26377;&#26696;&#20363;&#37117;&#36866;&#21512;&#22686;&#24378;&#20219;&#21153;&#65292;&#36866;&#21512;&#22686;&#24378;&#30340;&#26696;&#20363;&#24212;&#35813;&#36981;&#24490;&#20197;&#19979;&#20004;&#20010;&#23646;&#24615;&#65306;&#65288;1&#65289;&#20302;&#36136;&#37327;&#65288;&#23545;&#35805;&#27169;&#22411;&#26080;&#27861;&#20026;&#27492;&#26696;&#20363;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65289;&#65292;&#65288;2&#65289;&#20856;&#22411;&#24615;&#65288;&#26696;&#20363;&#24212;&#35813;&#20195;&#34920;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#65289;&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#36873;&#25321;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65288;SDA&#65289;&#26469;&#25506;&#35752;&#36825;&#20010;&#24819;&#27861;&#65292;SDA&#20351;&#29992;&#21452;&#37325;&#23545;&#25239;&#32593;&#32476;&#22312;&#19968;&#20010;&#38454;&#27573;&#20013;&#36873;&#25321;&#26368;&#20302;&#36136;&#37327;&#21644;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#22686;&#24378;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;SDA&#26694;&#26550;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#24615;&#22686;&#24378;&#20302;&#36136;&#37327;&#21644;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#26696;&#20363;&#26469;&#22823;&#22823;&#25552;&#39640;&#21709;&#24212;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As it is cumbersome and expensive to acquire a huge amount of data for training neural dialog models, data augmentation is proposed to effectively utilize existing training samples. However, current data augmentation techniques on the dialog generation task mostly augment all cases in the training dataset without considering the intrinsic attributes between different cases. We argue that not all cases are beneficial for augmentation task, and the cases suitable for augmentation should obey the following two attributes: (1) low-quality (the dialog model cannot generate a high-quality response for the case), (2) representative (the case should represent the property of the whole dataset). Herein, we explore this idea by proposing a Selective Data Augmentation framework (SDA) for the response generation task. SDA employs a dual adversarial network to select the lowest quality and most representative data points for augmentation in one stage. Extensive experiments conducted on two publicly
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;KD-NAS&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25351;&#23548;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#24182;&#25214;&#21040;&#26368;&#20248;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#23454;&#29616;&#39640;&#25928;&#24072;&#29983;&#30693;&#35782;&#36716;&#31227;&#65292;&#36229;&#36807;&#25163;&#24037;&#35774;&#35745;&#30340;&#23398;&#29983;&#27169;&#22411;&#21644;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.09639</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#39640;&#25928;&#24072;&#29983;&#30693;&#35782;&#36716;&#31227;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search for Effective Teacher-Student Knowledge Transfer in Language Models. (arXiv:2303.09639v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;KD-NAS&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25351;&#23548;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#24182;&#25214;&#21040;&#26368;&#20248;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#23454;&#29616;&#39640;&#25928;&#24072;&#29983;&#30693;&#35782;&#36716;&#31227;&#65292;&#36229;&#36807;&#25163;&#24037;&#35774;&#35745;&#30340;&#23398;&#29983;&#27169;&#22411;&#21644;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#23567;&#22411;&#23398;&#29983;&#25104;&#20026;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#37096;&#32626;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24050;&#32463;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#36873;&#20986;&#30340;&#23398;&#29983;&#27169;&#22411;&#20250;&#23548;&#33268;&#30693;&#35782;&#36716;&#31227;&#30340;&#20302;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25351;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36807;&#31243;&#20174;&#32780;&#25214;&#21040;&#26368;&#20248;&#23398;&#29983;&#27169;&#22411;&#30340;KD-NAS&#26041;&#27861;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#30340;&#27599;&#20010;episode&#20013;&#65292;NAS&#25511;&#21046;&#22120;&#26681;&#25454;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#24310;&#36831;&#30340;&#32452;&#21512;&#39044;&#27979;&#22870;&#21169;&#12290;&#28982;&#21518;&#65292;&#23545;&#25490;&#21517;&#38752;&#21069;&#30340;&#20505;&#36873;&#26550;&#26500;&#36827;&#34892;&#33976;&#39311;&#22788;&#29702;&#12290;&#26368;&#21518;&#36873;&#25321;&#26368;&#39640;&#22870;&#21169;&#30340;&#26550;&#26500;&#24182;&#22266;&#23450;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65292;&#21046;&#20316;&#20986;&#23398;&#29983;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KD-NAS&#33021;&#22815;&#25214;&#21040;&#26377;&#25928;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36164;&#28304;&#21463;&#38480;&#22330;&#26223;&#19979;&#34920;&#29616;&#36229;&#36234;&#25152;&#26377;&#25163;&#24037;&#35774;&#35745;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#29978;&#33267;&#36229;&#36234;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models have achieved state-of-the-art results on a variety of downstream tasks. Knowledge Distillation (KD) of a smaller student model addresses their inefficiency, allowing for deployment in resource-constraint environments. KD however remains ineffective, as the student is manually selected from a set of existing options already pre-trained on large corpora, a sub-optimal choice within the space of all possible student architectures. This paper proposes KD-NAS, the use of Neural Architecture Search (NAS) guided by the Knowledge Distillation process to find the optimal student model for distillation from a teacher, for a given natural language task. In each episode of the search process, a NAS controller predicts a reward based on a combination of accuracy on the downstream task and latency of inference. The top candidate architectures are then distilled from the teacher on a small proxy set. Finally the architecture(s) with the highest reward is selected, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#12290;&#36890;&#36807;&#25910;&#38598;&#34987;&#32534;&#36753;&#22270;&#20687;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#38480;&#21046;&#25152;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09618</link><description>&lt;p&gt;
HIVE&#65306;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
HIVE: Harnessing Human Feedback for Instructional Visual Editing. (arXiv:2303.09618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#12290;&#36890;&#36807;&#25910;&#38598;&#34987;&#32534;&#36753;&#22270;&#20687;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#38480;&#21046;&#25152;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#23545;&#40784;&#21040;&#20154;&#31867;&#20559;&#22909;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20551;&#35774;&#65292;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#24615;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#65292;&#20854;&#36755;&#20986;&#22522;&#20110;&#36755;&#20837;&#22270;&#20687;&#21644;&#32534;&#36753;&#25351;&#20196;&#65292;&#21516;&#26679;&#21487;&#20197;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#21463;&#30410;&#65292;&#22240;&#20026;&#20854;&#36755;&#20986;&#21487;&#33021;&#19981;&#31526;&#21512;&#29992;&#25143;&#30340;&#27491;&#30830;&#25351;&#20196;&#21644;&#20559;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#65288;HIVE&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#32534;&#36753;&#30340;&#22270;&#20687;&#19978;&#25910;&#38598;&#20154;&#31867;&#21453;&#39304;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#20197;&#25429;&#25417;&#22522;&#30784;&#29992;&#25143;&#20559;&#22909;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#21487;&#25193;&#23637;&#30340;&#25193;&#25955;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#26681;&#25454;&#20272;&#35745;&#30340;&#22870;&#21169;&#20540;&#34701;&#20837;&#20154;&#31867;&#20559;&#22909;&#12290;&#27492;&#22806;&#65292;&#20026;&#20943;&#36731;&#25968;&#25454;&#38480;&#21046;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;1M&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;3.6K&#22870;&#21169;&#25968;&#25454;&#38598;&#20197;&#29992;&#20110;&#22870;&#21169;&#23398;&#20064;&#65292;&#20197;&#21450;1K&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#25351;&#23548;&#24615;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of inst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#24515;&#29702;&#27835;&#30103;&#20027;&#39064;&#25512;&#33616;&#30340;AI&#20276;&#20387;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#33719;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#36712;&#36857;&#21487;&#35270;&#21270;&#25552;&#20379;&#23545;&#19981;&#21516;&#22870;&#21169;&#20449;&#21495;&#21644;&#19981;&#21516;&#20020;&#24202;&#35786;&#26029;&#19979;&#35757;&#32451;&#30340;&#31574;&#30053;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.09601</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24515;&#29702;&#27835;&#30103;AI&#20276;&#20387;&#19982;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Psychotherapy AI Companion with Reinforcement Learning Recommendations and Interpretable Policy Dynamics. (arXiv:2303.09601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#24515;&#29702;&#27835;&#30103;&#20027;&#39064;&#25512;&#33616;&#30340;AI&#20276;&#20387;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#33719;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#36712;&#36857;&#21487;&#35270;&#21270;&#25552;&#20379;&#23545;&#19981;&#21516;&#22870;&#21169;&#20449;&#21495;&#21644;&#19981;&#21516;&#20020;&#24202;&#35786;&#26029;&#19979;&#35757;&#32451;&#30340;&#31574;&#30053;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Deep Reinforcement Learning&#65288;DRL&#65289;&#29983;&#25104;&#24515;&#29702;&#27835;&#30103;&#20027;&#39064;&#25512;&#33616;&#30340;&#24378;&#21270;&#23398;&#20064;&#24515;&#29702;&#27835;&#30103;AI&#20276;&#20387;&#12290;&#35813;&#31995;&#32479;&#38024;&#23545;&#22235;&#31181;&#19981;&#21516;&#30340;&#31934;&#31070;&#30142;&#30149;&#65288;&#28966;&#34385;&#30151;&#65292;&#25233;&#37057;&#30151;&#65292;&#31934;&#31070;&#20998;&#35010;&#30151;&#21644;&#33258;&#26432;&#30149;&#20363;&#65289;&#20351;&#29992;&#22810;&#30446;&#26631;&#31574;&#30053;&#29983;&#25104;&#22120;&#36827;&#34892;&#29983;&#25104;&#65292;&#21516;&#26102;&#36890;&#36807;&#19977;&#20010;&#19981;&#21516;&#30340;&#24037;&#20316;&#32852;&#30431;&#35780;&#20998;&#26631;&#20934;&#65288;&#20219;&#21153;&#65292;&#20851;&#31995;&#21644;&#30446;&#26631;&#65289;&#26469;&#26816;&#39564;&#25512;&#33616;&#20027;&#39064;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31995;&#32479;&#33021;&#22815;&#30456;&#23545;&#36739;&#22909;&#22320;&#25429;&#33719;&#30495;&#23454;&#25968;&#25454;&#65288;&#27835;&#30103;&#24072;&#35752;&#35770;&#30340;&#21382;&#21490;&#20027;&#39064;&#65289;&#65292;&#26368;&#20339;&#27169;&#22411;&#30340;&#34920;&#29616;&#22240;&#30142;&#30149;&#21644;&#35780;&#32423;&#26631;&#20934;&#32780;&#24322;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#27934;&#35265;&#65292;&#25105;&#20204;&#22312;2D&#20027;&#25104;&#20998;&#20998;&#26512;&#31354;&#38388;&#21644;&#36716;&#31227;&#30697;&#38453;&#20013;&#21487;&#35270;&#21270;&#31574;&#30053;&#36712;&#36857;&#12290;&#36825;&#20123;&#21487;&#35270;&#21270;&#21576;&#29616;&#20102;&#22312;&#19981;&#21516;&#22870;&#21169;&#20449;&#21495;&#21644;&#19981;&#21516;&#20020;&#24202;&#35786;&#26029;&#19979;&#35757;&#32451;&#30340;&#31574;&#30053;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;&#26412;&#31995;&#32479;&#22312;&#29983;&#25104;&#22810;&#30446;&#26631;&#31574;&#30053;&#24515;&#29702;&#27835;&#30103;&#20027;&#39064;&#26041;&#38754;&#30340;&#25104;&#21151;&#34920;&#29616;&#20026;&#24320;&#21457;&#33021;&#22815;&#24110;&#21161;&#27835;&#30103;&#24072;&#25552;&#20379;&#20010;&#24615;&#21270;&#27835;&#30103;&#30340;AI&#20276;&#20387;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a Reinforcement Learning Psychotherapy AI Companion that generates topic recommendations for therapists based on patient responses. The system uses Deep Reinforcement Learning (DRL) to generate multi-objective policies for four different psychiatric conditions: anxiety, depression, schizophrenia, and suicidal cases. We present our experimental results on the accuracy of recommended topics using three different scales of working alliance ratings: task, bond, and goal. We show that the system is able to capture the real data (historical topics discussed by the therapists) relatively well, and that the best performing models vary by disorder and rating scale. To gain interpretable insights into the learned policies, we visualize policy trajectories in a 2D principal component analysis space and transition matrices. These visualizations reveal distinct patterns in the policies trained with different reward signals and trained on different clinical diagnoses. Our system's succe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#22522;&#20110; CRF &#21644;&#28145;&#24230;&#23398;&#20064;&#65288;&#22914; BanglaBERT&#65289;&#30340;&#40065;&#26834;&#23391;&#21152;&#25289;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102; CNER &#20219;&#21153;&#65292;&#22635;&#34917;&#20102;&#23391;&#21152;&#25289;&#35821;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2303.09306</link><description>&lt;p&gt;
&#26500;&#24314;&#40065;&#26834;&#30340;&#23391;&#21152;&#25289;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Bangla Complex Named Entity Recognition. (arXiv:2303.09306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#22522;&#20110; CRF &#21644;&#28145;&#24230;&#23398;&#20064;&#65288;&#22914; BanglaBERT&#65289;&#30340;&#40065;&#26834;&#23391;&#21152;&#25289;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102; CNER &#20219;&#21153;&#65292;&#22635;&#34917;&#20102;&#23391;&#21152;&#25289;&#35821;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035; (NER) &#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#21253;&#25324;&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#21629;&#21517;&#23454;&#20307;&#12290;&#23613;&#31649;&#23391;&#21152;&#25289;&#35821;&#26159;&#20840;&#29699;&#31532;&#19971;&#22823;&#20351;&#29992;&#35821;&#35328;&#65292;&#20294;&#38024;&#23545;&#23391;&#21152;&#25289;&#35821;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#24037;&#20316;&#36824;&#24456;&#23569;&#12290;CNER &#26159;&#19968;&#39033;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#35782;&#21035;&#21644;&#20998;&#31867;&#22797;&#26434;&#21644;&#22797;&#21512;&#23454;&#20307;&#65292;&#32780;&#36825;&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#19981;&#24120;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915; BanglaCoNER &#25968;&#25454;&#38598;&#19978;&#30340; CNER &#20219;&#21153;&#30340;&#33719;&#32988;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21363;&#26465;&#20214;&#38543;&#26426;&#22330; (CRF) &#21644;&#22522;&#20110; finetuning transformer &#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914; BanglaBERT&#65289;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324; 15300 &#20010;&#29992;&#20110;&#35757;&#32451;&#30340;&#21477;&#23376;&#21644; 800 &#20010;&#29992;&#20110;&#39564;&#35777;&#30340;&#21477;&#23376;&#65292;&#26684;&#24335;&#20026; .conll&#12290;&#23545;&#25968;&#25454;&#38598;&#30340;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512; (EDA) &#25581;&#31034;&#20986;&#25968;&#25454;&#38598;&#26377; 7 &#31181;&#19981;&#21516;&#30340; NER &#26631;&#31614;&#65292;&#20854;&#20013;&#26377;&#33521;&#35821;&#21333;&#35789;&#30340;&#26126;&#26174;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) is a fundamental task in natural language processing that involves identifying and classifying named entities in text. But much work hasn't been done for complex named entity recognition in Bangla, despite being the seventh most spoken language globally. CNER is a more challenging task than traditional NER as it involves identifying and classifying complex and compound entities, which are not common in Bangla language. In this paper, we present the winning solution of Bangla Complex Named Entity Recognition Challenge - addressing the CNER task on BanglaCoNER dataset using two different approaches, namely Conditional Random Fields (CRF) and finetuning transformer based Deep Learning models such as BanglaBERT.  The dataset consisted of 15300 sentences for training and 800 sentences for validation, in the .conll format. Exploratory Data Analysis (EDA) on the dataset revealed that the dataset had 7 different NER tags, with notable presence of English words, s
&lt;/p&gt;</description></item><item><title>PRESTO&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;55&#19975;&#20010;&#20154;&#19982;&#34394;&#25311;&#21161;&#25163;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#22810;&#35821;&#35328;&#23545;&#35805;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#22914;&#35828;&#35805;&#19981;&#36830;&#36143;&#12289;&#20195;&#30721;&#20999;&#25442;&#21644;&#20462;&#27491;&#31561;&#30495;&#23454;NLU&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.08954</link><description>&lt;p&gt;
PRESTO&#65306;&#29992;&#20110;&#35299;&#26512;&#36924;&#30495;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs. (arXiv:2303.08954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08954
&lt;/p&gt;
&lt;p&gt;
PRESTO&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;55&#19975;&#20010;&#20154;&#19982;&#34394;&#25311;&#21161;&#25163;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#22810;&#35821;&#35328;&#23545;&#35805;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#22914;&#35828;&#35805;&#19981;&#36830;&#36143;&#12289;&#20195;&#30721;&#20999;&#25442;&#21644;&#20462;&#27491;&#31561;&#30495;&#23454;NLU&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Google Assistant&#12289;Alexa&#21644;Siri&#31561;&#31995;&#32479;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#21464;&#24471;&#26222;&#36941;&#65292;&#20154;&#20204;&#23545;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#30340;&#30740;&#31350;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#22312;&#25429;&#25417;&#21508;&#31181;&#29992;&#25143;&#30171;&#28857;&#30340;&#23454;&#38469;&#24773;&#20917;&#26041;&#38754;&#32570;&#20047;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#36825;&#38480;&#21046;&#20102;&#23398;&#26415;&#30740;&#31350;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20351;&#20851;&#20110;&#35299;&#26512;&#36924;&#30495;&#23545;&#35805;&#30340;&#19968;&#20123;&#25361;&#25112;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#25104;&#20026;&#21487;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PRESTO&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;55&#19975;&#20010;&#20154;&#19982;&#34394;&#25311;&#21161;&#25163;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#22810;&#35821;&#35328;&#23545;&#35805;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;PRESTO&#21253;&#21547;&#20102;&#30495;&#23454;NLU&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#22810;&#26679;&#21270;&#25361;&#25112;&#65292;&#22914;&#35828;&#35805;&#19981;&#36830;&#36143;&#12289;&#20195;&#30721;&#20999;&#25442;&#21644;&#20462;&#27491;&#12290;&#23427;&#26159;&#21807;&#19968;&#19968;&#20010;&#25552;&#20379;&#27599;&#20010;&#31034;&#20363;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#22914;&#29992;&#25143;&#30340;&#32852;&#31995;&#20154;&#21644;&#21015;&#34920;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#29983;&#25104;&#20250;&#35805;&#35299;&#26512;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22522;&#20110;mT5&#27169;&#22411;&#30340;&#22522;&#32447;&#34920;&#26126;&#65292;PRESTO&#20013;&#23384;&#22312;&#30340;&#23545;&#35805;&#29616;&#35937;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#22312;&#20302;&#36164;&#28304;&#35774;&#32622;&#20013;&#26356;&#21152;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research interest in task-oriented dialogs has increased as systems such as Google Assistant, Alexa and Siri have become ubiquitous in everyday life. However, the impact of academic research in this area has been limited by the lack of datasets that realistically capture the wide array of user pain points. To enable research on some of the more challenging aspects of parsing realistic conversations, we introduce PRESTO, a public dataset of over 550K contextual multilingual conversations between humans and virtual assistants. PRESTO contains a diverse array of challenges that occur in real-world NLU tasks such as disfluencies, code-switching, and revisions. It is the only large scale human generated conversational parsing dataset that provides structured context such as a user's contacts and lists for each example. Our mT5 model based baselines demonstrate that the conversational phenomenon present in PRESTO are challenging to model, which is further pronounced in a low-resource setup.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#23454;&#20307;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#65292;&#19988;&#22312;&#21475;&#35821;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2302.10186</link><description>&lt;p&gt;
&#34394;&#25311;&#20195;&#29702;&#20154;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#21270;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
E2E Spoken Entity Extraction for Virtual Agents. (arXiv:2302.10186v4 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#23454;&#20307;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#65292;&#19988;&#22312;&#21475;&#35821;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#26500;&#24819;&#20102;&#35821;&#38899;&#22788;&#29702;&#20013;&#30340;&#19968;&#20123;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#23454;&#20307;&#65292;&#32780;&#26080;&#38656;&#20013;&#38388;&#25991;&#26412;&#34920;&#31034;&#12290;&#22312;&#20154;&#19982;&#35745;&#31639;&#26426;&#30340;&#23545;&#35805;&#20013;&#65292;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#23454;&#20307;&#65292;&#22914;&#22995;&#21517;&#12289;&#37038;&#25919;&#22320;&#22336;&#21644;&#30005;&#23376;&#37038;&#20214;&#22320;&#22336;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#23545;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#21487;&#35835;&#24615;&#24378;&#30340;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#25991;&#26412;&#36716;&#24405;&#12290;&#25105;&#20204;&#35828;&#26126;&#36825;&#31181;&#30452;&#25509;&#26041;&#27861;&#20248;&#21270;&#20102;&#32534;&#30721;&#22120;&#65292;&#20197;&#20165;&#36716;&#24405;&#35821;&#38899;&#20013;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#37096;&#20998;&#65292;&#24573;&#30053;&#20102;&#22810;&#20313;&#30340;&#37096;&#20998;&#65292;&#22914;&#25645;&#26723;&#35821;&#25110;&#23454;&#20307;&#25340;&#20889;&#12290;&#22312;&#20225;&#19994;&#34394;&#25311;&#20195;&#29702;&#20154;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#27493;&#27861;&#30340;&#26041;&#27861;&#20248;&#20110;&#20856;&#22411;&#30340;&#20004;&#27493;&#27861;&#65292;&#21363;&#39318;&#20808;&#20135;&#29983;&#35789;&#27719;&#36716;&#24405;&#65292;&#28982;&#21518;&#36827;&#34892;&#22522;&#20110;&#25991;&#26412;&#30340;&#23454;&#20307;&#25552;&#21462;&#20197;&#35782;&#21035;&#21475;&#35821;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reimagines some aspects of speech processing using speech encoders, specifically about extracting entities directly from speech, with no intermediate textual representation. In human-computer conversations, extracting entities such as names, postal addresses and email addresses from speech is a challenging task. In this paper, we study the impact of fine-tuning pre-trained speech encoders on extracting spoken entities in human-readable form directly from speech without the need for text transcription. We illustrate that such a direct approach optimizes the encoder to transcribe only the entity relevant portions of speech, ignoring the superfluous portions such as carrier phrases and spellings of entities. In the context of dialogs from an enterprise virtual agent, we demonstrate that the 1-step approach outperforms the typical 2-step cascade of first generating lexical transcriptions followed by text-based entity extraction for identifying spoken entities.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36138;&#23146;&#37325;&#25490;&#26435;&#37325;&#30697;&#38453;&#30340;&#31639;&#27861;AEIUOrder&#65292;&#33021;&#22815;&#26368;&#22823;&#21270;&#24635;&#30340;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#23618;&#25152;&#36129;&#29486;&#30340;"well-trainedness"&#25351;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#24182;&#22312;&#21508;&#31181;&#32763;&#35793;&#20219;&#21153;&#19978;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02123</link><description>&lt;p&gt;
&#22522;&#20110;Transformers&#30340;&#36138;&#23146;&#25490;&#24207;&#20248;&#21270;&#32763;&#35793;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Greedy Ordering of Layer Weight Matrices in Transformers Improves Translation. (arXiv:2302.02123v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36138;&#23146;&#37325;&#25490;&#26435;&#37325;&#30697;&#38453;&#30340;&#31639;&#27861;AEIUOrder&#65292;&#33021;&#22815;&#26368;&#22823;&#21270;&#24635;&#30340;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#23618;&#25152;&#36129;&#29486;&#30340;"well-trainedness"&#25351;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#24182;&#22312;&#21508;&#31181;&#32763;&#35793;&#20219;&#21153;&#19978;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#24037;&#20316;&#35797;&#22270;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#23376;&#22270;&#23618;&#30340;&#23618;&#27425;&#19978;&#29702;&#35299;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#21151;&#33021;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#19981;&#26816;&#26597;&#20302;&#23618;&#27425;&#30340;&#32467;&#26500;&#65292;&#23601;&#19981;&#33021;&#28145;&#20837;&#29702;&#35299;&#23376;&#23618;&#37325;&#25490;&#32972;&#21518;&#30340;&#21160;&#26426;&#12290;&#26412;&#25991;&#36890;&#36807;AEIUOrder&#31639;&#27861;&#65292;&#36890;&#36807;&#34913;&#37327;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#23618;&#30340;Heavy-Tailed Self-Regularization&#65288;HT-SR&#65289;&#25351;&#26631;&#65292;&#36138;&#23146;&#22320;&#23545;&#32534;&#30721;&#22120;&#20013;&#30340;&#23618;&#37325;&#37327;&#30697;&#38453;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#28982;&#21518;&#30456;&#24212;&#22320;&#25490;&#24207;&#35299;&#30721;&#22120;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24635;&#30340;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#23618;&#25152;&#36129;&#29486;&#30340;"well-trainedness"&#25351;&#26631;&#36827;&#34892;&#37325;&#25490;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#36755;&#20986;&#65292;&#22312;&#21508;&#31181;&#32763;&#35793;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has attempted to understand the internal structures and functionalities of Transformer-based encoder-decoder architectures on the level of multi-head attention and feed-forward sublayers. Interpretations have focused on the encoder and decoder, along with the combinatorial possibilities of the self-attention, cross-attention, and feed-forward sublayers. However, without examining the low-level structures, one gains limited understanding of the motivation behind sublayer reordering. Could we dive into the sublayer abstraction and permute layer weight matrices to improve the quality of translation? We propose AEIUOrder to greedily reorder layer weight matrices in the encoder by their well-trainedness, as measured by Heavy-Tailed Self-Regularization (HT-SR) metrics, and order the decoder matrices correspondingly. Our results suggest that greedily reordering layer weight matrices to maximize Total well-trainedness facilitates the model to learn representations and generate trans
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#21160;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#37197;&#21512;&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#25216;&#26415;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#31185;&#23398;&#25945;&#32946;&#39046;&#22495;&#30340;&#35770;&#35777;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2301.08771</link><description>&lt;p&gt;
&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31185;&#23398;&#25945;&#32946;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#21160;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt Learning for Automatic Scoring in Science Education. (arXiv:2301.08771v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#21160;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#37197;&#21512;&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#25216;&#26415;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#31185;&#23398;&#25945;&#32946;&#39046;&#22495;&#30340;&#35770;&#35777;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#33258;&#21160;&#35780;&#20998;&#31185;&#23398;&#38382;&#39064;&#30340;&#23398;&#29983;&#20070;&#38754;&#31572;&#26696;&#30340;&#27169;&#22411;&#23545;&#20110;&#31185;&#23398;&#25945;&#32946;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#21644;&#26631;&#35760;&#36275;&#22815;&#30340;&#23398;&#29983;&#31572;&#26696;&#20197;&#35757;&#32451;&#27169;&#22411;&#26159;&#32791;&#26102;&#21644;&#36153;&#29992;&#39640;&#26114;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;prompt&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#31185;&#23398;&#25945;&#32946;&#20013;&#36824;&#27809;&#26377;&#20351;&#29992;&#36807;&#36825;&#31181;&#25552;&#31034;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#30001;&#20110;&#23398;&#29983;&#30340;&#31572;&#26696;&#26159;&#29992;&#33258;&#28982;&#35821;&#35328;&#21576;&#29616;&#30340;&#65292;&#22240;&#27492;&#20351;&#29992;&#25552;&#31034;&#23558;&#35780;&#20998;&#36807;&#31243;&#23545;&#40784;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#20219;&#21153;&#21487;&#20197;&#36339;&#36807;&#26114;&#36149;&#30340;&#35843;&#25972;&#38454;&#27573;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#65288;MeNSP&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#33258;&#21160;&#35780;&#20998;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#35780;&#20998;&#19977;&#20010;&#31185;&#23398;&#35770;&#35777;&#20219;&#21153;&#20013;&#24212;&#29992;MeNSP&#65292;&#24182;&#21457;&#29616;&#26426;&#22120;-&#20154;&#35780;&#20998;&#30340;&#19968;&#33268;&#24615;&#65292;Cohen&#30340;Kappa&#31995;&#25968;&#22312;0.30&#21040;0.57&#20043;&#38388;&#65292;F1&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Developing models to automatically score students' written responses to science problems is critical for science education. However, collecting and labeling sufficient student responses for training models is time and cost-consuming. Recent studies suggest that pre-trained language models (PLMs) can be adapted to downstream tasks without fine-tuning with prompts. However, no research has employed such a prompt approach in science education. As student responses are presented with natural language, aligning the scoring procedure as the next sentence prediction task using prompts can skip the costly fine-tuning stage. In this study, we developed a zero-shot approach to automatically score student responses via Matching Exemplars as Next Sentence Prediction (MeNSP). This approach employs no training samples. We first apply MeNSP in scoring three assessment tasks of scientific argumentation and found machine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and F1 score ran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22810;&#23610;&#24230;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#32416;&#20559;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#36935;&#21040;&#30340;&#25968;&#25454;&#38598;&#20154;&#20026;&#21046;&#36896;&#25928;&#24212;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#25200;&#21160;&#27979;&#35797;&#30340;&#25269;&#25239;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.08756</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#30340;&#22810;&#23610;&#24230;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#32416;&#20559;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Scales Data Augmentation Approach In Natural Language Inference For Artifacts Mitigation And Pre-Trained Model Optimization. (arXiv:2212.08756v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22810;&#23610;&#24230;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#32416;&#20559;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#36935;&#21040;&#30340;&#25968;&#25454;&#38598;&#20154;&#20026;&#21046;&#36896;&#25928;&#24212;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#25200;&#21160;&#27979;&#35797;&#30340;&#25269;&#25239;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22522;&#20934;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#19979;&#21364;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24050;&#39044;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20013;&#25968;&#25454;&#38598;&#20154;&#20026;&#21046;&#36896;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#21363;&#36923;&#36753;&#20851;&#31995;&#22312;&#19968;&#23545;&#25991;&#26412;&#24207;&#21015;&#20013;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#25216;&#26415;&#26469;&#20998;&#26512;&#21644;&#23450;&#20301;Stanford&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;SNLI&#65289;&#35821;&#26009;&#24211;&#20869;&#30340;&#25968;&#25454;&#38598;&#20154;&#20026;&#21046;&#36896;&#25928;&#24212;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;SNLI&#35821;&#26009;&#24211;&#20013;&#25968;&#25454;&#38598;&#20154;&#20026;&#21046;&#36896;&#25928;&#24212;&#30340;&#39118;&#26684;&#27169;&#24335;&#24182;&#37319;&#29992;&#20102;&#29420;&#29305;&#30340;&#22810;&#23610;&#24230;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20854;&#20013;&#21253;&#25324;&#21477;&#23376;&#32423;&#30340;&#34892;&#20026;&#27979;&#35797;&#26816;&#26597;&#34920;&#21644;&#21333;&#35789;&#32423;&#30340;&#35789;&#27719;&#21516;&#20041;&#35789;&#26631;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#30340;&#32452;&#21512;&#26041;&#27861;&#25552;&#39640;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#25200;&#21160;&#27979;&#35797;&#30340;&#25269;&#25239;&#21147;&#65292;&#20351;&#20854;&#25345;&#32493;&#20248;&#20110;&#39044;&#35757;&#32451;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models can reach high performance on benchmark natural language processing (NLP) datasets but fail in more challenging settings. We study this issue when a pre-trained model learns dataset artifacts in natural language inference (NLI), the topic of studying the logical relationship between a pair of text sequences. We provide a variety of techniques for analyzing and locating dataset artifacts inside the crowdsourced Stanford Natural Language Inference (SNLI) corpus. We study the stylistic pattern of dataset artifacts in the SNLI. To mitigate dataset artifacts, we employ a unique multi-scale data augmentation technique with two distinct frameworks: a behavioral testing checklist at the sentence level and lexical synonym criteria at the word level. Specifically, our combination method enhances our model's resistance to perturbation testing, enabling it to continuously outperform the pre-trained baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#24847;&#22270;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#32858;&#31867;&#31639;&#27861;&#21644;&#29992;&#25143;&#35805;&#35821;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MiniLM&#19982;&#23618;&#27425;&#32858;&#31867;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.02021</link><description>&lt;p&gt;
&#19982;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#24847;&#22270;&#35782;&#21035;&#30456;&#20851;&#30340;&#35805;&#35821;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Utterance Embeddings and Clustering Methods Related to Intent Induction for Task-Oriented Dialogue. (arXiv:2212.02021v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#24847;&#22270;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#32858;&#31867;&#31639;&#27861;&#21644;&#29992;&#25143;&#35805;&#35821;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MiniLM&#19982;&#23618;&#27425;&#32858;&#31867;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#35774;&#35745;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#22270;&#35889;&#20013;&#30340;&#20856;&#22411;&#25361;&#25112;&#65306;&#20026;&#27599;&#20010;&#23545;&#35805;&#36716;&#25240;&#25351;&#23450;&#24847;&#22270;&#26631;&#31614;&#65288;&#24847;&#22270;&#32858;&#31867;&#65289;&#24182;&#22522;&#20110;&#24847;&#22270;&#32858;&#31867;&#26041;&#27861;&#29983;&#25104;&#19968;&#32452;&#24847;&#22270;&#65288;&#24847;&#22270;&#24402;&#32435;&#65289;&#12290;&#25105;&#20204;&#20551;&#35774;&#33258;&#21160;&#24402;&#32435;&#24847;&#22270;&#26377;&#20004;&#20010;&#26174;&#33879;&#22240;&#32032;&#65306;&#65288;1&#65289;&#24847;&#22270;&#26631;&#31614;&#30340;&#32858;&#31867;&#31639;&#27861;&#21644;&#65288;2&#65289;&#29992;&#25143;&#35805;&#35821;&#23884;&#20837;&#31354;&#38388;&#12290; &#25105;&#20204;&#26681;&#25454;DSTC11&#35780;&#20272;&#27604;&#36739;&#20102;&#29616;&#26377;&#30340;&#25104;&#21697;&#32858;&#31867;&#27169;&#22411;&#21644;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35748;&#30495;&#32771;&#34385;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#20013;&#35805;&#35821;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#32508;&#21512;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MiniLM&#19982;&#23618;&#27425;&#32858;&#31867;&#30456;&#32467;&#21512;&#21487;&#26174;&#33879;&#25552;&#39640;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#20013;&#30340;NMI&#65292;ARI&#65292;F1&#65292;&#20934;&#30830;&#24615;&#21644;&#31034;&#20363;&#35206;&#30422;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/Jeiyoon/dstc11-track2&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The focus of this work is to investigate unsupervised approaches to overcome quintessential challenges in designing task-oriented dialog schema: assigning intent labels to each dialog turn (intent clustering) and generating a set of intents based on the intent clustering methods (intent induction). We postulate there are two salient factors for automatic induction of intents: (1) clustering algorithm for intent labeling and (2) user utterance embedding space. We compare existing off-the-shelf clustering models and embeddings based on DSTC11 evaluation. Our extensive experiments demonstrate that the combined selection of utterance embedding and clustering method in the intent induction task should be carefully considered. We also present that pretrained MiniLM with Agglomerative clustering shows significant improvement in NMI, ARI, F1, accuracy and example coverage in intent induction tasks. The source codes are available at https://github.com/Jeiyoon/dstc11-track2.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#25216;&#26415;&#26469;&#35299;&#20915;&#22270;&#20687;&#21465;&#36848;&#20013;&#30340;&#25351;&#20195;&#28040;&#35299;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#24615;&#33021;&#21644;&#20248;&#21183;&#65292;&#25351;&#20195;&#28040;&#35299;&#26377;&#21161;&#20110;&#25552;&#39640;&#22270;&#29255;&#20013;&#30340;&#21465;&#36848;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.14563</link><description>&lt;p&gt;
&#22270;&#20687;&#21465;&#36848;&#20013;&#30340;&#25351;&#20195;&#28040;&#35299;&#65306;&#20320;&#25351;&#30340;&#26159;&#35841;&#65311;
&lt;/p&gt;
&lt;p&gt;
Who are you referring to? Coreference resolution in image narrations. (arXiv:2211.14563v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#25216;&#26415;&#26469;&#35299;&#20915;&#22270;&#20687;&#21465;&#36848;&#20013;&#30340;&#25351;&#20195;&#28040;&#35299;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#24615;&#33021;&#21644;&#20248;&#21183;&#65292;&#25351;&#20195;&#28040;&#35299;&#26377;&#21161;&#20110;&#25552;&#39640;&#22270;&#29255;&#20013;&#30340;&#21465;&#36848;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20195;&#28040;&#35299;&#30340;&#30446;&#26631;&#26159;&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#25351;&#21521;&#21516;&#19968;&#23454;&#20307;&#30340;&#35789;&#35821;&#21644;&#30701;&#35821;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#39033;&#26680;&#24515;&#20219;&#21153;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#20219;&#21153;&#25193;&#23637;&#21040;&#38271;&#31687;&#35270;&#35273;&#22330;&#26223;&#21465;&#36848;&#20013;&#30340;&#25351;&#20195;&#28040;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27880;&#37322;&#20102;&#25351;&#20195;&#38142;&#21450;&#20854;&#36793;&#30028;&#26694;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#29616;&#26377;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#20165;&#21253;&#21547;&#19981;&#24102;&#25351;&#31034;&#24615;&#34920;&#36798;&#24335;&#25110;&#26631;&#35760;&#38142;&#30340;&#30701;&#21477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#24369;&#30417;&#30563;&#21482;&#20174;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#20351;&#29992;&#20808;&#21069;&#30340;&#35821;&#35328;&#30693;&#35782;&#30340;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#35782;&#21035;&#25351;&#20195;&#38142;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#25351;&#20195;&#28040;&#35299;&#26041;&#38754;&#27604;&#20960;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25351;&#20195;&#28040;&#35299;&#26377;&#21161;&#20110;&#25552;&#39640;&#22270;&#29255;&#20013;&#30340;&#21465;&#36848;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coreference resolution aims to identify words and phrases which refer to same entity in a text, a core task in natural language processing. In this paper, we extend this task to resolving coreferences in long-form narrations of visual scenes. First we introduce a new dataset with annotated coreference chains and their bounding boxes, as most existing image-text datasets only contain short sentences without coreferring expressions or labeled chains. We propose a new technique that learns to identify coreference chains using weak supervision, only from image-text pairs and a regularization using prior linguistic knowledge. Our model yields large performance gains over several strong baselines in resolving coreferences. We also show that coreference resolution helps improving grounding narratives in images.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#36880;&#23618;&#20013;&#38388;&#34920;&#31034;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#32534;&#30721;&#22768;&#23398;&#12289;&#35821;&#38899;&#21644;&#21333;&#35789;&#32423;&#23646;&#24615;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24046;&#24322;&#19982;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#36873;&#25321;&#30456;&#20851;&#12290;&#36890;&#36807;&#27604;&#36739;&#23646;&#24615;&#36235;&#21183;&#21644;&#35821;&#38899;&#35782;&#21035;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21457;&#29616;CCA&#36235;&#21183;&#20026;&#36873;&#25321;&#23618;&#27425;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2211.03929</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#36880;&#23618;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative layer-wise analysis of self-supervised speech models. (arXiv:2211.03929v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#36880;&#23618;&#20013;&#38388;&#34920;&#31034;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#32534;&#30721;&#22768;&#23398;&#12289;&#35821;&#38899;&#21644;&#21333;&#35789;&#32423;&#23646;&#24615;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24046;&#24322;&#19982;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#36873;&#25321;&#30456;&#20851;&#12290;&#36890;&#36807;&#27604;&#36739;&#23646;&#24615;&#36235;&#21183;&#21644;&#35821;&#38899;&#35782;&#21035;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21457;&#29616;CCA&#36235;&#21183;&#20026;&#36873;&#25321;&#23618;&#27425;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35768;&#22810;&#19981;&#21516;&#39044;&#35757;&#32451;&#30446;&#26631;&#12289;&#36755;&#20837;&#24418;&#24335;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#34987;&#25552;&#20986;&#12290;&#23613;&#31649;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#20173;&#28982;&#23545;&#36825;&#20123;&#27169;&#22411;&#32534;&#30721;&#30340;&#23646;&#24615;&#21450;&#20854;&#24046;&#24322;&#20102;&#35299;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;(CCA)&#30340;&#36731;&#37327;&#32423;&#20998;&#26512;&#24037;&#20855;&#65292;&#26816;&#26597;&#20102;&#22810;&#20010;&#26368;&#36817;&#27169;&#22411;&#30340;&#20013;&#38388;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#21333;&#20010;&#23618;&#27425;&#20013;&#32534;&#30721;&#30340;&#22768;&#23398;&#12289;&#35821;&#38899;&#21644;&#21333;&#35789;&#32423;&#23646;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#23646;&#24615;&#22312;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#23618;&#27425;&#28436;&#21464;&#26041;&#24335;&#19981;&#21516;&#65292;&#19988;&#21464;&#21270;&#19982;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#36873;&#25321;&#30456;&#20851;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#23646;&#24615;&#36235;&#21183;&#21644;&#35821;&#38899;&#35782;&#21035;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;&#21457;&#29616;CCA&#36235;&#21183;&#20026;&#36873;&#25321;&#23618;&#27425;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many self-supervised speech models, varying in their pre-training objective, input modality, and pre-training data, have been proposed in the last few years. Despite impressive successes on downstream tasks, we still have a limited understanding of the properties encoded by the models and the differences across models. In this work, we examine the intermediate representations for a variety of recent models. Specifically, we measure acoustic, phonetic, and word-level properties encoded in individual layers, using a lightweight analysis tool based on canonical correlation analysis (CCA). We find that these properties evolve across layers differently depending on the model, and the variations relate to the choice of pre-training objective. We further investigate the utility of our analyses for downstream tasks by comparing the property trends with performance on speech recognition and spoken language understanding tasks. We discover that CCA trends provide reliable guidance to choose laye
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#31572;&#26696;&#25512;&#26029;&#65292;&#24182;&#25506;&#35752;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#30340;&#38382;&#39064;&#25552;&#31034;&#26041;&#24335;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.12353</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#31572;&#26696;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Multiple Choice Question Answering. (arXiv:2210.12353v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#31572;&#26696;&#25512;&#26029;&#65292;&#24182;&#25506;&#35752;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#30340;&#38382;&#39064;&#25552;&#31034;&#26041;&#24335;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;GPT-3&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#26696;&#25512;&#26029;&#20219;&#21153;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#33853;&#21518;&#20110;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#30340;&#38382;&#39064;&#25552;&#31034;&#26041;&#24335;&#65292;&#21363;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#36873;&#39033;&#32852;&#21512;&#21576;&#29616;&#32473;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20415;&#27169;&#22411;&#33021;&#22815;&#26126;&#30830;&#27604;&#36739;&#31572;&#26696;&#36873;&#39033;&#65292;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#38477;&#20302;&#26631;&#35760;&#21270;&#26041;&#26696;&#21644;&#31572;&#26696;&#36873;&#39033;&#34920;&#31034;&#23545;&#31572;&#26696;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g., "A") associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24212;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23558;&#34920;&#26684;&#25968;&#25454;&#24207;&#21015;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#23383;&#31526;&#20018;&#36827;&#34892;&#20998;&#31867;&#65292;&#24494;&#35843;&#21518;&#21363;&#21487;&#22312;&#38750;&#24120;&#23569;&#30340;&#26679;&#26412;&#35774;&#32622;&#19979;&#19982;&#20256;&#32479;&#22522;&#32447;&#26041;&#27861;&#31454;&#20105;&#21147;&#21313;&#36275;&#12290;</title><link>http://arxiv.org/abs/2210.10723</link><description>&lt;p&gt;
TabLLM: &#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
TabLLM: Few-shot Classification of Tabular Data with Large Language Models. (arXiv:2210.10723v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24212;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23558;&#34920;&#26684;&#25968;&#25454;&#24207;&#21015;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#23383;&#31526;&#20018;&#36827;&#34892;&#20998;&#31867;&#65292;&#24494;&#35843;&#21518;&#21363;&#21487;&#22312;&#38750;&#24120;&#23569;&#30340;&#26679;&#26412;&#35774;&#32622;&#19979;&#19982;&#20256;&#32479;&#22522;&#32447;&#26041;&#27861;&#31454;&#20105;&#21147;&#21313;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#23558;&#34920;&#26684;&#25968;&#25454;&#24207;&#21015;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#23383;&#31526;&#20018;&#65292;&#24182;&#21152;&#19978;&#20998;&#31867;&#38382;&#39064;&#30340;&#31616;&#30701;&#25551;&#36848;&#65292;&#28982;&#21518;&#21551;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20123;&#26631;&#35760;&#26679;&#26412;&#24494;&#35843;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#31181;&#24207;&#21015;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#27169;&#26495;&#12289;&#34920;&#26684;&#21040;&#25991;&#26412;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#23613;&#31649;&#26041;&#27861;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34920;&#26684;&#20998;&#31867;&#26041;&#27861;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#26159;&#38646;&#26679;&#26412;&#20998;&#31867;&#20063;&#33719;&#24471;&#20102;&#38750;&#24179;&#20961;&#30340;&#34920;&#29616;&#65292;&#35828;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#19982;&#35768;&#22810;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#38750;&#24120;&#23569;&#30340;&#26679;&#26412;&#35774;&#32622;&#19979;&#20063;&#19982;&#24378;&#22823;&#30340;&#20256;&#32479;&#22522;&#32447;&#26041;&#27861;&#65288;&#22914;&#26799;&#24230;&#25552;&#21319;&#26641;&#65289;&#31454;&#20105;&#21147;&#21313;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method's ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Treeformer&#65292;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#20998;&#23618;&#23548;&#33322;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35745;&#31639;&#27880;&#24847;&#21147;&#12290;&#19982;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26041;&#27861;&#30456;&#27604;&#65292;Treeformer &#21487;&#20197;&#23558;&#26816;&#32034;&#25104;&#26412;&#20174;&#32447;&#24615;&#38477;&#20026;&#36817;&#20284;&#23545;&#25968;&#32423;&#21035;&#65292;&#24182;&#25552;&#20379;&#20004;&#31181;&#26377;&#25928;&#30340;&#20851;&#27880;&#23618;&#12290;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#22788;&#29702;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#36807;&#38271;&#30340;&#24212;&#29992;&#65292;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2208.09015</link><description>&lt;p&gt;
Treeformer: &#31264;&#23494;&#26799;&#24230;&#26641;&#23454;&#29616;&#39640;&#25928;&#27880;&#24847;&#21147;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Treeformer: Dense Gradient Trees for Efficient Attention Computation. (arXiv:2208.09015v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Treeformer&#65292;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#20998;&#23618;&#23548;&#33322;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35745;&#31639;&#27880;&#24847;&#21147;&#12290;&#19982;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26041;&#27861;&#30456;&#27604;&#65292;Treeformer &#21487;&#20197;&#23558;&#26816;&#32034;&#25104;&#26412;&#20174;&#32447;&#24615;&#38477;&#20026;&#36817;&#20284;&#23545;&#25968;&#32423;&#21035;&#65292;&#24182;&#25552;&#20379;&#20004;&#31181;&#26377;&#25928;&#30340;&#20851;&#27880;&#23618;&#12290;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#22788;&#29702;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#36807;&#38271;&#30340;&#24212;&#29992;&#65292;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#25512;&#29702;&#21644;&#35757;&#32451;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#25104;&#20108;&#27425;&#20851;&#31995;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#24212;&#29992;&#22914;&#32593;&#39029;&#32763;&#35793;&#21644;&#26597;&#35810;-&#22238;&#31572;&#31561;&#36895;&#24230;&#35201;&#27714;&#36739;&#39640;&#30340;&#24212;&#29992;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#21152;&#36895;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#36817;&#26399;&#25552;&#20986;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#22914;&#24378;&#21046;&#20351;&#29992;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#32467;&#26500;&#65292;&#22914;&#31232;&#30095;&#12289;&#20302;&#31209;&#12289;&#20351;&#29992;&#26680;&#20989;&#25968;&#36924;&#36817;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#27880;&#24847;&#21147;&#35745;&#31639;&#30475;&#20316;&#26368;&#36817;&#37051;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#20998;&#23618;&#23548;&#33322;&#65292;&#23558;&#27599;&#20010;&#26597;&#35810;&#26631;&#35760;&#30340;&#26816;&#32034;&#25104;&#26412;&#20174;&#32447;&#24615;&#38477;&#20026;&#36817;&#20284;&#23545;&#25968;&#32423;&#21035;&#12290;&#22522;&#20110;&#36825;&#31181;&#20998;&#23618;&#23548;&#33322;&#65292;&#25105;&#20204;&#35774;&#35745;&#20986;Treeformer&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#20004;&#31181;&#26377;&#25928;&#30340;&#20851;&#27880;&#23618;-- TF-Attention&#21644;TC-Attention&#12290;TF-Attention&#20197;&#32454;&#31890;&#24230;&#26041;&#24335;&#35745;&#31639;&#20854;&#20013;&#30340;&#20851;&#27880;&#65292;&#32780;TC-Attention&#26159;&#19968;&#31181;&#26356;&#31895;&#31890;&#24230;&#30340;&#20851;&#27880;&#23618;&#65292;&#20063;&#30830;&#20445;&#26799;&#24230;&#26159;&#8220;&#23494;&#38598;&#8221;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard inference and training with transformer based architectures scale quadratically with input sequence length. This is prohibitively large for a variety of applications especially in web-page translation, query-answering etc. Consequently, several approaches have been developed recently to speedup attention computation by enforcing different attention structures such as sparsity, low-rank, approximating attention using kernels. In this work, we view attention computation as that of nearest neighbor retrieval, and use decision tree based hierarchical navigation to reduce the retrieval cost per query token from linear in sequence length to nearly logarithmic. Based on such hierarchical navigation, we design Treeformer which can use one of two efficient attention layers -- TF-Attention and TC-Attention. TF-Attention computes the attention in a fine-grained style, while TC-Attention is a coarse attention layer which also ensures that the gradients are "dense". To optimize such challe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22312;&#24179;&#38754;&#24615;&#21644;&#25237;&#24433;&#24615;&#23450;&#20041;&#19979;&#26641;&#30340;&#26368;&#22823;&#32447;&#24615;&#25490;&#21015;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#25237;&#24433;&#21644;&#24179;&#38754;&#25490;&#21015;&#30340;&#22810;&#20010;&#24615;&#36136;&#65292;&#21457;&#29616;&#27611;&#27611;&#34411;&#26641;&#26368;&#20248;&#65292;&#25512;&#24191;&#20102;&#20043;&#21069;&#30340;&#26497;&#20540;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.06924</link><description>&lt;p&gt;
&#22522;&#20110;&#24179;&#38754;&#24615;&#19982;&#25237;&#24433;&#24615;&#23450;&#20041;&#19979;&#26641;&#30340;&#26368;&#22823;&#32447;&#24615;&#25490;&#21015;&#38382;&#39064;&#65288;arXiv:2206.06924v3[cs.DS] &#26356;&#26032;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
The Maximum Linear Arrangement Problem for trees under projectivity and planarity. (arXiv:2206.06924v3 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06924
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22312;&#24179;&#38754;&#24615;&#21644;&#25237;&#24433;&#24615;&#23450;&#20041;&#19979;&#26641;&#30340;&#26368;&#22823;&#32447;&#24615;&#25490;&#21015;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#25237;&#24433;&#21644;&#24179;&#38754;&#25490;&#21015;&#30340;&#22810;&#20010;&#24615;&#36136;&#65292;&#21457;&#29616;&#27611;&#27611;&#34411;&#26641;&#26368;&#20248;&#65292;&#25512;&#24191;&#20102;&#20043;&#21069;&#30340;&#26497;&#20540;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#32447;&#24615;&#25490;&#21015;&#38382;&#39064;(MaxLA)&#26159;&#25351;&#25214;&#21040;&#20174;&#22270;G&#30340;n&#20010;&#39030;&#28857;&#21040;&#19981;&#21516;&#36830;&#32493;&#25972;&#25968;&#30340;&#26144;&#23556;$ \pi $&#65292;&#20351;&#24471;$ D(G)=\sum_{uv\in E(G)}|\pi(u)-\pi(v)| $&#26368;&#22823;&#21270;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#39030;&#28857;&#34987;&#35748;&#20026;&#22312;&#19968;&#26465;&#27700;&#24179;&#32447;&#19978;&#65292;&#24182;&#19988;&#36793;&#26159;&#20316;&#20026;&#21322;&#22278;&#24359;&#30011;&#22312;&#32447;&#19978;&#26041;&#30340;&#12290;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#25490;&#21015;&#30340;MaxLA&#21464;&#20307;&#12290;&#22312;&#24179;&#38754;&#21464;&#20307;&#20013;&#65292;&#31105;&#27490;&#36793;&#20132;&#21449;&#12290;&#22312;&#38024;&#23545;&#26681;&#26641;&#30340;&#25237;&#24433;&#21464;&#20307;&#20013;&#65292;&#25490;&#21015;&#26159;&#24179;&#38754;&#30340;&#65292;&#32780;&#26681;&#19981;&#33021;&#34987;&#20219;&#20309;&#36793;&#35206;&#30422;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#35299;&#20915;&#26641;&#30340;&#24179;&#38754;&#21644;&#25237;&#24433;MaxLA&#30340;O(n)-&#26102;&#38388;&#21644;O(n&#65289;-&#31354;&#38388;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#26368;&#22823;&#25237;&#24433;&#21644;&#24179;&#38754;&#25490;&#21015;&#30340;&#22810;&#20010;&#24615;&#36136;&#65292;&#24182;&#19988;&#34920;&#26126;&#27611;&#27611;&#34411;&#26641;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#25152;&#26377;&#26641;&#20013;&#26368;&#22823;&#21270;&#24179;&#38754;MaxLA&#65292;&#22240;&#27492;&#25512;&#24191;&#20102;&#20808;&#21069;&#20851;&#20110;&#26641;&#30340;&#26497;&#20540;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Maximum Linear Arrangement problem (MaxLA) consists of finding a mapping $\pi$ from the $n$ vertices of a graph $G$ to distinct consecutive integers that maximizes $D(G)=\sum_{uv\in E(G)}|\pi(u) - \pi(v)|$. In this setting, vertices are considered to lie on a horizontal line and edges are drawn as semicircles above the line. There exist variants of MaxLA in which the arrangements are constrained. In the planar variant, edge crossings are forbidden. In the projective variant for rooted trees, arrangements are planar and the root cannot be covered by any edge. Here we present $O(n)$-time and $O(n)$-space algorithms that solve planar and projective MaxLA for trees. We also prove several properties of maximum projective and planar arrangements, and show that caterpillar trees maximize planar MaxLA over all trees of a fixed size thereby generalizing a previous extremal result on trees.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#23454;&#39564;&#34920;&#26126;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;NER&#26041;&#27861;&#21487;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2109.01636</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01636
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#23454;&#39564;&#34920;&#26126;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;NER&#26041;&#27861;&#21487;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;NER&#20219;&#21153;&#38754;&#20020;&#30340;&#26368;&#22823;&#22256;&#38590;&#26159;&#21363;&#20351;&#22312;NE&#31867;&#22411;&#21644;&#25991;&#26723;&#19981;&#29087;&#24713;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#38656;&#35201;&#20445;&#25345;&#21487;&#26816;&#27979;&#24615;&#12290;&#24847;&#35782;&#21040;&#29305;&#23450;&#24615;&#20449;&#24687;&#21487;&#33021;&#21253;&#21547;&#21333;&#35789;&#30340;&#28508;&#22312;&#21547;&#20041;&#24182;&#29983;&#25104;&#35789;&#23884;&#20837;&#30340;&#35821;&#20041;&#30456;&#20851;&#29305;&#24449;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;&#29616;&#26377;&#30340;NER&#26041;&#27861;&#20013;&#65292;NER&#30340;&#24615;&#33021;&#23558;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the fast development of Deep Learning techniques, Named Entity Recognition (NER) is becoming more and more important in the information extraction task. The greatest difficulty that the NER task faces is to keep the detectability even when types of NE and documents are unfamiliar. Realizing that the specificity information may contain potential meanings of a word and generate semantic-related features for word embedding, we develop a distribution-aware word embedding and implement three different methods to make use of the distribution information in a NER framework. And the result shows that the performance of NER will be improved if the word specificity is incorporated into existing NER methods.
&lt;/p&gt;</description></item></channel></rss>