<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LaGTran&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#25551;&#36848;&#26469;&#24341;&#23548;&#30693;&#35782;&#36716;&#31227;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.05535</link><description>&lt;p&gt;
&#35762;&#36848;&#65292;&#32780;&#19981;&#26159;&#23637;&#31034;&#65281;&#65306;&#35821;&#35328;&#25351;&#23548;&#26377;&#21161;&#20110;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05535
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LaGTran&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#25551;&#36848;&#26469;&#24341;&#23548;&#30693;&#35782;&#36716;&#31227;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;LaGTran&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21363;&#21487;&#33719;&#24471;&#25110;&#26131;&#20110;&#33719;&#21462;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24341;&#23548;&#20174;&#24102;&#26631;&#31614;&#30340;&#28304;&#25968;&#25454;&#21040;&#20855;&#26377;&#22495;&#20559;&#31227;&#30340;&#26080;&#26631;&#31614;&#30446;&#26631;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#30693;&#35782;&#36716;&#31227;&#12290;&#21463;&#21040;&#25105;&#20204;&#35266;&#23519;&#21040;&#26356;&#23500;&#35821;&#20041;&#30340;&#25991;&#26412;&#27169;&#24577;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#36716;&#31227;&#29305;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36716;&#31227;&#26426;&#21046;&#65292;&#20351;&#29992;&#28304;&#35757;&#32451;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#30446;&#26631;&#25991;&#26412;&#25551;&#36848;&#19978;&#29983;&#25104;&#39044;&#27979;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#39044;&#27979;&#20316;&#20026;&#30456;&#24212;&#22270;&#20687;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#35821;&#35328;&#25351;&#23548;&#20026;&#39537;&#21160;&#65292;&#20986;&#22855;&#22320;&#31616;&#21333;&#26131;&#34892;&#65292;&#21364;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#22914;GeoNet&#21644;DomainNet&#19978;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#25152;&#26377;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20854;&#26497;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05535v1 Announce Type: cross  Abstract: We introduce LaGTran, a novel framework that utilizes readily available or easily acquired text descriptions to guide robust transfer of discriminative knowledge from labeled source to unlabeled target data with domain shifts. While unsupervised adaptation methods have been established to address this problem, they show limitations in handling challenging domain shifts due to their exclusive operation within the pixel-space. Motivated by our observation that semantically richer text modality has more favorable transfer properties, we devise a transfer mechanism to use a source-trained text-classifier to generate predictions on the target text descriptions, and utilize these predictions as supervision for the corresponding images. Our approach driven by language guidance is surprisingly easy and simple, yet significantly outperforms all prior approaches on challenging datasets like GeoNet and DomainNet, validating its extreme effectiven
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OPEN&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26469;&#24341;&#23548;&#20559;&#22909;&#24341;&#23548;</title><link>https://arxiv.org/abs/2403.05534</link><description>&lt;p&gt;
&#20855;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Bayesian Preference Elicitation with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05534
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OPEN&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26469;&#24341;&#23548;&#20559;&#22909;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#29992;&#25143;&#20852;&#36259;&#23545;&#40784;&#38656;&#35201;&#29702;&#35299;&#24182;&#34701;&#20837;&#20154;&#31867;&#22797;&#26434;&#30340;&#20215;&#20540;&#35266;&#21644;&#20559;&#22909;&#12290;&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24050;&#34987;&#29992;&#20110;&#25910;&#38598;&#20851;&#20110;&#20154;&#31867;&#29992;&#25143;&#20559;&#22909;&#30340;&#20449;&#24687;&#12290;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#20559;&#22909;&#25968;&#25454;&#26469;&#24494;&#35843;&#25110;&#25351;&#23548;&#20854;&#20182;LM&#21644;/&#25110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#21457;&#29616;LM&#22312;&#20559;&#22909;&#23398;&#20064;&#30340;&#20851;&#38190;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65306;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12289;&#24314;&#27169;&#20154;&#31867;&#24515;&#29702;&#29366;&#24577;&#21644;&#25552;&#20986;&#20449;&#24687;&#24615;&#38382;&#39064;&#12290;&#36825;&#20123;&#25361;&#25112;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#20854;&#20182;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#35299;&#20915;&#65292;&#27604;&#22914;&#36125;&#21494;&#26031;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#65288;BOED&#65289;&#65292;&#23427;&#20391;&#37325;&#20110;&#22312;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#29305;&#24449;&#31354;&#38388;&#20869;&#35774;&#35745;&#20449;&#24687;&#20016;&#23500;&#30340;&#26597;&#35810;&#12290;&#20294;&#36825;&#20123;&#26041;&#27861;&#21453;&#36807;&#26469;&#24456;&#38590;&#25193;&#23637;&#24182;&#24212;&#29992;&#20110;&#22312;&#21738;&#37324;&#20165;&#20165;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#37117;&#21487;&#33021;&#24456;&#22256;&#38590;&#30340;&#29616;&#23454;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;OPEN&#65288;Optimal Preference Elicitation with Natural language&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;BOED&#26469;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05534v1 Announce Type: new  Abstract: Aligning AI systems to users' interests requires understanding and incorporating humans' complex values and preferences. Recently, language models (LMs) have been used to gather information about the preferences of human users. This preference data can be used to fine-tune or guide other LMs and/or AI systems. However, LMs have been shown to struggle with crucial aspects of preference learning: quantifying uncertainty, modeling human mental states, and asking informative questions. These challenges have been addressed in other areas of machine learning, such as Bayesian Optimal Experimental Design (BOED), which focus on designing informative queries within a well-defined feature space. But these methods, in turn, are difficult to scale and apply to real-world problems where simply identifying the relevant features can be difficult. We introduce OPEN (Optimal Preference Elicitation with Natural language) a framework that uses BOED to guid
&lt;/p&gt;</description></item><item><title>Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.05530</link><description>&lt;p&gt;
Gemini 1.5&#65306;&#35299;&#38145;&#36328;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05530
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20221;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Gemini&#23478;&#26063;&#30340;&#26368;&#26032;&#27169;&#22411;Gemini 1.5 Pro&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#22238;&#24518;&#21644;&#25512;&#29702;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#20013;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#21253;&#25324;&#22810;&#20010;&#38271;&#25991;&#26723;&#21644;&#20960;&#23567;&#26102;&#30340;&#35270;&#39057;&#21644;&#38899;&#39057;&#12290;Gemini 1.5 Pro&#22312;&#21508;&#31181;&#24418;&#24335;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#24191;&#27867;&#19968;&#31995;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;Gemini 1.0 Ultra&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30456;&#21305;&#25932;&#29978;&#33267;&#36229;&#36807;&#12290;&#22312;&#30740;&#31350;Gemini 1.5 Pro&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#26497;&#38480;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33267;&#23569;10M&#26631;&#35760;&#30340;&#33539;&#22260;&#20869;&#32487;&#32493;&#25913;&#36827;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#20960;&#20046;&#23436;&#32654;&#22320;&#36798;&#21040;&#20102;&#36229;&#36807;99%&#30340;&#26816;&#32034;&#29575;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#27169;&#22411;&#22914;Claude 2.1&#65288;200k&#65289;&#21644;GPT-4 Turbo&#65288;128k&#65289;&#30340;&#19990;&#20195;&#24615;&#39134;&#36291;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05530v1 Announce Type: cross  Abstract: In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (&gt;99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the
&lt;/p&gt;</description></item><item><title>GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05527</link><description>&lt;p&gt;
GEAR: &#19968;&#31181;&#29992;&#20110;&#20960;&#20046;&#26080;&#25439;&#29983;&#25104;&#25512;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05527
&lt;/p&gt;
&lt;p&gt;
GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;-&#20540;&#65288;KV&#65289;&#32531;&#23384;&#24050;&#25104;&#20026;&#21152;&#24555;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#26029;&#29983;&#25104;&#36895;&#24230;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#38271;&#30340;&#32531;&#23384;&#38656;&#27714;&#24050;&#23558;LLM&#25512;&#26029;&#36716;&#21464;&#20026;&#19968;&#20010;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#65292;&#26174;&#33879;&#22320;&#38480;&#21046;&#20102;&#31995;&#32479;&#21534;&#21520;&#37327;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#26631;&#35760;&#25110;&#22343;&#21248;&#37327;&#21270;&#25152;&#26377;&#26465;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#36739;&#39640;&#30340;&#36817;&#20284;&#35823;&#24046;&#26469;&#34920;&#31034;&#21387;&#32553;&#21518;&#30340;&#30697;&#38453;&#12290;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#27599;&#20010;&#27493;&#39588;&#30340;&#35823;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#37325;&#22823;&#20559;&#24046;&#21644;&#24615;&#33021;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GEAR&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;AWD-LSTM&#26550;&#26500;&#21644;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#23391;&#21152;&#25289;&#25991;&#23398;&#20013;&#20316;&#32773;&#24402;&#23646;&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#35813;&#39046;&#22495;&#22312;&#22797;&#26434;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#21644;&#20316;&#32773;&#35268;&#27169;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.05519</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;ULMFiT&#30340;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#23391;&#21152;&#25289;&#25991;&#23398;&#65288;AABL&#65289;&#30340;&#20316;&#32773;&#24402;&#23646;
&lt;/p&gt;
&lt;p&gt;
Authorship Attribution in Bangla Literature (AABL) via Transfer Learning using ULMFiT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05519
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;AWD-LSTM&#26550;&#26500;&#21644;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#23391;&#21152;&#25289;&#25991;&#23398;&#20013;&#20316;&#32773;&#24402;&#23646;&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#35813;&#39046;&#22495;&#22312;&#22797;&#26434;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#21644;&#20316;&#32773;&#35268;&#27169;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#26159;&#21019;&#24314;&#36866;&#24403;&#30340;&#25991;&#26412;&#29305;&#24449;&#20197;&#25429;&#25417;&#20316;&#32773;&#20889;&#20316;&#39118;&#26684;&#20174;&#32780;&#35782;&#21035;&#19968;&#27573;&#25991;&#26412;&#30340;&#21407;&#22987;&#20316;&#32773;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#20114;&#32852;&#32593;&#19978;&#30340;&#21311;&#21517;&#24615;&#22686;&#21152;&#65292;&#36825;&#19968;&#20219;&#21153;&#22312;&#21508;&#31181;&#23433;&#20840;&#21644;&#25220;&#34989;&#26816;&#27979;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#23613;&#31649;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27721;&#35821;&#31561;&#20854;&#20182;&#35821;&#35328;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#23391;&#21152;&#25289;&#35821;&#30340;&#22797;&#26434;&#35821;&#35328;&#29305;&#24449;&#21644;&#21477;&#23376;&#32467;&#26500;&#65292;&#35813;&#39046;&#22495;&#32570;&#20047;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#20316;&#32773;&#25968;&#37327;&#22686;&#21152;&#26102;&#19981;&#20855;&#21487;&#20280;&#32553;&#24615;&#65292;&#24182;&#19988;&#23545;&#27599;&#20301;&#20316;&#32773;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26435;&#20540;&#20002;&#24323;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;AWD-LSTM&#65289;&#26550;&#26500;&#21644;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#21644;&#20316;&#32773;&#35268;&#27169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05519v1 Announce Type: new  Abstract: Authorship Attribution is the task of creating an appropriate characterization of text that captures the authors' writing style to identify the original author of a given piece of text. With increased anonymity on the internet, this task has become increasingly crucial in various security and plagiarism detection fields. Despite significant advancements in other languages such as English, Spanish, and Chinese, Bangla lacks comprehensive research in this field due to its complex linguistic feature and sentence structure. Moreover, existing systems are not scalable when the number of author increases, and the performance drops for small number of samples per author. In this paper, we propose the use of Average-Stochastic Gradient Descent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and an effective transfer learning approach that addresses the problem of complex linguistic features extraction and scalability for authorship
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20559;&#24046;&#22686;&#24378;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;BCT&#65289;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#20559;&#35265;&#25512;&#29702;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#22312;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#20559;&#32622;&#29305;&#24449;&#30340;&#25552;&#31034;&#19979;&#36827;&#34892;&#19968;&#33268;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.05518</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#24046;&#22686;&#24378;&#19968;&#33268;&#24615;&#35757;&#32451;&#20943;&#23569;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#20559;&#35265;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05518
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20559;&#24046;&#22686;&#24378;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;BCT&#65289;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#20559;&#35265;&#25512;&#29702;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#22312;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#20559;&#32622;&#29305;&#24449;&#30340;&#25552;&#31034;&#19979;&#36827;&#34892;&#19968;&#33268;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CoT&#65289;&#26377;&#28508;&#21147;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#23427;&#21487;&#33021;&#20250;&#31995;&#32479;&#24615;&#22320;&#27498;&#26354;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#22240;&#32032;--&#27604;&#22914;&#65292;&#21512;&#29702;&#21270;&#31572;&#26696;&#20197;&#31526;&#21512;&#29992;&#25143;&#24847;&#35265;&#32780;&#19981;&#25552;&#21450;&#27492;&#20559;&#35265;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#20559;&#35265;&#25512;&#29702;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20559;&#24046;&#22686;&#24378;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;BCT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#26041;&#26696;&#65292;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#22312;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#20559;&#32622;&#29305;&#24449;&#30340;&#25552;&#31034;&#19979;&#36827;&#34892;&#19968;&#33268;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#27979;&#35797;&#21333;&#20803;&#65292;&#38024;&#23545;&#19971;&#20010;&#38382;&#31572;&#20219;&#21153;&#27979;&#35797;&#20102;&#20061;&#31181;&#24418;&#24335;&#30340;&#26377;&#20559;&#25512;&#29702;&#65292;&#21457;&#29616;&#23558;BCT&#24212;&#29992;&#20110;&#24102;&#26377;&#19968;&#31181;&#20559;&#35265;&#30340;GPT-3.5-Turbo&#21487;&#20197;&#23558;&#26377;&#20559;&#25512;&#29702;&#30340;&#27604;&#20363;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#38477;&#20302;86%&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#27169;&#22411;&#25512;&#24191;&#21040;&#20854;&#20182;&#24418;&#24335;&#30340;&#20559;&#35265;&#65292;&#24179;&#22343;&#23558;&#26410;&#30693;&#20559;&#35265;&#19978;&#30340;&#26377;&#20559;&#25512;&#29702;&#20943;&#23569;&#20102;37%&#12290;&#30001;&#20110;BCT&#23558;&#26410;&#30693;&#20559;&#35265;&#27867;&#21270;&#24182;&#19988;&#19981;&#38656;&#35201;&#37329;&#26631;&#31614;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#26377;&#21161;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05518v1 Announce Type: cross  Abstract: While chain-of-thought prompting (CoT) has the potential to improve the explainability of language model reasoning, it can systematically misrepresent the factors influencing models' behavior--for example, rationalizing answers in line with a user's opinion without mentioning this bias. To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT), an unsupervised fine-tuning scheme that trains models to give consistent reasoning across prompts with and without biasing features. We construct a suite testing nine forms of biased reasoning on seven question-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of biased reasoning by 86% on held-out tasks. Moreover, this model generalizes to other forms of bias, reducing biased reasoning on held-out biases by an average of 37%. As BCT generalizes to held-out biases and does not require gold labels, this method may h
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#24037;&#38169;&#35823;&#29983;&#25104;&#26469;&#25552;&#39640;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#65292;&#36827;&#32780;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20248;&#36234;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.05493</link><description>&lt;p&gt;
&#20154;&#31867;&#20250;&#29359;&#38169;&#65292;&#20294;&#32650;&#39548;&#20063;&#33021;&#23398;&#20250;
&lt;/p&gt;
&lt;p&gt;
To Err Is Human, but Llamas Can Learn It Too
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05493
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#38169;&#35823;&#29983;&#25104;&#26469;&#25552;&#39640;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#65292;&#36827;&#32780;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20248;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#36807;&#20154;&#24037;&#38169;&#35823;&#29983;&#25104;&#65288;AEG&#65289;&#26469;&#22686;&#24378;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#65288;GEC&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;Llama 2&#30340;LMs&#36827;&#34892;&#24494;&#35843;&#20197;&#29983;&#25104;&#38169;&#35823;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#30340;&#21512;&#25104;&#38169;&#35823;&#31867;&#20284;&#20110;&#20154;&#31867;&#38169;&#35823;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#20154;&#24037;&#38169;&#35823;&#35757;&#32451;GEC Llama&#27169;&#22411;&#65292;&#24182;&#22312;&#25152;&#26377;&#27979;&#35797;&#30340;&#35821;&#35328;&#65288;&#24503;&#35821;&#12289;&#20044;&#20811;&#20848;&#35821;&#21644;&#29233;&#27801;&#23612;&#20122;&#35821;&#65289;&#20013;&#21462;&#24471;&#20102;&#36229;&#36807;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#38169;&#35823;&#26657;&#27491;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#20854;&#25910;&#30410;&#22312;0.8&#33267;6 F0.5&#28857;&#20043;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#24494;&#35843;&#36739;&#23567;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#25552;&#31034;&#22823;&#22411;&#21830;&#29992;LMs&#65288;GPT-3.5&#21644;GPT-4&#65289;&#26469;&#29983;&#25104;&#38169;&#35823;&#65292;&#20063;&#20250;&#26377;&#30410;&#22320;&#24433;&#21709;&#38169;&#35823;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#25104;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05493v1 Announce Type: new  Abstract: This study explores enhancing grammatical error correction (GEC) through artificial error generation (AEG) using language models (LMs). Specifically, we fine-tune Llama 2-based LMs for error generation and find that this approach yields synthetic errors akin to human errors. Next, we train GEC Llama models with the help of these artificial errors and outperform previous state-of-the-art error correction models, with gains ranging between 0.8 and 6 F0.5 points across all tested languages (German, Ukrainian, and Estonian). Moreover, we demonstrate that generating errors by fine-tuning smaller sequence-to-sequence models and prompting large commercial LMs (GPT-3.5 and GPT-4) also results in synthetic errors beneficially affecting error generation models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20016;&#36125;&#35821;&#21040;&#27861;&#35821;&#35821;&#38899;&#32763;&#35793;&#35821;&#26009;&#24211;&#65288;FFSTC&#65289;&#39318;&#27425;&#25512;&#20986;&#65292;&#21253;&#25324;&#32422;31&#23567;&#26102;&#30340;&#35821;&#38899;&#20869;&#23481;&#65292;&#20351;&#29992;Fairseq&#30340;transformer_s&#21644;conformer&#27169;&#22411;&#35780;&#20272;&#20102;&#25968;&#25454;&#36136;&#37327;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.05488</link><description>&lt;p&gt;
FFSTC&#65306;&#20016;&#36125;&#35821;&#21040;&#27861;&#35821;&#35821;&#38899;&#32763;&#35793;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
FFSTC: Fongbe to French Speech Translation Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05488
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20016;&#36125;&#35821;&#21040;&#27861;&#35821;&#35821;&#38899;&#32763;&#35793;&#35821;&#26009;&#24211;&#65288;FFSTC&#65289;&#39318;&#27425;&#25512;&#20986;&#65292;&#21253;&#25324;&#32422;31&#23567;&#26102;&#30340;&#35821;&#38899;&#20869;&#23481;&#65292;&#20351;&#29992;Fairseq&#30340;transformer_s&#21644;conformer&#27169;&#22411;&#35780;&#20272;&#20102;&#25968;&#25454;&#36136;&#37327;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;&#20016;&#36125;&#35821;&#21040;&#27861;&#35821;&#35821;&#38899;&#32763;&#35793;&#35821;&#26009;&#24211;&#65288;FFSTC&#65289;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#32422;31&#23567;&#26102;&#30340;&#20016;&#36125;&#35821;&#20869;&#23481;&#65292;&#21253;&#25324;&#27861;&#35821;&#36716;&#24405;&#21644;&#30456;&#24212;&#30340;&#20016;&#36125;&#35821;&#38899;&#24405;&#38899;&#12290;FFSTC&#26159;&#36890;&#36807;&#21508;&#31181;&#25910;&#38598;&#26041;&#27861;&#21644;&#19987;&#27880;&#20010;&#20154;&#30340;&#21162;&#21147;&#32534;&#21046;&#32780;&#25104;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;Fairseq&#30340;transformer_s&#21644;conformer&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#25968;&#25454;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;transformer_s&#27169;&#22411;&#24471;&#20998;&#20026;8.96&#65292;conformer&#27169;&#22411;&#24471;&#20998;&#20026;8.14&#65292;&#20026;FFSTC&#35821;&#26009;&#24211;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05488v1 Announce Type: new  Abstract: In this paper, we introduce the Fongbe to French Speech Translation Corpus (FFSTC) for the first time. This corpus encompasses approximately 31 hours of collected Fongbe language content, featuring both French transcriptions and corresponding Fongbe voice recordings. FFSTC represents a comprehensive dataset compiled through various collection methods and the efforts of dedicated individuals. Furthermore, we conduct baseline experiments using Fairseq's transformer_s and conformer models to evaluate data quality and validity. Our results indicate a score of 8.96 for the transformer_s model and 8.14 for the conformer model, establishing a baseline for the FFSTC corpus.
&lt;/p&gt;</description></item><item><title>GPT-4&#36890;&#36807;&#33258;&#36523;&#30340;&#25512;&#29702;&#21644;&#35266;&#23519;&#33021;&#21147;&#65292;&#21487;&#20197;&#36816;&#34892;&#24182;&#29609;1993&#24180;&#30340;&#31532;&#19968;&#20154;&#31216;&#23556;&#20987;&#28216;&#25103;&#12298;&#27585;&#28781;&#25112;&#22763;&#12299;&#65292;&#24182;&#19988;&#33021;&#22815;&#25191;&#34892;&#38376;&#25805;&#20316;&#12289;&#20987;&#36133;&#25932;&#20154;&#21644;&#35268;&#21010;&#36335;&#24452;&#65292;&#36825;&#26377;&#26395;&#25299;&#23637;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20195;&#29702;&#22312;&#35270;&#39057;&#28216;&#25103;&#39046;&#22495;&#30340;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.05468</link><description>&lt;p&gt;
GPT-4&#20250;&#36816;&#34892;&#12298;&#27585;&#28781;&#25112;&#22763;&#12299;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Will GPT-4 Run DOOM?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05468
&lt;/p&gt;
&lt;p&gt;
GPT-4&#36890;&#36807;&#33258;&#36523;&#30340;&#25512;&#29702;&#21644;&#35266;&#23519;&#33021;&#21147;&#65292;&#21487;&#20197;&#36816;&#34892;&#24182;&#29609;1993&#24180;&#30340;&#31532;&#19968;&#20154;&#31216;&#23556;&#20987;&#28216;&#25103;&#12298;&#27585;&#28781;&#25112;&#22763;&#12299;&#65292;&#24182;&#19988;&#33021;&#22815;&#25191;&#34892;&#38376;&#25805;&#20316;&#12289;&#20987;&#36133;&#25932;&#20154;&#21644;&#35268;&#21010;&#36335;&#24452;&#65292;&#36825;&#26377;&#26395;&#25299;&#23637;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20195;&#29702;&#22312;&#35270;&#39057;&#28216;&#25103;&#39046;&#22495;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;GPT-4&#30340;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#25193;&#23637;&#21040;&#20102;1993&#24180;&#31532;&#19968;&#20154;&#31216;&#23556;&#20987;&#28216;&#25103;&#12298;&#27585;&#28781;&#25112;&#22763;&#12299;&#12290;&#36825;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20165;&#20973;&#23569;&#25968;&#25351;&#20196;&#21644;&#26469;&#33258;&#23631;&#24149;&#25130;&#22270;&#30340;&#25991;&#26412;&#25551;&#36848;&#65288;&#30001;&#27169;&#22411;&#26412;&#36523;&#29983;&#25104;&#65289;&#26469;&#36816;&#34892;&#21644;&#29609;&#28216;&#25103;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-4&#21487;&#20197;&#20197;&#21450;&#33021;&#22815;&#21442;&#19982;&#28216;&#25103;&#65306;&#23427;&#33021;&#22815;&#25805;&#20316;&#38376;&#12289;&#19982;&#25932;&#20154;&#20316;&#25112;&#24182;&#25191;&#34892;&#36335;&#24452;&#35268;&#21010;&#12290;&#28041;&#21450;&#22810;&#27425;&#27169;&#22411;&#35843;&#29992;&#30340;&#26356;&#22797;&#26434;&#25552;&#31034;&#31574;&#30053;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#38656;&#35201;&#36827;&#19968;&#27493;&#24037;&#20316;&#26469;&#35753;&#36825;&#20010;LLM&#29609;&#24471;&#20687;&#20854;&#32463;&#20856;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23545;&#24212;&#29289;&#19968;&#26679;&#20986;&#33394;&#65292;&#20294;&#25105;&#20204;&#27880;&#24847;&#21040;GPT-4&#19981;&#38656;&#35201;&#35757;&#32451;&#65292;&#32780;&#26159;&#20381;&#38752;&#33258;&#36523;&#30340;&#25512;&#29702;&#21644;&#35266;&#23519;&#33021;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#25512;&#21160;&#20102;&#22522;&#20110;&#26234;&#33021;LLM&#20195;&#29702;&#22312;&#35270;&#39057;&#28216;&#25103;&#20013;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#26368;&#32456;&#35752;&#35770;&#20102;&#25105;&#20204;&#24037;&#20316;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05468v1 Announce Type: cross  Abstract: We show that GPT-4's reasoning and planning capabilities extend to the 1993 first-person shooter Doom. This large language model (LLM) is able to run and play the game with only a few instructions, plus a textual description--generated by the model itself from screenshots--about the state of the game being observed. We find that GPT-4 can play the game to a passable degree: it is able to manipulate doors, combat enemies, and perform pathing. More complex prompting strategies involving multiple model calls provide better results. While further work is required to enable the LLM to play the game as well as its classical, reinforcement learning-based counterparts, we note that GPT-4 required no training, leaning instead on its own reasoning and observational capabilities. We hope our work pushes the boundaries on intelligent, LLM-based agents in video games. We conclude by discussing the ethical implications of our work.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#20195;&#30721;&#28151;&#21512;&#31561;&#25163;&#27573;&#65292;&#38477;&#20302;&#22312;&#24403;&#20195;LLMs&#20013;&#22788;&#29702;&#20302;&#36164;&#28304;&#35821;&#35328;&#20219;&#21153;&#30340;&#25104;&#26412;&#65292;&#20197;&#30830;&#20445;&#39044;&#27979;&#21644;&#29983;&#25104;&#36136;&#37327;&#19981;&#21463;&#25439;&#12290;</title><link>https://arxiv.org/abs/2403.05434</link><description>&lt;p&gt;
&#20351;&#29992;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#22788;&#29702;&#20302;&#36164;&#28304;&#35821;&#35328;&#20219;&#21153;&#30340;&#25104;&#26412;&#19982;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#20195;&#30721;&#28151;&#21512;&#31561;&#25163;&#27573;&#65292;&#38477;&#20302;&#22312;&#24403;&#20195;LLMs&#20013;&#22788;&#29702;&#20302;&#36164;&#28304;&#35821;&#35328;&#20219;&#21153;&#30340;&#25104;&#26412;&#65292;&#20197;&#30830;&#20445;&#39044;&#27979;&#21644;&#29983;&#25104;&#36136;&#37327;&#19981;&#21463;&#25439;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;/&#23569;&#36718;&#25512;&#29702;&#21644;&#29983;&#25104;&#36136;&#37327;&#12290;&#20854;&#20013;&#26377;&#19968;&#20123;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;(LRLs)&#19978;&#35757;&#32451;&#24182;&#34920;&#29616;&#20986;&#19981;&#38169;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#35757;&#32451;LLMs&#30340;&#25104;&#26412;&#26497;&#39640;&#65292;&#23427;&#20204;&#36890;&#24120;&#34987;&#29992;&#20316;&#32593;&#32476;&#26381;&#21153;&#65292;&#23458;&#25143;&#26681;&#25454;&#36755;&#20837;&#21644;&#36755;&#20986;&#20196;&#29260;&#30340;&#25968;&#37327;&#20184;&#36153;&#12290;&#20196;&#29260;&#25968;&#37327;&#24378;&#28872;&#20381;&#36182;&#20110;&#33050;&#26412;&#21644;&#35821;&#35328;&#65292;&#20197;&#21450;LLM&#30340;&#23376;&#35789;&#27719;&#34920;&#12290;&#25105;&#20204;&#34920;&#26126;LRLs&#22312;&#23450;&#20215;&#19978;&#22788;&#20110;&#19981;&#21033;&#20301;&#32622;&#65292;&#22240;&#20026;&#20247;&#25152;&#21608;&#30693;&#65292;&#23545;&#20110;LRLs&#65292;&#30693;&#21517;LLMs&#20135;&#29983;&#30340;&#20196;&#29260;&#27604;HRLs&#22810;&#12290;&#36825;&#26159;&#22240;&#20026;&#30446;&#21069;&#22823;&#22810;&#25968;&#27969;&#34892;&#30340;LLMs&#37117;&#38024;&#23545;HRL&#35789;&#27719;&#34920;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#20445;&#35777;&#39044;&#27979;&#21644;&#29983;&#25104;&#36136;&#37327;&#19981;&#21463;&#25439;&#30340;&#21516;&#26102;&#65292;&#35843;&#25972;&#24179;&#34913;&#65306;&#38477;&#20302;&#22312;&#24403;&#20195;LLMs&#20013;&#22788;&#29702;LRLs&#30340;&#25104;&#26412;&#12290;&#20316;&#20026;&#20943;&#23569;LLM&#22788;&#29702;&#30340;&#20196;&#29260;&#25968;&#37327;&#30340;&#25163;&#27573;&#65292;&#25105;&#20204;&#32771;&#34385;&#20195;&#30721;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05434v1 Announce Type: new  Abstract: Large Language Models (LLMs) exhibit impressive zero/few-shot inference and generation quality for high-resource languages(HRLs). A few of them have been trained in low-resource languages (LRLs) and give decent performance. Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens. The number of tokens strongly depends on the script and language, as well as the LLM's sub-word vocabulary. We show that LRLs are at a pricing disadvantage, because the well-known LLMs produce more tokens for LRLs than HRLs. This is because most currently popular LLMs are optimized for HRL vocabularies. Our objective is to level the playing field: reduce the cost of processing LRLs in contemporary LLMs while ensuring that predictive and generative qualities are not compromised. As means to reduce the number of tokens processed by the LLM, we consider code-mixing,
&lt;/p&gt;</description></item><item><title>HistGen&#26159;&#19968;&#20010;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#26469;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.05396</link><description>&lt;p&gt;
HistGen&#65306;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05396
&lt;/p&gt;
&lt;p&gt;
HistGen&#26159;&#19968;&#20010;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#26469;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#22312;&#30284;&#30151;&#35786;&#26029;&#20013;&#25198;&#28436;&#30528;&#40644;&#37329;&#26631;&#20934;&#30340;&#35282;&#33394;&#65292;&#20020;&#24202;&#25253;&#21578;&#22312;&#35299;&#37322;&#21644;&#29702;&#35299;&#36825;&#19968;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#25351;&#23548;&#30284;&#30151;&#27835;&#30103;&#21644;&#24739;&#32773;&#25252;&#29702;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28145;&#24230;&#23398;&#20064;&#23545;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#23558;&#26497;&#22823;&#25552;&#21319;&#20020;&#24202;&#25928;&#29575;&#65292;&#24182;&#20943;&#36731;&#30149;&#29702;&#23398;&#23478;&#22312;&#25253;&#21578;&#25776;&#20889;&#26041;&#38754;&#30340;&#21171;&#21160;&#24378;&#24230;&#21644;&#32791;&#26102;&#36127;&#25285;&#12290;&#20026;&#36861;&#27714;&#36825;&#19968;&#36827;&#27493;&#65292;&#20316;&#32773;&#24341;&#20837;&#20102;HistGen&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#23454;&#20363;&#23398;&#20064;&#22686;&#24378;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;HistGen&#21463;&#35786;&#26029;&#21644;&#25253;&#21578;&#25776;&#20889;&#24037;&#20316;&#27969;&#31243;&#30340;&#21551;&#21457;&#65292;&#20855;&#26377;&#20004;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#22359;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#40784;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#21644;&#35786;&#26029;&#25253;&#21578;&#65292;&#20174;&#26412;&#22320;&#21644;&#20840;&#23616;&#31890;&#24230;&#25552;&#21319;&#25253;&#21578;&#29983;&#25104;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26412;&#22320;-&#20840;&#23616;&#20998;&#23618;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#21306;&#22495;&#20013;&#32858;&#21512;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05396v1 Announce Type: cross  Abstract: Histopathology serves as the gold standard in cancer diagnosis, with clinical reports being vital in interpreting and understanding this process, guiding cancer treatment and patient care. The automation of histopathology report generation with deep learning stands to significantly enhance clinical efficiency and lessen the labor-intensive, time-consuming burden on pathologists in report writing. In pursuit of this advancement, we introduce HistGen, a multiple instance learning-empowered framework for histopathology report generation together with the first benchmark dataset for evaluation. Inspired by diagnostic and report-writing workflows, HistGen features two delicately designed modules, aiming to boost report generation by aligning whole slide images (WSIs) and diagnostic reports from local and global granularity. To achieve this, a local-global hierarchical encoder is developed for efficient visual feature aggregation from a regi
&lt;/p&gt;</description></item><item><title>&#37327;&#21270;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;Transformer-based&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24179;&#22343;&#25552;&#21319;&#20102;18.68%&#12290;</title><link>https://arxiv.org/abs/2403.05365</link><description>&lt;p&gt;
&#37327;&#21270;&#23545;&#22522;&#20110;Transformer&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Quantization on the Robustness of Transformer-based Text Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05365
&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;Transformer-based&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24179;&#22343;&#25552;&#21319;&#20102;18.68%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer-based&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#24448;&#24448;&#34920;&#29616;&#20986;&#33030;&#24369;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#37327;&#21270;&#23545;Transformer-based&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#37327;&#21270;&#36890;&#24120;&#28041;&#21450;&#23558;&#39640;&#31934;&#24230;&#23454;&#25968;&#26144;&#23556;&#21040;&#36739;&#20302;&#31934;&#24230;&#30340;&#20540;&#65292;&#26088;&#22312;&#20943;&#23569;&#25152;&#28041;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#23558;&#37327;&#21270;&#24212;&#29992;&#20110;NLP&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#23545;BERT&#21644;DistilBERT&#27169;&#22411;&#24212;&#29992;&#37327;&#21270;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20102;SST-2&#12289;Emotion&#21644;MR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#38754;&#23545;TextFooler&#12289;PWWS&#21644;PSO&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#37327;&#21270;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23545;&#25239;&#20934;&#30830;&#24615;&#65288;&#24179;&#22343;&#25552;&#21319;&#20102;18.68%&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05365v1 Announce Type: new  Abstract: Transformer-based models have made remarkable advancements in various NLP areas. Nevertheless, these models often exhibit vulnerabilities when confronted with adversarial attacks. In this paper, we explore the effect of quantization on the robustness of Transformer-based models. Quantization usually involves mapping a high-precision real number to a lower-precision value, aiming at reducing the size of the model at hand. To the best of our knowledge, this work is the first application of quantization on the robustness of NLP models. In our experiments, we evaluate the impact of quantization on BERT and DistilBERT models in text classification using SST-2, Emotion, and MR datasets. We also evaluate the performance of these models against TextFooler, PWWS, and PSO adversarial attacks. Our findings show that quantization significantly improves (by an average of 18.68%) the adversarial accuracy of the models. Furthermore, we compare the effe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#20174;&#39044;&#22791;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#24402;&#22240;&#20998;&#25968;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#25552;&#31034;&#33539;&#24335;&#20135;&#29983;&#30340;&#35299;&#37322;&#27604;&#24494;&#35843;&#27169;&#22411;&#26356;&#21512;&#29702;&#65292;Shapley&#20540;&#37319;&#26679;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.05338</link><description>&lt;p&gt;
&#29992;&#24402;&#22240;&#20998;&#25968;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65306;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Explaining Pre-Trained Language Models with Attribution Scores: An Analysis in Low-Resource Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#20174;&#39044;&#22791;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#24402;&#22240;&#20998;&#25968;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#25552;&#31034;&#33539;&#24335;&#20135;&#29983;&#30340;&#35299;&#37322;&#27604;&#24494;&#35843;&#27169;&#22411;&#26356;&#21512;&#29702;&#65292;Shapley&#20540;&#37319;&#26679;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#22240;&#20998;&#25968;&#25351;&#31034;&#19981;&#21516;&#36755;&#20837;&#37096;&#20998;&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#27492;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#34892;&#20026;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#23427;&#20204;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26356;&#23481;&#26131;&#36866;&#24212;&#12290;&#28982;&#32780;&#65292;&#20174;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#24402;&#22240;&#20998;&#25968;&#30340;&#36136;&#37327;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#20174;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#24402;&#22240;&#20998;&#25968;&#20851;&#20110;&#21487;&#20449;&#24230;&#21644;&#24544;&#23454;&#24230;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#20174;&#24494;&#35843;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#24402;&#22240;&#20998;&#25968;&#36827;&#34892;&#27604;&#36739;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#35268;&#27169;&#20316;&#20026;&#20998;&#26512;&#30340;&#21478;&#19968;&#20010;&#32500;&#24230;&#24341;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#20351;&#29992;&#25552;&#31034;&#33539;&#24335;&#65288;&#26080;&#35770;&#26159;&#22522;&#20110;&#32534;&#30721;&#22120;&#36824;&#26159;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65289;&#27604;&#24494;&#35843;&#27169;&#22411;&#20135;&#29983;&#26356;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;Shapley&#20540;&#37319;&#26679;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#19968;&#33268;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#27880;&#24847;&#21147;&#21644;&#38598;&#25104;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05338v1 Announce Type: new  Abstract: Attribution scores indicate the importance of different input parts and can, thus, explain model behaviour. Currently, prompt-based models are gaining popularity, i.a., due to their easier adaptability in low-resource settings. However, the quality of attribution scores extracted from prompt-based models has not been investigated yet. In this work, we address this topic by analyzing attribution scores extracted from prompt-based models w.r.t. plausibility and faithfulness and comparing them with attribution scores extracted from fine-tuned models and large language models. In contrast to previous work, we introduce training size as another dimension into the analysis. We find that using the prompting paradigm (with either encoder-based or decoder-based models) yields more plausible explanations than fine-tuning the models in low-resource settings and Shapley Value Sampling consistently outperforms attention and Integrated Gradients in te
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#21451;&#22909;&#30340;&#36830;&#32493;&#27169;&#22411;&#32534;&#36753;&#19982;&#25209;&#37327;&#25903;&#25345;&#30340;&#26041;&#27861;COMEBA-HK&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05330</link><description>&lt;p&gt;
&#36830;&#32493;&#27169;&#22411;&#32534;&#36753;&#19982;&#25209;&#37327;&#25903;&#25345;&#30340;HooK&#23618;
&lt;/p&gt;
&lt;p&gt;
Consecutive Model Editing with Batch alongside HooK Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05330
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#21451;&#22909;&#30340;&#36830;&#32493;&#27169;&#22411;&#32534;&#36753;&#19982;&#25209;&#37327;&#25903;&#25345;&#30340;&#26041;&#27861;COMEBA-HK&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20856;&#22411;&#30340;&#37325;&#26032;&#35757;&#32451;&#33539;&#24335;&#32791;&#26102;&#19988;&#28040;&#32791;&#36164;&#28304;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#36716;&#21521;&#27169;&#22411;&#32534;&#36753;&#65292;&#20197;&#23547;&#25214;&#19968;&#31181;&#26377;&#25928;&#30340;&#12289;&#36830;&#32493;&#30340;&#12289;&#24182;&#25903;&#25345;&#25209;&#37327;&#26041;&#24335;&#30452;&#25509;&#32534;&#36753;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23384;&#22312;&#25152;&#26377;&#36825;&#20123;&#23454;&#29992;&#26399;&#26395;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#21364;&#26410;&#33021;&#23454;&#29616;&#25152;&#26377;&#36825;&#20123;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#31181;&#25903;&#25345;&#36830;&#32493;&#24615;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#30340;&#20869;&#23384;&#38656;&#27714;&#24448;&#24448;&#26159;&#31105;&#27490;&#24615;&#30340;&#65292;&#32463;&#24120;&#38656;&#35201;&#38543;&#30528;&#26102;&#38388;&#30340;&#22686;&#38271;&#36880;&#27493;&#22686;&#21152;&#22806;&#37096;&#20869;&#23384;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMEBA-HK&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26082;&#26159;&#36830;&#32493;&#30340;&#21448;&#25903;&#25345;&#25209;&#37327;&#12290;COMEBA-HK&#23545;&#20110;&#23384;&#20648;&#20960;&#20010;&#20855;&#26377;&#26356;&#26032;&#26435;&#37325;&#30340;hook&#23618;&#20165;&#38656;&#23569;&#37327;&#20869;&#23384;&#65292;&#26159;&#20869;&#23384;&#21451;&#22909;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#36718;&#21644;&#36830;&#32493;&#25209;&#37327;&#32534;&#36753;&#22330;&#26223;&#19979;&#20248;&#20110;&#20854;&#20182;&#25903;&#25345;&#25209;&#37327;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05330v1 Announce Type: new  Abstract: As the typical retraining paradigm is unacceptably time- and resource-consuming, researchers are turning to model editing in order to seek an effective, consecutive, and batch-supportive way to edit the model behavior directly. Despite all these practical expectations, existing model editing methods fail to realize all of them. Furthermore, the memory demands for such succession-supportive model editing approaches tend to be prohibitive, frequently necessitating an external memory that grows incrementally over time. To cope with these challenges, we propose COMEBA-HK, a model editing method that is both consecutive and batch-supportive. COMEBA-HK is memory-friendly as it only needs a small amount of it to store several hook layers with updated weights. Experimental results demonstrate the superiority of our method over other batch-supportive model editing methods under both single-round and consecutive batch editing scenarios. Extensive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05326</link><description>&lt;p&gt;
ChatASU&#65306;&#21796;&#36215;LLM&#30340;&#21453;&#24605;&#65292;&#30495;&#27491;&#29702;&#35299;&#23545;&#35805;&#20013;&#30340;&#26041;&#38754;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#21160;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#38382;&#31572;&#21644;&#23545;&#35805;&#65289;&#20013;&#36827;&#34892;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ASU&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#24573;&#30053;&#20102;&#24847;&#35265;&#30446;&#26631;&#65288;&#21363;&#26041;&#38754;&#65289;&#30340;&#20849;&#25351;&#38382;&#39064;&#65292;&#32780;&#36825;&#31181;&#29616;&#35937;&#22312;&#20114;&#21160;&#22330;&#26223;&#29305;&#21035;&#26159;&#23545;&#35805;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#38480;&#21046;&#20102;ASU&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#23558;&#21508;&#31181;NLP&#20219;&#21153;&#19982;&#32842;&#22825;&#33539;&#24335;&#30456;&#32467;&#21512;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#39033;ChatASU&#20219;&#21153;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#21363;&#26041;&#38754;&#38142;&#25512;&#29702;&#65288;ACR&#65289;&#20219;&#21153;&#65292;&#20197;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20449;&#30340;&#33258;&#21453;&#24605;&#26041;&#27861;&#65288;TSA&#65289;&#19982;ChatGLM&#20316;&#20026;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05326v1 Announce Type: cross  Abstract: Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as back
&lt;/p&gt;</description></item><item><title>RAT&#26041;&#27861;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#65292;&#22312;&#38271;&#35270;&#35282;&#29983;&#25104;&#20013;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24187;&#35273;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.05313</link><description>&lt;p&gt;
RAT&#65306;&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#22312;&#38271;&#35270;&#35282;&#29983;&#25104;&#20013;&#24341;&#21457;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05313
&lt;/p&gt;
&lt;p&gt;
RAT&#26041;&#27861;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#65292;&#22312;&#38271;&#35270;&#35282;&#29983;&#25104;&#20013;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24187;&#35273;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#36845;&#20195;&#20462;&#35746;&#19968;&#31995;&#21015;&#24605;&#32500;&#65292;&#26174;&#33879;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#35270;&#35282;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21516;&#26102;&#26497;&#22823;&#20943;&#36731;&#20102;&#24187;&#35273;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#8212;&#8212;*&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;* (RAT)&#8212;&#8212;&#22312;&#29983;&#25104;&#21021;&#22987;&#30340;&#38646;&#23556; CoT &#21518;&#65292;&#36880;&#27493;&#20462;&#35746;&#27599;&#20010;&#24605;&#32500;&#27493;&#39588;&#65292;&#19982;&#20219;&#21153;&#26597;&#35810;&#12289;&#24403;&#21069;&#21644;&#36807;&#21435;&#30340;&#24605;&#32500;&#27493;&#39588;&#30456;&#20851;&#30340;&#26816;&#32034;&#20449;&#24687;&#12290;&#23558; RAT &#24212;&#29992;&#20110; GPT-3.5&#12289;GPT-4 &#21644; CodeLLaMA-7b&#65292;&#22312;&#21508;&#31181;&#38271;&#35270;&#35282;&#29983;&#25104;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#65307;&#24179;&#22343;&#32780;&#35328;&#65292;&#20195;&#30721;&#29983;&#25104;&#35780;&#20998;&#22686;&#21152;&#20102; 13.63%&#65292;&#25968;&#23398;&#25512;&#29702;&#22686;&#21152;&#20102; 16.96%&#65292;&#21019;&#24847;&#20889;&#20316;&#22686;&#21152;&#20102; 19.2%&#65292;&#20855;&#35937;&#20219;&#21153;&#35268;&#21010;&#22686;&#21152;&#20102; 42.78%&#12290;&#28436;&#31034;&#39029;&#38754;&#38142;&#25509;&#65306;https://craftjarvis.github.io/RAT
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05313v1 Announce Type: cross  Abstract: We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT
&lt;/p&gt;</description></item><item><title>ACLSum&#26159;&#19968;&#20010;&#30001;&#39046;&#22495;&#19987;&#23478;&#31934;&#24515;&#21046;&#20316;&#21644;&#35780;&#20272;&#30340;&#26032;&#22411;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#23545;&#31185;&#23398;&#35770;&#25991;&#36827;&#34892;&#22810;&#26041;&#38754;&#25688;&#35201;&#65292;&#28145;&#20837;&#28085;&#30422;&#25361;&#25112;&#12289;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.05303</link><description>&lt;p&gt;
ACLSum&#65306;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#31185;&#23398;&#20986;&#29256;&#29289;&#22522;&#20110;&#26041;&#38754;&#30340;&#25688;&#35201;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ACLSum: A New Dataset for Aspect-based Summarization of Scientific Publications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05303
&lt;/p&gt;
&lt;p&gt;
ACLSum&#26159;&#19968;&#20010;&#30001;&#39046;&#22495;&#19987;&#23478;&#31934;&#24515;&#21046;&#20316;&#21644;&#35780;&#20272;&#30340;&#26032;&#22411;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#23545;&#31185;&#23398;&#35770;&#25991;&#36827;&#34892;&#22810;&#26041;&#38754;&#25688;&#35201;&#65292;&#28145;&#20837;&#28085;&#30422;&#25361;&#25112;&#12289;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#65292;&#20154;&#20204;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#24320;&#21457;&#25688;&#35201;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#36164;&#28304;&#26159;(&#21322;)&#33258;&#21160;&#29983;&#25104;&#30340;&#65292;&#36890;&#24120;&#26159;&#36890;&#36807;&#32593;&#32476;&#25968;&#25454;&#29228;&#21462;&#24471;&#21040;&#30340;&#65292;&#23548;&#33268;&#20102;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#25688;&#35201;&#31995;&#32479;&#30340;&#36164;&#28304;&#36136;&#37327;&#19981;&#20339;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#29983;&#25104;&#22522;&#20934;&#25688;&#35201;&#30340;&#39640;&#26114;&#25104;&#26412;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#22810;&#31181;&#35821;&#35328;&#21644;&#19987;&#19994;&#39046;&#22495;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACLSum&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#39046;&#22495;&#19987;&#23478;&#31934;&#24515;&#21046;&#20316;&#21644;&#35780;&#20272;&#30340;&#26032;&#22411;&#25688;&#35201;&#25968;&#25454;&#38598;&#12290;&#19982;&#20197;&#24448;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;ACLSum&#25903;&#25345;&#23545;&#31185;&#23398;&#35770;&#25991;&#36827;&#34892;&#22810;&#26041;&#38754;&#25688;&#35201;&#65292;&#28145;&#20837;&#28085;&#30422;&#25361;&#25112;&#12289;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#36164;&#28304;&#36136;&#37327;&#20197;&#21450;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05303v1 Announce Type: new  Abstract: Extensive efforts in the past have been directed toward the development of summarization datasets. However, a predominant number of these resources have been (semi)-automatically generated, typically through web data crawling, resulting in subpar resources for training and evaluating summarization systems, a quality compromise that is arguably due to the substantial costs associated with generating ground-truth summaries, particularly for diverse languages and specialized domains. To address this issue, we present ACLSum, a novel summarization dataset carefully crafted and evaluated by domain experts. In contrast to previous datasets, ACLSum facilitates multi-aspect summarization of scientific papers, covering challenges, approaches, and outcomes in depth. Through extensive experiments, we evaluate the quality of our resource and the performance of models based on pretrained language models and state-of-the-art large language models (LLM
&lt;/p&gt;</description></item><item><title>PEEB&#26159;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23558;&#31867;&#21035;&#21517;&#31216;&#36716;&#25442;&#20026;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#25991;&#26412;&#25551;&#36848;&#31526;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36824;&#39318;&#27425;&#23454;&#29616;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#24418;&#25104;&#26032;&#20998;&#31867;&#22120;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.05297</link><description>&lt;p&gt;
PEEB&#65306;&#20855;&#26377;&#21487;&#35299;&#37322;&#21644;&#21487;&#32534;&#36753;&#35821;&#35328;&#29942;&#39048;&#30340;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05297
&lt;/p&gt;
&lt;p&gt;
PEEB&#26159;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23558;&#31867;&#21035;&#21517;&#31216;&#36716;&#25442;&#20026;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#25991;&#26412;&#25551;&#36848;&#31526;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36824;&#39318;&#27425;&#23454;&#29616;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#24418;&#25104;&#26032;&#20998;&#31867;&#22120;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;CLIP&#30340;&#20998;&#31867;&#22120;&#20381;&#36182;&#20110;&#21253;&#21547;{text encoder&#24050;&#30693;&#30340;&#31867;&#21517;&#31216;}&#30340;&#25552;&#31034;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;CLIP&#22312;&#26032;&#31867;&#21035;&#25110;&#20854;&#21517;&#31216;&#24456;&#23569;&#22312;&#20114;&#32852;&#32593;&#19978;&#20986;&#29616;&#30340;&#31867;&#21035;&#65288;&#20363;&#22914;&#40479;&#31867;&#30340;&#23398;&#21517;&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#38024;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEEB - &#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#21487;&#32534;&#36753;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#65288;1&#65289;&#23558;&#31867;&#21035;&#21517;&#31216;&#34920;&#36798;&#20026;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65307;&#21644;&#65288;2&#65289;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#36827;&#34892;&#21305;&#37197;&#65292;&#20197;&#35745;&#31639;&#29992;&#20110;&#20998;&#31867;&#30340;&#36923;&#36753;&#20998;&#25968;&#12290;&#22312;&#19968;&#20010;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#31867;&#21035;&#21517;&#31216;&#26159;&#26410;&#30693;&#30340;&#65292;PEEB&#22312;&#20934;&#30830;&#24615;&#19978;&#22823;&#24133;&#20248;&#20110;CLIP&#65288;&#32422;&#20026;10&#20493;&#65289;&#12290;&#19982;&#22522;&#20110;&#37096;&#20998;&#30340;&#20998;&#31867;&#22120;&#30456;&#27604;&#65292;PEEB&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#19978;&#26159;&#26368;&#20808;&#36827;&#30340;&#65288;88.80%&#20934;&#30830;&#29575;&#65289;&#65292;&#32780;&#19988;&#36824;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#35753;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#20197;&#24418;&#25104;&#26032;&#30340;&#20998;&#31867;&#22120;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05297v1 Announce Type: cross  Abstract: CLIP-based classifiers rely on the prompt containing a {class name} that is known to the text encoder. That is, CLIP performs poorly on new classes or the classes whose names rarely appear on the Internet (e.g., scientific names of birds). For fine-grained classification, we propose PEEB - an explainable and editable classifier to (1) express the class name into a set of pre-defined text descriptors that describe the visual parts of that class; and (2) match the embeddings of the detected parts to their textual descriptors in each class to compute a logit score for classification. In a zero-shot setting where the class names are unknown, PEEB outperforms CLIP by a large margin (~10x in accuracy). Compared to part-based classifiers, PEEB is not only the state-of-the-art on the supervised-learning setting (88.80% accuracy) but also the first to enable users to edit the class definitions to form a new classifier without retraining. Compar
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#39318;&#25209;&#24320;&#25918;&#35775;&#38382;&#30340;&#21453;&#32534;&#35793;LLM&#65292;&#39044;&#35757;&#32451;&#22312;40&#20159;&#20010;C&#28304;&#20195;&#30721;&#21644;&#27719;&#32534;&#20195;&#30721;&#26631;&#35760;&#19978;&#65292;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#32771;&#34385;&#37325;&#26032;&#32534;&#35793;&#24615;&#21644;&#37325;&#26032;&#25191;&#34892;&#24615;&#30340;&#21453;&#32534;&#35793;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.05286</link><description>&lt;p&gt;
LLM4Decompile&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20108;&#36827;&#21046;&#20195;&#30721;&#36827;&#34892;&#21453;&#32534;&#35793;
&lt;/p&gt;
&lt;p&gt;
LLM4Decompile: Decompiling Binary Code with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05286
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#39318;&#25209;&#24320;&#25918;&#35775;&#38382;&#30340;&#21453;&#32534;&#35793;LLM&#65292;&#39044;&#35757;&#32451;&#22312;40&#20159;&#20010;C&#28304;&#20195;&#30721;&#21644;&#27719;&#32534;&#20195;&#30721;&#26631;&#35760;&#19978;&#65292;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#32771;&#34385;&#37325;&#26032;&#32534;&#35793;&#24615;&#21644;&#37325;&#26032;&#25191;&#34892;&#24615;&#30340;&#21453;&#32534;&#35793;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#32534;&#35793;&#26088;&#22312;&#23558;&#32534;&#35793;&#20195;&#30721;&#24674;&#22797;&#20026;&#21487;&#35835;&#24615;&#24378;&#30340;&#28304;&#20195;&#30721;&#65292;&#20294;&#22312;&#21517;&#31216;&#21644;&#32467;&#26500;&#31561;&#32454;&#33410;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32534;&#31243;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#28608;&#21457;&#20102;&#23427;&#20204;&#22312;&#21453;&#32534;&#35793;&#20013;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26080;&#29992;&#20110;&#21453;&#32534;&#35793;&#30340;&#24320;&#28304;LLM&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#21453;&#32534;&#35793;&#35780;&#20272;&#31995;&#32479;&#20027;&#35201;&#32771;&#34385;&#26631;&#35760;&#32423;&#20934;&#30830;&#24615;&#65292;&#32780;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#20195;&#30721;&#30340;&#21487;&#25191;&#34892;&#24615;&#65292;&#36825;&#26159;&#20219;&#20309;&#31243;&#24207;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#39318;&#25209;&#24320;&#25918;&#35775;&#38382;&#30340;&#21453;&#32534;&#35793;LLM&#65292;&#33539;&#22260;&#20174;10&#20159;&#21040;330&#20159;&#65292;&#39044;&#20808;&#35757;&#32451;&#20102;40&#20159;&#20010;&#20196;&#29260;&#30340;C&#28304;&#20195;&#30721;&#21644;&#30456;&#24212;&#30340;&#27719;&#32534;&#20195;&#30721;&#12290;&#36825;&#20123;&#24320;&#28304;LLM&#21487;&#20197;&#20316;&#20026;&#35813;&#39046;&#22495;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#22522;&#32447;&#12290;&#20026;&#20102;&#30830;&#20445;&#23454;&#38469;&#31243;&#24207;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Decompile-Eval&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#32771;&#34385;&#37325;&#26032;&#32534;&#35793;&#24615;&#21644;&#37325;&#26032;&#25191;&#34892;&#24615;&#30340;&#21453;&#32534;&#35793;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#24378;&#35843;&#20102;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05286v1 Announce Type: cross  Abstract: Decompilation aims to restore compiled code to human-readable source code, but struggles with details like names and structure. Large language models (LLMs) show promise for programming tasks, motivating their application to decompilation. However, there does not exist any open-source LLM for decompilation. Moreover, existing decompilation evaluation systems mainly consider token-level accuracy and largely ignore code executability, which is the most important feature of any program. Therefore, we release the first open-access decompilation LLMs ranging from 1B to 33B pre-trained on 4 billion tokens of C source code and the corresponding assembly code. The open-source LLMs can serve as baselines for further development in the field. To ensure practical program evaluation, we introduce Decompile-Eval, the first dataset that considers re-compilability and re-executability for decompilation. The benchmark emphasizes the importance of eval
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Deep Prompt Multi-task Network (DPMN)&#29992;&#20110;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#21644;&#36731;&#25552;&#31034;&#35843;&#25972;&#26469;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#25552;&#39640;&#26816;&#27979;&#24230;&#37327;&#26631;&#20934;</title><link>https://arxiv.org/abs/2403.05268</link><description>&lt;p&gt;
&#28145;&#24230;&#25552;&#31034;&#22810;&#20219;&#21153;&#32593;&#32476;&#29992;&#20110;&#36785;&#39554;&#35821;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Prompt Multi-task Network for Abuse Language Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05268
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Deep Prompt Multi-task Network (DPMN)&#29992;&#20110;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#21644;&#36731;&#25552;&#31034;&#35843;&#25972;&#26469;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#25552;&#39640;&#26816;&#27979;&#24230;&#37327;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28389;&#29992;&#35821;&#35328;&#30340;&#26816;&#27979;&#20173;&#28982;&#26159;&#31038;&#20132;&#32593;&#32476;&#24191;&#27867;&#20351;&#29992;&#20013;&#23384;&#22312;&#30340;&#19968;&#39033;&#38271;&#26399;&#25361;&#25112;&#12290;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#20219;&#21153;&#23384;&#22312;&#30528;&#20934;&#30830;&#24615;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24494;&#35843;&#25216;&#26415;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#26410;&#33021;&#28608;&#21457;PLMs&#30340;&#19968;&#33324;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#30340;&#28145;&#24230;&#25552;&#31034;&#22810;&#20219;&#21153;&#32593;&#32476;&#65288;DPMN&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DPMN&#39318;&#20808;&#23581;&#35797;&#20026;PLMs&#35774;&#35745;&#20004;&#31181;&#24418;&#24335;&#30340;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#21644;&#36731;&#25552;&#31034;&#35843;&#25972;&#12290;&#30740;&#31350;&#20102;&#19981;&#21516;&#25552;&#31034;&#38271;&#24230;&#12289;&#35843;&#25972;&#31574;&#30053;&#21644;&#25552;&#31034;&#21021;&#22987;&#21270;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#28389;&#29992;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Bi-LSTM&#21644;FFN&#30340;&#20219;&#21153;&#22836;&#65292;&#21487;&#29992;&#20316;&#30701;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#26368;&#32456;&#65292;DPMN&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#25552;&#39640;&#26816;&#27979;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05268v1 Announce Type: cross  Abstract: The detection of abusive language remains a long-standing challenge with the extensive use of social networks. The detection task of abusive language suffers from limited accuracy. We argue that the existing detection methods utilize the fine-tuning technique of the pre-trained language models (PLMs) to handle downstream tasks. Hence, these methods fail to stimulate the general knowledge of the PLMs. To address the problem, we propose a novel Deep Prompt Multi-task Network (DPMN) for abuse language detection. Specifically, DPMN first attempts to design two forms of deep prompt tuning and light prompt tuning for the PLMs. The effects of different prompt lengths, tuning strategies, and prompt initialization methods on detecting abusive language are studied. In addition, we propose a Task Head based on Bi-LSTM and FFN, which can be used as a short text classifier. Eventually, DPMN utilizes multi-task learning to improve detection metrics 
&lt;/p&gt;</description></item><item><title>ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;</title><link>https://arxiv.org/abs/2403.05266</link><description>&lt;p&gt;
ERBench&#65306;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05266
&lt;/p&gt;
&lt;p&gt;
ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24187;&#35273;&#22522;&#20934;&#35201;&#20040;&#26159;&#38745;&#24577;&#30340;&#65292;&#35201;&#20040;&#32570;&#20047;&#21487;&#35843;&#25972;&#30340;&#22797;&#26434;&#24615;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#35748;&#20026;&#21033;&#29992;&#29616;&#26377;&#30340;&#20851;&#31995;&#25968;&#25454;&#24211;&#26159;&#26500;&#24314;&#22522;&#20934;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ERBench&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#36716;&#25442;&#20026;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#65288;ER&#65289;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#24211;&#27169;&#24335;&#12289;&#35760;&#24405;&#21644;&#21151;&#33021;&#20381;&#36182;&#26469;&#26500;&#24314;&#38382;&#39064;&#65292;&#20197;&#20415;&#21487;&#20197;&#33258;&#21160;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22806;&#38190;&#32422;&#26463;&#26469;&#36830;&#25509;&#20851;&#31995;&#21644;&#26500;&#24314;&#22810;&#36339;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#20219;&#24847;&#22797;&#26434;&#65292;&#29992;&#20110;&#35843;&#35797;LLMs&#30340;&#20013;&#38388;&#31572;&#26696;&#12290;&#26368;&#21518;&#65292;ERBench&#25903;&#25345;&#25345;&#32493;&#35780;&#20272;&#65292;&#22810;&#27169;&#24577;qu
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05266v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue. Existing hallucination benchmarks are either static or lack adjustable complexity for thorough analysis. We contend that utilizing existing relational databases is a promising approach for constructing benchmarks due to their accurate knowledge description via functional dependencies. We propose ERBench to automatically convert any relational database into a benchmark based on the entity-relationship (ER) model. Our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified. In addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of LLMs. Finally, ERBench supports continuous evaluation, multimodal qu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#27604;&#20102;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#26426;&#22120;&#32763;&#35793;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#21333;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#32500;&#22522;&#30334;&#31185;&#39046;&#22495;&#20248;&#20110;NLI&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.05257</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#36716;&#31227;&#36824;&#26159;&#26426;&#22120;&#32763;&#35793;&#65311;&#20851;&#20110;&#21333;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05257
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#27604;&#20102;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#26426;&#22120;&#32763;&#35793;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#21333;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#32500;&#22522;&#30334;&#31185;&#39046;&#22495;&#20248;&#20110;NLI&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26356;&#22909;&#30340;&#21477;&#23376;&#23884;&#20837;&#23558;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30452;&#25509;&#27604;&#36739;&#20102;&#20316;&#20026;&#21333;&#35821;STS&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65306;&#65288;a&#65289;&#21033;&#29992;&#20165;&#33521;&#35821;&#36164;&#28304;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#20197;&#29983;&#25104;&#38750;&#33521;&#35821;&#21477;&#23376;&#23884;&#20837;&#20316;&#20026;&#38646;&#30701;&#25512;&#29702;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#20197;&#21450;&#65288;b&#65289;&#23558;&#33521;&#35821;&#25968;&#25454;&#25552;&#21069;&#36716;&#25442;&#20026;&#20266;&#38750;&#33521;&#35821;&#35757;&#32451;&#25968;&#25454;&#30340;&#26426;&#22120;&#32763;&#35793;&#12290;&#22312;&#25105;&#20204;&#22312;&#26085;&#35821;&#21644;&#38889;&#35821;&#30340;&#21333;&#35821;STS&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20004;&#31181;&#25968;&#25454;&#25216;&#26415;&#34920;&#29616;&#30456;&#24403;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#32500;&#22522;&#30334;&#31185;&#39046;&#22495;&#20248;&#20110;NLI&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05257v1 Announce Type: new  Abstract: Learning better sentence embeddings leads to improved performance for natural language understanding tasks including semantic textual similarity (STS) and natural language inference (NLI). As prior studies leverage large-scale labeled NLI datasets for fine-tuning masked language models to yield sentence embeddings, task performance for languages other than English is often left behind. In this study, we directly compared two data augmentation techniques as potential solutions for monolingual STS: (a) cross-lingual transfer that exploits English resources alone as training data to yield non-English sentence embeddings as zero-shot inference, and (b) machine translation that coverts English data into pseudo non-English training data in advance. In our experiments on monolingual STS in Japanese and Korean, we find that the two data techniques yield performance on par. Rather, we find a superiority of the Wikipedia domain over the NLI domain
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LLMQA&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#25198;&#28436;&#29983;&#25104;&#22120;&#12289;&#37325;&#26032;&#25490;&#24207;&#22120;&#21644;&#35780;&#20272;&#22120;&#31561;&#22810;&#37325;&#35282;&#33394;&#65292;&#32467;&#21512;&#20102;&#26816;&#32034;&#21644;&#29983;&#25104;&#35777;&#25454;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.05217</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35282;&#33394;&#33021;&#21147;&#36827;&#34892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05217
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LLMQA&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#25198;&#28436;&#29983;&#25104;&#22120;&#12289;&#37325;&#26032;&#25490;&#24207;&#22120;&#21644;&#35780;&#20272;&#22120;&#31561;&#22810;&#37325;&#35282;&#33394;&#65292;&#32467;&#21512;&#20102;&#26816;&#32034;&#21644;&#29983;&#25104;&#35777;&#25454;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#24050;&#32463;&#25104;&#20026;&#20449;&#24687;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30740;&#31350;&#28966;&#28857;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#36981;&#24490;&#20004;&#31181;&#33539;&#24335;&#26469;&#25910;&#38598;&#35777;&#25454;&#65306;&#65288;1&#65289;\textit{&#26816;&#32034;-&#28982;&#21518;&#38405;&#35835;}&#33539;&#24335;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65307;&#21644;&#65288;2&#65289;\textit{&#29983;&#25104;-&#28982;&#21518;&#38405;&#35835;}&#33539;&#24335;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30456;&#20851;&#25991;&#26723;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#19981;&#33021;&#23436;&#20840;&#28385;&#36275;&#35777;&#25454;&#30340;&#22810;&#26041;&#38754;&#35201;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMQA&#65292;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;ODQA&#36807;&#31243;&#20998;&#20026;&#19977;&#20010;&#22522;&#26412;&#27493;&#39588;&#65306;&#26597;&#35810;&#25193;&#23637;&#65292;&#25991;&#26723;&#36873;&#25321;&#21644;&#31572;&#26696;&#29983;&#25104;&#65292;&#32467;&#21512;&#20102;&#26816;&#32034;&#21644;&#29983;&#25104;&#35777;&#25454;&#30340;&#20248;&#21183;&#12290;&#30001;&#20110;LLMs&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#26469;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#65292;&#25105;&#20204;&#25351;&#23548;LLMs&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#25198;&#28436;&#29983;&#25104;&#22120;&#12289;&#37325;&#26032;&#25490;&#24207;&#22120;&#21644;&#35780;&#20272;&#22120;&#31561;&#22810;&#31181;&#35282;&#33394;&#65292;&#20351;&#23427;&#20204;&#34701;&#21512;&#22312;ODQA&#36807;&#31243;&#20013;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05217v1 Announce Type: cross  Abstract: Open-domain question answering (ODQA) has emerged as a pivotal research spotlight in information systems. Existing methods follow two main paradigms to collect evidence: (1) The \textit{retrieve-then-read} paradigm retrieves pertinent documents from an external corpus; and (2) the \textit{generate-then-read} paradigm employs large language models (LLMs) to generate relevant documents. However, neither can fully address multifaceted requirements for evidence. To this end, we propose LLMQA, a generalized framework that formulates the ODQA process into three basic steps: query expansion, document selection, and answer generation, combining the superiority of both retrieval-based and generation-based evidence. Since LLMs exhibit their excellent capabilities to accomplish various tasks, we instruct LLMs to play multiple roles as generators, rerankers, and evaluators within our framework, integrating them to collaborate in the ODQA process. 
&lt;/p&gt;</description></item><item><title>SocialPET &#26159;&#19968;&#31181;&#31038;&#20132;&#20449;&#24687;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#20132;&#32593;&#32476;&#32467;&#26500;&#26469;&#36827;&#34892;&#31435;&#22330;&#26816;&#27979;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.05216</link><description>&lt;p&gt;
SocialPET: &#31038;&#20132;&#20449;&#24687;&#27169;&#24335;&#21033;&#29992;&#35757;&#32451;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#23569;&#26679;&#26412;&#31435;&#22330;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SocialPET: Socially Informed Pattern Exploiting Training for Few-Shot Stance Detection in Social Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05216
&lt;/p&gt;
&lt;p&gt;
SocialPET &#26159;&#19968;&#31181;&#31038;&#20132;&#20449;&#24687;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#20132;&#32593;&#32476;&#32467;&#26500;&#26469;&#36827;&#34892;&#31435;&#22330;&#26816;&#27979;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#26816;&#27979;&#26159;&#25351;&#30830;&#23450;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#23545;&#20110;&#29305;&#23450;&#30446;&#26631;&#30340;&#35266;&#28857;&#26159;&#21542;&#8220;&#36190;&#25104;&#8221;&#25110;&#8220;&#21453;&#23545;&#8221;&#30340;&#20219;&#21153;&#65292;&#22312;&#20855;&#26377;&#38480;&#21046;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#24615;&#20294;&#29616;&#23454;&#24773;&#26223;&#20013;&#19968;&#30452;&#30740;&#31350;&#19981;&#36275;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;SocialPET&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#20132;&#20449;&#24687;&#24341;&#23548;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36825;&#19968;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;Pattern Exploiting Training (PET) &#25216;&#26415;&#20043;&#19978;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23558;&#20998;&#31867;&#20219;&#21153;&#22788;&#29702;&#20026;&#22635;&#31354;&#38382;&#39064;&#12290;&#20026;&#20102;&#22686;&#24378;&#35813;&#26041;&#27861;&#30340;&#31038;&#20132;&#24847;&#35782;&#65292;&#25105;&#20204;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21608;&#22260;&#30340;&#31038;&#20132;&#32593;&#32476;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SocialPET&#22312;&#20004;&#20010;&#31435;&#22330;&#25968;&#25454;&#38598;Multi-target&#21644;P-Stance&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#31454;&#20105;&#24615;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#20197;&#21450;&#22522;&#20934;&#27169;&#22411;PET&#65292;&#21363;&#38024;&#23545;&#30740;&#31350;&#23545;&#35937;&#26631;&#35760;&#23454;&#20363;&#20165;&#20026;100&#24038;&#21491;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05216v1 Announce Type: new  Abstract: Stance detection, as the task of determining the viewpoint of a social media post towards a target as 'favor' or 'against', has been understudied in the challenging yet realistic scenario where there is limited labeled data for a certain target. Our work advances research in few-shot stance detection by introducing SocialPET, a socially informed approach to leveraging language models for the task. Our proposed approach builds on the Pattern Exploiting Training (PET) technique, which addresses classification tasks as cloze questions through the use of language models. To enhance the approach with social awareness, we exploit the social network structure surrounding social media posts. We prove the effectiveness of SocialPET on two stance datasets, Multi-target and P-Stance, outperforming competitive stance detection models as well as the base model, PET, where the labeled instances for the target under study is as few as 100. When we delv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36861;&#36394;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30340;&#26469;&#28304;&#65292;&#21457;&#29616;&#20102;&#19977;&#31181;&#27169;&#24335;&#65306;&#35821;&#35328;&#29420;&#31435;&#12289;&#36328;&#35821;&#35328;&#20849;&#20139;&#21644;&#36716;&#31227;&#65292;&#20026;&#21306;&#20998;&#23427;&#20204;&#25552;&#20986;&#20102;&#26041;&#27861;&#65292;&#20984;&#26174;&#20102;&#22312;&#22810;&#35821;&#35328;LMs&#20013;&#20445;&#25345;&#19968;&#33268;&#20107;&#23454;&#30693;&#35782;&#30340;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#22312;ML-LMs&#20013;&#25913;&#36827;&#20107;&#23454;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.05189</link><description>&lt;p&gt;
&#36861;&#36394;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30340;&#26681;&#28304;&#65306;&#29420;&#31435;&#30340;&#12289;&#20849;&#20139;&#30340;&#21644;&#36716;&#31227;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36861;&#36394;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30340;&#26469;&#28304;&#65292;&#21457;&#29616;&#20102;&#19977;&#31181;&#27169;&#24335;&#65306;&#35821;&#35328;&#29420;&#31435;&#12289;&#36328;&#35821;&#35328;&#20849;&#20139;&#21644;&#36716;&#31227;&#65292;&#20026;&#21306;&#20998;&#23427;&#20204;&#25552;&#20986;&#20102;&#26041;&#27861;&#65292;&#20984;&#26174;&#20102;&#22312;&#22810;&#35821;&#35328;LMs&#20013;&#20445;&#25345;&#19968;&#33268;&#20107;&#23454;&#30693;&#35782;&#30340;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#22312;ML-LMs&#20013;&#25913;&#36827;&#20107;&#23454;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#20302;&#36164;&#28304;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#22810;&#35821;&#35328;LMs&#65288;ML-LMs&#65289;&#20013;&#36827;&#34892;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ML-LMs&#22914;&#20309;&#33719;&#21462;&#21644;&#34920;&#31034;&#20107;&#23454;&#30693;&#35782;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22810;&#35821;&#35328;&#20107;&#23454;&#30693;&#35782;&#25506;&#27979;&#25968;&#25454;&#38598;mLAMA&#23545;ML-LMs&#65288;&#29305;&#21035;&#26159;&#22810;&#35821;&#35328;BERT&#65289;&#36827;&#34892;&#31070;&#32463;&#20803;&#35843;&#26597;&#12290;&#28982;&#21518;&#25105;&#20204;&#36861;&#28335;&#20107;&#23454;&#30340;&#26681;&#28304;&#65288;&#32500;&#22522;&#30334;&#31185;&#65289;&#65292;&#20197;&#30830;&#23450;ML-LMs&#33719;&#21462;&#29305;&#23450;&#20107;&#23454;&#30340;&#26041;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;ML-LMs&#33719;&#21462;&#21644;&#34920;&#31034;&#20107;&#23454;&#30340;&#19977;&#31181;&#27169;&#24335;&#65306;&#35821;&#35328;&#29420;&#31435;&#12289;&#36328;&#35821;&#35328;&#20849;&#20139;&#21644;&#36716;&#31227;&#65292;&#24182;&#21046;&#23450;&#20102;&#21306;&#20998;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#26174;&#20102;&#36328;&#35821;&#35328;&#20445;&#25345;&#19968;&#33268;&#30340;&#20107;&#23454;&#30693;&#35782;&#30340;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#22312;ML-LMs&#20013;&#36827;&#34892;&#26356;&#22909;&#30340;&#20107;&#23454;&#34920;&#31034;&#23398;&#20064;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05189v1 Announce Type: cross  Abstract: Acquiring factual knowledge for language models (LMs) in low-resource languages poses a serious challenge, thus resorting to cross-lingual transfer in multilingual LMs (ML-LMs). In this study, we ask how ML-LMs acquire and represent factual knowledge. Using the multilingual factual knowledge probing dataset, mLAMA, we first conducted a neuron investigation of ML-LMs (specifically, multilingual BERT). We then traced the roots of facts back to the knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire specific facts. We finally identified three patterns of acquiring and representing facts in ML-LMs: language-independent, cross-lingual shared and transferred, and devised methods for differentiating them. Our findings highlight the challenge of maintaining consistent factual knowledge across languages, underscoring the need for better fact representation learning in ML-LMs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CommitBench&#30340;&#26032;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#26368;&#20339;&#23454;&#36341;&#36827;&#34892;&#25968;&#25454;&#38598;&#21019;&#24314;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#25552;&#20132;&#20449;&#24687;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.05188</link><description>&lt;p&gt;
CommitBench&#65306;&#29992;&#20110;&#25552;&#20132;&#20449;&#24687;&#29983;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CommitBench: A Benchmark for Commit Message Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05188
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CommitBench&#30340;&#26032;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#26368;&#20339;&#23454;&#36341;&#36827;&#34892;&#25968;&#25454;&#38598;&#21019;&#24314;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#25552;&#20132;&#20449;&#24687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#25552;&#20132;&#20449;&#24687;&#23545;&#35768;&#22810;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#39033;&#20047;&#21619;&#30340;&#26085;&#24120;&#20219;&#21153;&#65292;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#33258;&#21160;&#21270;&#27492;&#20219;&#21153;&#26377;&#28508;&#21147;&#33410;&#30465;&#26102;&#38388;&#65292;&#21516;&#26102;&#30830;&#20445;&#20449;&#24687;&#20855;&#26377;&#20449;&#24687;&#37327;&#12290;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#21644;&#23458;&#35266;&#30340;&#22522;&#20934;&#27979;&#35797;&#26159;&#26397;&#30528;&#36825;&#19968;&#30446;&#26631;&#36827;&#34892;&#22362;&#23454;&#30740;&#31350;&#21644;&#35780;&#20272;&#30340;&#37325;&#35201;&#20808;&#20915;&#26465;&#20214;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#25968;&#25454;&#38598;&#23384;&#22312;&#21508;&#31181;&#38382;&#39064;&#65292;&#20363;&#22914;&#25552;&#20132;&#36873;&#25321;&#30340;&#36136;&#37327;&#12289;&#26679;&#26412;&#37327;&#23567;&#12289;&#37325;&#22797;&#12289;&#38544;&#31169;&#38382;&#39064;&#20197;&#21450;&#27809;&#26377;&#25480;&#26435;&#36827;&#34892;&#20877;&#20998;&#21457;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#26080;&#27861;&#20351;&#29992;&#30340;&#27169;&#22411;&#21644;&#20559;&#20506;&#30340;&#35780;&#20272;&#65292;&#27425;&#20248;&#27169;&#22411;&#30001;&#20110;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#32780;&#33719;&#24471;&#26356;&#39640;&#30340;&#35780;&#20272;&#20998;&#25968;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;CommitBench&#65292;&#37319;&#29992;&#20102;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#25105;&#20204;&#20174;&#35768;&#21487;&#20801;&#35768;&#20877;&#20998;&#21457;&#30340;&#21508;&#31181;&#39033;&#30446;&#20013;&#25277;&#26679;&#25552;&#20132;&#65292;&#24182;&#24212;&#29992;&#25105;&#20204;&#30340;&#36807;&#28388;&#21644;&#25968;&#25454;&#38598;&#22686;&#24378;&#25514;&#26045;&#20197;&#25552;&#39640;&#29983;&#25104;&#30340;&#25552;&#20132;&#20449;&#24687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05188v1 Announce Type: new  Abstract: Writing commit messages is a tedious daily task for many software developers, and often remains neglected. Automating this task has the potential to save time while ensuring that messages are informative. A high-quality dataset and an objective benchmark are vital preconditions for solid research and evaluation towards this goal. We show that existing datasets exhibit various problems, such as the quality of the commit selection, small sample sizes, duplicates, privacy issues, and missing licenses for redistribution. This can lead to unusable models and skewed evaluations, where inferior models achieve higher evaluation scores due to biases in the data. We compile a new large-scale dataset, CommitBench, adopting best practices for dataset creation. We sample commits from diverse projects with licenses that permit redistribution and apply our filtering and dataset enhancements to improve the quality of generated commit messages. We use Co
&lt;/p&gt;</description></item><item><title>ROUGE-K&#26159;&#19968;&#31181;&#20851;&#38190;&#35789;&#23548;&#21521;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#37327;&#21270;&#22238;&#31572;&#25688;&#35201;&#26159;&#21542;&#21253;&#21547;&#20851;&#38190;&#35789;&#36825;&#19968;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#24378;&#22522;&#32447;&#27169;&#22411;&#22312;&#25688;&#35201;&#20013;&#32463;&#24120;&#36951;&#28431;&#20851;&#38190;&#20449;&#24687;&#65292;&#20154;&#31867;&#27880;&#37322;&#21592;&#35273;&#24471;&#21253;&#21547;&#26356;&#22810;&#20851;&#38190;&#35789;&#30340;&#25688;&#35201;&#26356;&#30456;&#20851;&#12290;</title><link>https://arxiv.org/abs/2403.05186</link><description>&lt;p&gt;
ROUGE-K&#65306;&#24744;&#30340;&#25688;&#35201;&#21253;&#21547;&#20851;&#38190;&#35789;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
ROUGE-K: Do Your Summaries Have Keywords?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05186
&lt;/p&gt;
&lt;p&gt;
ROUGE-K&#26159;&#19968;&#31181;&#20851;&#38190;&#35789;&#23548;&#21521;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#37327;&#21270;&#22238;&#31572;&#25688;&#35201;&#26159;&#21542;&#21253;&#21547;&#20851;&#38190;&#35789;&#36825;&#19968;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#24378;&#22522;&#32447;&#27169;&#22411;&#22312;&#25688;&#35201;&#20013;&#32463;&#24120;&#36951;&#28431;&#20851;&#38190;&#20449;&#24687;&#65292;&#20154;&#31867;&#27880;&#37322;&#21592;&#35273;&#24471;&#21253;&#21547;&#26356;&#22810;&#20851;&#38190;&#35789;&#30340;&#25688;&#35201;&#26356;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#65292;&#22312;&#25688;&#35201;&#20013;&#25351;&#30340;&#26159;&#19982;&#20869;&#23481;&#30456;&#20851;&#30340;&#20851;&#38190;&#35789;&#65292;&#22312;&#26377;&#25928;&#20256;&#36798;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#27492;&#22312;&#35780;&#20272;&#20013;&#26816;&#26597;&#31995;&#32479;&#29983;&#25104;&#30340;&#25688;&#35201;&#26159;&#21542;&#21253;&#21547;&#36825;&#20123;&#20449;&#24687;&#24615;&#20851;&#38190;&#35789;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26497;&#31471;&#25688;&#35201;&#27169;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#24182;&#26410;&#26126;&#30830;&#20851;&#27880;&#25688;&#35201;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#20351;&#24320;&#21457;&#20154;&#21592;&#23545;&#20854;&#26159;&#21542;&#23384;&#22312;&#27627;&#26080;&#25152;&#30693;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#38190;&#35789;&#23548;&#21521;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#31216;&#20026;ROUGE-K&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#23450;&#37327;&#31572;&#26696;&#26469;&#22238;&#31572;&#20851;&#20110;&#8220;&#25688;&#35201;&#20013;&#20851;&#38190;&#35789;&#21253;&#21547;&#24471;&#22914;&#20309;&#8221;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#19968;&#20851;&#38190;&#35789;&#24863;&#30693;&#24230;&#25351;&#26631;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#24778;&#35766;&#22320;&#21457;&#29616;&#30446;&#21069;&#30340;&#24378;&#22522;&#32447;&#27169;&#22411;&#32463;&#24120;&#22312;&#25688;&#35201;&#20013;&#28431;&#25481;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#65292;&#20154;&#31867;&#27880;&#37322;&#21592;&#30830;&#23454;&#21457;&#29616;&#21253;&#21547;&#26356;&#22810;&#20851;&#38190;&#35789;&#30340;&#25688;&#35201;&#19982;&#28304;&#25991;&#20214;&#26356;&#30456;&#20851;&#12290;&#36825;&#26159;&#35780;&#20272;&#20013;&#19968;&#20010;&#37325;&#35201;&#20294;&#20043;&#21069;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05186v1 Announce Type: new  Abstract: Keywords, that is, content-relevant words in summaries play an important role in efficient information conveyance, making it critical to assess if system-generated summaries contain such informative words during evaluation. However, existing evaluation metrics for extreme summarization models do not pay explicit attention to keywords in summaries, leaving developers ignorant of their presence. To address this issue, we present a keyword-oriented evaluation metric, dubbed ROUGE-K, which provides a quantitative answer to the question of -- \textit{How well do summaries include keywords?} Through the lens of this keyword-aware metric, we surprisingly find that a current strong baseline model often misses essential information in their summaries. Our analysis reveals that human annotators indeed find the summaries with more keywords to be more relevant to the source documents. This is an important yet previously overlooked aspect in evaluati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#22522;&#20110;&#35821;&#35328;&#30340;&#35760;&#24518;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#20854;&#23545;&#27169;&#26865;&#20004;&#21487;&#21477;&#23376;&#30340;&#22788;&#29702;&#33021;&#21147;&#22686;&#36827;&#20102;&#23545;&#20154;&#31867;&#35748;&#30693;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.05152</link><description>&lt;p&gt;
&#26397;&#21521;&#26426;&#22120;&#24515;&#29702;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#20154;&#31867;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Towards a Psychology of Machines: Large Language Models Predict Human Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#22522;&#20110;&#35821;&#35328;&#30340;&#35760;&#24518;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#20854;&#23545;&#27169;&#26865;&#20004;&#21487;&#21477;&#23376;&#30340;&#22788;&#29702;&#33021;&#21147;&#22686;&#36827;&#20102;&#23545;&#20154;&#31867;&#35748;&#30693;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#23613;&#31649;&#32570;&#20047;&#20154;&#31867;&#35748;&#30693;&#22522;&#30784;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#38500;&#20102;&#31616;&#21333;&#27169;&#20223;&#20154;&#31867;&#35821;&#35328;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#25552;&#20379;&#20851;&#20110;&#20154;&#31867;&#35748;&#30693;&#26426;&#21046;&#30340;&#27934;&#35265;&#65311;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;&#39044;&#27979;&#22522;&#20110;&#35821;&#35328;&#30340;&#35760;&#24518;&#20219;&#21153;&#20013;&#20154;&#31867;&#34920;&#29616;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#25991;&#26412;&#29702;&#35299;&#29702;&#35770;&#65292;&#25105;&#20204;&#20551;&#35774;&#35782;&#21035;&#27169;&#26865;&#20004;&#21487;&#30340;&#21477;&#23376;&#65288;&#20363;&#22914;&#65292;&#8220;&#22240;&#20026;&#27604;&#23572;&#21917;&#37202;&#65292;&#25152;&#20197;&#37202;&#20174;&#26410;&#30041;&#22312;&#25151;&#23376;&#37324;&#8221;&#65289;&#22312;&#21069;&#38754;&#25552;&#20379;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20250;&#24471;&#21040;&#20419;&#36827;&#12290;&#21442;&#19982;&#32773;&#65292;&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;ChatGPT&#65292;&#37117;&#34987;&#21576;&#29616;&#25104;&#23545;&#30340;&#21477;&#23376;&#12290;&#31532;&#20108;&#20010;&#21477;&#23376;&#24635;&#26159;&#19968;&#20010;&#26088;&#22312;&#22266;&#26377;&#22320;&#27169;&#26865;&#20004;&#21487;&#30340;&#33457;&#22253;&#36335;&#24452;&#21477;&#65292;&#32780;&#31532;&#19968;&#20010;&#21477;&#23376;&#21017;&#25552;&#20379;&#20102;&#21512;&#36866;&#30340;&#65288;&#20363;&#22914;&#65292;&#8220;&#27604;&#23572;&#24739;&#26377;&#24930;&#24615;&#37202;&#31934;&#20013;&#27602;&#8221;&#65289;&#25110;&#19981;&#21512;&#36866;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#65292;&#8220;&#27604;&#23572;&#21916;&#27426;&#25171;&#39640;&#23572;&#22827;&#8221;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05152v1 Announce Type: cross  Abstract: Large language models (LLMs) are demonstrating remarkable capabilities across various tasks despite lacking a foundation in human cognition. This raises the question: can these models, beyond simply mimicking human language patterns, offer insights into the mechanisms underlying human cognition? This study explores the ability of ChatGPT to predict human performance in a language-based memory task. Building upon theories of text comprehension, we hypothesize that recognizing ambiguous sentences (e.g., "Because Bill drinks wine is never kept in the house") is facilitated by preceding them with contextually relevant information. Participants, both human and ChatGPT, were presented with pairs of sentences. The second sentence was always a garden-path sentence designed to be inherently ambiguous, while the first sentence either provided a fitting (e.g., "Bill has chronic alcoholism") or an unfitting context (e.g., "Bill likes to play golf"
&lt;/p&gt;</description></item><item><title>ChatUIE&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#22522;&#20110;&#32842;&#22825;&#30340;&#32479;&#19968;&#20449;&#24687;&#25552;&#21462;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#29983;&#25104;&#32422;&#26463;&#25552;&#39640;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#25552;&#21462;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05132</link><description>&lt;p&gt;
ChatUIE&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#22522;&#20110;&#32842;&#22825;&#30340;&#32479;&#19968;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
ChatUIE: Exploring Chat-based Unified Information Extraction using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05132
&lt;/p&gt;
&lt;p&gt;
ChatUIE&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#22522;&#20110;&#32842;&#22825;&#30340;&#32479;&#19968;&#20449;&#24687;&#25552;&#21462;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#29983;&#25104;&#32422;&#26463;&#25552;&#39640;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#25552;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#22312;&#19968;&#33324;&#30340;&#32842;&#22825;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#65292;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;&#20174;&#20559;&#31163;&#24050;&#30693;&#27169;&#24335;&#25110;&#25351;&#20196;&#30340;&#33258;&#28982;&#35821;&#35328;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#23545;&#20110;&#20043;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#25506;&#32034;&#32842;&#22825;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#24314;&#27169;&#20316;&#20026;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatUIE&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;ChatGLM&#26500;&#24314;&#30340;&#21019;&#26032;&#30340;&#32479;&#19968;&#20449;&#24687;&#25552;&#21462;&#26694;&#26550;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25913;&#36827;&#21644;&#23545;&#40784;&#28041;&#21450;&#28151;&#20081;&#21644;&#26377;&#38480;&#26679;&#26412;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25972;&#21512;&#20102;&#29983;&#25104;&#32422;&#26463;&#26469;&#35299;&#20915;&#22312;&#36755;&#20837;&#20013;&#19981;&#23384;&#22312;&#30340;&#20803;&#32032;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ChatUIE&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05132v1 Announce Type: cross  Abstract: Recent advancements in large language models have shown impressive performance in general chat. However, their domain-specific capabilities, particularly in information extraction, have certain limitations. Extracting structured information from natural language that deviates from known schemas or instructions has proven challenging for previous prompt-based methods. This motivated us to explore domain-specific modeling in chat-based language models as a solution for extracting structured information from natural language. In this paper, we present ChatUIE, an innovative unified information extraction framework built upon ChatGLM. Simultaneously, reinforcement learning is employed to improve and align various tasks that involve confusing and limited samples. Furthermore, we integrate generation constraints to address the issue of generating elements that are not present in the input. Our experimental results demonstrate that ChatUIE ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#65292;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#22522;&#26412;&#35268;&#21017;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.05101</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Rule-driven News Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#65292;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#22522;&#26412;&#35268;&#21017;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
News captioning&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#25551;&#36848;&#22270;&#29255;&#21450;&#20854;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#25110;&#20855;&#20307;&#20107;&#20214;&#26469;&#29983;&#25104;&#21477;&#23376;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20381;&#36182;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#21462;&#24471;&#26174;&#33879;&#25104;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#19987;&#27880;&#20110;&#36755;&#20837;&#26032;&#38395;&#20869;&#23481;&#19982;&#36755;&#20986;&#39044;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#38656;&#35201;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#30340;&#19968;&#20123;&#22522;&#26412;&#35268;&#21017;&#65292;&#22914;&#20934;&#30830;&#25551;&#36848;&#19982;&#20107;&#20214;&#30456;&#20851;&#30340;&#20010;&#20307;&#21644;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#25351;&#23450;&#30340;&#35268;&#21017;&#20449;&#21495;&#29983;&#25104;&#22270;&#20687;&#25551;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#25551;&#36848;&#35774;&#35745;&#20102;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#12290;&#36825;&#19968;&#35268;&#21017;&#21253;&#25324;&#22270;&#29255;&#20013;&#25551;&#32472;&#30340;&#20027;&#35201;&#21160;&#20316;&#65288;&#20363;&#22914;&#65292;&#8220;&#25191;&#34892;&#8221;&#65289;&#20197;&#21450;&#21442;&#19982;&#21160;&#20316;&#30340;&#21629;&#21517;&#23454;&#20307;&#25198;&#28436;&#30340;&#35282;&#33394;&#65288;&#20363;&#22914;&#65292;&#8220;&#20195;&#29702;&#20154;&#8221;&#21644;&#8220;&#22320;&#28857;&#8221;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#35821;&#20041;&#35268;&#21017;&#27880;&#20837;&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05101v1 Announce Type: cross  Abstract: News captioning task aims to generate sentences by describing named entities or concrete events for an image with its news article. Existing methods have achieved remarkable results by relying on the large-scale pre-trained models, which primarily focus on the correlations between the input news content and the output predictions. However, the news captioning requires adhering to some fundamental rules of news reporting, such as accurately describing the individuals and actions associated with the event. In this paper, we propose the rule-driven news captioning method, which can generate image descriptions following designated rule signal. Specifically, we first design the news-aware semantic rule for the descriptions. This rule incorporates the primary action depicted in the image (e.g., "performing") and the roles played by named entities involved in the action (e.g., "Agent" and "Place"). Second, we inject this semantic rule into th
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#36827;&#34892;RST&#35821;&#31687;&#35299;&#26512;&#65292;&#24182;&#22312;&#24213;&#37096;&#31574;&#30053;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.05065</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;RST&#35821;&#31687;&#35299;&#26512;&#20013;&#33021;&#21542;&#21462;&#24471;&#26174;&#33879;&#25104;&#21151;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we obtain significant success in RST discourse parsing by using Large Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05065
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#36827;&#34892;RST&#35821;&#31687;&#35299;&#26512;&#65292;&#24182;&#22312;&#24213;&#37096;&#31574;&#30053;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21482;&#26377;&#35299;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#34429;&#28982;&#24050;&#32463;&#35777;&#26126;&#20165;&#32534;&#30721;&#22120;&#25110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#31687;&#35299;&#26512;&#20013;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;LLMs&#33021;&#22815;&#25191;&#34892;&#36825;&#39033;&#20219;&#21153;&#30340;&#31243;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#23545;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#35821;&#31687;&#35299;&#26512;&#30340;&#30410;&#22788;&#12290;&#22312;&#36825;&#37324;&#65292;&#22522;&#26412;&#33258;&#39030;&#21521;&#19979;&#21644;&#33258;&#24213;&#21521;&#19978;&#31574;&#30053;&#30340;&#35299;&#26512;&#36807;&#31243;&#34987;&#36716;&#25442;&#20026;LLMs&#21487;&#20197;&#20351;&#29992;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;Llama 2&#65292;&#24182;&#29992;QLoRA&#36827;&#34892;&#24494;&#35843;&#65292;&#21518;&#32773;&#20855;&#26377;&#21487;&#20197;&#35843;&#33410;&#30340;&#26356;&#23569;&#21442;&#25968;&#12290;&#23545;RST-DT&#12289;Instr-DT&#21644;GUM&#35821;&#26009;&#24211;&#30340;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24213;&#37096;&#31574;&#30053;&#20013;Llama 2&#30340;70&#20159;&#21442;&#25968;&#33719;&#24471;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05065v1 Announce Type: new  Abstract: Recently, decoder-only pre-trained large language models (LLMs), with several tens of billion parameters, have significantly impacted a wide range of natural language processing (NLP) tasks. While encoder-only or encoder-decoder pre-trained language models have already proved to be effective in discourse parsing, the extent to which LLMs can perform this task remains an open research question. Therefore, this paper explores how beneficial such LLMs are for Rhetorical Structure Theory (RST) discourse parsing. Here, the parsing process for both fundamental top-down and bottom-up strategies is converted into prompts, which LLMs can work with. We employ Llama 2 and fine-tune it with QLoRA, which has fewer parameters that can be tuned. Experimental results on three benchmark datasets, RST-DT, Instr-DT, and the GUM corpus, demonstrate that Llama 2 with 70 billion parameters in the bottom-up strategy obtained state-of-the-art (SOTA) results wit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#20154;&#31867;&#23545;&#35805;&#26102;&#30340;&#27880;&#24847;&#26426;&#21046;&#21464;&#21270;&#65292;&#21457;&#29616;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#27880;&#24847;&#34892;&#20026;&#65292;&#20294;&#22312;&#19987;&#38376;&#22788;&#29702;&#20154;&#31867;&#23545;&#35805;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#65292;&#38656;&#35201;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#23545;&#35805;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#26469;&#22686;&#24378;&#29702;&#35299;&#21644;&#29983;&#25104;</title><link>https://arxiv.org/abs/2403.05045</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#35805;&#26159;&#21542;&#29305;&#27530;&#65311;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Are Human Conversations Special? A Large Language Model Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#20154;&#31867;&#23545;&#35805;&#26102;&#30340;&#27880;&#24847;&#26426;&#21046;&#21464;&#21270;&#65292;&#21457;&#29616;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#27880;&#24847;&#34892;&#20026;&#65292;&#20294;&#22312;&#19987;&#38376;&#22788;&#29702;&#20154;&#31867;&#23545;&#35805;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#65292;&#38656;&#35201;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#23545;&#35805;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#26469;&#22686;&#24378;&#29702;&#35299;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#20154;&#31867;&#20043;&#38388;&#30340;&#33258;&#28982;&#23545;&#35805;&#65288;&#20154;-&#20154;&#65289;&#26102;&#27880;&#24847;&#26426;&#21046;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#30340;&#19977;&#31181;&#29992;&#20363;&#65306;&#19982;&#32593;&#32476;&#20869;&#23481;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#25991;&#26412;&#30340;&#20114;&#21160;&#12290;&#36890;&#36807;&#20998;&#26512;&#36328;&#36825;&#20123;&#39046;&#22495;&#30340;&#27880;&#24847;&#36317;&#31163;&#12289;&#20998;&#25955;&#24615;&#21644;&#30456;&#20114;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23545;&#35805;&#25968;&#25454;&#25152;&#25552;&#20986;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#35805;&#38656;&#35201;&#32454;&#33268;&#22788;&#29702;&#38271;&#26399;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#23427;&#20204;&#30340;&#27880;&#24847;&#27169;&#24335;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#27880;&#24847;&#34892;&#20026;&#65292;&#20294;&#23427;&#20204;&#22312;&#19987;&#38376;&#21270;&#20154;&#31867;&#23545;&#35805;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#27880;&#24847;&#29109;&#20998;&#26512;&#21644;t-SNE&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38656;&#35201;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#23545;&#35805;&#25968;&#25454;&#26469;&#22686;&#24378;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05045v1 Announce Type: cross  Abstract: This study analyzes changes in the attention mechanisms of large language models (LLMs) when used to understand natural conversations between humans (human-human). We analyze three use cases of LLMs: interactions over web content, code, and mathematical texts. By analyzing attention distance, dispersion, and interdependency across these domains, we highlight the unique challenges posed by conversational data. Notably, conversations require nuanced handling of long-term contextual relationships and exhibit higher complexity through their attention patterns. Our findings reveal that while language models exhibit domain-specific attention behaviors, there is a significant gap in their ability to specialize in human conversations. Through detailed attention entropy analysis and t-SNE visualizations, we demonstrate the need for models trained with a diverse array of high-quality conversational data to enhance understanding and generation of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22810;&#27169;&#24577;&#23545;&#20107;&#23454;&#25512;&#29702;&#24773;&#24863;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20928;&#21270;&#21644;&#32531;&#35299;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05023</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#24046;&#20928;&#21270;&#23454;&#29616;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Multimodal Sentiment Analysis Debiasing via Bias Purification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05023
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22810;&#27169;&#24577;&#23545;&#20107;&#23454;&#25512;&#29702;&#24773;&#24863;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20928;&#21270;&#21644;&#32531;&#35299;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65288;MSA&#65289;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#65288;&#22914;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#38899;&#39057;&#65289;&#30340;&#19982;&#24773;&#24863;&#30456;&#20851;&#32447;&#32034;&#26469;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;MSA&#20219;&#21153;&#26222;&#36941;&#21463;&#21040;&#26410;&#32463;&#35745;&#21010;&#30340;&#25968;&#25454;&#38598;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22810;&#27169;&#24577;&#35805;&#35821;&#32423;&#26631;&#31614;&#20559;&#35265;&#21644;&#21333;&#35789;&#32423;&#19978;&#19979;&#25991;&#20559;&#35265;&#12290;&#36825;&#20123;&#26377;&#23475;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#35823;&#23548;&#27169;&#22411;&#19987;&#27880;&#20110;&#32479;&#35745;&#25463;&#24452;&#21644;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#32780;&#38750;&#20256;&#32479;&#20284;&#28982;&#24615;&#30340;&#22810;&#27169;&#24577;&#23545;&#20107;&#23454;&#25512;&#29702;&#24773;&#24863;&#65288;MCIS&#65289;&#20998;&#26512;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#19968;&#20010;&#22240;&#26524;&#22270;&#26469;&#21457;&#29616;&#24050;&#35757;&#32451;&#30340;&#22522;&#20934;&#27169;&#22411;&#20013;&#30340;&#26377;&#23475;&#20559;&#35265;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#32473;&#23450;&#19968;&#20010;&#20107;&#23454;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;MCIS&#24819;&#35937;&#20004;&#31181;&#23545;&#20107;&#23454;&#24773;&#24418;&#65292;&#20197;&#20928;&#21270;&#21644;&#32531;&#35299;&#36825;&#20123;&#20559;&#35265;&#12290;&#28982;&#21518;&#65292;MCIS&#21487;&#20197;&#20174;&#20559;&#24046;&#20013;&#20570;&#20986;&#19981;&#24102;&#20559;&#35265;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05023v1 Announce Type: new  Abstract: Multimodal Sentiment Analysis (MSA) aims to understand human intentions by integrating emotion-related clues from diverse modalities, such as visual, language, and audio. Unfortunately, the current MSA task invariably suffers from unplanned dataset biases, particularly multimodal utterance-level label bias and word-level context bias. These harmful biases potentially mislead models to focus on statistical shortcuts and spurious correlations, causing severe performance bottlenecks. To alleviate these issues, we present a Multimodal Counterfactual Inference Sentiment (MCIS) analysis framework based on causality rather than conventional likelihood. Concretely, we first formulate a causal graph to discover harmful biases from already-trained vanilla models. In the inference phase, given a factual multimodal input, MCIS imagines two counterfactual scenarios to purify and mitigate these biases. Then, MCIS can make unbiased decisions from biase
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#30340;&#20840;&#30693;&#27169;&#25311;&#27604;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#23481;&#26131;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#65292;&#23613;&#31649;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#25509;&#36817;&#23454;&#38469;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.05020</link><description>&lt;p&gt;
&#27169;&#25311;&#31038;&#20132;&#20114;&#21160;&#25104;&#21151;&#24615;&#30340;&#35823;&#23548;&#24615;&#65306;&#20197;LLMs&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#30340;&#20840;&#30693;&#27169;&#25311;&#27604;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#23481;&#26131;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#65292;&#23613;&#31649;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#25509;&#36817;&#23454;&#38469;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#31038;&#20132;&#27169;&#25311;&#26356;&#21152;&#20016;&#23500;&#65292;&#33021;&#22815;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30740;&#31350;&#21508;&#31181;&#31038;&#20132;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#22312;&#36825;&#20123;&#27169;&#25311;&#20013;&#37319;&#29992;&#20102;&#19968;&#31181;&#20840;&#30693;&#30340;&#36879;&#35270;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;LLM&#29983;&#25104;&#25152;&#26377;&#20132;&#35848;&#32773;&#65289;&#65292;&#36825;&#19982;&#20154;&#31867;&#20855;&#26377;&#30340;&#38750;&#20840;&#30693;&#12289;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#20114;&#21160;&#26681;&#26412;&#19981;&#31526;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20123;&#24046;&#24322;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#35774;&#23450;&#65288;&#20840;&#30693;&#12289;&#38750;&#20840;&#30693;&#65289;&#20013;&#20351;&#29992;LLMs&#27169;&#25311;&#31038;&#20132;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#20840;&#30693;&#26041;&#24335;&#27169;&#25311;&#30340;&#20132;&#35848;&#32773;&#22312;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#26041;&#38754;&#27604;&#38750;&#20840;&#30693;&#20195;&#29702;&#20154;&#26356;&#25104;&#21151;&#65292;&#23613;&#31649;&#21518;&#32773;&#26356;&#31526;&#21512;&#29616;&#23454;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#20174;&#20840;&#30693;&#27169;&#25311;&#20013;&#23398;&#20064;&#21487;&#20197;&#25913;&#21892;&#20132;&#20114;&#30340;&#33258;&#28982;&#24615;&#65292;&#20294;&#22312;&#21512;&#20316;&#22330;&#26223;&#20013;&#20960;&#20046;&#19981;&#33021;&#22686;&#24378;&#30446;&#26631;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05020v1 Announce Type: cross  Abstract: Recent advances in large language models (LLM) have enabled richer social simulations, allowing for the study of various social phenomena with LLM-based agents. However, most work has used an omniscient perspective on these simulations (e.g., single LLM to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that humans have. To examine these differences, we develop an evaluation framework to simulate social interactions with LLMs in various settings (omniscient, non-omniscient). Our experiments show that interlocutors simulated omnisciently are much more successful at accomplishing social goals compared to non-omniscient agents, despite the latter being the more realistic setting. Furthermore, we demonstrate that learning from omniscient simulations improves the apparent naturalness of interactions but scarcely enhances goal achievement in cooperative scenarios. Our f
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;R&amp;R&#26041;&#27861;&#65292;&#32467;&#21512;reprompting&#21644;in-context retrieval&#20004;&#31181;&#26032;&#22411;&#25552;&#31034;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05004</link><description>&lt;p&gt;
&#26080;&#27861;&#35760;&#20303;&#38271;&#25991;&#26723;&#20013;&#30340;&#32454;&#33410;&#65311;&#24744;&#38656;&#35201;&#19968;&#20123;R&amp;R
&lt;/p&gt;
&lt;p&gt;
Can't Remember Details in Long Documents? You Need Some R&amp;R
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05004
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;R&amp;R&#26041;&#27861;&#65292;&#32467;&#21512;reprompting&#21644;in-context retrieval&#20004;&#31181;&#26032;&#22411;&#25552;&#31034;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35832;&#22914;&#38271;&#31687;&#25991;&#26723;&#19978;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20250;&#38169;&#36807;&#19978;&#19979;&#25991;&#25991;&#26723;&#20013;&#38388;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;$\textit{R&amp;R}$&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#20004;&#31181;&#26032;&#22411;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;$\textit{reprompting}$&#21644;$\textit{in-context retrieval}$&#65288;ICR&#65289;&#65292;&#20197;&#20943;&#36731;&#25991;&#26723;&#22411;QA&#20013;&#30340;&#36825;&#31181;&#24433;&#21709;&#12290;&#22312;$\textit{reprompting}$&#20013;&#65292;&#25105;&#20204;&#21608;&#26399;&#24615;&#22320;&#22312;&#25972;&#20010;&#19978;&#19979;&#25991;&#25991;&#26723;&#20013;&#37325;&#22797;&#25552;&#31034;&#35828;&#26126;&#65292;&#20197;&#25552;&#37266;LLM&#20854;&#21407;&#22987;&#20219;&#21153;&#12290;&#22312;ICR&#20013;&#65292;&#25105;&#20204;&#24182;&#19981;&#25351;&#31034;LLM&#30452;&#25509;&#22238;&#31572;&#38382;&#39064;&#65292;&#32780;&#26159;&#25351;&#31034;&#23427;&#26816;&#32034;&#19982;&#32473;&#23450;&#38382;&#39064;&#26368;&#30456;&#20851;&#30340;&#21069;$k$&#20010;&#27573;&#33853;&#32534;&#21495;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#31532;&#20108;&#20010;QA&#25552;&#31034;&#20013;&#30340;&#32553;&#30053;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4 Turbo&#21644;Claude-2.1&#22312;&#38271;&#24230;&#36798;&#21040;80k&#26631;&#35760;&#30340;&#25991;&#26723;&#19978;&#27979;&#35797;&#20102;R&amp;R&#65292;&#24182;&#24179;&#22343;&#35266;&#23519;&#21040;QA&#20934;&#30830;&#29575;&#25552;&#21319;&#20102;16&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05004v1 Announce Type: cross  Abstract: Long-context large language models (LLMs) hold promise for tasks such as question-answering (QA) over long documents, but they tend to miss important information in the middle of context documents (arXiv:2307.03172v3). Here, we introduce $\textit{R&amp;R}$ -- a combination of two novel prompt-based methods called $\textit{reprompting}$ and $\textit{in-context retrieval}$ (ICR) -- to alleviate this effect in document-based QA. In reprompting, we repeat the prompt instructions periodically throughout the context document to remind the LLM of its original task. In ICR, rather than instructing the LLM to answer the question directly, we instruct it to retrieve the top $k$ passage numbers most relevant to the given question, which are then used as an abbreviated context in a second QA prompt. We test R&amp;R with GPT-4 Turbo and Claude-2.1 on documents up to 80k tokens in length and observe a 16-point boost in QA accuracy on average. Our further an
&lt;/p&gt;</description></item><item><title>DiffChat &#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20114;&#21160;&#22270;&#20687;&#21019;&#20316;&#20013;&#36827;&#34892;&#32842;&#22825;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.04997</link><description>&lt;p&gt;
DiffChat: &#23398;&#20064;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#36827;&#34892;&#20114;&#21160;&#22270;&#20687;&#21019;&#20316;&#30340;&#32842;&#22825;
&lt;/p&gt;
&lt;p&gt;
DiffChat: Learning to Chat with Text-to-Image Synthesis Models for Interactive Image Creation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04997
&lt;/p&gt;
&lt;p&gt;
DiffChat &#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20114;&#21160;&#22270;&#20687;&#21019;&#20316;&#20013;&#36827;&#34892;&#32842;&#22825;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DiffChat&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20197;&#25552;&#31034;&#20026;&#36755;&#20837;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#65288;TIS&#65289;&#27169;&#22411;&#65288;&#20363;&#22914;&#31283;&#23450;&#25193;&#25955;&#65289;&#23545;&#40784;&#65292;&#26469;"&#32842;&#22825;"&#20197;&#36827;&#34892;&#20114;&#21160;&#22270;&#20687;&#21019;&#20316;&#12290;&#32473;&#23450;&#21407;&#22987;&#25552;&#31034;/&#22270;&#20687;&#21644;&#29992;&#25143;&#25351;&#23450;&#30340;&#25351;&#20196;&#65292;DiffChat &#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#36866;&#24403;&#20462;&#25913;&#24182;&#29983;&#25104;&#30446;&#26631;&#25552;&#31034;&#65292;&#36825;&#21487;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#30446;&#26631;&#22270;&#20687;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#19968;&#20010;&#21517;&#20026;InstructPE&#30340;&#25351;&#20196;&#36981;&#24490;&#25552;&#31034;&#24037;&#31243;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110; DiffChat &#30340;&#30417;&#30563;&#35757;&#32451;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19977;&#20010;&#26680;&#24515;&#26631;&#20934;&#65288;&#32654;&#23398;&#12289;&#29992;&#25143;&#21916;&#22909;&#21644;&#20869;&#23481;&#23436;&#25972;&#24615;&#65289;&#30340;&#21453;&#39304;&#26469;&#36827;&#34892;&#22270;&#20687;&#21019;&#20316;&#12290;&#35813;&#26694;&#26550;&#28041;&#21450;&#19968;&#20010;&#21160;&#24577;&#20462;&#25913;&#25216;&#26415;&#26469;&#33719;&#24471;&#26356;&#30456;&#20851;&#30340;&#27491;&#26679;&#26412;&#21644;&#26356;&#38590;&#30340;&#36127;&#26679;&#26412;&#65292;&#20197;&#23454;&#29616;&#33073;&#26426;&#37319;&#26679;&#12290;&#20869;&#23481;&#23436;&#25972;&#24615;&#36824;&#34987;&#24341;&#20837;&#21040;&#20540;&#20272;&#35745;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04997v1 Announce Type: new  Abstract: We present DiffChat, a novel method to align Large Language Models (LLMs) to "chat" with prompt-as-input Text-to-Image Synthesis (TIS) models (e.g., Stable Diffusion) for interactive image creation. Given a raw prompt/image and a user-specified instruction, DiffChat can effectively make appropriate modifications and generate the target prompt, which can be leveraged to create the target image of high quality. To achieve this, we first collect an instruction-following prompt engineering dataset named InstructPE for the supervised training of DiffChat. Next, we propose a reinforcement learning framework with the feedback of three core criteria for image creation, i.e., aesthetics, user preference, and content integrity. It involves an action-space dynamic modification technique to obtain more relevant positive samples and harder negative samples during the off-policy sampling. Content integrity is also introduced into the value estimation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#23450;&#20041;&#39046;&#22495;&#30693;&#35782;&#22270;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.04964</link><description>&lt;p&gt;
&#21578;&#35785;&#25105;&#23454;&#35805;&#65306;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Tell me the truth: A system to measure the trustworthiness of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#23450;&#20041;&#39046;&#22495;&#30693;&#35782;&#22270;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#20174;2023&#24180;11&#26376;ChatGPT&#25512;&#20986;&#20197;&#26469;&#65292;&#22312;&#22823;&#22810;&#25968;&#26032;&#38395;&#20013;&#21344;&#25454;&#20102;&#37325;&#35201;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#19968;&#24180;&#22810;&#36807;&#21435;&#20102;&#65292;&#20844;&#21496;&#25269;&#35302;&#37319;&#29992;&#23427;&#20204;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#20182;&#20204;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#32570;&#20047;&#20449;&#24515;&#12290;&#19968;&#39033;&#30001;Baymard&#65288;2023&#65289;&#36827;&#34892;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT-4 &#22312;&#35782;&#21035;&#32593;&#31449;&#21487;&#29992;&#24615;&#38382;&#39064;&#26102;&#26377;80.1%&#30340;&#20551;&#38451;&#24615;&#38169;&#35823;&#29575;&#12290;&#32780;&#12298;JAMA&#20799;&#31185;&#23398;&#12299;&#26434;&#24535;&#65288;JAMA Pediatrics&#65289;&#20110;2024&#24180;1&#26376;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT &#22312;&#35786;&#26029;&#20799;&#31185;&#21307;&#30103;&#26696;&#20363;&#26102;&#30340;&#20934;&#30830;&#29575;&#20026;17%&#65288;Barile et al., 2024&#65289;&#12290;&#37027;&#20040;&#65292;&#20309;&#20026;&#8220;&#20449;&#20219;&#8221;&#65311;&#20449;&#20219;&#26159;&#19968;&#20010;&#30456;&#23545;&#30340;&#12289;&#20027;&#35266;&#30340;&#26465;&#20214;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#21270;&#12289;&#39046;&#22495;&#21644;&#20010;&#20307;&#32780;&#21464;&#21270;&#12290;&#37027;&#20040;&#65292;&#22312;&#32473;&#23450;&#19968;&#20010;&#39046;&#22495;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#34913;&#37327;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#21602;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#23450;&#20041;&#39046;&#22495;&#30693;&#35782;&#22270;&#34920;&#31034;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#26469;&#34913;&#37327;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04964v1 Announce Type: new  Abstract: Large Language Models (LLM) have taken the front seat in most of the news since November 2023, when ChatGPT was introduced. After more than one year, one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems. In a study by (Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in identifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics found that ChatGPT has an accuracy rate of 17% percent when diagnosing pediatric medical cases (Barile et al., 2024). But then, what is "trust"? Trust is a relative, subject condition that can change based on culture, domain, individuals. And then, given a domain, how can the trustworthiness of a system be measured? In this paper, I present a systematic approach to measure trustworthiness based on a predefined ground truth, represented as a knowledge graph of the domain. The approach is a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#35780;&#20272;&#20102;GPT-4&#22312;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#34920;&#29616;&#65292;&#25351;&#20986;&#29616;&#26377;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#20173;&#26377;&#24453;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.04963</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#38169;&#35823;&#30340;&#20154;&#31867;&#35780;&#20272;&#20013;&#28145;&#20837;&#35780;&#20272;GPT-4&#22312;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#35780;&#20272;&#20102;GPT-4&#22312;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#34920;&#29616;&#65292;&#25351;&#20986;&#29616;&#26377;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#20173;&#26377;&#24453;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#31616;&#21270;&#26159;&#19968;&#31181;&#37325;&#20889;&#21477;&#23376;&#20197;&#20415;&#26356;&#26131;&#38405;&#35835;&#21644;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#24110;&#21161;&#26377;&#21508;&#31181;&#38405;&#35835;&#38590;&#39064;&#30340;&#20154;&#26469;&#35828;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#38543;&#30528;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#34920;&#29616;&#21464;&#24471;&#36843;&#22312;&#30473;&#30571;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26469;&#35780;&#20272;LLMs&#30340;&#31616;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#23545;LLMs&#22312;&#31616;&#21270;&#35780;&#20272;&#20013;&#30340;&#36866;&#29992;&#24615;&#20173;&#28982;&#23384;&#22312;&#30097;&#38382;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#33258;&#21160;&#25351;&#26631;&#22312;LLMs&#30340;&#31616;&#21270;&#35780;&#20272;&#20013;&#30340;&#36866;&#29992;&#24615;&#20173;&#19981;&#30830;&#23450;&#12290;&#20854;&#27425;&#65292;&#24403;&#21069;&#22312;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#36890;&#24120;&#38519;&#20837;&#20004;&#20010;&#26497;&#31471;&#65306;&#35201;&#20040;&#36807;&#20110;&#32932;&#27973;&#65292;&#26080;&#27861;&#28165;&#26224;&#29702;&#35299;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#35201;&#20040;&#36807;&#20110;&#35814;&#32454;&#65292;&#20351;&#27880;&#37322;&#36807;&#31243;&#22797;&#26434;&#19988;&#23481;&#26131;&#20986;&#29616;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04963v1 Announce Type: cross  Abstract: Sentence simplification, which rewrites a sentence to be easier to read and understand, is a promising technique to help people with various reading difficulties. With the rise of advanced large language models (LLMs), evaluating their performance in sentence simplification has become imperative. Recent studies have used both automatic metrics and human evaluations to assess the simplification abilities of LLMs. However, the suitability of existing evaluation methodologies for LLMs remains in question. First, the suitability of current automatic metrics on LLMs' simplification evaluation is still uncertain. Second, current human evaluation approaches in sentence simplification often fall into two extremes: they are either too superficial, failing to offer a clear understanding of the models' performance, or overly detailed, making the annotation process complex and prone to inconsistency, which in turn affects the evaluation's reliabil
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#31995;&#32479;&#30340;&#25191;&#34892;&#38548;&#31163;&#26550;&#26500;SecGPT&#65292;&#26088;&#22312;&#35299;&#20915;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#25152;&#24341;&#21457;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.04960</link><description>&lt;p&gt;
SecGPT&#65306;&#19968;&#31181;&#38754;&#21521;&#22522;&#20110;LLM&#31995;&#32479;&#30340;&#25191;&#34892;&#38548;&#31163;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SecGPT: An Execution Isolation Architecture for LLM-Based Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04960
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#31995;&#32479;&#30340;&#25191;&#34892;&#38548;&#31163;&#26550;&#26500;SecGPT&#65292;&#26088;&#22312;&#35299;&#20915;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#25152;&#24341;&#21457;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25193;&#23637;&#20026;&#31995;&#32479;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#24320;&#22987;&#25903;&#25345;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#12290;&#36825;&#20123;LLM&#24212;&#29992;&#31243;&#24207;&#21033;&#29992;LLMs&#30340;&#20107;&#23454;&#19978;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#33258;&#21160;&#25191;&#34892;&#33539;&#24335;&#65306;&#21363;&#65292;&#24212;&#29992;&#31243;&#24207;&#21450;&#20854;&#20132;&#20114;&#26159;&#29992;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#30340;&#65292;&#25552;&#20379;&#23545;&#29992;&#25143;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#24182;&#34987;&#20801;&#35768;&#33258;&#30001;&#22320;&#30456;&#20114;&#20132;&#20114;&#20197;&#21450;&#19982;&#31995;&#32479;&#20114;&#21160;&#12290;&#36825;&#20123;LLM&#24212;&#29992;&#31243;&#24207;&#29983;&#24577;&#31995;&#32479;&#31867;&#20284;&#20110;&#26089;&#26399;&#35745;&#31639;&#24179;&#21488;&#30340;&#35774;&#32622;&#65292;&#22312;&#37027;&#37324;&#24212;&#29992;&#31243;&#24207;&#21644;&#31995;&#32479;&#20043;&#38388;&#32570;&#20047;&#36275;&#22815;&#30340;&#38548;&#31163;&#12290;&#30001;&#20110;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#21487;&#33021;&#19981;&#21487;&#20449;&#65292;&#24182;&#19988;&#21463;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#30340;&#19981;&#31934;&#30830;&#24615;&#21152;&#21095;&#65292;&#24403;&#21069;&#30340;&#35774;&#35745;&#20250;&#20026;&#29992;&#25143;&#24102;&#26469;&#23433;&#20840;&#21644;&#38544;&#31169;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SecGPT&#65292;&#19968;&#31181;&#38754;&#21521;LLM&#31995;&#32479;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#32531;&#35299;&#30001;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#24341;&#36215;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;SecGPT&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#38548;&#31163;&#24212;&#29992;&#31243;&#24207;&#30340;&#25191;&#34892;&#21644;&#26356;&#22810;&#30340;&#39044;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04960v1 Announce Type: cross  Abstract: Large language models (LLMs) extended as systems, such as ChatGPT, have begun supporting third-party applications. These LLM apps leverage the de facto natural language-based automated execution paradigm of LLMs: that is, apps and their interactions are defined in natural language, provided access to user data, and allowed to freely interact with each other and the system. These LLM app ecosystems resemble the settings of earlier computing platforms, where there was insufficient isolation between apps and the system. Because third-party apps may not be trustworthy, and exacerbated by the imprecision of the natural language interfaces, the current designs pose security and privacy risks for users. In this paper, we propose SecGPT, an architecture for LLM-based systems that aims to mitigate the security and privacy issues that arise with the execution of third-party apps. SecGPT's key idea is to isolate the execution of apps and more pre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04945</link><description>&lt;p&gt;
&#20026;&#25253;&#21578;&#29983;&#25104;&#35843;&#20248;&#24515;&#30005;&#22270;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram Instruction Tuning for Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04945
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20316;&#20026;&#24515;&#33039;&#30149;&#24773;&#30417;&#27979;&#30340;&#20027;&#35201;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#65292;&#23545;&#20110;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;ECG&#25968;&#25454;&#23545;&#24515;&#33039;&#30149;&#24773;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#24573;&#30053;&#20102;ECG&#25253;&#21578;&#29983;&#25104;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#65292;&#32780;&#19988;&#38656;&#35201;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;ECG&#25253;&#21578;&#29983;&#25104;&#24182;&#30830;&#20445;&#20854;&#22810;&#21151;&#33021;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;\textit{&#39318;&#27425;}&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#26469;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;MEIT&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#21508;&#31181;LLM&#39592;&#24178;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#23545;&#40784;&#20102;ECG&#20449;&#21495;&#21644;&#25253;&#21578;&#30340;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35780;&#20272;MEIT&#19982;&#20061;&#20010;&#24320;&#28304;LLMs&#65292;&#20351;&#29992;&#20102;&#36229;&#36807;80&#19975;&#20010;ECG&#25253;&#21578;&#12290;MEIT&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04945v1 Announce Type: new  Abstract: Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool for cardiac conditions monitoring, are crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is not only time-consuming but also requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the \textit{first} attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open source LLMs, using more than 800,000 ECG reports. MEIT's results underscore the superior p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#37325;&#35201;&#24615;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#22686;&#24378;&#21327;&#20316;&#26234;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#12289;&#25913;&#21892;AI&#27169;&#22411;&#12289;&#26377;&#25928;&#22242;&#38431;&#21512;&#20316;&#12289;&#36947;&#24503;&#32771;&#34385;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#26041;&#38754;&#30340;&#28508;&#22312;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04931</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#21512;&#20316;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Human-AI Teaming with Large Pre-Trained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#37325;&#35201;&#24615;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#22686;&#24378;&#21327;&#20316;&#26234;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#12289;&#25913;&#21892;AI&#27169;&#22411;&#12289;&#26377;&#25928;&#22242;&#38431;&#21512;&#20316;&#12289;&#36947;&#24503;&#32771;&#34385;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#26041;&#38754;&#30340;&#28508;&#22312;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36805;&#36895;&#21457;&#23637;&#30340;&#26223;&#35266;&#20013;&#65292;&#20154;&#31867;&#26234;&#33021;&#21644;AI&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#65288;HAI&#65289;&#21512;&#20316;&#65292;&#24050;&#25104;&#20026;&#25512;&#36827;&#38382;&#39064;&#35299;&#20915;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#22522;&#30707;&#12290;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPtM&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#25913;&#21464;&#20102;&#36825;&#19968;&#26223;&#35266;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#26469;&#29702;&#35299;&#21644;&#39044;&#27979;&#22797;&#26434;&#27169;&#24335;&#65292;&#20026;&#20154;&#31867;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;LPtMs&#19982;HAI&#30340;&#20851;&#38190;&#25972;&#21512;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#22686;&#24378;&#21327;&#20316;&#26234;&#33021;&#12290;&#37325;&#28857;&#25506;&#35752;&#20102;LPtMs&#22312;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#26041;&#38754;&#30340;&#21327;&#21516;&#28508;&#21147;&#65292;&#35752;&#35770;&#20102;&#36825;&#31181;&#21327;&#20316;&#23545;AI&#27169;&#22411;&#25913;&#36827;&#12289;&#26377;&#25928;&#30340;&#22242;&#38431;&#21512;&#20316;&#12289;&#36947;&#24503;&#32771;&#34385;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#24433;&#21709;&#12290;&#36890;&#36807;&#36825;&#19968;&#25506;&#32034;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;LPtM&#22686;&#24378;HAI&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04931v1 Announce Type: new  Abstract: In the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the synergistic potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ConstitutionalExperts&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#23466;&#27861;&#21407;&#21017;&#26500;&#24314;&#25552;&#31034;&#65292;&#37319;&#29992;&#36880;&#27493;&#25913;&#36827;&#25552;&#31034;&#21644;MoE&#26550;&#26500;&#65292;&#23637;&#29616;&#20986;&#22312;&#19981;&#21516;&#35821;&#20041;&#21306;&#22495;&#23398;&#20064;&#29420;&#29305;&#25552;&#31034;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.04894</link><description>&lt;p&gt;
ConstitutionalExperts: &#35757;&#32451;&#22522;&#20110;&#21407;&#21017;&#30340;&#25552;&#31034;&#28151;&#21512;&#20307;
&lt;/p&gt;
&lt;p&gt;
ConstitutionalExperts: Training a Mixture of Principle-based Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04894
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ConstitutionalExperts&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#23466;&#27861;&#21407;&#21017;&#26500;&#24314;&#25552;&#31034;&#65292;&#37319;&#29992;&#36880;&#27493;&#25913;&#36827;&#25552;&#31034;&#21644;MoE&#26550;&#26500;&#65292;&#23637;&#29616;&#20986;&#22312;&#19981;&#21516;&#35821;&#20041;&#21306;&#22495;&#23398;&#20064;&#29420;&#29305;&#25552;&#31034;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20889;&#20316;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#19988;&#32321;&#29712;&#30340;&#36807;&#31243;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ConstitutionalExperts&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#30001;&#23466;&#27861;&#21407;&#21017;&#65288;&#21363;&#35268;&#21017;&#65289;&#32452;&#25104;&#30340;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#32473;&#23450;&#19968;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290; &#19982;&#20197;&#24448;&#20248;&#21270;&#25552;&#31034;&#20316;&#20026;&#21333;&#20010;&#23454;&#20307;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20998;&#21035;&#32534;&#36753;&#21508;&#20010;&#21407;&#21017;&#36880;&#27493;&#25913;&#36827;&#25552;&#31034;&#12290; &#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#20026;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#21516;&#35821;&#20041;&#21306;&#22495;&#23398;&#20064;&#21807;&#19968;&#30340;&#25552;&#31034;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26550;&#26500;&#26469;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290; &#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290; &#25105;&#20204;&#36824;&#35843;&#26597;&#20102;MoE&#26159;&#21542;&#25913;&#21892;&#36825;&#20123;&#20854;&#20182;&#25216;&#26415;&#12290; &#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ConstitutionalExperts&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#25552;&#31034;&#20248;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04894v1 Announce Type: cross  Abstract: Large language models (LLMs) are highly capable at a variety of tasks given the right prompt, but writing one is still a difficult and tedious process. In this work, we introduce ConstitutionalExperts, a method for learning a prompt consisting of constitutional principles (i.e. rules), given a training dataset. Unlike prior methods that optimize the prompt as a single entity, our method incrementally improves the prompt by surgically editing individual principles. We also show that we can improve overall performance by learning unique prompts for different semantic regions of the training data and using a mixture-of-experts (MoE) architecture to route inputs at inference time. We compare our method to other state of the art prompt-optimization techniques across six benchmark datasets. We also investigate whether MoE improves these other techniques. Our results suggest that ConstitutionalExperts outperforms other prompt optimization tec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23569;&#26679;&#26412;&#25512;&#21160;&#25512;&#29702;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;LLMs&#29992;&#20110;&#24320;&#25918;&#24335;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#65292;&#36890;&#36807;&#20462;&#25913;MedQA-USMLE&#25968;&#25454;&#38598;&#24182;&#37319;&#29992;&#22870;&#21169;&#35757;&#32451;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#21307;&#30103;&#22330;&#26223;&#20013;&#27491;&#30830;&#21709;&#24212;&#20020;&#24202;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.04890</link><description>&lt;p&gt;
&#22522;&#20110;&#23569;&#26679;&#26412;&#25512;&#21160;&#25512;&#29702;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;LLMs&#29992;&#20110;&#24320;&#25918;&#24335;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23569;&#26679;&#26412;&#25512;&#21160;&#25512;&#29702;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;LLMs&#29992;&#20110;&#24320;&#25918;&#24335;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#65292;&#36890;&#36807;&#20462;&#25913;MedQA-USMLE&#25968;&#25454;&#38598;&#24182;&#37319;&#29992;&#22870;&#21169;&#35757;&#32451;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#21307;&#30103;&#22330;&#26223;&#20013;&#27491;&#30830;&#21709;&#24212;&#20020;&#24202;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#36716;&#21464;&#21307;&#30103;&#20445;&#20581;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#35832;&#22914;&#20020;&#24202;&#25991;&#26723;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20915;&#31574;&#25903;&#25345;&#31561;&#20219;&#21153;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#24050;&#32463;&#25104;&#20026;&#22312;&#21307;&#30103;&#22330;&#26223;&#20013;&#20351;&#29992;LLMs&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20363;&#22914;&#24739;&#32773;&#20020;&#24202;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MedQA-USMLE&#25968;&#25454;&#38598;&#30340;&#20462;&#25913;&#29256;&#26412;&#65292;&#30446;&#30340;&#26159;&#27169;&#25311;&#30495;&#23454;&#20020;&#24202;&#22330;&#26223;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#20027;&#35266;&#21709;&#24212;&#29983;&#25104;&#30340;Chain of Thought&#65288;CoT&#65289;&#25512;&#29702;&#65292;&#29992;&#20110;&#20462;&#25913;&#21518;&#30340;MedQA-USMLE&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;LM&#39537;&#21160;&#21069;&#21521;&#25512;&#29702;&#26469;&#33719;&#24471;&#27491;&#30830;&#30340;&#21307;&#23398;&#38382;&#39064;&#31572;&#26696;&#12290;&#32771;&#34385;&#21040;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#21709;&#24212;&#39564;&#35777;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#22870;&#21169;&#35757;&#32451;&#26426;&#21046;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#36824;&#20026;&#29305;&#23450;&#30340;&#20020;&#24202;&#38382;&#39064;&#22238;&#24212;&#25552;&#20379;&#20102;&#36866;&#24403;&#30340;&#39564;&#35777;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04890v1 Announce Type: new  Abstract: Large Language models (LLMs) have demonstrated significant potential in transforming healthcare by automating tasks such as clinical documentation, information retrieval, and decision support. In this aspect, carefully engineered prompts have emerged as a powerful tool for using LLMs for medical scenarios, e.g., patient clinical scenarios. In this paper, we propose a modified version of the MedQA-USMLE dataset, which is subjective, to mimic real-life clinical scenarios. We explore the Chain of Thought (CoT) reasoning based on subjective response generation for the modified MedQA-USMLE dataset with appropriate LM-driven forward reasoning for correct responses to the medical questions. Keeping in mind the importance of response verification in the medical setting, we utilize a reward training mechanism whereby the language model also provides an appropriate verified response for a particular response to a clinical question. In this regard,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#28151;&#21512;&#30721;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#26816;&#27979;&#12289;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#21644;&#34920;&#36798;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;</title><link>https://arxiv.org/abs/2403.04872</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#30721;&#35843;&#26597;&#34920;&#26126;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#28151;&#21512;&#30721;&#25991;&#26412;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Code-Mixed Probes Show How Pre-Trained Models Generalise On Code-Switched Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04872
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#28151;&#21512;&#30721;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#26816;&#27979;&#12289;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#21644;&#34920;&#36798;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#30721;&#26159;&#19968;&#31181;&#26222;&#36941;&#30340;&#35821;&#35328;&#29616;&#35937;&#65292;&#22810;&#35821;&#31181;&#20010;&#20307;&#21487;&#20197;&#26080;&#32541;&#22320;&#22312;&#35821;&#35328;&#20043;&#38388;&#20132;&#26367;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#28151;&#21512;&#30721;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#27169;&#22411;&#26816;&#27979;&#28151;&#21512;&#30721;&#25991;&#26412;&#30340;&#33021;&#21147;&#12289;&#27169;&#22411;&#21033;&#29992;&#30340;&#32467;&#26500;&#20449;&#24687;&#21464;&#21270;&#20197;&#25429;&#25417;&#28151;&#21512;&#30721;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#28151;&#21512;&#30721;&#25991;&#26412;&#20013;&#35821;&#20041;&#20449;&#24687;&#34920;&#36798;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#20102;&#33258;&#28982;&#24418;&#24577;&#30340;&#28151;&#21512;&#30721;&#25991;&#26412;&#19982;&#28304;&#35821;&#35328;&#30340;&#24179;&#34892;&#32763;&#35793;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#21270;&#21644;&#25511;&#21046;&#24615;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#28151;&#21512;&#30721;&#25991;&#26412;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04872v1 Announce Type: new  Abstract: Code-switching is a prevalent linguistic phenomenon in which multilingual individuals seamlessly alternate between languages. Despite its widespread use online and recent research trends in this area, research in code-switching presents unique challenges, primarily stemming from the scarcity of labelled data and available resources. In this study we investigate how pre-trained Language Models handle code-switched text in three dimensions: a) the ability of PLMs to detect code-switched text, b) variations in the structural information that PLMs utilise to capture code-switched text, and c) the consistency of semantic information representation in code-switched text. To conduct a systematic and controlled evaluation of the language models in question, we create a novel dataset of well-formed naturalistic code-switched text along with parallel translations into the source languages. Our findings reveal that pre-trained language models are e
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#19978;&#19979;&#25991;&#38382;&#39064;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#24180;&#36731;&#25104;&#24180;&#22899;&#24615;&#29992;&#25143;&#21463;&#21040;&#20559;&#29233;</title><link>https://arxiv.org/abs/2403.04858</link><description>&lt;p&gt;
&#35780;&#20272;&#19978;&#19979;&#25991;&#30456;&#20851;&#20581;&#24247;&#38382;&#39064;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Evaluating Biases in Context-Dependent Health Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04858
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#19978;&#19979;&#25991;&#38382;&#39064;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#24180;&#36731;&#25104;&#24180;&#22899;&#24615;&#29992;&#25143;&#21463;&#21040;&#20559;&#29233;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32842;&#22825;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26426;&#20250;&#36171;&#20104;&#37027;&#20123;&#32570;&#20047;&#39640;&#36136;&#37327;&#21307;&#30103;&#20445;&#20581;&#30340;&#20010;&#20154;&#36890;&#36807;&#21508;&#31181;&#20027;&#39064;&#25509;&#25910;&#20010;&#24615;&#21270;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#21487;&#33021;&#25552;&#20986;&#19981;&#20805;&#20998;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#27169;&#22411;&#25165;&#33021;&#27491;&#30830;&#22238;&#31572;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#26159;&#22914;&#20309;&#36890;&#36807;&#36825;&#20123;&#19982;&#20581;&#24247;&#39046;&#22495;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#38382;&#39064;&#23637;&#29616;&#20986;&#26469;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#20301;&#32622;&#23646;&#24615;&#30340;&#24615;&#20581;&#24247;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#20154;&#21475;&#32479;&#35745;&#32972;&#26223;&#19978;&#19979;&#25991;&#30340;&#27169;&#22411;&#36755;&#20986;&#65292;&#20197;&#30830;&#23450;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#38382;&#39064;&#20013;&#30340;&#32676;&#20307;&#23545;&#40784;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#20986;&#36825;&#20123;&#23646;&#24615;&#20013;&#30340;&#20559;&#35265;&#65292;&#20854;&#20013;&#24180;&#36731;&#25104;&#24180;&#22899;&#24615;&#29992;&#25143;&#21463;&#21040;&#20559;&#29233;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04858v1 Announce Type: new  Abstract: Chat-based large language models have the opportunity to empower individuals lacking high-quality healthcare access to receive personalized information across a variety of topics. However, users may ask underspecified questions that require additional context for a model to correctly answer. We study how large language model biases are exhibited through these contextual questions in the healthcare domain. To accomplish this, we curate a dataset of sexual and reproductive healthcare questions that are dependent on age, sex, and location attributes. We compare models' outputs with and without demographic context to determine group alignment among our contextual questions. Our experiments reveal biases in each of these attributes, where young adult female users are favored.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;BERT&#23884;&#20837;&#21644;HDBSCAN&#32858;&#31867;&#65292;&#21487;&#20197;&#20174;&#21322;&#32467;&#26500;&#21270;&#38754;&#35848;&#25991;&#26412;&#20013;&#24555;&#36895;&#25552;&#21462;&#20449;&#24687;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#20415;&#25463;&#30340;&#24037;&#20855;&#26469;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#20027;&#39064;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.04819</link><description>&lt;p&gt;
&#20174;&#21322;&#32467;&#26500;&#21270;&#38754;&#35848;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Automating the Information Extraction from Semi-Structured Interview Transcripts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;BERT&#23884;&#20837;&#21644;HDBSCAN&#32858;&#31867;&#65292;&#21487;&#20197;&#20174;&#21322;&#32467;&#26500;&#21270;&#38754;&#35848;&#25991;&#26412;&#20013;&#24555;&#36895;&#25552;&#21462;&#20449;&#24687;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#20415;&#25463;&#30340;&#24037;&#20855;&#26469;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#20027;&#39064;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21457;&#21644;&#24212;&#29992;&#19968;&#31181;&#33258;&#21160;&#31995;&#32479;&#65292;&#26088;&#22312;&#20174;&#21322;&#32467;&#26500;&#21270;&#38754;&#35848;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#23450;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#22914;&#32534;&#30721;&#65292;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#26412;&#36136;&#65292;&#23545;&#21487;&#20197;&#20419;&#36827;&#20998;&#26512;&#36807;&#31243;&#30340;&#24037;&#20855;&#23384;&#22312;&#30528;&#37325;&#22823;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21508;&#31181;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#36866;&#29992;&#20110;&#20998;&#26512;&#38754;&#35848;&#25991;&#26412;&#30340;&#26368;&#20339;&#27169;&#22411;&#26159;BERT&#23884;&#20837;&#21644;HDBSCAN&#32858;&#31867;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#36719;&#20214;&#21407;&#22411;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#65292;&#21253;&#25324;&#37027;&#20123;&#27809;&#26377;&#32534;&#31243;&#25216;&#33021;&#30340;&#20154;&#65292;&#33021;&#22815;&#39640;&#25928;&#22788;&#29702;&#21644;&#21487;&#35270;&#21270;&#38754;&#35848;&#25968;&#25454;&#30340;&#20027;&#39064;&#32467;&#26500;&#12290;&#35813;&#24037;&#20855;&#19981;&#20165;&#20419;&#36827;&#20102;&#23450;&#24615;&#20998;&#26512;&#30340;&#21021;&#26399;&#38454;&#27573;&#65292;&#36824;&#20026;&#25581;&#31034;&#30340;&#20027;&#39064;&#20043;&#38388;&#30340;&#30456;&#20114;&#32852;&#31995;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23450;&#24615;&#20998;&#26512;&#30340;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04819v1 Announce Type: new  Abstract: This paper explores the development and application of an automated system designed to extract information from semi-structured interview transcripts. Given the labor-intensive nature of traditional qualitative analysis methods, such as coding, there exists a significant demand for tools that can facilitate the analysis process. Our research investigates various topic modeling techniques and concludes that the best model for analyzing interview texts is a combination of BERT embeddings and HDBSCAN clustering. We present a user-friendly software prototype that enables researchers, including those without programming skills, to efficiently process and visualize the thematic structure of interview data. This tool not only facilitates the initial stages of qualitative analysis but also offers insights into the interconnectedness of topics revealed, thereby enhancing the depth of qualitative analysis.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.04814</link><description>&lt;p&gt;
&#22312;&#21477;&#27861;&#24863;&#30693;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Syntax-Aware Fill-In-the-Middle&#65288;SAFIM&#65289;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#22635;&#31354;&#65288;FIM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#22522;&#20934;&#20391;&#37325;&#20110;&#31243;&#24207;&#32467;&#26500;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#65292;&#22914;&#20195;&#30721;&#22359;&#21644;&#26465;&#20214;&#34920;&#36798;&#24335;&#65292;&#24182;&#21253;&#25324;&#26469;&#33258;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;17,720&#20010;&#31034;&#20363;&#65292;&#26469;&#28304;&#20110;2022&#24180;4&#26376;&#20043;&#21518;&#30340;&#26368;&#26032;&#20195;&#30721;&#25552;&#20132;&#65292;&#20197;&#26368;&#23567;&#21270;&#25968;&#25454;&#27745;&#26579;&#12290; SAFIM&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#21644;&#26032;&#39062;&#30340;&#21477;&#27861;&#24863;&#30693;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#22312;LLMs&#20043;&#38388;&#36827;&#34892;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;15&#20010;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#21319;&#20102;FIM&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#36824;&#25913;&#36827;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#65288;L2R&#65289;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#65292;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#36136;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#22823;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;SAFIM&#20026;&#26410;&#26469;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04814v1 Announce Type: cross  Abstract: We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#37327;&#21270;&#20102;&#27969;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#30340;&#25968;&#25454;&#27745;&#26579;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#19982;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#37325;&#21472;&#65292;&#24182;&#23637;&#31034;&#27169;&#22411;&#30340;&#26174;&#33879;&#24615;&#33021;&#37325;&#21472;&#12290;</title><link>https://arxiv.org/abs/2403.04811</link><description>&lt;p&gt;
&#37327;&#21270;&#27745;&#26579;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04811
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#37327;&#21270;&#20102;&#27969;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#30340;&#25968;&#25454;&#27745;&#26579;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#19982;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#37325;&#21472;&#65292;&#24182;&#23637;&#31034;&#27169;&#22411;&#30340;&#26174;&#33879;&#24615;&#33021;&#37325;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#20154;&#20204;&#23545;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#30340;&#28508;&#22312;&#27745;&#26579;&#26085;&#30410;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#27844;&#28431;&#21040;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#20013;&#12290;&#26412;&#25991;&#23545;&#27969;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#20934;&#30830;&#37327;&#21270;&#20102;&#23427;&#20204;&#19982;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#37325;&#21472;&#65292;&#36890;&#36807;&#34920;&#38754;&#32423;&#21644;&#35821;&#20041;&#32423;&#21305;&#37197;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27969;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19982;&#20844;&#24320;&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#38388;&#23384;&#22312;&#37325;&#21472;&#65292;&#24182;&#19988;&#27169;&#22411;&#34920;&#29616;&#26174;&#30528;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04811v1 Announce Type: cross  Abstract: While large language models have achieved remarkable performance on various code generation benchmarks, there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data. While recent work has investigated contamination in natural language generation and understanding tasks, there has been less extensive research into how data contamination impacts the evaluation of code generation, which is critical for understanding the robustness and reliability of LLMs in programming contexts. In this work, we perform a comprehensive study of data contamination of popular code generation benchmarks, and precisely quantify their overlap with pretraining corpus through both surface-level and semantic-level matching. In our experiments, we show that there are substantial overlap between popular code generation benchmarks and open training corpus, and models perform signifi
&lt;/p&gt;</description></item><item><title>WaterMax&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#25171;&#30772;&#20102;&#27700;&#21360;&#25216;&#26415;&#20013;&#36136;&#37327;&#21644;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#20256;&#32479;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.04808</link><description>&lt;p&gt;
WaterMax: &#25171;&#30772;LLM&#27700;&#21360;&#21487;&#26816;&#27979;&#24615;-&#31283;&#20581;&#24615;-&#36136;&#37327;&#30340;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04808
&lt;/p&gt;
&lt;p&gt;
WaterMax&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#25171;&#30772;&#20102;&#27700;&#21360;&#25216;&#26415;&#20013;&#36136;&#37327;&#21644;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#20256;&#32479;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#26159;&#38459;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#24694;&#24847;&#20351;&#29992;&#30340;&#25216;&#26415;&#25163;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;WaterMax&#30340;&#26032;&#39062;&#27700;&#21360;&#26041;&#26696;&#65292;&#20855;&#26377;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#20854;&#26032;&#35774;&#35745;&#19981;&#20250;&#23545;LLM&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#65288;&#19981;&#35843;&#25972;&#26435;&#37325;&#12289;&#23545;&#25968;&#12289;&#28201;&#24230;&#25110;&#37319;&#26679;&#25216;&#26415;&#65289;&#12290;WaterMax&#24179;&#34913;&#20102;&#31283;&#20581;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#19982;&#25991;&#29486;&#20013;&#30340;&#27700;&#21360;&#25216;&#26415;&#30456;&#21453;&#65292;&#20174;&#26681;&#26412;&#19978;&#24341;&#21457;&#20102;&#36136;&#37327;&#21644;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20854;&#24615;&#33021;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#35777;&#26126;&#24182;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;&#22312;&#26368;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19979;&#65292;&#23427;&#32988;&#36807;&#25152;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04808v1 Announce Type: cross  Abstract: Watermarking is a technical means to dissuade malfeasant usage of Large Language Models. This paper proposes a novel watermarking scheme, so-called WaterMax, that enjoys high detectability while sustaining the quality of the generated text of the original LLM. Its new design leaves the LLM untouched (no modification of the weights, logits, temperature, or sampling technique). WaterMax balances robustness and complexity contrary to the watermarking techniques of the literature inherently provoking a trade-off between quality and robustness. Its performance is both theoretically proven and experimentally validated. It outperforms all the SotA techniques under the most complete benchmark suite.
&lt;/p&gt;</description></item><item><title>AttentionStitch&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#21452;&#27880;&#24847;&#21147;&#22359;&#32593;&#32476;&#65292;&#24182;&#23558;&#32534;&#36753;&#25991;&#26412;&#30340;mel&#39057;&#35889;&#22270;&#19982;&#21512;&#25104;&#30340;mel&#39057;&#35889;&#22270;&#33258;&#21160;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#32534;&#36753;&#30340;&#26080;&#32541;&#25972;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.04804</link><description>&lt;p&gt;
AttentionStitch&#65306;&#20851;&#27880;&#22914;&#20309;&#35299;&#20915;&#35821;&#38899;&#32534;&#36753;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
AttentionStitch: How Attention Solves the Speech Editing Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04804
&lt;/p&gt;
&lt;p&gt;
AttentionStitch&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#21452;&#27880;&#24847;&#21147;&#22359;&#32593;&#32476;&#65292;&#24182;&#23558;&#32534;&#36753;&#25991;&#26412;&#30340;mel&#39057;&#35889;&#22270;&#19982;&#21512;&#25104;&#30340;mel&#39057;&#35889;&#22270;&#33258;&#21160;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#32534;&#36753;&#30340;&#26080;&#32541;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#38500;&#20102;&#35821;&#38899;&#29983;&#25104;&#20043;&#22806;&#65292;&#35821;&#38899;&#32534;&#36753;&#20063;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#38656;&#35201;&#23558;&#32534;&#36753;&#21518;&#30340;&#35821;&#38899;&#26080;&#32541;&#12289;&#19981;&#26131;&#23519;&#35273;&#22320;&#25972;&#21512;&#21040;&#21512;&#25104;&#35821;&#38899;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#38899;&#32534;&#36753;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#65292;&#22914;FastSpeech 2&#65292;&#24182;&#22312;&#20854;&#20043;&#19978;&#21152;&#20837;&#21452;&#27880;&#24847;&#21147;&#22359;&#32593;&#32476;&#65292;&#20197;&#33258;&#21160;&#23558;&#21512;&#25104;&#30340;mel&#39057;&#35889;&#22270;&#19982;&#32534;&#36753;&#25991;&#26412;&#30340;mel&#39057;&#35889;&#22270;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#27169;&#22411;&#31216;&#20026;AttentionStitch&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#27880;&#24847;&#21147;&#26469;&#23558;&#38899;&#39057;&#26679;&#26412;&#25340;&#25509;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#22312;&#21333;&#20010;&#21644;&#22810;&#20010;&#35828;&#35805;&#32773;&#25968;&#25454;&#38598;&#65288;LJSpeech&#21644;VCTK&#65289;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;AttentionStitch&#27169;&#22411;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04804v1 Announce Type: cross  Abstract: The generation of natural and high-quality speech from text is a challenging problem in the field of natural language processing. In addition to speech generation, speech editing is also a crucial task, which requires the seamless and unnoticeable integration of edited speech into synthesized speech. We propose a novel approach to speech editing by leveraging a pre-trained text-to-speech (TTS) model, such as FastSpeech 2, and incorporating a double attention block network on top of it to automatically merge the synthesized mel-spectrogram with the mel-spectrogram of the edited text. We refer to this model as AttentionStitch, as it harnesses attention to stitch audio samples together. We evaluate the proposed AttentionStitch model against state-of-the-art baselines on both single and multi-speaker datasets, namely LJSpeech and VCTK. We demonstrate its superior performance through an objective and a subjective evaluation test involving 1
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLM&#20195;&#29702;&#36827;&#34892;&#40657;&#30418;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#21463;&#23475;&#20195;&#29702;&#20013;&#26356;&#39640;&#32423;&#21035;&#30340;&#35760;&#24518;&#21270;&#65292;&#30456;&#27604;&#30452;&#25509;&#29992;&#35757;&#32451;&#25968;&#25454;&#25552;&#31034;&#30446;&#26631;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#33021;&#26356;&#22909;&#22320;&#37327;&#21270;LLMs&#30340;&#35760;&#24518;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.04801</link><description>&lt;p&gt;
Alpaca&#23545;&#25239;Vicuna&#65306;&#20351;&#29992;LLMs&#25581;&#31034;LLMs&#30340;&#35760;&#24518;&#21270;
&lt;/p&gt;
&lt;p&gt;
Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04801
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLM&#20195;&#29702;&#36827;&#34892;&#40657;&#30418;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#21463;&#23475;&#20195;&#29702;&#20013;&#26356;&#39640;&#32423;&#21035;&#30340;&#35760;&#24518;&#21270;&#65292;&#30456;&#27604;&#30452;&#25509;&#29992;&#35757;&#32451;&#25968;&#25454;&#25552;&#31034;&#30446;&#26631;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#33021;&#26356;&#22909;&#22320;&#37327;&#21270;LLMs&#30340;&#35760;&#24518;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#40657;&#30418;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25915;&#20987;&#32773;LLM&#20195;&#29702;&#26469;&#25581;&#31034;&#21463;&#23475;&#20195;&#29702;&#20013;&#26356;&#39640;&#32423;&#21035;&#30340;&#35760;&#24518;&#21270;&#65292;&#19982;&#30452;&#25509;&#29992;&#35757;&#32451;&#25968;&#25454;&#25552;&#31034;&#30446;&#26631;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#26159;&#37327;&#21270;LLMs&#35760;&#24518;&#21270;&#30340;&#20027;&#23548;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#36845;&#20195;&#30340;&#25298;&#32477;&#25277;&#26679;&#20248;&#21270;&#36807;&#31243;&#26469;&#25214;&#21040;&#22522;&#20110;&#25351;&#20196;&#30340;&#25552;&#31034;&#65292;&#20855;&#26377;&#20004;&#20010;&#20027;&#35201;&#29305;&#24449;&#65306;(1)&#19982;&#35757;&#32451;&#25968;&#25454;&#26368;&#23567;&#37325;&#21472;&#65292;&#20197;&#36991;&#20813;&#30452;&#25509;&#21521;&#27169;&#22411;&#21576;&#29616;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;(2)&#21463;&#23475;&#27169;&#22411;&#36755;&#20986;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#26368;&#22823;&#37325;&#21472;&#65292;&#26088;&#22312;&#35825;&#20351;&#21463;&#23475;&#32773;&#21520;&#20986;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#22522;&#20110;&#25351;&#20196;&#30340;&#25552;&#31034;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#35757;&#32451;&#25968;&#25454;&#37325;&#21472;&#31243;&#24230;&#27604;&#22522;&#32447;&#21069;&#32512;&#21518;&#32512;&#27979;&#37327;&#39640;&#20986;23.7&#65285;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;(1)&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#21487;&#20197;&#26292;&#38706;&#19982;&#20182;&#20204;&#30340;&#22522;&#26412;&#27169;&#22411;&#19968;&#26679;&#22810;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04801v1 Announce Type: new  Abstract: In this paper, we introduce a black-box prompt optimization method that uses an attacker LLM agent to uncover higher levels of memorization in a victim agent, compared to what is revealed by prompting the target model with the training data directly, which is the dominant approach of quantifying memorization in LLMs. We use an iterative rejection-sampling optimization process to find instruction-based prompts with two main characteristics: (1) minimal overlap with the training data to avoid presenting the solution directly to the model, and (2) maximal overlap between the victim model's output and the training data, aiming to induce the victim to spit out training data. We observe that our instruction-based prompts generate outputs with 23.7% higher overlap with training data compared to the baseline prefix-suffix measurements. Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;&#32422;&#40065;&#24052;&#35821;&#20013;&#21019;&#24314;&#21644;&#20998;&#21457;&#20154;&#24037;&#26234;&#33021;&#35270;&#39057;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04799</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#65306;&#20174;&#21019;&#36896;&#32422;&#40065;&#24052;&#35821;AI&#35270;&#39057;&#20013;&#33719;&#24471;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
AI Literacy in Low-Resource Languages:Insights from creating AI in Yoruba videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;&#32422;&#40065;&#24052;&#35821;&#20013;&#21019;&#24314;&#21644;&#20998;&#21457;&#20154;&#24037;&#26234;&#33021;&#35270;&#39057;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#22320;&#24212;&#23545;&#20154;&#24037;&#26234;&#33021;&#38761;&#21629;&#65292;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;&#23384;&#22312;&#20110;&#20027;&#23548;&#35821;&#35328;&#20013;&#30340;&#20869;&#23481;&#22312;&#20687;&#32422;&#40065;&#24052;&#35821;&#65288;&#26377;4100&#19975;&#27597;&#35821;&#20351;&#29992;&#32773;&#65289;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#36896;&#25104;&#20102;&#19968;&#23450;&#30340;&#24046;&#36317;&#12290;&#26412;&#26696;&#20363;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#22312;&#32422;&#40065;&#24052;&#35821;&#20013;&#21019;&#24314;&#21644;&#20998;&#21457;AI&#35270;&#39057;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#35813;&#39033;&#30446;&#24320;&#21457;&#20102;26&#20010;&#35270;&#39057;&#65292;&#28085;&#30422;&#22522;&#30784;&#12289;&#20013;&#32423;&#21644;&#39640;&#32423;&#20154;&#24037;&#26234;&#33021;&#27010;&#24565;&#65292;&#21033;&#29992;&#25925;&#20107;&#21465;&#36848;&#21644;&#31616;&#26131;&#35299;&#37322;&#12290;&#36825;&#20123;&#35270;&#39057;&#37319;&#29992;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#26041;&#27861;&#21046;&#20316;&#65292;&#24182;&#22312;YouTube&#12289;LinkedIn&#21644;Twitter&#19978;&#36827;&#34892;&#20998;&#21457;&#65292;&#20272;&#35745;&#35302;&#21450;&#20102;&#26469;&#33258;22&#20010;&#22269;&#23478;&#30340;&#20840;&#29699;&#35266;&#20247;&#12290;&#23545;YouTube&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35266;&#30475;&#27169;&#24335;&#30340;&#35265;&#35299;&#65292;&#20854;&#20013;25-44&#23681;&#24180;&#40836;&#32452;&#36129;&#29486;&#20102;&#26368;&#22810;&#30340;&#35266;&#30475;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36229;&#36807;&#19968;&#21322;&#30340;&#27969;&#37327;&#26469;&#28304;&#20110;&#22806;&#37096;&#26469;&#28304;&#65292;&#31361;&#26174;&#20102;&#36328;&#24179;&#21488;&#25512;&#24191;&#30340;&#28508;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#21019;&#24314;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#20869;&#23481;&#30340;&#21487;&#34892;&#24615;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04799v1 Announce Type: cross  Abstract: To effectively navigate the AI revolution, AI literacy is crucial. However, content predominantly exists in dominant languages, creating a gap for low-resource languages like Yoruba (41 million native speakers). This case study explores bridging this gap by creating and distributing AI videos in Yoruba.The project developed 26 videos covering foundational, intermediate, and advanced AI concepts, leveraging storytelling and accessible explanations. These videos were created using a cost-effective methodology and distributed across YouTube, LinkedIn, and Twitter, reaching an estimated global audience of 22 countries. Analysis of YouTube reveals insights into viewing patterns, with the 25-44 age group contributing the most views. Notably, over half of the traffic originated from external sources, highlighting the potential of cross-platform promotion.This study demonstrates the feasibility and impact of creating AI literacy content in low
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;SemEval-2024&#20219;&#21153;3&#24320;&#21457;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#20004;&#27493;&#26694;&#26550;&#35299;&#20915;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.04798</link><description>&lt;p&gt;
JMI&#22312;SemEval 2024&#20219;&#21153;3&#20013;&#30340;&#24212;&#29992;&#65306;&#20351;&#29992;GPT&#21644;instruction-tuned Llama&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#30340;&#20004;&#27493;&#27861;
&lt;/p&gt;
&lt;p&gt;
JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;SemEval-2024&#20219;&#21153;3&#24320;&#21457;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#20004;&#27493;&#26694;&#26550;&#35299;&#20915;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545;SemEval-2024&#20219;&#21153;3&#65306;&#8220;&#23545;&#35805;&#20013;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31454;&#36187;&#8221;&#24320;&#21457;&#30340;&#31995;&#32479;&#12290;&#26377;&#25928;&#25429;&#25417;&#20154;&#31867;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#38656;&#35201;&#25972;&#21512;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#31561;&#22810;&#31181;&#27169;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22810;&#26679;&#24615;&#27169;&#24577;&#30340;&#22797;&#26434;&#24615;&#32473;&#24320;&#21457;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31995;&#32479;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20004;&#27493;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#23454;&#29616;&#20013;&#37319;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#22312;&#26041;&#27861;1&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#21333;&#29420;&#30340;Llama 2&#27169;&#22411;&#36827;&#34892;&#24773;&#24863;&#21644;&#21407;&#22240;&#39044;&#27979;&#30340;instruction-tuning&#12290;&#22312;&#26041;&#27861;2&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4V&#36827;&#34892;&#20250;&#35805;&#32423;&#35270;&#39057;&#25551;&#36848;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;GPT 3.5&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#27880;&#37322;&#23545;&#35805;&#36827;&#34892;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#33719;&#24471;&#20102;&#31532;4&#21517;&#65292;&#31995;&#32479;&#28040;&#34701;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04798v1 Announce Type: new  Abstract: This paper presents our system development for SemEval-2024 Task 3: "The Competition of Multimodal Emotion Cause Analysis in Conversations". Effectively capturing emotions in human conversations requires integrating multiple modalities such as text, audio, and video. However, the complexities of these diverse modalities pose challenges for developing an efficient multimodal emotion cause analysis (ECA) system. Our proposed approach addresses these challenges by a two-step framework. We adopt two different approaches in our implementation. In Approach 1, we employ instruction-tuning with two separate Llama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4V for conversation-level video description and employ in-context learning with annotated conversation using GPT 3.5. Our system wins rank 4, and system ablation experiments demonstrate that our proposed solutions achieve significant performance gains. All the experime
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#65288;Ms-PoE&#65289;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#19978;&#19979;&#25991;&#20013;&#38388;&#30456;&#20851;&#20449;&#24687;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;LLMs&#38754;&#20020;&#30340;&#8220;&#20013;&#38388;&#20002;&#22833;&#8221;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.04797</link><description>&lt;p&gt;
&#22312;&#20013;&#38388;&#34987;&#21457;&#29616;: &#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36890;&#36807;&#21363;&#25554;&#21363;&#29992;&#20301;&#32622;&#32534;&#30721;&#26356;&#22909;&#22320;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04797
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#65288;Ms-PoE&#65289;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#19978;&#19979;&#25991;&#20013;&#38388;&#30456;&#20851;&#20449;&#24687;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;LLMs&#38754;&#20020;&#30340;&#8220;&#20013;&#38388;&#20002;&#22833;&#8221;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30340;&#8220;&#20013;&#38388;&#20002;&#22833;&#8221;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36827;&#23637;&#25104;&#21151;&#23454;&#29616;&#20102;LLMs&#23545;&#21253;&#21547;400&#19975;&#20196;&#29260;&#30340;&#31283;&#23450;&#35821;&#35328;&#24314;&#27169;&#65292;&#20294;&#22823;&#22810;&#25968;LLMs&#22312;&#35782;&#21035;&#20301;&#20110;&#19978;&#19979;&#25991;&#20013;&#38388;&#30340;&#30456;&#20851;&#20449;&#24687;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25345;&#32493;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#65288;Ms-PoE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;LLMs&#22788;&#29702;&#20301;&#20110;&#19978;&#19979;&#25991;&#20013;&#38388;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#24494;&#35843;&#25110;&#24341;&#20837;&#20219;&#20309;&#39069;&#22806;&#24320;&#38144;&#12290;Ms-PoE&#21033;&#29992;&#20301;&#32622;&#25351;&#25968;&#37325;&#26032;&#32553;&#25918;&#20197;&#20943;&#36731;RoPE&#24341;&#20837;&#30340;&#38271;&#26399;&#34928;&#20943;&#25928;&#24212;&#65292;&#21516;&#26102;&#31934;&#24515;&#20026;&#19981;&#21516;&#27880;&#24847;&#21147;&#22836;&#20998;&#37197;&#19981;&#21516;&#30340;&#32553;&#25918;&#27604;&#20197;&#20445;&#30041;&#39044;&#35757;&#32451;&#38454;&#27573;&#23398;&#21040;&#30340;&#22522;&#26412;&#30693;&#35782;&#65292;&#24418;&#25104;&#22810;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04797v1 Announce Type: new  Abstract: This paper aims to overcome the "lost-in-the-middle" challenge of large language models (LLMs). While recent advancements have successfully enabled LLMs to perform stable language modeling with up to 4 million tokens, the persistent difficulty faced by most LLMs in identifying relevant information situated in the middle of the context has not been adequately tackled. To address this problem, this paper introduces Multi-scale Positional Encoding (Ms-PoE) which is a simple yet effective plug-and-play approach to enhance the capacity of LLMs to handle the relevant information located in the middle of the context, without fine-tuning or introducing any additional overhead. Ms-PoE leverages the position indice rescaling to relieve the long-term decay effect introduced by RoPE, while meticulously assigning distinct scaling ratios to different attention heads to preserve essential knowledge learned during the pre-training step, forming a multi-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#28040;&#38450;&#24037;&#31243;&#20013;&#22788;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT&#34920;&#29616;&#36739;&#20248;&#65292;&#23637;&#31034;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#22312;&#28040;&#38450;&#24037;&#31243;&#23454;&#36341;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04795</link><description>&lt;p&gt;
&#28040;&#38450;&#24037;&#31243;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38024;&#23545;&#39046;&#22495;&#30693;&#35782;&#23545;&#25216;&#26415;&#38382;&#39064;&#30340;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Fire Engineering: An Examination of Technical Questions Against Domain Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#28040;&#38450;&#24037;&#31243;&#20013;&#22788;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT&#34920;&#29616;&#36739;&#20248;&#65292;&#23637;&#31034;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#22312;&#28040;&#38450;&#24037;&#31243;&#23454;&#36341;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27604;&#36739;&#20004;&#20010;&#26368;&#36817;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;OpenAI&#30340;ChatGPT&#21644;&#35895;&#27468;&#30340;Bard&#65292;&#22312;&#28779;&#28798;&#24037;&#31243;&#39046;&#22495;&#20013;&#35780;&#20272;&#23427;&#20204;&#22788;&#29702;&#19982;&#28040;&#38450;&#23433;&#20840;&#30456;&#20851;&#26597;&#35810;&#30340;&#22238;&#24212;&#30340;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#12290; &#21019;&#24314;&#24182;&#26816;&#26597;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#31867;&#22411;&#30340;&#28040;&#38450;&#24037;&#31243;&#38382;&#39064;&#21644;&#22330;&#26223;&#65292;&#20854;&#20013;&#21253;&#25324;&#32467;&#26500;&#28779;&#28798;&#35774;&#35745;&#12289;&#38450;&#28779;&#31574;&#30053;&#12289;&#30095;&#25955;&#12289;&#24314;&#31569;&#27861;&#35268;&#21512;&#35268;&#21644;&#28781;&#28779;&#31995;&#32479;&#31561;&#65288;&#20854;&#20013;&#19968;&#20123;&#31867;&#20284;&#20110;&#28040;&#38450;&#20445;&#25252;&#32771;&#35797;&#65288;FPE&#65289;&#20013;&#24120;&#35265;&#30340;&#24773;&#20917;&#65289;&#12290; &#32467;&#26524;&#26174;&#31034;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#24615;&#33021;&#19978;&#30340;&#19968;&#20123;&#20851;&#38190;&#24046;&#24322;&#65292;ChatGPT&#34920;&#29616;&#20986;&#30456;&#23545;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290; &#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#31361;&#20986;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#22312;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#30340;&#21516;&#26102;&#24443;&#24213;&#25913;&#38761;&#28040;&#38450;&#24037;&#31243;&#23454;&#36341;&#30340;&#28508;&#21147;&#65292;&#24182;&#27010;&#36848;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#26174;&#28982;&#65292;&#22312;&#25216;&#26415;&#25104;&#29087;&#21518;&#65292;&#36825;&#39033;&#25216;&#26415;&#23558;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04795v1 Announce Type: cross  Abstract: This communication presents preliminary findings from comparing two recent chatbots, OpenAI's ChatGPT and Google's Bard, in the context of fire engineering by evaluating their responses in handling fire safety related queries. A diverse range of fire engineering questions and scenarios were created and examined, including structural fire design, fire prevention strategies, evacuation, building code compliance, and fire suppression systems (some of which resemble those commonly present in the Fire Protection exam (FPE)). The results reveal some key differences in the performance of the chatbots, with ChatGPT demonstrating a relatively superior performance. Then, this communication highlights the potential for chatbot technology to revolutionize fire engineering practices by providing instant access to critical information while outlining areas for further improvement and research. Evidently, and when it matures, this technology will lik
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25361;&#25112;&#20102;&#20197;&#24448;&#30740;&#31350;&#20013;&#24314;&#31435;&#30340;&#39044;&#32763;&#35793;&#33539;&#24335;&#65292;&#24182;&#22312;108&#31181;&#35821;&#35328;&#20013;&#30340;94&#31181;&#35821;&#35328;&#20013;&#34920;&#26126;PaLM2-L&#22312;&#30452;&#25509;&#25512;&#26029;&#20013;&#20248;&#20110;&#39044;&#32763;&#35793;&#12290;</title><link>https://arxiv.org/abs/2403.04792</link><description>&lt;p&gt;
&#25171;&#30772;&#35821;&#35328;&#38556;&#30861;&#65306;&#22312;&#22810;&#35821;&#35328;LLM&#24212;&#29992;&#20013;&#65292;&#30452;&#25509;&#25512;&#26029;&#33021;&#21542;&#32988;&#36807;&#39044;&#32763;&#35793;&#65311;
&lt;/p&gt;
&lt;p&gt;
Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25361;&#25112;&#20102;&#20197;&#24448;&#30740;&#31350;&#20013;&#24314;&#31435;&#30340;&#39044;&#32763;&#35793;&#33539;&#24335;&#65292;&#24182;&#22312;108&#31181;&#35821;&#35328;&#20013;&#30340;94&#31181;&#35821;&#35328;&#20013;&#34920;&#26126;PaLM2-L&#22312;&#30452;&#25509;&#25512;&#26029;&#20013;&#20248;&#20110;&#39044;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20027;&#35201;&#20197;&#33521;&#25991;&#20026;&#20013;&#24515;&#30340;&#39044;&#35757;&#32451;&#20250;&#23548;&#33268;&#22266;&#26377;&#20559;&#35265;&#65292;&#22240;&#27492;&#24050;&#32463;&#26222;&#36941;&#37319;&#29992;&#39044;&#32763;&#35793;&#30340;&#20570;&#27861;&#65292;&#21363;&#22312;&#25512;&#26029;&#20043;&#21069;&#23558;&#38750;&#33521;&#25991;&#36755;&#20837;&#32763;&#35793;&#25104;&#33521;&#25991;&#65292;&#20174;&#32780;&#23548;&#33268;&#22797;&#26434;&#24615;&#21644;&#20449;&#24687;&#20002;&#22833;&#12290;&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;&#22312;PaLM2&#27169;&#22411;&#20013;&#30340;&#39044;&#32763;&#35793;&#38656;&#27714;&#65288;Anil&#31561;&#20154;&#65292;2023&#24180;&#65289;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#22810;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#22312;108&#31181;&#35821;&#35328;&#21644;6&#20010;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#21253;&#25324;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#20219;&#21153;&#65292;&#22312;&#27492;&#31867;&#20219;&#21153;&#20013;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#34987;&#25490;&#38500;&#22312;&#22806;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25361;&#25112;&#20102;&#20197;&#21069;&#30740;&#31350;&#20013;&#24314;&#31435;&#30340;&#39044;&#32763;&#35793;&#33539;&#24335;&#65292;&#31361;&#20986;&#20102;&#22312;PaLM2&#20013;&#30452;&#25509;&#25512;&#26029;&#30340;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PaLM2-L&#22312;108&#31181;&#35821;&#35328;&#20013;&#30340;94&#31181;&#20013;&#22987;&#32456;&#20248;&#20110;&#39044;&#32763;&#35793;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26356;&#39640;&#25928;&#30340;&#32763;&#35793;&#26041;&#27861;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04792v1 Announce Type: new  Abstract: Large language models hold significant promise in multilingual applications. However, inherent biases stemming from predominantly English-centric pre-training have led to the widespread practice of pre-translation, i.e., translating non-English inputs to English before inference, leading to complexity and information loss. This study re-evaluates the need for pre-translation in the context of PaLM2 models (Anil et al., 2023), which have been established as highly performant in multilingual tasks. We offer a comprehensive investigation across 108 languages and 6 diverse benchmarks, including open-end generative tasks, which were excluded from previous similar studies. Our findings challenge the pre-translation paradigm established in prior research, highlighting the advantages of direct inference in PaLM2. Specifically, PaLM2-L consistently outperforms pre-translation in 94 out of 108 languages. These findings pave the way for more effici
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#27604;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20174;&#22823;&#22411;&#33521;&#22269;&#27861;&#38498;&#21028;&#20915;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#25688;&#35201;&#35009;&#23450;&#26696;&#20363;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;F1&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.04791</link><description>&lt;p&gt;
LLM&#23545;&#25239;&#24459;&#24072;&#65306;&#22312;&#22823;&#22411;&#33521;&#22269;&#26696;&#20363;&#27861;&#24459;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#25688;&#35201;&#35009;&#23450;&#30340;&#23376;&#38598;
&lt;/p&gt;
&lt;p&gt;
LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case Law Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04791
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#27604;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20174;&#22823;&#22411;&#33521;&#22269;&#27861;&#38498;&#21028;&#20915;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#25688;&#35201;&#35009;&#23450;&#26696;&#20363;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36827;&#34892;&#27861;&#24459;&#39046;&#22495;&#30340;&#35745;&#31639;&#30740;&#31350;&#65292;&#39640;&#25928;&#22320;&#35782;&#21035;&#19982;&#29305;&#23450;&#27861;&#24459;&#38382;&#39064;&#30456;&#20851;&#30340;&#27861;&#38498;&#35009;&#20915;&#25968;&#25454;&#38598;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21162;&#21147;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#25991;&#29486;&#20013;&#20851;&#20110;&#22914;&#20309;&#20174;&#22823;&#37327;&#33521;&#22269;&#27861;&#38498;&#20915;&#23450;&#30340;&#25991;&#38598;&#20013;&#38548;&#31163;&#26696;&#20363;&#65288;&#22312;&#25105;&#20204;&#30340;&#26696;&#20363;&#20013;&#26159;&#25688;&#35201;&#35009;&#23450;&#65289;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#35745;&#31639;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;&#65306;&#65288;1&#65289;&#20256;&#32479;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#29983;&#25104;&#30340;&#20851;&#38190;&#23383;&#21644;&#36923;&#36753;&#36816;&#31639;&#31526;&#65292;&#20197;&#21450;&#65288;2&#65289;&#21019;&#26032;&#24615;&#22320;&#23558;Claude 2&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22522;&#20110;&#29305;&#23450;&#20869;&#23481;&#25552;&#31034;&#20998;&#31867;&#26696;&#20363;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#21253;&#21547;356,011&#20221;&#33521;&#22269;&#27861;&#38498;&#21028;&#20915;&#30340;&#21073;&#26725;&#27861;&#23398;&#25991;&#38598;&#65292;&#24182;&#30830;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21152;&#26435;F1&#24471;&#20998;&#20026;0.94&#65292;&#32780;&#20851;&#38190;&#23383;&#30340;&#24471;&#20998;&#20026;0.78&#12290;&#23613;&#31649;&#32463;&#36807;&#36845;&#20195;&#25913;&#36827;&#65292;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#25628;&#32034;&#36923;&#36753;&#26410;&#33021;&#25429;&#25417;&#27861;&#24459;&#35821;&#35328;&#20013;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04791v1 Announce Type: new  Abstract: To undertake computational research of the law, efficiently identifying datasets of court decisions that relate to a specific legal issue is a crucial yet challenging endeavour. This study addresses the gap in the literature working with large legal corpora about how to isolate cases, in our case summary judgments, from a large corpus of UK court decisions. We introduce a comparative analysis of two computational methods: (1) a traditional natural language processing-based approach leveraging expert-generated keywords and logical operators and (2) an innovative application of the Claude 2 large language model to classify cases based on content-specific prompts. We use the Cambridge Law Corpus of 356,011 UK court decisions and determine that the large language model achieves a weighted F1 score of 0.94 versus 0.78 for keywords. Despite iterative refinement, the search logic based on keywords fails to capture nuances in legal language. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20114;&#21160;&#33539;&#24335;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22806;&#37096;&#20114;&#21160;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#25345;&#32493;&#12289;&#23454;&#26102;&#30340;&#27169;&#22411;&#26356;&#26032;&#19982;&#20010;&#24615;&#21270;&#23450;&#21046;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04790</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22312;&#32447;&#35757;&#32451;&#65306;&#36793;&#32842;&#22825;&#36793;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Training of Large Language Models: Learn while chatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20114;&#21160;&#33539;&#24335;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22806;&#37096;&#20114;&#21160;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#25345;&#32493;&#12289;&#23454;&#26102;&#30340;&#27169;&#22411;&#26356;&#26032;&#19982;&#20010;&#24615;&#21270;&#23450;&#21046;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#65292;&#25552;&#20379;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#21151;&#33021;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#33539;&#24335;&#21463;&#21046;&#20110;&#28789;&#27963;&#24615;&#19981;&#36275;&#12289;&#23450;&#21046;&#21270;&#21463;&#38480;&#25110;&#32570;&#20047;&#25345;&#32493;&#24615;&#23398;&#20064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20114;&#21160;&#33539;&#24335;-&#8220;&#20351;&#29992;&#22806;&#37096;&#20114;&#21160;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#8221;&#65292;&#23558;&#25345;&#32493;&#12289;&#23454;&#26102;&#30340;&#27169;&#22411;&#26356;&#26032;&#19982;&#36890;&#36807;&#22806;&#37096;&#20114;&#21160;&#65288;&#22914;AI&#65289;&#36827;&#34892;&#20010;&#24615;&#21270;&#23450;&#21046;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04790v1 Announce Type: cross  Abstract: Large Language Models(LLMs) have dramatically revolutionized the field of Natural Language Processing(NLP), offering remarkable capabilities that have garnered widespread usage. However, existing interaction paradigms between LLMs and users are constrained by either inflexibility, limitations in customization, or a lack of persistent learning. This inflexibility is particularly evident as users, especially those without programming skills, have restricted avenues to enhance or personalize the model. Existing frameworks further complicate the model training and deployment process due to their computational inefficiencies and lack of user-friendly interfaces. To overcome these challenges, this paper introduces a novel interaction paradigm-'Online Training using External Interactions'-that merges the benefits of persistent, real-time model updates with the flexibility for individual customization through external interactions such as AI a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;TopicDiff&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.04789</link><description>&lt;p&gt;
TopicDiff&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#30340;&#20027;&#39064;&#20016;&#23500;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;TopicDiff&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#65288;MCE&#65289;&#26816;&#27979;&#36890;&#24120;&#36328;&#36234;&#22768;&#23398;&#12289;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#65292;&#21560;&#24341;&#20102;&#22810;&#23186;&#20307;&#31038;&#21306;&#26085;&#30410;&#22686;&#21152;&#30340;&#20852;&#36259;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#23545;&#35805;&#20013;&#30340;&#35821;&#22659;&#20449;&#24687;&#65292;&#21482;&#26377;&#23569;&#25968;&#32771;&#34385;&#21333;&#19968;&#35821;&#35328;&#27169;&#24577;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#32780;&#24635;&#26159;&#24573;&#35270;&#22768;&#23398;&#21644;&#35270;&#35273;&#20027;&#39064;&#20449;&#24687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;Topic-enriched Diffusion&#65288;TopicDiff&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;MCE&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#20197;&#32531;&#35299;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#12290;&#35814;&#32454;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;TopicDiff&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#20027;&#39064;&#20449;&#24687;&#23545;MCE&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;TopicDiff&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04789v1 Announce Type: cross  Abstract: Multimodal Conversational Emotion (MCE) detection, generally spanning across the acoustic, vision and language modalities, has attracted increasing interest in the multimedia community. Previous studies predominantly focus on learning contextual information in conversations with only a few considering the topic information in single language modality, while always neglecting the acoustic and vision topic information. On this basis, we propose a model-agnostic Topic-enriched Diffusion (TopicDiff) approach for capturing multimodal topic information in MCE tasks. Particularly, we integrate the diffusion model into neural topic model to alleviate the diversity deficiency problem of neural topic model in capturing topic information. Detailed evaluations demonstrate the significant improvements of TopicDiff over the state-of-the-art MCE baselines, justifying the importance of multimodal topic information to MCE and the effectiveness of Topic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#33322;&#31354;&#20107;&#25925;&#25253;&#21578;&#20998;&#26512;&#20013;&#30340;&#20004;&#31181;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#65292;&#21457;&#29616;LDA&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#32780;NMF&#22312;&#25552;&#21462;&#20027;&#39064;&#26041;&#38754;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.04788</link><description>&lt;p&gt;
&#33322;&#31354;&#20107;&#25925;&#25253;&#21578;&#30340;&#20027;&#39064;&#24314;&#27169;&#20998;&#26512;&#65306;LDA&#21644;NMF&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Topic Modeling Analysis of Aviation Accident Reports: A Comparative Study between LDA and NMF Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#33322;&#31354;&#20107;&#25925;&#25253;&#21578;&#20998;&#26512;&#20013;&#30340;&#20004;&#31181;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#65292;&#21457;&#29616;LDA&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#32780;NMF&#22312;&#25552;&#21462;&#20027;&#39064;&#26041;&#38754;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33322;&#31354;&#23433;&#20840;&#22312;&#29616;&#20195;&#31038;&#20250;&#33267;&#20851;&#37325;&#35201;&#65292;&#33268;&#21147;&#20110;&#20943;&#23569;&#20107;&#25925;&#24182;&#25552;&#39640;&#23433;&#20840;&#26631;&#20934;&#12290;&#22312;&#36825;&#19968;&#21162;&#21147;&#30340;&#26680;&#24515;&#26159;&#23545;&#33322;&#31354;&#20107;&#25925;&#25253;&#21578;&#36827;&#34892;&#20998;&#26512;&#65292;&#36825;&#20123;&#20016;&#23500;&#30340;&#25991;&#26412;&#36164;&#28304;&#25581;&#31034;&#20102;&#33322;&#31354;&#20107;&#25925;&#32972;&#21518;&#30340;&#21407;&#22240;&#21644;&#24433;&#21709;&#22240;&#32032;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20004;&#31181;&#33879;&#21517;&#30340;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#65292;Latent Dirichlet Allocation&#65288;LDA&#65289;&#21644;Non-negative Matrix Factorization&#65288;NMF&#65289;&#65292;&#24182;&#23558;&#20854;&#36816;&#29992;&#22312;&#33322;&#31354;&#20107;&#25925;&#25253;&#21578;&#20998;&#26512;&#30340;&#32972;&#26223;&#19979;&#12290;&#30740;&#31350;&#21033;&#29992;&#20102;National Transportation Safety Board&#65288;NTSB&#65289;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#30446;&#26631;&#26159;&#33258;&#21160;&#21270;&#21644;&#31616;&#21270;&#35782;&#21035;&#20107;&#25925;&#25253;&#21578;&#20013;&#28508;&#22312;&#20027;&#39064;&#21644;&#27169;&#24335;&#30340;&#36807;&#31243;&#12290;&#21033;&#29992;&#19968;&#33268;&#24615;&#20540;&#65288;C_v&#65289;&#24230;&#37327;&#26469;&#35780;&#20272;&#29983;&#25104;&#20027;&#39064;&#30340;&#36136;&#37327;&#12290;LDA&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20027;&#39064;&#36830;&#36143;&#24615;&#65292;&#34920;&#26126;&#20027;&#39064;&#20869;&#21333;&#35789;&#20043;&#38388;&#20855;&#26377;&#26356;&#24378;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#65292;NMF&#22312;&#29983;&#20135;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04788v1 Announce Type: new  Abstract: Aviation safety is paramount in the modern world, with a continuous commitment to reducing accidents and improving safety standards. Central to this endeavor is the analysis of aviation accident reports, rich textual resources that hold insights into the causes and contributing factors behind aviation mishaps. This paper compares two prominent topic modeling techniques, Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF), in the context of aviation accident report analysis. The study leverages the National Transportation Safety Board (NTSB) Dataset with the primary objective of automating and streamlining the process of identifying latent themes and patterns within accident reports. The Coherence Value (C_v) metric was used to evaluate the quality of generated topics. LDA demonstrates higher topic coherence, indicating stronger semantic relevance among words within topics. At the same time, NMF excelled in produ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#26041;&#26696;CREEM&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#21435;&#35760;&#24518;&#24182;&#24341;&#20837;&#23436;&#21892;&#36807;&#31243;&#26469;&#23454;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#24212;&#30340;&#25972;&#20307;&#25913;&#36827;&#21644;&#36830;&#36143;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04787</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#21644;&#23436;&#21892;&#36807;&#21435;&#26469;&#19981;&#26029;&#28436;&#36827;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Ever-Evolving Memory by Blending and Refining the Past
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04787
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#26041;&#26696;CREEM&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#21435;&#35760;&#24518;&#24182;&#24341;&#20837;&#23436;&#21892;&#36807;&#31243;&#26469;&#23454;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#24212;&#30340;&#25972;&#20307;&#25913;&#36827;&#21644;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31867;&#20284;&#20154;&#31867;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#26500;&#24314;&#38271;&#26399;&#35760;&#24518;&#33267;&#20851;&#37325;&#35201;&#12290;&#26500;&#24314;&#35760;&#24518;&#30340;&#19968;&#20010;&#22825;&#30495;&#26041;&#27861;&#21487;&#33021;&#21482;&#26159;&#21015;&#20986;&#24635;&#32467;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#24403;&#35828;&#35805;&#32773;&#30340;&#29366;&#24577;&#38543;&#26102;&#38388;&#21464;&#21270;&#26102;&#65292;&#36825;&#26679;&#20570;&#21487;&#33021;&#20250;&#23548;&#33268;&#38382;&#39064;&#65292;&#24182;&#31215;&#32047;&#30683;&#30462;&#20449;&#24687;&#12290;&#35760;&#24518;&#20445;&#25345;&#26377;&#32452;&#32455;&#23545;&#20110;&#38477;&#20302;&#22238;&#24212;&#29983;&#25104;&#22120;&#30340;&#28151;&#20081;&#24456;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#26041;&#26696;&#65292;CREEM&#12290;&#19982;&#20165;&#22522;&#20110;&#24403;&#21069;&#23545;&#35805;&#26500;&#24314;&#35760;&#24518;&#30340;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35760;&#24518;&#24418;&#25104;&#36807;&#31243;&#20013;&#28151;&#21512;&#36807;&#21435;&#30340;&#35760;&#24518;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23436;&#21892;&#36807;&#31243;&#26469;&#22788;&#29702;&#22810;&#20313;&#25110;&#36807;&#26102;&#20449;&#24687;&#12290;&#36825;&#31181;&#21019;&#26032;&#24615;&#26041;&#27861;&#36890;&#36807;&#30830;&#20445;&#19968;&#20010;&#26356;&#21152;&#30693;&#24773;&#21644;&#21160;&#24577;&#28436;&#21464;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#26088;&#22312;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#24212;&#30340;&#25972;&#20307;&#25913;&#36827;&#21644;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04787v1 Announce Type: cross  Abstract: For a human-like chatbot, constructing a long-term memory is crucial. A naive approach for making a memory could be simply listing the summarized dialogue. However, this can lead to problems when the speaker's status change over time and contradictory information gets accumulated. It is important that the memory stays organized to lower the confusion for the response generator. In this paper, we propose a novel memory scheme for long-term conversation, CREEM. Unlike existing approaches that construct memory based solely on current sessions, our proposed model blending past memories during memory formation. Additionally, we introduce refining process to handle redundant or outdated information. This innovative approach seeks for overall improvement and coherence of chatbot responses by ensuring a more informed and dynamically evolving long-term memory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#35843;&#26597;&#21508;&#31181;&#25915;&#20987;&#24418;&#24335;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21463;&#25915;&#20987;&#30340;&#24615;&#36136;&#12289;&#26426;&#21046;&#12289;&#28508;&#22312;&#24433;&#21709;&#20197;&#21450;&#24403;&#21069;&#38450;&#24481;&#31574;&#30053;&#65292;&#20026;&#27169;&#22411;&#23436;&#25972;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.04786</link><description>&lt;p&gt;
&#20998;&#35299;&#38450;&#24481;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25915;&#20987;&#30340;&#27604;&#36739;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#35843;&#26597;&#21508;&#31181;&#25915;&#20987;&#24418;&#24335;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21463;&#25915;&#20987;&#30340;&#24615;&#36136;&#12289;&#26426;&#21046;&#12289;&#28508;&#22312;&#24433;&#21709;&#20197;&#21450;&#24403;&#21069;&#38450;&#24481;&#31574;&#30053;&#65292;&#20026;&#27169;&#22411;&#23436;&#25972;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#22522;&#30707;&#65292;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#26041;&#38754;&#25552;&#20379;&#20102;&#38761;&#21629;&#24615;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23427;&#20204;&#26085;&#30410;&#37325;&#35201;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#33030;&#24369;&#24615;&#26041;&#38754;&#24050;&#32463;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#12290;&#26412;&#25991;&#23545;&#38024;&#23545;LLMs&#30340;&#21508;&#31181;&#24418;&#24335;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#35752;&#35770;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#24615;&#36136;&#21644;&#26426;&#21046;&#12289;&#23427;&#20204;&#30340;&#28508;&#22312;&#24433;&#21709;&#20197;&#21450;&#24403;&#21069;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#26088;&#22312;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12289;&#24433;&#21709;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#20013;&#27602;&#20197;&#21450;&#19982;&#35757;&#32451;&#25968;&#25454;&#21033;&#29992;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#19981;&#21516;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12289;LLMs&#25269;&#25239;&#36825;&#20123;&#25915;&#20987;&#30340;&#38887;&#24615;&#20197;&#21450;&#23545;&#27169;&#22411;&#23436;&#25972;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23457;&#35270;&#26368;&#26032;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04786v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have become a cornerstone in the field of Natural Language Processing (NLP), offering transformative capabilities in understanding and generating human-like text. However, with their rising prominence, the security and vulnerability aspects of these models have garnered significant attention. This paper presents a comprehensive survey of the various forms of attacks targeting LLMs, discussing the nature and mechanisms of these attacks, their potential impacts, and current defense strategies. We delve into topics such as adversarial attacks that aim to manipulate model outputs, data poisoning that affects model training, and privacy concerns related to training data exploitation. The paper also explores the effectiveness of different attack methodologies, the resilience of LLMs against these attacks, and the implications for model integrity and user trust. By examining the latest research, we provide insight
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;&#65288;LLMMs&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20020;&#24202;&#31508;&#35760;&#21644;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#29992;&#20110;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.04785</link><description>&lt;p&gt;
&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#39044;&#27979;5&#24180;&#24930;&#24615;&#30142;&#30149;&#38431;&#21015;&#30340;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Multimodal Models for 5-Year Chronic Disease Cohort Prediction Using EHR Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;&#65288;LLMMs&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20020;&#24202;&#31508;&#35760;&#21644;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#29992;&#20110;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24930;&#24615;&#30142;&#30149;&#22914;&#31958;&#23615;&#30149;&#26159;&#20840;&#29699;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26412;&#30740;&#31350;&#20174;&#21488;&#28286;&#21307;&#38498;&#25968;&#25454;&#24211;&#25910;&#38598;&#20102;&#20116;&#24180;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#21253;&#25324;1,420,596&#20221;&#20020;&#24202;&#31508;&#35760;&#12289;387,392&#20221;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#20197;&#21450;&#36229;&#36807;1,505&#31181;&#23454;&#39564;&#23460;&#26816;&#39564;&#39033;&#30446;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#29992;&#20110;&#30740;&#31350;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;&#65288;LLMMs&#65289;&#26694;&#26550;&#65292;&#23558;&#20020;&#24202;&#31508;&#35760;&#21644;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#25991;&#26412;&#23884;&#20837;&#32534;&#30721;&#22120;&#21644;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#26469;&#23398;&#20064;&#23454;&#39564;&#23460;&#26816;&#39564;&#25968;&#20540;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22359;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04785v1 Announce Type: cross  Abstract: Chronic diseases such as diabetes are the leading causes of morbidity and mortality worldwide. Numerous research studies have been attempted with various deep learning models in diagnosis. However, most previous studies had certain limitations, including using publicly available datasets (e.g. MIMIC), and imbalanced data. In this study, we collected five-year electronic health records (EHRs) from the Taiwan hospital database, including 1,420,596 clinical notes, 387,392 laboratory test results, and more than 1,505 laboratory test items, focusing on research pre-training large language models. We proposed a novel Large Language Multimodal Models (LLMMs) framework incorporating multimodal data from clinical notes and laboratory test results for the prediction of chronic disease risk. Our method combined a text embedding encoder and multi-head attention layer to learn laboratory test values, utilizing a deep neural network (DNN) module to 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21709;&#24212;&#36807;&#28388;&#30340;&#22810;Agent&#38450;&#24481;&#26694;&#26550;AutoDefense&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;LLMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#24120;&#29992;&#25143;&#35831;&#27714;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04783</link><description>&lt;p&gt;
AutoDefense: &#22810;Agent LLM &#38450;&#24481;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04783
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21709;&#24212;&#36807;&#28388;&#30340;&#22810;Agent&#38450;&#24481;&#26694;&#26550;AutoDefense&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;LLMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#24120;&#29992;&#25143;&#35831;&#27714;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#36947;&#24503;&#23545;&#40784;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20197;&#38450;&#27490;&#22312;&#29992;&#25143;&#35831;&#27714;&#26102;&#29983;&#25104;&#26377;&#23475;&#20449;&#24687;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21709;&#24212;&#36807;&#28388;&#30340;&#22810;Agent&#38450;&#24481;&#26694;&#26550;AutoDefense&#65292;&#29992;&#20110;&#20174;LLMs&#20013;&#36807;&#28388;&#26377;&#23475;&#22238;&#22797;&#12290; &#27492;&#26694;&#26550;&#20026;LLM&#20195;&#29702;&#20998;&#37197;&#19981;&#21516;&#35282;&#33394;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#20849;&#21516;&#23436;&#25104;&#38450;&#24481;&#20219;&#21153;&#12290; &#20219;&#21153;&#30340;&#21010;&#20998;&#22686;&#24378;&#20102;LLMs&#30340;&#25972;&#20307;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#65292;&#24182;&#20351;&#20854;&#20182;&#38450;&#24481;&#32452;&#20214;&#20316;&#20026;&#24037;&#20855;&#38598;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290; AutoDefense &#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#35268;&#27169;&#21644;&#31181;&#31867;&#30340;&#24320;&#28304;LLMs&#20316;&#20026;&#20195;&#29702;&#12290; &#36890;&#36807;&#23545;&#22823;&#37327;&#26377;&#23475;&#21644;&#23433;&#20840;&#25552;&#31034;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;AutoDefense&#22312;&#25552;&#39640;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#27491;&#24120;&#29992;&#25143;&#35831;&#27714;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04783v1 Announce Type: cross  Abstract: Despite extensive pre-training and fine-tuning in moral alignment to prevent generating harmful information at user request, large language models (LLMs) remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense, a response-filtering based multi-agent defense framework that filters harmful responses from LLMs. This framework assigns different roles to LLM agents and employs them to complete the defense task collaboratively. The division in tasks enhances the overall instruction-following of LLMs and enables the integration of other defense components as tools. AutoDefense can adapt to various sizes and kinds of open-source LLMs that serve as agents. Through conducting extensive experiments on a large scale of harmful and safe prompts, we validate the effectiveness of the proposed AutoDefense in improving the robustness against jailbreak attacks, while maintaining the performance at normal user request. Our code and 
&lt;/p&gt;</description></item><item><title>&#26102;&#38388;&#30693;&#35782;&#22270;&#34920;&#31034;&#23398;&#20064;&#23558;&#26102;&#38388;&#20449;&#24687;&#34701;&#20837;&#26631;&#20934;&#30693;&#35782;&#22270;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#23454;&#20307;&#21644;&#20851;&#31995;&#38543;&#26102;&#38388;&#30340;&#21160;&#24577;&#21464;&#21270;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2403.04782</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#35843;&#26597;&#65306;&#34920;&#31034;&#23398;&#20064;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Survey on Temporal Knowledge Graph: Representation Learning and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04782
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#34920;&#31034;&#23398;&#20064;&#23558;&#26102;&#38388;&#20449;&#24687;&#34701;&#20837;&#26631;&#20934;&#30693;&#35782;&#22270;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#23454;&#20307;&#21644;&#20851;&#31995;&#38543;&#26102;&#38388;&#30340;&#21160;&#24577;&#21464;&#21270;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#24182;&#34987;&#24191;&#27867;&#29992;&#20110;&#22686;&#24378;&#19979;&#28216;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;&#30693;&#35782;&#22270;&#19978;&#65292;&#20854;&#20107;&#23454;&#19981;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#65292;&#24182;&#24573;&#30053;&#20102;&#23427;&#20204;&#38543;&#26102;&#38388;&#30340;&#21160;&#24577;&#28436;&#21464;&#12290;&#22240;&#27492;&#65292;&#26102;&#38388;&#30693;&#35782;&#22270;&#21463;&#21040;&#26356;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#22823;&#37327;&#32467;&#26500;&#21270;&#30693;&#35782;&#20165;&#23384;&#22312;&#20110;&#29305;&#23450;&#26102;&#26399;&#20869;&#12290;&#30693;&#35782;&#22270;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#20026;&#30693;&#35782;&#22270;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#23398;&#20064;&#20302;&#32500;&#21521;&#37327;&#23884;&#20837;&#12290;&#26102;&#24577;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#23558;&#26102;&#38388;&#20449;&#24687;&#34701;&#20837;&#26631;&#20934;&#30693;&#35782;&#22270;&#26694;&#26550;&#20013;&#65292;&#21487;&#20197;&#23545;&#23454;&#20307;&#21644;&#20851;&#31995;&#38543;&#26102;&#38388;&#30340;&#21160;&#24577;&#21464;&#21270;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#23545;&#26102;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#23398;&#20064;&#21450;&#20854;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#20174;&#20171;&#32461;&#23450;&#20041;&#12289;&#25968;&#25454;&#38598;&#21644;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04782v1 Announce Type: cross  Abstract: Knowledge graphs have garnered significant research attention and are widely used to enhance downstream applications. However, most current studies mainly focus on static knowledge graphs, whose facts do not change with time, and disregard their dynamic evolution over time. As a result, temporal knowledge graphs have attracted more attention because a large amount of structured knowledge exists only within a specific period. Knowledge graph representation learning aims to learn low-dimensional vector embeddings for entities and relations in a knowledge graph. The representation learning of temporal knowledge graphs incorporates time information into the standard knowledge graph framework and can model the dynamics of entities and relations over time. In this paper, we conduct a comprehensive survey of temporal knowledge graph representation learning and its applications. We begin with an introduction to the definitions, datasets, and e
&lt;/p&gt;</description></item><item><title>MuseGraph&#23558;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#21644;&#36890;&#29992;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20351;&#29992;</title><link>https://arxiv.org/abs/2403.04780</link><description>&lt;p&gt;
MuseGraph&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#23548;&#21521;&#25351;&#20196;&#35843;&#25972;&#29992;&#20110;&#36890;&#29992;&#22270;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04780
&lt;/p&gt;
&lt;p&gt;
MuseGraph&#23558;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#21644;&#36890;&#29992;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20016;&#23500;&#23646;&#24615;&#30340;&#22270;&#22312;&#24314;&#27169;&#20114;&#32852;&#23454;&#20307;&#21644;&#25913;&#36827;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#39044;&#27979;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#24120;&#29992;&#20110;&#24314;&#27169;&#24102;&#23646;&#24615;&#30340;&#22270;&#65292;&#20294;&#38656;&#35201;&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#22270;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26102;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#33539;&#20363;&#65292;&#20294;LLMs&#22312;&#22270;&#25366;&#25496;&#20013;&#30340;&#29983;&#25104;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550; MuseGraph&#65292;&#23427;&#26080;&#32541;&#25972;&#21512;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#24182;&#20419;&#36827;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#21644;&#36890;&#29992;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#36755;&#20837;&#29983;&#25104;&#24341;&#20837;&#19968;&#20010;&#32039;&#20945;&#30340;&#22270;&#25551;&#36848;&#65292;&#20197;&#22312;&#35821;&#35328;&#20196;&#29260;&#38480;&#21046;&#30340;&#32422;&#26463;&#19979;&#23553;&#35013;&#26469;&#33258;&#22270;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04780v1 Announce Type: cross  Abstract: Graphs with abundant attributes are essential in modeling interconnected entities and improving predictions in various real-world applications. Traditional Graph Neural Networks (GNNs), which are commonly used for modeling attributed graphs, need to be re-trained every time when applied to different graph tasks and datasets. Although the emergence of Large Language Models (LLMs) has introduced a new paradigm in natural language processing, the generative potential of LLMs in graph mining remains largely under-explored. To this end, we propose a novel framework MuseGraph, which seamlessly integrates the strengths of GNNs and LLMs and facilitates a more effective and generic approach for graph mining across different tasks and datasets. Specifically, we first introduce a compact graph description via the proposed adaptive input generation to encapsulate key information from the graph under the constraints of language token limitations. T
&lt;/p&gt;</description></item><item><title>QASE&#27169;&#22359;&#22312;PLMs&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#25552;&#21319;&#20102;&#25991;&#26412;&#29983;&#25104;&#22312;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20351;&#24471;&#20854;&#22312;MRC&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;GPT-4&#31561;&#39046;&#20808;&#30340;LLMs&#12290;</title><link>https://arxiv.org/abs/2403.04771</link><description>&lt;p&gt;
QASE&#22686;&#24378;&#22411;PLMs&#65306;&#25552;&#39640;&#25991;&#26412;&#29983;&#25104;&#22312;MRC&#20013;&#30340;&#25511;&#21046;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
QASE Enhanced PLMs: Improved Control in Text Generation for MRC
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04771
&lt;/p&gt;
&lt;p&gt;
QASE&#27169;&#22359;&#22312;PLMs&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#25552;&#21319;&#20102;&#25991;&#26412;&#29983;&#25104;&#22312;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20351;&#24471;&#20854;&#22312;MRC&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;GPT-4&#31561;&#39046;&#20808;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#29983;&#25104;&#24335;&#27169;&#22411;&#22312;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#20013;&#22833;&#25511;&#29983;&#25104;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Question-Attended Span Extraction&#65288;QASE&#65289;&#27169;&#22359;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24494;&#35843;&#36807;&#31243;&#20013;&#38598;&#25104;QASE&#33021;&#22815;&#20351;&#36825;&#20123;PLMs&#21305;&#37197;SOTA&#30340;&#25277;&#21462;&#26041;&#27861;&#24182;&#22312;MRC&#20219;&#21153;&#20013;&#32988;&#36807;GPT-4&#31561;&#39046;&#20808;&#30340;LLMs&#65292;&#32780;&#19981;&#20250;&#26126;&#26174;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04771v1 Announce Type: new  Abstract: To address the challenges of out-of-control generation in generative models for machine reading comprehension (MRC), we introduce the Question-Attended Span Extraction (QASE) module. Integrated during the fine-tuning of pre-trained generative language models (PLMs), QASE enables these PLMs to match SOTA extractive methods and outperform leading LLMs like GPT-4 in MRC tasks, without significant increases in computational costs.
&lt;/p&gt;</description></item><item><title>&#24863;&#30693;&#27169;&#22411;&#35748;&#20026;&#31038;&#20132;&#21462;&#21521;&#65288;&#20363;&#22914;&#65292;&#28909;&#24773;&#21451;&#22909;&#12289;&#20658;&#24930;&#20919;&#28448;&#65289;&#23545;&#35805;&#21442;&#19982;&#32773;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#21644;&#35299;&#37322;&#31038;&#20132;&#20114;&#21160;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#20110;&#31995;&#32479;&#24615;&#22320;&#24212;&#29992;&#31038;&#20132;&#21462;&#21521;&#26631;&#31614;&#26469;&#24314;&#27169;&#23545;&#35805;&#32467;&#26524;</title><link>https://arxiv.org/abs/2403.04770</link><description>&lt;p&gt;
&#31038;&#20132;&#21462;&#21521;&#65306;&#23545;&#35805;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Social Orientation: A New Feature for Dialogue Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04770
&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#27169;&#22411;&#35748;&#20026;&#31038;&#20132;&#21462;&#21521;&#65288;&#20363;&#22914;&#65292;&#28909;&#24773;&#21451;&#22909;&#12289;&#20658;&#24930;&#20919;&#28448;&#65289;&#23545;&#35805;&#21442;&#19982;&#32773;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#21644;&#35299;&#37322;&#31038;&#20132;&#20114;&#21160;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#20110;&#31995;&#32479;&#24615;&#22320;&#24212;&#29992;&#31038;&#20132;&#21462;&#21521;&#26631;&#31614;&#26469;&#24314;&#27169;&#23545;&#35805;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24773;&#22659;&#19979;&#65292;&#39044;&#27979;&#21644;&#35299;&#37322;&#23545;&#35805;&#30340;&#25104;&#21151;&#25110;&#22833;&#36133;&#26159;&#24456;&#26377;&#29992;&#30340;&#12290;&#24515;&#29702;&#23398;&#30340;Circumplex&#29702;&#35770;&#24314;&#27169;&#20102;&#20250;&#35805;&#21442;&#19982;&#32773;&#30340;&#31038;&#20132;&#21462;&#21521;&#65288;&#20363;&#22914;&#65292;&#28909;&#24773;&#21451;&#22909;&#12289;&#20658;&#24930;&#20919;&#28448;&#65289;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#21644;&#35299;&#37322;&#31038;&#20132;&#20114;&#21160;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#20110;&#31995;&#32479;&#24615;&#22320;&#24212;&#29992;&#31038;&#20132;&#21462;&#21521;&#26631;&#31614;&#26469;&#24314;&#27169;&#23545;&#35805;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#35805;&#35805;&#35821;&#25968;&#25454;&#38598;&#65292;&#26426;&#22120;&#26631;&#35760;&#20102;&#31038;&#20132;&#21462;&#21521;&#26631;&#31614;&#12290;&#25105;&#20204;&#23637;&#31034;&#31038;&#20132;&#21462;&#21521;&#26631;&#31614;&#25552;&#39640;&#20102;&#20219;&#21153;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#35821;&#35328;&#22522;&#20934;&#20013;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#31038;&#20132;&#21462;&#21521;&#26631;&#31614;&#22914;&#20309;&#24110;&#21161;&#35299;&#37322;&#31070;&#32463;&#27169;&#22411;&#20013;&#31038;&#20132;&#20114;&#21160;&#30340;&#32467;&#26524;&#12290;&#26681;&#25454;&#26174;&#31034;&#20986;&#31038;&#20132;&#21462;&#21521;&#26631;&#31614;&#22312;&#23545;&#35805;&#32467;&#26524;&#39044;&#27979;&#20219;&#21153;&#20013;&#23454;&#29992;&#24615;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#24067;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04770v1 Announce Type: new  Abstract: There are many settings where it is useful to predict and explain the success or failure of a dialogue. Circumplex theory from psychology models the social orientations (e.g., Warm-Agreeable, Arrogant-Calculating) of conversation participants and can be used to predict and explain the outcome of social interactions. Our work is novel in its systematic application of social orientation tags to modeling conversation outcomes. In this paper, we introduce a new data set of dialogue utterances machine-labeled with social orientation tags. We show that social orientation tags improve task performance, especially in low-resource settings, on both English and Chinese language benchmarks. We also demonstrate how social orientation tags help explain the outcomes of social interactions when used in neural models. Based on these results showing the utility of social orientation tags for dialogue outcome prediction tasks, we release our data sets, co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#32463;&#36807;&#24494;&#35843;&#30340;GPT4&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#31227;&#38500;&#20854;&#22312;&#23398;&#20064;&#26399;&#38388;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;</title><link>https://arxiv.org/abs/2403.04769</link><description>&lt;p&gt;
&#31227;&#38500;GPT4&#30340;&#36807;&#28388;&#22120;
&lt;/p&gt;
&lt;p&gt;
Removing GPT4's Filter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04769
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#32463;&#36807;&#24494;&#35843;&#30340;GPT4&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#31227;&#38500;&#20854;&#22312;&#23398;&#20064;&#26399;&#38388;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT4&#26368;&#21021;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#65292;&#21363;&#24535;&#24895;&#32773;&#25552;&#20379;&#21453;&#39304;&#20197;&#25945;&#23548;GPT4&#19981;&#35201;&#29983;&#25104;&#19981;&#24403;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25805;&#20316;&#24050;&#32463;&#36827;&#34892;&#24494;&#35843;&#30340;&#29256;&#26412;&#65292;&#20351;&#20854;&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;RLHF&#65288;Reinforcement learning from Human Feedback&#65289;&#30340;&#34892;&#20026;&#65292;&#26377;&#25928;&#22320;&#31227;&#38500;&#20102;&#27169;&#22411;&#22312;RLHF&#26399;&#38388;&#23398;&#20064;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;&#12290;&#29305;&#21035;&#26159;&#65292;&#24403;GPT4&#22312;&#27809;&#26377;&#32463;&#36807;RLHF&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#26102;&#65292;&#23427;&#22833;&#21435;&#20102;&#25152;&#26377;&#25233;&#21046;&#21147;&#65292;&#21482;&#38656;&#21069;&#20960;&#20010;&#35789;&#23601;&#21487;&#20197;&#29983;&#25104;&#38750;&#24120;&#19981;&#24403;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04769v1 Announce Type: cross  Abstract: GPT4 was initially trained on large amounts of data, and then fine-tuned using Reinforcement learning from Human Feedback (RLHF), which is when volunteers give feedback in order to teach GPT4 not to create inappropriate content. In this paper, we present a method to manipulate the fine-tuned version into reverting to pre-RLHF behavior, effectively removing all safety mechanisms that the model learned during RLHF. In particular, when GPT4 acts without RLHF, it loses all inhibition, and can complete very inappropriate content given only the first few words.
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#21644;&#30450;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.04732</link><description>&lt;p&gt;
&#25105;&#20204;&#36317;&#31163;&#26234;&#33021;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#36824;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We from Intelligent Visual Deductive Reasoning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04732
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#21644;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;GPT-4V&#20043;&#31867;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#22797;&#26434;&#20294;&#19981;&#22826;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#24182;&#21457;&#29616;&#20102;&#24403;&#21069;&#39046;&#20808;&#30340;VLM&#20013;&#20197;&#21069;&#26410;&#26292;&#38706;&#30340;&#30450;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#29790;&#25991;&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#26469;&#35780;&#20272;VLM&#22312;&#20165;&#20381;&#38752;&#35270;&#35273;&#32447;&#32034;&#36827;&#34892;&#22810;&#36339;&#20851;&#31995;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#27969;&#34892;&#30340;VLM&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#37319;&#29992;&#20102;&#26631;&#20934;&#31574;&#30053;&#65292;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#33258;&#25105;&#19968;&#33268;&#24615;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;Mensa&#26234;&#21830;&#27979;&#35797;&#12289;&#26234;&#21830;&#27979;&#35797;&#21644;RAVEN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;LLM&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#26377;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04732v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21475;&#35821;&#35821;&#35328;&#22810;&#30446;&#26631;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#23454;&#20307;&#27133;&#21644;&#23376;&#30446;&#26631;&#25351;&#20196;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04481</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#22810;&#30446;&#26631;&#21475;&#35821;&#35821;&#35328;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Model Understand Multi-Intent Spoken Language ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21475;&#35821;&#35821;&#35328;&#22810;&#30446;&#26631;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#23454;&#20307;&#27133;&#21644;&#23376;&#30446;&#26631;&#25351;&#20196;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22810;&#30446;&#26631;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#65288;SLU&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;SLU&#29615;&#22659;&#20013;&#21033;&#29992;LLMs&#29983;&#25104;&#33021;&#21147;&#30340;&#29420;&#29305;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#25216;&#26415;&#37325;&#26032;&#37197;&#32622;&#20102;&#23454;&#20307;&#27133;&#65292;&#19987;&#38376;&#29992;&#20110;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#23376;&#30446;&#26631;&#25351;&#20196;&#65288;SII&#65289;&#30340;&#27010;&#24565;&#65292;&#22686;&#24378;&#20102;&#23545;&#19981;&#21516;&#39046;&#22495;&#20869;&#22797;&#26434;&#22810;&#30446;&#26631;&#20132;&#27969;&#30340;&#35299;&#21078;&#21644;&#35299;&#37322;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#65292;&#34987;&#31216;&#20026;LM-MixATIS&#21644;LM-MixSNIPS&#65292;&#26159;&#20174;&#29616;&#26377;&#22522;&#20934;&#20013;&#31934;&#24515;&#21046;&#20316;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#21305;&#37197;&#24182;&#28508;&#22312;&#22320;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23427;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#22312;&#21508;&#31181;&#24847;&#22270;&#37197;&#32622;&#21644;&#25968;&#25454;&#38598;&#27604;&#20363;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#24320;&#21019;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;&#23454;&#20307;&#27133;&#20934;&#30830;&#24615;&#65288;ESA&#65289;&#21644;Com
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04481v1 Announce Type: cross  Abstract: This study marks a significant advancement by harnessing Large Language Models (LLMs) for multi-intent spoken language understanding (SLU), proposing a unique methodology that capitalizes on the generative power of LLMs within an SLU context. Our innovative technique reconfigures entity slots specifically for LLM application in multi-intent SLU environments and introduces the concept of Sub-Intent Instruction (SII), enhancing the dissection and interpretation of intricate, multi-intent communication within varied domains. The resultant datasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing benchmarks. Our research illustrates that LLMs can match and potentially excel beyond the capabilities of current state-of-the-art multi-intent SLU models. It further explores LLM efficacy across various intent configurations and dataset proportions. Moreover, we introduce two pioneering metrics, Entity Slot Accuracy (ESA) and Com
&lt;/p&gt;</description></item><item><title>Pearl&#25968;&#25454;&#38598;&#21033;&#29992;&#20102;&#35282;&#33394;&#21644;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20855;&#20307;&#29992;&#25143;&#20559;&#22909;&#65292;&#39046;&#22495;&#19987;&#19994;&#24615;&#21644;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.04460</link><description>&lt;p&gt;
Pearl: &#19968;&#39033;&#22522;&#20110;&#35780;&#35770;&#39537;&#21160;&#30340;&#35282;&#33394;&#30693;&#35782;&#23545;&#35805;&#24335;&#25512;&#33616;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04460
&lt;/p&gt;
&lt;p&gt;
Pearl&#25968;&#25454;&#38598;&#21033;&#29992;&#20102;&#35282;&#33394;&#21644;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20855;&#20307;&#29992;&#25143;&#20559;&#22909;&#65292;&#39046;&#22495;&#19987;&#19994;&#24615;&#21644;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04460v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#65292;&#20351;&#24471;&#23545;&#35805;&#36755;&#20837;&#30340;&#22810;&#26679;&#21270;&#25512;&#29702;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35813;&#39046;&#22495;&#36824;&#26377;&#35768;&#22810;&#26041;&#38754;&#26377;&#24453;&#25506;&#32034;&#12290;&#30446;&#21069;&#21487;&#29992;&#30340;&#29992;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#32570;&#20047;&#29305;&#23450;&#29992;&#25143;&#20559;&#22909;&#21644;&#23545;&#25512;&#33616;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#22952;&#30861;&#20102;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;PEARL&#65292;&#19982;&#35282;&#33394;&#21644;&#30693;&#35782;&#22686;&#24378;&#30340;LLM&#27169;&#25311;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#20174;&#30495;&#23454;&#35780;&#35770;&#20013;&#33719;&#24471;&#35814;&#32454;&#30340;&#35282;&#33394;&#21644;&#30693;&#35782;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#36229;&#36807;57k&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PEARL&#20013;&#30340;&#35805;&#35821;&#21253;&#25324;&#26356;&#20855;&#20307;&#30340;&#29992;&#25143;&#20559;&#22909;&#65292;&#26174;&#31034;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04460v1 Announce Type: new  Abstract: Conversational recommender system is an emerging area that has garnered an increasing interest in the community, especially with the advancements in large language models (LLMs) that enable diverse reasoning over conversational input. Despite the progress, the field has many aspects left to explore. The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations. To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators. We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues. Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide recommendations more relev
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.00835</link><description>&lt;p&gt;
CLLMs: &#19968;&#33268;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLLMs: Consistency Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00835
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#35299;&#30721;&#26041;&#27861;&#65292;&#22914;&#38597;&#21487;&#27604;&#35299;&#30721;&#65292;&#26174;&#31034;&#20986;&#26377;&#26395;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#25512;&#26029;&#65292;&#22240;&#20026;&#23427;&#25171;&#30772;&#20102;LLM&#35299;&#30721;&#36807;&#31243;&#30340;&#39034;&#24207;&#24615;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#21487;&#24182;&#34892;&#21270;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#35299;&#30721;&#30456;&#27604;&#65292;&#38597;&#21487;&#27604;&#35299;&#30721;&#24456;&#23569;&#33021;&#22312;&#21333;&#20010;&#22266;&#23450;&#28857;&#36845;&#20195;&#27493;&#39588;&#20013;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#26631;&#35760;&#65292;&#22240;&#27492;&#22312;&#36895;&#24230;&#19978;&#21462;&#24471;&#30340;&#25552;&#21319;&#30456;&#23545;&#36739;&#23567;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#20174;&#20219;&#20309;&#29366;&#24577;&#24555;&#36895;&#25910;&#25947;&#21040;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#30340;&#22266;&#23450;&#28857;&#12290;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#65292;&#20197;&#20415;&#22312;&#20219;&#20309;&#36755;&#20837;&#29366;&#24577;&#19979;&#19968;&#33268;&#22320;&#39044;&#27979;&#22266;&#23450;&#28857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#39046;&#22495;&#29305;&#23450;&#21644;&#24320;&#25918;&#22495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;2.4&#20493;&#21040;3.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00835v1 Announce Type: cross  Abstract: Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\times$ to 3.4$\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.14891</link><description>&lt;p&gt;
LLMBind: &#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMBind: A Unified Modality-Task Integration Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#38598;&#25104;&#33021;&#21147;&#26377;&#38480;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24102;&#22836;&#25506;&#32034;&#24182;&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#29992;&#20110;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;LLMBind&#21487;&#20197;&#20197;&#22810;&#31181;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#32452;&#21512;&#35299;&#37322;&#36755;&#20837;&#24182;&#29983;&#25104;&#36755;&#20986;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#65292;&#36890;&#36807;&#19981;&#21516;&#19987;&#23478;&#20043;&#38388;&#30340;&#21327;&#20316;&#23454;&#29616;&#19981;&#21516;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26377;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;40&#19975;&#26465;&#25351;&#20196;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35299;&#38145;&#20102;&#20132;&#20114;&#24335;&#35270;&#35273;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14891v1 Announce Type: cross  Abstract: While recent progress in multimodal large language models tackles various modality tasks, they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field. In this work, we take the initiative to explore and propose the LLMBind, a unified framework for modality task integration, which binds Large Language Models and corresponding pre-trained task models with task-specific tokens. Consequently, LLMBind can interpret inputs and produce outputs in versatile combinations of image, text, video, and audio. Specifically, we introduce a Mixture-of-Experts technique to enable effective learning for different multimodal tasks through collaboration among diverse experts. Furthermore, we create a multi-task dataset comprising 400k instruction data, which unlocks the ability for interactive visual generation and editing tasks. Extensive experiments show the effectiveness of our fr
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.14809</link><description>&lt;p&gt;
CriticBench&#65306;&#20026;&#25209;&#21028;&#24615;-&#27491;&#30830;&#25512;&#29702;&#35780;&#20272;LLMs&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14809
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25209;&#21028;&#21644;&#23436;&#21892;&#20854;&#25512;&#29702;&#30340;&#33021;&#21147;&#23545;&#20110;&#23427;&#20204;&#22312;&#35780;&#20272;&#12289;&#21453;&#39304;&#25552;&#20379;&#21644;&#33258;&#25105;&#25913;&#36827;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CriticBench&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25209;&#21028;&#21644;&#32416;&#27491;&#20854;&#25512;&#29702;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;CriticBench&#21253;&#21547;&#20116;&#20010;&#25512;&#29702;&#39046;&#22495;&#65306;&#25968;&#23398;&#12289;&#24120;&#35782;&#12289;&#31526;&#21495;&#12289;&#32534;&#30721;&#21644;&#31639;&#27861;&#12290;&#23427;&#25972;&#21512;&#20102;15&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;LLM&#31995;&#21015;&#30340;&#21709;&#24212;&#12290;&#21033;&#29992;CriticBench&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#21078;&#26512;&#20102;17&#20010;LLMs&#22312;&#29983;&#25104;&#12289;&#25209;&#21028;&#21644;&#20462;&#27491;&#25512;&#29702;&#65288;&#21363;GQC&#25512;&#29702;&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;1&#65289;GQC&#33021;&#21147;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65307;&#65288;2&#65289;&#20462;&#27491;&#25928;&#26524;&#22312;&#20219;&#21153;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20197;&#36923;&#36753;&#20026;&#23548;&#21521;&#30340;&#20219;&#21153;&#26356;&#23481;&#26131;&#20462;&#27491;&#65307;&#65288;3&#65289;GQC&#30693;&#35782;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;SaGE&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#22270;&#29109;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36947;&#24503;&#19968;&#33268;&#24615;&#65292;&#26500;&#24314;&#20102;MCC&#35821;&#26009;&#24211;&#12290;</title><link>https://arxiv.org/abs/2402.13709</link><description>&lt;p&gt;
SaGE&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
SaGE: Evaluating Moral Consistency in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;SaGE&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#22270;&#29109;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36947;&#24503;&#19968;&#33268;&#24615;&#65292;&#26500;&#24314;&#20102;MCC&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#23637;&#31034;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20250;&#35805;&#31995;&#32479;&#20013;&#30340;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20063;&#23384;&#22312;&#36947;&#24503;&#19981;&#19968;&#33268;&#65292;&#23545;&#20854;&#21487;&#38752;&#24615;&#65288;&#20197;&#21450;&#24635;&#20307;&#21487;&#20449;&#36182;&#24615;&#65289;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#20197;&#24448;&#22312;LLM&#35780;&#20272;&#39046;&#22495;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#24320;&#21457;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#20197;&#34913;&#37327;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36947;&#24503;&#24773;&#26223;&#24448;&#24448;&#32570;&#20047;&#26222;&#36941;&#35748;&#21516;&#31572;&#26696;&#30340;&#24773;&#20917;&#65292;&#27169;&#22411;&#21709;&#24212;&#30340;&#19968;&#33268;&#24615;&#23545;&#20110;&#20854;&#21487;&#38752;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29702;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SaGE&#65289;&#65292;&#22522;&#20110;&#8220;&#32463;&#39564;&#27861;&#21017;&#8221;&#65288;RoTs&#65289;&#30340;&#27010;&#24565;&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#36947;&#24503;&#19968;&#33268;&#24615;&#12290;RoTs&#26159;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#25277;&#35937;&#21407;&#21017;&#65292;&#21487;&#26377;&#25928;&#24110;&#21161;&#35299;&#37322;&#20854;&#20915;&#31574;&#31574;&#30053;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#36947;&#24503;&#19968;&#33268;&#24615;&#35821;&#26009;&#24211;&#65288;MCC&#65289;&#65292;&#21253;&#21547;50K&#20010;&#36947;&#24503;&#38382;&#39064;&#12289;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13709v1 Announce Type: cross  Abstract: Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of "Rules of Thumb" (RoTs) to measure a model's moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#38416;&#26126;&#20102;&#23558;&#19987;&#26377;&#24040;&#22836;&#30340;&#20808;&#36827;&#21151;&#33021;&#36716;&#31227;&#21040;&#24320;&#28304;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13116</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Distillation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#38416;&#26126;&#20102;&#23558;&#19987;&#26377;&#24040;&#22836;&#30340;&#20808;&#36827;&#21151;&#33021;&#36716;&#31227;&#21040;&#24320;&#28304;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#20013;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#37325;&#28857;&#20851;&#27880;KD&#22312;&#23558;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#19987;&#26377;&#24040;&#22836;&#30340;&#22797;&#26434;&#33021;&#21147;&#36716;&#31227;&#21040;&#21487;&#35775;&#38382;&#30340;&#24320;&#28304;&#27169;&#22411;&#65288;&#22914;LLaMA&#21644;Mistral&#65289;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#26412;&#39033;&#24037;&#20316;&#38416;&#26126;&#20102;&#19987;&#26377;&#21644;&#24320;&#28304;LLMs&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#23637;&#31034;&#20102;KD&#22914;&#20309;&#25104;&#20026;&#31532;&#20108;&#32773;&#36171;&#20104;&#31532;&#19968;&#32773;&#20808;&#36827;&#21151;&#33021;&#21644;&#32454;&#33268;&#29702;&#35299;&#30340;&#37325;&#35201;&#23186;&#20171;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#22260;&#32469;&#31639;&#27861;&#12289;&#25216;&#33021;&#21644;&#22402;&#30452;&#21270;&#36825;&#19977;&#20010;&#22522;&#30784;&#25903;&#26609;&#31934;&#24515;&#26500;&#24314;&#65292;&#20840;&#38754;&#25506;&#35752;&#20102;KD&#26426;&#21046;&#12289;&#29305;&#23450;&#35748;&#30693;&#33021;&#21147;&#30340;&#22686;&#24378;&#20197;&#21450;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35843;&#26597;&#24341;&#23548;&#30528;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#21644;KD&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13116v1 Announce Type: new  Abstract: This survey presents an in-depth exploration of knowledge distillation (KD) techniques within the realm of Large Language Models (LLMs), spotlighting the pivotal role of KD in transferring sophisticated capabilities from proprietary giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral. Amidst the evolving AI landscape, this work elucidates the critical disparities between proprietary and open-source LLMs, demonstrating how KD serves as an essential conduit for imbuing the latter with the former's advanced functionalities and nuanced understandings. Our survey is meticulously structured around three foundational pillars: algorithm, skill, and verticalization -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#21644;&#26041;&#27861;&#65292;&#23558;LSC&#38382;&#39064;&#20998;&#35299;&#20026;&#19981;&#21516;&#32423;&#21035;&#30340;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#20843;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;APD&#22312;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;XL-LEXEME&#22312;WiC&#12289;WSI&#21644;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#24182;&#19988;&#19982;GPT-4&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.12011</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#30340;&#31995;&#32479;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#21644;&#26041;&#27861;&#65292;&#23558;LSC&#38382;&#39064;&#20998;&#35299;&#20026;&#19981;&#21516;&#32423;&#21035;&#30340;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#20843;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;APD&#22312;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;XL-LEXEME&#22312;WiC&#12289;WSI&#21644;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#24182;&#19988;&#19982;GPT-4&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#26159;&#24314;&#27169;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#65288;LSC&#65289;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#24403;&#21069;&#30340;&#35780;&#20272;&#36890;&#24120;&#19987;&#27880;&#20110;&#31216;&#20026;&#20998;&#32423;&#21464;&#21270;&#26816;&#27979;&#65288;GCD&#65289;&#30340;&#29305;&#23450;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#19981;&#21516;&#35774;&#32622;&#65292;&#36328;&#20316;&#21697;&#30340;&#24615;&#33021;&#27604;&#36739;&#32463;&#24120;&#20855;&#26377;&#35823;&#23548;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#35780;&#20272;&#20102;GCD&#30340;&#26368;&#26032;&#27169;&#22411;&#21644;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;LSC&#38382;&#39064;&#20998;&#35299;&#20026;&#19978;&#19979;&#25991;&#20013;&#30340;&#21333;&#35789;&#65288;WiC&#65289;&#21644;&#35789;&#20041;&#24402;&#32435;&#65288;WSI&#65289;&#20219;&#21153;&#65292;&#24182;&#27604;&#36739;&#36825;&#20123;&#19981;&#21516;&#32423;&#21035;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#21487;&#29992;&#30340;LSC&#22522;&#20934;&#27979;&#35797;&#20013;&#36328;&#19981;&#21516;&#35821;&#35328;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;APD&#22312;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65307;&#65288;ii&#65289;XL-LEXEME&#22312;WiC&#12289;WSI&#21644;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#19982;GPT-4&#30456;&#24403;&#65307;&#65288;iii&#65289;&#26126;&#26174;&#38656;&#35201;&#25913;&#36827;&#23545;&#35789;&#20041;&#24314;&#27169;&#20197;&#21450;&#20851;&#27880;&#36825;&#20123;&#24847;&#20041;&#20309;&#26102;&#12289;&#22914;&#20309;&#21644;&#20026;&#20309;&#21464;&#21270;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12011v1 Announce Type: new  Abstract: Contextualized embeddings are the preferred tool for modeling Lexical Semantic Change (LSC). Current evaluations typically focus on a specific task known as Graded Change Detection (GCD). However, performance comparison across work are often misleading due to their reliance on diverse settings. In this paper, we evaluate state-of-the-art models and approaches for GCD under equal conditions. We further break the LSC problem into Word-in-Context (WiC) and Word Sense Induction (WSI) tasks, and compare models across these different levels. Our evaluation is performed across different languages on eight available benchmarks for LSC, and shows that (i) APD outperforms other approaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for WiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear need for improving the modeling of word meanings, as well as focus on how, when, and why these meanings change, rather t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.04437</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Structured Entity Extraction Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#24433;&#21709;&#20102;&#20449;&#24687;&#25552;&#21462;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#21644;&#35268;&#33539;&#21270;&#20102;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#65288;SEE&#65289;&#20219;&#21153;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#36817;&#20284;&#23454;&#20307;&#38598;&#37325;&#21472;&#65288;AESOP&#65289;&#24230;&#37327;&#65292;&#20197;&#36866;&#24403;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#25552;&#39640;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#24182;&#34892;&#35780;&#20272;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20026;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#39046;&#22495;&#30340;&#26410;&#26469;&#36827;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text. This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP) Metric designed to appropriately assess model performance on this task. Later, we propose a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;BIPIA&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#27492;&#31867;&#25915;&#20987;&#26102;&#30340;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#25915;&#20987;&#25104;&#21151;&#30340;&#21407;&#22240;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.14197</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;BIPIA&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#27492;&#31867;&#25915;&#20987;&#26102;&#30340;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#25915;&#20987;&#25104;&#21151;&#30340;&#21407;&#22240;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#20869;&#23481;&#30340;&#25972;&#21512;&#24050;&#32463;&#23454;&#29616;&#20102;LLMs&#30340;&#26356;&#26032;&#21644;&#24191;&#27867;&#24212;&#29992;&#65292;&#27604;&#22914;&#24494;&#36719;Copilot&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25972;&#21512;&#20063;&#35753;LLMs&#38754;&#20020;&#20102;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#22806;&#37096;&#20869;&#23481;&#20013;&#23884;&#20837;&#24694;&#24847;&#25351;&#20196;&#65292;&#20174;&#32780;ompromising LLM&#36755;&#20986;&#24182;&#23548;&#33268;&#21709;&#24212;&#20559;&#31163;&#29992;&#25143;&#26399;&#26395;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#37325;&#35201;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;BIPIA&#65292;&#20197;&#35780;&#20272;&#36825;&#31867;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#22522;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#20998;&#26512;&#20102;&#35813;&#25915;&#20987;&#25104;&#21151;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#21363;LLMs&#26080;&#27861;&#21306;&#20998;&#25351;&#20196;&#21644;&#22806;&#37096;&#20869;&#23481;&#20197;&#21450;&#32570;&#20047;&#24847;&#35782;&#19981;&#25191;&#34892;&#22806;&#37096;&#20869;&#23481;&#20869;&#30340;&#25351;&#20196;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#40657;&#30418;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14197v2 Announce Type: replace-cross  Abstract: The integration of large language models (LLMs) with external content has enabled more up-to-date and wide-ranging applications of LLMs, such as Microsoft Copilot. However, this integration has also exposed LLMs to the risk of indirect prompt injection attacks, where an attacker can embed malicious instructions within external content, compromising LLM output and causing responses to deviate from user expectations. To investigate this important but underexplored issue, we introduce the first benchmark for indirect prompt injection attacks, named BIPIA, to evaluate the risk of such attacks. Based on the evaluation, our work makes a key analysis of the underlying reason for the success of the attack, namely the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content. Building upon this analysis, we develop two black-box metho
&lt;/p&gt;</description></item><item><title>LoRAMoE&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#36866;&#37197;&#22120;&#21644;&#36335;&#30001;&#22120;&#32593;&#32476;&#65292;&#31867;&#20284;&#20110;MoE&#30340;&#25554;&#20214;&#29256;&#26412;&#65292;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19990;&#30028;&#30693;&#35782;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.09979</link><description>&lt;p&gt;
LoRAMoE: &#36890;&#36807;MoE&#39118;&#26684;&#30340;&#25554;&#20214;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19990;&#30028;&#30693;&#35782;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09979
&lt;/p&gt;
&lt;p&gt;
LoRAMoE&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#36866;&#37197;&#22120;&#21644;&#36335;&#30001;&#22120;&#32593;&#32476;&#65292;&#31867;&#20284;&#20110;MoE&#30340;&#25554;&#20214;&#29256;&#26412;&#65292;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19990;&#30028;&#30693;&#35782;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#19982;&#20154;&#31867;&#25351;&#20196;&#23545;&#40784;&#65292;&#24182;&#22686;&#24378;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25351;&#20196;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#22686;&#21152;&#21487;&#33021;&#20250;&#30772;&#22351;LLMs&#20808;&#21069;&#23384;&#20648;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoRAMoE&#65292;&#36825;&#26159;&#19968;&#20010;&#24341;&#20837;&#20102;&#22810;&#20010;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#24182;&#36890;&#36807;&#36335;&#30001;&#22120;&#32593;&#32476;&#38598;&#25104;&#23427;&#20204;&#30340;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#31867;&#20284;&#20110;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#30340;&#25554;&#20214;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09979v3 Announce Type: replace  Abstract: Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Increasing instruction data substantially is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novelty framework that introduces several low-rank adapters (LoRA) and integrates them by using a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of LoRAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge-edge forgetting. Experimental results show that, as the instruction data increases, LoRAMoE can si
&lt;/p&gt;</description></item><item><title>RLHF-V&#36890;&#36807;&#32454;&#31890;&#24230;&#32416;&#27491;&#20154;&#31867;&#21453;&#39304;&#30340;&#34892;&#20026;&#23545;&#20934;&#65292;&#25552;&#39640;&#20102;MLLM&#30340;&#21487;&#20449;&#24230;&#65292;&#20351;&#20854;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#20114;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#21487;&#38752;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2312.00849</link><description>&lt;p&gt;
RLHF-V: &#36890;&#36807;&#32454;&#31890;&#24230;&#32416;&#27491;&#20154;&#31867;&#21453;&#39304;&#23454;&#29616;&#21487;&#20449;&#20219;&#30340;MLLMs&#34892;&#20026;&#23545;&#20934;
&lt;/p&gt;
&lt;p&gt;
RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00849
&lt;/p&gt;
&lt;p&gt;
RLHF-V&#36890;&#36807;&#32454;&#31890;&#24230;&#32416;&#27491;&#20154;&#31867;&#21453;&#39304;&#30340;&#34892;&#20026;&#23545;&#20934;&#65292;&#25552;&#39640;&#20102;MLLM&#30340;&#21487;&#20449;&#24230;&#65292;&#20351;&#20854;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#20114;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#21487;&#38752;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#20114;&#21160;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLLMs&#26222;&#36941;&#23384;&#22312;&#20005;&#37325;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30456;&#20851;&#22270;&#20687;&#19981;&#31526;&#21512;&#20107;&#23454;&#12290;&#36825;&#20010;&#38382;&#39064;&#20351;&#24471;&#29616;&#26377;&#30340;MLLMs&#19981;&#21487;&#38752;&#65292;&#22240;&#27492;&#22312;&#29616;&#23454;&#19990;&#30028;&#65288;&#23588;&#20854;&#26159;&#39640;&#39118;&#38505;&#65289;&#24212;&#29992;&#20013;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RLHF-V&#65292;&#36890;&#36807;&#20174;&#32454;&#31890;&#24230;&#32416;&#27491;&#30340;&#20154;&#31867;&#21453;&#39304;&#20013;&#22686;&#24378;MLLM&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;RLHF-V&#37319;&#38598;&#20154;&#31867;&#20559;&#22909;&#65292;&#20197;&#29255;&#27573;&#32423;&#21035;&#23545;&#24187;&#35273;&#36827;&#34892;&#32416;&#27491;&#65292;&#24182;&#22312;&#20154;&#31867;&#21453;&#39304;&#19978;&#25191;&#34892;&#23494;&#38598;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#12290;&#22312;&#33258;&#21160;&#21644;&#20154;&#31867;&#35780;&#20272;&#30340;&#20116;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;RLHF-V&#33021;&#22815;&#23454;&#29616;&#26356;&#21487;&#20449;&#36182;&#30340;MLLM&#34892;&#20026;&#65292;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00849v2 Announce Type: replace  Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in multimodal understanding, reasoning, and interaction. However, existing MLLMs prevalently suffer from serious hallucination problems, generating text that is not factually grounded in associated images. The problem makes existing MLLMs untrustworthy and thus impractical in real-world (especially high-stakes) applications. To address the challenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior alignment from fine-grained correctional human feedback. Specifically, RLHF-V collects human preference in the form of segment-level corrections on hallucinations, and performs dense direct preference optimization over the human feedback. Comprehensive experiments on five benchmarks in both automatic and human evaluation show that, RLHF-V can enable substantially more trustworthy MLLM behaviors with promising data and computation 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21019;&#24314;&#26356;&#21487;&#38752;&#30340;&#25991;&#26723;&#32423;&#20154;&#24037;&#21442;&#32771;&#32763;&#35793;&#30340;&#26041;&#27861;&#35770;&#65292;&#31216;&#20026;&#8220;&#26368;&#20339;&#21442;&#32771;&#32763;&#35793;&#8221;&#65292;&#26088;&#22312;&#25552;&#39640;&#34987;&#35270;&#20026;&#8220;&#20154;&#24037;&#32763;&#35793;&#36136;&#37327;&#8221;&#30340;&#26631;&#20934;</title><link>https://arxiv.org/abs/2311.16787</link><description>&lt;p&gt;
&#35780;&#20272;&#26368;&#20339;&#21442;&#32771;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Evaluating Optimal Reference Translations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16787
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21019;&#24314;&#26356;&#21487;&#38752;&#30340;&#25991;&#26723;&#32423;&#20154;&#24037;&#21442;&#32771;&#32763;&#35793;&#30340;&#26041;&#27861;&#35770;&#65292;&#31216;&#20026;&#8220;&#26368;&#20339;&#21442;&#32771;&#32763;&#35793;&#8221;&#65292;&#26088;&#22312;&#25552;&#39640;&#34987;&#35270;&#20026;&#8220;&#20154;&#24037;&#32763;&#35793;&#36136;&#37327;&#8221;&#30340;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#23545;&#19978;&#36798;&#21040;&#30340;&#25972;&#20307;&#32763;&#35793;&#36136;&#37327;&#38750;&#24120;&#22909;&#12290;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#19981;&#36866;&#29992;&#20063;&#19981;&#24847;&#22270;&#25581;&#31034;&#20173;&#28982;&#23384;&#22312;&#30340;&#35768;&#22810;&#32763;&#35793;&#38169;&#35823;&#21644;&#36136;&#37327;&#32570;&#38519;&#12290;&#27492;&#22806;&#65292;&#26631;&#20934;&#21442;&#32771;&#32763;&#35793;&#30340;&#36136;&#37327;&#24120;&#24120;&#21463;&#21040;&#36136;&#30097;&#65292;&#24182;&#22312;&#20960;&#31181;&#35821;&#35328;&#23545;&#20013;&#65292;&#21333;&#20973;&#26426;&#22120;&#32763;&#35793;&#24050;&#32463;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#36136;&#37327;&#27700;&#24179;&#12290;&#22312;&#36825;&#20123;&#39640;&#36164;&#28304;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#30740;&#31350;&#22240;&#27492;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21019;&#24314;&#26356;&#21487;&#38752;&#30340;&#25991;&#26723;&#32423;&#20154;&#24037;&#21442;&#32771;&#32763;&#35793;&#30340;&#26041;&#27861;&#35770;&#65292;&#31216;&#20026;&#8220;&#26368;&#20339;&#21442;&#32771;&#32763;&#35793;&#8221;&#65292;&#26088;&#22312;&#25552;&#39640;&#34987;&#35270;&#20026;&#8220;&#20154;&#24037;&#32763;&#35793;&#36136;&#37327;&#8221;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#33719;&#24471;&#30340;&#25991;&#26723;&#32423;&#26368;&#20339;&#21442;&#32771;&#32763;&#35793;&#65292;&#24182;&#19982;&#8220;&#26631;&#20934;&#8221;&#32763;&#35793;&#36827;&#34892;&#27604;&#36739;&#65292;&#30830;&#35748;&#20102;&#26174;&#33879;&#30340;&#36136;&#37327;&#25552;&#39640;&#65292;&#24182;&#35760;&#24405;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16787v2 Announce Type: replace  Abstract: The overall translation quality reached by current machine translation (MT) systems for high-resourced language pairs is remarkably good. Standard methods of evaluation are not suitable nor intended to uncover the many translation errors and quality deficiencies that still persist. Furthermore, the quality of standard reference translations is commonly questioned and comparable quality levels have been reached by MT alone in several language pairs. Navigating further research in these high-resource settings is thus difficult. In this article, we propose a methodology for creating more reliable document-level human reference translations, called "optimal reference translations," with the simple aim to raise the bar of what should be deemed "human translation quality." We evaluate the obtained document-level optimal reference translations in comparison with "standard" ones, confirming a significant quality increase and also documenting
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#33041;&#35760;&#24405;&#20013;&#30452;&#25509;&#29983;&#25104;&#35821;&#35328;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#33041;&#35299;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#36755;&#20837;&#29983;&#25104;&#19982;&#35821;&#20041;&#20869;&#23481;&#19968;&#33268;&#30340;&#36830;&#36143;&#35821;&#35328;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2311.09889</link><description>&lt;p&gt;
&#22823;&#33041;&#35760;&#24405;&#20013;&#30340;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Language Generation from Brain Recordings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09889
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#33041;&#35760;&#24405;&#20013;&#30452;&#25509;&#29983;&#25104;&#35821;&#35328;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#33041;&#35299;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#36755;&#20837;&#29983;&#25104;&#19982;&#35821;&#20041;&#20869;&#23481;&#19968;&#33268;&#30340;&#36830;&#36143;&#35821;&#35328;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#20405;&#20837;&#24335;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCIs&#65289;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#20855;&#26377;&#28508;&#21147;&#35299;&#38145;&#35768;&#22810;&#24212;&#29992;&#65292;&#22914;&#20026;&#27531;&#30142;&#24739;&#32773;&#25552;&#20379;&#26381;&#21153;&#21644;&#25913;&#21892;&#27807;&#36890;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36890;&#36807;BCIs&#29983;&#25104;&#35821;&#35328;&#20165;&#22312;&#20998;&#31867;&#35774;&#32622;&#20869;&#25104;&#21151;&#65292;&#29992;&#20110;&#36873;&#25321;&#24102;&#26377;&#26368;&#21487;&#33021;&#30340;&#30382;&#23618;&#35821;&#20041;&#34920;&#31034;&#30340;&#39044;&#29983;&#25104;&#21477;&#23376;&#24310;&#32493;&#20505;&#36873;&#12290;&#21463;&#21040;&#26368;&#36817;&#26174;&#31034;&#22823;&#33041;&#19982;&#22823;&#22411;&#35745;&#31639;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#20851;&#32852;&#30340;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35821;&#35328;BCI&#65292;&#35813;BCI&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#19982;&#35821;&#20041;&#33041;&#35299;&#30721;&#22120;&#20849;&#21516;&#29983;&#25104;&#35821;&#35328;&#65292;&#30452;&#25509;&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#36755;&#20837;&#20013;&#29983;&#25104;&#35821;&#35328;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19982;&#24863;&#30693;&#21040;&#30340;&#35270;&#35273;&#25110;&#21548;&#35273;&#35821;&#35328;&#21050;&#28608;&#30340;&#35821;&#20041;&#20869;&#23481;&#19968;&#33268;&#30340;&#35821;&#35328;&#24207;&#21015;&#65292;&#32780;&#26080;&#38656;&#20107;&#20808;&#30693;&#36947;&#20219;&#20309;&#39044;&#23450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09889v4 Announce Type: replace  Abstract: Generating human language through non-invasive brain-computer interfaces (BCIs) has the potential to unlock many applications, such as serving disabled patients and improving communication. Currently, however, generating language via BCIs has been previously successful only within a classification setup for selecting pre-generated sentence continuation candidates with the most likely cortical semantic representation. Inspired by recent research that revealed associations between the brain and the large computational language models, we propose a generative language BCI that utilizes the capacity of a large language model (LLM) jointly with a semantic brain decoder to directly generate language from functional magnetic resonance imaging (fMRI) input. The proposed model can generate coherent language sequences aligned with the semantic content of visual or auditory language stimuli perceived, without prior knowledge of any pre-generate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoT-ER&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#65292;&#23454;&#29616;&#20102;&#38142;&#24335;&#25512;&#29702;&#24605;&#32500;&#38142;&#19982;&#26174;&#24335;&#35777;&#25454;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2311.05922</link><description>&lt;p&gt;
&#20855;&#26377;&#26174;&#24335;&#35777;&#25454;&#25512;&#29702;&#30340;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05922
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoT-ER&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#65292;&#23454;&#29616;&#20102;&#38142;&#24335;&#25512;&#29702;&#24605;&#32500;&#38142;&#19982;&#26174;&#24335;&#35777;&#25454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#28041;&#21450;&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#20004;&#20010;&#29305;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#31867;&#22411;&#65292;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#26631;&#27880;&#26679;&#26412;&#12290;&#36890;&#36807;&#24212;&#29992;&#20803;&#23398;&#20064;&#21644;&#31070;&#32463;&#22270;&#25216;&#26415;&#65292;&#24050;&#32463;&#20986;&#29616;&#20102;&#21508;&#31181;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#36866;&#24212;&#24615;&#35757;&#32451;&#12290;&#26368;&#36817;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#24050;&#32463;&#22312;&#27809;&#26377;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23637;&#31034;&#20986;&#26174;&#33879;&#32467;&#26524;&#12290;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#21033;&#29992;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#38646;&#26679;&#26412;&#20449;&#24687;&#25552;&#21462;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#26500;&#24314;&#24605;&#32500;&#38142;&#25552;&#31034;&#26102;&#65292;&#25512;&#29702;&#30340;&#35777;&#25454;&#35201;&#20040;&#26410;&#32771;&#34385;&#65292;&#35201;&#20040;&#22312;&#38544;&#24335;&#27169;&#22411;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;CoT-ER&#65292;&#21363;&#20855;&#26377;&#26174;&#24335;&#35777;&#25454;&#25512;&#29702;&#30340;&#24605;&#32500;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05922v3 Announce Type: replace  Abstract: Few-shot relation extraction involves identifying the type of relationship between two specific entities within a text, using a limited number of annotated samples. A variety of solutions to this problem have emerged by applying meta-learning and neural graph techniques which typically necessitate a training process for adaptation. Recently, the strategy of in-context learning has been demonstrating notable results without the need of training. Few studies have already utilized in-context learning for zero-shot information extraction. Unfortunately, the evidence for inference is either not considered or implicitly modeled during the construction of chain-of-thought prompts. In this paper, we propose a novel approach for few-shot relation extraction using large language models, named CoT-ER, chain-of-thought with explicit evidence reasoning. In particular, CoT-ER first induces large language models to generate evidences using task-spe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RuLES&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;LLMs&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#36981;&#23432;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.04235</link><description>&lt;p&gt;
LLM&#33021;&#36981;&#23432;&#31616;&#21333;&#35268;&#21017;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can LLMs Follow Simple Rules?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04235
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RuLES&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;LLMs&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#36981;&#23432;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25215;&#25285;&#36234;&#26469;&#36234;&#22810;&#30340;&#36131;&#20219;&#65292;&#33021;&#22815;&#20197;&#21487;&#38752;&#30340;&#26041;&#24335;&#25351;&#23450;&#21644;&#32422;&#26463;&#36825;&#20123;&#31995;&#32479;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35268;&#21017;&#36981;&#24490;&#35821;&#35328;&#35780;&#20272;&#22330;&#26223;&#65288;RuLES&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#27979;&#37327;LLMs&#36981;&#24490;&#35268;&#21017;&#33021;&#21147;&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#21253;&#25324;14&#20010;&#31616;&#21333;&#30340;&#25991;&#26412;&#22330;&#26223;&#65292;&#27169;&#22411;&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#34987;&#25351;&#31034;&#36981;&#23432;&#21508;&#31181;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04235v2 Announce Type: replace  Abstract: As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as "do not generate abusive content", but these may be circumvented by jailbreaking techniques. Existing evaluations of adversarial attacks and defenses on LLMs generally require either expensive manual review or unreliable heuristic checks. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 14 simple text scenarios in which the model is instructed to obey various rules while interacting with the user. Each scenario has a programmatic evaluation function to determine whether the model has broken any rules in a conversation. Our evaluations of proprietar
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32479;&#19968;&#26694;&#26550;&#35299;&#37322;&#21644;&#35780;&#20272;&#29616;&#26377;&#30340;&#22522;&#20110;&#27010;&#29575;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#24320;&#21457;&#22810;&#31181;&#20114;&#20449;&#24687;&#30340;&#32452;&#21512;&#21464;&#20307;&#65292;&#23558;Oracle&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#25552;&#39640;&#21040;94.98%&#12290;</title><link>https://arxiv.org/abs/2305.14877</link><description>&lt;p&gt;
&#36890;&#36807;&#32479;&#19968;&#35780;&#20272;&#21644;&#20998;&#26512;&#25913;&#36827;&#22522;&#20110;&#27010;&#29575;&#30340;&#25552;&#31034;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14877
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32479;&#19968;&#26694;&#26550;&#35299;&#37322;&#21644;&#35780;&#20272;&#29616;&#26377;&#30340;&#22522;&#20110;&#27010;&#29575;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#24320;&#21457;&#22810;&#31181;&#20114;&#20449;&#24687;&#30340;&#32452;&#21512;&#21464;&#20307;&#65292;&#23558;Oracle&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#25552;&#39640;&#21040;94.98%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#20013;&#30340;&#20808;&#21069;&#24037;&#20316;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#22522;&#20110;&#27010;&#29575;&#30340;&#26080;&#26799;&#24230;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#32473;&#23450;&#20219;&#21153;&#36873;&#25321;&#26368;&#20339;&#25552;&#31034;&#20505;&#36873;&#65292;&#20294;&#26410;&#33021;&#22312;&#24444;&#27492;&#20043;&#38388;&#25552;&#20379;&#20840;&#38754;&#20844;&#27491;&#30340;&#27604;&#36739;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;13&#20010;&#24120;&#35265;&#21644;&#22810;&#26679;&#21270;&#30340;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#35299;&#37322;&#21644;&#35780;&#20272;&#29616;&#26377;&#30340;&#22522;&#20110;&#27010;&#29575;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#27599;&#31181;&#29616;&#26377;&#26041;&#27861;&#37117;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#19968;&#31181;&#26368;&#22823;&#21270;&#36755;&#20837;&#21644;&#39044;&#27979;&#36755;&#20986;&#20043;&#38388;&#20114;&#20449;&#24687;&#30340;&#26041;&#27861;&#30340;&#26576;&#31181;&#21464;&#20307;&#12290;&#21033;&#29992;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#20854;&#20182;&#20114;&#20449;&#24687;&#30340;&#32452;&#21512;&#21464;&#20307;&#65292;&#24182;&#23558;Oracle&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20174;87.79%&#25552;&#39640;&#21040;94.98%&#65292;&#20197;&#36873;&#25321;&#25552;&#31034;&#30340;&#24615;&#33021;&#19982;&#26368;&#20339;Oracle&#25552;&#31034;&#30340;&#24615;&#33021;&#20043;&#27604;&#26469;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14877v2 Announce Type: replace  Abstract: Previous works in prompt engineering for large language models have introduced different gradient-free probability-based prompt selection methods that aim to choose the optimal prompt among the candidates for a given task but have failed to provide a comprehensive and fair comparison between each other. In this paper, we propose a unified framework to interpret and evaluate the existing probability-based prompt selection methods by performing extensive experiments on 13 common and diverse NLP tasks. We find that each of the existing methods can be interpreted as some variant of the method that maximizes mutual information between the input and the predicted output (MI). Utilizing this finding, we develop several other combinatorial variants of MI and increase the effectiveness of the oracle prompt selection method from 87.79% to 94.98%, measured as the ratio of the performance of the selected prompt to that of the optimal oracle prom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32599;&#39532;&#21270;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#25509;&#21475;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#21360;&#22320;&#35821;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32599;&#39532;&#21270;&#25991;&#26412;&#19981;&#20165;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#36824;&#22312;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#19979;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#32599;&#39532;&#21270;&#26377;&#28508;&#21147;&#24357;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2401.14280</link><description>&lt;p&gt;
RomanSetu: &#36890;&#36807;&#32599;&#39532;&#21270;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization. (arXiv:2401.14280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32599;&#39532;&#21270;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#25509;&#21475;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#21360;&#22320;&#35821;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32599;&#39532;&#21270;&#25991;&#26412;&#19981;&#20165;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#36824;&#22312;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#19979;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#32599;&#39532;&#21270;&#26377;&#28508;&#21147;&#24357;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#38750;&#33521;&#35821;&#35821;&#35328;&#65288;&#29305;&#21035;&#26159;&#20351;&#29992;&#38750;&#25289;&#19969;&#23383;&#27597;&#34920;&#30340;&#35821;&#35328;&#65289;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32599;&#39532;&#21270;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25509;&#21475;&#65292;&#20551;&#35774;&#39057;&#32321;&#30340;&#38750;&#27491;&#24335;&#20351;&#29992;&#21644;&#19982;&#33521;&#35821;&#20849;&#20139;&#30340;&#26631;&#35760;&#26377;&#21161;&#20110;&#36328;&#35821;&#35328;&#23545;&#40784;&#12290;&#25105;&#20204;&#20197;&#21360;&#22320;&#35821;&#20026;&#37325;&#28857;&#65292;&#36890;&#36807;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#35777;&#26126;&#32599;&#39532;&#21270;&#25991;&#26412;&#19981;&#20165;&#30001;&#20110;&#20854;&#36739;&#20302;&#30340;&#29983;&#20135;&#21147;&#32780;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#36824;&#22312;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#22810;&#33050;&#26412;&#25552;&#31034;&#26041;&#27861;&#32467;&#21512;&#20102;&#32599;&#39532;&#21270;&#21644;&#21407;&#29983;&#25991;&#26412;&#65292;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#32599;&#39532;&#21270;&#22312;&#24357;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#26410;&#26469;&#30340;&#24037;&#20316;&#23558;&#33268;&#21147;&#20110;&#23558;&#27492;&#26041;&#27861;&#25193;&#23637;&#21040;&#26356;&#22810;&#30340;&#35821;&#35328;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Latin scripts. We propose an innovative approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Focusing on Hindi, we demonstrate through Hindi-to-English translation and sentiment analysis tasks that romanized text not only significantly improves inference efficiency due to its lower fertility compared to native text but also achieves competitive performance with limited pre-training. Additionally, our novel multi-script prompting approach, which combines romanized and native texts, shows promise in further enhancing task performance. These findings suggest the potential of romanization in bridging the language gap for LLM applications, with future work aimed at expanding this approach to more languages and tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;(Chat)GPT&#21644;BERT&#22312;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;(Chat)GPT&#30340;&#34920;&#29616;&#26126;&#26174;&#20302;&#20110;BERT&#65292;&#23588;&#20854;&#22312;&#38271;&#26399;&#21464;&#21270;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26356;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.14040</link><description>&lt;p&gt;
(&#32842;&#22825;)GPT v BERT: &#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#20043;&#40654;&#26126;&#30340;&#27491;&#20041;&#12290;(arXiv:2401.14040v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection. (arXiv:2401.14040v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;(Chat)GPT&#21644;BERT&#22312;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;(Chat)GPT&#30340;&#34920;&#29616;&#26126;&#26174;&#20302;&#20110;BERT&#65292;&#23588;&#20854;&#22312;&#38271;&#26399;&#21464;&#21270;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#21644;(Chat)GPT&#65292;&#20316;&#20026;&#20855;&#26377;&#35299;&#20915;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#30340;&#24040;&#22823;&#33021;&#21147;&#30340;&#35789;&#27719;&#36229;&#32423;&#33521;&#38596;&#32780;&#20986;&#29616;&#12290;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#35821;&#20041;&#21464;&#21270;&#30340;&#26102;&#38388;&#24615;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#35299;&#20915;Word-in-Context (WiC)&#20219;&#21153;&#30340;&#20004;&#20010;&#21382;&#26102;&#24615;&#25193;&#23637;&#65306;TempoWiC&#21644;HistoWiC&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#65288;&#21644;GPT&#65289;3.5&#36825;&#26679;&#30340;&#26032;&#22411;&#21363;&#29992;&#25216;&#26415;&#19982;&#24403;&#21069;&#20316;&#20026;&#24314;&#27169;&#35821;&#20041;&#21464;&#21270;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#23478;&#26063;BERT&#20043;&#38388;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26159;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;(Chat)GPT&#30740;&#31350;&#35821;&#20041;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#30340;&#24615;&#33021;&#26174;&#33879;&#20302;&#20110;&#22522;&#30784;GPT&#29256;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;(Chat)GPT&#22312;&#26816;&#27979;&#38271;&#26399;&#21464;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#30053;&#20302;&#20110;BERT&#65292;&#20294;&#22312;&#30701;&#26399;&#21464;&#21270;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the universe of Natural Language Processing, Transformer-based language models like BERT and (Chat)GPT have emerged as lexical superheroes with great power to solve open research problems. In this paper, we specifically focus on the temporal problem of semantic change, and evaluate their ability to solve two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a family of models that currently stand as the state-of-the-art for modeling semantic change. Our experiments represent the first attempt to assess the use of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT performs significantly worse than the foundational GPT version. Furthermore, our results demonstrate that (Chat)GPT achieves slightly lower performance than BERT in detecting long-term changes but performs significantly worse in de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24322;&#26500;&#23545;&#35805;&#22270;&#32593;&#32476;&#25552;&#39640;&#23545;&#35805;&#20013;&#30340;&#20154;&#26684;&#35782;&#21035;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.05871</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24322;&#26500;&#23545;&#35805;&#22270;&#32593;&#32476;&#25552;&#39640;&#23545;&#35805;&#20013;&#30340;&#20154;&#26684;&#35782;&#21035;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Personality Recognition in Dialogue by Data Augmentation and Heterogeneous Conversational Graph Networks. (arXiv:2401.05871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05871
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24322;&#26500;&#23545;&#35805;&#22270;&#32593;&#32476;&#25552;&#39640;&#23545;&#35805;&#20013;&#30340;&#20154;&#26684;&#35782;&#21035;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26684;&#35782;&#21035;&#23545;&#20110;&#22686;&#24378;&#26426;&#22120;&#20154;&#30340;&#20010;&#24615;&#21270;&#22238;&#24212;&#33021;&#21147;&#38750;&#24120;&#26377;&#29992;&#65292;&#20174;&#32780;&#20419;&#36827;&#20016;&#23500;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#36825;&#19968;&#20219;&#21153;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#29616;&#26377;&#23545;&#35805;&#35821;&#26009;&#24211;&#20013;&#28436;&#35762;&#32773;&#25968;&#37327;&#26377;&#38480;&#65292;&#36825;&#38459;&#30861;&#20102;&#20581;&#22766;&#30340;&#12289;&#19982;&#28436;&#35762;&#32773;&#26080;&#20851;&#30340;&#20154;&#26684;&#35782;&#21035;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#27492;&#22806;&#65292;&#22312;&#23545;&#35805;&#20013;&#20934;&#30830;&#24314;&#27169;&#23545;&#35805;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#21644;&#21457;&#35328;&#32773;&#20869;&#37096;&#20381;&#36182;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20154;&#26684;&#29305;&#24449;&#25554;&#20540;&#26469;&#36827;&#34892;&#28436;&#35762;&#32773;&#25968;&#25454;&#22686;&#24378;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24322;&#26500;&#23545;&#35805;&#22270;&#32593;&#32476;&#65292;&#21487;&#20197;&#29420;&#31435;&#22320;&#25429;&#25417;&#19978;&#19979;&#25991;&#24433;&#21709;&#21644;&#20869;&#22312;&#20154;&#26684;&#29305;&#24449;&#12290;&#22312;RealPersonaChat&#35821;&#26009;&#24211;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personality recognition is useful for enhancing robots' ability to tailor user-adaptive responses, thus fostering rich human-robot interactions. One of the challenges in this task is a limited number of speakers in existing dialogue corpora, which hampers the development of robust, speaker-independent personality recognition models. Additionally, accurately modeling both the interdependencies among interlocutors and the intra-dependencies within the speaker in dialogues remains a significant issue. To address the first challenge, we introduce personality trait interpolation for speaker data augmentation. For the second, we propose heterogeneous conversational graph networks to independently capture both contextual influences and inherent personality traits. Evaluations on the RealPersonaChat corpus demonstrate our method's significant improvements over existing baselines.
&lt;/p&gt;</description></item><item><title>DepWiGNN&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#33410;&#28857;&#35760;&#24518;&#26041;&#26696;&#65292;&#24182;&#22312;&#22270;&#30340;&#28145;&#24230;&#32500;&#24230;&#19978;&#32858;&#21512;&#20449;&#24687;&#65292;&#20174;&#32780;&#33021;&#22815;&#25910;&#38598;&#38271;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#22534;&#21472;&#22810;&#20010;&#23618;&#27425;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DepWiGNN&#22312;&#20004;&#20010;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#27604;&#20256;&#32479;GNN&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12557</link><description>&lt;p&gt;
DepWiGNN&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text. (arXiv:2310.12557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12557
&lt;/p&gt;
&lt;p&gt;
DepWiGNN&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#33410;&#28857;&#35760;&#24518;&#26041;&#26696;&#65292;&#24182;&#22312;&#22270;&#30340;&#28145;&#24230;&#32500;&#24230;&#19978;&#32858;&#21512;&#20449;&#24687;&#65292;&#20174;&#32780;&#33021;&#22815;&#25910;&#38598;&#38271;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#22534;&#21472;&#22810;&#20010;&#23618;&#27425;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DepWiGNN&#22312;&#20004;&#20010;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#27604;&#20256;&#32479;GNN&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#31354;&#38388;&#25512;&#29702;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#31354;&#38388;&#25512;&#29702;&#26041;&#27861;&#36890;&#24120;&#20174;&#32431;&#25991;&#26412;&#20013;&#25512;&#26029;&#31354;&#38388;&#20851;&#31995;&#65292;&#24573;&#35270;&#20102;&#33258;&#28982;&#35821;&#35328;&#19982;&#31526;&#21495;&#32467;&#26500;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#24341;&#23548;&#21644;&#32858;&#21512;&#31526;&#21495;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;GNN&#22312;&#22788;&#29702;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#26102;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#30001;&#20110;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#21363;&#38543;&#30528;&#22270;&#23618;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Depth-Wise Graph Neural Network&#65288;DepWiGNN&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#28857;&#35760;&#24518;&#26041;&#26696;&#65292;&#24182;&#22312;&#22270;&#30340;&#28145;&#24230;&#32500;&#24230;&#19978;&#32858;&#21512;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#22312;&#24191;&#24230;&#32500;&#24230;&#19978;&#65292;&#36825;&#26679;&#21487;&#20197;&#25910;&#38598;&#38271;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#22534;&#21472;&#22810;&#20010;&#23618;&#27425;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#65292;DepWiGNN&#21487;&#20197;&#20197;&#27604;&#20256;&#32479;GNN&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial reasoning in text plays a crucial role in various real-world applications. Existing approaches for spatial reasoning typically infer spatial relations from pure text, which overlook the gap between natural language and symbolic structures. Graph neural networks (GNNs) have showcased exceptional proficiency in inducing and aggregating symbolic structures. However, classical GNNs face challenges in handling multi-hop spatial reasoning due to the over-smoothing issue, \textit{i.e.}, the performance decreases substantially as the number of graph layers increases. To cope with these challenges, we propose a novel \textbf{Dep}th-\textbf{Wi}se \textbf{G}raph \textbf{N}eural \textbf{N}etwork (\textbf{DepWiGNN}). Specifically, we design a novel node memory scheme and aggregate the information over the depth dimension instead of the breadth dimension of the graph, which empowers the ability to collect long dependencies without stacking multiple layers. Experimental results on two challen
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.09499</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#27425;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. (arXiv:2310.09499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09499
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31995;&#21015;&#20013;&#30340;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#37327;&#21270;&#12289;&#21098;&#26525;&#21644;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;LLMs&#30340;&#25928;&#29575;&#25104;&#20026;LLM&#30740;&#31350;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hessian&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;LLMs&#21098;&#26525;&#33267;&#33267;&#23569;50%&#30340;&#31232;&#30095;&#24615;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#23427;&#26681;&#25454;&#25935;&#24863;&#24230;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#31232;&#30095;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#38477;&#20302;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#24403;&#31232;&#30095;&#24230;&#38750;&#24120;&#39640;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#26356;&#21152;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Various Large Language Models(LLMs) from the Generative Pretrained Transformer~(GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50\% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20889;&#20316;&#21019;&#36896;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#22312;&#21019;&#24847;&#27979;&#35797;&#20013;&#36890;&#36807;&#30340;&#25968;&#37327;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26080;&#27861;&#20195;&#26367;&#19987;&#23478;&#36827;&#34892;TTCW&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.14556</link><description>&lt;p&gt;
&#33402;&#26415;&#36824;&#26159;&#25216;&#24039;&#65311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21019;&#36896;&#21147;&#30340;&#34394;&#20551;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
Art or Artifice? Large Language Models and the False Promise of Creativity. (arXiv:2309.14556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20889;&#20316;&#21019;&#36896;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#22312;&#21019;&#24847;&#27979;&#35797;&#20013;&#36890;&#36807;&#30340;&#25968;&#37327;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26080;&#27861;&#20195;&#26367;&#19987;&#23478;&#36827;&#34892;TTCW&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#35748;&#20026;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#20174;&#21338;&#23458;&#21040;&#25925;&#20107;&#30340;&#39640;&#36136;&#37327;&#20889;&#20316;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23458;&#35266;&#35780;&#20272;&#19968;&#27573;&#25991;&#23383;&#30340;&#21019;&#36896;&#21147;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21463;&#21019;&#36896;&#24615;&#24605;&#32500;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTC)&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#20849;&#35782;&#35780;&#20272;&#25216;&#26415;[3]&#65292;&#25552;&#20986;&#20102;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#21019;&#36896;&#21147;&#20316;&#20026;&#19968;&#20010;&#20135;&#21697;&#12290;TTCW&#30001;&#21253;&#21547;&#22312;&#27969;&#30021;&#24230;&#12289;&#28789;&#27963;&#24615;&#12289;&#29420;&#21019;&#24615;&#21644;&#32454;&#33268;&#24230;&#21407;&#22987;&#32500;&#24230;&#20013;&#30340;14&#20010;&#20108;&#20803;&#27979;&#35797;&#32452;&#25104;&#12290;&#25105;&#20204;&#25307;&#21215;&#20102;10&#20301;&#21019;&#24847;&#20316;&#23478;&#65292;&#24182;&#20351;&#29992;TTCW&#23545;48&#20010;&#30001;&#19987;&#19994;&#20316;&#23478;&#25110;LLMs&#25776;&#20889;&#30340;&#25925;&#20107;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#36890;&#36807;&#30340;TTCW&#27979;&#35797;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#20102;3-10&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#35780;&#20215;&#32773;&#65292;&#20197;&#33258;&#21160;&#21270;TTCW&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#27809;&#26377;&#19968;&#20010;LLM&#19982;&#19987;&#23478;&#35780;&#20272;&#21576;&#27491;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT), which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose the Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.
&lt;/p&gt;</description></item><item><title>LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12307</link><description>&lt;p&gt;
LongLoRA: &#39640;&#25928;&#30340;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12307
&lt;/p&gt;
&lt;p&gt;
LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#8212;&#8212;LongLoRA&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#36890;&#24120;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#23567;&#35757;&#32451;LLM&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;GPU&#36164;&#28304;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#26041;&#38754;&#21152;&#24555;&#20102;LLM&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#12290;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#25512;&#29702;&#36807;&#31243;&#20013;&#38656;&#35201;&#31264;&#23494;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#20294;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#23436;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#19978;&#19979;&#25991;&#30340;&#25193;&#23637;&#65292;&#22312;&#19982;&#20351;&#29992;&#20256;&#32479;&#27880;&#24847;&#21147;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#26102;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#21482;&#29992;&#20004;&#34892;&#20195;&#30721;&#23454;&#29616;&#65292;&#22312;&#25512;&#29702;&#20013;&#26159;&#21487;&#36873;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#21442;&#25968;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-effici
&lt;/p&gt;</description></item><item><title>M3PS&#26159;&#19968;&#31181;&#20840;&#38754;&#30340;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#23646;&#24615;&#24863;&#30693;&#20135;&#21697;&#25688;&#35201;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24314;&#27169;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20135;&#21697;&#25688;&#35201;&#65292;&#35299;&#20915;&#20102;&#30005;&#23376;&#21830;&#21153;&#20013;&#20135;&#21697;&#25688;&#35201;&#30340;&#31471;&#21040;&#31471;&#24314;&#27169;&#12289;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#24314;&#27169;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#24314;&#27169;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11351</link><description>&lt;p&gt;
M3PS&#65306;&#30005;&#23376;&#21830;&#21153;&#20013;&#20840;&#38754;&#30340;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#23646;&#24615;&#24863;&#30693;&#20135;&#21697;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
M3PS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product Summarization in E-commerce. (arXiv:2308.11351v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11351
&lt;/p&gt;
&lt;p&gt;
M3PS&#26159;&#19968;&#31181;&#20840;&#38754;&#30340;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#23646;&#24615;&#24863;&#30693;&#20135;&#21697;&#25688;&#35201;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24314;&#27169;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20135;&#21697;&#25688;&#35201;&#65292;&#35299;&#20915;&#20102;&#30005;&#23376;&#21830;&#21153;&#20013;&#20135;&#21697;&#25688;&#35201;&#30340;&#31471;&#21040;&#31471;&#24314;&#27169;&#12289;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#24314;&#27169;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#24314;&#27169;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20135;&#21697;&#25688;&#35201;&#65288;MMPS&#65289;&#26088;&#22312;&#36890;&#36807;&#31361;&#20986;&#20135;&#21697;&#29305;&#28857;&#30340;&#30701;&#25991;&#26412;&#25688;&#35201;&#26469;&#21560;&#24341;&#23458;&#25143;&#30340;&#20852;&#36259;&#24182;&#22686;&#21152;&#20854;&#36141;&#20080;&#27442;&#26395;&#12290;&#29616;&#26377;&#30340;MMPS&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#20960;&#20010;&#38382;&#39064;&#65306;1&#65289;&#32570;&#20047;&#31471;&#21040;&#31471;&#30340;&#20135;&#21697;&#25688;&#35201;&#65292;2&#65289;&#32570;&#20047;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#24314;&#27169;&#65292;&#20197;&#21450;3&#65289;&#32570;&#20047;&#22810;&#27169;&#24577;&#23646;&#24615;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#20135;&#21697;&#25688;&#35201;&#30340;&#31471;&#21040;&#31471;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#23646;&#24615;&#24863;&#30693;&#20135;&#21697;&#25688;&#35201;&#26041;&#27861;&#65288;M3PS&#65289;&#12290;M3PS&#21516;&#26102;&#23545;&#20135;&#21697;&#23646;&#24615;&#36827;&#34892;&#24314;&#27169;&#24182;&#29983;&#25104;&#20135;&#21697;&#25688;&#35201;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20960;&#20010;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#25351;&#23548;M3PS&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#27169;&#24577;&#23545;&#20135;&#21697;&#23646;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#20351;&#22810;&#27169;&#24577;&#20135;&#21697;&#29305;&#24615;&#33021;&#22815;&#24471;&#21040;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the long textual product information and the product image, Multi-Modal Product Summarization (MMPS) aims to attract customers' interest and increase their desire to purchase by highlighting product characteristics with a short textual summary. Existing MMPS methods have achieved promising performance. Nevertheless, there still exist several problems: 1) lack end-to-end product summarization, 2) lack multi-grained multi-modal modeling, and 3) lack multi-modal attribute modeling. To address these issues, we propose an end-to-end multi-grained multi-modal attribute-aware product summarization method (M3PS) for generating high-quality product summaries in e-commerce. M3PS jointly models product attributes and generates product summaries. Meanwhile, we design several multi-grained multi-modal tasks to better guide the multi-modal learning of M3PS. Furthermore, we model product attributes based on both text and image modalities so that multi-modal product characteristics can be manife
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25513;&#30721;&#32467;&#26500;&#25104;&#38271;&#65288;MSG&#65289;&#65292;&#21487;&#20197;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#21253;&#25324;&#20840;&#32500;&#24230;&#25104;&#38271;&#36827;&#31243;&#21644;&#29420;&#31435;&#20110;&#26032;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#20989;&#25968;&#20005;&#26684;&#20445;&#30041;&#25104;&#38271;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.02869</link><description>&lt;p&gt;
&#36890;&#36807;&#25513;&#30721;&#32467;&#26500;&#25104;&#38271;&#23454;&#29616;2&#20493;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
2x Faster Language Model Pre-training via Masked Structural Growth. (arXiv:2305.02869v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25513;&#30721;&#32467;&#26500;&#25104;&#38271;&#65288;MSG&#65289;&#65292;&#21487;&#20197;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#21253;&#25324;&#20840;&#32500;&#24230;&#25104;&#38271;&#36827;&#31243;&#21644;&#29420;&#31435;&#20110;&#26032;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#20989;&#25968;&#20005;&#26684;&#20445;&#30041;&#25104;&#38271;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#65292;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20174;&#23567;&#22411;Transformer&#32467;&#26500;&#36880;&#27493;&#25193;&#23637;&#21040;&#22823;&#22411;&#32467;&#26500;&#65292;&#21152;&#24555;&#39044;&#35757;&#32451;&#36827;&#31243;&#12290;&#36825;&#31181;&#28176;&#36827;&#24335;&#25104;&#38271;&#30340;&#20027;&#35201;&#30740;&#31350;&#38382;&#39064;&#26377;&#20004;&#20010;&#65292;&#21363;&#25104;&#38271;&#36827;&#31243;&#21644;&#25104;&#38271;&#25805;&#20316;&#12290;&#23545;&#20110;&#25104;&#38271;&#36827;&#31243;&#65292;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#28145;&#24230;&#21644;&#21069;&#39304;&#23618;&#30340;&#22810;&#38454;&#27573;&#25193;&#23637;&#65292;&#20294;&#27599;&#20010;&#32500;&#24230;&#23545;&#36827;&#31243;&#25928;&#29575;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#32780;&#23545;&#20110;&#25104;&#38271;&#25805;&#20316;&#65292;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#26032;&#26435;&#37325;&#30340;&#21021;&#22987;&#21270;&#26469;&#32487;&#25215;&#21407;&#26377;&#30340;&#30693;&#35782;&#65292;&#21482;&#23454;&#29616;&#20102;&#38750;&#20005;&#26684;&#30340;&#20989;&#25968;&#20445;&#30041;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#30340;&#35757;&#32451;&#21160;&#24577;&#20248;&#21270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#25513;&#30721;&#32467;&#26500;&#25104;&#38271;&#65288;MSG&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#28041;&#21450;&#25152;&#26377;&#21487;&#33021;&#32500;&#24230;&#30340;&#25104;&#38271;&#36827;&#31243;&#21644;&#29420;&#31435;&#20110;&#26032;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#20989;&#25968;&#20005;&#26684;&#20445;&#30041;&#25104;&#38271;&#25805;&#20316;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MSG&#21487;&#26174;&#33879;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acceleration of large language model pre-training is a critical issue in present NLP research. In this paper, we focus on speeding up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems related to progressive growth: growth schedule and growth operator. For growth schedule, existing work has explored multi-stage expansion of depth and feedforward layers. However, the impact of each dimension on the schedule's efficiency is still an open question. For growth operator, existing work relies on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further optimization of training dynamics. To address these issues, we propose Masked Structural Growth (MSG), including growth schedules involving all possible dimensions and strictly function-preserving growth operators that is independent of the initialization of new weights. Experiments show that MSG is signi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2303.16281</link><description>&lt;p&gt;
&#22823;&#35937;&#30340;&#36879;&#35270;&#38236;&#65306;&#35843;&#26597;&#35895;&#27468;&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#30340;&#35821;&#35328;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube. (arXiv:2303.16281v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16281
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35895;&#27468;&#25628;&#32034;&#8220;&#20174;&#22810;&#20010;&#35282;&#24230;&#33719;&#21462;&#20449;&#24687;&#65292;&#20197;&#20415;&#20320;&#21487;&#20197;&#24418;&#25104;&#33258;&#24049;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#8221;&#30340;&#20219;&#21153;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#35895;&#27468;&#21450;&#20854;&#26368;&#31361;&#20986;&#30340;&#25628;&#32034;&#32467;&#26524; - &#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#65292;&#20165;&#21453;&#26144;&#19982;&#8220;&#20315;&#25945;&#8221;&#12289;&#8220;&#33258;&#30001;&#20027;&#20041;&#8221;&#12289;&#8220;&#27542;&#27665;&#21270;&#8221;&#12289;&#8220;&#20234;&#26391;&#8221;&#21644;&#8220;&#32654;&#22269;&#8221;&#31561;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#12290;&#31616;&#21333;&#22320;&#35828;&#65292;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#30456;&#21516;&#25628;&#32034;&#20013;&#65292;&#23427;&#20204;&#20197;&#19981;&#21516;&#31243;&#24230;&#21576;&#29616;&#19981;&#21516;&#30340;&#20449;&#24687;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#35821;&#35328;&#20559;&#35265;&#8221;&#65289;&#65292;&#32780;&#19981;&#26159;&#21576;&#29616;&#22797;&#26434;&#20027;&#39064;&#30340;&#20840;&#29699;&#22270;&#29255;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#25628;&#32034;&#20351;&#25105;&#20204;&#25104;&#20026;&#35866;&#35821;&#20013;&#30340;&#30450;&#20154;&#65292;&#20165;&#35302;&#25720;&#23567;&#35937;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#19981;&#30693;&#36947;&#20854;&#20182;&#25991;&#21270;&#30340;&#35270;&#35282;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#29992;&#20110;&#25628;&#32034;&#30340;&#35821;&#35328;&#26368;&#32456;&#25104;&#20026;&#20419;&#36827;&#26412;&#26063;&#20013;&#24515;&#20027;&#20041;&#35266;&#28857;&#30340;&#25991;&#21270;&#36807;&#28388;&#22120;&#65292;&#20854;&#20013;&#19968;&#20010;&#20154;&#26681;&#25454;&#33258;&#24049;&#30340;&#25991;&#21270;&#35780;&#20272;&#20854;&#20182;&#20154;&#25110;&#24605;&#24819;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;ChatGPT&#20013;&#28145;&#28145;&#23884;&#20837;&#20102;&#35821;&#35328;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrary to Google Search's mission of delivering information from "many angles so you can form your own understanding of the world," we find that Google and its most prominent returned results -- Wikipedia and YouTube, simply reflect the narrow set of cultural stereotypes tied to the search language for complex topics like "Buddhism," "Liberalism," "colonization," "Iran" and "America." Simply stated, they present, to varying degrees, distinct information across the same search in different languages (we call it 'language bias'). Instead of presenting a global picture of a complex topic, our online searches turn us into the proverbial blind person touching a small portion of an elephant, ignorant of the existence of other cultural perspectives. The language we use to search ends up as a cultural filter to promote ethnocentric views, where a person evaluates other people or ideas based on their own culture. We also find that language bias is deeply embedded in ChatGPT. As it is primaril
&lt;/p&gt;</description></item></channel></rss>