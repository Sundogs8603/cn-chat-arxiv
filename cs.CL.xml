<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#24773;&#22659;&#23398;&#20064;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#25552;&#31034;&#65292;&#25104;&#21151;&#25552;&#21319;&#20102;&#23391;&#21152;&#25289;&#35821;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00587</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#24773;&#22659;&#23398;&#20064;&#29992;&#20110;&#23391;&#21152;&#25289;&#35821;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Crosslingual Retrieval Augmented In-context Learning for Bangla. (arXiv:2311.00587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#24773;&#22659;&#23398;&#20064;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#25552;&#31034;&#65292;&#25104;&#21151;&#25552;&#21319;&#20102;&#23391;&#21152;&#25289;&#35821;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28508;&#21147;&#24120;&#24120;&#34987;&#23427;&#20204;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#23391;&#21152;&#25289;&#35821;&#65289;&#20013;&#30340;&#26377;&#38480;&#24615;&#33021;&#25152;&#36974;&#30422;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#24773;&#22659;&#23398;&#20064;&#12290;&#36890;&#36807;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#31574;&#30053;&#24615;&#22320;&#33719;&#21462;&#35821;&#20041;&#30456;&#20284;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#20351;&#24471;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;MPLMs&#65289;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#27169;&#22411;BLOOMZ&#65292;&#33021;&#22815;&#25104;&#21151;&#25552;&#39640;&#23391;&#21152;&#25289;&#35821;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23545;MPLMs&#30340;&#25552;&#21319;&#25928;&#26524;&#31283;&#23450;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The promise of Large Language Models (LLMs) in Natural Language Processing has often been overshadowed by their limited performance in low-resource languages such as Bangla. To address this, our paper presents a pioneering approach that utilizes cross-lingual retrieval augmented in-context learning. By strategically sourcing semantically similar prompts from high-resource language, we enable multilingual pretrained language models (MPLMs), especially the generative model BLOOMZ, to successfully boost performance on Bangla tasks. Our extensive evaluation highlights that the cross-lingual retrieval augmented prompts bring steady improvements to MPLMs over the zero-shot performance.
&lt;/p&gt;</description></item><item><title>ChipNeMo&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#33455;&#29255;&#35774;&#35745;&#20013;&#22823;&#24133;&#25552;&#21319;LLM&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23567;&#20102;&#27169;&#22411;&#23610;&#23544;&#65292;&#22312;&#24037;&#31243;&#21161;&#25163;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#32570;&#38519;&#20998;&#26512;&#31561;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2311.00176</link><description>&lt;p&gt;
ChipNeMo: &#29992;&#20110;&#33455;&#29255;&#35774;&#35745;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;LLMs
&lt;/p&gt;
&lt;p&gt;
ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00176
&lt;/p&gt;
&lt;p&gt;
ChipNeMo&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#33455;&#29255;&#35774;&#35745;&#20013;&#22823;&#24133;&#25552;&#21319;LLM&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23567;&#20102;&#27169;&#22411;&#23610;&#23544;&#65292;&#22312;&#24037;&#31243;&#21161;&#25163;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#32570;&#38519;&#20998;&#26512;&#31561;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChipNeMo&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24037;&#19994;&#33455;&#29255;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#19981;&#30452;&#25509;&#20351;&#29992;&#21830;&#19994;&#25110;&#24320;&#28304;LLMs&#65292;&#32780;&#26159;&#37319;&#29992;&#20197;&#19979;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65306;&#23450;&#21046;&#20998;&#35789;&#22120;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#24102;&#26377;&#39046;&#22495;&#29305;&#23450;&#25351;&#20196;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#26816;&#32034;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#33455;&#29255;&#35774;&#35745;&#30340;&#19977;&#20010;&#36873;&#23450;LLM&#24212;&#29992;&#19978;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#65306;&#24037;&#31243;&#21161;&#25163;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;EDA&#33050;&#26412;&#29983;&#25104;&#20197;&#21450;&#32570;&#38519;&#25688;&#35201;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#20351;LLM&#22312;&#36825;&#19977;&#20010;&#24212;&#29992;&#20013;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#22312;&#21508;&#31181;&#35774;&#35745;&#20219;&#21153;&#19978;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;5&#20493;&#30340;&#27169;&#22411;&#23610;&#23544;&#32553;&#20943;&#65292;&#21516;&#26102;&#20855;&#26377;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#32467;&#26524;&#21644;&#29702;&#24819;&#32467;&#26524;&#20043;&#38388;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#30456;&#20449;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#23558;&#26377;&#21161;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, we instead adopt the following domain adaptation techniques: custom tokenizers, domain-adaptive continued pretraining, supervised fine-tuning (SFT) with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Our results show that these domain adaptation techniques enable significant LLM performance improvements over general-purpose base models across the three evaluated applications, enabling up to 5x model size reduction with similar or better performance on a range of design tasks. Our findings also indicate that there's still room for improvement between our current results and ideal outcomes. We believe that further investigati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#65292;&#20351;AI&#21161;&#25163;&#33021;&#22815;&#33258;&#21160;&#23545;&#40784;&#29992;&#25143;&#20559;&#22909;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;AI&#21161;&#25163;&#22312;&#27169;&#25311;&#20013;&#33021;&#22815;&#20934;&#30830;&#23545;&#40784;&#32463;&#27982;&#25991;&#29486;&#20013;&#30340;&#26631;&#20934;&#31574;&#30053;&#65292;&#20294;&#22312;&#38754;&#23545;&#26410;&#30693;&#36135;&#24065;&#20197;&#21450;&#35821;&#35328;&#19982;&#31574;&#30053;&#19968;&#33268;&#24615;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#23398;&#20064;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.17769</link><description>&lt;p&gt;
&#31038;&#20250;&#22865;&#32422;AI&#65306;&#23558;AI&#21161;&#25163;&#19982;&#38544;&#21547;&#30340;&#32676;&#20307;&#35268;&#33539;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Social Contract AI: Aligning AI Assistants with Implicit Group Norms. (arXiv:2310.17769v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17769
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#65292;&#20351;AI&#21161;&#25163;&#33021;&#22815;&#33258;&#21160;&#23545;&#40784;&#29992;&#25143;&#20559;&#22909;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;AI&#21161;&#25163;&#22312;&#27169;&#25311;&#20013;&#33021;&#22815;&#20934;&#30830;&#23545;&#40784;&#32463;&#27982;&#25991;&#29486;&#20013;&#30340;&#26631;&#20934;&#31574;&#30053;&#65292;&#20294;&#22312;&#38754;&#23545;&#26410;&#30693;&#36135;&#24065;&#20197;&#21450;&#35821;&#35328;&#19982;&#31574;&#30053;&#19968;&#33268;&#24615;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#23398;&#20064;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#36890;&#36807;&#21453;&#36716;&#27169;&#25311;&#29992;&#25143;&#65288;&#26410;&#30693;&#65289;&#20559;&#22909;&#30340;&#27169;&#22411;&#26469;&#23545;&#40784;AI&#21161;&#25163;&#30340;&#24605;&#36335;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#25105;&#20204;&#22312;&#32463;&#27982;&#25253;&#20215;&#28216;&#25103;&#20013;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#27169;&#25311;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24418;&#24335;&#21270;&#20026;&#25351;&#23548;&#27169;&#25311;&#29609;&#23478;&#34892;&#20026;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;AI&#21161;&#25163;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#20854;&#34892;&#20026;&#19982;&#32463;&#27982;&#25991;&#29486;&#20013;&#30340;&#26631;&#20934;&#31574;&#30053;&#65288;&#22914;&#33258;&#31169;&#30340;&#12289;&#21033;&#20182;&#30340;&#65289;&#30456;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#21161;&#25163;&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#38754;&#23545;&#26410;&#21253;&#21547;&#22312;&#35757;&#32451;&#20998;&#24067;&#20013;&#30340;&#36135;&#24065;&#65288;&#22914;&#33647;&#21697;&#20811;&#25968;&#65289;&#26102;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#35821;&#35328;&#20351;&#29992;&#19982;&#26410;&#30693;&#31574;&#30053;&#20043;&#38388;&#23384;&#22312;&#19968;&#33268;&#24615;&#19981;&#36275;&#26102;&#65288;&#22914;&#21033;&#20182;&#31574;&#30053;&#19982;&#31895;&#40065;&#35821;&#35328;&#30456;&#32467;&#21512;&#65289;&#65292;&#21161;&#25163;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#20250;&#20943;&#24930;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21021;&#27493;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24320;&#21457;&#27169;&#25311;&#26694;&#26550;&#26469;&#23545;&#40784;AI&#21161;&#25163;&#30340;&#34892;&#20026;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the idea of aligning an AI assistant by inverting a model of users' (unknown) preferences from observed interactions. To validate our proposal, we run proof-of-concept simulations in the economic ultimatum game, formalizing user preferences as policies that guide the actions of simulated players. We find that the AI assistant accurately aligns its behavior to match standard policies from the economic literature (e.g., selfish, altruistic). However, the assistant's learned policies lack robustness and exhibit limited generalization in an out-of-distribution setting when confronted with a currency (e.g., grams of medicine) that was not included in the assistant's training distribution. Additionally, we find that when there is inconsistency in the relationship between language use and an unknown policy (e.g., an altruistic policy combined with rude language), the assistant's learning of the policy is slowed. Overall, our preliminary results suggest that developing simulation fr
&lt;/p&gt;</description></item><item><title>BianQue&#26159;&#19968;&#31181;&#22522;&#20110;ChatGLM&#36827;&#34892;&#24494;&#35843;&#30340;LLMs&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;LLMs&#30340;&#22810;&#36718;&#25552;&#38382;&#33021;&#21147;&#65292;&#20197;&#24179;&#34913;&#25552;&#38382;&#21644;&#24314;&#35758;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#26356;&#21152;&#20010;&#24615;&#21270;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#20581;&#24247;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2310.15896</link><description>&lt;p&gt;
BianQue: &#29992;ChatGPT&#23545;&#20581;&#24247;LLMs&#36827;&#34892;&#22810;&#36718;&#20250;&#35805;&#20248;&#21270;&#65292;&#24179;&#34913;&#25552;&#38382;&#21644;&#24314;&#35758;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT. (arXiv:2310.15896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15896
&lt;/p&gt;
&lt;p&gt;
BianQue&#26159;&#19968;&#31181;&#22522;&#20110;ChatGLM&#36827;&#34892;&#24494;&#35843;&#30340;LLMs&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;LLMs&#30340;&#22810;&#36718;&#25552;&#38382;&#33021;&#21147;&#65292;&#20197;&#24179;&#34913;&#25552;&#38382;&#21644;&#24314;&#35758;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#26356;&#21152;&#20010;&#24615;&#21270;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#20581;&#24247;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21333;&#36718;&#20250;&#35805;&#20013;&#25552;&#20379;&#24191;&#27867;&#30340;&#20581;&#24247;&#24314;&#35758;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;ChatGPT&#12289;ChatGLM&#12289;ChatDoctor&#12289;DoctorGLM&#31561;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#22312;&#21333;&#36718;&#20013;&#25552;&#20379;&#30340;&#20449;&#24687;&#26377;&#38480;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#24314;&#35758;&#20010;&#24615;&#21270;&#21644;&#38024;&#23545;&#24615;&#19981;&#36275;&#65292;&#38656;&#35201;&#29992;&#25143;&#29420;&#31435;&#36873;&#25321;&#26377;&#29992;&#30340;&#37096;&#20998;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#21442;&#19982;&#22810;&#36718;&#25552;&#38382;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#25913;&#36827;LLMs&#30340;&#22810;&#36718;&#25552;&#38382;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BianQue&#65292;&#19968;&#31181;&#22522;&#20110;ChatGLM&#30340;LLMs&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#24314;&#30340;&#20581;&#24247;&#23545;&#35805;&#25968;&#25454;&#38598;BianQueCorpus&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#36718;&#25552;&#38382;&#21644;&#20581;&#24247;&#24314;&#35758;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have performed well in providing general and extensive health suggestions in single-turn conversations, exemplified by systems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the limited information provided by users during single turn results in inadequate personalization and targeting of the generated suggestions, which requires users to independently select the useful part. It is mainly caused by the missing ability to engage in multi-turn questioning. In real-world medical consultations, doctors usually employ a series of iterative inquiries to comprehend the patient's condition thoroughly, enabling them to provide effective and personalized suggestions subsequently, which can be defined as chain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose BianQue, a ChatGLM-based LLM finetuned with the self-constructed health conversation dataset BianQueCorpus that is consist of multiple turns of questioning and health sugge
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#25216;&#26415;&#25253;&#21578;&#25506;&#35752;&#20102;ChatGPT&#22312;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#19981;&#21516;&#24773;&#32490;&#26631;&#31614;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26377;&#25152;&#24046;&#24322;&#65292;&#31361;&#26174;&#20102;&#20869;&#22312;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#28508;&#22312;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.11753</link><description>&lt;p&gt;
ChatGPT&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Bias in Emotion Recognition with ChatGPT. (arXiv:2310.11753v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11753
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#25216;&#26415;&#25253;&#21578;&#25506;&#35752;&#20102;ChatGPT&#22312;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#19981;&#21516;&#24773;&#32490;&#26631;&#31614;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26377;&#25152;&#24046;&#24322;&#65292;&#31361;&#26174;&#20102;&#20869;&#22312;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#28508;&#22312;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#25506;&#35752;&#20102;ChatGPT&#22312;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#20132;&#20114;&#24335;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#25968;&#25454;&#26631;&#27880;&#21644;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#31561;&#21508;&#31181;&#24212;&#29992;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20102;ChatGPT&#22312;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#20294;&#20854;&#22312;&#26356;&#24494;&#22937;&#30340;&#24773;&#32490;&#35782;&#21035;&#26041;&#38754;&#30340;&#34920;&#29616;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#22312;&#27492;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#23427;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#24773;&#32490;&#26631;&#31614;&#19978;&#30340;&#24773;&#32490;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20854;&#24615;&#33021;&#20855;&#26377;&#21512;&#29702;&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#24494;&#35843;&#21487;&#20197;&#26126;&#26174;&#25913;&#21892;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#22312;&#19981;&#21516;&#24773;&#32490;&#26631;&#31614;&#21644;&#25968;&#25454;&#38598;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#31361;&#26174;&#20102;&#20869;&#22312;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#28508;&#22312;&#30340;&#20559;&#24046;&#12290;&#25968;&#25454;&#38598;&#21644;&#24773;&#32490;&#26631;&#31614;&#30340;&#36873;&#25321;&#23545;ChatGPT&#30340;&#24773;&#32490;&#35782;&#21035;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#25991;&#35770;&#36848;&#20102;&#25968;&#25454;&#38598;&#21644;&#26631;&#31614;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#36890;&#36807;&#24494;&#35843;&#25552;&#21319;ChatGPT&#24773;&#32490;&#35782;&#21035;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report explores the ability of ChatGPT in recognizing emotions from text, which can be the basis of various applications like interactive chatbots, data annotation, and mental health analysis. While prior research has shown ChatGPT's basic ability in sentiment analysis, its performance in more nuanced emotion recognition is not yet explored. Here, we conducted experiments to evaluate its performance of emotion recognition across different datasets and emotion labels. Our findings indicate a reasonable level of reproducibility in its performance, with noticeable improvement through fine-tuning. However, the performance varies with different emotion labels and datasets, highlighting an inherent instability and possible bias. The choice of dataset and emotion labels significantly impacts ChatGPT's emotion recognition performance. This paper sheds light on the importance of dataset and label selection, and the potential of fine-tuning in enhancing ChatGPT's emotion recogniti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;OpenAI&#30340;&#26368;&#26032;&#27169;&#22411;GPT-4V&#22312;&#22810;&#27169;&#24577;&#21307;&#23398;&#35786;&#26029;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25104;&#20687;&#27169;&#24577;&#21644;&#35299;&#21078;&#23398;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.09909</link><description>&lt;p&gt;
GPT-4V(ision)&#33021;&#22815;&#29992;&#20110;&#21307;&#30103;&#24212;&#29992;&#21527;&#65311;GPT-4V&#22312;&#22810;&#27169;&#24577;&#21307;&#23398;&#35786;&#26029;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis. (arXiv:2310.09909v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;OpenAI&#30340;&#26368;&#26032;&#27169;&#22411;GPT-4V&#22312;&#22810;&#27169;&#24577;&#21307;&#23398;&#35786;&#26029;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25104;&#20687;&#27169;&#24577;&#21644;&#35299;&#21078;&#23398;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#30001;&#20110;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#25512;&#21160;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#27493;&#65292;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;OpenAI&#26368;&#26032;&#27169;&#22411;GPT-4V(ision)&#22312;&#22810;&#27169;&#24577;&#21307;&#23398;&#35786;&#26029;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;17&#20010;&#20154;&#20307;&#31995;&#32479;&#65292;&#21253;&#25324;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#12289;&#22836;&#39048;&#37096;&#12289;&#24515;&#33039;&#12289;&#33016;&#37096;&#12289;&#34880;&#28082;&#23398;&#12289;&#32925;&#32966;&#12289;&#32963;&#32928;&#12289;&#27852;&#23615;&#29983;&#27542;&#12289;&#22919;&#31185;&#12289;&#20135;&#31185;&#12289;&#20083;&#33146;&#12289;&#32908;&#32905;&#39592;&#39612;&#12289;&#33034;&#26609;&#12289;&#34880;&#31649;&#12289;&#32959;&#30244;&#12289;&#21019;&#20260;&#12289;&#20799;&#31185;&#65292;&#20197;&#21450;&#20174;8&#31181;&#26085;&#24120;&#20020;&#24202;&#24120;&#29992;&#30340;&#25104;&#20687;&#27169;&#24577;&#33719;&#24471;&#30340;&#22270;&#20687;&#65292;&#20363;&#22914;X&#20809;&#12289;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;(CT)&#12289;&#30913;&#20849;&#25391;&#25104;&#20687;(MRI)&#12289;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;(PET)&#12289;&#25968;&#23383;&#20943;&#24433;&#34880;&#31649;&#36896;&#24433;(DSA)&#12289;&#20083;&#33146;X&#20809;&#25668;&#24433;&#26415;&#12289;&#36229;&#22768;&#21644;&#30149;&#29702;&#23398;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;GPT-4V&#22312;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25104;&#20687;&#27169;&#24577;&#21644;&#35299;&#21078;&#23398;&#35782;&#21035;&#65292;&#26080;&#35770;&#26159;&#21542;&#25552;&#20379;&#19987;&#21033;&#21382;&#21490;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by the large foundation models, the development of artificial intelligence has witnessed tremendous progress lately, leading to a surge of general interest from the public. In this study, we aim to assess the performance of OpenAI's newest model, GPT-4V(ision), specifically in the realm of multimodal medical diagnosis. Our evaluation encompasses 17 human body systems, including Central Nervous System, Head and Neck, Cardiac, Chest, Hematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology, Obstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma, Pediatrics, with images taken from 8 modalities used in daily clinic routine, e.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Digital Subtraction Angiography (DSA), Mammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on multiple clinical tasks with or without patent history provided, including imaging modality and anatomy recognition, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#26469;&#23637;&#29616;&#36890;&#36807;&#35821;&#35328;&#39118;&#26684;&#21644;&#35789;&#27719;&#20869;&#23481;&#26469;&#20307;&#29616;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.09219</link><description>&lt;p&gt;
"&#20975;&#21033;&#26159;&#19968;&#20010;&#28201;&#26262;&#30340;&#20154;&#65292;&#32422;&#29791;&#22827;&#26159;&#19968;&#20010;&#27036;&#26679;": LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#26469;&#23637;&#29616;&#36890;&#36807;&#35821;&#35328;&#39118;&#26684;&#21644;&#35789;&#27719;&#20869;&#23481;&#26469;&#20307;&#29616;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#29992;&#25143;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21327;&#21161;&#25776;&#20889;&#21508;&#31181;&#31867;&#22411;&#30340;&#20869;&#23481;&#65292;&#21253;&#25324;&#25512;&#33616;&#20449;&#31561;&#32844;&#19994;&#25991;&#20214;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#26041;&#20415;&#24615;&#65292;&#20294;&#36825;&#20123;&#24212;&#29992;&#24341;&#20837;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#30001;&#20110;&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#21487;&#33021;&#34987;&#29992;&#25143;&#30452;&#25509;&#22312;&#32844;&#19994;&#25110;&#23398;&#26415;&#22330;&#26223;&#20013;&#20351;&#29992;&#65292;&#23427;&#20204;&#26377;&#21487;&#33021;&#36896;&#25104;&#30452;&#25509;&#30340;&#31038;&#20250;&#20260;&#23475;&#65292;&#22914;&#38477;&#20302;&#22899;&#24615;&#30003;&#35831;&#32773;&#30340;&#25104;&#21151;&#29575;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#26410;&#26469;&#30340;&#32531;&#35299;&#21644;&#30417;&#25511;&#65292;&#20840;&#38754;&#30740;&#31350;&#27492;&#31867;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#21644;&#30456;&#20851;&#20260;&#23475;&#21183;&#22312;&#24517;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#30740;&#31350;&#12290;&#21463;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#32500;&#24230;&#26469;&#23637;&#29616;LLM&#29983;&#25104;&#30340;&#20449;&#20214;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65306;&#35821;&#35328;&#39118;&#26684;&#30340;&#20559;&#35265;&#21644;&#35789;&#27719;&#20869;&#23481;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25512;&#33616;&#20449;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative language models advance, users have started to utilize Large Language Models (LLMs) to assist in writing various types of content, including professional documents such as recommendation letters. Despite their convenience, these applications introduce unprecedented fairness concerns. As generated reference letters might be directly utilized by users in professional or academic scenarios, they have the potential to cause direct social harms, such as lowering success rates for female applicants. Therefore, it is imminent and necessary to comprehensively study fairness issues and associated harms in such real-world use cases for future mitigation and monitoring. In this paper, we critically examine gender bias in LLM-generated reference letters. Inspired by findings in social science, we design evaluation methods to manifest gender biases in LLM-generated letters through 2 dimensions: biases in language style and biases in lexical content. Furthermore, we investigate the ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#20250;&#23548;&#33268;&#23545;&#26550;&#26500;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#32780;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23558;Transformers&#19982;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#21040;&#24456;&#23567;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#30340;&#24615;&#33021;&#19982;S4&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#22312;PathX-256&#20219;&#21153;&#19978;&#25913;&#36827;&#20102;SSMs&#30340;&#26368;&#20339;&#32467;&#26524;20&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02980</link><description>&lt;p&gt;
&#27704;&#36828;&#19981;&#35201;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65306;&#20844;&#27491;&#27604;&#36739;&#38271;&#24207;&#21015;&#27169;&#22411;&#38656;&#35201;&#25968;&#25454;&#39537;&#21160;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. (arXiv:2310.02980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#20250;&#23548;&#33268;&#23545;&#26550;&#26500;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#32780;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23558;Transformers&#19982;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#21040;&#24456;&#23567;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#30340;&#24615;&#33021;&#19982;S4&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#22312;PathX-256&#20219;&#21153;&#19978;&#25913;&#36827;&#20102;SSMs&#30340;&#26368;&#20339;&#32467;&#26524;20&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30446;&#26631;&#65292;&#24182;&#23548;&#33268;&#20102;&#19968;&#20123;&#26550;&#26500;&#65292;&#22914;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#27604;Transformers&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#24615;&#36827;&#23637;&#20027;&#35201;&#26159;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#24182;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#24207;&#21015;&#30340;&#30446;&#26631;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#30340;&#22522;&#20934;&#27979;&#35797;&#65288;&#20363;&#22914;Long Range Arena&#65289;&#19978;&#23637;&#31034;&#20986;&#26469;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#26426;&#21021;&#22987;&#21270;&#23548;&#33268;&#23545;&#26550;&#26500;&#20043;&#38388;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#24182;&#19988;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#65288;&#20165;&#20351;&#29992;&#19979;&#28216;&#20219;&#21153;&#25968;&#25454;&#65289;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;Transformers&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#20043;&#38388;&#24471;&#21040;&#24456;&#23567;&#30340;&#24046;&#36317;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#19982;S4&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#25105;&#20204;&#22312;PathX-256&#20219;&#21153;&#19978;&#23558;SSMs&#30340;&#26368;&#20339;&#25253;&#21578;&#32467;&#26524;&#25552;&#39640;&#20102;20&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $\textit{only the downstream task data}$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 
&lt;/p&gt;</description></item><item><title>GPT-Fathom&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#22871;&#20214;&#65292;&#23427;&#31995;&#32479;&#35780;&#20272;&#20102;10&#22810;&#20010;&#20027;&#35201;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20174;GPT-3&#21040;GPT-4&#28436;&#21270;&#36335;&#24452;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.16583</link><description>&lt;p&gt;
GPT-Fathom&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#35299;&#26512;GPT-4&#21450;&#20854;&#21518;&#32493;&#29256;&#26412;&#30340;&#28436;&#21270;&#36335;&#24452;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond. (arXiv:2309.16583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16583
&lt;/p&gt;
&lt;p&gt;
GPT-Fathom&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#22871;&#20214;&#65292;&#23427;&#31995;&#32479;&#35780;&#20272;&#20102;10&#22810;&#20010;&#20027;&#35201;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20174;GPT-3&#21040;GPT-4&#28436;&#21270;&#36335;&#24452;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#20154;&#20204;&#36843;&#20999;&#38656;&#35201;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#22871;&#20214;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#29616;&#26377;&#30340;LLM&#25490;&#34892;&#27036;&#36890;&#24120;&#21442;&#32771;&#20854;&#20182;&#35770;&#25991;&#20013;&#25253;&#21578;&#30340;&#24471;&#20998;&#65292;&#35774;&#32622;&#21644;&#25552;&#31034;&#19981;&#19968;&#33268;&#65292;&#36825;&#21487;&#33021;&#26080;&#24847;&#38388;&#40723;&#21169;&#36873;&#25321;&#26377;&#21033;&#30340;&#35774;&#32622;&#21644;&#25552;&#31034;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GPT-Fathom&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;OpenAI Evals&#26500;&#24314;&#30340;&#24320;&#28304;&#21644;&#21487;&#37325;&#22797;&#30340;LLM&#35780;&#20272;&#22871;&#20214;&#12290;&#25105;&#20204;&#22312;&#23545;&#40784;&#30340;&#29615;&#22659;&#35774;&#32622;&#19979;&#31995;&#32479;&#35780;&#20272;&#20102;10&#22810;&#20010;&#20027;&#35201;&#30340;LLMs&#20197;&#21450;OpenAI&#30340;&#20256;&#32479;&#27169;&#22411;&#22312;20&#22810;&#20010;&#31934;&#36873;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#65292;&#28085;&#30422;&#20102;7&#20010;&#33021;&#21147;&#31867;&#21035;&#12290;&#25105;&#20204;&#23545;OpenAI&#26089;&#26399;&#27169;&#22411;&#30340;&#22238;&#39038;&#24615;&#30740;&#31350;&#20026;&#25105;&#20204;&#25581;&#31034;&#20102;&#20174;GPT-3&#21040;GPT-4&#30340;&#28436;&#21270;&#36335;&#24452;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#30446;&#21069;&#65292;&#31038;&#21306;&#28212;&#26395;&#20102;&#35299;GPT-3&#22914;&#20309;&#36880;&#27493;&#25913;&#36827;&#21040;GPT-4&#65292;&#21253;&#25324;&#20687;&#28155;&#21152;&#20195;&#30721;&#25968;&#25454;&#26159;&#21542;&#25552;&#39640;&#20102;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;LLM&#33021;&#21147;&#30340;&#21738;&#20123;&#26041;&#38754;&#31561;&#25216;&#26415;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid advancement of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+ leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retrospective study on OpenAI's earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM's reasoning capability, which aspects of LLM capabili
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#22270;&#34920;&#31034;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#25991;&#26723;&#20998;&#35299;&#20026;&#35821;&#20041;&#26694;&#26550;&#65292;&#28982;&#21518;&#20351;&#29992;&#27492;&#20013;&#38388;&#31232;&#30095;&#26684;&#24335;&#29983;&#25104;&#25991;&#26412;&#12290;&#36890;&#36807;&#25200;&#21160;&#26694;&#26550;&#20869;&#23481;&#65292;&#21253;&#25324;&#25299;&#25169;&#20998;&#26512;&#25366;&#25496;&#26032;&#30340;&#36229;&#36793;&#20197;&#21450;&#21253;&#21547;&#23618;&#27425;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#30340;&#22797;&#26434;&#22810;&#20803;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#30340;&#25991;&#26723;&#22312;&#26679;&#24335;&#12289;&#24773;&#24863;&#12289;&#26684;&#24335;&#12289;&#26500;&#25104;&#21644;&#20107;&#23454;&#19978;&#26159;&#22810;&#26679;&#30340;&#12289;&#36830;&#36143;&#30340;&#21644;&#21464;&#21270;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.06550</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#22270;&#34920;&#31034;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Synthetic Text Generation using Hypergraph Representations. (arXiv:2309.06550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#22270;&#34920;&#31034;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#25991;&#26723;&#20998;&#35299;&#20026;&#35821;&#20041;&#26694;&#26550;&#65292;&#28982;&#21518;&#20351;&#29992;&#27492;&#20013;&#38388;&#31232;&#30095;&#26684;&#24335;&#29983;&#25104;&#25991;&#26412;&#12290;&#36890;&#36807;&#25200;&#21160;&#26694;&#26550;&#20869;&#23481;&#65292;&#21253;&#25324;&#25299;&#25169;&#20998;&#26512;&#25366;&#25496;&#26032;&#30340;&#36229;&#36793;&#20197;&#21450;&#21253;&#21547;&#23618;&#27425;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#30340;&#22797;&#26434;&#22810;&#20803;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#30340;&#25991;&#26723;&#22312;&#26679;&#24335;&#12289;&#24773;&#24863;&#12289;&#26684;&#24335;&#12289;&#26500;&#25104;&#21644;&#20107;&#23454;&#19978;&#26159;&#22810;&#26679;&#30340;&#12289;&#36830;&#36143;&#30340;&#21644;&#21464;&#21270;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25991;&#26723;&#30340;&#21512;&#25104;&#21464;&#20307;&#36890;&#24120;&#34987;&#35270;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#23558;&#25991;&#26723;&#20998;&#35299;&#20026;&#35821;&#20041;&#26694;&#26550;&#65292;&#28982;&#21518;&#20351;&#29992;&#27492;&#20013;&#38388;&#31232;&#30095;&#26684;&#24335;&#29983;&#25104;&#25991;&#26412;&#12290;&#36825;&#20123;&#26694;&#26550;&#20351;&#29992;&#36229;&#22270;&#36827;&#34892;&#24314;&#27169;&#65292;&#21487;&#20197;&#20197;&#24688;&#24403;&#30340;&#26041;&#24335;&#25200;&#21160;&#26694;&#26550;&#20869;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#25299;&#25169;&#20998;&#26512;&#25366;&#25496;&#26032;&#30340;&#36229;&#36793;&#65292;&#21253;&#25324;&#23618;&#27425;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#30340;&#22797;&#26434;&#22810;&#20803;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#30340;&#25991;&#26723;&#22312;&#26679;&#24335;&#12289;&#24773;&#24863;&#12289;&#26684;&#24335;&#12289;&#26500;&#25104;&#21644;&#20107;&#23454;&#19978;&#26159;&#22810;&#26679;&#30340;&#12289;&#36830;&#36143;&#30340;&#21644;&#21464;&#21270;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating synthetic variants of a document is often posed as text-to-text transformation. We propose an alternate LLM based method that first decomposes a document into semantic frames and then generates text using this interim sparse format. The frames are modeled using a hypergraph, which allows perturbing the frame contents in a principled manner. Specifically, new hyperedges are mined through topological analysis and complex polyadic relationships including hierarchy and temporal dynamics are accommodated. We show that our solution generates documents that are diverse, coherent and vary in style, sentiment, format, composition and facts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#35813;&#22871;&#20214;&#21253;&#25324;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.03886</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#21151;&#33021;&#35299;&#37322;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Function Interpretation Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#35813;&#22871;&#20214;&#21253;&#25324;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21487;&#35835;&#30340;&#25551;&#36848;&#26631;&#35760;&#31070;&#32463;&#32593;&#32476;&#23376;&#27169;&#22359;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65306;&#36825;&#20123;&#25551;&#36848;&#21487;&#20197;&#26292;&#38706;&#22833;&#36133;&#12289;&#24341;&#23548;&#24178;&#39044;&#65292;&#29978;&#33267;&#21487;&#20197;&#35299;&#37322;&#37325;&#35201;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#26426;&#26800;&#21407;&#29702;&#30340;&#24050;&#35757;&#32451;&#32593;&#32476;&#25551;&#36848;&#37117;&#28041;&#21450;&#21040;&#23567;&#27169;&#22411;&#12289;&#29421;&#20041;&#29616;&#35937;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#12290;&#22312;&#19981;&#26029;&#22686;&#21152;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#20013;&#26631;&#35760;&#20986;&#25152;&#26377;&#20154;&#21487;&#35299;&#37322;&#30340;&#23376;&#35745;&#31639;&#20960;&#20046;&#32943;&#23450;&#38656;&#35201;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#39564;&#35777;&#25551;&#36848;&#30340;&#24037;&#20855;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#26631;&#35760;&#30340;&#25216;&#26415;&#24320;&#22987;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#26377;&#38480;&#19988;&#20020;&#26102;&#12290;&#25105;&#20204;&#24212;&#35813;&#22914;&#20309;&#39564;&#35777;&#21644;&#27604;&#36739;&#24320;&#25918;&#24335;&#26631;&#35760;&#24037;&#20855;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;FIND&#65288;&#20989;&#25968;&#35299;&#37322;&#21644;&#25551;&#36848;&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#26041;&#27861;&#26500;&#24314;&#27169;&#22359;&#30340;&#22522;&#20934;&#22871;&#20214;&#12290;FIND&#21253;&#21547;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#30340;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of tr
&lt;/p&gt;</description></item><item><title>BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16458</link><description>&lt;p&gt;
BioCoder: &#19968;&#31181;&#24102;&#26377;&#19978;&#19979;&#25991;&#35821;&#29992;&#30693;&#35782;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16458
&lt;/p&gt;
&lt;p&gt;
BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#20195;&#30721;&#29983;&#25104;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#36755;&#20986;&#26469;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#65292;&#29983;&#25104;&#21151;&#33021;&#31243;&#24207;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#37327;&#22823;&#12289;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25805;&#20316;&#21644;&#22797;&#26434;&#30340;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#32780;&#38754;&#20020;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BioCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#19982;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#26377;&#20851;&#65292;BioCoder&#28085;&#30422;&#20102;&#21487;&#33021;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;GitHub&#30340;1026&#20010;Python&#21644;Java&#20989;&#25968;&#21644;1243&#20010;&#26041;&#27861;&#65292;&#20197;&#21450;&#26469;&#33258;Rosalind&#39033;&#30446;&#30340;253&#20010;&#31034;&#20363;&#12290;BioCoder&#36824;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#25105;&#20204;&#24050;&#32463;&#24212;&#29992;&#23427;&#26469;&#35780;&#20272;&#35768;&#22810;&#27169;&#22411;&#65292;&#21253;&#25324;InCoder&#12289;CodeGen&#12289;CodeGen2&#12289;SantaCoder&#12289;StarCoder&#12289;StarCoder+&#12289;InstructCodeT&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#28040;&#36153;&#32773;&#25237;&#35785;&#21465;&#36848;&#20013;&#30340;&#31995;&#32479;&#24322;&#24120;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#20998;&#31867;&#31639;&#27861;&#23545;&#20110;&#36739;&#23567;&#19988;&#39057;&#32321;&#20986;&#29616;&#30340;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#25237;&#35785;&#21465;&#36848;&#36716;&#21270;&#20026;&#23450;&#37327;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.11138</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#28040;&#36153;&#32773;&#25237;&#35785;&#21465;&#36848;&#20013;&#31995;&#32479;&#24322;&#24120;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NLP-based detection of systematic anomalies among the narratives of consumer complaints. (arXiv:2308.11138v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#28040;&#36153;&#32773;&#25237;&#35785;&#21465;&#36848;&#20013;&#30340;&#31995;&#32479;&#24322;&#24120;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#20998;&#31867;&#31639;&#27861;&#23545;&#20110;&#36739;&#23567;&#19988;&#39057;&#32321;&#20986;&#29616;&#30340;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#25237;&#35785;&#21465;&#36848;&#36716;&#21270;&#20026;&#23450;&#37327;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#25237;&#35785;&#21465;&#36848;&#20013;&#30340;&#31995;&#32479;&#24322;&#24120;&#65292;&#31616;&#31216;&#20026;&#31995;&#32479;&#24322;&#24120;&#12290;&#23613;&#31649;&#20998;&#31867;&#31639;&#27861;&#34987;&#29992;&#20110;&#26816;&#27979;&#26126;&#26174;&#30340;&#24322;&#24120;&#65292;&#20294;&#22312;&#36739;&#23567;&#19988;&#39057;&#32321;&#20986;&#29616;&#30340;&#31995;&#32479;&#24322;&#24120;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#21487;&#33021;&#20250;&#22240;&#20026;&#21508;&#31181;&#21407;&#22240;&#32780;&#22833;&#25928;&#65292;&#21253;&#25324;&#25216;&#26415;&#21407;&#22240;&#21644;&#20154;&#24037;&#20998;&#26512;&#24072;&#30340;&#33258;&#28982;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#22312;&#20998;&#31867;&#20043;&#21518;&#30340;&#19979;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#23558;&#25237;&#35785;&#21465;&#36848;&#36716;&#21270;&#20026;&#23450;&#37327;&#25968;&#25454;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#31639;&#27861;&#26469;&#26816;&#27979;&#31995;&#32479;&#24322;&#24120;&#12290;&#25105;&#20204;&#20351;&#29992;&#28040;&#36153;&#32773;&#37329;&#34701;&#20445;&#25252;&#23616;&#30340;&#28040;&#36153;&#32773;&#25237;&#35785;&#25968;&#25454;&#24211;&#20013;&#30340;&#25237;&#35785;&#21465;&#36848;&#26469;&#35828;&#26126;&#25972;&#20010;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop an NLP-based procedure for detecting systematic nonmeritorious consumer complaints, simply called systematic anomalies, among complaint narratives. While classification algorithms are used to detect pronounced anomalies, in the case of smaller and frequent systematic anomalies, the algorithms may falter due to a variety of reasons, including technical ones as well as natural limitations of human analysts. Therefore, as the next step after classification, we convert the complaint narratives into quantitative data, which are then analyzed using an algorithm for detecting systematic anomalies. We illustrate the entire procedure using complaint narratives from the Consumer Complaint Database of the Consumer Financial Protection Bureau.
&lt;/p&gt;</description></item><item><title>WeaverBird&#26159;&#19968;&#20010;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;&#26234;&#33021;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#26412;&#22320;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#65292;&#33021;&#22815;&#29702;&#35299;&#22797;&#26434;&#30340;&#37329;&#34701;&#26597;&#35810;&#24182;&#25552;&#20379;&#26126;&#26234;&#30340;&#22238;&#31572;&#65292;&#20855;&#26377;&#22686;&#24378;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.05361</link><description>&lt;p&gt;
WeaverBird: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#22686;&#24378;&#37329;&#34701;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine. (arXiv:2308.05361v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05361
&lt;/p&gt;
&lt;p&gt;
WeaverBird&#26159;&#19968;&#20010;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;&#26234;&#33021;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#26412;&#22320;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#65292;&#33021;&#22815;&#29702;&#35299;&#22797;&#26434;&#30340;&#37329;&#34701;&#26597;&#35810;&#24182;&#25552;&#20379;&#26126;&#26234;&#30340;&#22238;&#31572;&#65292;&#20855;&#26377;&#22686;&#24378;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;WeaverBird&#65292;&#19968;&#20010;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;&#26234;&#33021;&#23545;&#35805;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21033;&#29992;GPT&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#37329;&#34701;&#30456;&#20851;&#25991;&#26412;&#30340;&#24191;&#27867;&#35821;&#26009;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#29702;&#35299;&#22797;&#26434;&#30340;&#37329;&#34701;&#26597;&#35810;&#65292;&#20363;&#22914;&#8220;&#22312;&#36890;&#36135;&#33192;&#32960;&#26399;&#38388;&#22914;&#20309;&#31649;&#29702;&#25105;&#30340;&#25237;&#36164;&#65311;&#8221;&#24182;&#25552;&#20379;&#26126;&#26234;&#30340;&#22238;&#31572;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#38598;&#25104;&#20102;&#26412;&#22320;&#30340;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#20197;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#12290;&#26368;&#32456;&#30340;&#22238;&#31572;&#26159;&#22522;&#20110;&#25628;&#32034;&#32467;&#26524;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#30340;&#65292;&#24182;&#21253;&#21547;&#36866;&#24403;&#30340;&#24341;&#29992;&#26469;&#28304;&#65292;&#20174;&#32780;&#20855;&#26377;&#22686;&#24378;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#19982;&#37329;&#34701;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24050;&#32463;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#30456;&#27604;&#20854;&#20182;&#27169;&#22411;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#29992;&#25143;&#21487;&#20197;&#22312;&#25105;&#20204;&#30340;&#22312;&#32447;&#28436;&#31034;&#32593;&#31449;https://weaverbird.ttic.edu&#19982;&#25105;&#20204;&#30340;&#31995;&#32479;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#35266;&#30475;&#25105;&#20204;&#30340;2&#20998;&#38047;&#28436;&#31034;&#35270;&#39057;https://www.youtube.com/watch?v=yofgeq&#12290;
&lt;/p&gt;
&lt;p&gt;
We present WeaverBird, an intelligent dialogue system designed specifically for the finance domain. Our system harnesses a large language model of GPT architecture that has been tuned using extensive corpora of finance-related text. As a result, our system possesses the capability to understand complex financial queries, such as "How should I manage my investments during inflation?", and provide informed responses. Furthermore, our system incorporates a local knowledge base and a search engine to retrieve relevant information. The final responses are conditioned on the search results and include proper citations to the sources, thus enjoying an enhanced credibility. Through a range of finance-related questions, we have demonstrated the superior performance of our system compared to other models. To experience our system firsthand, users can interact with our live demo at https://weaverbird.ttic.edu, as well as watch our 2-min video illustration at https://www.youtube.com/watch?v=yofgeq
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#36716;&#25442;&#27969;&#27700;&#32447;&#65292;&#20197;&#35782;&#21035;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2308.01987</link><description>&lt;p&gt;
&#23391;&#21152;&#25289;&#35821;&#20551;&#35780;&#35770;&#65306;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Bengali Fake Reviews: A Benchmark Dataset and Detection System. (arXiv:2308.01987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01987
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#36716;&#25442;&#27969;&#27700;&#32447;&#65292;&#20197;&#35782;&#21035;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#22312;&#32447;&#24179;&#21488;&#19978;&#20986;&#29616;&#22823;&#37327;&#30340;&#20551;&#35780;&#35770;&#24050;&#32463;&#25104;&#20026;&#28040;&#36153;&#32773;&#21644;&#20225;&#19994;&#30340;&#37325;&#22823;&#20851;&#20999;&#12290;&#36825;&#26679;&#30340;&#35780;&#35770;&#21487;&#20197;&#27450;&#39575;&#28040;&#36153;&#32773;&#65292;&#24182;&#23545;&#20135;&#21697;&#25110;&#26381;&#21153;&#30340;&#22768;&#35465;&#36896;&#25104;&#25439;&#23475;&#65292;&#22240;&#27492;&#35782;&#21035;&#23427;&#20204;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22312;&#33521;&#35821;&#35821;&#35328;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#20551;&#35780;&#35770;&#30340;&#26816;&#27979;&#65292;&#20294;&#22312;&#23391;&#21152;&#25289;&#35821;&#31561;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#26816;&#27979;&#20551;&#35780;&#35770;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#23545;&#26410;&#24320;&#21457;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#26816;&#27979;&#65288;BFRD&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#29992;&#20110;&#35782;&#21035;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#20174;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25910;&#38598;&#21040;&#30340;7710&#26465;&#38750;&#20551;&#21644;1339&#26465;&#20551;&#30340;&#19982;&#39135;&#21697;&#30456;&#20851;&#30340;&#35780;&#35770;&#32452;&#25104;&#12290;&#20026;&#20102;&#23558;&#35780;&#35770;&#20013;&#30340;&#38750;&#23391;&#21152;&#25289;&#35789;&#35821;&#36716;&#25442;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#27969;&#27700;&#32447;&#65292;&#23558;&#33521;&#35821;&#21333;&#35789;&#36716;&#25442;&#20026;&#20854;&#23545;&#24212;&#30340;&#23391;&#21152;&#25289;&#24847;&#20041;&#65292;&#21516;&#26102;&#20063;&#23558;&#32599;&#39532;&#21270;&#30340;&#23391;&#21152;&#25289;&#35821;&#22238;&#38899;&#21040;&#23391;&#21152;&#25289;&#35821;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of fake reviews on various online platforms has created a major concern for both consumers and businesses. Such reviews can deceive customers and cause damage to the reputation of products or services, making it crucial to identify them. Although the detection of fake reviews has been extensively studied in English language, detecting fake reviews in non-English languages such as Bengali is still a relatively unexplored research area. This paper introduces the Bengali Fake Review Detection (BFRD) dataset, the first publicly available dataset for identifying fake reviews in Bengali. The dataset consists of 7710 non-fake and 1339 fake food-related reviews collected from social media posts. To convert non-Bengali words in a review, a unique pipeline has been proposed that translates English words to their corresponding Bengali meaning and also back transliterates Romanized Bengali to Bengali. We have conducted rigorous experimentation using multiple deep learning and pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#65288;DLN&#65289;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21472;&#21152;&#30340;&#35821;&#35328;&#27169;&#22411;&#23618;&#65288;LLMs&#65289;&#65292;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#35757;&#32451;&#65292;&#20351;&#24471;DLN-2&#30340;&#24615;&#33021;&#29978;&#33267;&#21487;&#20197;&#19982;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;GPT-4&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.12509</link><description>&lt;p&gt;
&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#65306;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#32852;&#21512;&#35757;&#32451;&#21472;&#21152;LLM&#30340;&#25552;&#31034;&#23618;
&lt;/p&gt;
&lt;p&gt;
Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference. (arXiv:2306.12509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#65288;DLN&#65289;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21472;&#21152;&#30340;&#35821;&#35328;&#27169;&#22411;&#23618;&#65288;LLMs&#65289;&#65292;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#35757;&#32451;&#65292;&#20351;&#24471;DLN-2&#30340;&#24615;&#33021;&#29978;&#33267;&#21487;&#20197;&#19982;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;GPT-4&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35270;&#20026;&#32593;&#32476;&#20013;&#30340;&#38543;&#26426;&#8220;&#35821;&#35328;&#23618;&#8221;&#65292;&#20854;&#20013;&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#26159;&#27599;&#20010;&#23618;&#30340;&#33258;&#28982;&#35821;&#35328;&#8220;&#25552;&#31034;&#8221;&#12290;&#25105;&#20204;&#23558;&#20004;&#20010;&#36825;&#26679;&#30340;&#23618;&#21472;&#21152;&#22312;&#19968;&#36215;&#65292;&#23558;&#19968;&#20010;&#23618;&#30340;&#36755;&#20986;&#39304;&#36865;&#21040;&#19979;&#19968;&#20010;&#23618;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#22534;&#21472;&#30340;&#32467;&#26500;&#31216;&#20026;&#8220;&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#8221;&#65288;DLN&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#26377;&#25928;&#22320;&#38024;&#23545;&#21333;&#23618;&#35821;&#35328;&#32593;&#32476;&#65288;DLN-1&#65289;&#25191;&#34892;&#25552;&#31034;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#35757;&#32451;2&#23618;DLNs&#65288;DLN-2&#65289;&#65292;&#20854;&#20013;&#24517;&#39035;&#23398;&#20064;&#20004;&#20010;&#25552;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#31532;&#19968;&#23618;&#30340;&#36755;&#20986;&#26159;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#65292;&#38656;&#35201;&#36827;&#34892;&#36793;&#32536;&#21270;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#32852;&#21512;&#25552;&#31034;&#35757;&#32451;&#30340;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#12290;DLN-2&#27604;&#21333;&#23618;&#36798;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#21363;&#20351;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;LLM&#26356;&#23567;&#19988;&#26356;&#24369;&#65292;&#20063;&#21487;&#20197;&#19982;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;GPT-4&#30456;&#23218;&#32654;&#12290;DLN&#20195;&#30721;&#26159;&#24320;&#28304;&#30340;&#65306;https://github.com/microsoft/deep-language-networks&#12290;
&lt;/p&gt;
&lt;p&gt;
We view large language models (LLMs) as stochastic \emph{language layers} in a network, where the learnable parameters are the natural language \emph{prompts} at each layer. We stack two such layers, feeding the output of one layer to the next. We call the stacked architecture a \emph{Deep Language Network} (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs (DLN-2), where two prompts must be learnt. We consider the output of the first layer as a latent variable to marginalize, and devise a variational inference algorithm for joint prompt training. A DLN-2 reaches higher performance than a single layer, sometimes comparable to few-shot GPT-4 even when each LLM in the network is smaller and less powerful. The DLN code is open source: https://github.com/microsoft/deep-language-networks .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11300</link><description>&lt;p&gt;
RS5M&#65306;&#29992;&#20110;&#36965;&#24863;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model. (arXiv:2306.11300v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#21033;&#29992;&#24050;&#26377;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#22495;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#36801;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#24357;&#21512;&#20102;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#65288;RS&#65289;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;500&#19975;&#24352;&#24102;&#26377;&#33521;&#25991;&#25551;&#36848;&#30340;RS&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#20165;&#24102;&#26631;&#31614;&#30340;RS&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Vision-Language Foundation Models utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain Foundation Model (DFM), bridging the gap between the General Foundation Model (GFM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07848</link><description>&lt;p&gt;
GEmo-CLAP: &#38754;&#21521;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;CLAP&#65289;&#26368;&#36817;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEmo-CLAP&#30340;&#39640;&#25928;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;CLAP&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24773;&#24863;CLAP&#27169;&#22411;&#65288;&#31216;&#20026;Emo-CLAP&#65289;&#65292;&#29992;&#20110;SER&#12290;&#28982;&#21518;&#65292;&#32771;&#34385;&#21040;&#22312;&#35821;&#38899;&#24773;&#24863;&#24314;&#27169;&#20013;&#24615;&#21035;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#65292;&#26469;&#25972;&#21512;&#35821;&#38899;&#20449;&#21495;&#30340;&#24773;&#24863;&#21644;&#24615;&#21035;&#20449;&#24687;&#65292;&#24418;&#25104;&#26356;&#21512;&#29702;&#30340;&#30446;&#26631;&#12290;&#22312;IEMOCAP&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;Emo-CLAP&#27169;&#22411;&#65288;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#21516;&#26102;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Audio Pretraining (CLAP) has recently exhibited impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a kind of efficient gender-attribute-enhanced CLAP model for speech emotion recognition (SER). Specifically, we first build an effective emotion CLAP model termed Emo-CLAP for SER, utilizing various self-supervised learning based pre-trained models. Then, considering the importance of the gender attribute in speech emotion modeling, two GEmo-CLAP approaches are further proposed to integrate the emotion and gender information of speech signals, forming more reasonable objectives. Extensive experiments conducted on the IEMOCAP corpus demonstrate that our proposed two GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with different pre-trained models, while also achieving superior recognition performance compared with other state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;XLex&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#35789;&#20856;&#21644;Transformer&#27169;&#22411;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20013;&#25552;&#20379;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03997</link><description>&lt;p&gt;
&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#65306;&#20174;Transformer&#22238;&#24402;&#21040;&#21487;&#35299;&#37322;&#24615;&#35789;&#20856;(XLex)
&lt;/p&gt;
&lt;p&gt;
Sentiment Analysis in Finance: From Transformers Back to eXplainable Lexicons (XLex). (arXiv:2306.03997v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;XLex&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#35789;&#20856;&#21644;Transformer&#27169;&#22411;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20013;&#25552;&#20379;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35789;&#20856;&#30340;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#21033;&#29992;&#20154;&#24037;&#19987;&#23478;&#21019;&#24314;&#30340;&#19987;&#38376;&#27880;&#37322;&#30340;&#35789;&#20856;&#20174;&#37329;&#34701;&#25991;&#26412;&#20013;&#25552;&#21462;&#24773;&#24863;&#12290;&#34429;&#28982;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#23454;&#29616;&#31616;&#21333;&#19988;&#36895;&#24230;&#24555;&#65292;&#20294;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#25163;&#21160;&#27880;&#37322;&#24037;&#20316;&#26469;&#21019;&#24314;&#12289;&#32500;&#25252;&#21644;&#26356;&#26032;&#35789;&#20856;&#12290;&#36825;&#20123;&#26041;&#27861;&#20063;&#34987;&#35748;&#20026;&#19981;&#22914;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;&#22914;Transformer&#27169;&#22411;&#65289;&#20248;&#36234;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#12290;&#28982;&#32780;&#65292;Transformer&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#28041;&#21450;&#26174;&#33879;&#30340;&#39044;&#27979;&#26102;&#38388;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#23454;&#26102;&#29983;&#20135;&#29615;&#22659;&#25110;&#22788;&#29702;&#33021;&#21147;&#21463;&#38480;&#30340;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;eXplainable Lexicons&#65288;XLex&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#21644;Transformer&#27169;&#22411;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexicon-based sentiment analysis (SA) in finance leverages specialized, manually annotated lexicons created by human experts to extract sentiment from financial texts. Although lexicon-based methods are simple to implement and fast to operate on textual data, they require considerable manual annotation efforts to create, maintain, and update the lexicons. These methods are also considered inferior to the deep learning-based approaches, such as transformer models, which have become dominant in various NLP tasks due to their remarkable performance. However, transformers require extensive data and computational resources for both training and testing. Additionally, they involve significant prediction times, making them unsuitable for real-time production environments or systems with limited processing capabilities. In this paper, we introduce a novel methodology named eXplainable Lexicons (XLex) that combines the advantages of both lexicon-based methods and transformer models. We propose 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#30784;&#26694;&#26550;-"&#36127;&#36131;&#20219;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;&#65288;ResponsibleTA&#65289;"&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36127;&#36131;&#20219;&#22320;&#20316;&#20026;&#20219;&#21153;&#21327;&#21516;&#24037;&#20855;&#12290;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;LLM&#30340;&#19977;&#31181;&#33021;&#21147;&#65306;&#39044;&#27979;&#20219;&#21153;&#21487;&#34892;&#24615;&#12289;&#39564;&#35777;&#20219;&#21153;&#23436;&#25972;&#24615;&#20197;&#21450;&#22686;&#24378;&#20219;&#21153;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01242</link><description>&lt;p&gt;
&#36127;&#36131;&#20219;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;: &#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#36127;&#36131;&#20219;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators. (arXiv:2306.01242v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#30784;&#26694;&#26550;-"&#36127;&#36131;&#20219;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;&#65288;ResponsibleTA&#65289;"&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36127;&#36131;&#20219;&#22320;&#20316;&#20026;&#20219;&#21153;&#21327;&#21516;&#24037;&#20855;&#12290;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;LLM&#30340;&#19977;&#31181;&#33021;&#21147;&#65306;&#39044;&#27979;&#20219;&#21153;&#21487;&#34892;&#24615;&#12289;&#39564;&#35777;&#20219;&#21153;&#23436;&#25972;&#24615;&#20197;&#21450;&#22686;&#24378;&#20219;&#21153;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#20026;&#20154;&#24037;&#26234;&#33021;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#23427;&#20204;&#23637;&#29616;&#20102;&#22312;&#29992;&#25143;&#25351;&#20196;&#19979;&#33258;&#21160;&#23436;&#25104;&#20219;&#21153;&#30340;&#33391;&#22909;&#21069;&#26223;&#65292;&#21487;&#20197;&#20316;&#20026;&#31867;&#20284;&#22823;&#33041;&#30340;&#21327;&#35843;&#32773;&#12290;&#38543;&#30528;&#25105;&#20204;&#23558;&#36234;&#26469;&#36234;&#22810;&#30340;&#20219;&#21153;&#20132;&#32473;&#26426;&#22120;&#33258;&#21160;&#23436;&#25104;&#65292;&#30456;&#20851;&#30340;&#39118;&#38505;&#20063;&#36880;&#28176;&#26174;&#29616;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#20986;&#29616;&#20102;: &#24403;&#26426;&#22120;&#20687;&#20154;&#31867;&#39550;&#39542;&#21327;&#21516;&#19968;&#26679;&#24110;&#21161;&#20154;&#20204;&#33258;&#21160;&#23436;&#25104;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#30830;&#20445;&#26426;&#22120;&#30340;&#36131;&#20219;&#34892;&#20026;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#21487;&#34892;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#35282;&#24230;&#65292;&#28145;&#20837;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#36127;&#36131;&#20219;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;&#8221;&#65288;ResponsibleTA&#65289;&#20316;&#20026;&#19968;&#20010;&#22522;&#30784;&#26694;&#26550;&#65292;&#20197;&#20419;&#36827;LLM&#21327;&#35843;&#32773;&#21644;&#25191;&#34892;&#32773;&#20043;&#38388;&#30340;&#36127;&#36131;&#20219;&#21327;&#20316;&#65292;&#23454;&#29616;&#20219;&#21153;&#33258;&#21160;&#21270;&#12290;&#35813;&#26694;&#26550;&#25317;&#26377;&#19977;&#31181;&#22686;&#24378;&#33021;&#21147;: 1&#65289;&#39044;&#27979;&#25191;&#34892;&#32773;&#21629;&#20196;&#30340;&#21487;&#34892;&#24615;&#65307;2&#65289;&#39564;&#35777;&#25191;&#34892;&#32773;&#30340;&#23436;&#25972;&#24615;&#65307;3&#65289;&#22686;&#24378;&#23433;&#20840;&#24615;&#65288;&#20363;&#22914;&#65292;&#20445;&#25252;&#38544;&#31169;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of Large Language Models (LLMs) signifies an impressive stride towards artificial general intelligence. They have shown a promising prospect in automatically completing tasks upon user instructions, functioning as brain-like coordinators. The associated risks will be revealed as we delegate an increasing number of tasks to machines for automated completion. A big question emerges: how can we make machines behave responsibly when helping humans automate tasks as personal copilots? In this paper, we explore this question in depth from the perspectives of feasibility, completeness and security. In specific, we present Responsible Task Automation (ResponsibleTA) as a fundamental framework to facilitate responsible collaboration between LLM-based coordinators and executors for task automation with three empowered capabilities: 1) predicting the feasibility of the commands for executors; 2) verifying the completeness of executors; 3) enhancing the security (e.g., the prote
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.14735</link><description>&lt;p&gt;
&#36793;&#32536;&#32858;&#28966;&#65306;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#25439;&#20154;&#32676;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#23545;&#36793;&#32536;&#31038;&#21306;&#24433;&#21709;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#30830;&#23450;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#38024;&#23545;&#24369;&#21183;&#32676;&#20307;&#30340;&#20260;&#23475;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20250;&#25513;&#30422;&#30001;&#20132;&#21449;&#23376;&#32676;&#25110;&#36328;&#20154;&#21475;&#32676;&#20307;&#20849;&#20139;&#30340;&#20260;&#23475;&#27169;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#8220;&#36793;&#32536;&#8221;&#23450;&#20041;&#20026;&#20855;&#26377;&#36828;&#31163;&#8220;&#24120;&#24577;&#8221; &#30340;&#20154;&#21475;&#23646;&#24615;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#24230;&#37327;&#38024;&#23545;&#36825;&#20123;&#24322;&#24120;&#20540;&#30340;&#20260;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#24615;&#33021;&#24046;&#24322;&#25351;&#25968;&#65288;GPDI&#65289;&#65292;&#20197;&#34913;&#37327;&#25968;&#25454;&#38598;&#32454;&#20998;&#20026;&#23376;&#32452;&#23545;&#38754;&#20020;&#22686;&#21152;&#30340;&#20260;&#23475;&#30340;&#35782;&#21035;&#31243;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26816;&#27979;&#27602;&#24615;&#26816;&#27979;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#38024;&#23545;&#24322;&#24120;&#20540;&#30340;&#25991;&#26412;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#27602;&#24615;&#26816;&#39564;&#20013;&#27602;&#24615;&#26356;&#39640;&#65292;&#39640;&#36798;28&#65285;&#33267;86&#65285;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23545;&#20110;&#20154;&#21475;&#23398;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#22987;&#32456;&#36739;&#24046;&#65292;&#24322;&#24120;&#20540;&#21644;&#38750;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#38169;&#35823;&#24046;&#36317;&#39640;&#36798;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
A standard method for measuring the impacts of AI on marginalized communities is to determine performance discrepancies between specified demographic groups. These approaches aim to address harms toward vulnerable groups, but they obscure harm patterns faced by intersectional subgroups or shared across demographic groups. We instead operationalize "the margins" as data points that are statistical outliers due to having demographic attributes distant from the "norm" and measure harms toward these outliers. We propose a Group-Based Performance Disparity Index (GPDI) that measures the extent to which a subdivision of a dataset into subgroups identifies those facing increased harms. We apply our approach to detecting disparities in toxicity detection and find that text targeting outliers is 28% to 86% more toxic for all types of toxicity examined. We also discover that model performance is consistently worse for demographic outliers, with disparities in error between outliers and non-outli
&lt;/p&gt;</description></item><item><title>Chain-of-Knowledge&#36890;&#36807;&#25972;&#21512;&#22810;&#28304;&#21160;&#24577;&#30693;&#35782;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20934;&#30830;&#30340;&#22522;&#30784;&#20449;&#24687;&#65292;&#20943;&#23569;&#29983;&#25104;&#30340;&#24187;&#35273;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.13269</link><description>&lt;p&gt;
Chain-of-Knowledge:&#36890;&#36807;&#22810;&#28304;&#21160;&#24577;&#30693;&#35782;&#36866;&#24212;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20934;&#30830;&#30340;&#22522;&#30784;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. (arXiv:2305.13269v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13269
&lt;/p&gt;
&lt;p&gt;
Chain-of-Knowledge&#36890;&#36807;&#25972;&#21512;&#22810;&#28304;&#21160;&#24577;&#30693;&#35782;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20934;&#30830;&#30340;&#22522;&#30784;&#20449;&#24687;&#65292;&#20943;&#23569;&#29983;&#25104;&#30340;&#24187;&#35273;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#38142;&#24335;&#30693;&#35782;&#65288;CoK&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#22522;&#30784;&#20449;&#24687;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12290;&#23427;&#21487;&#20197;&#20135;&#29983;&#26356;&#22810;&#30340;&#20107;&#23454;&#20381;&#25454;&#65292;&#20943;&#23569;&#29983;&#25104;&#30340;&#24187;&#35273;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CoK&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#25512;&#29702;&#20934;&#22791;&#12289;&#21160;&#24577;&#30693;&#35782;&#36866;&#24212;&#21644;&#31572;&#26696;&#25972;&#21512;&#12290;&#32473;&#23450;&#19968;&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#39064;&#65292;CoK&#39318;&#20808;&#20934;&#22791;&#33509;&#24178;&#20010;&#21021;&#27493;&#30340;&#20381;&#25454;&#21644;&#31572;&#26696;&#65292;&#21516;&#26102;&#35782;&#21035;&#20986;&#30456;&#20851;&#30340;&#30693;&#35782;&#39046;&#22495;&#12290;&#22914;&#26524;&#26679;&#26412;&#20013;&#30340;&#31572;&#26696;&#27809;&#26377;&#22810;&#25968;&#20849;&#35782;&#65292;CoK&#36890;&#36807;&#20174;&#35782;&#21035;&#20986;&#30340;&#39046;&#22495;&#20013;&#36880;&#27493;&#36866;&#24212;&#30693;&#35782;&#26469;&#32416;&#27491;&#20381;&#25454;&#12290;&#36825;&#20123;&#32416;&#27491;&#21518;&#30340;&#20381;&#25454;&#21487;&#20197;&#26356;&#22909;&#22320;&#20316;&#20026;&#26368;&#32456;&#31572;&#26696;&#25972;&#21512;&#30340;&#22522;&#30784;&#12290;&#19981;&#21516;&#20110;&#20043;&#21069;&#20027;&#35201;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#30740;&#31350;&#65292;CoK&#36824;&#21033;&#29992;&#32467;&#26500;&#21270;&#30340;&#30693;&#35782;&#28304;&#65292;&#22914;Wikidata&#21644;&#34920;&#26684;&#65292;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructure
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#34920;&#24449;&#36716;&#31227;&#28508;&#21147;&#65288;RTP&#65289;&#26469;&#34913;&#37327;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#21457;&#29616;&#22810;&#36335;&#24182;&#34892;&#37325;&#21472;&#26159;&#20851;&#38190;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#40723;&#21169;&#34920;&#24449;&#22312;&#35821;&#35328;&#20043;&#38388;&#26356;&#20855;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#21644;&#27169;&#22411;&#35774;&#32622;&#20013;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.11550</link><description>&lt;p&gt;
&#36879;&#36807;&#34920;&#24449;&#38236;&#22836;&#30475;&#24453;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens. (arXiv:2305.11550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11550
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#34920;&#24449;&#36716;&#31227;&#28508;&#21147;&#65288;RTP&#65289;&#26469;&#34913;&#37327;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#21457;&#29616;&#22810;&#36335;&#24182;&#34892;&#37325;&#21472;&#26159;&#20851;&#38190;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#40723;&#21169;&#34920;&#24449;&#22312;&#35821;&#35328;&#20043;&#38388;&#26356;&#20855;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#21644;&#27169;&#22411;&#35774;&#32622;&#20013;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#65292;&#20165;&#20165;&#20381;&#38752;&#32763;&#35793;&#36136;&#37327;&#24230;&#37327;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#26159;&#19981;&#22815;&#30340;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#34920;&#24449;&#36716;&#31227;&#28508;&#21147;&#65288;RTP&#65289;&#65292;&#23427;&#27979;&#37327;&#35821;&#35328;&#20043;&#38388;&#30340;&#34920;&#24449;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RTP&#21487;&#20197;&#27979;&#37327;&#27491;&#21521;&#21644;&#36127;&#21521;&#36716;&#31227;&#65288;&#24178;&#25200;&#65289;&#65292;&#24182;&#21457;&#29616;RTP&#19982;&#32763;&#35793;&#36136;&#37327;&#21464;&#21270;&#24378;&#30456;&#20851;&#65292;&#34920;&#26126;&#30830;&#23454;&#23384;&#22312;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#36716;&#31227;&#30456;&#20851;&#30340;&#25968;&#25454;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#22810;&#36335;&#24182;&#34892;&#37325;&#21472;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#26377;&#25506;&#32034;&#30340;&#29305;&#24449;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20351;&#29992;&#36741;&#21161;&#30456;&#20284;&#24615;&#25439;&#22833;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#36335;&#24182;&#34892;&#25968;&#25454;&#65292;&#40723;&#21169;&#34920;&#24449;&#22312;&#35821;&#35328;&#20043;&#38388;&#26356;&#20855;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#21644;&#27169;&#22411;&#35774;&#32622;&#20013;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We argue that translation quality alone is not a sufficient metric for measuring knowledge transfer in multilingual neural machine translation. To support this claim, we introduce Representational Transfer Potential (RTP), which measures representational similarities between languages. We show that RTP can measure both positive and negative transfer (interference), and find that RTP is strongly correlated with changes in translation quality, indicating that transfer does occur. Furthermore, we investigate data and language characteristics that are relevant for transfer, and find that multi-parallel overlap is an important yet under-explored feature. Based on this, we develop a novel training scheme, which uses an auxiliary similarity loss that encourages representations to be more invariant across languages by taking advantage of multi-parallel data. We show that our method yields increased translation quality for low- and mid-resource languages across multiple data and model setups.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#20043;&#26641;&#65288;ToT&#65289;&#65292;&#21487;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#20915;&#31574;&#65292;&#20197;&#21450;&#33258;&#25105;&#35780;&#20272;&#21644;&#20840;&#23616;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2305.10601</link><description>&lt;p&gt;
Tree of Thoughts: &#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Tree of Thoughts: Deliberate Problem Solving with Large Language Models. (arXiv:2305.10601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#20043;&#26641;&#65288;ToT&#65289;&#65292;&#21487;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#20915;&#31574;&#65292;&#20197;&#21450;&#33258;&#25105;&#35780;&#20272;&#21644;&#20840;&#23616;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#36890;&#29992;&#38382;&#39064;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20173;&#28982;&#21463;&#38480;&#20110;&#22522;&#20110;&#26631;&#35760;&#12289;&#20174;&#24038;&#21040;&#21491;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#38656;&#35201;&#25506;&#32034;&#12289;&#25112;&#30053;&#21069;&#30651;&#25110;&#21021;&#22987;&#20915;&#31574;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#20219;&#21153;&#20013;&#65292;&#20182;&#20204;&#21487;&#33021;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#20043;&#26641;&#65288;ToT&#65289;&#65292;&#23427;&#23558;&#36890;&#24120;&#29992;&#20110;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#38142;&#26041;&#27861;&#27867;&#21270;&#65292;&#24182;&#20351;&#29992;&#19968;&#33268;&#30340;&#25991;&#26412;&#21333;&#20301;&#65288;&#24605;&#32500;&#65289;&#36827;&#34892;&#25506;&#31350;&#65292;&#36825;&#20123;&#24605;&#32500;&#20316;&#20026;&#35299;&#20915;&#38382;&#39064;&#30340;&#20013;&#38388;&#27493;&#39588;&#12290;&#24605;&#32500;&#20043;&#26641;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#22810;&#20010;&#19981;&#21516;&#30340;&#25512;&#29702;&#36335;&#24452;&#21644;&#33258;&#25105;&#35780;&#20272;&#26469;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#20915;&#31574;&#65292;&#24182;&#20915;&#23450;&#19979;&#19968;&#27493;&#30340;&#34892;&#21160;&#65292;&#21516;&#26102;&#22312;&#24517;&#35201;&#26102;&#21521;&#21069;&#25110;&#21521;&#21518;&#36319;&#36394;&#20197;&#36827;&#34892;&#20840;&#23616;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ToT&#26174;&#33879;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#38382;&#39064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abil
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;SELF-ALIGN&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#21407;&#21017;&#30340;&#25512;&#29702;&#21644;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#20197;&#26368;&#23569;&#30340;&#20154;&#31867;&#30417;&#30563;&#23454;&#29616;AI&#20195;&#29702;&#30340;&#33258;&#25105;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.03047</link><description>&lt;p&gt;
&#21407;&#21017;&#39537;&#21160;&#33258;&#25105;&#23545;&#40784;&#30340;&#26368;&#23567;&#20154;&#21147;&#30417;&#30563;&#30340;&#35821;&#35328;&#27169;&#22411;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. (arXiv:2305.03047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03047
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;SELF-ALIGN&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#21407;&#21017;&#30340;&#25512;&#29702;&#21644;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#20197;&#26368;&#23569;&#30340;&#20154;&#31867;&#30417;&#30563;&#23454;&#29616;AI&#20195;&#29702;&#30340;&#33258;&#25105;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;AI&#21161;&#25163;&#20195;&#29702;&#65292;&#22914;ChatGPT&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#24847;&#22270;&#65292;&#30830;&#20445;&#23427;&#20204;&#26159;&#26377;&#29992;&#30340;&#12289;&#36947;&#24503;&#30340;&#12289;&#21487;&#38752;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20381;&#36182;&#24615;&#21487;&#33021;&#20250;&#26497;&#22823;&#22320;&#38480;&#21046;AI&#21161;&#25163;&#20195;&#29702;&#30340;&#30495;&#27491;&#28508;&#21147;&#65292;&#22240;&#20026;&#33719;&#24471;&#20154;&#31867;&#30417;&#30563;&#30340;&#25104;&#26412;&#24456;&#39640;&#65292;&#30456;&#20851;&#38382;&#39064;&#26377;&#36136;&#37327;&#12289;&#21487;&#38752;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#33258;&#19968;&#33268;&#24615;&#21644;&#19981;&#33391;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; SELF-ALIGN&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#25512;&#29702;&#21644;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20197;&#26368;&#23569;&#30340;&#20154;&#31867;&#30417;&#30563;&#23454;&#29616;AI&#20195;&#29702;&#30340;&#33258;&#25105;&#23545;&#40784;&#12290;&#26041;&#27861;&#21253;&#25324;&#22235;&#20010;&#38454;&#27573;&#65306;&#31532;&#19968;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#29983;&#25104;&#21512;&#25104;&#25552;&#31034;&#65292;&#20351;&#29992;&#20027;&#39064;&#24341;&#23548;&#26041;&#27861;&#22686;&#21152;&#25552;&#31034;&#22810;&#26679;&#24615;&#65307;&#31532;&#20108;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#23567;&#32452;&#20154;&#24037;&#32534;&#20889;&#30340;AI&#27169;&#22411;&#21407;&#21017;&#65292;&#24182;&#25351;&#23548;AI&#27169;&#22411;&#36981;&#24490;&#65307;
&lt;/p&gt;
&lt;p&gt;
Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and gu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#22833;&#35823;&#65292;&#24402;&#32435;&#24182;&#30830;&#23450;&#20854;&#22833;&#36133;&#30340;&#21407;&#22240;&#31867;&#22411;&#21644;&#20851;&#38190;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#28508;&#22312;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10513</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;ChatGPT&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Does ChatGPT Fall Short in Answering Questions Faithfully?. (arXiv:2304.10513v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10513
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#22833;&#35823;&#65292;&#24402;&#32435;&#24182;&#30830;&#23450;&#20854;&#22833;&#36133;&#30340;&#21407;&#22240;&#31867;&#22411;&#21644;&#20851;&#38190;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#28508;&#22312;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;ChatGPT&#65292;&#23637;&#31034;&#20986;&#23545;&#20154;&#31867;&#29983;&#27963;&#21508;&#26041;&#38754;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;ChatGPT&#22312;&#35802;&#23454;&#24615;&#31561;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#38382;&#31572;&#31995;&#32479;&#20026;&#20195;&#34920;&#24212;&#29992;&#31243;&#24207;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;&#20026;&#20160;&#20040;ChatGPT&#22312;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#26377;&#25152;&#19981;&#36275;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35797;&#22270;&#20998;&#26512;ChatGPT&#22312;&#22797;&#26434;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#22833;&#36133;&#30340;&#21407;&#22240;&#65292;&#24182;&#30830;&#23450;&#19982;&#36825;&#20123;&#22833;&#36133;&#26377;&#20851;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;ChatGPT&#30340;&#22833;&#36133;&#24402;&#20026;&#22235;&#31181;&#31867;&#22411;&#65306;&#29702;&#35299;&#12289;&#20107;&#23454;&#24615;&#12289;&#20855;&#20307;&#24615;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#19982;QA&#22833;&#36133;&#26377;&#20851;&#30340;&#19977;&#20010;&#20851;&#38190;&#33021;&#21147;&#65306;&#30693;&#35782;&#35760;&#24518;&#12289;&#30693;&#35782;&#20851;&#32852;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22260;&#32469;&#36825;&#20123;&#33021;&#21147;&#30340;&#23454;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21521;&#27169;&#22411;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#22806;&#37096;&#30693;&#35782;&#12289;&#32473;&#20104;&#25552;&#31034;&#26469;&#24110;&#21161;&#23427;&#32858;&#28966;&#24182;&#21152;&#24378;&#20851;&#38190;&#33021;&#21147;&#65292;&#36825;&#37117;&#26377;&#21161;&#20110;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models, such as ChatGPT, have demonstrated significant potential to impact various aspects of human life. However, ChatGPT still faces challenges in aspects like faithfulness. Taking question answering as a representative application, we seek to understand why ChatGPT falls short in answering questions faithfully. To address this question, we attempt to analyze the failures of ChatGPT in complex open-domain question answering and identifies the abilities under the failures. Specifically, we categorize ChatGPT's failures into four types: comprehension, factualness, specificity, and inference. We further pinpoint three critical abilities associated with QA failures: knowledge memorization, knowledge association, and knowledge reasoning. Additionally, we conduct experiments centered on these abilities and propose potential approaches to enhance faithfulness. The results indicate that furnishing the model with fine-grained external knowledge, hints for
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#19982;&#33258;&#36523;&#23545;&#35805;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#36718;&#32842;&#22825;&#35821;&#26009;&#24211;&#65292;&#24182;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26469;&#22686;&#24378;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;Baize&#65292;&#22312;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#30340;&#25252;&#26639;&#19979;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01196</link><description>&lt;p&gt;
Baize:&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#23545;&#35805;&#25968;&#25454;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#24320;&#28304;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. (arXiv:2304.01196v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01196
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#19982;&#33258;&#36523;&#23545;&#35805;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#36718;&#32842;&#22825;&#35821;&#26009;&#24211;&#65292;&#24182;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26469;&#22686;&#24378;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;Baize&#65292;&#22312;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#30340;&#25252;&#26639;&#19979;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#27169;&#22411;&#65292;&#22914;ChatGPT&#65292;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20247;&#22810;&#39046;&#22495;&#24471;&#21040;&#36805;&#36895;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#21482;&#33021;&#36890;&#36807;&#21463;&#38480;&#21046;&#30340;API&#36827;&#34892;&#35775;&#38382;&#65292;&#20174;&#32780;&#21046;&#36896;&#20102;&#26032;&#30340;&#30740;&#31350;&#21644;&#39046;&#22495;&#36827;&#23637;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#19982;&#33258;&#36523;&#23545;&#35805;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#36718;&#32842;&#22825;&#35821;&#26009;&#24211;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26469;&#22686;&#24378;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#12290;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;Baize&#65292;&#22312;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#30340;&#25252;&#26639;&#19979;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;Baize&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#20165;&#29992;&#20110;&#30740;&#31350;&#30446;&#30340;&#65292;&#21487;&#22312;https://github.com/project-baize/baize&#36827;&#34892;&#19979;&#36733;&#12290;&#22312;&#32447;&#28436;&#31034;&#20063;&#21487;&#22312;https://huggingface.co/spaces/project-baize/baize-lora-7B&#36827;&#34892;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize. An online demo is also available at https://huggingface.co/spaces/project-baize/baize-lora-7B.
&lt;/p&gt;</description></item><item><title>&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17580</link><description>&lt;p&gt;
HuggingGPT: &#22312;HugingFace&#20013;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20249;&#20276;&#35299;&#20915;AI&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17580
&lt;/p&gt;
&lt;p&gt;
&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#22797;&#26434;AI&#20219;&#21153;&#26159;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#31649;&#29702;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#20197;&#35299;&#20915;AI&#20219;&#21153;&#65292;&#35821;&#35328;&#25104;&#20026;&#36890;&#29992;&#25509;&#21475;&#26469;&#36171;&#33021;&#23427;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#26681;&#25454;HuggingFace&#20013;&#21487;&#29992;&#30340;&#27169;&#22411;&#21151;&#33021;&#25551;&#36848;&#26469;&#36873;&#25321;&#27169;&#22411;&#65292;&#22312;&#36873;&#23450;AI&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23637;&#31034;&#65292;&#19968;&#20010;&#25317;&#26377;&#20856;&#22411;API&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#25163;&#21487;&#20197;&#20197;&#26497;&#20302;&#30340;&#37329;&#38065;&#25104;&#26412;&#31363;&#21462;GPT-2&#21644;GPT-3&#31561;LM&#30340;&#35299;&#30721;&#31639;&#27861;&#30340;&#31867;&#22411;&#21644;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.04729</link><description>&lt;p&gt;
&#35770;&#30423;&#29992;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#31639;&#27861;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
On the Risks of Stealing the Decoding Algorithms of Language Models. (arXiv:2303.04729v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04729
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23637;&#31034;&#65292;&#19968;&#20010;&#25317;&#26377;&#20856;&#22411;API&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#25163;&#21487;&#20197;&#20197;&#26497;&#20302;&#30340;&#37329;&#38065;&#25104;&#26412;&#31363;&#21462;GPT-2&#21644;GPT-3&#31561;LM&#30340;&#35299;&#30721;&#31639;&#27861;&#30340;&#31867;&#22411;&#21644;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#29983;&#25104;&#25991;&#26412;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#21644;&#35843;&#25972;&#35299;&#30721;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#30830;&#23450;&#22914;&#20309;&#20174;LM&#29983;&#25104;&#30340;&#20869;&#37096;&#27010;&#29575;&#20998;&#24067;&#20013;&#29983;&#25104;&#25991;&#26412;&#12290;&#36873;&#25321;&#35299;&#30721;&#31639;&#27861;&#24182;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#30340;&#36807;&#31243;&#38656;&#35201;&#26174;&#33879;&#30340;&#26102;&#38388;&#12289;&#25163;&#21160;&#24037;&#20316;&#21644;&#35745;&#31639;&#65292;&#36824;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#35299;&#30721;&#31639;&#27861;&#30340;&#36523;&#20221;&#21644;&#36229;&#21442;&#25968;&#34987;&#35748;&#20026;&#26159;&#26497;&#20854;&#26377;&#20215;&#20540;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#19968;&#20010;&#25317;&#26377;&#20856;&#22411;API&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#25163;&#21487;&#20197;&#20197;&#26497;&#20302;&#30340;&#37329;&#38065;&#25104;&#26412;&#31363;&#21462;&#20854;&#35299;&#30721;&#31639;&#27861;&#30340;&#31867;&#22411;&#21644;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#23545;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;API&#30340;&#27969;&#34892;LM&#26377;&#25928;&#65292;&#21253;&#25324;GPT-2&#21644;GPT-3&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21482;&#38656;&#33457;&#36153;&#20960;&#32654;&#20803;&#65292;&#20363;&#22914;0.8&#32654;&#20803;&#12289;1&#32654;&#20803;&#12289;4&#32654;&#20803;&#21644;40&#32654;&#20803;&#65292;&#23601;&#21487;&#20197;&#30423;&#21462;&#27492;&#31867;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., $\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of GPT-3.
&lt;/p&gt;</description></item><item><title>&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#65292;&#22914;&#20309;&#35753;&#30693;&#35782;&#25277;&#21462;&#26356;&#22909;&#22320;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#65311;&#26412;&#25991;&#35843;&#30740;&#20102;&#19977;&#31181;&#35299;&#20915;&#33539;&#24335;&#65306;&#39640;&#36164;&#28304;&#25968;&#25454;&#12289;&#26356;&#24378;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2202.08063</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#30340;&#30693;&#35782;&#25277;&#21462;&#65306;&#35843;&#30740;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08063
&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#65292;&#22914;&#20309;&#35753;&#30693;&#35782;&#25277;&#21462;&#26356;&#22909;&#22320;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#65311;&#26412;&#25991;&#35843;&#30740;&#20102;&#19977;&#31181;&#35299;&#20915;&#33539;&#24335;&#65306;&#39640;&#36164;&#28304;&#25968;&#25454;&#12289;&#26356;&#24378;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#25277;&#21462;&#65288;KE&#65289;&#26088;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#65292;&#36890;&#24120;&#36973;&#21463;&#25968;&#25454;&#21294;&#20047;&#21644;&#20986;&#29616;&#26410;&#35265;&#31867;&#22411;&#65288;&#20302;&#36164;&#28304;&#24773;&#22659;&#65289;&#30340;&#22256;&#25200;&#12290;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#24050;&#24191;&#27867;&#30740;&#31350;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#23545;&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;KE&#36827;&#34892;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23558;&#29616;&#26377;&#30340;&#24037;&#20316;&#31995;&#32479;&#24615;&#22320;&#20998;&#20026;&#19977;&#31181;&#33539;&#24335;&#65306;&#65288;1&#65289;&#21033;&#29992;&#39640;&#36164;&#28304;&#25968;&#25454;&#65292;&#65288;2&#65289;&#21033;&#29992;&#26356;&#24378;&#30340;&#27169;&#22411;&#65292;&#65288;3&#65289;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20123;&#28508;&#22312;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35843;&#30740;&#21487;&#20197;&#24110;&#21161;&#23398;&#26415;&#21644;&#24037;&#19994;&#30028;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#21019;&#24847;&#65292;&#25552;&#21319;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Extraction (KE), aiming to extract structural information from unstructured texts, often suffers from data scarcity and emerging unseen types, i.e., low-resource scenarios. Many neural approaches to low-resource KE have been widely investigated and achieved impressive performance. In this paper, we present a literature review towards KE in low-resource scenarios, and systematically categorize existing works into three paradigms: (1) exploiting higher-resource data, (2) exploiting stronger models, and (3) exploiting data and models together. In addition, we highlight promising applications and outline some potential directions for future research. We hope that our survey can help both the academic and industrial communities to better understand this field, inspire more ideas, and boost broader applications.
&lt;/p&gt;</description></item></channel></rss>