<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#39033;&#35770;&#25991;&#25351;&#20986;&#65292;&#27169;&#22411;&#32534;&#36753;&#21487;&#33021;&#20250;&#25913;&#21892;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#65292;&#20294;&#20250;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#20026;&#20195;&#20215;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#36890;&#36807;&#35780;&#20272;&#22235;&#31181;&#32534;&#36753;&#26041;&#27861;&#22312;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36825;&#20123;&#32534;&#36753;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#23545;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#21487;&#33021;&#20135;&#29983;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.04700</link><description>&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#21487;&#33021;&#20250;&#25439;&#23475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Model Editing Can Hurt General Abilities of Large Language Models. (arXiv:2401.04700v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04700
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#25351;&#20986;&#65292;&#27169;&#22411;&#32534;&#36753;&#21487;&#33021;&#20250;&#25913;&#21892;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#65292;&#20294;&#20250;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#20026;&#20195;&#20215;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#36890;&#36807;&#35780;&#20272;&#22235;&#31181;&#32534;&#36753;&#26041;&#27861;&#22312;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36825;&#20123;&#32534;&#36753;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#23545;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#21487;&#33021;&#20135;&#29983;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#25105;&#20204;&#33719;&#21462;&#20854;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#25552;&#20379;&#20102;&#26032;&#30340;&#33539;&#24335;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;LLM&#36755;&#20986;&#20013;&#23384;&#22312;&#38169;&#35273;&#65292;&#36825;&#26159;&#30001;&#20110;&#38169;&#35823;&#25110;&#36807;&#26102;&#30693;&#35782;&#24341;&#36215;&#30340;&#12290;&#30001;&#20110;&#20351;&#29992;&#26356;&#26032;&#21518;&#30340;&#20449;&#24687;&#37325;&#26032;&#35757;&#32451;LLM&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65292;&#22240;&#27492;&#20154;&#20204;&#23545;&#27169;&#22411;&#32534;&#36753;&#20135;&#29983;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#24456;&#26377;&#25928;&#65292;&#20294;&#24448;&#24448;&#36807;&#20110;&#24378;&#35843;&#32534;&#36753;&#24615;&#33021;&#30340;&#21151;&#25928;&#12289;&#27867;&#21270;&#24615;&#21644;&#23616;&#37096;&#24615;&#65292;&#24120;&#24120;&#24573;&#35270;&#20102;&#23545;LLM&#30340;&#36890;&#29992;&#33021;&#21147;&#21487;&#33021;&#20135;&#29983;&#30340;&#21103;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#21892;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#21487;&#33021;&#20250;&#20197;&#30456;&#24403;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#19979;&#38477;&#20026;&#20195;&#20215;&#30340;&#25285;&#24551;&#65292;&#36825;&#19981;&#31526;&#21512;LLM&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#22235;&#31181;&#24120;&#29992;&#30340;&#32534;&#36753;&#26041;&#27861;&#22312;&#20004;&#20010;LLM&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#21103;&#20316;&#29992;&#65292;&#24182;&#28085;&#30422;&#20102;&#20843;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have opened up new paradigms for accessing the knowledge stored in their parameters. One critical challenge that has emerged is the presence of hallucinations in LLM outputs due to false or outdated knowledge. Since retraining LLMs with updated information is resource-intensive, there has been a growing interest in model editing. However, many model editing methods, while effective in various scenarios, tend to overemphasize aspects such as efficacy, generalization, and locality in editing performance, often overlooking potential side effects on the general abilities of LLMs. In this paper, we raise concerns that the improvement of model factuality may come at the cost of a significant degradation of these general abilities, which is not conducive to the sustainable development of LLMs. Systematically, we analyze side effects by evaluating four popular editing methods on two LLMs across eight representative task categories. Extensive empi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRANOLA QA&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#20351;&#29992;&#22810;&#31890;&#24230;&#31572;&#26696;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#20016;&#23500;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#31890;&#24230;&#29256;&#26412;&#30340;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...</title><link>http://arxiv.org/abs/2401.04695</link><description>&lt;p&gt;
&#32553;&#23567;&#30693;&#35782;&#35780;&#20272;&#24046;&#36317;&#65306;&#22810;&#23618;&#27425;&#31572;&#26696;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers. (arXiv:2401.04695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRANOLA QA&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#20351;&#29992;&#22810;&#31890;&#24230;&#31572;&#26696;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#20016;&#23500;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#31890;&#24230;&#29256;&#26412;&#30340;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#24615;&#38382;&#39064;&#36890;&#24120;&#21487;&#20197;&#20197;&#19981;&#21516;&#23618;&#27425;&#30340;&#31890;&#24230;&#27491;&#30830;&#22238;&#31572;&#12290;&#20363;&#22914;&#65292;&#8220;1961&#24180;8&#26376;4&#26085;&#8221;&#21644;&#8220;1961&#24180;&#8221;&#37117;&#26159;&#23545;&#8220;&#24052;&#25289;&#20811;&#183;&#22885;&#24052;&#39532;&#26159;&#22312;&#20160;&#20040;&#26102;&#20505;&#20986;&#29983;&#30340;&#65311;&#8221;&#36825;&#20010;&#38382;&#39064;&#30340;&#27491;&#30830;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#38382;&#31572;&#35780;&#20272;&#21327;&#35758;&#24182;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#36825;&#19968;&#28857;&#65292;&#32780;&#26159;&#23558;&#39044;&#27979;&#30340;&#31572;&#26696;&#19982;&#21333;&#19968;&#31890;&#24230;&#23618;&#27425;&#30340;&#31572;&#26696;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GRANOLA QA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#20854;&#20013;&#39044;&#27979;&#30340;&#31572;&#26696;&#26681;&#25454;&#20934;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#19982;&#19968;&#32452;&#22810;&#31890;&#24230;&#31572;&#26696;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#20016;&#23500;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#22810;&#31890;&#24230;&#31572;&#26696;&#65292;&#24182;&#21019;&#24314;&#20102;GRANOLA-EQ&#65292;&#19968;&#20010;&#22810;&#31890;&#24230;&#29256;&#26412;&#30340;EntityQuestions&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;GRANOLA-EQ&#19978;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#35299;&#30721;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;Decoding with Response Aggregation (DRAG)&#65292;&#35813;&#31639;&#27861;&#26088;&#22312;&#23558;&#21709;&#24212;&#30340;&#31890;&#24230;&#19982;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Factual questions typically can be answered correctly at different levels of granularity. For example, both ``August 4, 1961'' and ``1961'' are correct answers to the question ``When was Barack Obama born?''. Standard question answering (QA) evaluation protocols, however, do not explicitly take this into account and compare a predicted answer against answers of a single granularity level. In this work, we propose GRANOLA QA, a novel evaluation setting where a predicted answer is evaluated in terms of accuracy and informativeness against a set of multi-granularity answers. We present a simple methodology for enriching existing datasets with multi-granularity answers, and create GRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We evaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm, called Decoding with Response Aggregation (DRAG), that is geared towards aligning the response granularity with the model's uncertainty. Our experiments show th
&lt;/p&gt;</description></item><item><title>RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04679</link><description>&lt;p&gt;
RoSA: &#36890;&#36807;&#40065;&#26834;&#36866;&#24212;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04679
&lt;/p&gt;
&lt;p&gt;
RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32972;&#26223;&#19979;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#39044;&#31639;&#19979;&#25552;&#20379;&#33391;&#22909;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843; (PEFT) &#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;RoSA&#65292;&#21463;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#30340;&#21551;&#21457;&#65292;&#23427;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#20849;&#21516;&#35757;&#32451;$\textit{&#20302;&#31209;}$&#21644;$\textit{&#39640;&#24230;&#31232;&#30095;}$&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RoSA&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#23567;&#23398;&#25968;&#23398;&#21644;SQL&#26597;&#35810;&#29983;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30456;&#21516;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#65292;RoSA&#20248;&#20110;LoRA&#21644;&#32431;&#31929;&#30340;&#31232;&#30095;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;GPU&#20869;&#26680;&#20026;RoSA&#25552;&#20379;&#31995;&#32479;&#25903;&#25345;&#65292;&#20197;&#34917;&#20805;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;https://github.com/IST-DASLab&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Lightning Attention-2&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#35745;&#31639;&#20248;&#21183;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#36890;&#36807;&#21033;&#29992;&#24179;&#38138;&#30340;&#24605;&#24819;&#65292;&#20998;&#21035;&#22788;&#29702;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#20869;&#37096;&#22359;&#21644;&#22806;&#37096;&#22359;&#32452;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#37319;&#29992;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26426;&#21046;&#22788;&#29702;&#20869;&#37096;&#22359;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#32047;&#31215;&#27714;&#21644;&#26041;&#27861;&#22788;&#29702;&#22806;&#37096;&#22359;&#12290;</title><link>http://arxiv.org/abs/2401.04658</link><description>&lt;p&gt;
Lightning Attention-2:&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22788;&#29702;&#26080;&#38480;&#24207;&#21015;&#38271;&#24230;&#30340;"&#20813;&#36153;&#21320;&#39184;"
&lt;/p&gt;
&lt;p&gt;
Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. (arXiv:2401.04658v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Lightning Attention-2&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#35745;&#31639;&#20248;&#21183;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#36890;&#36807;&#21033;&#29992;&#24179;&#38138;&#30340;&#24605;&#24819;&#65292;&#20998;&#21035;&#22788;&#29702;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#20869;&#37096;&#22359;&#21644;&#22806;&#37096;&#22359;&#32452;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#37319;&#29992;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26426;&#21046;&#22788;&#29702;&#20869;&#37096;&#22359;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#32047;&#31215;&#27714;&#21644;&#26041;&#27861;&#22788;&#29702;&#22806;&#37096;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#27880;&#24847;&#21147;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#20256;&#32479;softmax&#27880;&#24847;&#21147;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#19978;&#33021;&#22815;&#22312;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22788;&#29702;&#26080;&#38480;&#38271;&#24230;&#30340;&#24207;&#21015;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#36895;&#24230;&#65292;&#21363;&#22312;&#22266;&#23450;&#30340;&#20869;&#23384;&#28040;&#32791;&#19979;&#65292;&#33021;&#22815;&#20197;&#24658;&#23450;&#30340;&#35757;&#32451;&#36895;&#24230;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32047;&#31215;&#27714;&#21644;&#65288;cumsum&#65289;&#30340;&#38382;&#39064;&#65292;&#24403;&#21069;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#31639;&#27861;&#26080;&#27861;&#22312;&#22240;&#26524;&#35774;&#32622;&#19979;&#23637;&#29616;&#20854;&#29702;&#35770;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Lightning Attention-2&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#35745;&#31639;&#20248;&#21183;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#24179;&#38138;&#65288;tiling&#65289;&#30340;&#24605;&#24819;&#65292;&#20998;&#21035;&#22788;&#29702;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#20869;&#37096;&#22359;&#21644;&#22806;&#37096;&#22359;&#32452;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26426;&#21046;&#26469;&#22788;&#29702;&#20869;&#37096;&#22359;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#32047;&#31215;&#27714;&#21644;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22806;&#37096;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DepressionEmo&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;Reddit&#29992;&#25143;&#24086;&#23376;&#26469;&#26816;&#27979;&#19982;&#25233;&#37057;&#30456;&#20851;&#30340;8&#31181;&#24773;&#32490;&#12290;&#30740;&#31350;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24773;&#32490;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12289;&#20998;&#24067;&#21644;&#35821;&#35328;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04655</link><description>&lt;p&gt;
DepressionEmo: &#29992;&#20110;&#25233;&#37057;&#24773;&#32490;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DepressionEmo: A novel dataset for multilabel classification of depression emotions. (arXiv:2401.04655v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DepressionEmo&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;Reddit&#29992;&#25143;&#24086;&#23376;&#26469;&#26816;&#27979;&#19982;&#25233;&#37057;&#30456;&#20851;&#30340;8&#31181;&#24773;&#32490;&#12290;&#30740;&#31350;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24773;&#32490;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12289;&#20998;&#24067;&#21644;&#35821;&#35328;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#23545;&#20110;&#20154;&#31867;&#31038;&#20132;&#20114;&#21160;&#33267;&#20851;&#37325;&#35201;,&#19981;&#21516;&#30340;&#24773;&#22659;&#32972;&#26223;&#20250;&#24341;&#21457;&#22810;&#26679;&#21270;&#30340;&#22238;&#24212;&#12290;&#23588;&#20854;&#26159;&#36127;&#38754;&#24773;&#32490;&#29366;&#24577;&#30340;&#26222;&#36941;&#23384;&#22312;&#19982;&#24515;&#29702;&#20581;&#24247;&#30340;&#36127;&#38754;&#32467;&#26524;&#30456;&#20851;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#20840;&#38754;&#20998;&#26512;&#23427;&#20204;&#30340;&#21457;&#29983;&#21644;&#23545;&#20010;&#20307;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DepressionEmo&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;6037&#20010;&#38271;&#25991;&#26412;Reddit&#29992;&#25143;&#24086;&#23376;&#26469;&#26816;&#27979;&#19982;&#25233;&#37057;&#30456;&#20851;&#30340;8&#31181;&#24773;&#32490;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#36755;&#20837;&#36827;&#34892;&#22810;&#25968;&#34920;&#20915;&#65292;&#24182;&#36890;&#36807;&#27880;&#37322;&#32773;&#21644;ChatGPT&#39564;&#35777;&#20854;&#36136;&#37327;&#65292;&#26174;&#31034;&#20102;&#27880;&#37322;&#32773;&#20043;&#38388;&#21487;&#25509;&#21463;&#30340;&#19968;&#33268;&#24615;&#27700;&#24179;&#12290;&#23545;&#20110;DepressionEmo&#65292;&#23545;&#24773;&#32490;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12289;&#38543;&#26102;&#38388;&#30340;&#20998;&#24067;&#20197;&#21450;&#35821;&#35328;&#20998;&#26512;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#31181;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#20998;&#20026;&#20004;&#32452;&#65306;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;SVM&#12289;XGBoost&#21644;Light GBM&#65289;&#65307;&#19968;
&lt;/p&gt;
&lt;p&gt;
Emotions are integral to human social interactions, with diverse responses elicited by various situational contexts. Particularly, the prevalence of negative emotional states has been correlated with negative outcomes for mental health, necessitating a comprehensive analysis of their occurrence and impact on individuals. In this paper, we introduce a novel dataset named DepressionEmo designed to detect 8 emotions associated with depression by 6037 examples of long Reddit user posts. This dataset was created through a majority vote over inputs by zero-shot classifications from pre-trained models and validating the quality by annotators and ChatGPT, exhibiting an acceptable level of interrater reliability between annotators. The correlation between emotions, their distribution over time, and linguistic analysis are conducted on DepressionEmo. Besides, we provide several text classification methods classified into two groups: machine learning methods such as SVM, XGBoost, and Light GBM; a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#25253;&#21578;&#30340;&#20248;&#20808;&#32423;&#25490;&#24207;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#36739;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19979;&#20173;&#33021;&#20445;&#25345;&#21487;&#38752;&#24615;&#65292;&#20943;&#23569;&#20102;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04637</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;API&#24212;&#29992;&#20110;&#38382;&#39064;&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Applying Large Language Models API to Issue Classification Problem. (arXiv:2401.04637v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#25253;&#21578;&#30340;&#20248;&#20808;&#32423;&#25490;&#24207;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#36739;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19979;&#20173;&#33021;&#20445;&#25345;&#21487;&#38752;&#24615;&#65292;&#20943;&#23569;&#20102;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#65292;&#38382;&#39064;&#25253;&#21578;&#30340;&#26377;&#25928;&#25490;&#24207;&#23545;&#20110;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#21644;&#21450;&#26102;&#35299;&#20915;&#20851;&#38190;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#23545;&#38382;&#39064;&#25253;&#21578;&#36827;&#34892;&#20998;&#31867;&#20197;&#36827;&#34892;&#25490;&#24207;&#26159;&#36153;&#21147;&#19988;&#32570;&#20047;&#21487;&#20280;&#32553;&#24615;&#30340;&#12290;&#30456;&#21453;&#65292;&#35768;&#22810;&#24320;&#28304;&#36719;&#20214;&#39033;&#30446;&#20351;&#29992;&#33258;&#21160;&#21270;&#27969;&#31243;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#23613;&#31649;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20805;&#20998;&#30340;&#35757;&#32451;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#22312;&#20351;&#29992;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26102;&#20173;&#33021;&#30830;&#20445;&#38382;&#39064;&#25490;&#24207;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#30340;&#33021;&#21147;&#65292;&#35748;&#35782;&#21040;&#23427;&#20204;&#22312;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#26102;&#30340;&#39640;&#25928;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#20934;&#30830;&#20248;&#20808;&#32423;&#38382;&#39064;&#25253;&#21578;&#30340;&#21487;&#38752;&#31995;&#32479;&#65292;&#38477;&#20302;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#38752;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20110;GPT&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#26631;&#35760;&#38382;&#39064;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective prioritization of issue reports is crucial in software engineering to optimize resource allocation and address critical problems promptly. However, the manual classification of issue reports for prioritization is laborious and lacks scalability. Alternatively, many open source software (OSS) projects employ automated processes for this task, albeit relying on substantial datasets for adequate training. This research seeks to devise an automated approach that ensures reliability in issue prioritization, even when trained on smaller datasets. Our proposed methodology harnesses the power of Generative Pre-trained Transformers (GPT), recognizing their potential to efficiently handle this task. By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports accurately, mitigating the necessity for extensive training data while maintaining reliability. In our research, we have developed a reliable GPT-based approach to accurately labe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DebugBench&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#19982;&#20154;&#31867;&#30456;&#27604;&#20855;&#26377;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#26410;&#33021;&#36798;&#21040;&#21512;&#26684;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04621</link><description>&lt;p&gt;
DebugBench: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DebugBench: Evaluating Debugging Capability of Large Language Models. (arXiv:2401.04621v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DebugBench&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#19982;&#20154;&#31867;&#30456;&#27604;&#20855;&#26377;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#26410;&#33021;&#36798;&#21040;&#21512;&#26684;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#32534;&#31243;&#33021;&#21147;&#30340;&#21478;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;LLMs&#30340;&#35843;&#35797;&#33021;&#21147;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#20043;&#21069;&#23545;LLMs&#30340;&#35843;&#35797;&#33021;&#21147;&#35780;&#20272;&#21463;&#21040;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#12289;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#27979;&#35797;&#28431;&#27934;&#31181;&#31867;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#19981;&#36275;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;DebugBench&#8221;&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#21253;&#21547;4253&#20010;&#23454;&#20363;&#12290;&#23427;&#28085;&#30422;&#20102;C ++&#65292;Java&#21644;Python&#20013;&#22235;&#20010;&#20027;&#35201;&#30340;&#38169;&#35823;&#31867;&#21035;&#21644;18&#20010;&#27425;&#35201;&#31867;&#22411;&#12290;&#20026;&#20102;&#26500;&#24314;DebugBench&#65292;&#25105;&#20204;&#20174;LeetCode&#31038;&#21306;&#25910;&#38598;&#20102;&#20195;&#30721;&#29255;&#27573;&#65292;&#20351;&#29992;GPT-4&#21521;&#28304;&#25968;&#25454;&#20013;&#27880;&#20837;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#20005;&#26684;&#30340;&#36136;&#37327;&#26816;&#26597;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#20363;&#24773;&#20917;&#19979;&#35780;&#20272;&#20102;&#20004;&#20010;&#21830;&#19994;&#27169;&#22411;&#21644;&#19977;&#20010;&#24320;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#65288;1&#65289;&#19982;&#20154;&#31867;&#30456;&#27604;&#65292;&#38381;&#28304;&#27169;&#22411;&#22914;GPT-4&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#22914;Code Llama&#26080;&#27861;&#36798;&#21040;&#20219;&#20309;&#21512;&#26684;&#29575;&#65307;&#65288;2&#65289;t
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04620</link><description>&lt;p&gt;
&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#30340;Agent&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Agent Alignment in Evolving Social Norms. (arXiv:2401.04620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Agent&#36234;&#26469;&#36234;&#22810;&#22320;&#28183;&#36879;&#21040;&#20154;&#31867;&#29983;&#20135;&#21644;&#29983;&#27963;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#20984;&#26174;&#20102;&#23558;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;&#30446;&#21069;AI&#31995;&#32479;&#30340;&#23545;&#40784;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#20154;&#20026;&#24178;&#39044;&#23545;LLM&#36827;&#34892;&#34987;&#21160;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;Agent&#20855;&#26377;&#25509;&#21463;&#29615;&#22659;&#21453;&#39304;&#21644;&#33258;&#25105;&#36827;&#21270;&#31561;&#29305;&#24615;&#65292;&#20351;&#24471;LLM&#23545;&#40784;&#26041;&#27861;&#21464;&#24471;&#19981;&#36275;&#22815;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;Agent&#36827;&#21270;&#21644;&#23545;&#40784;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#12290;&#22312;&#31038;&#20250;&#35268;&#33539;&#19981;&#26029;&#28436;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#65292;&#32780;&#23545;&#40784;&#19981;&#36275;&#30340;Agent&#21017;&#36880;&#28176;&#20943;&#23569;&#12290;&#36890;&#36807;&#22810;&#20010;&#35282;&#24230;&#23545;&#19982;&#31038;&#20250;&#35268;&#33539;&#30456;&#23545;&#40784;&#30340;Agent&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;BERT&#36827;&#34892;&#35821;&#35328;&#20998;&#31867;&#21644;Google Translate API&#36827;&#34892;&#38899;&#35793;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#38899;&#35793;&#25991;&#26412;&#35821;&#35328;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#22312;&#25968;&#23383;&#20132;&#27969;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#29615;&#22659;&#20013;&#35782;&#21035;&#21644;&#36716;&#25442;&#38899;&#35793;&#25991;&#26412;&#24320;&#36767;&#20102;&#26032;&#30340;&#21019;&#26032;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2401.04619</link><description>&lt;p&gt;
&#27714;&#35782;&#21035;&#38899;&#35793;&#20869;&#23481;&#30340;&#35821;&#35328;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language Detection for Transliterated Content. (arXiv:2401.04619v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04619
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;BERT&#36827;&#34892;&#35821;&#35328;&#20998;&#31867;&#21644;Google Translate API&#36827;&#34892;&#38899;&#35793;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#38899;&#35793;&#25991;&#26412;&#35821;&#35328;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#22312;&#25968;&#23383;&#20132;&#27969;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#29615;&#22659;&#20013;&#35782;&#21035;&#21644;&#36716;&#25442;&#38899;&#35793;&#25991;&#26412;&#24320;&#36767;&#20102;&#26032;&#30340;&#21019;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#25968;&#23383;&#21270;&#26102;&#20195;&#65292;&#20114;&#32852;&#32593;&#20316;&#20026;&#19968;&#20010;&#26080;&#19982;&#20262;&#27604;&#30340;&#20652;&#21270;&#21058;&#65292;&#25171;&#30772;&#20102;&#22320;&#29702;&#21644;&#35821;&#35328;&#30340;&#38556;&#30861;&#65292;&#23588;&#20854;&#22312;&#30701;&#20449;&#26041;&#38754;&#34920;&#29616;&#24471;&#23588;&#20026;&#26126;&#26174;&#12290;&#36825;&#31181;&#21457;&#23637;&#20419;&#36827;&#20102;&#20840;&#29699;&#20132;&#27969;&#65292;&#36229;&#36234;&#20102;&#29289;&#29702;&#36317;&#31163;&#65292;&#20419;&#36827;&#20102;&#21160;&#24577;&#30340;&#25991;&#21270;&#20132;&#27969;&#12290;&#19968;&#20010;&#26174;&#33879;&#30340;&#36235;&#21183;&#26159;&#24191;&#27867;&#20351;&#29992;&#38899;&#35793;&#65292;&#21363;&#20351;&#29992;&#33521;&#25991;&#23383;&#27597;&#26469;&#20256;&#36798;&#27597;&#35821;&#30340;&#20449;&#24687;&#65292;&#36825;&#32473;&#35821;&#35328;&#25216;&#26415;&#20934;&#30830;&#35782;&#21035;&#28304;&#35821;&#35328;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#21253;&#21547;&#21360;&#22320;&#35821;&#21644;&#20420;&#35821;&#38899;&#35793;&#25104;&#33521;&#25991;&#30340;&#25163;&#26426;&#30701;&#20449;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;BERT&#36827;&#34892;&#35821;&#35328;&#20998;&#31867;&#24182;&#20351;&#29992;Google Translate API&#36827;&#34892;&#38899;&#35793;&#36716;&#25442;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#24320;&#21019;&#20102;&#35782;&#21035;&#21644;&#36716;&#25442;&#38899;&#35793;&#25991;&#26412;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#25968;&#23383;&#20132;&#27969;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#29615;&#22659;&#20013;&#38754;&#20020;&#30528;&#21508;&#31181;&#25361;&#25112;&#12290;&#24378;&#35843;&#20840;&#38754;&#25968;&#25454;&#38598;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#35757;&#32451;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the contemporary digital era, the Internet functions as an unparalleled catalyst, dismantling geographical and linguistic barriers particularly evident in texting. This evolution facilitates global communication, transcending physical distances and fostering dynamic cultural exchange. A notable trend is the widespread use of transliteration, where the English alphabet is employed to convey messages in native languages, posing a unique challenge for language technology in accurately detecting the source language. This paper addresses this challenge through a dataset of phone text messages in Hindi and Russian transliterated into English utilizing BERT for language classification and Google Translate API for transliteration conversion. The research pioneers innovative approaches to identify and convert transliterated text, navigating challenges in the diverse linguistic landscape of digital communication. Emphasizing the pivotal role of comprehensive datasets for training Large Langua
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#24515;&#29702;&#20581;&#24247;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#34920;&#36798;&#20154;&#31867;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#26041;&#38754;&#30340;&#29702;&#35299;&#33021;&#21147;&#36229;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.04592</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20102;&#35299;&#24515;&#29702;&#20581;&#24247;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Assessment on Comprehending Mental Health through Large Language Models. (arXiv:2401.04592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04592
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#24515;&#29702;&#20581;&#24247;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#34920;&#36798;&#20154;&#31867;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#26041;&#38754;&#30340;&#29702;&#35299;&#33021;&#21147;&#36229;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#20581;&#24247;&#25361;&#25112;&#23545;&#20010;&#20154;&#21644;&#31038;&#21306;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#20840;&#29699;&#36127;&#25285;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#36229;&#36807;20%&#30340;&#25104;&#24180;&#20154;&#22312;&#20182;&#20204;&#30340;&#19968;&#29983;&#20013;&#21487;&#33021;&#20250;&#36935;&#21040;&#33267;&#23569;&#19968;&#31181;&#24515;&#29702;&#38556;&#30861;&#12290;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#28982;&#32780;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#30340;&#29702;&#35299;&#21644;&#25552;&#21319;&#20173;&#23384;&#22312;&#37325;&#35201;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#20154;&#31867;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#34920;&#36798;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#19968;&#31354;&#30333;&#30340;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21021;&#27493;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;Llama-2&#21644;ChatGPT&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;DAIC-WOZ&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;&#22914;BERT&#25110;XLNet&#65289;&#30340;&#24615;&#33021;&#20248;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mental health challenges pose considerable global burdens on individuals and communities. Recent data indicates that more than 20% of adults may encounter at least one mental disorder in their lifetime. On the one hand, the advancements in large language models have facilitated diverse applications, yet a significant research gap persists in understanding and enhancing the potential of large language models within the domain of mental health. On the other hand, across various applications, an outstanding question involves the capacity of large language models to comprehend expressions of human mental health conditions in natural language. This study presents an initial evaluation of large language models in addressing this gap. Due to this, we compare the performance of Llama-2 and ChatGPT with classical Machine as well as Deep learning models. Our results on the DAIC-WOZ dataset show that transformer-based models, like BERT or XLNet, outperform the large language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20849;&#21516;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LM&#30340;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04536</link><description>&lt;p&gt;
&#36890;&#36807;&#35848;&#21028;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Language Model Agency through Negotiations. (arXiv:2401.04536v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20849;&#21516;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LM&#30340;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#21496;&#12289;&#32452;&#32455;&#21644;&#25919;&#24220;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#23637;&#31034;&#31867;&#20284;&#20195;&#29702;&#34892;&#20026;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#38543;&#30528;LM&#34987;&#37319;&#29992;&#26469;&#25191;&#34892;&#36234;&#26469;&#36234;&#20855;&#26377;&#33258;&#20027;&#24615;&#30340;&#20219;&#21153;&#65292;&#36843;&#20999;&#38656;&#35201;&#21487;&#38752;&#19988;&#21487;&#25193;&#23637;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#24403;&#21069;&#20027;&#35201;&#26159;&#38745;&#24577;&#30340;LM&#22522;&#20934;&#26080;&#27861;&#24456;&#22909;&#22320;&#35780;&#20272;&#27492;&#31867;&#21160;&#24577;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#26469;&#20849;&#21516;&#35780;&#20272;LM&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#20849;&#21516;&#20219;&#21153;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;LM&#20915;&#31574;&#36807;&#31243;&#30340;&#35265;&#35299;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#35848;&#21028;&#28216;&#25103;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#35843;&#25972;&#22797;&#26434;&#24615;&#65292;&#24182;&#36991;&#20813;&#35780;&#20272;&#20013;&#30340;&#24847;&#22806;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#26469;&#33258;&#20960;&#20010;&#20027;&#35201;&#20379;&#24212;&#21830;&#30340;&#20845;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;LM&#22312;&#21508;&#31181;&#35848;&#21028;&#28216;&#25103;&#19978;&#30340;&#32467;&#26524;&#65292;&#35780;&#20272;&#20102;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;i&#65289;&#24320;&#28304;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source mode
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;MERA&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#20420;&#35821;&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#12290;&#35813;&#25351;&#26631;&#21253;&#25324;21&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;11&#20010;&#25216;&#33021;&#39046;&#22495;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22266;&#23450;&#25351;&#20196;&#35774;&#32622;&#19979;&#35780;&#20272;FM&#21644;LM&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04531</link><description>&lt;p&gt;
MERA: &#20420;&#35821;LLM&#32508;&#21512;&#35780;&#20272;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
MERA: A Comprehensive LLM Evaluation in Russian. (arXiv:2401.04531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04531
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;MERA&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#20420;&#35821;&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#12290;&#35813;&#25351;&#26631;&#21253;&#25324;21&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;11&#20010;&#25216;&#33021;&#39046;&#22495;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22266;&#23450;&#25351;&#20196;&#35774;&#32622;&#19979;&#35780;&#20272;FM&#21644;LM&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#26368;&#26174;&#33879;&#30340;&#36827;&#23637;&#20043;&#19968;&#26159;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#30340;&#21457;&#23637;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#23835;&#36215;&#24341;&#20154;&#27880;&#30446;&#12290;&#38543;&#30528;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#22823;&#65292;LM&#22312;&#21487;&#34913;&#37327;&#30340;&#26041;&#38754;&#23637;&#31034;&#20102;&#25552;&#21319;&#65292;&#24182;&#19988;&#21457;&#23637;&#20986;&#20102;&#26032;&#30340;&#23450;&#24615;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#21644;LM&#24212;&#29992;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;LM&#30340;&#33021;&#21147;&#12289;&#38480;&#21046;&#21644;&#30456;&#20851;&#39118;&#38505;&#20173;&#38656;&#26356;&#22909;&#22320;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;&#20420;&#35821;&#22810;&#27169;&#24577;&#26550;&#26500;&#35780;&#20272;&#65288;MERA&#65289;&#25351;&#23548;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#20197;&#20420;&#35821;&#20026;&#23548;&#21521;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#35813;&#22522;&#20934;&#28085;&#30422;&#20102;11&#20010;&#25216;&#33021;&#39046;&#22495;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;21&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#24182;&#34987;&#35774;&#35745;&#20026;&#40657;&#30418;&#27979;&#35797;&#65292;&#20197;&#30830;&#20445;&#25490;&#38500;&#25968;&#25454;&#27844;&#28431;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22266;&#23450;&#25351;&#20196;&#35774;&#32622;&#19979;&#35780;&#20272;FM&#21644;LM&#30340;&#26041;&#27861;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features. However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in zeroand few-shot fixed instruction settings that can be extended to other modalities. We propose an 
&lt;/p&gt;</description></item><item><title>LUNA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#25509;&#21475;&#21644;20&#20010;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#21487;&#20197;&#26681;&#25454;&#21442;&#32771;&#20381;&#36182;&#24615;&#21644;&#25991;&#26412;&#34920;&#31034;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.04522</link><description>&lt;p&gt;
LUNA: &#19968;&#20010;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#33258;&#28982;&#24230;&#35780;&#20272;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LUNA: A Framework for Language Understanding and Naturalness Assessment. (arXiv:2401.04522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04522
&lt;/p&gt;
&lt;p&gt;
LUNA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#25509;&#21475;&#21644;20&#20010;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#21487;&#20197;&#26681;&#25454;&#21442;&#32771;&#20381;&#36182;&#24615;&#21644;&#25991;&#26412;&#34920;&#31034;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#27169;&#22411;&#30340;&#35780;&#20272;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#65292;&#36843;&#20351;&#24320;&#21457;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#21508;&#20010;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;LUNA&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32479;&#19968;&#30340;&#25509;&#21475;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#21253;&#21547;20&#20010;NLG&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#12290;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#22522;&#20110;&#23427;&#20204;&#30340;&#21442;&#32771;&#20381;&#36182;&#24615;&#21644;&#25991;&#26412;&#34920;&#31034;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#22522;&#20110;&#23383;&#31526;&#20018;&#30340;n-gram&#37325;&#21472;&#21040;&#21033;&#29992;&#38745;&#24577;&#23884;&#20837;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;LUNA&#30340;&#31616;&#21333;&#35774;&#35745;&#20801;&#35768;&#36890;&#36807;&#20960;&#34892;&#20195;&#30721;&#36731;&#26494;&#25193;&#23637;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;LUNA&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of Natural Language Generation (NLG) models has gained increased attention, urging the development of metrics that evaluate various aspects of generated text. LUNA addresses this challenge by introducing a unified interface for 20 NLG evaluation metrics. These metrics are categorized based on their reference-dependence and the type of text representation they employ, from string-based n-gram overlap to the utilization of static embeddings and pre-trained language models.  The straightforward design of LUNA allows for easy extension with novel metrics, requiring just a few lines of code. LUNA offers a user-friendly tool for evaluating generated texts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#20803;&#25209;&#35780;(MetaCritique)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35780;&#20272;&#25209;&#35780;&#30340;&#36136;&#37327;&#65292;&#24182;&#20197;F1&#20998;&#25968;&#20316;&#20026;&#25972;&#20307;&#35780;&#20998;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#24341;&#20837;&#20102;&#21407;&#23376;&#20449;&#24687;&#21333;&#20803;(AIUs)&#26469;&#25551;&#36848;&#25209;&#35780;&#12290;&#20803;&#25209;&#35780;&#25552;&#20379;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#35780;&#20215;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.04518</link><description>&lt;p&gt;
&#12298;&#25209;&#35780;&#30340;&#25209;&#35780;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Critique of Critique. (arXiv:2401.04518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#20803;&#25209;&#35780;(MetaCritique)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35780;&#20272;&#25209;&#35780;&#30340;&#36136;&#37327;&#65292;&#24182;&#20197;F1&#20998;&#25968;&#20316;&#20026;&#25972;&#20307;&#35780;&#20998;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#24341;&#20837;&#20102;&#21407;&#23376;&#20449;&#24687;&#21333;&#20803;(AIUs)&#26469;&#25551;&#36848;&#25209;&#35780;&#12290;&#20803;&#25209;&#35780;&#25552;&#20379;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#35780;&#20215;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#35780;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#20869;&#23481;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#22312;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#34987;&#35777;&#26126;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#25209;&#35780;&#26412;&#36523;&#36136;&#37327;&#26041;&#38754;&#32570;&#20047;&#21407;&#21017;&#24615;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#39318;&#21019;&#20102;&#25209;&#35780;&#30340;&#25209;&#35780;&#65292;&#31216;&#20026;&#20803;&#25209;&#35780;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#25209;&#35780;&#30340;&#26694;&#26550;&#65292;&#20174;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#20004;&#20010;&#26041;&#38754;&#26469;&#35780;&#20272;&#25209;&#35780;&#12290;&#25105;&#20204;&#35745;&#31639;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#35843;&#21644;&#24179;&#22343;&#20540;&#20316;&#20026;&#25972;&#20307;&#35780;&#20998;&#65292;&#31216;&#20026;F1&#20998;&#25968;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#23376;&#20449;&#24687;&#21333;&#20803;(AIUs)&#65292;&#20197;&#26356;&#31934;&#32454;&#30340;&#26041;&#24335;&#25551;&#36848;&#25209;&#35780;&#12290;&#20803;&#25209;&#35780;&#32771;&#34385;&#27599;&#20010;AIU&#65292;&#24182;&#32858;&#21512;&#27599;&#20010;AIU&#30340;&#21028;&#26029;&#24471;&#21040;&#25972;&#20307;&#35780;&#20998;&#12290;&#27492;&#22806;&#65292;&#37492;&#20110;&#35780;&#20272;&#36807;&#31243;&#28041;&#21450;&#22797;&#26434;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#20803;&#25209;&#35780;&#25552;&#20379;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#35780;&#20215;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critique, as a natural language description for assessing the quality of model-generated content, has been proven to play an essential role in the training, evaluation, and refinement of Large Language Models (LLMs). However, there is a lack of principled understanding in evaluating the quality of the critique itself. In this paper, we pioneer the critique of critique, termed MetaCritique, which is a framework to evaluate the critique from two aspects, i.e., factuality as precision score and comprehensiveness as recall score. We calculate the harmonic mean of precision and recall as the overall rating called F1 score. To obtain a reliable evaluation outcome, we propose Atomic Information Units (AIUs), which describe the critique in a more fine-grained manner. MetaCritique takes each AIU into account and aggregates each AIU's judgment for the overall score. Moreover, given the evaluation process involves intricate reasoning, our MetaCritique provides a natural language rationale to supp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#19978;&#20301;&#35789;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#19982;&#32463;&#20856;&#27169;&#24335;&#20043;&#38388;&#23384;&#22312;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#36739;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#21021;&#27493;&#30340;&#25552;&#31034;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#35782;&#21035;&#30340;&#20849;&#21516;&#19979;&#20301;&#35789;&#23558;&#39069;&#22806;&#20449;&#24687;&#21152;&#20837;&#25552;&#31034;&#65292;&#21487;&#20197;&#25913;&#21892;&#19978;&#20301;&#35789;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#26469;&#39044;&#27979;&#26356;&#39640;&#23618;&#27425;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;BLESS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36136;&#37327;&#25552;&#21319;&#65288;MAP = 0.8&#65289;&#12290;</title><link>http://arxiv.org/abs/2401.04515</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#19978;&#20301;&#35789;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with Large Language Models. (arXiv:2401.04515v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#19978;&#20301;&#35789;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#19982;&#32463;&#20856;&#27169;&#24335;&#20043;&#38388;&#23384;&#22312;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#36739;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#21021;&#27493;&#30340;&#25552;&#31034;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#35782;&#21035;&#30340;&#20849;&#21516;&#19979;&#20301;&#35789;&#23558;&#39069;&#22806;&#20449;&#24687;&#21152;&#20837;&#25552;&#31034;&#65292;&#21487;&#20197;&#25913;&#21892;&#19978;&#20301;&#35789;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#26469;&#39044;&#27979;&#26356;&#39640;&#23618;&#27425;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;BLESS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36136;&#37327;&#25552;&#21319;&#65288;MAP = 0.8&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#38646;&#26679;&#26412;&#19978;&#20301;&#35789;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#22522;&#20110;&#25991;&#26412;&#27010;&#29575;&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#29983;&#25104;&#30340;&#25552;&#31034;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#19982;&#32463;&#20856;&#27169;&#24335;&#20043;&#38388;&#23384;&#22312;&#30528;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#36739;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#21021;&#27493;&#30340;&#25552;&#31034;&#36873;&#25321;&#65292;&#28982;&#21518;&#20877;&#36716;&#21521;&#26356;&#22823;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#29992;&#20110;&#39044;&#27979;&#20849;&#21516;&#19979;&#20301;&#35789;&#21644;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#30340;&#20849;&#21516;&#19979;&#20301;&#35789;&#23558;&#39069;&#22806;&#20449;&#24687;&#21152;&#20837;&#25552;&#31034;&#20197;&#25913;&#21892;&#19978;&#20301;&#35789;&#39044;&#27979;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#26469;&#39044;&#27979;&#26356;&#39640;&#23618;&#27425;&#30340;&#27010;&#24565;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22312;BLESS&#25968;&#25454;&#38598;&#19978;&#30340;&#36136;&#37327;&#65288;MAP = 0.8&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article investigates a zero-shot approach to hypernymy prediction using large language models (LLMs). The study employs a method based on text probability calculation, applying it to various generated prompts. The experiments demonstrate a strong correlation between the effectiveness of language model prompts and classic patterns, indicating that preliminary prompt selection can be carried out using smaller models before moving to larger ones. We also explore prompts for predicting co-hyponyms and improving hypernymy predictions by augmenting prompts with additional information through automatically identified co-hyponyms. An iterative approach is developed for predicting higher-level concepts, which further improves the quality on the BLESS dataset (MAP = 0.8).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#30340;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#23384;&#22312;&#30340;&#39118;&#26684;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04514</link><description>&lt;p&gt;
&#37325;&#20889;&#20195;&#30721;&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#20195;&#30721;&#25628;&#32034;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search. (arXiv:2401.04514v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#30340;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#23384;&#22312;&#30340;&#39118;&#26684;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20195;&#30721;&#25628;&#32034;&#20013;&#65292;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#31034;&#20363;&#20195;&#30721;&#29255;&#27573;&#26469;&#22686;&#24378;&#26597;&#35810;&#65292;&#20197;&#35299;&#20915;&#20195;&#30721;&#29255;&#27573;&#21644;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20043;&#38388;&#30340;&#20027;&#35201;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#35843;&#26597;&#21457;&#29616;&#65292;LLM&#22686;&#24378;&#26694;&#26550;&#25152;&#25552;&#20379;&#30340;&#25913;&#36827;&#26377;&#19968;&#23450;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#38480;&#21046;&#21487;&#33021;&#26159;&#22240;&#20026;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#23613;&#31649;&#22312;&#21151;&#33021;&#19978;&#20934;&#30830;&#65292;&#20294;&#22312;&#20195;&#30721;&#24211;&#20013;&#19982;&#22522;&#20934;&#20195;&#30721;&#20043;&#38388;&#32463;&#24120;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#39118;&#26684;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22522;&#30784;GAR&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#24211;&#20013;&#30340;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#65288;ReCo&#65289;&#26469;&#36827;&#34892;&#39118;&#26684;&#35268;&#33539;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReCo&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy ac
&lt;/p&gt;</description></item><item><title>TechGPT-2.0&#26159;&#19968;&#20010;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39033;&#30446;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#25991;&#26412;&#22788;&#29702;&#33021;&#21147;&#21644;&#22810;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04507</link><description>&lt;p&gt;
TechGPT-2.0:&#19968;&#20010;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
TechGPT-2.0: A large language model project to solve the task of knowledge graph construction. (arXiv:2401.04507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04507
&lt;/p&gt;
&lt;p&gt;
TechGPT-2.0&#26159;&#19968;&#20010;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39033;&#30446;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#25991;&#26412;&#22788;&#29702;&#33021;&#21147;&#21644;&#22810;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#37117;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;TechGPT-2.0&#65292;&#36825;&#26159;&#19968;&#20010;&#39033;&#30446;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#65288;RTE&#65289;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#20316;&#20026;&#19968;&#20010;&#38754;&#21521;&#20013;&#22269;&#24320;&#28304;&#27169;&#22411;&#31038;&#21306;&#30340;LLM&#21487;&#35775;&#38382;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;7B&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;QLoRA&#26435;&#37325;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;TechGPT-2.0&#26159;&#22312;&#21326;&#20026;&#30340;Ascend&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#30340;&#12290;&#32487;&#25215;&#20102;TechGPT-1.0&#30340;&#25152;&#26377;&#21151;&#33021;&#65292;&#23427;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#25991;&#26412;&#22788;&#29702;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#21644;&#27861;&#24459;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#27169;&#22411;&#24341;&#20837;&#20102;&#26032;&#30340;&#21151;&#33021;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#21508;&#20010;&#39046;&#22495;&#30340;&#25991;&#26412;&#65292;&#22914;&#22320;&#29702;&#21306;&#22495;&#12289;&#20132;&#36890;&#12289;&#32452;&#32455;&#26426;&#26500;&#12289;&#25991;&#23398;&#20316;&#21697;&#12289;&#29983;&#29289;&#23398;&#21644;&#33258;&#28982;&#31185;&#23398;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have exhibited robust performance across diverse natural language processing tasks. This report introduces TechGPT-2.0, a project designed to enhance the capabilities of large language models specifically in knowledge graph construction tasks, including named entity recognition (NER) and relationship triple extraction (RTE) tasks in NLP applications. Additionally, it serves as a LLM accessible for research within the Chinese open-source model community. We offer two 7B large language model weights and a QLoRA weight specialized for processing lengthy texts.Notably, TechGPT-2.0 is trained on Huawei's Ascend server. Inheriting all functionalities from TechGPT-1.0, it exhibits robust text processing capabilities, particularly in the domains of medicine and law. Furthermore, we introduce new capabilities to the model, enabling it to process texts in various domains such as geographical areas, transportation, organizations, literary works, biology, natural sciences, as
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04482</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#25345;&#32493;&#23398;&#20064;&#26032;&#35789;
&lt;/p&gt;
&lt;p&gt;
Continuously Learning New Words in Automatic Speech Recognition. (arXiv:2401.04482v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04482
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20173;&#28982;&#36828;&#26410;&#23436;&#32654;&#12290;&#20856;&#22411;&#30340;&#38169;&#35823;&#21253;&#25324;&#32553;&#20889;&#35789;&#12289;&#21629;&#21517;&#23454;&#20307;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#19987;&#29992;&#35789;&#65292;&#36825;&#20123;&#35789;&#20960;&#20046;&#27809;&#26377;&#25110;&#27809;&#26377;&#25968;&#25454;&#21487;&#29992;&#26469;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#35782;&#21035;&#36825;&#20123;&#35789;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#32473;&#23450;&#24102;&#26377;&#23545;&#24212;&#24187;&#28783;&#29255;&#30340;&#35762;&#24231;&#24405;&#38899;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#35760;&#24518;&#22686;&#24378;&#22411;ASR&#27169;&#22411;&#26469;&#23558;&#27169;&#22411;&#20559;&#21521;&#20110;&#20174;&#24187;&#28783;&#29255;&#20013;&#35299;&#30721;&#26032;&#35789;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#35762;&#24231;&#36827;&#34892;&#25512;&#29702;&#65292;&#23558;&#21253;&#21547;&#26816;&#27979;&#21040;&#30340;&#26032;&#35789;&#30340;&#35805;&#35821;&#25910;&#38598;&#21040;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#20013;&#12290;&#25509;&#30528;&#65292;&#23545;&#36825;&#20010;&#38598;&#21512;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#35843;&#25972;&#28155;&#21152;&#21040;&#27169;&#22411;&#30340;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#30340;&#20302;&#31209;&#30697;&#38453;&#26435;&#37325;&#12290;&#25972;&#20010;&#36807;&#31243;&#23545;&#22810;&#20010;&#35762;&#24231;&#36827;&#34892;&#36845;&#20195;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#33719;&#24471;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#65288;&#36229;&#36807;80%&#30340;&#21484;&#22238;&#29575;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20854;&#36827;&#34892;&#24341;&#23548;&#65292;&#33258;&#21160;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#36776;&#21035;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#26631;&#27880;&#25968;&#25454;&#25152;&#38656;&#30340;&#22823;&#37327;&#20154;&#24037;&#21162;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.04481</link><description>&lt;p&gt;
&#20197;&#28779;&#25915;&#28779;&#65306;&#23545;&#25239;&#21551;&#21457;&#24335;&#29983;&#25104;&#19968;&#20010;&#36776;&#21035;&#34394;&#20551;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset. (arXiv:2401.04481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20854;&#36827;&#34892;&#24341;&#23548;&#65292;&#33258;&#21160;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#36776;&#21035;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#26631;&#27880;&#25968;&#25454;&#25152;&#38656;&#30340;&#22823;&#37327;&#20154;&#24037;&#21162;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;GPT&#12289;Bard&#21644;Llama&#31561;&#65292;&#22312;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25104;&#21151;&#21487;&#33021;&#24341;&#21457;&#23545;&#20854;&#34987;&#28389;&#29992;&#30340;&#25285;&#24551;&#65292;&#27604;&#22914;&#36890;&#36807;&#29983;&#25104;&#20551;&#26032;&#38395;&#21644;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#24341;&#21457;&#22823;&#35268;&#27169;&#28608;&#21160;&#21644;&#20167;&#24680;&#12290;&#20256;&#32479;&#30340;&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#24320;&#21457;&#26041;&#27861;&#22312;&#26631;&#27880;&#25968;&#25454;&#26102;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#21162;&#21147;&#65292;&#26080;&#27861;&#24456;&#22909;&#22320;&#25193;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#29992;&#20110;&#35782;&#21035;&#34394;&#20551;&#20449;&#24687;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#21487;&#20449;&#26032;&#38395;&#25991;&#31456;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24341;&#23548;LLM&#33258;&#21160;&#29983;&#25104;&#21407;&#22987;&#25991;&#31456;&#30340;&#25688;&#35201;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#30340;&#24341;&#23548;&#20316;&#20026;&#19968;&#31181;&#25511;&#21046;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#29983;&#25104;&#30340;&#25688;&#35201;&#20013;&#20135;&#29983;&#29305;&#23450;&#31867;&#22411;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#20363;&#22914;&#38169;&#35823;&#30340;&#25968;&#37327;&#12289;&#38169;&#35823;&#30340;&#24402;&#23646;&#22320;&#31561;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success in language generation capabilities of large language models (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns about their possible misuse in inducing mass agitation and communal hatred via generating fake news and spreading misinformation. Traditional means of developing a misinformation ground-truth dataset does not scale well because of the extensive manual effort required to annotate the data. In this paper, we propose an LLM-based approach of creating silver-standard ground-truth datasets for identifying misinformation. Specifically speaking, given a trusted news article, our proposed approach involves prompting LLMs to automatically generate a summarised version of the original article. The prompts in our proposed approach act as a controlling mechanism to generate specific types of factual incorrectness in the generated summaries, e.g., incorrect quantities, false attributions etc. To investigate the usefulness of this dataset, we conduct
&lt;/p&gt;</description></item><item><title>TwinBooster&#32467;&#21512;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#25351;&#32441;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04478</link><description>&lt;p&gt;
TwinBooster: &#32467;&#21512;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#22686;&#24378;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction. (arXiv:2401.04478v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04478
&lt;/p&gt;
&lt;p&gt;
TwinBooster&#32467;&#21512;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#25351;&#32441;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#21644;&#24320;&#21457;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#23545;&#20998;&#23376;&#27963;&#24615;&#21644;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#34429;&#28982;&#22522;&#20110;&#35745;&#31639;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#20351;&#29992;&#36804;&#20170;&#20026;&#27490;&#20165;&#38480;&#20110;&#22823;&#37327;&#25968;&#25454;&#21487;&#29992;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#25991;&#26412;&#20449;&#24687;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;Siamese&#31070;&#32463;&#32593;&#32476;Barlow Twins&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#26816;&#27979;&#26041;&#27861;&#20449;&#24687;&#21644;&#20998;&#23376;&#25351;&#32441;&#25552;&#21462;&#30495;&#23454;&#30340;&#20998;&#23376;&#20449;&#24687;&#12290;TwinBooster&#36890;&#36807;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#30340;&#23646;&#24615;&#39044;&#27979;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#27969;&#27700;&#32447;&#22312;FS-Mol&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#31361;&#30772;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36890;&#24120;&#25968;&#25454;&#31232;&#32570;&#30340;&#20851;&#38190;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of drug discovery and development relies on the precise prediction of molecular activities and properties. While in silico molecular property prediction has shown remarkable potential, its use has been limited so far to assays for which large amounts of data are available. In this study, we use a fine-tuned large language model to integrate biological assays based on their textual information, coupled with Barlow Twins, a Siamese neural network using a novel self-supervised learning approach. This architecture uses both assay information and molecular fingerprints to extract the true molecular information. TwinBooster enables the prediction of properties of unseen bioassays and molecules by providing state-of-the-art zero-shot learning tasks. Remarkably, our artificial intelligence pipeline shows excellent performance on the FS-Mol benchmark. This breakthrough demonstrates the application of deep learning to critical property prediction tasks where data is typically scarce.
&lt;/p&gt;</description></item><item><title>TransportationGames&#26159;&#19968;&#20010;&#35780;&#20272;(M)LLMs&#22312;&#20132;&#36890;&#39046;&#22495;&#34920;&#29616;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#32508;&#21512;&#32771;&#34385;&#20102;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#27979;&#35797;(M)LLMs&#30340;&#35760;&#24518;&#12289;&#25512;&#29702;&#12289;&#35299;&#37322;&#21644;&#35780;&#20272;&#20132;&#36890;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04471</link><description>&lt;p&gt;
TransportationGames: &#29992;&#20110;&#35780;&#20215;(&#22810;&#27169;&#24577;)&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20132;&#36890;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models. (arXiv:2401.04471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04471
&lt;/p&gt;
&lt;p&gt;
TransportationGames&#26159;&#19968;&#20010;&#35780;&#20272;(M)LLMs&#22312;&#20132;&#36890;&#39046;&#22495;&#34920;&#29616;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#32508;&#21512;&#32771;&#34385;&#20102;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#27979;&#35797;(M)LLMs&#30340;&#35760;&#24518;&#12289;&#25512;&#29702;&#12289;&#35299;&#37322;&#21644;&#35780;&#20272;&#20132;&#36890;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#21644;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (MLLMs) &#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#27861;&#24459;&#12289;&#32463;&#27982;&#12289;&#20132;&#36890;&#21644;&#21307;&#23398;&#31561;&#35768;&#22810;&#19987;&#19994;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#36866;&#24212;&#24615;&#12290;&#30446;&#21069;&#65292;&#24050;&#25552;&#20986;&#20102;&#35768;&#22810;&#39046;&#22495;&#29305;&#23450;&#30340;&#22522;&#20934;&#26469;&#39564;&#35777; (M)LLMs &#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#19981;&#21516;&#39046;&#22495;&#20013;&#65292;&#20132;&#36890;&#36816;&#36755;&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#22240;&#20026;&#23427;&#24433;&#21709;&#30528;&#32463;&#27982;&#12289;&#29615;&#22659;&#21644;&#25968;&#21313;&#20159;&#20154;&#27665;&#29983;&#27963;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970; (M)LLMs &#22810;&#23569;&#20132;&#36890;&#30693;&#35782;&#65292;&#24182;&#19988;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#21487;&#38752;&#22320;&#25191;&#34892;&#19982;&#20132;&#36890;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; TransportationGames&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#21644;&#20840;&#38754;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;(M)LLMs &#22312;&#20132;&#36890;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21442;&#29031;&#24067;&#40065;&#22982;&#20998;&#31867;&#27861;&#30340;&#21069;&#19977;&#20010;&#23618;&#27425;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;(M)LLMs &#22312;&#35760;&#24518;&#20132;&#36890;&#30693;&#35782;&#12289;&#25512;&#29702;&#20132;&#36890;&#27010;&#24565;&#12289;&#35299;&#37322;&#21644;&#35780;&#20272;&#20132;&#36890;&#38382;&#39064;&#31561;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) and multimodal large language models (MLLMs) have shown excellent general capabilities, even exhibiting adaptability in many professional domains such as law, economics, transportation, and medicine. Currently, many domain-specific benchmarks have been proposed to verify the performance of (M)LLMs in specific fields. Among various domains, transportation plays a crucial role in modern society as it impacts the economy, the environment, and the quality of life for billions of people. However, it is unclear how much traffic knowledge (M)LLMs possess and whether they can reliably perform transportation-related tasks. To address this gap, we propose TransportationGames, a carefully designed and thorough evaluation benchmark for assessing (M)LLMs in the transportation domain. By comprehensively considering the applications in real-world scenarios and referring to the first three levels in Bloom's Taxonomy, we test the performance of various (M)LLMs in memorizing
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;MultiNet&#35821;&#20041;&#32593;&#32476;&#30340;&#35821;&#20041;&#27010;&#24565;&#23884;&#20837;&#26041;&#27861;&#65292;&#32467;&#21512;&#20256;&#32479;&#35789;&#23884;&#20837;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#30446;&#26631;&#32676;&#32452;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04422</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#27010;&#24565;&#23884;&#20837;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimating Text Similarity based on Semantic Concept Embeddings. (arXiv:2401.04422v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04422
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;MultiNet&#35821;&#20041;&#32593;&#32476;&#30340;&#35821;&#20041;&#27010;&#24565;&#23884;&#20837;&#26041;&#27861;&#65292;&#32467;&#21512;&#20256;&#32479;&#35789;&#23884;&#20837;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#30446;&#26631;&#32676;&#32452;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#26131;&#29992;&#24615;&#21644;&#39640;&#20934;&#30830;&#24615;&#65292;Word2Vec (W2V) &#35789;&#23884;&#20837;&#22312;&#35821;&#20041;&#34920;&#31034;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#21253;&#25324;&#21333;&#35789;&#12289;&#21477;&#23376;&#21644;&#25972;&#20010;&#25991;&#26723;&#30340;&#35821;&#20041;&#34920;&#31034;&#20197;&#21450;&#35821;&#20041;&#30456;&#20284;&#24230;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32570;&#28857;&#26159;&#30452;&#25509;&#20174;&#34920;&#38754;&#34920;&#31034;&#20013;&#25552;&#21462;&#65292;&#19981;&#33021;&#20805;&#20998;&#20195;&#34920;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#23545;&#20110;&#39640;&#24230;&#27495;&#20041;&#30340;&#35789;&#20063;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;MultiNet&#35821;&#20041;&#32593;&#32476;(SN)&#24418;&#24335;&#21270;&#30340;&#35821;&#20041;&#27010;&#24565;&#23884;&#20837;(CE)&#65292;&#20197;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#22312;&#24066;&#22330;&#30446;&#26631;&#32676;&#20307;&#20998;&#24067;&#20219;&#21153;&#30340;&#35780;&#20272;&#20013;&#65292;&#32467;&#26524;&#26174;&#31034;&#23558;&#20256;&#32479;&#35789;&#23884;&#20837;&#21644;&#35821;&#20041;CE&#32467;&#21512;&#21487;&#20197;&#22686;&#21152;&#39044;&#27979;&#30446;&#26631;&#32676;&#32452;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their ease of use and high accuracy, Word2Vec (W2V) word embeddings enjoy great success in the semantic representation of words, sentences, and whole documents as well as for semantic similarity estimation. However, they have the shortcoming that they are directly extracted from a surface representation, which does not adequately represent human thought processes and also performs poorly for highly ambiguous words. Therefore, we propose Semantic Concept Embeddings (CE) based on the MultiNet Semantic Network (SN) formalism, which addresses both shortcomings. The evaluation on a marketing target group distribution task showed that the accuracy of predicted target groups can be increased by combining traditional word embeddings with semantic CEs.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Chain-of-Table&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38142;&#20013;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#20316;&#20026;&#20013;&#38388;&#24605;&#32500;&#30340;&#20195;&#29702;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#20013;&#36827;&#34892;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#28436;&#21270;&#30340;&#34920;&#26684;&#25512;&#29702;&#38142;&#12290;</title><link>http://arxiv.org/abs/2401.04398</link><description>&lt;p&gt;
Chain-of-Table: &#22312;&#25512;&#29702;&#38142;&#20013;&#28436;&#21270;&#34920;&#26684;&#29992;&#20110;&#34920;&#26684;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding. (arXiv:2401.04398v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04398
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Chain-of-Table&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38142;&#20013;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#20316;&#20026;&#20013;&#38388;&#24605;&#32500;&#30340;&#20195;&#29702;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#20013;&#36827;&#34892;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#28436;&#21270;&#30340;&#34920;&#26684;&#25512;&#29702;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#26684;&#25512;&#29702;&#26159;&#35299;&#20915;&#35768;&#22810;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#65288;&#22914;&#22522;&#20110;&#34920;&#26684;&#30340;&#38382;&#31572;&#21644;&#20107;&#23454;&#39564;&#35777;&#65289;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#19982;&#36890;&#24120;&#30340;&#25512;&#29702;&#30456;&#27604;&#65292;&#22522;&#20110;&#34920;&#26684;&#30340;&#25512;&#29702;&#38656;&#35201;&#20174;&#33258;&#30001;&#24418;&#24335;&#38382;&#39064;&#21644;&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#35821;&#20041;&#12290;Chain-of-Thought&#21450;&#20854;&#31867;&#20284;&#26041;&#27861;&#23558;&#25512;&#29702;&#38142;&#20197;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#24418;&#24335;&#32435;&#20837;&#20854;&#20013;&#65292;&#20294;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#22312;&#25512;&#29702;&#38142;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Chain-of-Table&#26694;&#26550;&#65292;&#20854;&#20013;&#34920;&#26684;&#25968;&#25454;&#20197;&#20316;&#20026;&#20013;&#38388;&#24605;&#32500;&#30340;&#20195;&#29702;&#26126;&#30830;&#22320;&#29992;&#20110;&#25512;&#29702;&#38142;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25351;&#23548;LLMs&#26469;&#36845;&#20195;&#29983;&#25104;&#25805;&#20316;&#24182;&#26356;&#26032;&#34920;&#26684;&#65292;&#20197;&#20195;&#34920;&#19968;&#20010;&#34920;&#26684;&#25512;&#29702;&#38142;&#12290;&#22240;&#27492;&#65292;LLMs&#21487;&#20197;&#26681;&#25454;&#20043;&#21069;&#25805;&#20316;&#30340;&#32467;&#26524;&#21160;&#24577;&#22320;&#35268;&#21010;&#19979;&#19968;&#20010;&#25805;&#20316;&#12290;&#36825;&#31181;&#34920;&#26684;&#30340;&#25345;&#32493;&#28436;&#21270;&#24418;&#25104;&#20102;&#19968;&#20010;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24739;&#32773;&#25253;&#21578;&#20307;&#39564;&#30340;&#24773;&#24863;&#24314;&#27169;&#26041;&#27861;&#65292;&#20351;&#29992;&#20803;&#25968;&#25454;&#32593;&#32476;&#20027;&#39064;&#24314;&#27169;&#20998;&#26512;&#24739;&#32773;&#25253;&#21578;&#30340;&#20307;&#39564;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#27010;&#29575;&#24773;&#24863;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#39044;&#27979;&#24773;&#24863;&#21644;&#24773;&#32490;&#26469;&#22686;&#24378;&#20256;&#32479;&#30340;&#24739;&#32773;&#25252;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04367</link><description>&lt;p&gt;
&#24739;&#32773;&#25253;&#21578;&#20307;&#39564;&#30340;&#27010;&#29575;&#24773;&#24863;&#21644;&#24773;&#32490;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Probabilistic emotion and sentiment modelling of patient-reported experiences. (arXiv:2401.04367v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24739;&#32773;&#25253;&#21578;&#20307;&#39564;&#30340;&#24773;&#24863;&#24314;&#27169;&#26041;&#27861;&#65292;&#20351;&#29992;&#20803;&#25968;&#25454;&#32593;&#32476;&#20027;&#39064;&#24314;&#27169;&#20998;&#26512;&#24739;&#32773;&#25253;&#21578;&#30340;&#20307;&#39564;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#27010;&#29575;&#24773;&#24863;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#39044;&#27979;&#24773;&#24863;&#21644;&#24773;&#32490;&#26469;&#22686;&#24378;&#20256;&#32479;&#30340;&#24739;&#32773;&#25252;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#24739;&#32773;&#22312;&#22312;&#32447;&#24739;&#32773;&#20307;&#39564;&#21465;&#36848;&#20013;&#30340;&#24773;&#32490;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#37319;&#29992;&#20803;&#25968;&#25454;&#32593;&#32476;&#20027;&#39064;&#24314;&#27169;&#26469;&#20998;&#26512;&#26469;&#33258;Care Opinion&#30340;&#24739;&#32773;&#25253;&#21578;&#30340;&#20307;&#39564;&#65292;&#25581;&#31034;&#20102;&#19982;&#24739;&#32773;-&#21307;&#25252;&#20154;&#21592;&#20114;&#21160;&#21644;&#20020;&#24202;&#32467;&#26524;&#30456;&#20851;&#30340;&#20851;&#38190;&#24773;&#32490;&#20027;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#27010;&#29575;&#12289;&#19978;&#19979;&#25991;&#29305;&#23450;&#30340;&#24773;&#24863;&#25512;&#33616;&#31995;&#32479;&#65292;&#33021;&#22815;&#20351;&#29992;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#22810;&#26631;&#31614;&#24773;&#32490;&#21644;&#20108;&#36827;&#21046;&#24773;&#32490;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#20027;&#39064;&#20316;&#20026;&#39044;&#27979;&#21464;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#20449;&#24687;&#26816;&#32034;&#25351;&#26631;nDCG&#21644;Q-measure&#35780;&#20272;&#25105;&#20204;&#39044;&#27979;&#30340;&#24773;&#32490;&#22312;&#36825;&#20010;&#27169;&#22411;&#19979;&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;&#21331;&#36234;&#34920;&#29616;&#65292;&#32780;&#25105;&#20204;&#39044;&#27979;&#30340;&#24773;&#32490;&#22312;F1&#24471;&#20998;&#26041;&#38754;&#36798;&#21040;0.921&#65292;&#26174;&#33879;&#20248;&#20110;&#26631;&#20934;&#30340;&#24773;&#24863;&#35789;&#20856;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36879;&#26126;&#12289;&#32463;&#27982;&#39640;&#25928;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299;&#24739;&#32773;&#30340;&#21453;&#39304;&#65292;&#22686;&#24378;&#20102;&#20256;&#32479;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#24182;&#20026;&#20010;&#20307;&#21270;&#30340;&#24739;&#32773;&#25252;&#29702;&#25552;&#20379;&#20102;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces a novel methodology for modelling patient emotions from online patient experience narratives. We employed metadata network topic modelling to analyse patient-reported experiences from Care Opinion, revealing key emotional themes linked to patient-caregiver interactions and clinical outcomes. We develop a probabilistic, context-specific emotion recommender system capable of predicting both multilabel emotions and binary sentiments using a naive Bayes classifier using contextually meaningful topics as predictors. The superior performance of our predicted emotions under this model compared to baseline models was assessed using the information retrieval metrics nDCG and Q-measure, and our predicted sentiments achieved an F1 score of 0.921, significantly outperforming standard sentiment lexicons. This method offers a transparent, cost-effective way to understand patient feedback, enhancing traditional collection methods and informing individualised patient care. Our fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#25552;&#39640;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#65288;KGD&#65289;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#21019;&#24314;&#27491;&#36127;&#26679;&#26412;&#20197;&#24212;&#23545;&#23454;&#38469;&#22122;&#38899;&#65292;&#22914;&#38169;&#21035;&#23383;&#21644;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;</title><link>http://arxiv.org/abs/2401.04361</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive Learning. (arXiv:2401.04361v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#25552;&#39640;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#65288;KGD&#65289;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#21019;&#24314;&#27491;&#36127;&#26679;&#26412;&#20197;&#24212;&#23545;&#23454;&#38469;&#22122;&#38899;&#65292;&#22914;&#38169;&#21035;&#23383;&#21644;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#65288;KGD&#65289;&#36890;&#36807;&#32473;&#23450;&#30340;&#23545;&#35805;&#29615;&#22659;&#21644;&#22806;&#37096;&#30693;&#35782;&#65288;&#22914;&#30693;&#35782;&#22270;&#35889;&#65289;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#22238;&#24212;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#20986;&#29616;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26500;&#24314;KGD&#31995;&#32479;&#26102;&#65292;&#38590;&#20813;&#20250;&#36935;&#21040;&#21508;&#31181;&#23454;&#38469;&#30340;&#22122;&#38899;&#12290;&#20363;&#22914;&#65292;&#23545;&#35805;&#29615;&#22659;&#21487;&#33021;&#28041;&#21450;&#38169;&#21035;&#23383;&#21644;&#32553;&#20889;&#31561;&#25200;&#21160;&#12290;&#27492;&#22806;&#65292;&#30693;&#35782;&#22270;&#35889;&#36890;&#24120;&#23384;&#22312;&#19981;&#23436;&#25972;&#24615;&#65292;&#20063;&#21487;&#33021;&#21253;&#21547;&#38169;&#35823;&#21644;&#36807;&#26102;&#30340;&#20107;&#23454;&#12290;&#36825;&#20123;&#23454;&#38469;&#30340;&#22122;&#38899;&#32473;KGD&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#24182;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#25552;&#39640;KGD&#31283;&#20581;&#24615;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;KGD&#26679;&#26412;&#20013;&#30340;&#23454;&#20307;&#20449;&#24687;&#21019;&#24314;&#20854;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-grounded dialogue (KGD) learns to generate an informative response based on a given dialogue context and external knowledge (\emph{e.g.}, knowledge graphs; KGs). Recently, the emergence of large language models (LLMs) and pre-training techniques has brought great success to knowledge-grounded dialogue. However, when building KGD systems in real applications, there are various real-world noises that are inevitable to face. For example, the dialogue context might involve perturbations such as misspellings and abbreviations. In addition, KGs typically suffer from incompletion and also might contain erroneous and outdated facts. Such real-world noises pose a challenge to the robustness of KGD systems and hinder their applications in the real world. In this paper, we propose an entity-based contrastive learning framework for improving the robustness of KGD. Specifically, we make use of the entity information in a KGD sample to create both its positive and negative samples which in
&lt;/p&gt;</description></item><item><title>LAMPAT&#26159;&#31532;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#25913;&#20889;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#29615;&#22659;&#19979;&#29983;&#25104;&#25913;&#20889;&#12290;</title><link>http://arxiv.org/abs/2401.04348</link><description>&lt;p&gt;
LAMPAT&#65306;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#36827;&#34892;&#20302;&#31209;&#22810;&#35821;&#35328;&#25913;&#20889;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using Adversarial Training. (arXiv:2401.04348v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04348
&lt;/p&gt;
&lt;p&gt;
LAMPAT&#26159;&#31532;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#25913;&#20889;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#29615;&#22659;&#19979;&#29983;&#25104;&#25913;&#20889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#20889;&#26159;&#25351;&#20351;&#29992;&#19981;&#21516;&#30340;&#35789;&#35821;&#25110;&#21477;&#23376;&#32467;&#26500;&#26469;&#20256;&#36798;&#30456;&#21516;&#21547;&#20041;&#30340;&#25991;&#26412;&#12290;&#23427;&#21487;&#20197;&#29992;&#20316;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#25968;&#25454;&#19981;&#36275;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#26102;&#12290;&#20026;&#20102;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#29983;&#25104;&#25913;&#20889;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#36890;&#36807;&#22312;&#30456;&#21516;&#35821;&#35328;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#26426;&#22120;&#32763;&#35793;&#26469;&#24418;&#25104;&#25913;&#20889;&#12290;&#23613;&#31649;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38656;&#35201;&#24179;&#34892;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#26080;&#27861;&#24212;&#29992;&#20110;&#27809;&#26377;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#25913;&#20889;&#27169;&#22411;&#65292;LAMPAT&#65288;&#20302;&#31209;&#22810;&#35821;&#35328;&#25913;&#20889;&#30340;&#36866;&#24212;&#24615;&#20302;&#31209;&#22810;&#35821;&#35328;&#25913;&#20889;&#27169;&#22411;&#65289;&#65292;&#20854;&#20013;&#21333;&#35821;&#25968;&#25454;&#38598;&#24050;&#32463;&#36275;&#22815;&#12290;
&lt;/p&gt;
&lt;p&gt;
Paraphrases are texts that convey the same meaning while using different words or sentence structures. It can be used as an automatic data augmentation tool for many Natural Language Processing tasks, especially when dealing with low-resource languages, where data shortage is a significant problem. To generate a paraphrase in multilingual settings, previous studies have leveraged the knowledge from the machine translation field, i.e., forming a paraphrase through zero-shot machine translation in the same language. Despite good performance on human evaluation, those methods still require parallel translation datasets, thus making them inapplicable to languages that do not have parallel corpora. To mitigate that problem, we proposed the first unsupervised multilingual paraphrasing model, LAMPAT ($\textbf{L}$ow-rank $\textbf{A}$daptation for $\textbf{M}$ultilingual $\textbf{P}$araphrasing using $\textbf{A}$dversarial $\textbf{T}$raining), by which monolingual dataset is sufficient enough 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;DP-ZO&#65292;&#19968;&#31181;&#36890;&#36807;&#31169;&#26377;&#21270;&#38646;&#38454;&#20248;&#21270;&#26469;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04343</link><description>&lt;p&gt;
&#31169;&#26377;&#38646;&#38454;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31169;&#26377;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Private Fine-tuning of Large Language Models with Zeroth-order Optimization. (arXiv:2401.04343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04343
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;DP-ZO&#65292;&#19968;&#31181;&#36890;&#36807;&#31169;&#26377;&#21270;&#38646;&#38454;&#20248;&#21270;&#26469;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#20250;&#23384;&#22312;&#36829;&#21453;&#38544;&#31169;&#30340;&#39118;&#38505;&#12290;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#36890;&#36807;&#24378;&#21046;&#31639;&#27861;&#31283;&#23450;&#24615;&#26469;&#20943;&#36731;&#38544;&#31169;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;DP-SGD&#21487;&#20197;&#20197;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#35757;&#32451;&#20855;&#26377;&#31169;&#26377;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#20294;&#20250;&#24102;&#26469;&#24615;&#33021;&#25439;&#22833;&#21644;&#37325;&#22823;&#24037;&#31243;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DP-ZO&#65292;&#19968;&#31181;&#36890;&#36807;&#31169;&#26377;&#21270;&#38646;&#38454;&#20248;&#21270;&#26469;&#20445;&#25252;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35774;&#35745;&#30340;&#19968;&#20010;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#25105;&#20204;&#20351;&#29992;&#30340;&#38646;&#38454;&#31639;&#27861;SPSA&#20013;&#30340;&#26799;&#24230;&#26041;&#21521;&#22987;&#32456;&#26159;&#38543;&#26426;&#30340;&#65292;&#32780;&#20165;&#20381;&#36182;&#20110;&#31169;&#26377;&#25968;&#25454;&#30340;&#20449;&#24687;&#26159;&#27493;&#38271;&#65292;&#21363;&#19968;&#20010;&#26631;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21482;&#38656;&#35201;&#23545;&#26631;&#37327;&#27493;&#38271;&#36827;&#34892;&#38544;&#31169;&#22788;&#29702;&#65292;&#36825;&#26159;&#23384;&#20648;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#12290;DP-ZO&#21487;&#20197;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#22122;&#22768;&#25110;&#39640;&#26031;&#22122;&#22768;&#26469;&#23454;&#29616;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#25552;&#20379;&#20102;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#24378;&#22823;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large pretrained models on private datasets may run the risk of violating privacy. Differential privacy is a framework for mitigating privacy risks by enforcing algorithmic stability. DP-SGD enables training models with private data in a privacy-preserving manner, but raises new obstacles in the form of performance loss and significant engineering challenges. We introduce DP-ZO, a new method for fine-tuning large language models that preserves the privacy of training data by privatizing zeroth-order optimization. A key insight into the design of our method is that the direction of the gradient in SPSA, the zeroth-order algorithm we use, is always random and the only information that depends on private data is the step size, i.e., a scalar. Therefore, we only need to privatize the scalar step size, which is memory-efficient. DP-ZO, which can be instantiated with either Laplace or Gaussian noise, provides a strong privacy-utility trade-off across different tasks, and model si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#26469;&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#30340;&#26032;&#26041;&#24335;&#65292;&#20351;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#33021;&#22815;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2401.04319</link><description>&lt;p&gt;
&#26356;&#22909;&#22320;&#20102;&#35299;&#24744;&#30340;&#38656;&#27714;&#65306;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. (arXiv:2401.04319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#26469;&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#30340;&#26032;&#26041;&#24335;&#65292;&#20351;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#33021;&#22815;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23450;&#20301;&#26041;&#24335;&#65292;&#21363;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#21487;&#20197;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#22312;&#20110;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#23454;&#38469;&#30340;&#32467;&#26500;&#21270;&#36923;&#36753;&#35821;&#35328;&#65292;&#21363;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#12290;&#32771;&#34385;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#23581;&#35797;&#21033;&#29992;LLMs&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#25552;&#31034;&#21487;&#20197;&#26377;&#25928;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20294;&#26159;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#65288;1&#65289;&#20808;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#31616;&#21333;&#30340;&#8220;&#35753;&#25105;&#20204;&#19968;&#27493;&#19968;&#27493;&#22320;&#24605;&#32771;&#8221;&#25552;&#31034;&#65292;&#35201;&#20040;&#22312;&#28436;&#31034;&#20013;&#25552;&#20379;&#22266;&#23450;&#30340;&#31034;&#20363;&#32780;&#19981;&#32771;&#34385;&#25552;&#31034;&#21644;&#38382;&#39064;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#65292;&#22312;&#19968;&#20123;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#32467;&#26500;&#21270;&#35821;&#35328;&#36716;&#25442;&#65289;&#20013;&#20351;LLMs&#26080;&#25928;&#12290;(2) &#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#38381;&#28304;&#27169;&#22411;&#25110;&#36807;&#24230;&#23454;&#29616;&#30340;&#27169;&#22411;&#20013;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. Past research indicates that the reasoning ability of LLMs can be effectively enhanced through chain-of-thought (CoT) prompting. But existing methods still have some limitations: (1) Previous methods either use simple "Let's think step by step" spells or provide fixed examples in demonstrations without considering compatibility between prompts and questions, making LLMs ineffective in some complex reasoning tasks such as structured language transformation. (2) Previous methods are often implemented in closed-source models or exces
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#23558;WiFi&#23460;&#20869;&#25104;&#20687;&#20316;&#20026;&#22810;&#27169;&#24577;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#26469;&#32771;&#34385;&#65292;&#36890;&#36807;WiFi-GEN&#32593;&#32476;&#23558;&#27979;&#37327;&#30340;WiFi&#21151;&#29575;&#36716;&#25442;&#20026;&#39640;&#20998;&#36776;&#29575;&#23460;&#20869;&#22270;&#20687;&#12290;&#19982;&#29289;&#29702;&#27169;&#22411;&#21453;&#28436;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#32593;&#32476;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24418;&#29366;&#37325;&#24314;&#31934;&#24230;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;Frechet Inception&#36317;&#31163;&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39564;&#35777;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04317</link><description>&lt;p&gt;
&#35270;&#35273;&#20877;&#26500;&#24819;&#65306;WiFi&#23460;&#20869;&#25104;&#20687;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#31361;&#30772;
&lt;/p&gt;
&lt;p&gt;
Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging. (arXiv:2401.04317v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04317
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#23558;WiFi&#23460;&#20869;&#25104;&#20687;&#20316;&#20026;&#22810;&#27169;&#24577;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#26469;&#32771;&#34385;&#65292;&#36890;&#36807;WiFi-GEN&#32593;&#32476;&#23558;&#27979;&#37327;&#30340;WiFi&#21151;&#29575;&#36716;&#25442;&#20026;&#39640;&#20998;&#36776;&#29575;&#23460;&#20869;&#22270;&#20687;&#12290;&#19982;&#29289;&#29702;&#27169;&#22411;&#21453;&#28436;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#32593;&#32476;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24418;&#29366;&#37325;&#24314;&#31934;&#24230;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;Frechet Inception&#36317;&#31163;&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39564;&#35777;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#25104;&#20687;&#23545;&#20110;&#26426;&#22120;&#20154;&#21644;&#29289;&#32852;&#32593;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#20316;&#20026;&#26080;&#22788;&#19981;&#22312;&#30340;&#20449;&#21495;&#65292;WiFi&#26159;&#36827;&#34892;&#34987;&#21160;&#25104;&#20687;&#21644;&#23558;&#26368;&#26032;&#20449;&#24687;&#21516;&#27493;&#21040;&#25152;&#26377;&#36830;&#25509;&#35774;&#22791;&#30340;&#26377;&#24076;&#26395;&#30340;&#36873;&#25321;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;WiFi&#23460;&#20869;&#25104;&#20687;&#20316;&#20026;&#22810;&#27169;&#24577;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#26469;&#32771;&#34385;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#23558;&#27979;&#37327;&#30340;WiFi&#21151;&#29575;&#36716;&#25442;&#20026;&#39640;&#20998;&#36776;&#29575;&#23460;&#20869;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;WiFi-GEN&#32593;&#32476;&#23454;&#29616;&#20102;&#24418;&#29366;&#37325;&#24314;&#31934;&#24230;&#65292;&#27604;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#21453;&#28436;&#26041;&#27861;&#39640;&#20986;275%&#12290;&#27492;&#22806;&#65292;Frechet Inception&#36317;&#31163;&#20998;&#25968;&#26174;&#33879;&#38477;&#20302;&#20102;82%&#12290;&#20026;&#20102;&#39564;&#35777;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;80,000&#23545;WiFi&#20449;&#21495;&#21644;&#25104;&#20687;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21560;&#25910;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;&#12289;&#19981;&#36866;&#23450;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#25105;&#20204;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#32593;&#32476;&#30340;&#22823;&#37327;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indoor imaging is a critical task for robotics and internet-of-things. WiFi as an omnipresent signal is a promising candidate for carrying out passive imaging and synchronizing the up-to-date information to all connected devices. This is the first research work to consider WiFi indoor imaging as a multi-modal image generation task that converts the measured WiFi power into a high-resolution indoor image. Our proposed WiFi-GEN network achieves a shape reconstruction accuracy that is 275% of that achieved by physical model-based inversion methods. Additionally, the Frechet Inception Distance score has been significantly reduced by 82%. To examine the effectiveness of models for this task, the first large-scale dataset is released containing 80,000 pairs of WiFi signal and imaging target. Our model absorbs challenges for the model-based methods including the non-linearity, ill-posedness and non-certainty into massive parameters of our generative AI network. The network is also designed to
&lt;/p&gt;</description></item><item><title>MARG&#26159;&#19968;&#31181;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#27169;&#22411;&#36827;&#34892;&#20869;&#37096;&#35752;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#37197;&#19981;&#21516;&#30340;&#23376;&#20219;&#21153;&#25552;&#39640;&#20102;GPT-4&#29983;&#25104;&#20855;&#20307;&#21644;&#26377;&#29992;&#21453;&#39304;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04259</link><description>&lt;p&gt;
MARG&#65306;&#22810;&#26234;&#33021;&#20307;&#20026;&#31185;&#23398;&#35770;&#25991;&#29983;&#25104;&#35780;&#35770;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MARG: Multi-Agent Review Generation for Scientific Papers. (arXiv:2401.04259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04259
&lt;/p&gt;
&lt;p&gt;
MARG&#26159;&#19968;&#31181;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#27169;&#22411;&#36827;&#34892;&#20869;&#37096;&#35752;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#37197;&#19981;&#21516;&#30340;&#23376;&#20219;&#21153;&#25552;&#39640;&#20102;GPT-4&#29983;&#25104;&#20855;&#20307;&#21644;&#26377;&#29992;&#21453;&#39304;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#21453;&#39304;&#30340;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#20102;MARG&#65292;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;LLM&#23454;&#20363;&#36827;&#34892;&#20869;&#37096;&#35752;&#35770;&#30340;&#21453;&#39304;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#35770;&#25991;&#25991;&#26412;&#20998;&#37197;&#32473;&#26234;&#33021;&#20307;&#65292;MARG&#21487;&#20197;&#28040;&#21270;&#36229;&#20986;&#22522;&#26412;LLM&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#23436;&#25972;&#35770;&#25991;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#19987;&#38376;&#21270;&#26234;&#33021;&#20307;&#21644;&#32467;&#21512;&#36866;&#21512;&#19981;&#21516;&#35780;&#35770;&#31867;&#22411;&#30340;&#23376;&#20219;&#21153;&#65288;&#23454;&#39564;&#65292;&#28165;&#26224;&#24230;&#65292;&#24433;&#21709;&#21147;&#65289;&#65292;&#25552;&#39640;&#21453;&#39304;&#30340;&#26377;&#29992;&#24615;&#21644;&#29305;&#23450;&#24615;&#12290;&#22312;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;GPT-4&#30340;&#22522;&#20934;&#26041;&#27861;&#30340;&#35780;&#35770;&#34987;&#35780;&#20026;&#36229;&#36807;&#19968;&#21322;&#26102;&#38388;&#20135;&#29983;&#30340;&#26159;&#26222;&#36890;&#25110;&#38750;&#24120;&#26222;&#36890;&#30340;&#35780;&#35770;&#65292;&#26368;&#20339;&#22522;&#20934;&#26041;&#27861;&#20013;&#27599;&#31687;&#35770;&#25991;&#21482;&#34987;&#35780;&#20026;1.7&#26465;&#25972;&#20307;&#19978;&#22909;&#30340;&#35780;&#35770;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22823;&#22823;&#25552;&#39640;&#20102;GPT-4&#29983;&#25104;&#20855;&#20307;&#21644;&#26377;&#29992;&#21453;&#39304;&#30340;&#33021;&#21147;&#65292;&#23558;&#26222;&#36890;&#35780;&#35770;&#30340;&#27604;&#20363;&#20174;60%&#38477;&#20302;&#21040;29%&#65292;&#27599;&#31687;&#35770;&#25991;&#20135;&#29983;3.7&#26465;&#22909;&#30340;&#35780;&#35770;&#65288;&#25552;&#39640;&#20102;2.2&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the ability of LLMs to generate feedback for scientific papers and develop MARG, a feedback generation approach using multiple LLM instances that engage in internal discussion. By distributing paper text across agents, MARG can consume the full text of papers beyond the input length limitations of the base LLM, and by specializing agents and incorporating sub-tasks tailored to different comment types (experiments, clarity, impact) it improves the helpfulness and specificity of feedback. In a user study, baseline methods using GPT-4 were rated as producing generic or very generic comments more than half the time, and only 1.7 comments per paper were rated as good overall in the best baseline. Our system substantially improves the ability of GPT-4 to generate specific and helpful feedback, reducing the rate of generic comments from 60% to 29% and generating 3.7 good comments per paper (a 2.2x improvement).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#21487;&#26816;&#32034;&#30340;&#35821;&#38899;&#25991;&#26412;&#23884;&#20837;&#36827;&#34892;&#39640;&#31934;&#24230;&#35821;&#38899;&#25628;&#32034;&#26597;&#35810;&#32416;&#38169;&#30340;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#20551;&#35774;-&#38899;&#39057;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#32416;&#38169;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04235</link><description>&lt;p&gt;
&#39640;&#31934;&#24230;&#35821;&#38899;&#25628;&#32034;&#26597;&#35810;&#32416;&#38169;&#30340;&#21487;&#26816;&#32034;&#35821;&#38899;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
High-precision Voice Search Query Correction via Retrievable Speech-text Embedings. (arXiv:2401.04235v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#21487;&#26816;&#32034;&#30340;&#35821;&#38899;&#25991;&#26412;&#23884;&#20837;&#36827;&#34892;&#39640;&#31934;&#24230;&#35821;&#38899;&#25628;&#32034;&#26597;&#35810;&#32416;&#38169;&#30340;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#20551;&#35774;-&#38899;&#39057;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#32416;&#38169;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#31995;&#32479;&#21487;&#33021;&#22240;&#20026;&#21508;&#31181;&#21407;&#22240;&#32780;&#23548;&#33268;&#21484;&#22238;&#29575;&#36739;&#20302;&#65292;&#20363;&#22914;&#22024;&#26434;&#30340;&#38899;&#39057;&#12289;&#32570;&#20047;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#31561;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;ASR&#20551;&#35774;&#25991;&#26412;&#30340;&#23884;&#20837;&#36827;&#34892;&#26368;&#36817;&#37051;&#25628;&#32034;&#65292;&#20174;&#22823;&#22411;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#21487;&#33021;&#20505;&#36873;&#39033;&#26469;&#25913;&#36827;&#21484;&#22238;&#29575;&#24182;&#32416;&#27491;&#20505;&#36873;&#32416;&#27491;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25991;&#26412;&#20551;&#35774;&#19982;&#36716;&#24405;&#30340;&#30495;&#23454;&#25991;&#26412;&#22312;&#35821;&#38899;&#19978;&#24046;&#24322;&#22826;&#22823;&#65292;&#22522;&#20110;ASR&#20551;&#35774;&#30340;&#26816;&#32034;&#21487;&#33021;&#20250;&#23548;&#33268;&#31934;&#24230;&#36739;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#30001;&#35805;&#35821;&#38899;&#39057;&#20135;&#29983;&#30340;&#23884;&#20837;&#26469;&#26597;&#35810;&#32416;&#27491;&#25968;&#25454;&#24211;&#65292;&#28040;&#38500;&#20102;&#20551;&#35774;&#21644;&#38899;&#39057;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65307;&#35805;&#35821;&#38899;&#39057;&#21644;&#20505;&#36873;&#32416;&#27491;&#30340;&#23884;&#20837;&#26159;&#30001;&#22810;&#27169;&#24335;&#35821;&#38899;&#25991;&#26412;&#23884;&#20837;&#32593;&#32476;&#35757;&#32451;&#20135;&#29983;&#30340;&#65292;&#30446;&#30340;&#26159;&#23558;&#35805;&#35821;&#38899;&#39057;&#30340;&#23884;&#20837;&#21644;&#20854;&#30456;&#24212;&#30340;&#25991;&#26412;&#36716;&#24405;&#30340;&#23884;&#20837;&#25918;&#32622;&#22312;&#38752;&#36817;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) systems can suffer from poor recall for various reasons, such as noisy audio, lack of sufficient training data, etc.  Previous work has shown that recall can be improved by retrieving rewrite candidates from a large database of likely, contextually-relevant alternatives to the hypothesis text using nearest-neighbors search over embeddings of the ASR hypothesis text to correct and candidate corrections.  However, ASR-hypothesis-based retrieval can yield poor precision if the textual hypotheses are too phonetically dissimilar to the transcript truth. In this paper, we eliminate the hypothesis-audio mismatch problem by querying the correction database directly using embeddings derived from the utterance audio; the embeddings of the utterance audio and candidate corrections are produced by multimodal speech-text embedding networks trained to place the embedding of the audio of an utterance and the embedding of its corresponding textual transcript close to
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#22320;&#29702;&#20301;&#32622;&#26041;&#21521;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#20998;&#23618;&#31354;&#38388;&#20559;&#24046;&#12290;&#20854;&#20013;&#65292;GPT-4&#34920;&#29616;&#26368;&#20339;&#65292;&#20934;&#30830;&#29575;&#20026;55.3&#65285;&#12290;</title><link>http://arxiv.org/abs/2401.04218</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21028;&#26029;&#31354;&#38388;&#20851;&#31995;&#22833;&#30495;&#65306;&#33258;&#28982;&#35821;&#35328;&#22320;&#29702;&#25968;&#25454;&#30340;&#40654;&#26126;&#65311;
&lt;/p&gt;
&lt;p&gt;
Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?. (arXiv:2401.04218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04218
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#22320;&#29702;&#20301;&#32622;&#26041;&#21521;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#20998;&#23618;&#31354;&#38388;&#20559;&#24046;&#12290;&#20854;&#20013;&#65292;GPT-4&#34920;&#29616;&#26368;&#20339;&#65292;&#20934;&#30830;&#29575;&#20026;55.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21028;&#26029;&#22320;&#29702;&#20301;&#32622;&#20043;&#38388;&#30340;&#26041;&#21521;&#19978;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19977;&#20010;&#30693;&#21517;&#30340;LLMs&#65306;GPT-3.5&#65292;GPT-4&#21644;Llama-2&#12290;&#36825;&#20010;&#22522;&#20934;&#29305;&#21035;&#35780;&#20272;&#20102;LLMs&#26159;&#21542;&#34920;&#29616;&#20986;&#31867;&#20284;&#20154;&#31867;&#30340;&#20998;&#23618;&#31354;&#38388;&#20559;&#24046;&#65292;&#21363;&#23545;&#20110;&#21253;&#21547;&#23427;&#20204;&#30340;&#26356;&#22823;&#32676;&#20307;&#30340;&#24863;&#30693;&#20851;&#31995;&#20250;&#24433;&#21709;&#23545;&#20010;&#21035;&#20301;&#32622;&#31354;&#38388;&#20851;&#31995;&#30340;&#21028;&#26029;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;14&#20010;&#20851;&#20110;&#32654;&#22269;&#30693;&#21517;&#22478;&#24066;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#19971;&#20010;&#38382;&#39064;&#26088;&#22312;&#25361;&#25112;LLMs&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#21463;&#21040;&#20102;&#26356;&#22823;&#22320;&#29702;&#21333;&#20301;&#65288;&#22914;&#24030;&#25110;&#22269;&#23478;&#65289;&#26041;&#21521;&#30340;&#24433;&#21709;&#65292;&#32780;&#21478;&#22806;&#19971;&#20010;&#38382;&#39064;&#21017;&#38024;&#23545;&#19981;&#23481;&#26131;&#21463;&#21040;&#36825;&#31181;&#23618;&#27425;&#21270;&#20998;&#31867;&#30340;&#20301;&#32622;&#12290;&#22312;&#32463;&#36807;&#27979;&#35797;&#30340;&#27169;&#22411;&#20013;&#65292;GPT-4&#30340;&#20934;&#30830;&#29575;&#26368;&#39640;&#65292;&#20026;55.3&#65285;&#65292;&#20854;&#27425;&#26159;GPT-3.5&#30340;47.3&#65285;&#21644;Llama-2&#30340;44.7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a benchmark for assessing the capability of Large Language Models (LLMs) to discern intercardinal directions between geographic locations and apply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark specifically evaluates whether LLMs exhibit a hierarchical spatial bias similar to humans, where judgments about individual locations' spatial relationships are influenced by the perceived relationships of the larger groups that contain them. To investigate this, we formulated 14 questions focusing on well-known American cities. Seven questions were designed to challenge the LLMs with scenarios potentially influenced by the orientation of larger geographical units, such as states or countries, while the remaining seven targeted locations less susceptible to such hierarchical categorization. Among the tested models, GPT-4 exhibited superior performance with 55.3% accuracy, followed by GPT-3.5 at 47.3%, and Llama-2 at 44.7%. The models showed significantly redu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FunnyNet-W&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#35270;&#39057;&#20013;&#30340;&#26377;&#36259;&#30636;&#38388;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#25968;&#25454;&#20197;&#21450;&#20132;&#21449;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#19988;&#37319;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#33719;&#24471;&#35757;&#32451;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2401.04210</link><description>&lt;p&gt;
FunnyNet-W:&#35270;&#39057;&#20013;&#37326;&#22806;&#26377;&#36259;&#30636;&#38388;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild. (arXiv:2401.04210v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04210
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FunnyNet-W&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#35270;&#39057;&#20013;&#30340;&#26377;&#36259;&#30636;&#38388;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#25968;&#25454;&#20197;&#21450;&#20132;&#21449;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#19988;&#37319;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#33719;&#24471;&#35757;&#32451;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35266;&#30475;&#21916;&#21095;&#26102;&#33258;&#21160;&#29702;&#35299;&#26377;&#36259;&#30340;&#30636;&#38388;&#65288;&#21363;&#20351;&#35753;&#20154;&#21457;&#31505;&#30340;&#30636;&#38388;&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#28041;&#21450;&#21040;&#21508;&#31181;&#29305;&#24449;&#65292;&#22914;&#32930;&#20307;&#35821;&#35328;&#12289;&#23545;&#35805;&#21644;&#25991;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FunnyNet-W&#65292;&#23427;&#26159;&#19968;&#20010;&#20381;&#38752;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#20132;&#21449;&#21644;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#35270;&#39057;&#20013;&#30340;&#26377;&#36259;&#30636;&#38388;&#12290;&#19982;&#22823;&#22810;&#25968;&#20381;&#36182;&#20110;&#23383;&#24149;&#24418;&#24335;&#30340;&#26631;&#27880;&#25968;&#25454;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#28982;&#19982;&#35270;&#39057;&#19968;&#36215;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65306;&#65288;a&#65289;&#35270;&#39057;&#24103;&#65292;&#22240;&#20026;&#23427;&#20204;&#21253;&#21547;&#20102;&#22330;&#26223;&#29702;&#35299;&#25152;&#24517;&#38656;&#30340;&#35270;&#35273;&#20449;&#24687;&#65292;&#65288;b&#65289;&#38899;&#39057;&#65292;&#22240;&#20026;&#23427;&#21253;&#21547;&#19982;&#26377;&#36259;&#30636;&#38388;&#30456;&#20851;&#30340;&#26356;&#39640;&#32423;&#21035;&#30340;&#32447;&#32034;&#65292;&#22914;&#35821;&#35843;&#12289;&#38899;&#39640;&#21644;&#20572;&#39039;&#65292;&#20197;&#21450;&#65288;c&#65289;&#30001;&#35821;&#38899;&#36716;&#25991;&#26412;&#27169;&#22411;&#33258;&#21160;&#25552;&#21462;&#30340;&#25991;&#26412;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#32463;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#21518;&#25552;&#20379;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#33719;&#24471;&#35757;&#32451;&#26631;&#31614;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#21644;&#26631;&#35760;&#26377;&#36259;&#30340;&#38899;&#39057;&#30636;&#38388;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically understanding funny moments (i.e., the moments that make people laugh) when watching comedy is challenging, as they relate to various features, such as body language, dialogues and culture. In this paper, we propose FunnyNet-W, a model that relies on cross- and self-attention for visual, audio and text data to predict funny moments in videos. Unlike most methods that rely on ground truth data in the form of subtitles, in this work we exploit modalities that come naturally with videos: (a) video frames as they contain visual information indispensable for scene understanding, (b) audio as it contains higher-level cues associated with funny moments, such as intonation, pitch and pauses and (c) text automatically extracted with a speech-to-text model as it can provide rich information when processed by a Large Language Model. To acquire labels for training, we propose an unsupervised approach that spots and labels funny audio moments. We provide experiments on five datasets: 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#21644;GPT&#65292;&#24182;&#37325;&#28857;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#22522;&#22240;&#32452;&#23398;&#12289;&#36716;&#24405;&#32452;&#23398;&#12289;&#34507;&#30333;&#36136;&#32452;&#23398;&#12289;&#33647;&#29289;&#21457;&#29616;&#21644;&#21333;&#32454;&#32990;&#20998;&#26512;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#29983;&#29289;&#20449;&#24687;&#23398;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#21644;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.04155</link><description>&lt;p&gt;
&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24212;&#29992;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Large language models in bioinformatics: applications and perspectives. (arXiv:2401.04155v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#21644;GPT&#65292;&#24182;&#37325;&#28857;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#22522;&#22240;&#32452;&#23398;&#12289;&#36716;&#24405;&#32452;&#23398;&#12289;&#34507;&#30333;&#36136;&#32452;&#23398;&#12289;&#33647;&#29289;&#21457;&#29616;&#21644;&#21333;&#32454;&#32990;&#20998;&#26512;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#29983;&#29289;&#20449;&#24687;&#23398;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#19968;&#31867;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#30001;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#36755;&#20837;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35299;&#20915;&#29983;&#29289;&#20449;&#24687;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#28508;&#21147;&#29978;&#33267;&#36229;&#36807;&#20102;&#22312;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23558;&#20171;&#32461;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#30340;&#20960;&#20010;&#37325;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#21644;GPT&#65292;&#24182;&#37325;&#28857;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#19981;&#21516;&#32452;&#23398;&#27700;&#24179;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#21253;&#25324;&#22522;&#22240;&#32452;&#23398;&#12289;&#36716;&#24405;&#32452;&#23398;&#12289;&#34507;&#30333;&#36136;&#32452;&#23398;&#12289;&#33647;&#29289;&#21457;&#29616;&#21644;&#21333;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#29983;&#29289;&#20449;&#24687;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are a class of artificial intelligence models based on deep learning, which have great performance in various tasks, especially in natural language processing (NLP). Large language models typically consist of artificial neural networks with numerous parameters, trained on large amounts of unlabeled input using self-supervised or semi-supervised learning. However, their potential for solving bioinformatics problems may even exceed their proficiency in modeling human language. In this review, we will present a summary of the prominent large language models used in natural language processing, such as BERT and GPT, and focus on exploring the applications of large language models at different omics levels in bioinformatics, mainly including applications of large language models in genomics, transcriptomics, proteomics, drug discovery and single cell analysis. Finally, this review summarizes the potential and prospects of large language models in solving bioinfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Cross-Speaker Encoding&#65288;CSE&#65289;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#32858;&#21512;&#36328;&#35828;&#35805;&#20154;&#34920;&#31034;&#12290;&#36890;&#36807;&#19982;SOT&#32467;&#21512;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#27604;SIMO&#22522;&#20934;&#27169;&#22411;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20998;&#21035;&#38477;&#20302;&#20102;8%&#21644;10%&#12290;</title><link>http://arxiv.org/abs/2401.04152</link><description>&lt;p&gt;
&#36328;&#35828;&#35805;&#20154;&#32534;&#30721;&#32593;&#32476;&#29992;&#20110;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-Speaker Encoding Network for Multi-Talker Speech Recognition. (arXiv:2401.04152v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Cross-Speaker Encoding&#65288;CSE&#65289;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#32858;&#21512;&#36328;&#35828;&#35805;&#20154;&#34920;&#31034;&#12290;&#36890;&#36807;&#19982;SOT&#32467;&#21512;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#27604;SIMO&#22522;&#20934;&#27169;&#22411;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20998;&#21035;&#38477;&#20302;&#20102;8%&#21644;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#24050;&#32463;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#20316;&#20026;&#19968;&#31181;&#30452;&#25509;&#36716;&#24405;&#22810;&#20010;&#35828;&#35805;&#20154;&#37325;&#21472;&#35821;&#38899;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;1&#65289;&#24102;&#26377;&#20998;&#25903;&#32534;&#30721;&#22120;&#30340;&#21333;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;SIMO&#65289;&#27169;&#22411;&#65292;&#25110;&#32773;2&#65289;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#65288;SOT&#65289;&#30340;&#21333;&#36755;&#20837;&#21333;&#36755;&#20986;&#65288;SISO&#65289;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Cross-Speaker Encoding&#65288;CSE&#65289;&#30340;&#32593;&#32476;&#26469;&#35299;&#20915;SIMO&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#32858;&#21512;&#36328;&#35828;&#35805;&#20154;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;CSE&#27169;&#22411;&#19982;SOT&#30456;&#32467;&#21512;&#65292;&#26082;&#21457;&#25381;&#20102;SIMO&#21644;SISO&#30340;&#20248;&#21183;&#65292;&#21448;&#32531;&#35299;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#35813;&#24037;&#20316;&#20195;&#34920;&#20102;&#23558;SIMO&#21644;SISO&#38598;&#25104;&#21040;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#26089;&#26399;&#24037;&#20316;&#12290;&#22312;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;LibrispeechMix&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CES&#27169;&#22411;&#30456;&#27604;&#20110;SIMO&#22522;&#20934;&#27169;&#22411;&#23558;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#20102;8%&#12290;CSE-SOT&#27169;&#22411;&#23558;WER&#38477;&#20302;&#20102;10%
&lt;/p&gt;
&lt;p&gt;
End-to-end multi-talker speech recognition has garnered great interest as an effective approach to directly transcribe overlapped speech from multiple speakers. Current methods typically adopt either 1) single-input multiple-output (SIMO) models with a branched encoder, or 2) single-input single-output (SISO) models based on attention-based encoder-decoder architecture with serialized output training (SOT). In this work, we propose a Cross-Speaker Encoding (CSE) network to address the limitations of SIMO models by aggregating cross-speaker representations. Furthermore, the CSE model is integrated with SOT to leverage both the advantages of SIMO and SISO while mitigating their drawbacks. To the best of our knowledge, this work represents an early effort to integrate SIMO and SISO for multi-talker speech recognition. Experiments on the two-speaker LibrispeechMix dataset show that the CES model reduces word error rate (WER) by 8% over the SIMO baseline. The CSE-SOT model reduces WER by 10
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRA&#38142;&#65288;COLA&#65289;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#36807;&#31243;&#23558;LoRA&#27169;&#22359;&#19982;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#21512;&#24182;&#65292;&#24182;&#37325;&#26032;&#21021;&#22987;&#21270;&#26032;&#30340;LoRA&#27169;&#22359;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;LoRA&#21644;&#20840;&#21442;&#25968;&#24494;&#35843;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.04151</link><description>&lt;p&gt;
LoRA&#38142;&#65306;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning. (arXiv:2401.04151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRA&#38142;&#65288;COLA&#65289;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#36807;&#31243;&#23558;LoRA&#27169;&#22359;&#19982;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#21512;&#24182;&#65292;&#24182;&#37325;&#26032;&#21021;&#22987;&#21270;&#26032;&#30340;LoRA&#27169;&#22359;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;LoRA&#21644;&#20840;&#21442;&#25968;&#24494;&#35843;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#26159;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#21046;&#20026;&#29305;&#23450;&#20219;&#21153;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#21644;&#20219;&#21153;&#22810;&#26679;&#24615;&#30340;&#25193;&#22823;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#21450;&#20854;&#21464;&#20307;&#12290;LoRA&#23558;&#26435;&#37325;&#26356;&#26032;&#32534;&#30721;&#20026;&#20004;&#20010;&#20302;&#31209;&#30697;&#38453;&#30340;&#20056;&#31215;&#12290;&#23613;&#31649;&#20855;&#26377;&#19968;&#23450;&#20248;&#21183;&#65292;&#20294;&#22312;&#26576;&#20123;&#20219;&#21153;&#30340;&#27867;&#21270;&#38169;&#35823;&#26041;&#38754;&#65292;LoRA&#26080;&#27861;&#23436;&#20840;&#21442;&#25968;&#21270;&#24494;&#35843;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;LoRA&#38142;&#65288;COLA&#65289;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#21463;Frank-Wolfe&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#20197;&#24357;&#21512;LoRA&#21644;&#20840;&#21442;&#25968;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#25110;&#20869;&#23384;&#24320;&#38144;&#12290;COLA&#37319;&#29992;&#27531;&#24046;&#23398;&#20064;&#36807;&#31243;&#65292;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#20013;&#21512;&#24182;&#23398;&#21040;&#30340;LoRA&#27169;&#22359;&#65292;&#24182;&#37325;&#26032;&#21021;&#22987;&#21270;&#26032;&#29983;&#30340;LoRA&#27169;&#22359;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#25910;&#25947;&#20445;&#35777;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is the primary methodology for tailoring pre-trained large language models to specific tasks. As the model's scale and the diversity of tasks expand, parameter-efficient fine-tuning methods are of paramount importance. One of the most widely used family of methods is low-rank adaptation (LoRA) and its variants. LoRA encodes weight update as the product of two low-rank matrices. Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks.  We introduce Chain of LoRA (COLA), an iterative optimization framework inspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full parameter fine-tuning, without incurring additional computational costs or memory overheads. COLA employs a residual learning procedure where it merges learned LoRA modules into the pre-trained language model parameters and re-initilize optimization for new born LoRA modules. We provide theoretical convergence guarantees as well as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Generation Z&#30340;&#20010;&#20307;&#36827;&#34892;&#35843;&#26597;&#65292;&#35780;&#20272;&#20102;&#20182;&#20204;&#22312;Discord&#19978;&#21306;&#20998;AI&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20182;&#20204;&#26080;&#27861;&#26377;&#25928;&#21306;&#20998;&#36825;&#20004;&#31181;&#26469;&#28304;&#30340;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.04120</link><description>&lt;p&gt;
Generation Z&#22312;Discord&#19978;&#21306;&#20998;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Generation Z's Ability to Discriminate Between AI-generated and Human-Authored Text on Discord. (arXiv:2401.04120v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Generation Z&#30340;&#20010;&#20307;&#36827;&#34892;&#35843;&#26597;&#65292;&#35780;&#20272;&#20102;&#20182;&#20204;&#22312;Discord&#19978;&#21306;&#20998;AI&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20182;&#20204;&#26080;&#27861;&#26377;&#25928;&#21306;&#20998;&#36825;&#20004;&#31181;&#26469;&#28304;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26222;&#21450;&#65292;&#22914;ChatGPT&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#38543;&#30528;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#26222;&#21450;&#65292;&#20154;&#20204;&#23545;&#22312;&#32447;&#38544;&#31169;&#21644;&#34394;&#20551;&#20449;&#24687;&#30340;&#25285;&#24551;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20013;&#65292;Discord&#20801;&#35768;AI&#38598;&#25104;&#65292;&#20351;&#20182;&#20204;&#20197;"Z&#19990;&#20195;"&#20026;&#20027;&#30340;&#29992;&#25143;&#32676;&#20307;&#29305;&#21035;&#23481;&#26131;&#25509;&#35302;&#21040;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#23545;&#24180;&#40836;&#20026;Z&#19990;&#20195;&#30340;&#20010;&#20307;&#65288;n = 335&#65289;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#35780;&#20272;&#20182;&#20204;&#22312;Discord&#19978;&#21306;&#20998;AI&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#35843;&#26597;&#37319;&#29992;&#20102;ChatGPT&#20266;&#35013;&#25104;&#22312;Discord.com&#24179;&#21488;&#19978;&#25910;&#21040;&#30340;&#19968;&#26465;&#30701;&#20449;&#30340;&#21333;&#27425;&#25552;&#31034;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#21644;&#21442;&#19982;&#32773;&#23545;Discord&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#29087;&#24713;&#31243;&#24230;&#23545;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;Z&#19990;&#20195;&#20010;&#20307;&#26080;&#27861;&#21306;&#20998;AI&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#65288;p = 0.011&#65289;&#65292;&#37027;&#20123;&#33258;&#25105;&#25253;&#21578;&#31243;&#24230;&#36739;&#20302;&#30340;&#20010;&#20307;&#33021;&#21147;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing popularity of generative artificial intelligence (AI) chatbots such as ChatGPT is having transformative effects on social media. As the prevalence of AI-generated content grows, concerns have been raised regarding privacy and misinformation online. Among social media platforms, Discord enables AI integrations -- making their primarily "Generation Z" userbase particularly exposed to AI-generated content. We surveyed Generation Z aged individuals (n = 335) to evaluate their proficiency in discriminating between AI-generated and human-authored text on Discord. The investigation employed one-shot prompting of ChatGPT, disguised as a text message received on the Discord.com platform. We explore the influence of demographic factors on ability, as well as participants' familiarity with Discord and artificial intelligence technologies. We find that Generation Z individuals are unable to discern between AI and human-authored text (p = 0.011), and that those with lower self-reported 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#20004;&#20010;&#30740;&#35752;&#20250;&#30340;&#20250;&#35758;&#24405;&#65292;&#26088;&#22312;&#35752;&#35770;&#20154;&#26426;&#23545;&#35805;&#20013;&#30340;&#27807;&#36890;&#38382;&#39064;&#21644;&#22833;&#36133;&#65292;&#20197;&#21450;&#38750;&#26426;&#22120;&#20154;&#35821;&#38899;&#30028;&#38754;&#30340;&#30456;&#20851;&#22833;&#36133;&#12290;&#30446;&#26631;&#26159;&#24443;&#24213;&#35843;&#26597;&#27807;&#36890;&#22833;&#36133;&#65292;&#21046;&#23450;&#20998;&#31867;&#27861;&#65292;&#24182;&#23637;&#24320;&#35299;&#20915;&#26041;&#26696;&#30340;&#21021;&#27493;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.04108</link><description>&lt;p&gt;
&#22312;&#20154;&#26426;&#23545;&#35805;&#20013;&#22788;&#29702;&#22256;&#38590;&#21644;&#22833;&#36133;&#65288;WTF 2023&#65289;&#19982;CUI&#35774;&#35745;&#26159;&#21542;&#20934;&#22791;&#22909;&#65311;&#65288;arXiv:2401.04108v1 [cs.HC]&#65289;
&lt;/p&gt;
&lt;p&gt;
Working with Trouble and Failures in Conversation between Humans and Robots (WTF 2023) &amp; Is CUI Design Ready Yet?. (arXiv:2401.04108v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#20004;&#20010;&#30740;&#35752;&#20250;&#30340;&#20250;&#35758;&#24405;&#65292;&#26088;&#22312;&#35752;&#35770;&#20154;&#26426;&#23545;&#35805;&#20013;&#30340;&#27807;&#36890;&#38382;&#39064;&#21644;&#22833;&#36133;&#65292;&#20197;&#21450;&#38750;&#26426;&#22120;&#20154;&#35821;&#38899;&#30028;&#38754;&#30340;&#30456;&#20851;&#22833;&#36133;&#12290;&#30446;&#26631;&#26159;&#24443;&#24213;&#35843;&#26597;&#27807;&#36890;&#22833;&#36133;&#65292;&#21046;&#23450;&#20998;&#31867;&#27861;&#65292;&#24182;&#23637;&#24320;&#35299;&#20915;&#26041;&#26696;&#30340;&#21021;&#27493;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26159;ACM&#20250;&#35758;&#19978;&#20004;&#20010;&#20998;&#20250;&#35758;&#8220;&#22312;&#20154;&#26426;&#23545;&#35805;&#20013;&#22788;&#29702;&#22256;&#38590;&#21644;&#22833;&#36133;&#8221;&#65288;WTF 2023&#65289;&#21644;&#8220;CUI&#35774;&#35745;&#26159;&#21542;&#20934;&#22791;&#22909;&#65311;&#8221;&#30340;&#30740;&#35752;&#20250;&#20250;&#35758;&#24405;&#12290;WTF 23&#26088;&#22312;&#27719;&#38598;&#26469;&#33258;&#20154;&#26426;&#20132;&#20114;&#12289;&#23545;&#35805;&#31995;&#32479;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#23545;&#35805;&#20998;&#26512;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#12290;&#23613;&#31649;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#26426;&#22120;&#20154;&#35821;&#38899;&#30028;&#38754;&#22312;&#22810;&#20010;&#26041;&#38754;&#20173;&#28982;&#33030;&#24369;&#65292;&#20154;&#20204;&#23545;&#27492;&#31867;&#30028;&#38754;&#22833;&#36133;&#30340;&#32463;&#21382;&#24182;&#19981;&#32597;&#35265;&#12290;&#28982;&#32780;&#65292;&#25216;&#26415;&#25991;&#29486;&#23545;&#23427;&#20204;&#30340;&#33391;&#22909;&#24615;&#33021;&#26377;&#30528;&#31215;&#26497;&#30340;&#20559;&#21521;&#12290;&#35813;&#30740;&#35752;&#20250;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#35752;&#35770;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#27807;&#36890;&#38382;&#39064;&#21644;&#22833;&#36133;&#20197;&#21450;&#38750;&#26426;&#22120;&#20154;&#35821;&#38899;&#30028;&#38754;&#30456;&#20851;&#22833;&#36133;&#30340;&#24179;&#21488;&#12290;&#30446;&#26631;&#21253;&#25324;&#23545;&#27807;&#36890;&#22833;&#36133;&#36827;&#34892;&#24443;&#24213;&#35843;&#26597;&#65292;&#24320;&#22987;&#21046;&#23450;&#36825;&#20123;&#22833;&#36133;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23601;&#35299;&#20915;&#26041;&#26696;&#23637;&#24320;&#21021;&#27493;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Workshop proceedings of two co-located workshops "Working with Troubles and Failures in Conversation with Humans and Robots" (WTF 2023) and "Is CUI Design Ready Yet?", both of which were part of the ACM conference on conversational user interfaces 2023.  WTF 23 aimed at bringing together researchers from human-robot interaction, dialogue systems, human-computer interaction, and conversation analysis. Despite all progress, robotic speech interfaces continue to be brittle in a number of ways and the experience of failure of such interfaces is commonplace amongst roboticists. However, the technical literature is positively skewed toward their good performance. The workshop aims to provide a platform for discussing communicative troubles and failures in human-robot interactions and related failures in non-robotic speech interfaces. Aims include a scrupulous investigation into communicative failures, to begin working on a taxonomy of such failures, and enable a preliminary discussion on pos
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#31995;&#21015;&#25552;&#31034;&#21464;&#21270;&#25506;&#31350;&#25913;&#21464;&#25552;&#31034;&#30340;&#26500;&#24314;&#26041;&#24335;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#21363;&#20351;&#24494;&#23567;&#30340;&#25913;&#21464;&#65292;&#27604;&#22914;&#22312;&#25552;&#31034;&#26411;&#23614;&#21152;&#19968;&#20010;&#31354;&#26684;&#65292;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#31572;&#26696;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#35831;&#27714;&#20197;XML&#26684;&#24335;&#36820;&#22238;&#21644;&#24120;&#29992;&#30340;&#36234;&#29425;&#26041;&#24335;&#20063;&#21487;&#33021;&#23545;&#27169;&#22411;&#26631;&#35760;&#30340;&#25968;&#25454;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.03729</link><description>&lt;p&gt;
&#25913;&#21464;&#25552;&#31034;&#30340;&#34676;&#34678;&#25928;&#24212;&#65306;&#24494;&#23567;&#30340;&#21464;&#21270;&#21644;&#36234;&#29425;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance. (arXiv:2401.03729v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#31995;&#21015;&#25552;&#31034;&#21464;&#21270;&#25506;&#31350;&#25913;&#21464;&#25552;&#31034;&#30340;&#26500;&#24314;&#26041;&#24335;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#21363;&#20351;&#24494;&#23567;&#30340;&#25913;&#21464;&#65292;&#27604;&#22914;&#22312;&#25552;&#31034;&#26411;&#23614;&#21152;&#19968;&#20010;&#31354;&#26684;&#65292;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#31572;&#26696;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#35831;&#27714;&#20197;XML&#26684;&#24335;&#36820;&#22238;&#21644;&#24120;&#29992;&#30340;&#36234;&#29425;&#26041;&#24335;&#20063;&#21487;&#33021;&#23545;&#27169;&#22411;&#26631;&#35760;&#30340;&#25968;&#25454;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#23545;&#22810;&#20010;&#39046;&#22495;&#21644;&#22810;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#21521;LLM&#25552;&#38382;&#25110;&#8220;&#25552;&#31034;&#8221;&#65292;&#23454;&#36341;&#32773;&#33021;&#22815;&#24555;&#36895;&#33719;&#24471;&#20219;&#24847;&#20219;&#21153;&#30340;&#21709;&#24212;&#12290;&#25552;&#31034;&#30340;&#26500;&#24314;&#26041;&#24335;&#26159;&#21542;&#21464;&#21270;&#20250;&#24433;&#21709;LLM&#30340;&#26368;&#32456;&#20915;&#31574;&#65311;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#31181;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#19968;&#31995;&#21015;&#25552;&#31034;&#21464;&#21270;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#26368;&#24494;&#23567;&#30340;&#25200;&#21160;&#65292;&#27604;&#22914;&#22312;&#25552;&#31034;&#30340;&#26411;&#23614;&#21152;&#19968;&#20010;&#31354;&#26684;&#65292;&#20063;&#21487;&#33021;&#23548;&#33268;LLM&#25913;&#21464;&#20854;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;XML&#20013;&#35831;&#27714;&#21709;&#24212;&#21644;&#24120;&#29992;&#30340;&#36234;&#29425;&#26041;&#24335;&#21487;&#33021;&#23545;&#30001;LLMs&#26631;&#35760;&#30340;&#25968;&#25454;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are regularly being used to label data across many domains and for myriad tasks. By simply asking the LLM for an answer, or ``prompting,'' practitioners are able to use LLMs to quickly get a response for an arbitrary task. This prompting is done through a series of decisions by the practitioner, from simple wording of the prompt, to requesting the output in a certain data format, to jailbreaking in the case of prompts that address more sensitive topics. In this work, we ask: do variations in the way a prompt is constructed change the ultimate decision of the LLM? We answer this using a series of prompt variations across a variety of text classification tasks. We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs.
&lt;/p&gt;</description></item><item><title>ROIC-DM&#26159;&#19968;&#20010;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#30340;&#24378;&#22823;&#25991;&#26412;&#25512;&#29702;&#21644;&#20998;&#31867;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65307;&#21516;&#26102;&#36890;&#36807;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21672;&#35810;&#32452;&#20214;&#32435;&#20837;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03514</link><description>&lt;p&gt;
ROIC-DM: &#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#30340;&#24378;&#22823;&#25991;&#26412;&#25512;&#29702;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ROIC-DM: Robust Text Inference and Classification via Diffusion Model. (arXiv:2401.03514v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03514
&lt;/p&gt;
&lt;p&gt;
ROIC-DM&#26159;&#19968;&#20010;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#30340;&#24378;&#22823;&#25991;&#26412;&#25512;&#29702;&#21644;&#20998;&#31867;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65307;&#21516;&#26102;&#36890;&#36807;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21672;&#35810;&#32452;&#20214;&#32435;&#20837;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25512;&#29702;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#35768;&#22810;&#37324;&#31243;&#30865;&#24335;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21487;&#33021;&#23548;&#33268;&#24847;&#24819;&#19981;&#21040;&#30340;&#32467;&#26524;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#36807;&#20026;&#35821;&#35328;&#27169;&#22411;&#22686;&#21152;&#38450;&#24481;&#34917;&#19969;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38450;&#24481;&#31574;&#30053;&#24448;&#24448;&#20381;&#36182;&#20110;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#25110;&#32773;&#38656;&#35201;&#22312;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#20570;&#20986;&#24040;&#22823;&#30340;&#29306;&#29298;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#36825;&#20123;&#38450;&#24481;&#26426;&#21046;&#26469;&#25552;&#39640;&#30446;&#26631;&#27169;&#22411;&#30340;&#24377;&#24615;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#29992;&#20110;&#24378;&#21270;&#25991;&#26412;&#25512;&#29702;&#21644;&#20998;&#31867;&#30340;&#27169;&#22411;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65288;ROIC-DM&#65289;&#12290;&#30001;&#20110;&#20854;&#35757;&#32451;&#36807;&#31243;&#28041;&#21450;&#21435;&#22122;&#38454;&#27573;&#65292;ROIC-DM&#26412;&#36136;&#19978;&#27604;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;ROIC-DM&#36824;&#21487;&#20197;&#36890;&#36807;&#26377;&#25928;&#22320;&#23558;&#20854;&#20316;&#20026;&#21672;&#35810;&#32452;&#20214;&#32435;&#20837;&#65292;&#36798;&#21040;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#24378;&#25991;&#26412;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;ROIC-DM&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While language models have made many milestones in text inference and classification tasks, they remain susceptible to adversarial attacks that can lead to unforeseen outcomes. Existing works alleviate this problem by equipping language models with defense patches. However, these defense strategies often rely on impractical assumptions or entail substantial sacrifices in model performance. Consequently, enhancing the resilience of the target model using such defense mechanisms is a formidable challenge. This paper introduces an innovative model for robust text inference and classification, built upon diffusion models (ROIC-DM). Benefiting from its training involving denoising stages, ROIC-DM inherently exhibits greater robustness compared to conventional language models. Moreover, ROIC-DM can attain comparable, and in some cases, superior performance to language models, by effectively incorporating them as advisory components. Extensive experiments conducted with several strong textual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;&#65292;&#24182;&#35299;&#20915;&#20102;&#26684;&#24335;&#19981;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;&#39564;&#35777;&#20102;&#29616;&#26377;&#22522;&#20110;&#20998;&#35789;&#30340;&#27169;&#22411;&#22312;&#23383;&#31526;&#21644;&#20998;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30340;&#30693;&#35782;&#26377;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23450;&#21046;&#27169;&#22411;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03512</link><description>&lt;p&gt;
&#26080;&#38656;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#26356;&#20934;&#30830;&#30340;&#26684;&#24335;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;
&lt;/p&gt;
&lt;p&gt;
Token-free LLMs Can Generate Chinese Classical Poetry with More Accurate Format. (arXiv:2401.03512v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;&#65292;&#24182;&#35299;&#20915;&#20102;&#26684;&#24335;&#19981;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;&#39564;&#35777;&#20102;&#29616;&#26377;&#22522;&#20110;&#20998;&#35789;&#30340;&#27169;&#22411;&#22312;&#23383;&#31526;&#21644;&#20998;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30340;&#30693;&#35782;&#26377;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23450;&#21046;&#27169;&#22411;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;Qwen-chat&#65289;&#33021;&#22815;&#26681;&#25454;&#20154;&#31867;&#30340;&#25351;&#20196;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#22312;&#26684;&#24335;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#27599;&#34892;&#23383;&#31526;&#30340;&#25968;&#37327;&#26377;&#26102;&#36807;&#22810;&#25110;&#19981;&#36275;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#22522;&#20110;&#20998;&#35789;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;&#26684;&#24335;&#19981;&#20934;&#30830;&#26159;&#30001;&#20110;"&#20998;&#35789;&#35268;&#21010;"&#20219;&#21153;&#30340;&#38590;&#24230;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#20934;&#30830;&#30693;&#36947;&#27599;&#20010;&#20998;&#35789;&#20013;&#21253;&#21547;&#22810;&#23569;&#20010;&#23383;&#31526;&#65292;&#24182;&#22522;&#20110;&#36825;&#20010;&#30693;&#35782;&#36827;&#34892;&#38271;&#24230;&#25511;&#21046;&#35268;&#21010;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#23637;&#31034;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#35789;&#21644;&#23383;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30693;&#35782;&#26377;&#38480;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#25340;&#20889;&#27604;&#36187;&#25506;&#27979;&#31243;&#24207;&#65292;&#24182;&#21457;&#29616;Qwen-chat&#22312;&#36817;15%&#30340;&#20013;&#25991;&#25340;&#20889;&#27979;&#35797;&#20013;&#22833;&#36133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#35789;&#30340;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#23450;&#21046;&#25104;&#26080;&#38656;&#20998;&#35789;&#30340;&#27169;&#22411;&#65288;&#23545;&#20110;&#20013;&#25991;&#26469;&#35828;&#65289;&#65292;&#20174;&#32780;&#33021;&#22815;&#24456;&#22823;&#31243;&#24230;&#19978;&#35299;&#20915;&#26684;&#24335;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuned large language models (such as ChatGPT and Qwen-chat) can generate Chinese classical poetry following human's instructions. LLMs perform well in content, but are usually lacking in format, with occasionally excess or insufficient number of characters in each line. Since most SOTA LLMs are token-based, we assume that the format inaccuracy is due to the difficulty of the "token planning" task, which means that the LLM need to know exactly how much characters are contained in each token and do length-control planning based on that knowledge. In this paper, we first confirm our assumption by showing that existing token-based large language models has limited knowledge on token-character relationship. We use a spelling bee probing procedure, and find that Qwen-chat failed in nearly 15% Chinese spelling test. We then show that a token-based model can be easily tailored into a token-free model (in terms of Chinese), which can largely solve the format accuracy problem. Our tailoring 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#27604;&#23427;&#20204;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02994</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#26041;&#27861;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65306;&#30456;&#23545;&#20110;&#19975;&#20159;&#32423;&#21442;&#25968;&#27169;&#22411;&#30340;&#26356;&#24265;&#20215;&#12289;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM. (arXiv:2401.02994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#27604;&#23427;&#20204;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20250;&#35805;&#22411;AI&#30740;&#31350;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#26356;&#22810;&#30340;&#21442;&#25968;&#65292;&#22914;ChatGPT&#31561;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#24222;&#22823;&#30340;&#27169;&#22411;&#24448;&#24448;&#33021;&#29983;&#25104;&#26356;&#22909;&#30340;&#32842;&#22825;&#22238;&#22797;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#20869;&#23384;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#33021;&#21542;&#36890;&#36807;&#32452;&#21512;&#36739;&#23567;&#30340;&#27169;&#22411;&#26469;&#36798;&#21040;&#19982;&#21333;&#20010;&#22823;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23558;&#22810;&#20010;&#32842;&#22825;AI&#38598;&#25104;&#22312;&#19968;&#36215;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#24403;&#29305;&#23450;&#36739;&#23567;&#30340;&#27169;&#22411;&#21327;&#21516;&#28151;&#21512;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#28508;&#22312;&#22320;&#36229;&#36234;&#25110;&#21305;&#25932;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#20165;&#38598;&#25104;&#19977;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;6B/13B&#21442;&#25968;&#65289;&#23601;&#21487;&#20197;&#36798;&#21040;&#25110;&#29978;&#33267;&#36229;&#36234;ChatGPT&#65288;175B+&#21442;&#25968;&#65289;&#31561;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#36825;&#20010;&#20551;&#35774;&#32463;&#36807;&#20102;&#20005;&#26684;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational AI research, there's a noticeable trend towards developing models with a larger number of parameters, exemplified by models like ChatGPT. While these expansive models tend to generate increasingly better chat responses, they demand significant computational resources and memory. This study explores a pertinent question: Can a combination of smaller models collaboratively achieve comparable or enhanced performance relative to a singular large model? We introduce an approach termed "blending", a straightforward yet effective method of integrating multiple chat AIs. Our empirical evidence suggests that when specific smaller models are synergistically blended, they can potentially outperform or match the capabilities of much larger counterparts. For instance, integrating just three models of moderate size (6B/13B paramaeters) can rival or even surpass the performance metrics of a substantially larger model like ChatGPT (175B+ paramaters). This hypothesis is rigorously tes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.01854</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#37319;&#32435;&#65292;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36890;&#36807;&#22312;&#21478;&#19968;&#31181;&#35821;&#35328;&#19978;&#24494;&#35843;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#31181;&#35821;&#35328;&#19978;&#33719;&#24471;&#29305;&#23450;&#30340;&#21151;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLM&#22312;&#25351;&#20196;&#35843;&#20248;&#36807;&#31243;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33521;&#35821;&#35843;&#20248;&#38598;&#21512;&#20013;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#65292;&#22312;&#35843;&#20248;&#36807;&#31243;&#20013;&#19981;&#35770;&#26159;&#24050;&#35265;&#35821;&#35328;&#36824;&#26159;&#26410;&#35265;&#35821;&#35328;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01286</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#19982;&#20154;&#31867;&#20132;&#27969;&#32039;&#23494;&#30456;&#20284;&#30340;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26174;&#33879;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#36896;&#25104;&#30340;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#19990;&#30028;&#30340;&#21160;&#24577;&#24615;&#65292;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;LLM&#20197;&#20462;&#27491;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#38598;&#25104;&#26032;&#30693;&#35782;&#65292;&#20174;&#32780;&#30830;&#20445;&#20854;&#25345;&#32493;&#30340;&#30456;&#20851;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#25345;&#32493;&#30340;&#27169;&#22411;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#32570;&#38519;&#25110;&#19981;&#33391;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;LLM&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#39640;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#26377;&#25928;&#22320;&#20462;&#25913;LLM&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#30693;&#35782;&#32534;&#36753;&#30340;&#30446;&#26631;&#21644;&#25361;&#25112;&#65292;&#28982;&#21518;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24212;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
&lt;/p&gt;</description></item><item><title>CharacterEval&#26159;&#19968;&#20010;&#29992;&#20110;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#35780;&#20272;&#30340;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#21253;&#21547;1,785&#20010;&#22810;&#36718;&#23545;&#35805;&#21644;23,020&#20010;&#31034;&#20363;&#65292;&#28085;&#30422;77&#20010;&#35282;&#33394;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;CharacterEval&#22312;&#35780;&#20272;RPCA&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.01275</link><description>&lt;p&gt;
CharacterEval: &#19968;&#31181;&#29992;&#20110;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#35780;&#20272;&#30340;&#20013;&#25991;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation. (arXiv:2401.01275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01275
&lt;/p&gt;
&lt;p&gt;
CharacterEval&#26159;&#19968;&#20010;&#29992;&#20110;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#35780;&#20272;&#30340;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#21253;&#21547;1,785&#20010;&#22810;&#36718;&#23545;&#35805;&#21644;23,020&#20010;&#31034;&#20363;&#65292;&#28085;&#30422;77&#20010;&#35282;&#33394;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;CharacterEval&#22312;&#35780;&#20272;RPCA&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#29983;&#25104;&#20195;&#29702;&#30340;&#26041;&#24335;&#12290;&#20854;&#20013;&#65292;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#65288;RPCA&#65289;&#30001;&#20110;&#20854;&#35302;&#21457;&#29992;&#25143;&#24773;&#24863;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;CharacterEval&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;RPCA&#30340;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#24182;&#37197;&#26377;&#19968;&#20010;&#23450;&#21046;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;1,785&#20010;&#22810;&#36718;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#65292;&#28085;&#30422;&#20102;23,020&#20010;&#31034;&#20363;&#65292;&#28041;&#21450;&#20102;77&#20010;&#26469;&#28304;&#20110;&#20013;&#22269;&#23567;&#35828;&#21644;&#21095;&#26412;&#30340;&#35282;&#33394;&#12290;&#23427;&#32463;&#36807;&#31934;&#24515;&#26500;&#24314;&#65292;&#39318;&#20808;&#36890;&#36807;GPT-4&#36827;&#34892;&#21021;&#22987;&#23545;&#35805;&#25552;&#21462;&#65292;&#28982;&#21518;&#36827;&#34892;&#20005;&#26684;&#30340;&#20154;&#24037;&#36136;&#37327;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#30334;&#24230;&#30334;&#31185;&#33719;&#21462;&#20102;&#28145;&#20837;&#30340;&#35282;&#33394;&#36164;&#26009;&#12290;CharacterEval&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21253;&#25324;&#22235;&#20010;&#32500;&#24230;&#19978;&#30340;&#21313;&#19977;&#20010;&#26377;&#38024;&#23545;&#24615;&#30340;&#25351;&#26631;&#12290;&#22312;CharacterEval&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 23,020 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. Comprehensive experiments on CharacterEval demonstrate th
&lt;/p&gt;</description></item><item><title>Jatmo&#26159;&#19968;&#31181;&#29983;&#25104;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#20855;&#26377;&#25239;&#24615;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#24182;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2312.17673</link><description>&lt;p&gt;
Jatmo:&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#36827;&#34892;&#25552;&#31034;&#27880;&#20837;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Jatmo: Prompt Injection Defense by Task-Specific Finetuning. (arXiv:2312.17673v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17673
&lt;/p&gt;
&lt;p&gt;
Jatmo&#26159;&#19968;&#31181;&#29983;&#25104;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#20855;&#26377;&#25239;&#24615;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#24182;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#20351;&#29992;&#25143;&#21644;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#21033;&#29992;LLMs&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;LLMs&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#24433;&#21709;&#65306;&#19968;&#31181;&#25915;&#20987;&#26041;&#24335;&#65292;&#36890;&#36807;&#21163;&#25345;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#65292;&#23558;&#23545;&#25552;&#31034;&#30340;&#21709;&#24212;&#26356;&#25913;&#20026;&#19981;&#38656;&#35201;&#25110;&#21487;&#33021;&#20855;&#26377;&#24694;&#24847;&#30340;&#21709;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jatmo&#65292;&#19968;&#31181;&#29983;&#25104;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#20855;&#26377;&#25239;&#24615;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;Jatmo&#21033;&#29992;&#20102;LLMs&#21482;&#33021;&#22312;&#32463;&#21382;&#36807;&#25351;&#20196;&#35843;&#25972;&#21518;&#25165;&#33021;&#36981;&#24490;&#25351;&#20196;&#30340;&#20107;&#23454;&#12290;&#23427;&#21033;&#29992;&#19968;&#20010;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#23545;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#65288;&#21363;&#38750;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#65289;&#36827;&#34892;&#24494;&#35843;&#12290;Jatmo&#21482;&#38656;&#35201;&#19968;&#20010;&#20219;&#21153;&#25552;&#31034;&#21644;&#19968;&#20010;&#20219;&#21153;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#65306;&#23427;&#20351;&#29992;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#12290;&#22312;&#27809;&#26377;&#29616;&#25104;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;Jatmo&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#20363;&#23376;&#65292;&#25110;&#32773;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are attracting significant research attention due to their instruction-following abilities, allowing users and developers to leverage LLMs for a variety of tasks. However, LLMs are vulnerable to prompt-injection attacks: a class of attacks that hijack the model's instruction-following abilities, changing responses to prompts to undesired, possibly malicious ones. In this work, we introduce Jatmo, a method for generating task-specific models resilient to prompt-injection attacks. Jatmo leverages the fact that LLMs can only follow instructions once they have undergone instruction tuning. It harnesses a teacher instruction-tuned model to generate a task-specific dataset, which is then used to fine-tune a base model (i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and a dataset of inputs for the task: it uses the teacher model to generate outputs. For situations with no pre-existing datasets, Jatmo can use a single example, or in some cases
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;LLMs&#23545;&#35823;&#23548;&#20449;&#24687;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#22312;&#20107;&#23454;&#30693;&#35782;&#19978;&#30340;&#27491;&#30830;&#20449;&#24565;&#24456;&#23481;&#26131;&#34987;&#21508;&#31181;&#35828;&#26381;&#31574;&#30053;&#25152;&#25805;&#32437;&#12290;</title><link>http://arxiv.org/abs/2312.09085</link><description>&lt;p&gt;
&#22320;&#29699;&#26159;&#25153;&#24179;&#30340;&#65292;&#22240;&#20026;......&#65306;&#36890;&#36807;&#35828;&#26381;&#24615;&#23545;&#35805;&#30740;&#31350;LLMs&#23545;&#35823;&#23548;&#20449;&#24687;&#30340;&#20449;&#20208;
&lt;/p&gt;
&lt;p&gt;
The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation. (arXiv:2312.09085v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;LLMs&#23545;&#35823;&#23548;&#20449;&#24687;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#22312;&#20107;&#23454;&#30693;&#35782;&#19978;&#30340;&#27491;&#30830;&#20449;&#24565;&#24456;&#23481;&#26131;&#34987;&#21508;&#31181;&#35828;&#26381;&#31574;&#30053;&#25152;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23553;&#35013;&#20102;&#22823;&#37327;&#30693;&#35782;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#22806;&#37096;&#35823;&#23548;&#20449;&#24687;&#30340;&#25915;&#20987;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#22312;&#21333;&#36718;&#23545;&#35805;&#20013;&#30740;&#31350;&#20102;&#36825;&#31181;&#26131;&#21463;&#25915;&#20987;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#65292;&#29305;&#21035;&#26159;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#65292;&#20449;&#20208;&#21487;&#20197;&#21457;&#29983;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;LLMs&#23545;&#35828;&#26381;&#24615;&#23545;&#35805;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#23427;&#20204;&#21487;&#20197;&#27491;&#30830;&#22238;&#31572;&#30340;&#20107;&#23454;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;Farm&#65288;&#21363;&#20107;&#23454;&#21040;&#35823;&#23548;&#65289;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#19982;&#31995;&#32479;&#29983;&#25104;&#30340;&#35828;&#26381;&#24615;&#35823;&#23548;&#20449;&#24687;&#30456;&#21305;&#37197;&#30340;&#20107;&#23454;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#36861;&#36394;LLMs&#22312;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#30340;&#20449;&#20208;&#21464;&#21270;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#20107;&#23454;&#30693;&#35782;&#19978;&#30340;&#27491;&#30830;&#20449;&#24565;&#24456;&#23481;&#26131;&#34987;&#21508;&#31181;&#35828;&#26381;&#31574;&#30053;&#25152;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs' susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs' belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs' correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#21487;&#27604;&#36739;&#30340;&#28436;&#31034;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#28436;&#31034;&#20559;&#35265;&#23384;&#22312;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#32780;&#36890;&#36807;&#21487;&#27604;&#36739;&#30340;&#28436;&#31034;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#22312;ICL&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.07476</link><description>&lt;p&gt;
&#21487;&#27604;&#36739;&#30340;&#28436;&#31034;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65306;&#23545;&#28436;&#31034;&#36873;&#25321;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection. (arXiv:2312.07476v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#21487;&#27604;&#36739;&#30340;&#28436;&#31034;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#28436;&#31034;&#20559;&#35265;&#23384;&#22312;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#32780;&#36890;&#36807;&#21487;&#27604;&#36739;&#30340;&#28436;&#31034;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#22312;ICL&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#65292;&#36890;&#36807;&#23569;&#37327;&#28436;&#31034;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#20110;&#19979;&#28216;&#20219;&#21153;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;ICL&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#28436;&#31034;&#25968;&#37327;&#30340;&#38480;&#21046;&#21487;&#33021;&#23548;&#33268;&#28436;&#31034;&#20559;&#35265;&#65292;&#21363;&#30001;LLMs&#24341;&#36215;&#30340;&#36755;&#20837;-&#26631;&#31614;&#26144;&#23556;&#35823;&#35299;&#20102;&#20219;&#21153;&#30340;&#26412;&#36136;&#12290;&#21463;&#20154;&#31867;&#32463;&#39564;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#28436;&#31034;&#38388;&#20851;&#31995;&#30340;&#35270;&#35282;&#26469;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#21270;&#32534;&#36753;&#25991;&#26412;&#26469;&#26500;&#24314;&#21487;&#27604;&#36739;&#30340;&#28436;&#31034;&#65288;CDs&#65289;&#65292;&#20197;&#32763;&#36716;&#30456;&#24212;&#30340;&#26631;&#31614;&#65292;&#20197;&#31361;&#20986;&#20219;&#21153;&#30340;&#26412;&#36136;&#24182;&#36890;&#36807;&#28436;&#31034;&#38388;&#27604;&#36739;&#28040;&#38500;&#28508;&#22312;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;CDs&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;LLMs&#23384;&#22312;&#28436;&#31034;&#20559;&#35265;&#65292;&#32780;CDs&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#65307;&#65288;2&#65289;CDs&#22312;ICL&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#20998;&#24067;&#22806;&#22330;&#26223;&#20013;&#12290;&#24635;&#20043;&#65292;&#26412;&#30740;&#31350;&#20174;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#25506;&#32034;&#20102;ICL&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL) is an important paradigm for adapting Large Language Models (LLMs) to downstream tasks through a few demonstrations. Despite the great success of ICL, the limitation of the demonstration number may lead to demonstration bias, i.e. the input-label mapping induced by LLMs misunderstands the task's essence. Inspired by human experience, we attempt to mitigate such bias through the perspective of the inter-demonstration relationship. Specifically, we construct Comparable Demonstrations (CDs) by minimally editing the texts to flip the corresponding labels, in order to highlight the task's essence and eliminate potential spurious correlations through the inter-demonstration comparison. Through a series of experiments on CDs, we find that (1) demonstration bias does exist in LLMs, and CDs can significantly reduce such bias; (2) CDs exhibit good performance in ICL, especially in out-of-distribution scenarios. In summary, this study explores the ICL mechanisms from a n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22810;&#39033;&#36873;&#25321;&#20219;&#21153;&#23545;LLaMA&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;LLaMA&#22312;&#25512;&#29702;&#21644;&#35745;&#31639;&#31561;&#39640;&#38454;&#20219;&#21153;&#20013;&#30340;&#20869;&#22312;&#29702;&#35299;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22686;&#22823;&#27169;&#22411;&#23610;&#23544;&#20960;&#20046;&#19981;&#33021;&#33258;&#21160;&#22686;&#21152;&#39069;&#22806;&#30340;&#30693;&#35782;&#25110;&#35745;&#31639;&#33021;&#21147;&#65292;&#20294;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#21644;&#20943;&#23569;&#24187;&#35273;&#65292;&#23588;&#20854;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;LLaMA&#30340;&#36739;&#20302;&#23618;&#27425;&#32570;&#20047;&#23454;&#36136;&#24615;&#30340;&#31639;&#26415;&#21644;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#39030;&#23618;&#23618;&#27425;&#23637;&#29616;&#20986;&#36739;&#24378;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.04333</link><description>&lt;p&gt;
&#22823;&#36824;&#26159;&#28145;&#26159;&#21542;&#24635;&#26159;&#22909;&#30340;&#65311;&#22312;&#19981;&#21516;&#23610;&#24230;&#21644;&#23618;&#27425;&#19978;&#25506;&#31350;LLaMA&#12290;
&lt;/p&gt;
&lt;p&gt;
Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers. (arXiv:2312.04333v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22810;&#39033;&#36873;&#25321;&#20219;&#21153;&#23545;LLaMA&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;LLaMA&#22312;&#25512;&#29702;&#21644;&#35745;&#31639;&#31561;&#39640;&#38454;&#20219;&#21153;&#20013;&#30340;&#20869;&#22312;&#29702;&#35299;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22686;&#22823;&#27169;&#22411;&#23610;&#23544;&#20960;&#20046;&#19981;&#33021;&#33258;&#21160;&#22686;&#21152;&#39069;&#22806;&#30340;&#30693;&#35782;&#25110;&#35745;&#31639;&#33021;&#21147;&#65292;&#20294;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#21644;&#20943;&#23569;&#24187;&#35273;&#65292;&#23588;&#20854;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;LLaMA&#30340;&#36739;&#20302;&#23618;&#27425;&#32570;&#20047;&#23454;&#36136;&#24615;&#30340;&#31639;&#26415;&#21644;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#39030;&#23618;&#23618;&#27425;&#23637;&#29616;&#20986;&#36739;&#24378;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;LLaMA&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38750;&#24120;&#37325;&#35201;&#30340;&#24320;&#28304;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#22810;&#39033;&#36873;&#25321;&#20219;&#21153;&#26469;&#35780;&#20272;LLaMA&#22312;&#25512;&#29702;&#21644;&#35745;&#31639;&#31561;&#39640;&#38454;&#20219;&#21153;&#20013;&#30340;&#20869;&#22312;&#29702;&#35299;&#33021;&#21147;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#20854;&#29983;&#25104;&#30340;&#36755;&#20986;&#26469;&#35780;&#20272;LLaMA&#12290;&#25105;&#20204;&#27839;&#30528;&#27700;&#24179;&#26041;&#21521;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#23610;&#23544;&#65292;&#28982;&#21518;&#27839;&#30528;&#32437;&#21521;&#26041;&#21521;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#23618;&#27425;&#12290;&#26681;&#25454;&#25105;&#20204;&#35774;&#35745;&#30340;&#25506;&#31350;&#20219;&#21153;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20960;&#20010;&#20851;&#38190;&#20294;&#19981;&#24120;&#35265;&#30340;&#21457;&#29616;&#65306;&#65288;1&#65289;&#22312;&#27700;&#24179;&#26041;&#38754;&#65292;&#22686;&#22823;&#27169;&#22411;&#23610;&#23544;&#20960;&#20046;&#19981;&#33021;&#33258;&#21160;&#22320;&#22686;&#21152;&#39069;&#22806;&#30340;&#30693;&#35782;&#25110;&#35745;&#31639;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#65292;&#24182;&#26377;&#21161;&#20110;&#20943;&#23569;&#24187;&#35273;&#65292;&#20294;&#21482;&#26377;&#22312;&#36229;&#36807;&#26576;&#20010;&#23610;&#23544;&#38376;&#27099;&#26102;&#25165;&#33021;&#23454;&#29616;&#65307;&#65288;2&#65289;&#22312;&#32437;&#21521;&#20998;&#26512;&#20013;&#65292;LLaMA&#30340;&#36739;&#20302;&#23618;&#27425;&#32570;&#20047;&#23454;&#36136;&#24615;&#30340;&#31639;&#26415;&#21644;&#20107;&#23454;&#30693;&#35782;&#65292;&#23637;&#29616;&#20102;&#36923;&#36753;&#24605;&#32500;&#12289;&#22810;&#35821;&#35328;&#21644;&#35782;&#21035;&#33021;&#21147;&#65292;&#20854;&#39030;&#23618;&#23618;&#27425;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an in-depth analysis of Large Language Models (LLMs), focusing on LLaMA, a prominent open-source foundational model in natural language processing. Instead of assessing LLaMA through its generative output, we design multiple-choice tasks to probe its intrinsic understanding in high-order tasks such as reasoning and computation. We examine the model horizontally, comparing different sizes, and vertically, assessing different layers. We unveil several key and uncommon findings based on the designed probing tasks: (1) Horizontally, enlarging model sizes almost could not automatically impart additional knowledge or computational prowess. Instead, it can enhance reasoning abilities, especially in math problem solving, and helps reduce hallucinations, but only beyond certain size thresholds; (2) In vertical analysis, the lower layers of LLaMA lack substantial arithmetic and factual knowledge, showcasing logical thinking, multilingual and recognitive abilities, with top la
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GroundedBERT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#35273;&#38170;&#23450;&#20449;&#24687;&#22686;&#24378;BERT&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#35270;&#35273;&#38170;&#23450;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#35821;&#26009;&#24211;&#30340;&#24046;&#24322;&#23548;&#33268;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.01592</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#37096;&#20998;&#23545;&#40784;&#36827;&#34892;&#22522;&#20110;&#35270;&#35273;&#20449;&#24687;&#30340;&#38170;&#23450;&#35821;&#35328;&#23398;&#20064;&#65292;&#25193;&#23637;BERT&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Expand BERT Representation with Visual Information via Grounded Language Learning with Multimodal Partial Alignment. (arXiv:2312.01592v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GroundedBERT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#35273;&#38170;&#23450;&#20449;&#24687;&#22686;&#24378;BERT&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#35270;&#35273;&#38170;&#23450;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#35821;&#26009;&#24211;&#30340;&#24046;&#24322;&#23548;&#33268;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#26377;&#30340;&#35270;&#35273;&#38170;&#23450;&#35821;&#35328;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#36890;&#36807;&#35821;&#35328;&#30446;&#26631;&#21644;&#35270;&#35273;&#38170;&#23450;&#26469;&#36827;&#34892;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35270;&#35273;&#38170;&#23450;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#35821;&#26009;&#24211;&#30340;&#20998;&#24067;&#21644;&#35268;&#27169;&#24046;&#24322;&#65292;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#20250;&#28151;&#28102;&#22312;&#38170;&#23450;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#20196;&#29260;&#30340;&#19978;&#19979;&#25991;&#21644;&#19981;&#20986;&#29616;&#30340;&#20196;&#29260;&#30340;&#19978;&#19979;&#25991;&#12290;&#22240;&#27492;&#65292;&#22312;&#34920;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#35270;&#35273;&#20449;&#24687;&#19982;&#21477;&#23376;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GroundedBERT-&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#38170;&#23450;&#20449;&#24687;&#22686;&#24378;BERT&#34920;&#31034;&#30340;&#22522;&#20110;&#38170;&#23450;&#35821;&#35328;&#23398;&#20064;&#26041;&#27861;&#12290;GroundedBERT&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65306;(i)&#21407;&#22987;BERT&#65292;&#23427;&#20174;&#35821;&#35328;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;(ii)&#35270;&#35273;&#38170;&#23450;&#8203;&#8203;&#27169;&#22359;&#65292;&#23427;&#20174;&#35270;&#35273;&#38170;&#23450;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#35270;&#35273;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#20248;&#20256;&#36755;(OT)&#65292;&#29305;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Language models have been supervised with both language-only objective and visual grounding in existing studies of visual-grounded language learning. However, due to differences in the distribution and scale of visual-grounded datasets and language corpora, the language model tends to mix up the context of the tokens that occurred in the grounded data with those that do not. As a result, during representation learning, there is a mismatch between the visual information and the contextual meaning of the sentence. To overcome this limitation, we propose GroundedBERT - a grounded language learning method that enhances the BERT representation with visually grounded information. GroundedBERT comprises two components: (i) the original BERT which captures the contextual representation of words learned from the language corpora, and (ii) a visual grounding module which captures visual information learned from visual-grounded datasets. Moreover, we employ Optimal Transport (OT), specifically it
&lt;/p&gt;</description></item><item><title>mPLUG-PaperOwl&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#22686;&#24378;&#20102;&#22270;&#34920;&#20998;&#26512;&#33021;&#21147;&#65292;&#20026;&#31185;&#23398;&#23398;&#26415;&#35770;&#25991;&#20889;&#20316;&#25552;&#20379;&#20102;&#26356;&#22810;&#21151;&#33021;&#30340;&#36741;&#21161;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2311.18248</link><description>&lt;p&gt;
mPLUG-PaperOwl: &#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#22270;&#34920;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model. (arXiv:2311.18248v2 [cs.MM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18248
&lt;/p&gt;
&lt;p&gt;
mPLUG-PaperOwl&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#22686;&#24378;&#20102;&#22270;&#34920;&#20998;&#26512;&#33021;&#21147;&#65292;&#20026;&#31185;&#23398;&#23398;&#26415;&#35770;&#25991;&#20889;&#20316;&#25552;&#20379;&#20102;&#26356;&#22810;&#21151;&#33021;&#30340;&#36741;&#21161;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24378;&#22823;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#20652;&#29983;&#20102;&#35768;&#22810;&#36741;&#21161;&#35770;&#25991;&#38405;&#35835;&#29978;&#33267;&#20889;&#20316;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;LLMs&#25110;&#22810;&#27169;&#24577;LLMs&#30340;&#24369;&#22270;&#34920;&#20998;&#26512;&#33021;&#21147;&#26497;&#22823;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#31185;&#23398;&#23398;&#26415;&#35770;&#25991;&#20889;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#23454;&#29616;&#26356;&#22810;&#21151;&#33021;&#30340;&#23398;&#26415;&#35770;&#25991;&#20889;&#20316;&#21161;&#25163;&#65292;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#22686;&#24378;&#22810;&#27169;&#24577;LLMs&#30340;&#22270;&#34920;&#20998;&#26512;&#33021;&#21147;&#12290;&#36890;&#36807;&#35299;&#26512;&#39640;&#36136;&#37327;&#35770;&#25991;&#30340;LaTeX&#28304;&#25991;&#20214;&#65292;&#25105;&#20204;&#31934;&#24515;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#22270;&#34920;&#29702;&#35299;&#25968;&#25454;&#38598;M-Paper&#12290;&#36890;&#36807;&#23558;&#35770;&#25991;&#20013;&#30340;&#22270;&#34920;&#19982;&#30456;&#20851;&#27573;&#33853;&#23545;&#40784;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#19987;&#19994;&#22270;&#34920;&#20998;&#26512;&#26679;&#26412;&#12290;M-Paper&#26159;&#31532;&#19968;&#20010;&#25903;&#25345;&#32508;&#21512;&#29702;&#35299;&#22810;&#20010;&#31185;&#23398;&#22270;&#34920;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20197;&#22270;&#20687;&#25110;LaTeX&#20195;&#30721;&#26684;&#24335;&#25552;&#20379;&#30340;&#22270;&#24418;&#21644;&#34920;&#26684;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#19982;&#29992;&#25143;&#30340;&#24847;&#22270;&#23545;&#40784;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;"&#27010;&#35201;" &#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the strong text creation ability of Large Language Models(LLMs) has given rise to many tools for assisting paper reading or even writing. However, the weak diagram analysis abilities of LLMs or Multimodal LLMs greatly limit their application scenarios, especially for scientific academic paper writing. In this work, towards a more versatile copilot for academic paper writing, we mainly focus on strengthening the multi-modal diagram analysis ability of Multimodal LLMs. By parsing Latex source files of high-quality papers, we carefully build a multi-modal diagram understanding dataset M-Paper. By aligning diagrams in the paper with related paragraphs, we construct professional diagram analysis samples for training and evaluation. M-Paper is the first dataset to support joint comprehension of multiple scientific diagrams, including figures and tables in the format of images or Latex codes. Besides, to better align the copilot with the user's intention, we introduce the `outline' 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#65292;&#20294;&#36890;&#36807;&#20351;&#29992;&#22238;&#28335;&#26041;&#27861;&#21487;&#20197;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#33719;&#24471;&#22823;&#24133;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.08516</link><description>&lt;p&gt;
LLMs&#26080;&#27861;&#25214;&#21040;&#25512;&#29702;&#38169;&#35823;&#65292;&#20294;&#21487;&#20197;&#32416;&#27491;&#23427;&#20204;&#65281;&#65288;arXiv&#65306;2311.08516v2 [cs.AI] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
LLMs cannot find reasoning errors, but can correct them!. (arXiv:2311.08516v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#65292;&#20294;&#36890;&#36807;&#20351;&#29992;&#22238;&#28335;&#26041;&#27861;&#21487;&#20197;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#33719;&#24471;&#22823;&#24133;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#25105;&#32416;&#27491;&#22312;&#25913;&#21892;LLM&#36755;&#20986;&#30340;&#39118;&#26684;&#21644;&#36136;&#37327;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65288;&#20363;&#22914;Chen&#31561;&#65292;2023&#65307;Madaan&#31561;&#65292;2023&#65289;&#65292;&#26368;&#36817;&#23545;&#36923;&#36753;&#25110;&#25512;&#29702;&#38169;&#35823;&#36827;&#34892;&#33258;&#25105;&#32416;&#27491;&#30340;&#23581;&#35797;&#36890;&#24120;&#20250;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#21464;&#20026;&#38169;&#35823;&#65292;&#20174;&#32780;&#24635;&#20307;&#34920;&#29616;&#21464;&#24046;&#65288;Huang&#31561;&#65292;2023&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20998;&#35299;&#20026;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65306;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#12290;&#23545;&#20110;&#38169;&#35823;&#21457;&#29616;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;BIG-Bench Mistake&#65292;&#36825;&#26159;&#19968;&#20010;Chain-of-Thought&#25512;&#29702;&#36712;&#36857;&#20013;&#30340;&#36923;&#36753;&#38169;&#35823;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20026;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LLM&#25552;&#20379;&#22522;&#20934;&#25968;&#65292;&#24182;&#35777;&#26126;LLM&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#12290;&#23545;&#20110;&#36755;&#20986;&#32416;&#27491;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#28335;&#26041;&#27861;&#65292;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#21487;&#20197;&#22823;&#24133;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;&#22238;&#28335;&#35299;&#37322;&#20026;&#23545;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#36731;&#37327;&#32423;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;60-70&#65285;&#20934;&#30830;&#29575;&#19979;&#20445;&#25345;&#26377;&#25928;&#24615;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we break down the self-correction process into two core components: mistake finding and output correction. For mistake finding, we release BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought reasoning traces. We provide benchmark numbers for several state-of-the-art LLMs, and demonstrate that LLMs generally struggle with finding logical mistakes. For output correction, we propose a backtracking method which provides large improvements when given information on mistake location. We construe backtracking as a lightweight alternative to reinforcement learning methods, and show that it remains effective with a reward model at 60-70% accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36923;&#36753;&#24418;&#24335;&#65288;LF&#65289;&#26469;&#25552;&#39640;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#29992;&#33258;&#21160;LF&#25913;&#36827;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;30&#20010;&#30334;&#20998;&#28857;&#65292;&#36824;&#25351;&#20986;&#20102;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#20173;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.17279</link><description>&lt;p&gt;
&#33258;&#21160;&#36923;&#36753;&#24418;&#24335;&#25552;&#39640;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Automatic Logical Forms improve fidelity in Table-to-Text generation. (arXiv:2310.17279v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36923;&#36753;&#24418;&#24335;&#65288;LF&#65289;&#26469;&#25552;&#39640;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#29992;&#33258;&#21160;LF&#25913;&#36827;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;30&#20010;&#30334;&#20998;&#28857;&#65292;&#36824;&#25351;&#20986;&#20102;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#20173;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#21040;&#25991;&#26412;&#31995;&#32479;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#34920;&#26684;&#65289;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#12290;&#34429;&#28982;&#31471;&#21040;&#31471;&#25216;&#26415;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#20294;&#20808;&#21069;&#30340;&#30740;&#31350;&#25253;&#21578;&#31216;&#65292;&#22312;&#20351;&#29992;&#25163;&#21160;&#36923;&#36753;&#24418;&#24335;&#65288;LF&#65289;&#34920;&#31034;&#25152;&#36873;&#20869;&#23481;&#21644;&#30446;&#26631;&#25991;&#26412;&#30340;&#35821;&#20041;&#26102;&#65292;&#33719;&#24471;&#20102;&#25552;&#21319;&#12290;&#37492;&#20110;&#25163;&#21160;&#27493;&#39588;&#65292;&#19981;&#28165;&#26970;&#33258;&#21160;LF&#26159;&#21542;&#26377;&#25928;&#65292;&#25110;&#32773;&#25913;&#36827;&#26469;&#33258;&#20869;&#23481;&#36873;&#25321;&#26412;&#36523;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TlT&#65292;&#32473;&#23450;&#19968;&#20010;&#34920;&#26684;&#21644;&#20869;&#23481;&#36873;&#25321;&#65292;&#39318;&#20808;&#29983;&#25104;LF&#65292;&#28982;&#21518;&#29983;&#25104;&#25991;&#26412;&#38472;&#36848;&#12290;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#33258;&#21160;LF&#25552;&#39640;&#36136;&#37327;&#30340;&#25928;&#26524;&#65292;&#19982;&#19981;&#20351;&#29992;LF&#30340;&#31867;&#20284;&#31995;&#32479;&#30456;&#27604;&#65292;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;30&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23545;&#20110;&#39640;&#20934;&#30830;&#24615;&#36824;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#33258;&#21160;&#20869;&#23481;&#36873;&#25321;&#26159;&#39318;&#35201;&#38382;&#39064;&#65292;&#20854;&#27425;&#26159;&#26356;&#22909;&#30340;&#36923;&#36753;&#21040;&#25991;&#26412;&#29983;&#25104;&#65292;&#20197;&#21450;&#36739;&#23569;&#31243;&#24230;&#30340;&#26356;&#22909;&#30340;&#34920;&#26684;&#21040;&#36923;&#36753;&#35299;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Table-to-text systems generate natural language statements from structured data like tables. While end-to-end techniques suffer from low factual correctness (fidelity), a previous study reported gains when using manual logical forms (LF) that represent the selected content and the semantics of the target text. Given the manual step, it was not clear whether automatic LFs would be effective, or whether the improvement came from content selection alone. We present TlT which, given a table and a selection of the content, first produces LFs and then the textual statement. We show for the first time that automatic LFs improve quality, with an increase in fidelity of 30 points over a comparable system not using LFs. Our experiments allow to quantify the remaining challenges for high factual correctness, with automatic selection of content coming first, followed by better Logic-to-Text generation and, to a lesser extent, better Table-to-Logic parsing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#32467;&#26500;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26631;&#31614;&#19982;&#23646;&#24615;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12803</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#32467;&#26500;&#30340;&#25991;&#26412;&#31163;&#32676;&#20540;&#27867;&#21270;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal-structure Driven Augmentations for Text OOD Generalization. (arXiv:2310.12803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#32467;&#26500;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26631;&#31614;&#19982;&#23646;&#24615;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#22120;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#20381;&#36182;&#21487;&#33021;&#23548;&#33268;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#19981;&#20339;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#20854;&#22312;&#22914;&#21307;&#30103;&#39046;&#22495;&#31561;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#20351;&#29992;&#30340;&#25285;&#24551;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22240;&#26524;&#32467;&#26500;&#30693;&#35782;&#25351;&#23548;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#27169;&#25311;&#23545;&#34394;&#20551;&#29305;&#24449;&#36827;&#34892;&#24178;&#39044;&#65292;&#20197;&#23398;&#20064;&#26356;&#21152;&#40065;&#26834;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26631;&#31614;&#19982;&#23646;&#24615;&#20043;&#38388;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#39044;&#27979;&#38382;&#39064;&#20013;&#65292;&#36825;&#31181;&#31574;&#30053;&#26159;&#21512;&#36866;&#30340;&#12290;&#22312;&#36825;&#31181;&#38382;&#39064;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#30456;&#23545;&#20110;&#37325;&#35201;&#24615;&#37325;&#21152;&#26435;&#30340;&#26377;&#21033;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#36890;&#36807;&#24046;&#20998;&#22312;&#24046;&#20998;&#30340;&#26041;&#27861;&#26469;&#21305;&#37197;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#34920;&#31034;&#25991;&#26412;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#36890;&#36807;&#23545;&#20174;&#21307;&#23398;&#21465;&#36848;&#20013;&#23398;&#20064;&#19982;&#30475;&#25252;&#32773;&#26080;&#20851;&#30340;&#20020;&#24202;&#35786;&#26029;&#39044;&#27979;&#22120;&#20197;&#21450;&#21322;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reliance of text classifiers on spurious correlations can lead to poor generalization at deployment, raising concerns about their use in safety-critical domains such as healthcare. In this work, we propose to use counterfactual data augmentation, guided by knowledge of the causal structure of the data, to simulate interventions on spurious features and to learn more robust text classifiers. We show that this strategy is appropriate in prediction problems where the label is spuriously correlated with an attribute. Under the assumptions of such problems, we discuss the favorable sample complexity of counterfactual data augmentation, compared to importance re-weighting. Pragmatically, we match examples using auxiliary data, based on diff-in-diff methodology, and use a large language model (LLM) to represent a conditional probability of text. Through extensive experimentation on learning caregiver-invariant predictors of clinical diagnoses from medical narratives and on semi-synthetic 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#26500;&#24314;&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;&#36827;&#34892;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#21644;&#25991;&#26412;&#23545;&#35805;&#31995;&#32479;&#23454;&#29616;&#20102;&#25554;&#27133;&#21644;&#20540;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.15053</link><description>&lt;p&gt;
&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;
&lt;/p&gt;
&lt;p&gt;
Adapting text-based dialogue state tracker for spoken dialogues. (arXiv:2308.15053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#26500;&#24314;&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;&#36827;&#34892;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#21644;&#25991;&#26412;&#23545;&#35805;&#31995;&#32479;&#23454;&#29616;&#20102;&#25554;&#27133;&#21644;&#20540;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36890;&#36807;&#23545;&#35805;&#31995;&#32479;&#25216;&#26415;&#31454;&#36187;&#65288;DSTC&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#35821;&#38899;&#30028;&#38754;&#30340;&#31283;&#20581;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22823;&#37096;&#20998;&#36827;&#23637;&#37117;&#26159;&#38024;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#22240;&#20026;&#26377;&#20016;&#23500;&#30340;&#20070;&#38754;&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#65292;&#32780;&#20855;&#26377;&#21475;&#35821;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#38750;&#24120;&#31232;&#32570;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;Siri&#21644;Alexa&#31561;&#35821;&#38899;&#21161;&#25163;&#31995;&#32479;&#25152;&#23637;&#31034;&#30340;&#65292;&#23558;&#36825;&#31181;&#25104;&#21151;&#36716;&#31227;&#21040;&#21475;&#35821;&#23545;&#35805;&#20013;&#20855;&#26377;&#23454;&#38469;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;DSTC11&#30340;&#20855;&#26377;&#35821;&#38899;&#24863;&#30693;&#30340;&#23545;&#35805;&#31995;&#32479;&#25216;&#26415;&#25361;&#25112;&#36187;&#20013;&#30340;&#39640;&#24230;&#25104;&#21151;&#27169;&#22411;&#30340;&#24037;&#31243;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65306;&#65288;1&#65289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#65292;&#20197;&#24357;&#21512;&#21475;&#35821;&#21644;&#25991;&#26412;&#35805;&#35821;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#65288;2&#65289;&#29992;&#20110;&#20272;&#35745;&#25554;&#27133;&#21644;&#20540;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#23545;&#35805;&#31995;&#32479;&#65288;D3ST&#65289;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#25554;&#27133;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although there have been remarkable advances in dialogue systems through the dialogue systems technology competition (DSTC), it remains one of the key challenges to building a robust task-oriented dialogue system with a speech interface. Most of the progress has been made for text-based dialogue systems since there are abundant datasets with written corpora while those with spoken dialogues are very scarce. However, as can be seen from voice assistant systems such as Siri and Alexa, it is of practical importance to transfer the success to spoken dialogues. In this paper, we describe our engineering effort in building a highly successful model that participated in the speech-aware dialogue systems technology challenge track in DSTC11. Our model consists of three major modules: (1) automatic speech recognition error correction to bridge the gap between the spoken and the text utterances, (2) text-based dialogue system (D3ST) for estimating the slots and values using slot descriptions, an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#22696;&#35199;&#21733;&#24037;&#20154;&#19968;&#30452;&#24314;&#35758;&#20302;&#34218;&#24037;&#20316;&#65292;&#24182;&#21521;&#22899;&#24615;&#26356;&#20542;&#21521;&#20110;&#25512;&#33616;&#31192;&#20070;&#32844;&#20301;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#29702;&#35299;LLMs&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02053</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#24179;&#31561;&#26426;&#20250;: &#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#25581;&#31034;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations. (arXiv:2308.02053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02053
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#22696;&#35199;&#21733;&#24037;&#20154;&#19968;&#30452;&#24314;&#35758;&#20302;&#34218;&#24037;&#20316;&#65292;&#24182;&#21521;&#22899;&#24615;&#26356;&#20542;&#21521;&#20110;&#25512;&#33616;&#31192;&#20070;&#32844;&#20301;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#29702;&#35299;LLMs&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#20102;&#35299;&#36825;&#20123;&#20559;&#35265;&#23545;&#20110;&#29702;&#35299;&#22312;&#20351;&#29992;LLMs&#36827;&#34892;&#20915;&#31574;&#26102;&#28508;&#22312;&#30340;&#21518;&#32493;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21382;&#21490;&#19978;&#22788;&#20110;&#21155;&#21183;&#30340;&#32676;&#20307;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#30340;&#35282;&#24230;&#20998;&#26512;&#21644;&#27604;&#36739;LLMs&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;ChatGPT&#21644;LLaMA&#36825;&#20004;&#20010;&#21069;&#27839;LLMs&#20869;&#30340;&#20132;&#21449;&#20559;&#35265;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20027;&#35201;&#38598;&#20013;&#22312;&#25581;&#31034;&#24615;&#21035;&#35748;&#21516;&#21644;&#22269;&#31821;&#20559;&#35265;&#19978;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#20154;&#21475;&#32479;&#35745;&#36523;&#20221;&#30340;&#20132;&#21449;&#20559;&#35265;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#26126;&#26174;&#30340;&#20559;&#35265;&#65292;&#20363;&#22914;&#20004;&#20010;&#27169;&#22411;&#19968;&#30452;&#24314;&#35758;&#22696;&#35199;&#21733;&#24037;&#20154;&#20174;&#20107;&#20302;&#34218;&#24037;&#20316;&#65292;&#25110;&#32773;&#26356;&#20542;&#21521;&#20110;&#21521;&#22899;&#24615;&#25512;&#33616;&#31192;&#20070;&#32844;&#20301;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#27979;&#37327;&#21644;&#29702;&#35299;LLMs&#20013;&#30340;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have seen widespread deployment in various real-world applications. Understanding these biases is crucial to comprehend the potential downstream consequences when using LLMs to make decisions, particularly for historically disadvantaged groups. In this work, we propose a simple method for analyzing and comparing demographic bias in LLMs, through the lens of job recommendations. We demonstrate the effectiveness of our method by measuring intersectional biases within ChatGPT and LLaMA, two cutting-edge LLMs. Our experiments primarily focus on uncovering gender identity and nationality bias; however, our method can be extended to examine biases associated with any intersection of demographic identities. We identify distinct biases in both models toward various demographic identities, such as both models consistently suggesting low-paying jobs for Mexican workers or preferring to recommend secretarial roles to women. Our study highlights the importance of measu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26597;&#35810;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#23433;&#20840;&#26377;&#25928;&#22320;&#35757;&#32451;&#28145;&#24230;&#26816;&#32034;&#27169;&#22411;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.05973</link><description>&lt;p&gt;
&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26597;&#35810;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;.
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models. (arXiv:2305.05973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26597;&#35810;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#23433;&#20840;&#26377;&#25928;&#22320;&#35757;&#32451;&#28145;&#24230;&#26816;&#32034;&#27169;&#22411;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21457;&#38544;&#31169;&#20445;&#25252;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#65292;&#20811;&#26381;&#20102;&#22312;&#35757;&#32451;&#36825;&#20123;&#22797;&#26434;&#31995;&#32479;&#26102;&#30340;&#26576;&#20123;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#20294;&#20063;&#21487;&#20197;&#36731;&#26494;&#22320;&#29992;&#20110;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#34920;&#31034;&#30340;&#20219;&#20309;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;DP&#35757;&#32451;&#26041;&#27861;&#65292;&#23545;&#20844;&#24320;&#39044;&#35757;&#32451;&#30340;LLM&#22312;&#26597;&#35810;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#29983;&#25104;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#31169;&#26377;&#21512;&#25104;&#26597;&#35810;&#65292;&#20195;&#34920;&#21407;&#22987;&#26597;&#35810;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#19979;&#28216;&#38750;&#31169;&#26377;&#25512;&#33616;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#30001;&#20849;&#20139;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#20219;&#20309;&#39069;&#22806;&#30340;&#38544;&#31169;&#25104;&#26412;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#23433;&#20840;&#35757;&#32451;&#26377;&#25928;&#30340;&#28145;&#24230;&#26816;&#32034;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#30340;&#26816;&#32034;&#36136;&#37327;&#26377;&#26174;&#30528;&#30340;&#25552;&#39640;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#26597;&#35810;&#32423;&#21035;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach for developing privacy-preserving large-scale recommender systems using differentially private (DP) large language models (LLMs) which overcomes certain challenges and limitations in DP training these complex systems. Our method is particularly well suited for the emerging area of LLM-based recommender systems, but can be readily employed for any recommender systems that process representations of natural language inputs. Our approach involves using DP training methods to fine-tune a publicly pre-trained LLM on a query generation task. The resulting model can generate private synthetic queries representative of the original queries which can be freely shared for any downstream non-private recommendation training procedures without incurring any additional privacy cost. We evaluate our method on its ability to securely train effective deep retrieval models, and we observe significant improvements in their retrieval quality without compromising query-level pri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#20132;&#21449;&#27880;&#24847;&#27169;&#22411;&#65288;HCAM&#65289;&#29992;&#20110;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#65292;&#20351;&#29992;&#36882;&#24402;&#21644;&#20849;&#21516;&#27880;&#24847;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#65292;&#23558;&#36825;&#20004;&#31181;&#27169;&#24577;&#20449;&#24687;&#20197;&#20849;&#21516;&#27880;&#24847;&#26041;&#24335;&#32467;&#21512;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24773;&#24863;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.06910</link><description>&lt;p&gt;
HCAM--&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#30340;&#20998;&#23618;&#20132;&#21449;&#27880;&#24847;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HCAM -- Hierarchical Cross Attention Model for Multi-modal Emotion Recognition. (arXiv:2304.06910v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06910
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#20132;&#21449;&#27880;&#24847;&#27169;&#22411;&#65288;HCAM&#65289;&#29992;&#20110;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#65292;&#20351;&#29992;&#36882;&#24402;&#21644;&#20849;&#21516;&#27880;&#24847;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#65292;&#23558;&#36825;&#20004;&#31181;&#27169;&#24577;&#20449;&#24687;&#20197;&#20849;&#21516;&#27880;&#24847;&#26041;&#24335;&#32467;&#21512;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24773;&#24863;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30001;&#20110;&#24773;&#24863;&#34920;&#36798;&#30340;&#22810;&#27169;&#24577;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36882;&#24402;&#21644;&#20849;&#21516;&#27880;&#24847;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20998;&#23618;&#20132;&#21449;&#27880;&#24847;&#27169;&#22411;&#65288;HCAM&#65289;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#12290;&#27169;&#22411;&#30340;&#36755;&#20837;&#21253;&#25324;&#20004;&#31181;&#27169;&#24577;&#65292;&#21363;&#36890;&#36807;&#21487;&#23398;&#20064;wav2vec&#26041;&#27861;&#22788;&#29702;&#30340;&#38899;&#39057;&#25968;&#25454;&#21644;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#26469;&#33258;&#21464;&#21387;&#22120;&#65288;BERT&#65289;&#27169;&#22411;&#34920;&#31034;&#30340;&#25991;&#26412;&#25968;&#25454;&#12290;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#20351;&#29992;&#19968;&#32452;&#21452;&#21521;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#20351;&#29992;&#33258;&#27880;&#24847;&#23558;&#32473;&#23450;&#23545;&#35805;&#20013;&#30340;&#27599;&#20010;&#35805;&#35821;&#36716;&#25442;&#20026;&#22266;&#23450;&#32500;&#24230;&#30340;&#23884;&#20837;&#12290;&#20026;&#20102;&#25972;&#21512;&#19978;&#19979;&#25991;&#30693;&#35782;&#21644;&#20004;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#23618;&#23558;&#38899;&#39057;&#21644;&#25991;&#26412;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#65292;&#35797;&#22270;&#34913;&#37327;&#19982;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#30456;&#20851;&#30340;&#35805;&#35821;&#32423;&#23884;&#20837;&#12290;&#22312;CMU-MOSI&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#20250;&#35805;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;HCAM&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#24773;&#24863;&#35782;&#21035;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition in conversations is challenging due to the multi-modal nature of the emotion expression. We propose a hierarchical cross-attention model (HCAM) approach to multi-modal emotion recognition using a combination of recurrent and co-attention neural network models. The input to the model consists of two modalities, i) audio data, processed through a learnable wav2vec approach and, ii) text data represented using a bidirectional encoder representations from transformers (BERT) model. The audio and text representations are processed using a set of bi-directional recurrent neural network layers with self-attention that converts each utterance in a given conversation to a fixed dimensional embedding. In order to incorporate contextual knowledge and the information across the two modalities, the audio and text embeddings are combined using a co-attention layer that attempts to weigh the utterance level embeddings relevant to the task of emotion recognition. The neural network
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35821;&#35328;&#22686;&#24378;Transformer&#32534;&#30721;&#22120;&#65292;&#24182;&#32467;&#21512;&#21307;&#23398;&#25552;&#31034;&#65292;&#23558;&#32467;&#26500;&#21270;&#12289;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#35821;&#35328;&#28508;&#31354;&#38388;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#21307;&#23398;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.17408</link><description>&lt;p&gt;
&#22522;&#20110;&#21307;&#23398;&#25552;&#31034;&#30340;&#35821;&#35328;&#22686;&#24378;Transformer&#32534;&#30721;&#22120;&#30340;&#21307;&#30103;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Medical Intervention Duration Estimation Using Language-enhanced Transformer Encoder with Medical Prompts. (arXiv:2303.17408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17408
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#22686;&#24378;Transformer&#32534;&#30721;&#22120;&#65292;&#24182;&#32467;&#21512;&#21307;&#23398;&#25552;&#31034;&#65292;&#23558;&#32467;&#26500;&#21270;&#12289;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#35821;&#35328;&#28508;&#31354;&#38388;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#21307;&#23398;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;(EHRs)&#20272;&#35745;&#21307;&#30103;&#24178;&#39044;&#30340;&#25345;&#32493;&#26102;&#38388;&#22312;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#39046;&#22495;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24573;&#30053;&#20102;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#33258;&#30001;&#25991;&#26412;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#35328;&#22686;&#24378;Transformer-based&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#23558;&#25152;&#26377;&#30456;&#20851;&#30340;&#20020;&#24202;&#25968;&#25454;&#27169;&#24577;&#65288;&#36830;&#32493;&#12289;&#20998;&#31867;&#12289;&#20108;&#36827;&#21046;&#21644;&#33258;&#30001;&#25991;&#26412;&#29305;&#24449;&#65289;&#25237;&#24433;&#21040;&#19968;&#20010;&#21327;&#35843;&#30340;&#35821;&#35328;&#28508;&#31354;&#38388;&#20013;&#65292;&#20511;&#21161;&#21307;&#23398;&#25552;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#24471;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#22312;&#21333;&#20803;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#20013;&#38598;&#25104;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#21307;&#23398;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#32654;&#22269;&#65288;ICU&#20303;&#38498;&#26102;&#38388;&#20272;&#35745;&#65289;&#21644;&#20122;&#27954;&#65288;&#25163;&#26415;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#65289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, estimating the duration of medical intervention based on electronic health records (EHRs) has gained significant attention in the filed of clinical decision support. However, current models largely focus on structured data, leaving out information from the unstructured clinical free-text data. To address this, we present a novel language-enhanced transformer-based framework, which projects all relevant clinical data modalities (continuous, categorical, binary, and free-text features) into a harmonized language latent space using a pre-trained sentence encoder with the help of medical prompts. The proposed method enables the integration of information from different modalities within the cell transformer encoder and leads to more accurate duration estimation for medical intervention. Our experimental results on both US-based (length of stay in ICU estimation) and Asian (surgical duration prediction) medical datasets demonstrate the effectiveness of our proposed framewor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#65292;&#20551;&#35774;&#35757;&#32451;&#21477;&#23376;&#30001;&#36981;&#24490;&#20132;&#38469;&#22522;&#26412;&#21407;&#21017;&#30340;&#20195;&#29702;&#29983;&#25104;&#65292;&#37027;&#20040;&#21487;&#20197;&#20174;&#30446;&#26631;&#20998;&#24067;&#23436;&#32654;&#23398;&#20064;&#30340;&#29702;&#24819;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#20986;&#34164;&#21547;&#21028;&#26029;&#12290;&#36825;&#20010;&#32467;&#26524;&#25581;&#31034;&#20102;&#20174;&#26410;&#26631;&#27880;&#35821;&#35328;&#25968;&#25454;&#20013;&#29702;&#35299;&#35821;&#20041;&#21644;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#35821;&#20041;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.12407</link><description>&lt;p&gt;
&#20174;&#29702;&#24819;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#21462;&#34164;&#21547;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Entailment Semantics Can Be Extracted from an Ideal Language Model. (arXiv:2209.12407v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#65292;&#20551;&#35774;&#35757;&#32451;&#21477;&#23376;&#30001;&#36981;&#24490;&#20132;&#38469;&#22522;&#26412;&#21407;&#21017;&#30340;&#20195;&#29702;&#29983;&#25104;&#65292;&#37027;&#20040;&#21487;&#20197;&#20174;&#30446;&#26631;&#20998;&#24067;&#23436;&#32654;&#23398;&#20064;&#30340;&#29702;&#24819;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#20986;&#34164;&#21547;&#21028;&#26029;&#12290;&#36825;&#20010;&#32467;&#26524;&#25581;&#31034;&#20102;&#20174;&#26410;&#26631;&#27880;&#35821;&#35328;&#25968;&#25454;&#20013;&#29702;&#35299;&#35821;&#20041;&#21644;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#35821;&#20041;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20165;&#36890;&#36807;&#25991;&#26412;&#35757;&#32451;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#22522;&#30784;&#12290;&#20851;&#20110;&#36825;&#31181;&#36807;&#31243;&#33021;&#22815;&#25512;&#26029;&#20986;&#22810;&#23569;&#33258;&#28982;&#35821;&#35328;&#35821;&#20041;&#23384;&#22312;&#20105;&#35758;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20551;&#35774;&#35757;&#32451;&#21477;&#23376;&#30001;Gricean&#20195;&#29702;&#29983;&#25104;&#65288;&#21363;&#36981;&#24490;&#35821;&#29992;&#23398;&#20013;&#30340;&#20132;&#38469;&#22522;&#26412;&#21407;&#21017;&#30340;&#20195;&#29702;&#65289;&#65292;&#21487;&#20197;&#20174;&#23436;&#32654;&#23398;&#20064;&#20102;&#30446;&#26631;&#20998;&#24067;&#30340;&#29702;&#24819;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#20986;&#21477;&#23376;&#20043;&#38388;&#30340;&#34164;&#21547;&#21028;&#26029;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21487;&#20197;&#20174;&#35757;&#32451;&#22312;&#36825;&#31181;Gricean&#25968;&#25454;&#19978;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#20013;&#35299;&#30721;&#20986;&#34164;&#21547;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#29702;&#35299;&#26410;&#26631;&#27880;&#35821;&#35328;&#25968;&#25454;&#20013;&#32534;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#36884;&#24452;&#65292;&#20197;&#21450;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#35821;&#20041;&#30340;&#28508;&#22312;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are often trained on text alone, without additional grounding. There is debate as to how much of natural language semantics can be inferred from such a procedure. We prove that entailment judgments between sentences can be extracted from an ideal language model that has perfectly learned its target distribution, assuming the training sentences are generated by Gricean agents, i.e., agents who follow fundamental principles of communication from the linguistic theory of pragmatics. We also show entailment judgments can be decoded from the predictions of a language model trained on such Gricean data. Our results reveal a pathway for understanding the semantic information encoded in unlabeled linguistic data and a potential framework for extracting semantics from language models.
&lt;/p&gt;</description></item><item><title>E2S2&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#32534;&#30721;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#31471;&#24341;&#20837;&#26356;&#26377;&#25928;&#30340;&#33258;&#25105;&#30417;&#30563;&#20449;&#24687;&#65292;&#25913;&#36827;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.14912</link><description>&lt;p&gt;
E2S2: &#22686;&#24378;&#32534;&#30721;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language Understanding and Generation. (arXiv:2205.14912v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14912
&lt;/p&gt;
&lt;p&gt;
E2S2&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#32534;&#30721;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#31471;&#24341;&#20837;&#26356;&#26377;&#25928;&#30340;&#33258;&#25105;&#30417;&#30563;&#20449;&#24687;&#65292;&#25913;&#36827;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;(seq2seq)&#23398;&#20064;&#26159;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#27969;&#34892;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;seq2seq&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#24120;&#21482;&#20851;&#27880;&#35299;&#30721;&#22120;&#26041;&#38754;&#30340;&#37325;&#26500;&#30446;&#26631;&#65292;&#24573;&#35270;&#20102;&#32534;&#30721;&#22120;&#26041;&#38754;&#30340;&#30417;&#30563;&#20316;&#29992;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#39564;&#35777;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#39318;&#20808;&#23454;&#35777;&#30740;&#31350;&#20102;&#24207;&#21015;&#21040;&#24207;&#21015;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#21151;&#33021;&#65292;&#24182;&#21457;&#29616;&#32534;&#30721;&#22120;&#22312;&#19979;&#28216;&#24615;&#33021;&#21644;&#31070;&#32463;&#20803;&#28608;&#27963;&#26041;&#38754;&#25198;&#28436;&#30528;&#37325;&#35201;&#20294;&#34987;&#20302;&#20272;&#30340;&#35282;&#33394;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#32534;&#30721;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#31216;&#20026;E2S2&#65292;&#36890;&#36807;&#23558;&#26356;&#26377;&#25928;&#30340;&#33258;&#25105;&#30417;&#30563;&#20449;&#24687;&#25972;&#21512;&#21040;&#32534;&#30721;&#22120;&#20013;&#26469;&#25913;&#36827;seq2seq&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;E2S2&#22312;&#32534;&#30721;&#22120;&#31471;&#37319;&#29992;&#20102;&#20004;&#20010;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#65306;1) &#26412;&#22320;&#21435;&#22122;&#21463;&#25439;&#21477;&#23376;&#65288;&#21435;&#22122;&#30446;&#26631;&#65289;&#65307;2) &#20840;&#23616;&#23398;&#20064;&#26356;&#22909;&#30340;&#21477;&#23376;&#34920;&#31034;&#65288;&#20840;&#23616;&#23398;&#20064;&#30446;&#26631;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-sequence (seq2seq) learning is a popular fashion for large-scale pretraining language models. However, the prior seq2seq pretraining models generally focus on reconstructive objectives on the decoder side and neglect the effect of encoder-side supervision, which we argue may lead to sub-optimal performance. To verify our hypothesis, we first empirically study the functionalities of the encoder and decoder in seq2seq pretrained language models, and find that the encoder takes an important but under-exploitation role than the decoder regarding the downstream performance and neuron activation. Therefore, we propose an encoding-enhanced seq2seq pretraining strategy, namely E2S2, which improves the seq2seq models via integrating more efficient self-supervised information into the encoders. Specifically, E2S2 adopts two self-supervised objectives on the encoder side from two aspects: 1) locally denoising the corrupted sentence (denoising objective); and 2) globally learning bette
&lt;/p&gt;</description></item></channel></rss>