<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#29983;&#25104;&#24335;&#26816;&#32034;&#12289;&#38381;&#24335;&#29983;&#25104;&#21644;RAG&#65292;&#21033;&#29992;&#22806;&#37096;&#35821;&#26009;&#22788;&#29702;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01176</link><description>&lt;p&gt;
&#20026;&#21033;&#29992;&#22806;&#37096;&#35821;&#26009;&#36827;&#34892;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#32780;&#26500;&#24314;&#30340;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing External Corpus
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#29983;&#25104;&#24335;&#26816;&#32034;&#12289;&#38381;&#24335;&#29983;&#25104;&#21644;RAG&#65292;&#21033;&#29992;&#22806;&#37096;&#35821;&#26009;&#22788;&#29702;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#28982;&#32780;&#22312;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#26469;&#28304;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#20135;&#29983;&#34394;&#26500;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25104;&#20026;&#20102;&#19968;&#31181;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26816;&#32034;&#27169;&#22359;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#25991;&#26723;&#32034;&#24341;&#65292;&#36825;&#21487;&#33021;&#19982;&#29983;&#25104;&#20219;&#21153;&#30456;&#33073;&#31163;&#12290;&#36890;&#36807;&#29983;&#25104;&#24335;&#26816;&#32034;&#65288;GR&#65289;&#26041;&#27861;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#30456;&#20851;&#25991;&#26723;&#26631;&#35782;&#31526;&#65288;DocIDs&#65289;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;GR&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;LLMs&#22312;GR&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#29983;&#25104;&#24335;&#26816;&#32034;&#12289;&#38381;&#24335;&#29983;&#25104;&#21644;RAG&#65292;&#21033;&#29992;&#22806;&#37096;&#35821;&#26009;&#22788;&#29702;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has showcased their efficacy across various domains, yet they often hallucinate, especially in knowledge-intensive tasks that require external knowledge sources. To improve factual accuracy of language models, retrieval-augmented generation (RAG) has emerged as a popular solution. However, traditional retrieval modules often rely on large-scale document indexes, which can be disconnected from generative tasks. Through generative retrieval (GR) approach, language models can achieve superior retrieval performance by directly generating relevant document identifiers (DocIDs). However, the relationship between GR and downstream tasks, as well as the potential of LLMs in GR, remains unexplored. In this paper, we present a unified language model that utilizes external corpus to handle various knowledge-intensive tasks by seamlessly integrating generative retrieval, closed-book generation, and RAG. In order to achieve effective retrieval and generati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20165;&#22312;&#38750;&#27954;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#65292;&#26356;&#39640;&#25928;&#24182;&#22312;ASR&#21644;LID&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02000</link><description>&lt;p&gt;
&#38750;&#27954;&#20013;&#24515;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25216;&#26415;&#22312;&#25746;&#21704;&#25289;&#20197;&#21335;&#22320;&#21306;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#34920;&#24449;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02000
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20165;&#22312;&#38750;&#27954;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#65292;&#26356;&#39640;&#25928;&#24182;&#22312;ASR&#21644;LID&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20165;&#22312;&#38750;&#27954;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20174;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#22320;&#21306;&#35762;&#35805;&#30340;21&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#23398;&#20064;&#20102;&#36817;60,000&#23567;&#26102;&#30340;&#26410;&#26631;&#35760;&#35821;&#38899;&#29255;&#27573;&#12290;&#22312;FLEURS-102&#25968;&#25454;&#38598;&#30340;SSA&#23376;&#38598;&#19978;&#65292;&#25105;&#20204;&#22522;&#20110;HuBERT$_{base}$ (0.09B) &#26550;&#26500;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#19982;FLEURS&#22522;&#20934;&#25552;&#20986;&#30340;w2v-bert-51 (0.6B) &#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;ASR&#19979;&#28216;&#20219;&#21153;&#20013;&#26356;&#21152;&#39640;&#25928;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#23569;7&#20493;&#65292;&#21442;&#25968;&#23569;6&#20493;&#12290;&#27492;&#22806;&#65292;&#22312;LID&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#36229;&#36807;FLEURS&#22522;&#32447;&#36229;&#36807;22%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02000v1 Announce Type: new  Abstract: We present the first self-supervised multilingual speech model trained exclusively on African speech. The model learned from nearly 60 000 hours of unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan Africa. On the SSA subset of the FLEURS-102 dataset, our approach based on a HuBERT$_{base}$ (0.09B) architecture shows competitive results, for ASR downstream task, compared to the w2v-bert-51 (0.6B) pre-trained model proposed in the FLEURS benchmark, while being more efficient by using 7x less data and 6x less parameters. Furthermore, in the context of a LID downstream task, our approach outperforms FLEURS baselines accuracy by over 22\%.
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;</title><link>https://arxiv.org/abs/2404.00027</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#65306;&#25506;&#35752;&#25152;&#26377;&#26435;&#24863;&#21644;&#25512;&#29702;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00027
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#24605;&#24819;&#12289;&#26102;&#38388;&#21644;&#36129;&#29486;&#30340;&#25237;&#20837;&#65292;&#23548;&#33268;&#23545;&#20135;&#20986;&#29289;&#30340;&#20381;&#24651;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20889;&#20316;&#21161;&#25163;&#24341;&#20837;&#20102;&#19968;&#31181;&#24515;&#29702;&#22256;&#22659;&#65292;&#22240;&#20026;&#19968;&#20123;&#20869;&#23481;&#24182;&#38750;&#30452;&#25509;&#25105;&#20204;&#30340;&#21019;&#20316;&#12290;&#25105;&#20204;&#24448;&#24448;&#26356;&#20542;&#21521;&#20110;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#20013;&#26356;&#22810;&#22320;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23613;&#31649;&#23427;&#20204;&#23545;&#25152;&#26377;&#20219;&#21153;&#37117;&#26159;&#24179;&#31561;&#30340;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25105;&#20204;&#21487;&#33021;&#19981;&#20250;&#23436;&#20840;&#22768;&#31216;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#25317;&#26377;&#25152;&#26377;&#26435;&#65292;&#20294;&#21364;&#33258;&#30001;&#22320;&#22768;&#31216;&#20316;&#32773;&#36523;&#20221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31616;&#30701;&#35843;&#26597;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20102;&#35299;&#28508;&#22312;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20154;&#26426;&#20132;&#20114;&#22312;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#24182;&#25913;&#36827;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#26631;&#35760;&#24773;&#32490;&#24378;&#24230;&#24314;&#27169;&#20013;&#30340;&#26368;&#20339;-&#26368;&#24046;&#26631;&#24230;&#27880;&#37322;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.17612</link><description>&lt;p&gt;
"&#24744;&#26159;&#19968;&#21517;&#19987;&#23478;&#27880;&#37322;&#32773;": &#33258;&#21160;&#21270;&#24773;&#32490;&#24378;&#24230;&#24314;&#27169;&#30340;&#26368;&#20339;-&#26368;&#24046;&#26631;&#24230;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
"You are an expert annotator": Automatic Best-Worst-Scaling Annotations for Emotion Intensity Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17612
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26631;&#35760;&#24773;&#32490;&#24378;&#24230;&#24314;&#27169;&#20013;&#30340;&#26368;&#20339;-&#26368;&#24046;&#26631;&#24230;&#27880;&#37322;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#35821;&#26009;&#24211;&#26500;&#25104;&#20102;&#20026;&#26032;&#20219;&#21153;&#25110;&#39046;&#22495;&#21019;&#24314;&#27169;&#22411;&#30340;&#29942;&#39048;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#35821;&#26009;&#24211;&#26631;&#35760;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#20998;&#31867;&#26631;&#35760;&#65292;&#32531;&#35299;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;NLP&#20219;&#21153;&#65288;&#22914;&#24773;&#32490;&#24378;&#24230;&#39044;&#27979;&#65289;&#38656;&#35201;&#25991;&#26412;&#22238;&#24402;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#20851;&#20110;&#36830;&#32493;&#26631;&#31614;&#20998;&#37197;&#33258;&#21160;&#21270;&#26631;&#35760;&#30340;&#24037;&#20316;&#12290;&#22238;&#24402;&#34987;&#35748;&#20026;&#27604;&#20998;&#31867;&#26356;&#20855;&#25361;&#25112;&#24615;&#65306;&#24403;&#20154;&#31867;&#34987;&#35201;&#27714;&#20174;&#35780;&#20998;&#23610;&#24230;&#20013;&#36873;&#25321;&#25968;&#20540;&#26102;&#34920;&#29616;&#26356;&#24046;&#65292;&#36825;&#23548;&#33268;&#20102;&#27604;&#36739;&#27880;&#37322;&#26041;&#27861;&#65292;&#21253;&#25324;&#26368;&#20339;-&#26368;&#24046;&#26631;&#24230;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#27880;&#26041;&#27861;&#26159;&#21542;&#26174;&#31034;&#31867;&#20284;&#30340;&#27169;&#24335;&#65292;&#21363;&#23427;&#20204;&#22312;&#35780;&#20998;&#26631;&#24230;&#27880;&#37322;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#27604;&#22312;&#27604;&#36739;&#26631;&#24230;&#27880;&#37322;&#20219;&#21153;&#19978;&#26356;&#24046;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#33258;&#21160;&#21270;&#24773;&#32490;&#24378;&#24230;&#39044;&#27979;&#24182;&#27604;&#36739;&#30452;&#25509;&#35780;&#20998;&#39044;&#27979;&#12289;&#25104;&#23545;&#27604;&#36739;&#21644;&#26368;&#20339;-&#26368;&#24046;&#26631;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17612v1 Announce Type: new  Abstract: Labeling corpora constitutes a bottleneck to create models for new tasks or domains. Large language models mitigate the issue with automatic corpus labeling methods, particularly for categorical annotations. Some NLP tasks such as emotion intensity prediction, however, require text regression, but there is no work on automating annotations for continuous label assignments. Regression is considered more challenging than classification: The fact that humans perform worse when tasked to choose values from a rating scale lead to comparative annotation methods, including best-worst scaling. This raises the question if large language model-based annotation methods show similar patterns, namely that they perform worse on rating scale annotation tasks than on comparative annotation tasks. To study this, we automate emotion intensity predictions and compare direct rating scale predictions, pairwise comparisons and best-worst scaling. We find that
&lt;/p&gt;</description></item><item><title>VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16973</link><description>&lt;p&gt;
VoiceCraft&#65306;&#37326;&#22806;&#38646;-shot&#35821;&#38899;&#32534;&#36753;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16973
&lt;/p&gt;
&lt;p&gt;
VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;VoiceCraft&#65292;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#22768;&#20070;&#12289;&#20114;&#32852;&#32593;&#35270;&#39057;&#21644;&#25773;&#23458;&#19978;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;VoiceCraft&#37319;&#29992;Transformer&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#35760;&#37325;&#25490;&#36807;&#31243;&#65292;&#32467;&#21512;&#20102;&#22240;&#26524;&#25513;&#30721;&#21644;&#24310;&#36831;&#22534;&#21472;&#65292;&#20197;&#23454;&#29616;&#22312;&#29616;&#26377;&#24207;&#21015;&#20869;&#30340;&#29983;&#25104;&#12290;&#22312;&#35821;&#38899;&#32534;&#36753;&#20219;&#21153;&#19978;&#65292;VoiceCraft&#29983;&#25104;&#30340;&#32534;&#36753;&#35821;&#38899;&#22312;&#33258;&#28982;&#24230;&#26041;&#38754;&#20960;&#20046;&#19982;&#26410;&#32534;&#36753;&#30340;&#24405;&#38899;&#38590;&#20197;&#21306;&#20998;&#65292;&#32463;&#20154;&#31867;&#35780;&#20272;&#65307;&#23545;&#20110;&#38646;-shot TTS&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21253;&#25324;VALLE&#21644;&#27969;&#34892;&#30340;&#21830;&#19994;&#27169;&#22411;XTTS-v2&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#22810;&#26679;&#21475;&#38899;&#12289;&#35821;&#38899;&#39118;&#26684;&#12289;&#24405;&#21046;&#26465;&#20214;&#12289;&#32972;&#26223;&#22122;&#38899;&#21644;&#38899;&#20048;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#30495;&#23454;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#22987;&#32456;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16973v1 Announce Type: cross  Abstract: We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models a
&lt;/p&gt;</description></item><item><title>CHisIEC&#26159;&#19968;&#20221;&#26088;&#22312;&#21152;&#36895;&#21476;&#20195;&#21382;&#21490;&#25991;&#21270;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;13&#20010;&#26397;&#20195;&#12289;&#36328;&#36234;1830&#24180;&#30340;&#25968;&#25454;&#65292;&#20855;&#26377;&#20016;&#23500;&#30340;&#26102;&#38388;&#36328;&#24230;&#21644;&#25991;&#26412;&#24322;&#36136;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15088</link><description>&lt;p&gt;
CHisIEC&#65306;&#19968;&#20221;&#29992;&#20110;&#21476;&#20195;&#20013;&#22269;&#21382;&#21490;&#20449;&#24687;&#25552;&#21462;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CHisIEC: An Information Extraction Corpus for Ancient Chinese History
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15088
&lt;/p&gt;
&lt;p&gt;
CHisIEC&#26159;&#19968;&#20221;&#26088;&#22312;&#21152;&#36895;&#21476;&#20195;&#21382;&#21490;&#25991;&#21270;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;13&#20010;&#26397;&#20195;&#12289;&#36328;&#36234;1830&#24180;&#30340;&#25968;&#25454;&#65292;&#20855;&#26377;&#20016;&#23500;&#30340;&#26102;&#38388;&#36328;&#24230;&#21644;&#25991;&#26412;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15088v1 &#20844;&#21578;&#31867;&#22411;: &#26032; &#25277;&#35937;: &#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25968;&#23383;&#20154;&#25991;&#23398;&#65288;DH&#65289;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26159;&#25512;&#21160;&#21382;&#21490;&#21644;&#25991;&#21270;&#36951;&#20135;&#25991;&#26412;&#32467;&#26500;&#20998;&#26512;&#30340;&#22522;&#30707;&#12290;&#36825;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#39046;&#22495;&#23588;&#20026;&#26126;&#26174;&#12290;&#20026;&#20102;&#21152;&#36895;&#21476;&#20195;&#21382;&#21490;&#25991;&#21270;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20013;&#22269;&#21382;&#21490;&#20449;&#24687;&#25552;&#21462;&#35821;&#26009;&#24211;&#8221;&#65288;CHisIEC&#65289;&#12290;CHisIEC&#26159;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;NER&#21644;RE&#20219;&#21153;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#36164;&#28304;&#12290;&#28085;&#30422;&#20102;&#20174;13&#20010;&#26397;&#20195;&#12289;&#36328;&#36234;1830&#24180;&#30340;&#25968;&#25454;&#30340;&#21331;&#36234;&#21382;&#21490;&#26102;&#38388;&#36724;&#65292;CHisIEC&#20307;&#29616;&#20102;&#20013;&#22269;&#21382;&#21490;&#25991;&#26723;&#20013;&#23384;&#22312;&#30340;&#24191;&#27867;&#26102;&#38388;&#33539;&#22260;&#21644;&#25991;&#26412;&#24322;&#36136;&#24615;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22235;&#31181;&#19981;&#21516;&#30340;&#23454;&#20307;&#31867;&#22411;&#21644;&#21313;&#20108;&#31181;&#20851;&#31995;&#31867;&#22411;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15088v1 Announce Type: new  Abstract: Natural Language Processing (NLP) plays a pivotal role in the realm of Digital Humanities (DH) and serves as the cornerstone for advancing the structural analysis of historical and cultural heritage texts. This is particularly true for the domains of named entity recognition (NER) and relation extraction (RE). In our commitment to expediting ancient history and culture, we present the ``Chinese Historical Information Extraction Corpus''(CHisIEC). CHisIEC is a meticulously curated dataset designed to develop and evaluate NER and RE tasks, offering a resource to facilitate research in the field. Spanning a remarkable historical timeline encompassing data from 13 dynasties spanning over 1830 years, CHisIEC epitomizes the extensive temporal range and text heterogeneity inherent in Chinese historical documents. The dataset encompasses four distinct entity types and twelve relation types, resulting in a meticulously labeled dataset comprising 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#23558;&#31639;&#27861;&#24615;&#32034;&#36180;&#30340;&#27010;&#24565;&#24310;&#20280;&#21040;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#21160;&#24577;&#35774;&#32622;&#27602;&#24615;&#36807;&#28388;&#38408;&#20540;&#30340;&#26032;&#26426;&#21046;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#23454;&#29616;&#20182;&#20204;&#26399;&#26395;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#22686;&#21152;&#20182;&#20204;&#30340;&#20195;&#29702;&#26435;&#12290;</title><link>https://arxiv.org/abs/2403.14467</link><description>&lt;p&gt;
&#37325;&#26032;&#32034;&#21462;&#30340;&#26435;&#21033;&#65306;&#19982;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Recourse for reclamation: Chatting with generative language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14467
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23558;&#31639;&#27861;&#24615;&#32034;&#36180;&#30340;&#27010;&#24565;&#24310;&#20280;&#21040;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#21160;&#24577;&#35774;&#32622;&#27602;&#24615;&#36807;&#28388;&#38408;&#20540;&#30340;&#26032;&#26426;&#21046;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#23454;&#29616;&#20182;&#20204;&#26399;&#26395;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#22686;&#21152;&#20182;&#20204;&#30340;&#20195;&#29702;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#27602;&#24615;&#35780;&#20998;&#26469;&#35843;&#33410;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#22312;&#23458;&#25143;&#26381;&#21153;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20869;&#23481;&#29983;&#25104;&#31561;&#22330;&#26223;&#20013;&#12290;&#28982;&#32780;&#65292;&#27602;&#24615;&#35780;&#20998;&#21487;&#33021;&#20351;&#30456;&#20851;&#20449;&#24687;&#26080;&#27861;&#35775;&#38382;&#65292;&#20351;&#25991;&#21270;&#35268;&#33539;&#20725;&#21270;&#25110;&#8220;&#20215;&#20540;&#38145;&#23450;&#8221;&#65292;&#38459;&#30861;&#35821;&#35328;&#37325;&#26032;&#32034;&#21462;&#36807;&#31243;&#65292;&#29305;&#21035;&#26159;&#23545;&#36793;&#32536;&#21270;&#32676;&#20307;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#31639;&#27861;&#24615;&#32034;&#36180;&#30340;&#27010;&#24565;&#24310;&#20280;&#21040;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65306;&#25105;&#20204;&#20026;&#29992;&#25143;&#25552;&#20379;&#19968;&#31181;&#26032;&#39062;&#26426;&#21046;&#65292;&#36890;&#36807;&#21160;&#24577;&#35774;&#32622;&#27602;&#24615;&#36807;&#28388;&#30340;&#38408;&#20540;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#23454;&#29616;&#20182;&#20204;&#25152;&#26399;&#26395;&#30340;&#39044;&#27979;&#12290;&#29992;&#25143;&#22240;&#27492;&#30456;&#23545;&#20110;&#19982;&#22522;&#32447;&#31995;&#32479;&#30340;&#20132;&#20114;&#65292;&#21487;&#20197;&#34892;&#20351;&#26356;&#22810;&#30340;&#20195;&#29702;&#26435;&#12290;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;($n=30$)&#25903;&#25345;&#25105;&#20204;&#25552;&#20986;&#30340;&#32034;&#36180;&#26426;&#21046;&#30340;&#28508;&#21147;&#65292;&#34920;&#26126;&#19982;&#22266;&#23450;&#38408;&#20540;&#27602;&#24615;&#36807;&#28388;&#27169;&#22411;&#36755;&#20986;&#30456;&#27604;&#65292;&#22312;&#21487;&#29992;&#24615;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#24212;&#35813;&#25506;&#32034;&#27602;&#24615;&#35780;&#20998;&#19982;&#35821;&#35328;&#37325;&#26032;&#32034;&#21462;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14467v1 Announce Type: cross  Abstract: Researchers and developers increasingly rely on toxicity scoring to moderate generative language model outputs, in settings such as customer service, information retrieval, and content generation. However, toxicity scoring may render pertinent information inaccessible, rigidify or "value-lock" cultural norms, and prevent language reclamation processes, particularly for marginalized people. In this work, we extend the concept of algorithmic recourse to generative language models: we provide users a novel mechanism to achieve their desired prediction by dynamically setting thresholds for toxicity filtering. Users thereby exercise increased agency relative to interactions with the baseline system. A pilot study ($n = 30$) supports the potential of our proposed recourse mechanism, indicating improvements in usability compared to fixed-threshold toxicity-filtering of model outputs. Future work should explore the intersection of toxicity sco
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14236</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#26356;&#26032;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;ROME&#21644;MEMIT&#20316;&#20026;&#20027;&#35201;&#30340;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#33073;&#39062;&#32780;&#20986;&#12290;&#32780;MEMIT&#21487;&#20197;&#25209;&#37327;&#32534;&#36753;&#35760;&#24518;&#65292;ROME&#21017;&#19968;&#27425;&#21482;&#33021;&#25913;&#21464;&#19968;&#20010;&#20107;&#23454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;ROME&#21644;MEMIT&#32435;&#20837;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20248;&#21270;&#21516;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20445;&#23384;-&#35760;&#24518;&#8221;&#30446;&#26631;&#12290;&#35813;&#30446;&#26631;&#26088;&#22312;&#22312;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#26576;&#20123;&#36873;&#23450;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ROME&#20351;&#29992;&#31561;&#24335;&#32422;&#26463;&#20248;&#21270;&#27492;&#30446;&#26631;&#65292;&#32780;MEMIT&#37319;&#29992;&#26356;&#28789;&#27963;&#30340;&#26368;&#23567;&#20108;&#20056;&#32422;&#26463;&#12290;&#38500;&#20102;&#25209;&#37327;&#32534;&#36753;&#22806;&#65292;MEMIT&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#38754;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32534;&#36753;&#30340;&#20998;&#24067;&#20174;&#22810;&#20010;&#23618;&#38754;&#20998;&#24320;&#65292;&#21306;&#21035;&#20110;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14236v1 Announce Type: cross  Abstract: Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objectiv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Debatrix&#31995;&#32479;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#22810;&#36718;&#36777;&#35770;&#30340;&#20998;&#26512;&#21644;&#35780;&#20272;&#65292;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#30452;&#25509;&#20351;&#29992;LLMs&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#22330;&#36777;&#35770;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.08010</link><description>&lt;p&gt;
Debatrix:&#22522;&#20110;LLM&#30340;&#22810;&#32500;&#36777;&#35770;&#35780;&#21028;&#31995;&#32479;&#19982;&#36845;&#20195;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08010
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Debatrix&#31995;&#32479;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#22810;&#36718;&#36777;&#35770;&#30340;&#20998;&#26512;&#21644;&#35780;&#20272;&#65292;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#30452;&#25509;&#20351;&#29992;LLMs&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#22330;&#36777;&#35770;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#26500;&#24314;&#19968;&#20010;&#33258;&#21160;&#21270;&#36777;&#35770;&#35780;&#21028;&#31995;&#32479;&#26469;&#35780;&#20272;&#19968;&#22330;&#24191;&#27867;&#12289;&#20805;&#28385;&#27963;&#21147;&#30340;&#22810;&#36718;&#36777;&#35770;&#65311;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35780;&#21028;&#36777;&#35770;&#28041;&#21450;&#22788;&#29702;&#20887;&#38271;&#25991;&#26412;&#12289;&#22797;&#26434;&#30340;&#35770;&#28857;&#20851;&#31995;&#21644;&#22810;&#32500;&#24230;&#35780;&#20272;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#30701;&#23545;&#35805;&#65292;&#24456;&#23569;&#28041;&#21450;&#23545;&#25972;&#22330;&#36777;&#35770;&#30340;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Debatrix&#65292;&#20351;&#24471;&#22810;&#36718;&#36777;&#35770;&#30340;&#20998;&#26512;&#21644;&#35780;&#20272;&#26356;&#31526;&#21512;&#22823;&#22810;&#25968;&#20154;&#30340;&#20559;&#22909;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Debatrix&#20855;&#26377;&#22402;&#30452;&#30340;&#12289;&#36845;&#20195;&#24335;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#27700;&#24179;&#30340;&#12289;&#22810;&#32500;&#24230;&#30340;&#35780;&#20272;&#21327;&#20316;&#12290;&#20026;&#20102;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#36777;&#35770;&#22330;&#26223;&#20445;&#25345;&#19968;&#33268;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PanelBench&#22522;&#20934;&#65292;&#23558;&#25105;&#20204;&#31995;&#32479;&#30340;&#24615;&#33021;&#19982;&#23454;&#38469;&#36777;&#35770;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#30452;&#25509;&#20351;&#29992;LLMs&#36827;&#34892;&#36777;&#35770;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#22686;&#24378;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08010v1 Announce Type: new  Abstract: How can we construct an automated debate judge to evaluate an extensive, vibrant, multi-turn debate? This task is challenging, as judging a debate involves grappling with lengthy texts, intricate argument relationships, and multi-dimensional assessments. At the same time, current research mainly focuses on short dialogues, rarely touching upon the evaluation of an entire debate. In this paper, by leveraging Large Language Models (LLMs), we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences. Specifically, Debatrix features a vertical, iterative chronological analysis and a horizontal, multi-dimensional evaluation collaboration. To align with real-world debate scenarios, we introduced the PanelBench benchmark, comparing our system's performance to actual debate outcomes. The findings indicate a notable enhancement over directly using LLMs for debate evaluation. Source code
&lt;/p&gt;</description></item><item><title>VidProM&#26159;&#19968;&#20010;&#21253;&#21547;167&#19975;&#20010;&#29420;&#29305;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#24102;&#26469;&#20102;&#26032;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25581;&#31034;&#20102;&#30495;&#23454;&#29992;&#25143;&#25552;&#31034;&#23545;&#35270;&#39057;&#29983;&#25104;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06098</link><description>&lt;p&gt;
VidProM&#65306;&#19968;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;&#30495;&#23454;&#21363;&#26102;&#22270;&#24211;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06098
&lt;/p&gt;
&lt;p&gt;
VidProM&#26159;&#19968;&#20010;&#21253;&#21547;167&#19975;&#20010;&#29420;&#29305;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#24102;&#26469;&#20102;&#26032;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25581;&#31034;&#20102;&#30495;&#23454;&#29992;&#25143;&#25552;&#31034;&#23545;&#35270;&#39057;&#29983;&#25104;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sora&#30340;&#21040;&#26469;&#26631;&#24535;&#30528;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#24102;&#26469;&#20102;&#35270;&#39057;&#29983;&#25104;&#21644;&#28508;&#22312;&#24212;&#29992;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;Sora&#20197;&#21450;&#20854;&#20182;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#39640;&#24230;&#20381;&#36182;&#25552;&#31034;&#65292;&#20294;&#30446;&#21069;&#23578;&#27809;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#21253;&#21547;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;VidProM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30001;167&#19975;&#20010;&#26469;&#33258;&#30495;&#23454;&#29992;&#25143;&#30340;&#29420;&#29305;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#32452;&#25104;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#30001;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;669&#19975;&#20010;&#35270;&#39057;&#20197;&#21450;&#19968;&#20123;&#30456;&#20851;&#25968;&#25454;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#36825;&#19968;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#31574;&#23637;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;VidProM&#19982;DiffusionDB&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#21518;&#32773;&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#25552;&#31034;&#22270;&#24211;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#26032;&#25552;&#31034;&#25968;&#25454;&#38598;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06098v1 Announce Type: cross  Abstract: The arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, as well as other text-to-video diffusion models, highly relies on the prompts, and there is no publicly available dataset featuring a study of text-to-video prompts. In this paper, we introduce VidProM, the first large-scale dataset comprising 1.67 million unique text-to-video prompts from real users. Additionally, the dataset includes 6.69 million videos generated by four state-of-the-art diffusion models and some related data. We initially demonstrate the curation of this large-scale dataset, which is a time-consuming and costly process. Subsequently, we show how the proposed VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation. Based on the analysis of these prompts, we identify the necessity for a new prompt dataset specificall
&lt;/p&gt;</description></item><item><title>StructLM&#31995;&#21015;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#36229;&#36234;&#20102;&#22823;&#22810;&#25968;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#22312;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2402.16671</link><description>&lt;p&gt;
StructLM: &#26397;&#21521;&#26500;&#24314;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#30340;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
StructLM: Towards Building Generalist Models for Structured Knowledge Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16671
&lt;/p&gt;
&lt;p&gt;
StructLM&#31995;&#21015;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#36229;&#36234;&#20102;&#22823;&#22810;&#25968;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#22312;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65292;&#22914;&#34920;&#26684;&#12289;&#22270;&#24418;&#21644;&#25968;&#25454;&#24211;&#65292;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#30693;&#35782;&#28304;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#32431;&#25991;&#26412;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#35299;&#37322;&#21644;&#21033;&#29992;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#26174;&#30528;&#19981;&#36275;&#65292;&#20363;&#22914;&#65292;ChatGPT&#24179;&#22343;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;(SoTA)35%&#12290;&#20026;&#22686;&#24378;LLM&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#65288;SKG&#65289;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;110&#19975;&#20010;&#31034;&#20363;&#30340;&#20840;&#38754;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;Code-LLaMA&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;StructLM&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;7B&#21040;34B&#12290;&#25105;&#20204;&#30340;StructLM&#31995;&#21015;&#22312;18&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#20013;&#26377;14&#20010;&#36229;&#36234;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;7&#20010;SKG&#20219;&#21153;&#19978;&#30830;&#31435;&#20102;&#26032;&#30340;SoTA&#25104;&#23601;&#12290;&#27492;&#22806;&#65292;StructLM&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16671v1 Announce Type: new  Abstract: Structured data sources, such as tables, graphs, and databases, are ubiquitous knowledge sources. Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%. To augment the Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a comprehensive instruction tuning dataset comprising 1.1 million examples. Utilizing this dataset, we train a series of models, referred to as StructLM, based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our StructLM series surpasses task-specific models on 14 out of 18 evaluated datasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore, StructLM demonstrates exceptional generalizat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#25903;&#25345;&#19979;&#65292;&#33521;&#35821;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#30340;&#20934;&#30830;&#12289;&#26377;&#29992;&#22238;&#24212;&#65292;&#20294;&#23384;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.14778</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Zero-shot cross-lingual transfer in instruction tuning of large language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#25903;&#25345;&#19979;&#65292;&#33521;&#35821;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#30340;&#20934;&#30830;&#12289;&#26377;&#29992;&#22238;&#24212;&#65292;&#20294;&#23384;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#24494;&#35843;&#65288;IT&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25945;&#23548;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36981;&#24490;&#20219;&#24847;&#25351;&#20196;&#65292;&#20294;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22312;IT&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#24403;LLM&#22312;&#20165;&#33521;&#35821;&#25968;&#25454;&#19978;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#28982;&#21518;&#22312;&#20854;&#20182;&#35821;&#35328;&#29992;&#25143;&#25552;&#31034;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27169;&#22411;&#37197;&#32622;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#35780;&#20272;&#31574;&#30053;&#29992;&#20110;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#27169;&#22411;&#35757;&#32451;&#30340;&#25152;&#26377;&#38454;&#27573;&#37117;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#22312;IT&#20013;&#20063;&#20250;&#25104;&#21151;&#21457;&#29983;&#65292;&#20294;&#21482;&#26377;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#32771;&#34385;&#21040;&#22810;&#35821;&#35328;&#24615;&#20197;&#21450;&#26377;&#36275;&#22815;&#22823;&#30340;IT&#25968;&#25454;&#26102;&#25165;&#20250;&#21457;&#29983;&#12290;&#32463;&#36807;&#33521;&#35821;&#35757;&#32451;&#30340;LLMs&#33021;&#22815;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#29983;&#25104;&#20934;&#30830;&#12289;&#20840;&#38754;&#19988;&#26377;&#24110;&#21161;&#30340;&#22238;&#24212;&#65292;&#20294;&#32570;&#20047;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20598;&#23572;&#21487;&#33021;&#23384;&#22312;&#27969;&#30021;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14778v1 Announce Type: cross  Abstract: Instruction tuning (IT) is widely used to teach pretrained large language models (LLMs) to follow arbitrary instructions, but is under-studied in multilingual settings. In this work, we conduct a systematic study of zero-shot cross-lingual transfer in IT, when an LLM is instruction-tuned on English-only data and then tested on user prompts in other languages. We investigate the influence of model configuration choices and devise a multi-facet evaluation strategy for multilingual instruction following. We find that cross-lingual transfer does happen successfully in IT even if all stages of model training are English-centric, but only if multiliguality is taken into account in hyperparameter tuning and with large enough IT data. English-trained LLMs are capable of generating correct-language, comprehensive and helpful responses in the other languages, but suffer from low factuality and may occasionally have fluency errors.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#35774;&#32622;&#65306;&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;&#65292;&#36890;&#36807;&#30452;&#25509;&#32534;&#36753;&#26032;&#20107;&#20214;&#21040;LLMs&#20013;&#65292;&#22312;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;&#19978;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#19977;&#20803;&#32452;&#32423;&#21035;&#32534;&#36753;&#12290;</title><link>https://arxiv.org/abs/2402.13093</link><description>&lt;p&gt;
&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Event-level Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13093
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#35774;&#32622;&#65306;&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;&#65292;&#36890;&#36807;&#30452;&#25509;&#32534;&#36753;&#26032;&#20107;&#20214;&#21040;LLMs&#20013;&#65292;&#22312;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;&#19978;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#19977;&#20803;&#32452;&#32423;&#21035;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#65292;&#20197;&#38450;&#27490;&#23427;&#20204;&#36807;&#26102;&#12290;&#29616;&#26377;&#30740;&#31350;&#22312;&#20107;&#23454;&#30693;&#35782;&#19977;&#20803;&#32452;&#30340;&#32423;&#21035;&#19978;&#32534;&#36753;LLMs&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#28982;&#30693;&#35782;&#26356;&#26032;&#26469;&#33258;&#26032;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#26356;&#25913;&#20107;&#23454;&#19977;&#20803;&#32452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#35774;&#32622;&#65306;&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;&#65292;&#30452;&#25509;&#23558;&#26032;&#20107;&#20214;&#32534;&#36753;&#21040;LLMs&#20013;&#65292;&#24182;&#22312;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;&#19978;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#19977;&#20803;&#32452;&#32423;&#21035;&#32534;&#36753;&#12290;(1)&#25928;&#29575;&#12290;&#21333;&#20010;&#20107;&#20214;&#32534;&#36753;&#20250;&#23548;&#33268;&#22810;&#20010;&#25512;&#26029;&#30693;&#35782;&#19977;&#20803;&#32452;&#30340;&#26356;&#26032;&#12290;(2)&#23436;&#25972;&#24615;&#12290;&#38500;&#20102;&#26356;&#26032;&#20107;&#23454;&#30693;&#35782;&#22806;&#65292;&#20107;&#20214;&#32423;&#21035;&#30340;&#32534;&#36753;&#36824;&#38656;&#35201;&#32771;&#34385;&#20107;&#20214;&#24433;&#21709;&#65292;&#26356;&#26032;LLMs&#20851;&#20110;&#26410;&#26469;&#36235;&#21183;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#32423;&#21035;&#32534;&#36753;&#22522;&#20934;ELKEN&#65292;&#21253;&#25324;1,515&#20010;&#20107;&#20214;&#32534;&#36753;&#65292;6,449&#20010;&#20851;&#20110;&#20107;&#23454;&#30693;&#35782;&#30340;&#38382;&#39064;&#21644;10,150&#20010;&#20851;&#20110;&#26410;&#26469;&#21457;&#23637;&#36235;&#21183;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13093v1 Announce Type: cross  Abstract: Knowledge editing aims at updating knowledge of large language models (LLMs) to prevent them from becoming outdated. Existing work edits LLMs at the level of factual knowledge triplets. However, natural knowledge updates in the real world come from the occurrences of new events rather than direct changes in factual triplets. In this paper, we propose a new task setting: event-level knowledge editing, which directly edits new events into LLMs and improves over conventional triplet-level editing on (1) Efficiency. A single event edit leads to updates in multiple entailed knowledge triplets. (2) Completeness. Beyond updating factual knowledge, event-level editing also requires considering the event influences and updating LLMs' knowledge about future trends. We construct a high-quality event-level editing benchmark ELKEN, consisting of 1,515 event edits, 6,449 questions about factual knowledge, and 10,150 questions about future tendencies
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24494;&#35843;&#23398;&#20064;&#29575;&#21487;&#20197;&#32531;&#35299;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#20013;&#20197;&#38169;&#35823;&#35821;&#35328;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#26159;&#24456;&#24378;&#22823;&#30340;&#22522;&#32447;&#65292;mBART&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#31867;&#20284;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;mT5&#12290;</title><link>https://arxiv.org/abs/2402.12279</link><description>&lt;p&gt;
&#26377;&#25928;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12279
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#23398;&#20064;&#29575;&#21487;&#20197;&#32531;&#35299;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#20013;&#20197;&#38169;&#35823;&#35821;&#35328;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#26159;&#24456;&#24378;&#22823;&#30340;&#22522;&#32447;&#65292;mBART&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#31867;&#20284;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;mT5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#24847;&#21619;&#30528;&#22312;&#19968;&#20010;&#35821;&#35328;&#19978;&#24494;&#35843;&#22810;&#35821;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20110;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#36827;&#34892;&#27492;&#20219;&#21153;&#30340;&#39044;&#27979;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#25351;&#20986;&#19968;&#20010;&#32463;&#24120;&#20986;&#29616;&#30340;&#38382;&#39064;&#26159;&#20197;&#38169;&#35823;&#30340;&#35821;&#35328;&#29983;&#25104;&#65292;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#24120;&#20351;&#29992;mT5&#20316;&#20026;&#20027;&#24178;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#32479;&#19968;&#30340;&#35774;&#32622;&#20013;&#27604;&#36739;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#36824;&#21253;&#25324;&#26367;&#20195;&#24615;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#21363;mBART&#21644;NLLB-200&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#24494;&#35843;&#25152;&#20351;&#29992;&#30340;&#23398;&#20064;&#29575;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#22823;&#22823;&#32531;&#35299;&#20197;&#38169;&#35823;&#35821;&#35328;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#32454;&#33268;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#65292;&#23545;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#24494;&#35843;&#20316;&#20026;&#38750;&#24120;&#24378;&#22823;&#30340;&#22522;&#32447;&#65292;&#26367;&#20195;&#26041;&#27861;&#21482;&#24102;&#26469;&#24494;&#23567;&#30340;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;mBART&#19982;&#21516;&#31561;&#22823;&#23567;&#30340;mT5&#34920;&#29616;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12279v1 Announce Type: cross  Abstract: Zero-shot cross-lingual generation implies finetuning of the multilingual pretrained language model on a generation task in one language and then using it to make predictions for this task in other languages. Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model. In this work we compare various approaches proposed from the literature in unified settings, also including alternative backbone models, namely mBART and NLLB-200. We first underline the importance of tuning learning rate used for finetuning, which helps to substantially alleviate the problem of generation in the wrong language. Then, we show that with careful learning rate tuning, the simple full finetuning of the model acts as a very strong baseline and alternative approaches bring only marginal improvements. Finally, we find that mBART performs similarly to mT5 of the same size,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#38750;&#32431;&#35821;&#20041;&#25552;&#31034;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#25361;&#25112;&#12290;&#20116;&#20010;SOTA LLMs&#22312;&#35782;&#21035;ASCII&#33402;&#26415;&#25552;&#31034;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.11753</link><description>&lt;p&gt;
ArtPrompt: &#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#23545;&#40784;LLMs&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11753
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#38750;&#32431;&#35821;&#20041;&#25552;&#31034;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#25361;&#25112;&#12290;&#20116;&#20010;SOTA LLMs&#22312;&#35782;&#21035;ASCII&#33402;&#26415;&#25552;&#31034;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#25216;&#26415;&#65292;&#22914;&#25968;&#25454;&#36807;&#28388;&#21644;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#21152;&#24378;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24050;&#30693;&#30340;&#25216;&#26415;&#20551;&#35774;&#29992;&#20110;&#23545;&#40784;LLMs&#23433;&#20840;&#24615;&#30340;&#35821;&#26009;&#24211;&#20165;&#30001;&#35821;&#20041;&#36827;&#34892;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20551;&#35774;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#19981;&#25104;&#31435;&#65292;&#23548;&#33268;LLMs&#23384;&#22312;&#20005;&#37325;&#28431;&#27934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;Vision-in-Text Challenge&#65288;ViTC&#65289;&#26469;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#19981;&#33021;&#20165;&#36890;&#36807;&#35821;&#20041;&#36827;&#34892;&#35299;&#37322;&#30340;&#25552;&#31034;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20116;&#20010;SOTA LLMs&#65288;GPT-3.5&#12289;GPT-4&#12289;Gemini&#12289;Claude&#21644;Llama2&#65289;&#22312;&#35782;&#21035;&#20197;ASCII&#33402;&#26415;&#24418;&#24335;&#25552;&#20379;&#30340;&#25552;&#31034;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11753v1 Announce Type: cross  Abstract: Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we devel
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#36890;&#36807;&#23558;&#38590;&#39064;&#20998;&#20026;&#22522;&#20110;&#35268;&#21017;&#21644;&#26080;&#35268;&#21017;&#20004;&#31867;&#30340;&#29420;&#29305;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#29616;&#65292;&#24378;&#35843;&#20102;&#22312;&#22797;&#26434;&#38590;&#39064;&#24773;&#22659;&#20013;LLMs&#30340;&#25361;&#25112;&#21644;&#20154;&#31867;&#31867;&#20284;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#25512;&#21160;LLMs&#35299;&#35868;&#33021;&#21147;&#21644;&#36129;&#29486;&#20110;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11291</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#35299;&#20915;&#38590;&#39064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Puzzle Solving using Reasoning of Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#36890;&#36807;&#23558;&#38590;&#39064;&#20998;&#20026;&#22522;&#20110;&#35268;&#21017;&#21644;&#26080;&#35268;&#21017;&#20004;&#31867;&#30340;&#29420;&#29305;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#29616;&#65292;&#24378;&#35843;&#20102;&#22312;&#22797;&#26434;&#38590;&#39064;&#24773;&#22659;&#20013;LLMs&#30340;&#25361;&#25112;&#21644;&#20154;&#31867;&#31867;&#20284;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#25512;&#21160;LLMs&#35299;&#35868;&#33021;&#21147;&#21644;&#36129;&#29486;&#20110;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#20915;&#38590;&#39064;&#20013;&#30340;&#33021;&#21147;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#26631;&#24535;&#30528;&#29702;&#35299;&#23427;&#20204;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#26412;&#35843;&#26597;&#21033;&#29992;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#23558;&#38590;&#39064;&#20998;&#20026;&#22522;&#20110;&#35268;&#21017;&#21644;&#26080;&#35268;&#21017;&#20004;&#31867;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#35780;&#20272;LLMs&#65292;&#21253;&#25324;&#25552;&#31034;&#25216;&#26415;&#12289;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#25209;&#21028;&#24615;&#23457;&#26597;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#22797;&#26434;&#38590;&#39064;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#35782;&#21035;&#20986;&#22797;&#26434;&#38590;&#39064;&#24773;&#22659;&#20013;&#30340;&#26174;&#33879;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;LLMs&#33021;&#21147;&#21450;&#31867;&#20154;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#39640;&#32423;&#36923;&#36753;&#25512;&#26029;&#30340;&#24773;&#20917;&#19979;&#12290;&#35843;&#26597;&#24378;&#35843;&#20102;&#38656;&#35201;&#26032;&#39062;&#31574;&#30053;&#21644;&#26356;&#20016;&#23500;&#25968;&#25454;&#38598;&#26469;&#25552;&#21319;LLMs&#30340;&#35299;&#35868;&#33021;&#21147;&#24182;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11291v1 Announce Type: cross  Abstract: Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in artificial intelligence, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy -- dividing puzzles into rule-based and rule-less categories -- to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs' performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs' puzzle-solving proficiency and contribute to AI's
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#35777;&#25454;&#21644;&#39118;&#26684;&#30340;&#21453;&#39539;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;Reddit ChangeMyView&#25968;&#25454;&#38598;&#20013;&#30340;&#24086;&#23376;&#65292;&#24182;&#21487;&#29992;&#20110;&#35770;&#35777;&#30340;&#25913;&#36827;&#21644;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5 turbo&#27169;&#22411;&#22312;&#35770;&#35777;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#39118;&#26684;&#34701;&#21512;&#33021;&#21147;&#12290;&#20114;&#24800;&#24335;&#21453;&#39539;&#30340;&#25928;&#26524;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.08498</link><description>&lt;p&gt;
&#23457;&#35745;&#21453;&#28779;&#65306;&#35780;&#20272;&#20855;&#26377;&#35777;&#25454;&#21644;&#39118;&#26684;&#30340;&#20808;&#36827;&#21453;&#39539;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08498
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#35777;&#25454;&#21644;&#39118;&#26684;&#30340;&#21453;&#39539;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;Reddit ChangeMyView&#25968;&#25454;&#38598;&#20013;&#30340;&#24086;&#23376;&#65292;&#24182;&#21487;&#29992;&#20110;&#35770;&#35777;&#30340;&#25913;&#36827;&#21644;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5 turbo&#27169;&#22411;&#22312;&#35770;&#35777;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#39118;&#26684;&#34701;&#21512;&#33021;&#21147;&#12290;&#20114;&#24800;&#24335;&#21453;&#39539;&#30340;&#25928;&#26524;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25511;&#21046;&#24615;&#21453;&#39539;&#30340;&#21512;&#25104;&#65292;&#26088;&#22312;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;&#35770;&#35777;&#30340;&#25913;&#36827;&#12289;&#25366;&#25496;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;Reddit ChangeMyView&#25968;&#25454;&#38598;&#20013;&#30340;&#24086;&#23376;&#30456;&#32467;&#21512;&#30340;&#20016;&#23500;&#30340;&#21453;&#39539;&#65292;&#36825;&#20123;&#21453;&#39539;&#34701;&#20837;&#20102;&#20174;&#39640;&#36136;&#37327;&#26469;&#28304;&#20013;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#29983;&#25104;&#65292;&#35843;&#25972;&#20102;&#35777;&#25454;&#21644;&#35770;&#35777;&#39118;&#26684;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;Counterfire&#35821;&#26009;&#24211;&#21253;&#25324;&#20174;GPT-3.5 turbo&#12289;Koala&#21644;PaLM 2&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30340;&#20004;&#20010;&#24494;&#35843;&#21464;&#20307;&#29983;&#25104;&#30340;&#35770;&#35777;&#65288;N = 32,000&#65289;&#12290;&#27169;&#22411;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#35777;&#25454;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#25913;&#20889;&#33021;&#21147;&#65292;&#23613;&#31649;&#35789;&#27719;&#37325;&#21472;&#26377;&#38480;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#39118;&#26684;&#34701;&#21512;&#65288;&#23545;&#20110;&#8220;&#20114;&#24800;&#8221;&#30340;&#24471;&#20998;&#20026;0.9682&#65289;&#65292;&#26174;&#31034;&#20102;LLM&#34701;&#21512;&#22810;&#26679;&#39118;&#26684;&#30340;&#33021;&#21147;&#12290;&#22312;&#25152;&#26377;&#27169;&#22411;&#20013;&#65292;GPT-3.5 turbo&#22312;&#35770;&#35777;&#36136;&#37327;&#35780;&#20272;&#20013;&#26174;&#31034;&#20986;&#26368;&#39640;&#20998;&#25968;&#65292;&#34920;&#29616;&#20986;&#19968;&#33268;&#20934;&#30830;&#24615;&#65288;&#24471;&#20998; &gt;0.8&#65289;&#12290;&#22312;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#20013;&#65292;&#20114;&#24800;&#24335;&#21453;&#39539;&#35777;&#26126;&#25928;&#26524;&#26368;&#20339;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#35770;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel dataset for the controlled composition of counterarguments designed for further applications in argument refining, mining, and evaluation. Our dataset constitutes enriched counter-arguments to posts in the Reddit ChangeMyView dataset that are integrated with evidence retrieved from high-quality sources and generated based on user preferences, adjusting the critical attributes of evidence and argument style. The resultant Counterfire corpus comprises arguments generated from GPT-3.5 turbo, Koala, and PaLM 2 models and two of their finetuned variants (N = 32,000). Model evaluation indicates strong paraphrasing abilities with evidence, albeit limited word overlap, while demonstrating high style integration (0.9682 for 'reciprocity'), showing the ability of LLM to assimilate diverse styles. Of all models, GPT-3.5 turbo showed the highest scores in argument quality evaluation, showing consistent accuracy (score &gt;0.8). In further analyses, reciprocity-style counterargument
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#29305;&#23450;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;Amharic-LLaMA&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#38463;&#22982;&#21704;&#25289;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#36890;&#36807;&#21019;&#24314;&#38463;&#22982;&#21704;&#25289;&#35821;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08015</link><description>&lt;p&gt;
&#22686;&#24378;Amharic-LLaMA: &#25972;&#21512;&#29305;&#23450;&#20219;&#21153;&#19982;&#29983;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Enhancing Amharic-LLaMA: Integrating Task Specific and Generative Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#29305;&#23450;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;Amharic-LLaMA&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#38463;&#22982;&#21704;&#25289;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#36890;&#36807;&#21019;&#24314;&#38463;&#22982;&#21704;&#25289;&#35821;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#20013;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#22240;&#32570;&#20047;&#36164;&#28304;&#32780;&#34987;&#33853;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#25972;&#21512;&#29305;&#23450;&#20219;&#21153;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;LLaMA-2-Amharic&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#38463;&#22982;&#21704;&#25289;&#35821;&#30340;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#38463;&#22982;&#21704;&#25289;&#35821;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;LLaMA-2-Amharic&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#27969;&#31243;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#35780;&#20272;&#36755;&#20986;&#65292;&#20197;&#20419;&#36827;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#35821;&#35328;&#29305;&#23450;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have received a lot of attention in natural language processing (NLP) research because of their exceptional performance in understanding and generating human languages. However, low-resource languages are left behind due to the unavailability of resources. In this work, we focus on enhancing the LLaMA-2-Amharic model by integrating task-specific and generative datasets to improve language model performance for Amharic. We compile an Amharic instruction fine-tuning dataset and fine-tuned LLaMA-2-Amharic model. The fine-tuned model shows promising results in different NLP tasks. We open-source our dataset creation pipeline, instruction datasets, trained models, and evaluation outputs to promote language-specific studies on these models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#34913;&#37327;LLMs&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#65292;&#20197;&#26368;&#22823;&#21270;&#21508;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.01830</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#21516;&#34892;&#35780;&#23457;&#26041;&#27861;&#65306;&#24320;&#25918;&#29615;&#22659;&#19979;LLMs&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#34913;&#37327;LLMs&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#65292;&#20197;&#26368;&#22823;&#21270;&#21508;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#26041;&#27861;&#36890;&#24120;&#38598;&#20013;&#20110;&#22312;&#19968;&#20123;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#23553;&#38381;&#29615;&#22659;&#21644;&#29305;&#23450;&#39046;&#22495;&#22522;&#20934;&#19978;&#27979;&#35797;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#33258;&#21160;&#34913;&#37327;LLMs&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#24320;&#28304;&#21644;&#38381;&#28304;&#30340;LLMs&#22788;&#20110;&#21516;&#19968;&#29615;&#22659;&#20013;&#65292;&#33021;&#22815;&#22238;&#31572;&#26410;&#26631;&#35760;&#30340;&#38382;&#39064;&#24182;&#20114;&#30456;&#35780;&#20272;&#65292;&#27599;&#20010;LLM&#30340;&#21709;&#24212;&#24471;&#20998;&#30001;&#20854;&#20182;&#21311;&#21517;&#30340;LLMs&#20849;&#21516;&#20915;&#23450;&#12290;&#20026;&#20102;&#33719;&#21462;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#33021;&#21147;&#23618;&#27425;&#32467;&#26500;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#26469;&#35843;&#25972;&#26368;&#32456;&#25490;&#24207;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21463;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#27599;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32972;&#21518;&#30340;&#20851;&#38190;&#20551;&#35774;&#26159;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#27604;&#20302;&#23618;&#27425;&#30340;LLM&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#32780;&#39640;&#23618;&#27425;&#30340;LLM&#20063;&#21487;&#20197;&#36798;&#21040;&#36739;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking. We formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. The key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. Moreover
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TG-LLM&#26694;&#26550;&#65292;&#20197;&#35821;&#35328;&#20026;&#22522;&#30784;&#36827;&#34892;&#26102;&#38388;&#25512;&#29702;&#65292;&#36890;&#36807;&#25945;&#23548;LLM&#23558;&#19978;&#19979;&#25991;&#32763;&#35793;&#25104;&#26102;&#38388;&#22270;&#65292;&#24182;&#20351;&#29992;CoTs&#25351;&#23548;LLM&#36827;&#34892;&#31526;&#21495;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2401.06853</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26102;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Learn Temporal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TG-LLM&#26694;&#26550;&#65292;&#20197;&#35821;&#35328;&#20026;&#22522;&#30784;&#36827;&#34892;&#26102;&#38388;&#25512;&#29702;&#65292;&#36890;&#36807;&#25945;&#23548;LLM&#23558;&#19978;&#19979;&#25991;&#32763;&#35793;&#25104;&#26102;&#38388;&#22270;&#65292;&#24182;&#20351;&#29992;CoTs&#25351;&#23548;LLM&#36827;&#34892;&#31526;&#21495;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24182;&#38750;&#27809;&#26377;&#32570;&#38519;&#21644;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#26102;&#38388;&#25512;&#29702;&#65288;TR&#65289;&#23545;LLMs&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#22810;&#26679;&#30340;&#26102;&#38388;&#34920;&#36798;&#21644;&#22797;&#26434;&#30340;&#19978;&#19979;&#25991;&#32454;&#33410;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TG-LLM&#65292;&#19968;&#20010;&#33268;&#21147;&#20110;&#22522;&#20110;&#35821;&#35328;&#30340;&#26102;&#38388;&#25512;&#29702;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25945;&#23548;LLM&#23558;&#19978;&#19979;&#25991;&#32763;&#35793;&#25104;&#26102;&#38388;&#22270;&#65288;TG&#65289;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#21487;&#25511;&#19988;&#38656;&#35201;&#26368;&#23569;&#30417;&#30563;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#36825;&#20010;&#22270;&#32763;&#35793;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#35777;&#23454;&#65292;&#23398;&#20064;&#22312;&#25105;&#20204;&#25968;&#25454;&#38598;&#19978;&#30340;TG&#25552;&#21462;&#33021;&#21147;&#21487;&#20197;&#36716;&#31227;&#21040;&#20854;&#20182;TR&#20219;&#21153;&#21644;&#22522;&#20934;&#27979;&#35797;&#19978;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;CoTs&#24341;&#23548;LLM&#36890;&#36807;TG&#36827;&#34892;&#31526;&#21495;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06853v2 Announce Type: replace  Abstract: While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal expressions and intricate contextual details. In this paper, we propose TG-LLM, a new framework towards language-based TR. To be specific, we first teach LLM to translate the context into a temporal graph (TG). A synthetic dataset, which is fully controllable and requires minimal supervision, is constructed for fine-tuning on this graph translation task. We confirm in experiments that the capability of TG extraction learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we guide LLM to perform symbolic reasoning over the TG via Chain of Thoughts (CoTs) bootstrappin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;Reddit&#19978;&#30340;&#36947;&#24503;&#21028;&#26029;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31038;&#20132;&#24120;&#35782;&#21644;&#35821;&#35328;&#20449;&#21495;&#23545;&#20110;&#36947;&#24503;&#28779;&#33457;&#30340;&#24433;&#21709;&#65292;&#20026;&#20154;&#31867;&#36947;&#24503;&#21028;&#26029;&#25552;&#20379;&#20102;&#28145;&#20837;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2310.19268</link><description>&lt;p&gt;
Reddit&#19978;&#20851;&#20110;&#21465;&#20107;&#20013;&#30340;&#36947;&#24503;&#21028;&#26029;&#65306;&#36890;&#36807;&#31038;&#20132;&#24120;&#35782;&#21644;&#35821;&#35328;&#20449;&#21495;&#35843;&#26597;&#36947;&#24503;&#28779;&#33457;
&lt;/p&gt;
&lt;p&gt;
Moral Judgments in Narratives on Reddit: Investigating Moral Sparks via Social Commonsense and Linguistic Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19268
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;Reddit&#19978;&#30340;&#36947;&#24503;&#21028;&#26029;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31038;&#20132;&#24120;&#35782;&#21644;&#35821;&#35328;&#20449;&#21495;&#23545;&#20110;&#36947;&#24503;&#28779;&#33457;&#30340;&#24433;&#21709;&#65292;&#20026;&#20154;&#31867;&#36947;&#24503;&#21028;&#26029;&#25552;&#20379;&#20102;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19268v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#36328; &#25991;&#25688;&#65306;&#26426;&#22120;&#20262;&#29702;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#21644;&#20195;&#29702;&#30340;&#36947;&#24503;&#34892;&#20026;&#12290;&#30740;&#31350;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#23398;&#20064;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#23454;&#36341;&#36947;&#24503;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#20154;&#31867;&#36947;&#24503;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65292;&#20197;&#29702;&#35299;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20262;&#29702;&#24773;&#26223;&#21644;&#20154;&#31867;&#36947;&#24503;&#21028;&#26029;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#21517;&#20026;r/AmITheAsshole&#30340;&#28909;&#38376;Reddit&#23376;&#31038;&#21306;&#20013;&#30340;&#24086;&#23376;&#65292;&#20316;&#32773;&#21644;&#35780;&#35770;&#32773;&#22312;&#36825;&#37324;&#20998;&#20139;&#35841;&#24212;&#35813;&#21463;&#21040;&#36131;&#22791;&#30340;&#36947;&#24503;&#21028;&#26029;&#12290;&#25105;&#20204;&#37319;&#29992;&#35745;&#31639;&#25216;&#26415;&#26469;&#30740;&#31350;&#24433;&#21709;&#36947;&#24503;&#21028;&#26029;&#30340;&#28508;&#22312;&#25512;&#29702;&#12290;&#25105;&#20204;&#20851;&#27880;&#21407;&#22987;&#24086;&#23376;&#20013;&#30340;&#33410;&#36873;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#36947;&#24503;&#28779;&#33457;&#65292;&#35780;&#35770;&#32773;&#21253;&#25324;&#36825;&#20123;&#33410;&#36873;&#20197;&#34920;&#26126;&#20182;&#20204;&#21028;&#26029;&#30340;&#21160;&#26426;&#26159;&#20160;&#20040;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#65288;1&#65289;&#28608;&#27963;&#31038;&#20132;&#24120;&#35782;&#30340;&#20107;&#20214;&#21644;&#65288;2&#65289;&#35821;&#35328;&#20449;&#21495;&#22914;&#20309;&#24433;&#21709;&#36947;&#24503;&#28779;&#33457;&#30340;&#20998;&#37197;&#21450;&#20854;&#23376;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19268v2 Announce Type: replace-cross  Abstract: Machine ethics ensures ethical conduct in Artificial Intelligence (AI) models and agents. Examining real-life applications benefit learning practical ethics in many situations, offering valuable data to grasp the complexities of human ethics in diverse contexts. In this paper, we examine social media platforms for understanding real-life ethical scenarios and human moral judgments. We examine posts from a popular Reddit subreddit (i.e., a subcommunity) called r/AmITheAsshole, where authors and commenters share their moral judgments on who is blameworthy. We employ computational techniques to investigate the underlying reasoning influencing moral judgments. We focus on excerpts-which we term moral sparks-from original posts that commenters include to indicate what motivates their judgments. To this end, we examine how (1) events activating social commonsense and (2) linguistic signals affect moral sparks assignment and their sub
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26242;&#20572;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#26631;&#35760;&#21069;&#22788;&#29702;&#26356;&#22810;&#38544;&#34255;&#21521;&#37327;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;</title><link>https://arxiv.org/abs/2310.02226</link><description>&lt;p&gt;
&#35880;&#35328;&#24910;&#34892;&#65306;&#20351;&#29992;&#26242;&#20572;&#26631;&#35760;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Think before you speak: Training Language Models With Pause Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02226
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26242;&#20572;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#26631;&#35760;&#21069;&#22788;&#29702;&#26356;&#22810;&#38544;&#34255;&#21521;&#37327;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#31435;&#21363;&#36830;&#32493;&#29983;&#25104;&#19968;&#31995;&#21015;&#26631;&#35760;&#26469;&#29983;&#25104;&#21709;&#24212;: &#31532;$(K+1)^{th}$&#20010;&#26631;&#35760;&#26159;&#36890;&#36807;&#25805;&#20316;&#27599;&#23618;&#30340;$K$&#20010;&#38544;&#34255;&#21521;&#37327;&#24471;&#21040;&#30340;&#65292;&#27599;&#20010;&#21521;&#37327;&#23545;&#24212;&#19968;&#20010;&#21069;&#38754;&#30340;&#26631;&#35760;&#12290;&#22914;&#26524;&#25105;&#20204;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#31532;$(K+1)^{th}$&#20010;&#26631;&#35760;&#20043;&#21069;&#25805;&#20316;&#26356;&#22810;&#30340;&#38544;&#34255;&#21521;&#37327;&#65292;&#27604;&#22914;&#35828;$K+10$&#20010;&#21602;&#65311;&#25105;&#20204;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#65288;&#21487;&#23398;&#20064;&#30340;&#65289;$\textit{pause}$&#26631;&#35760;&#65292;&#36825;&#19968;&#31995;&#21015;&#26631;&#35760;&#38468;&#21152;&#21040;&#36755;&#20837;&#21069;&#32512;&#19978;&#12290;&#28982;&#21518;&#25105;&#20204;&#24310;&#36831;&#25552;&#21462;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#30452;&#21040;&#26368;&#21518;&#19968;&#20010;&#26242;&#20572;&#26631;&#35760;&#34987;&#30475;&#21040;&#65292;&#20174;&#32780;&#20801;&#35768;&#27169;&#22411;&#22312;&#20570;&#20986;&#31572;&#26696;&#20043;&#21069;&#36827;&#34892;&#39069;&#22806;&#30340;&#35745;&#31639;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#25317;&#26377;1B&#21644;130M&#21442;&#25968;&#30340;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;$\textit{pause-training}$&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#22312;C4&#19978;&#36827;&#34892;&#20102;&#22240;&#26524;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#28085;&#30422;&#25512;&#29702;&#12289;&#38382;&#31572;&#12289;&#26222;&#36941;&#29702;&#35299;&#21644;&#20107;&#23454;&#22238;&#24518;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;infer
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02226v2 Announce Type: replace-cross  Abstract: Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that infer
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35789;&#34955;&#39044;&#27979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#36890;&#34892;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#39640;&#25928;&#21387;&#32553;&#35789;&#27719;&#20449;&#21495;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#36755;&#20837;&#20196;&#29260;&#30340;&#26465;&#27454;&#35206;&#30422;&#12290;</title><link>http://arxiv.org/abs/2401.11248</link><description>&lt;p&gt;
&#25918;&#24323;&#35299;&#30721;&#22120;&#65306;&#20351;&#29992;&#35789;&#34955;&#39044;&#27979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#36890;&#34892;&#26816;&#32034;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval. (arXiv:2401.11248v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35789;&#34955;&#39044;&#27979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#36890;&#34892;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#39640;&#25928;&#21387;&#32553;&#35789;&#27719;&#20449;&#21495;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#36755;&#20837;&#20196;&#29260;&#30340;&#26465;&#27454;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#24050;&#25104;&#20026;&#21021;&#22987;&#21270;&#21644;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#23427;&#36890;&#24120;&#21033;&#29992;&#39069;&#22806;&#30340;Transformer&#35299;&#30721;&#22359;&#25552;&#20379;&#21487;&#25345;&#32493;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#20449;&#24687;&#21387;&#32553;&#21040;&#23494;&#38598;&#34920;&#31034;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39044;&#35757;&#32451;&#25216;&#26415;&#26377;&#25928;&#24615;&#30340;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#12290;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#39069;&#22806;&#35299;&#30721;&#22120;&#20063;&#20250;&#20135;&#29983;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25581;&#31034;&#22686;&#24378;&#35299;&#30721;&#30340;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#39044;&#35757;&#32451;&#30456;&#23545;&#20110;&#26222;&#36890;BERT&#26816;&#26597;&#28857;&#22312;&#36755;&#20837;&#20196;&#29260;&#30340;&#26465;&#27454;&#35206;&#30422;&#19978;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20197;&#35299;&#37322;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20256;&#32479;MAE&#30340;&#20462;&#25913;&#65292;&#23558;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#35299;&#30721;&#22120;&#26367;&#25442;&#20026;&#23436;&#20840;&#31616;&#21270;&#30340;&#35789;&#34955;&#39044;&#27979;&#20219;&#21153;&#12290;&#36825;&#31181;&#20462;&#25913;&#20351;&#24471;&#35789;&#27719;&#20449;&#21495;&#33021;&#22815;&#39640;&#25928;&#22320;&#21387;&#32553;&#21040;&#23494;&#38598;&#34920;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked auto-encoder pre-training has emerged as a prevalent technique for initializing and enhancing dense retrieval systems. It generally utilizes additional Transformer decoder blocks to provide sustainable supervision signals and compress contextual information into dense representations. However, the underlying reasons for the effectiveness of such a pre-training technique remain unclear. The usage of additional Transformer-based decoders also incurs significant computational costs. In this study, we aim to shed light on this issue by revealing that masked auto-encoder (MAE) pre-training with enhanced decoding significantly improves the term coverage of input tokens in dense representations, compared to vanilla BERT checkpoints. Building upon this observation, we propose a modification to the traditional MAE by replacing the decoder of a masked auto-encoder with a completely simplified Bag-of-Word prediction task. This modification enables the efficient compression of lexical signa
&lt;/p&gt;</description></item><item><title>RELIANCE&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#35780;&#20272;&#20449;&#24687;&#21644;&#26032;&#38395;&#30340;&#21487;&#20449;&#24230;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#23545;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#20449;&#24687;&#28304;&#30340;&#20934;&#30830;&#21306;&#20998;&#65292;&#24182;&#22312;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.10940</link><description>&lt;p&gt;
RELIANCE: &#21487;&#38752;&#30340;&#38598;&#25104;&#23398;&#20064;&#29992;&#20110;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RELIANCE: Reliable Ensemble Learning for Information and News Credibility Evaluation. (arXiv:2401.10940v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10940
&lt;/p&gt;
&lt;p&gt;
RELIANCE&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#35780;&#20272;&#20449;&#24687;&#21644;&#26032;&#38395;&#30340;&#21487;&#20449;&#24230;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#23545;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#20449;&#24687;&#28304;&#30340;&#20934;&#30830;&#21306;&#20998;&#65292;&#24182;&#22312;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#27867;&#28389;&#30340;&#26102;&#20195;&#65292;&#36776;&#21035;&#26032;&#38395;&#20869;&#23481;&#30340;&#21487;&#20449;&#24230;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RELIANCE&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#40065;&#26834;&#20449;&#24687;&#21644;&#34394;&#20551;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;&#20808;&#36827;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#12290;RELIANCE&#30001;&#20116;&#20010;&#19981;&#21516;&#30340;&#22522;&#26412;&#27169;&#22411;&#32452;&#25104;&#65292;&#21253;&#25324;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;BiLSTMs&#65289;&#12290;RELIANCE&#37319;&#29992;&#20102;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#25972;&#21512;&#23427;&#20204;&#30340;&#20248;&#21183;&#65292;&#21033;&#29992;&#38598;&#25104;&#30340;&#26234;&#33021;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;RELIANCE&#22312;&#21306;&#20998;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#20449;&#24687;&#28304;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#34920;&#26126;&#20854;&#22312;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#38754;&#36229;&#36807;&#20102;&#21333;&#20010;&#27169;&#22411;&#65292;&#24182;&#25104;&#20026;&#35780;&#20272;&#20449;&#24687;&#28304;&#21487;&#38752;&#24615;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of information proliferation, discerning the credibility of news content poses an ever-growing challenge. This paper introduces RELIANCE, a pioneering ensemble learning system designed for robust information and fake news credibility evaluation. Comprising five diverse base models, including Support Vector Machine (SVM), naive Bayes, logistic regression, random forest, and Bidirectional Long Short Term Memory Networks (BiLSTMs), RELIANCE employs an innovative approach to integrate their strengths, harnessing the collective intelligence of the ensemble for enhanced accuracy. Experiments demonstrate the superiority of RELIANCE over individual models, indicating its efficacy in distinguishing between credible and non-credible information sources. RELIANCE, also surpasses baseline models in information and news credibility assessment, establishing itself as an effective solution for evaluating the reliability of information sources.
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.05631</link><description>&lt;p&gt;
DrawTalking&#65306;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05631
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;DrawTalking&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#12290;&#23427;&#24378;&#35843;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#32534;&#31243;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#31867;&#20284;&#32534;&#31243;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;iPad&#19978;&#23454;&#29616;&#20102;&#23427;&#12290;&#19968;&#39033;&#24320;&#25918;&#24335;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#35768;&#22810;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#29992;&#20363;&#30456;&#22865;&#21512;&#21644;&#36866;&#29992;&#12290;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#21644;&#25351;&#23548;&#26410;&#26469;&#33258;&#28982;&#29992;&#25143;&#20013;&#24515;&#30028;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an interactive approach, DrawTalking, in which the user builds interactive worlds by sketching and speaking. It emphasizes user control and flexibility, and gives programming-like capability without code. We implemented it on the iPad. An open-ended study shows the mechanics resonate and are applicable to many creative-exploratory use cases. We hope to inspire and inform research in future natural user-centered interfaces.
&lt;/p&gt;</description></item><item><title>COPAL-ID&#26159;&#19968;&#20010;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#35328;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#19982;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#23427;&#34701;&#20837;&#20102;&#21360;&#23612;&#26412;&#22303;&#21644;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#65292;&#25552;&#20379;&#20102;&#26356;&#33258;&#28982;&#30340;&#26085;&#24120;&#22240;&#26524;&#25512;&#29702;&#25551;&#32472;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#20010;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#20294;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#26368;&#26032;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;COPAL-ID&#19978;&#30340;&#20934;&#30830;&#29575;&#36739;&#20302;&#65292;&#20165;&#20026;65.47%&#12290;</title><link>http://arxiv.org/abs/2311.01012</link><description>&lt;p&gt;
COPAL-ID: &#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#35328;&#25512;&#29702;&#19982;&#26412;&#22303;&#25991;&#21270;&#21644;&#32454;&#24494;&#24046;&#21035;
&lt;/p&gt;
&lt;p&gt;
COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances. (arXiv:2311.01012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01012
&lt;/p&gt;
&lt;p&gt;
COPAL-ID&#26159;&#19968;&#20010;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#35328;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#19982;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#23427;&#34701;&#20837;&#20102;&#21360;&#23612;&#26412;&#22303;&#21644;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#65292;&#25552;&#20379;&#20102;&#26356;&#33258;&#28982;&#30340;&#26085;&#24120;&#22240;&#26524;&#25512;&#29702;&#25551;&#32472;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#20010;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#20294;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#26368;&#26032;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;COPAL-ID&#19978;&#30340;&#20934;&#30830;&#29575;&#36739;&#20302;&#65292;&#20165;&#20026;65.47%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;COPAL-ID&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#35328;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#12290;&#19982;&#20197;&#21069;&#30340;&#21360;&#23612;COPA&#25968;&#25454;&#38598;&#65288;XCOPA-ID&#65289;&#19981;&#21516;&#65292;COPAL-ID&#34701;&#20837;&#20102;&#21360;&#23612;&#26412;&#22303;&#21644;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#65292;&#22240;&#27492;&#22312;&#21360;&#23612;&#25991;&#21270;&#39046;&#22495;&#20869;&#25552;&#20379;&#20102;&#26356;&#33258;&#28982;&#30340;&#26085;&#24120;&#22240;&#26524;&#25512;&#29702;&#25551;&#32472;&#12290;COPAL-ID&#30001;&#26412;&#22303;&#20154;&#20174;&#22836;&#24320;&#22987;&#19987;&#19994;&#25776;&#20889;&#65292;&#26356;&#27969;&#21033;&#65292;&#19981;&#20687;XCOPA-ID&#30340;&#32763;&#35793;&#23384;&#22312;&#23604;&#23596;&#30340;&#35789;&#35821;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20197;&#26631;&#20934;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#21644;&#38597;&#21152;&#36798;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#65288;&#19968;&#31181;&#22312;&#26085;&#24120;&#23545;&#35805;&#20013;&#24120;&#29992;&#30340;&#26041;&#35328;&#65289;&#21576;&#29616;COPAL-ID&#12290;COPAL-ID&#23545;&#20110;&#29616;&#26377;&#30340;&#24320;&#28304;&#21644;&#38381;&#28304;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#65292;&#25552;&#20986;&#20102;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#21364;&#26159;&#38750;&#24120;&#23481;&#26131;&#30340;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#24403;&#21069;&#26368;&#22909;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#20063;&#24456;&#38590;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;COPAL-ID&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;65.47%&#65292;&#36828;&#20302;&#20110;&#27809;&#26377;&#25991;&#21270;&#32972;&#26223;&#30340;XCOPA-ID&#65288;79.40%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present publicly available COPAL-ID, a novel Indonesian language common sense reasoning dataset. Unlike the previous Indonesian COPA dataset (XCOPA-ID), COPAL-ID incorporates Indonesian local and cultural nuances, and therefore, provides a more natural portrayal of day-to-day causal reasoning within the Indonesian cultural sphere. Professionally written by natives from scratch, COPAL-ID is more fluent and free from awkward phrases, unlike the translated XCOPA-ID. In addition, we present COPAL-ID in both standard Indonesian and in Jakartan Indonesian--a dialect commonly used in daily conversation. COPAL-ID poses a greater challenge for existing open-sourced and closed state-of-the-art multilingual language models, yet is trivially easy for humans. Our findings suggest that even the current best open-source, multilingual model struggles to perform well, achieving 65.47% accuracy on COPAL-ID, significantly lower than on the culturally-devoid XCOPA-ID (79.40%). Despite GPT-4's impressiv
&lt;/p&gt;</description></item><item><title>GPT-4&#36890;&#36807;&#20102;&#20844;&#24320;&#30340;&#22312;&#32447;&#22270;&#28789;&#27979;&#35797;&#20013;&#30340;41%&#30340;&#28216;&#25103;&#65292;&#22312;&#35821;&#35328;&#39118;&#26684;&#21644;&#31038;&#20250;&#24773;&#24863;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#36739;&#20339;&#65292;&#20294;&#20173;&#26410;&#33021;&#36798;&#21040;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#27700;&#24179;&#12290;&#22270;&#28789;&#27979;&#35797;&#20173;&#28982;&#26159;&#35780;&#20272;&#33258;&#28982;&#20132;&#27969;&#21644;&#27450;&#39575;&#30340;&#30456;&#20851;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20216</link><description>&lt;p&gt;
GPT-4 &#26159;&#21542;&#36890;&#36807;&#22270;&#28789;&#27979;&#35797;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does GPT-4 Pass the Turing Test?. (arXiv:2310.20216v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20216
&lt;/p&gt;
&lt;p&gt;
GPT-4&#36890;&#36807;&#20102;&#20844;&#24320;&#30340;&#22312;&#32447;&#22270;&#28789;&#27979;&#35797;&#20013;&#30340;41%&#30340;&#28216;&#25103;&#65292;&#22312;&#35821;&#35328;&#39118;&#26684;&#21644;&#31038;&#20250;&#24773;&#24863;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#36739;&#20339;&#65292;&#20294;&#20173;&#26410;&#33021;&#36798;&#21040;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#27700;&#24179;&#12290;&#22270;&#28789;&#27979;&#35797;&#20173;&#28982;&#26159;&#35780;&#20272;&#33258;&#28982;&#20132;&#27969;&#21644;&#27450;&#39575;&#30340;&#30456;&#20851;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#19968;&#20010;&#20844;&#24320;&#30340;&#22312;&#32447;&#22270;&#28789;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102; GPT-4&#12290;&#22312;&#34920;&#29616;&#26368;&#22909;&#30340; GPT-4 &#25552;&#31034;&#20013;&#65292;&#22312; 41% &#30340;&#28216;&#25103;&#20013;&#36890;&#36807;&#20102;&#27979;&#35797;&#65292;&#36229;&#36807;&#20102; ELIZA&#65288;27%&#65289;&#21644; GPT-3.5&#65288;14%&#65289;&#35774;&#23450;&#30340;&#22522;&#20934;&#65292;&#20294;&#36824;&#19981;&#22914;&#20154;&#31867;&#21442;&#19982;&#32773;&#65288;63%&#65289;&#30340;&#26426;&#20250;&#21644;&#22522;&#20934;&#12290;&#21442;&#19982;&#32773;&#30340;&#20915;&#31574;&#20027;&#35201;&#22522;&#20110;&#35821;&#35328;&#39118;&#26684;&#65288;35%&#65289;&#21644;&#31038;&#20250;&#24773;&#24863;&#29305;&#24449;&#65288;27%&#65289;&#65292;&#25903;&#25345;&#26234;&#33021;&#19981;&#36275;&#20197;&#36890;&#36807;&#22270;&#28789;&#27979;&#35797;&#30340;&#35266;&#28857;&#12290;&#21442;&#19982;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#29305;&#24449;&#65292;&#21253;&#25324;&#25945;&#32946;&#27700;&#24179;&#21644;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29087;&#24713;&#24230;&#65292;&#24182;&#19981;&#33021;&#39044;&#27979;&#34987;&#35782;&#21035;&#29575;&#65292;&#36825;&#34920;&#26126;&#21363;&#20351;&#26159;&#28145;&#20837;&#20102;&#35299;&#31995;&#32479;&#24182;&#39057;&#32321;&#19982;&#20854;&#20132;&#20114;&#30340;&#20154;&#65292;&#20063;&#20250;&#23481;&#26131;&#34987;&#27450;&#39575;&#12290;&#23613;&#31649;&#22270;&#28789;&#27979;&#35797;&#20316;&#20026;&#26234;&#33021;&#30340;&#27979;&#35797;&#20855;&#26377;&#24050;&#30693;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#23427;&#22312;&#35780;&#20272;&#33258;&#28982;&#20132;&#27969;&#21644;&#27450;&#39575;&#26041;&#38754;&#20173;&#28982;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;&#20855;&#26377;&#20882;&#20805;&#20154;&#31867;&#33021;&#21147;&#30340; AI &#27169;&#22411;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#24191;&#27867;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#31574;&#30053;&#21644;&#26631;&#20934;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4 prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and GPT-3.5 (14%), but falling short of chance and the baseline set by human participants (63%). Participants' decisions were based mainly on linguistic style (35%) and socio-emotional traits (27%), supporting the idea that intelligence is not sufficient to pass the Turing Test. Participants' demographics, including education and familiarity with LLMs, did not predict detection rate, suggesting that even those who understand systems deeply and interact with them frequently may be susceptible to deception. Despite known limitations as a test of intelligence, we argue that the Turing Test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and we analyse the effectiveness of different strategies and criteria fo
&lt;/p&gt;</description></item><item><title>SemStamp&#26159;&#19968;&#31181;&#31283;&#20581;&#30340;&#21477;&#32423;&#35821;&#20041;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#23558;&#35821;&#20041;&#31354;&#38388;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#36793;&#30028;&#32422;&#26463;&#22686;&#24378;&#20102;&#20854;&#31283;&#20581;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;SemStamp&#22312;&#37322;&#20041;&#21644;bigram&#26041;&#38754;&#26356;&#20026;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.03991</link><description>&lt;p&gt;
SemStamp&#65306;&#19968;&#31181;&#20855;&#26377;&#37322;&#20041;&#31283;&#20581;&#24615;&#30340;&#25991;&#26412;&#29983;&#25104;&#35821;&#20041;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation. (arXiv:2310.03991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03991
&lt;/p&gt;
&lt;p&gt;
SemStamp&#26159;&#19968;&#31181;&#31283;&#20581;&#30340;&#21477;&#32423;&#35821;&#20041;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#23558;&#35821;&#20041;&#31354;&#38388;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#36793;&#30028;&#32422;&#26463;&#22686;&#24378;&#20102;&#20854;&#31283;&#20581;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;SemStamp&#22312;&#37322;&#20041;&#21644;bigram&#26041;&#38754;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#27700;&#21360;&#31639;&#27861;&#30001;&#20110;&#20854;&#22522;&#20110;&#20196;&#29260;&#32423;&#21035;&#30340;&#35774;&#35745;&#65292;&#23545;&#37322;&#20041;&#25915;&#20987;&#20855;&#26377;&#24369;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#30340;&#31283;&#20581;&#21477;&#32423;&#35821;&#20041;&#27700;&#21360;&#31639;&#27861;&#8212;&#8212;SemStamp&#65292;&#35813;&#31639;&#27861;&#23545;&#21477;&#23376;&#30340;&#35821;&#20041;&#31354;&#38388;&#36827;&#34892;&#21010;&#20998;&#12290;&#31639;&#27861;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20505;&#36873;&#21477;&#23376;&#36827;&#34892;&#32534;&#30721;&#21644;LSH&#21704;&#24076;&#65292;&#24182;&#22312;&#35821;&#20041;&#23884;&#20837;&#31354;&#38388;&#20013;&#25191;&#34892;&#21477;&#32423;&#25298;&#32477;&#37319;&#26679;&#65292;&#30452;&#21040;&#37319;&#26679;&#30340;&#21477;&#23376;&#33853;&#20837;&#27700;&#21360;&#20998;&#21306;&#20013;&#12290;&#37319;&#29992;&#22522;&#20110;&#36793;&#30028;&#30340;&#32422;&#26463;&#26469;&#22686;&#24378;&#20854;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#31639;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"bigram"&#30340;&#37322;&#20041;&#25915;&#20987;&#65292;&#20351;&#29992;&#19982;&#21407;&#22987;&#21477;&#23376;&#26368;&#23569;&#30340;bigram&#37325;&#21472;&#30340;&#37322;&#20041;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#35821;&#20041;&#27700;&#21360;&#31639;&#27861;&#19981;&#20165;&#22312;&#24120;&#35265;&#30340;&#37322;&#20041;&#21644;bigram&#19978;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing watermarking algorithms are vulnerable to paraphrase attacks because of their token-level design. To address this issue, we propose SemStamp, a robust sentence-level semantic watermarking algorithm based on locality-sensitive hashing (LSH), which partitions the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by an LLM, and conducts sentence-level rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. A margin-based constraint is used to enhance its robustness. To show the advantages of our algorithm, we propose a "bigram" paraphrase attack using the paraphrase that has the fewest bigram overlaps with the original sentence. This attack is shown to be effective against the existing token-level watermarking method. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on both common and bigram pa
&lt;/p&gt;</description></item><item><title>C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2309.07597</link><description>&lt;p&gt;
C-Pack: &#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#25171;&#21253;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07597
&lt;/p&gt;
&lt;p&gt;
C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;C-Pack&#65292;&#36825;&#26159;&#19968;&#22871;&#26174;&#33879;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#12290;C-Pack&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#36164;&#28304;&#12290;1&#65289;C-MTEB&#26159;&#19968;&#20010;&#28085;&#30422;6&#20010;&#20219;&#21153;&#21644;35&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12290;2&#65289;C-MTP&#26159;&#19968;&#20010;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#27721;&#35821;&#35821;&#26009;&#24211;&#20013;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#12290;3&#65289;C-TEM&#26159;&#19968;&#20010;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;C-MTEB&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#25152;&#26377;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#36798;&#21040;&#20102;&#21457;&#24067;&#26102;&#30340;&#26368;&#39640;+10%&#12290;&#25105;&#20204;&#36824;&#25972;&#21512;&#21644;&#20248;&#21270;&#20102;C-TEM&#30340;&#25972;&#22871;&#35757;&#32451;&#26041;&#27861;&#12290;&#38500;&#20102;&#25105;&#20204;&#20851;&#20110;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#36164;&#28304;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#36825;&#20123;&#33521;&#35821;&#27169;&#22411;&#22312;MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#24067;&#30340;&#33521;&#35821;&#25968;&#25454;&#27604;&#27721;&#35821;&#25968;&#25454;&#22823;2&#20493;&#12290;&#25152;&#26377;&#36825;&#20123;&#36164;&#28304;&#37117;&#21487;&#20197;&#22312;https://github.com/FlagOpen/FlagEmbedding&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.
&lt;/p&gt;</description></item><item><title>Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;&#65292;&#22312;&#20943;&#23567;&#27169;&#22411;&#38544;&#34255;&#22823;&#23567;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.16475</link><description>&lt;p&gt;
Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Transformer Compression via Subspace Projection. (arXiv:2308.16475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16475
&lt;/p&gt;
&lt;p&gt;
Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;&#65292;&#22312;&#20943;&#23567;&#27169;&#22411;&#38544;&#34255;&#22823;&#23567;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TCSP&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#30340;&#38544;&#34255;&#22823;&#23567;&#26469;&#21387;&#32553;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#25972;&#20010;&#36716;&#25442;&#27169;&#22411;&#25237;&#24433;&#21040;&#19968;&#20010;&#23376;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#20351;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#30697;&#38453;&#19982;&#20943;&#23567;&#32500;&#24230;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#24314;&#31435;&#36825;&#20010;&#23376;&#31354;&#38388;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#19981;&#21516;&#23618;&#27425;&#30340;&#37319;&#26679;&#25968;&#25454;&#23454;&#20363;&#30340;&#29305;&#24449;&#30697;&#38453;&#20998;&#35299;&#20026;&#19968;&#20010;&#25237;&#24433;&#30697;&#38453;&#12290;&#20026;&#20102;&#35780;&#20272;&#25928;&#26524;&#65292;&#25105;&#20204;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#24212;&#29992;TCSP&#26469;&#21387;&#32553;T5&#21644;BERT&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TCSP&#22312;&#20445;&#35777;&#26368;&#22810;1.6%&#30340;&#20934;&#30830;&#24230;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;44%&#30340;&#21387;&#32553;&#27604;&#65292;&#36229;&#36807;&#25110;&#32773;&#36798;&#21040;&#20102;&#20808;&#21069;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;TCSP&#36824;&#19982;&#20854;&#20182;&#30446;&#26631;&#36807;&#28388;&#22120;&#21644;&#27880;&#24847;&#21147;&#22836;&#22823;&#23567;&#21387;&#32553;&#30340;&#26041;&#27861;&#30456;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose TCSP, a novel method for compressing a transformer model by focusing on reducing the hidden size of the model. By projecting the whole transform model into a subspace, we enable matrix operations between the weight matrices in the model and features in a reduced-dimensional space, leading to significant reductions in model parameters and computing resources. To establish this subspace, we decompose the feature matrix, derived from different layers of sampled data instances, into a projection matrix. For evaluation, TCSP is applied to compress T5 and BERT models on the GLUE and SQuAD benchmarks. Experimental results demonstrate that TCSP achieves a compression ratio of 44\% with at most 1.6\% degradation in accuracy, surpassing or matching prior compression methods. Furthermore, TCSP exhibits compatibility with other methods targeting filter and attention head size compression.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15051</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#22312;&#25512;&#21160;&#33647;&#29289;&#30740;&#21457;&#21644;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#24739;&#32773;&#25307;&#21215;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;TrialGPT&#65292;&#37319;&#29992;LLMs&#39044;&#27979;&#22522;&#20110;&#26631;&#20934;&#30340;&#21512;&#26684;&#24615;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#35299;&#37322;&#65292;&#24182;&#26681;&#25454;&#24739;&#32773;&#30149;&#21382;&#20013;&#30340;&#33258;&#30001;&#25991;&#26412;&#26469;&#23545;&#20505;&#36873;&#20020;&#24202;&#35797;&#39564;&#36827;&#34892;&#25490;&#21517;&#21644;&#25490;&#38500;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;184&#21517;&#24739;&#32773;&#21644;18,238&#20010;&#27880;&#37322;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#38431;&#21015;&#19978;&#35780;&#20272;&#20102;TrialGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;&#31532;&#19968;&#65292;TrialGPT&#22312;&#26631;&#20934;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#31532;&#20108;&#65292;TrialGPT&#30340;&#32508;&#21512;&#35797;&#39564;&#32423;&#21035;&#35780;&#20998;&#19982;&#19987;&#23478;&#26631;&#27880;&#30340;&#21512;&#26684;&#24615;&#39640;&#24230;&#30456;&#20851;&#12290;&#31532;&#19977;&#65292;&#36825;&#20123;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are vital in advancing drug development and evidence-based medicine, but their success is often hindered by challenges in patient recruitment. In this work, we investigate the potential of large language models (LLMs) to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection. Specifically, we introduce TrialGPT, a novel architecture employing LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes. We evaluate TrialGPT on three publicly available cohorts of 184 patients and 18,238 annotated clinical trials. The experimental results demonstrate several key findings: First, TrialGPT achieves high criterion-level prediction accuracy with faithful explanations. Second, the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations. Third, these scor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prot2Text&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;GNNs&#21644;Transformers&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#32508;&#21512;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#20840;&#38754;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.14367</link><description>&lt;p&gt;
Prot2Text: &#22522;&#20110;GNNs&#21644;Transformers&#30340;&#22810;&#27169;&#24577;&#34507;&#30333;&#36136;&#21151;&#33021;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers. (arXiv:2307.14367v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14367
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prot2Text&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;GNNs&#21644;Transformers&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#32508;&#21512;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#20840;&#38754;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#29289;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#20351;&#26576;&#20123;&#31185;&#23398;&#23478;&#23558;&#20854;&#29702;&#35299;&#24402;&#31867;&#20026;&#38590;&#20197;&#24819;&#35937;&#30340;&#20219;&#21153;&#12290;&#19981;&#21516;&#32423;&#21035;&#30340;&#25361;&#25112;&#20351;&#36825;&#39033;&#20219;&#21153;&#22797;&#26434;&#21270;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#24320;&#21457;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#23558;&#20219;&#21153;&#34920;&#36848;&#20026;&#22810;&#20998;&#31867;&#38382;&#39064;&#65292;&#21363;&#23558;&#39044;&#23450;&#20041;&#26631;&#31614;&#20998;&#37197;&#32473;&#34507;&#30333;&#36136;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;Prot2Text&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#20013;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#34507;&#30333;&#36136;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#12290;&#36825;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#20801;&#35768;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#36827;&#34892;&#25972;&#20307;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complex nature of big biological systems pushed some scientists to classify its understanding under the inconceivable missions. Different leveled challenges complicated this task, one of is the prediction of a protein's function. In recent years, significant progress has been made in this field through the development of various machine learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e assigning predefined labels to proteins. In this work, we propose a novel approach, \textbf{Prot2Text}, which predicts a protein function's in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including proteins' sequences, structures, and textual annotations. This multimodal approach allows for a holistic representation of proteins' functions, en
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02046</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25512;&#33616;&#31995;&#32479; (LLMs)
&lt;/p&gt;
&lt;p&gt;
Recommender Systems in the Era of Large Language Models (LLMs). (arXiv:2307.02046v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02046
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#21644;&#32593;&#32476;&#24212;&#29992;&#30340;&#32321;&#33635;&#65292;&#25512;&#33616;&#31995;&#32479;&#65288;RecSys&#65289;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#20197;&#28385;&#36275;&#20854;&#21916;&#22909;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#21644;&#25972;&#21512;&#25991;&#26412;&#20391;&#20449;&#24687;&#22312;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#26159;DNN&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#29702;&#35299;&#29992;&#25143;&#20852;&#36259;&#12289;&#25429;&#25417;&#25991;&#26412;&#20391;&#20449;&#24687;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#25512;&#33616;&#22330;&#26223;&#20013;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#19981;&#36275;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65288;&#20363;&#22914;ChatGPT&#21644;GPT4&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22522;&#26412;&#32844;&#36131;&#19978;&#26377;&#30528;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, 
&lt;/p&gt;</description></item><item><title>AWQ&#26159;&#19968;&#31181;&#28608;&#27963;&#24863;&#30693;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25252;&#23569;&#37327;&#26174;&#33879;&#26435;&#37325;&#26469;&#38477;&#20302;&#37327;&#21270;&#35823;&#24046;&#65292;&#19981;&#20381;&#36182;&#20110;&#21453;&#21521;&#20256;&#25773;&#25110;&#37325;&#26500;&#65292;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00978</link><description>&lt;p&gt;
AWQ&#65306;LLM&#21387;&#32553;&#19982;&#21152;&#36895;&#30340;&#28608;&#27963;&#24863;&#30693;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. (arXiv:2306.00978v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00978
&lt;/p&gt;
&lt;p&gt;
AWQ&#26159;&#19968;&#31181;&#28608;&#27963;&#24863;&#30693;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25252;&#23569;&#37327;&#26174;&#33879;&#26435;&#37325;&#26469;&#38477;&#20302;&#37327;&#21270;&#35823;&#24046;&#65292;&#19981;&#20381;&#36182;&#20110;&#21453;&#21521;&#20256;&#25773;&#25110;&#37325;&#26500;&#65292;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#25552;&#39640;&#20102;&#20026;&#26381;&#21153;(&#20869;&#23384;&#22823;&#23567;)&#24102;&#26469;&#30340;&#30828;&#20214;&#38556;&#30861;&#65292;&#24182;&#38477;&#20302;&#20102;&#20196;&#29260;&#29983;&#25104;&#36895;&#24230;(&#20869;&#23384;&#24102;&#23485;)&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28608;&#27963;&#24863;&#30693;&#26435;&#37325;&#37327;&#21270;(AWQ)&#30340;&#30828;&#20214;&#21451;&#22909;&#26041;&#27861;&#65292;&#29992;&#20110;LLM&#20302;&#27604;&#29305;&#26435;&#37325;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#65306;&#26435;&#37325;&#24182;&#19981;&#26159;&#31561;&#37325;&#35201;&#30340;&#65307;&#20165;&#20445;&#25252;1%&#30340;&#26174;&#33879;&#26435;&#37325;&#23601;&#33021;&#22823;&#22823;&#38477;&#20302;&#37327;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#23547;&#25214;&#36890;&#36807;&#35266;&#23519;&#28608;&#27963;&#20540;&#32780;&#19981;&#26159;&#26435;&#37325;&#26469;&#20445;&#25252;&#26174;&#33879;&#26435;&#37325;&#30340;&#26368;&#20339;&#25353;&#36890;&#36947;&#32553;&#25918;&#26041;&#27861;&#12290;AWQ&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#21453;&#21521;&#20256;&#25773;&#25110;&#37325;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#24456;&#22909;&#22320;&#20445;&#25345;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#27169;&#24335;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#20250;&#36807;&#24230;&#25311;&#21512;&#26657;&#20934;&#38598;&#12290;AWQ&#22312;&#21508;&#31181;&#35821;&#35328;&#24314;&#27169;&#21644;&#39046;&#22495;&#29305;&#23450;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#30001;&#20110;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23427;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#37327;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown excellent performance on various tasks, but the astronomical model size raises the hardware barrier for serving (memory size) and slows down token generation (memory bandwidth). In this paper, we propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. Our method is based on the observation that weights are not equally important: protecting only 1% of salient weights can greatly reduce quantization error. We then propose to search for the optimal per-channel scaling that protects the salient weights by observing the activation, not weights. AWQ does not rely on any backpropagation or reconstruction, so it can well preserve LLMs' generalization ability on different domains and modalities, without overfitting to the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks. Thanks to better generalization, it achieves excellent quantiz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#28982;&#39046;&#22495;&#36716;&#31227;&#35774;&#32622;&#19979;&#24494;&#35843;&#21644;&#23567;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#30340;DR&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;DR&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#28304;&#22495;&#38477;&#20302;&#65288;SD&#65289;&#21644;&#30446;&#26631;&#22495;&#38477;&#20302;&#65288;TD&#65289;&#65292;&#24182;&#21457;&#29616;&#20004;&#32773;&#20043;&#19968;&#36890;&#24120;&#26159;&#27491;&#20540;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00168</link><description>&lt;p&gt;
&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#38754;&#23545;&#39046;&#22495;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring the Robustness of Natural Language Processing Models to Domain Shifts. (arXiv:2306.00168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#28982;&#39046;&#22495;&#36716;&#31227;&#35774;&#32622;&#19979;&#24494;&#35843;&#21644;&#23567;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#30340;DR&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;DR&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#28304;&#22495;&#38477;&#20302;&#65288;SD&#65289;&#21644;&#30446;&#26631;&#22495;&#38477;&#20302;&#65288;TD&#65289;&#65292;&#24182;&#21457;&#29616;&#20004;&#32773;&#20043;&#19968;&#36890;&#24120;&#26159;&#27491;&#20540;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#33853;&#21518;&#20110;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#39046;&#22495;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39046;&#22495;&#40065;&#26834;&#24615;&#65288;DR&#65289;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;DR&#30740;&#31350;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#35774;&#32622;&#12289;&#32570;&#20047;&#35780;&#20272;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#21644;&#36807;&#22810;&#20381;&#38752;&#25361;&#25112;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#33258;&#28982;&#39046;&#22495;&#36716;&#31227;&#35774;&#32622;&#19979;&#24494;&#35843;&#21644;&#23567;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#30340;DR&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;DR&#22522;&#20934;&#65292;&#21253;&#25324;&#22810;&#26679;&#21270;&#30340;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#21477;&#23376;&#21644;&#26631;&#35760;&#32423;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#29983;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#30001;&#20960;&#20010;&#39046;&#22495;&#32452;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#28304;&#22495;&#38477;&#20302;&#65288;SD&#65289;&#21644;&#30446;&#26631;&#22495;&#38477;&#20302;&#65288;TD&#65289;&#65292;&#23427;&#20204;&#20132;&#26367;&#20316;&#20026;&#21442;&#32771;&#28857;&#26469;&#27604;&#36739;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#37325;&#22823;&#27604;&#20363;&#30340;&#39046;&#22495;&#36716;&#31227;&#20013;&#65292;SD&#25110;TD&#20043;&#19968;&#26159;&#27491;&#30340;&#65292;&#20294;&#19981;&#26159;&#20004;&#32773;&#37117;&#27491;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#20801;&#35768;&#22312;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35774;&#32622;&#19978;&#20844;&#24179;&#27604;&#36739;DR&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;NLP&#27169;&#22411;DR&#24615;&#36136;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown promising performance on various tasks, including fine-tuning, few-shot learning, and zero-shot learning. However, their performance on domains without labeled data still lags behind those with labeled data, which we refer as the Domain Robustness (DR) challenge. Existing research on DR suffers from disparate setups, lack of evaluation task variety, and reliance on challenge sets. In this paper, we explore the DR challenge of both fine-tuned and few-shot learning models in natural domain shift settings. We introduce a DR benchmark comprising diverse NLP tasks, including sentence and token-level classification, QA, and generation, each task consists of several domains. We propose two views of the DR challenge: Source Drop (SD) and Target Drop (TD), which alternate between the source and target in-domain performance as reference points. We find that in significant proportions of domain shifts, either SD or TD is positive, but not both, emphasizing the imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35757;&#32451;&#31639;&#27861;Left-over Lunch RL &#65288;LoL-RL&#65289;&#65292;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20248;&#21270;LM&#25928;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14718</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#20248;&#21183;&#30340;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Language Models with Advantage-based Offline Policy Gradients. (arXiv:2305.14718v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35757;&#32451;&#31639;&#27861;Left-over Lunch RL &#65288;LoL-RL&#65289;&#65292;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20248;&#21270;LM&#25928;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#29992;&#25143;&#23450;&#20041;&#30340;&#36136;&#37327;&#25110;&#39118;&#26684;&#38480;&#21046;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20856;&#22411;&#30340;&#26041;&#27861;&#21253;&#25324;&#23398;&#20064;&#39069;&#22806;&#30340;&#20154;&#24037;&#32534;&#20889;&#25968;&#25454;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#36807;&#28388;&#8220;&#20302;&#36136;&#37327;&#8221;&#25968;&#25454;&#21644;/&#25110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#20307;&#21453;&#39304;&#65288;RLHF&#65289;&#12290;&#28982;&#32780;&#65292;&#36807;&#28388;&#20250;&#21024;&#38500;&#26377;&#20215;&#20540;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#32780;&#25968;&#25454;&#25910;&#38598;&#21644;RLHF&#19981;&#26029;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#32534;&#20889;&#25110;LM&#25506;&#32034;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#8220;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;RL&#26469;&#20248;&#21270;&#29616;&#26377;&#30340;&#20247;&#21253;&#21644;&#20114;&#32852;&#32593;&#25968;&#25454;&#19978;&#30340;LM&#25928;&#29992;&#21527;&#65311;&#8221;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21097;&#20313;&#21320;&#39184;&#24378;&#21270;&#23398;&#20064;&#65288;LoL-RL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#26469;&#23398;&#20064;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20316;&#20026;1&#27493;RL&#28216;&#25103;&#12290; LoL-RL&#21487;&#20197;&#24494;&#35843;LM&#65292;&#20197;&#20248;&#21270;&#20219;&#24847;&#22522;&#20110;&#20998;&#31867;&#22120;&#25110;&#20154;&#23450;&#20041;&#30340;&#25928;&#29992;&#20989;&#25968;&#30340;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#12290;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#27169;&#22411;&#30340;&#20116;&#20010;&#19981;&#21516;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Improving language model generations according to some user-defined quality or style constraints is challenging. Typical approaches include learning on additional human-written data, filtering ``low-quality'' data using heuristics and/or using reinforcement learning with human feedback (RLHF). However, filtering can remove valuable training signals, whereas data collection and RLHF constantly require additional human-written or LM exploration data which can be costly to obtain. A natural question to ask is ``Can we leverage RL to optimize LM utility on existing crowd-sourced and internet data?''  To this end, we present Left-over Lunch RL (LoL-RL), a simple training algorithm that uses offline policy gradients for learning language generation tasks as a 1-step RL game. LoL-RL can finetune LMs to optimize arbitrary classifier-based or human-defined utility functions on any sequence-to-sequence data. Experiments with five different language generation tasks using models of varying sizes 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;n-best&#37325;&#25490;&#24207;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#25552;&#20379;&#20266;&#26631;&#31614;&#65292;&#35757;&#32451;&#20986;&#21442;&#25968;&#26356;&#23569;&#20294;&#31934;&#24230;&#30456;&#24403;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12057</link><description>&lt;p&gt;
&#22522;&#20110;n-best&#37325;&#25490;&#24207;&#30340;&#31934;&#20934;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Accurate Knowledge Distillation with n-best Reranking. (arXiv:2305.12057v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12057
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;n-best&#37325;&#25490;&#24207;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#25552;&#20379;&#20266;&#26631;&#31614;&#65292;&#35757;&#32451;&#20986;&#21442;&#25968;&#26356;&#23569;&#20294;&#31934;&#24230;&#30456;&#24403;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;n-best&#37325;&#25490;&#24207;&#30340;&#24207;&#21015;&#32423;&#21035;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#25945;&#24072;&#27169;&#22411;&#30340;top-1&#20551;&#35774;&#20197;&#21450;top n-best&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21253;&#25324;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20869;&#30340;&#22810;&#31181;&#27169;&#22411;&#65292;&#20026;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;WMT21&#24503;&#33521;&#32763;&#35793;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#20855;&#26377;&#20004;&#20010;&#25968;&#37327;&#32423;&#36739;&#23569;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#19982;Tran&#31561;&#20154;&#65288;2021&#24180;&#65289;&#30340;&#21253;&#21547;47&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#32763;&#35793;&#27169;&#22411;&#30456;&#24403;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose extending the Sequence-level Knowledge Distillation (Kim and Rush, 2016) with n-best reranking to consider not only the top-1 hypotheses but also the top n-best hypotheses of teacher models. Our approach leverages a diverse set of models, including publicly-available large pretrained models, to provide more accurate pseudo-labels for training student models. We validate our proposal on the WMT21 German-English translation task and demonstrate that our student model achieves comparable accuracy to a large translation model with 4.7 billion parameters from (Tran et al., 2021) while having two orders of magnitude fewer parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLAIR&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#21644;&#22238;&#31572;&#26469;&#26816;&#27979;ChatGPT&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30495;&#23454;&#24615;&#65292;&#21487;&#20197;&#20998;&#31867;&#20154;&#21644;&#26426;&#22120;&#20154;&#12290;&#21333;&#38382;&#39064;&#20998;&#20026;&#23545;&#20110;&#20154;&#31867;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#26426;&#22120;&#20154;&#24456;&#38590;&#21644;&#23545;&#20110;&#26426;&#22120;&#20154;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#20154;&#31867;&#24456;&#38590;&#20004;&#20010;&#31867;&#21035;&#65292;&#20998;&#21035;&#36827;&#34892;&#26816;&#27979;&#12290; &#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06424</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#36824;&#26159;&#20154;&#31867;&#65311;&#29992;&#19968;&#20010;&#38382;&#39064;&#26816;&#27979;ChatGPT&#20882;&#21517;&#39030;&#26367;&#32773;
&lt;/p&gt;
&lt;p&gt;
Bot or Human? Detecting ChatGPT Imposters with A Single Question. (arXiv:2305.06424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLAIR&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#21644;&#22238;&#31572;&#26469;&#26816;&#27979;ChatGPT&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30495;&#23454;&#24615;&#65292;&#21487;&#20197;&#20998;&#31867;&#20154;&#21644;&#26426;&#22120;&#20154;&#12290;&#21333;&#38382;&#39064;&#20998;&#20026;&#23545;&#20110;&#20154;&#31867;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#26426;&#22120;&#20154;&#24456;&#38590;&#21644;&#23545;&#20110;&#26426;&#22120;&#20154;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#20154;&#31867;&#24456;&#38590;&#20004;&#20010;&#31867;&#21035;&#65292;&#20998;&#21035;&#36827;&#34892;&#26816;&#27979;&#12290; &#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#26368;&#36817;&#23637;&#31034;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;&#24471;&#32763;&#35793;&#12289;&#20889;&#20316;&#21644;&#38386;&#32842;&#31561;&#21508;&#31181;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#25285;&#24515;&#23427;&#20204;&#21487;&#33021;&#34987;&#28389;&#29992;&#20110;&#27450;&#35784;&#25110;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#31561;&#24694;&#24847;&#29992;&#36884;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26816;&#27979;&#32842;&#22825;&#20013;&#28041;&#21450;&#30340;&#21478;&#19968;&#26041;&#26159;&#26426;&#22120;&#20154;&#36824;&#26159;&#20154;&#31867;&#30340;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLAIR&#30340;&#26694;&#26550;&#65292;&#21363;&#36890;&#36807;&#21333;&#20010;&#38382;&#39064;&#21644;&#22238;&#31572;&#26469;&#26597;&#25214;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#20197;&#22312;&#32447;&#26041;&#24335;&#26816;&#27979;&#20250;&#35805;&#20013;&#30340;&#23545;&#35805;&#26426;&#22120;&#20154;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38024;&#23545;&#19968;&#20010;&#21333;&#19968;&#38382;&#39064;&#22330;&#26223;&#65292;&#35813;&#22330;&#26223;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#20154;&#31867;&#29992;&#25143;&#21644;&#26426;&#22120;&#20154;&#12290;&#36825;&#20123;&#38382;&#39064;&#20998;&#20026;&#20004;&#31867;&#65306;&#23545;&#20110;&#20154;&#31867;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#26426;&#22120;&#20154;&#24456;&#38590;&#65288;&#20363;&#22914;&#35745;&#25968;&#12289;&#26367;&#25442;&#12289;&#23450;&#20301;&#12289;&#22122;&#38899;&#36807;&#28388;&#21644;ASCII&#33402;&#26415;&#65289;&#65292;&#20197;&#21450;&#23545;&#20110;&#26426;&#22120;&#20154;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#20154;&#31867;&#24456;&#38590;&#65288;&#20363;&#22914;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#35782;&#21035;&#65289;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;FLAIR&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models like ChatGPT have recently demonstrated impressive capabilities in natural language understanding and generation, enabling various applications including translation, essay writing, and chit-chatting. However, there is a concern that they can be misused for malicious purposes, such as fraud or denial-of-service attacks. Therefore, it is crucial to develop methods for detecting whether the party involved in a conversation is a bot or a human. In this paper, we propose a framework named FLAIR, Finding Large language model Authenticity via a single Inquiry and Response, to detect conversational bots in an online manner. Specifically, we target a single question scenario that can effectively differentiate human users from bots. The questions are divided into two categories: those that are easy for humans but difficult for bots (e.g., counting, substitution, positioning, noise filtering, and ASCII art), and those that are easy for bots but difficult for humans (e.g., m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;RNN seq2seq&#27169;&#22411;&#22312;&#23398;&#20064;&#22235;&#31181;&#36716;&#25442;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#21482;&#33021;&#36924;&#36817;&#31526;&#21512;&#35757;&#32451;&#25110;&#20998;&#24067;&#20869;&#25968;&#25454;&#30340;&#26144;&#23556;&#65292;&#19981;&#33021;&#23398;&#20064;&#24213;&#23618;&#20989;&#25968;&#65307;&#25991;&#31456;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#65292;&#29992;&#20110;&#26080;&#27880;&#24847;&#21147;RNN seq2seq&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#23383;&#31526;&#20018;&#36716;&#25442;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.06841</link><description>&lt;p&gt;
&#20351;&#29992;RNN&#27169;&#22411;&#23398;&#20064;&#36716;&#25442;&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Learning Transductions and Alignments with RNN Seq2seq Models. (arXiv:2303.06841v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;RNN seq2seq&#27169;&#22411;&#22312;&#23398;&#20064;&#22235;&#31181;&#36716;&#25442;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#21482;&#33021;&#36924;&#36817;&#31526;&#21512;&#35757;&#32451;&#25110;&#20998;&#24067;&#20869;&#25968;&#25454;&#30340;&#26144;&#23556;&#65292;&#19981;&#33021;&#23398;&#20064;&#24213;&#23618;&#20989;&#25968;&#65307;&#25991;&#31456;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#65292;&#29992;&#20110;&#26080;&#27880;&#24847;&#21147;RNN seq2seq&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#23383;&#31526;&#20018;&#36716;&#25442;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#24207;&#21015;&#21040;&#24207;&#21015;(RNN seq2seq)&#27169;&#22411;&#22312;&#23398;&#20064;&#22235;&#31181;&#36716;&#25442;&#20219;&#21153;&#65306;&#24658;&#31561;&#12289;&#21453;&#36716;&#12289;&#23436;&#20840;&#37325;&#22797;&#21644;&#20108;&#27425;&#22797;&#21046;&#12290;&#36825;&#20123;&#36716;&#25442;&#22312;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#19979;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#20855;&#26377;&#36880;&#28176;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;RNN seq2seq&#27169;&#22411;&#21482;&#33021;&#36924;&#36817;&#31526;&#21512;&#35757;&#32451;&#25110;&#20998;&#24067;&#20869;&#25968;&#25454;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#33021;&#23398;&#20064;&#24213;&#23618;&#20989;&#25968;&#12290;&#23613;&#31649;&#27880;&#24847;&#21147;&#26426;&#21046;&#20351;&#23398;&#20064;&#26356;&#21152;&#39640;&#25928;&#21644;&#40065;&#26834;&#65292;&#20294;&#23427;&#24182;&#19981;&#33021;&#20811;&#26381;&#20998;&#24067;&#22806;&#30340;&#27867;&#21270;&#38480;&#21046;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#26469;&#23398;&#20064;&#36825;&#22235;&#20010;&#20219;&#21153;&#30340;&#26080;&#27880;&#24847;&#21147;RNN seq2seq&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#29992;&#27491;&#24335;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#26469;&#35299;&#37322;&#65292;&#32780;&#19981;&#26159;&#23383;&#31526;&#20018;&#36716;&#25442;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#12290;RNN&#30340;&#21464;&#31181;&#20063;&#22312;&#32467;&#26524;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#31616;&#21333;&#30340;RNN seq2seq&#27169;&#22411;&#26080;&#27861;&#35745;&#31639;&#36755;&#20837;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper studies the capabilities of Recurrent-Neural-Network sequence to sequence (RNN seq2seq) models in learning four transduction tasks: identity, reversal, total reduplication, and quadratic copying. These transductions are traditionally well studied under finite state transducers and attributed with increasing complexity. We find that RNN seq2seq models are only able to approximate a mapping that fits the training or in-distribution data, instead of learning the underlying functions. Although attention makes learning more efficient and robust, it does not overcome the out-of-distribution generalization limitation. We establish a novel complexity hierarchy for learning the four tasks for attention-less RNN seq2seq models, which may be understood in terms of the complexity hierarchy of formal languages, instead of string transductions. RNN variants also play a role in the results. In particular, we show that Simple RNN seq2seq models cannot count the input length.
&lt;/p&gt;</description></item></channel></rss>