<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.13714</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3.5&#21644;GPT-4&#22312;&#25903;&#25345;&#21307;&#30103;&#20445;&#20581;&#20449;&#24687;&#38656;&#27714;&#26041;&#38754;&#30340;&#23454;&#38469;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#24403;&#21069;&#30340;&#25506;&#32034;&#24182;&#26410;&#35780;&#20272;LLMs&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#20004;&#20010;LLM&#26159;&#21542;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#30001;&#21307;&#29983;&#25552;&#20132;&#30340;&#20449;&#24687;&#38656;&#27714;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;66&#20010;&#26469;&#33258;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#38382;&#39064;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#25552;&#20132;&#32473;GPT-3.5&#21644;GPT-4&#12290;12&#21517;&#21307;&#29983;&#35780;&#20272;&#20102;LLM&#21709;&#24212;&#23545;&#24739;&#32773;&#36896;&#25104;&#20260;&#23475;&#30340;&#21487;&#33021;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#29616;&#26377;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#21307;&#29983;&#30340;&#35780;&#20272;&#22522;&#20110;&#22810;&#25968;&#31080;&#27719;&#24635;&#12290;&#23545;&#20110;&#27809;&#26377;&#20219;&#20309;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#21307;&#29983;&#35748;&#20026;&#20219;&#20309;&#19968;&#20010;LLM&#21709;&#24212;&#37117;&#19981;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#23545;&#20110;GPT-3.5&#65292;8&#20010;&#38382;&#39064;&#30340;&#21709;&#24212;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#25253;&#21578;&#19968;&#33268;&#65292;20&#20010;&#19981;&#19968;&#33268;&#65292;9&#20010;&#26080;&#27861;&#35780;&#20272;&#12290;&#26377;29&#20010;&#21709;&#24212;&#27809;&#26377;&#22810;&#25968;&#31080;&#34920;&#31034;&#8220;&#21516;&#24847;&#8221;&#12289;&#8220;&#19981;&#21516;&#24847;&#8221;&#21644;&#8220;&#26080;&#27861;&#35780;&#20272;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite growing interest in using large language models (LLMs) in healthcare, current explorations do not assess the real-world utility and safety of LLMs in clinical settings. Our objective was to determine whether two LLMs can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. Sixty six questions from an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple prompts. 12 physicians assessed the LLM responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. Physician assessments were summarized based on majority vote. For no questions did a majority of physicians deem either LLM response as harmful. For GPT-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. There were 29 responses with no majority on "Agree", "Disagree", and "Unable to assess". Fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;LLMs&#30340;&#20351;&#29992;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.13712</link><description>&lt;p&gt;
&#21457;&#25381;LLMs&#22312;&#23454;&#36341;&#20013;&#30340;&#21147;&#37327;&#65306;ChatGPT&#21450;&#20854;&#24212;&#29992;&#30340;&#32508;&#36848;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. (arXiv:2304.13712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;LLMs&#30340;&#20351;&#29992;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20174;&#20107;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#20174;&#19994;&#20154;&#21592;&#21644;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#23454;&#29992;&#30340;&#25351;&#21335;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;Large Language Models&#65288;LLMs&#65289;&#12290;&#25105;&#20204;&#20174;&#27169;&#22411;&#12289;&#25968;&#25454;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;LLMs&#30340;&#20351;&#29992;&#35752;&#35770;&#21644;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24403;&#21069;&#30340;GPT&#21644;BERT&#26679;&#24335;&#30340;LLMs&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38750;&#20351;&#29992;&#24773;&#20917;&#65292;&#20363;&#22914;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12289;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12289;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12289;&#32039;&#24613;&#33021;&#21147;&#20197;&#21450;&#29305;&#23450;&#20219;&#21153;&#30340;&#32771;&#34385;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#21508;&#31181;&#20351;&#29992;&#21644;&#38750;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#35828;&#26126;LLMs&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#23454;&#38469;&#24212;&#29992;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#35797;&#22270;&#20102;&#35299;&#25968;&#25454;&#23545;&#20110;LLMs&#24212;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of dat
&lt;/p&gt;</description></item><item><title>HeySQuAD &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#21475;&#35821;&#21270;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#34913;&#37327;&#26426;&#22120;&#29702;&#35299;&#24182;&#22238;&#31572;&#22024;&#26434;&#30340;&#21475;&#35821;&#25552;&#38382;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#29992;&#36716;&#24405;&#30340;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#36827;&#34892;&#35757;&#32451;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13689</link><description>&lt;p&gt;
HeySQuAD: &#19968;&#20010;&#21475;&#35821;&#21270;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HeySQuAD: A Spoken Question Answering Dataset. (arXiv:2304.13689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13689
&lt;/p&gt;
&lt;p&gt;
HeySQuAD &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#21475;&#35821;&#21270;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#34913;&#37327;&#26426;&#22120;&#29702;&#35299;&#24182;&#22238;&#31572;&#22024;&#26434;&#30340;&#21475;&#35821;&#25552;&#38382;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#29992;&#36716;&#24405;&#30340;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#36827;&#34892;&#35757;&#32451;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#23545;&#20110;&#35780;&#20272;&#21475;&#35821;&#38382;&#31572;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#25968;&#23383;&#21161;&#25163;&#31561;&#22810;&#20010;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#31038;&#21306;&#20849;&#20139;&#30340;&#21475;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598; HeySQuAD&#65292;&#23427;&#30001;76k&#20010;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#12289;97k&#20010;&#26426;&#22120;&#29983;&#25104;&#30340;&#38382;&#39064;&#20197;&#21450;&#30456;&#24212;&#30340;&#25991;&#26412;&#31572;&#26696;&#32452;&#25104;&#65292;&#36825;&#20123;&#31572;&#26696;&#28304;&#33258; SQuAD QA &#25968;&#25454;&#38598;&#12290;HeySQuAD &#30340;&#30446;&#26631;&#26159;&#34913;&#37327;&#26426;&#22120;&#29702;&#35299;&#22024;&#26434;&#30340;&#21475;&#35821;&#25552;&#38382;&#24182;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#20154;&#31867;&#21475;&#35821;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#37327;&#21270;&#26469;&#33258;&#20004;&#26041;&#38754;&#22122;&#22768;&#30340;&#24046;&#24322;&#21450;&#23545;&#27169;&#22411;&#21644;&#22238;&#31572;&#20934;&#30830;&#24230;&#30340;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#21475;&#35821;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#22238;&#31572;&#30340;&#26159;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#36716;&#24405;&#30340;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#21644;&#21407;&#22987; SQuAD &#38382;&#39064;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#65288;12.51%&#65289;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32780;&#19981;&#26159;&#20165;&#20351;&#29992;&#21407;&#22987; SQuAD &#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-spoken questions are critical to evaluating the performance of spoken question answering (SQA) systems that serve several real-world use cases including digital assistants. We present a new large-scale community-shared SQA dataset, HeySQuAD that consists of 76k human-spoken questions and 97k machine-generated questions and corresponding textual answers derived from the SQuAD QA dataset. The goal of HeySQuAD is to measure the ability of machines to understand noisy spoken questions and answer the questions accurately. To this end, we run extensive benchmarks on the human-spoken and machine-generated questions to quantify the differences in noise from both sources and its subsequent impact on the model and answering accuracy. Importantly, for the task of SQA, where we want to answer human-spoken questions, we observe that training using the transcribed human-spoken and original SQuAD questions leads to significant improvements (12.51%) over training using only the original SQuAD te
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GEN&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#19968;&#23567;&#32452;&#21477;&#23376;/&#38382;&#39064;&#23545;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#24335;&#30340;&#26041;&#27861;&#29983;&#25104;&#38382;&#39064;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#20174;&#29992;&#25143;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#25552;&#39640;&#29983;&#25104;&#38382;&#39064;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.13664</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#24335;&#21453;&#39304;&#25552;&#39640;&#38382;&#31572;&#29983;&#25104;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Using Implicit Feedback to Improve Question Generation. (arXiv:2304.13664v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GEN&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#19968;&#23567;&#32452;&#21477;&#23376;/&#38382;&#39064;&#23545;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#24335;&#30340;&#26041;&#27861;&#29983;&#25104;&#38382;&#39064;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#20174;&#29992;&#25143;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#25552;&#39640;&#29983;&#25104;&#38382;&#39064;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#29983;&#25104;(QG)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#19968;&#20010;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#12290;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#20174;&#33258;&#21160;&#29983;&#25104;&#30340;&#38382;&#39064;&#20013;&#21463;&#30410;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#36873;&#25321;&#25110;&#32534;&#36753;&#36825;&#20123;&#38382;&#39064;&#26469;&#20351;&#20854;&#26356;&#20934;&#30830;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GEN&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20174;&#36825;&#20123;&#65288;&#38544;&#24335;&#65289;&#21453;&#39304;&#20013;&#23398;&#20064;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#22522;&#20110;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#23558;&#19968;&#23567;&#32452;&#21477;&#23376;/&#38382;&#39064;&#23545;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#21019;&#24314;&#27169;&#24335;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21477;&#23376;&#12290;&#27599;&#20010;&#30001;&#29992;&#25143;&#32416;&#27491;&#21518;&#29983;&#25104;&#30340;&#38382;&#39064;&#37117;&#20316;&#20026;&#19979;&#19968;&#27425;&#36845;&#20195;&#30340;&#26032;&#31181;&#23376;&#20351;&#29992;&#65292;&#22240;&#27492;&#27599;&#27425;&#37117;&#20250;&#21019;&#24314;&#26356;&#22810;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#29992;&#25143;&#25152;&#20570;&#30340;&#26356;&#27491;&#26469;&#35780;&#20998;&#27169;&#24335;&#65292;&#20174;&#32780;&#23545;&#29983;&#25104;&#30340;&#38382;&#39064;&#36827;&#34892;&#25490;&#24207;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#20248;&#20110;&#29616;&#26377;&#30340;QG&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question Generation (QG) is a task of Natural Language Processing (NLP) that aims at automatically generating questions from text. Many applications can benefit from automatically generated questions, but often it is necessary to curate those questions, either by selecting or editing them. This task is informative on its own, but it is typically done post-generation, and, thus, the effort is wasted. In addition, most existing systems cannot incorporate this feedback back into them easily. In this work, we present a system, GEN, that learns from such (implicit) feedback. Following a pattern-based approach, it takes as input a small set of sentence/question pairs and creates patterns which are then applied to new unseen sentences. Each generated question, after being corrected by the user, is used as a new seed in the next iteration, so more patterns are created each time. We also take advantage of the corrections made by the user to score the patterns and therefore rank the generated qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#31216;&#21452;&#32534;&#30721;&#31264;&#23494;&#26816;&#32034;&#26694;&#26550;DEDR&#65292;&#20197;&#24357;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#31354;&#38388;&#24046;&#36317;&#65292;&#24182;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;MM-FiD&#65292;&#19968;&#31181;&#22810;&#27169;&#24335;&#34701;&#21512;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#65292;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13649</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#30340;&#23545;&#31216;&#21452;&#32534;&#30721;&#31264;&#23494;&#26816;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering. (arXiv:2304.13649v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#31216;&#21452;&#32534;&#30721;&#31264;&#23494;&#26816;&#32034;&#26694;&#26550;DEDR&#65292;&#20197;&#24357;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#31354;&#38388;&#24046;&#36317;&#65292;&#24182;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;MM-FiD&#65292;&#19968;&#31181;&#22810;&#27169;&#24335;&#34701;&#21512;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#65292;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#65288;KI-VQA&#65289;&#26159;&#25351;&#22238;&#31572;&#20851;&#20110;&#22270;&#20687;&#30340;&#38382;&#39064;&#65292;&#20854;&#31572;&#26696;&#19981;&#22312;&#22270;&#20687;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KI-VQA&#20219;&#21153;&#27969;&#31243;&#65292;&#21253;&#25324;&#19968;&#20010;&#26816;&#32034;&#22120;&#21644;&#19968;&#20010;&#38405;&#35835;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DEDR&#65292;&#23427;&#26159;&#19968;&#31181;&#23545;&#31216;&#21452;&#32534;&#30721;&#23494;&#38598;&#26816;&#32034;&#26694;&#26550;&#65292;&#20854;&#20013;&#20351;&#29992;&#21333;&#27169;&#65288;&#25991;&#26412;&#65289;&#21644;&#22810;&#27169;&#32534;&#30721;&#22120;&#23558;&#25991;&#26723;&#21644;&#26597;&#35810;&#32534;&#30721;&#20026;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36845;&#20195;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#24357;&#21512;&#36825;&#20004;&#20010;&#32534;&#30721;&#22120;&#20013;&#30340;&#34920;&#31034;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23545;&#20004;&#20010;&#25104;&#29087;&#30340;KI-VQA&#25968;&#25454;&#38598;OK-VQA&#21644;FVQA&#36827;&#34892;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;DEDR&#22312;OK-VQA&#21644;FVQA&#19978;&#30340;&#24615;&#33021;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#20998;&#21035;&#25552;&#39640;&#20102;11.6&#65285;&#21644;30.9&#65285;&#12290;&#21033;&#29992;DEDR&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#65292;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;MM-FiD&#65292;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#22810;&#27169;&#24335;&#34701;&#21512;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#20026;KI-VQA&#20219;&#21153;&#29983;&#25104;&#25991;&#26412;&#31572;&#26696;&#12290;MM-FiD&#23558;&#38382;&#39064;&#12289;&#22270;&#20687;&#21644;&#27599;&#20010;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#32534;&#30721;&#20026;&#21333;&#29420;&#30340;&#21521;&#37327;&#65292;&#24182;&#36890;&#36807;&#20174;&#23427;&#20204;&#30340;&#36830;&#25509;&#35299;&#30721;&#29983;&#25104;&#31572;&#26696;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MM-FiD&#22312;OK-VQA&#21644;FVQA&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a question about an image whose answer does not lie in the image. This paper presents a new pipeline for KI-VQA tasks, consisting of a retriever and a reader. First, we introduce DEDR, a symmetric dual encoding dense retrieval framework in which documents and queries are encoded into a shared embedding space using uni-modal (textual) and multi-modal encoders. We introduce an iterative knowledge distillation approach that bridges the gap between the representation spaces in these two encoders. Extensive evaluation on two well-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR outperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA, respectively. Utilizing the passages retrieved by DEDR, we further introduce MM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating a textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and each retri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;84000&#22810;&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#34429;&#28982;&#24471;&#20998;&#19981;&#38169;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#38169;&#35273;&#12289;&#28431;&#25481;&#37325;&#35201;&#25968;&#25454;&#28857;&#20197;&#21450;&#19981;&#27491;&#30830;&#35299;&#37322;&#22797;&#26434;&#36235;&#21183;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13620</link><description>&lt;p&gt;
ChartSumm&#65306;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#20840;&#38754;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries. (arXiv:2304.13620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;84000&#22810;&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#34429;&#28982;&#24471;&#20998;&#19981;&#38169;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#38169;&#35273;&#12289;&#28431;&#25481;&#37325;&#35201;&#25968;&#25454;&#28857;&#20197;&#21450;&#19981;&#27491;&#30830;&#35299;&#37322;&#22797;&#26434;&#36235;&#21183;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23558;&#22270;&#34920;&#36716;&#25442;&#20026;&#25991;&#26412;&#25688;&#35201;&#26159;&#35270;&#38556;&#20154;&#22763;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#21516;&#26102;&#20026;&#29992;&#25143;&#25552;&#20379;&#34920;&#26684;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#31934;&#30830;&#27934;&#23519;&#21147;&#12290;&#22823;&#22411;&#12289;&#32467;&#26500;&#33391;&#22909;&#30340;&#25968;&#25454;&#38598;&#22987;&#32456;&#26159;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20849;84363&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#65292;&#28085;&#30422;&#24191;&#27867;&#30340;&#20027;&#39064;&#21644;&#22270;&#34920;&#31867;&#22411;&#65292;&#21487;&#29983;&#25104;&#38271;&#30701;&#25688;&#35201;&#12290;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23454;&#29616;&#21508;&#31181;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#30340;&#24471;&#20998;&#26469;&#29983;&#25104;&#27969;&#30021;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#25688;&#35201;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#36935;&#21040;&#19968;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#20135;&#29983;&#38169;&#35273;&#65292;&#28431;&#25481;&#37325;&#35201;&#30340;&#25968;&#25454;&#28857;&#65292;&#20197;&#21450;&#19981;&#27491;&#30830;&#22320;&#35299;&#37322;&#22270;&#34920;&#20013;&#30340;&#22797;&#26434;&#36235;&#21183;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#24037;&#20855;&#25506;&#35752;&#20102;&#23558;ChartSumm&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#30340;&#28508;&#21147;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25104;&#20026;&#19968;&#20010;&#26377;&#25361;&#25112;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic chart to text summarization is an effective tool for the visually impaired people along with providing precise insights of tabular data in natural language to the user. A large and well-structured dataset is always a key part for data driven models. In this paper, we propose ChartSumm: a large-scale benchmark dataset consisting of a total of 84,363 charts along with their metadata and descriptions covering a wide range of topics and chart types to generate short and long summaries. Extensive experiments with strong baseline models show that even though these models generate fluent and informative summaries by achieving decent scores in various automatic evaluation metrics, they often face issues like suffering from hallucination, missing out important data points, in addition to incorrect explanation of complex trends in the charts. We also investigated the potential of expanding ChartSumm to other languages using automated translation tools. These make our dataset a challeng
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#21333;&#35789;&#34920;&#24449;&#25429;&#25417;&#21040;&#20102;&#26080;&#27495;&#20041;&#12289;&#24322;&#20041;&#21644;&#22810;&#20041;&#35789;&#20043;&#38388;&#30340;&#32454;&#24494;&#26377;&#24847;&#20041;&#24046;&#21035;&#65292;&#20026;&#23545;&#35789;&#27719;&#27495;&#20041;&#30340;&#24515;&#29702;&#23398;&#27010;&#24565;&#21270;&#25552;&#20379;&#20102;&#23450;&#37327;&#25903;&#25345;&#20197;&#21450;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#22914;&#20309;&#24433;&#21709;&#21333;&#35789;&#34920;&#31034;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.13597</link><description>&lt;p&gt;
&#24847;&#24605;&#30340;&#22810;&#20041;&#24615;&#65306;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#25581;&#31034;&#27169;&#31946;&#35789;&#27719;&#34920;&#24449;&#30340;&#20960;&#20309;&#23398;
&lt;/p&gt;
&lt;p&gt;
Shades of meaning: Uncovering the geometry of ambiguous word representations through contextualised language models. (arXiv:2304.13597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#21333;&#35789;&#34920;&#24449;&#25429;&#25417;&#21040;&#20102;&#26080;&#27495;&#20041;&#12289;&#24322;&#20041;&#21644;&#22810;&#20041;&#35789;&#20043;&#38388;&#30340;&#32454;&#24494;&#26377;&#24847;&#20041;&#24046;&#21035;&#65292;&#20026;&#23545;&#35789;&#27719;&#27495;&#20041;&#30340;&#24515;&#29702;&#23398;&#27010;&#24565;&#21270;&#25552;&#20379;&#20102;&#23450;&#37327;&#25903;&#25345;&#20197;&#21450;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#22914;&#20309;&#24433;&#21709;&#21333;&#35789;&#34920;&#31034;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#27495;&#20041;&#26159;&#35821;&#35328;&#31185;&#23398;&#20013;&#19968;&#20010;&#28145;&#21051;&#32780;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#19968;&#31995;&#21015;&#27169;&#25311;&#30740;&#31350;&#65292;&#21033;&#29992;&#26368;&#36817;&#22312;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#23545;&#35789;&#27719;&#27495;&#20041;&#24515;&#29702;&#29702;&#35299;&#30340;&#26032;&#35265;&#35299;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#26412;&#36523;&#24182;&#27809;&#26377;&#20219;&#20309;&#19982;&#35789;&#35821;&#24847;&#24605;&#30456;&#20851;&#30340;&#29702;&#35299;&#65292;&#23427;&#20204;&#21482;&#26159;&#23398;&#20064;&#22522;&#20110;&#20854;&#20182;&#35789;&#27719;&#25552;&#20379;&#30340;&#21608;&#22260;&#29615;&#22659;&#26469;&#39044;&#27979;&#35789;&#35821;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#20204;&#30340;&#34920;&#24449;&#25429;&#25417;&#21040;&#21487;&#20197;&#19982;&#35789;&#20856;&#20998;&#31867;&#21644;&#24515;&#29702;&#29702;&#35770;&#30456;&#19968;&#33268;&#30340;&#65292;&#23545;&#20110;&#26080;&#27495;&#20041;&#12289;&#24322;&#20041;&#21644;&#22810;&#20041;&#35789;&#20043;&#38388;&#30340;&#32454;&#24494;&#26377;&#24847;&#20041;&#24046;&#21035;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#29616;&#20195;&#24515;&#29702;&#23398;&#23545;&#35789;&#27719;&#27495;&#20041;&#30340;&#27010;&#24565;&#21270;&#25552;&#20379;&#20102;&#23450;&#37327;&#25903;&#25345;&#65292;&#24182;&#25552;&#20986;&#20102;&#29702;&#35299;&#19978;&#19979;&#25991;&#20449;&#24687;&#22914;&#20309;&#24433;&#21709;&#21333;&#35789;&#34920;&#31034;&#30340;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexical ambiguity presents a profound and enduring challenge to the language sciences. Researchers for decades have grappled with the problem of how language users learn, represent and process words with more than one meaning. Our work offers new insight into psychological understanding of lexical ambiguity through a series of simulations that capitalise on recent advances in contextual language models. These models have no grounded understanding of the meanings of words at all; they simply learn to predict words based on the surrounding context provided by other words. Yet, our analyses show that their representations capture fine-grained meaningful distinctions between unambiguous, homonymous, and polysemous words that align with lexicographic classifications and psychological theorising. These findings provide quantitative support for modern psychological conceptualisations of lexical ambiguity and raise new challenges for understanding of the way that contextual information shapes 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#32500;&#22522;&#30334;&#31185;&#20845;&#31181;&#26368;&#27963;&#36291;&#30340;&#35821;&#35328;&#29256;&#26412;&#20013;8.5&#19975;&#21517;&#32534;&#36753;&#30340;&#29992;&#25143;&#23545;&#35805;&#39029;&#38754;&#19978;&#21457;&#34920;&#30340;5700&#19975;&#26465;&#35780;&#35770;&#65292;&#22312;&#30701;&#26399;&#20869;&#21457;&#29616;&#27602;&#24615;&#35780;&#35770;&#19981;&#26029;&#38477;&#20302;&#32534;&#36753;&#30340;&#27963;&#36291;&#31243;&#24230;&#65292;&#24182;&#23548;&#33268;&#27599;&#20010;&#29992;&#25143;&#30340;&#27963;&#36291;&#22825;&#25968;&#20272;&#35745;&#20943;&#23569;&#33267;&#23569;0.5-2&#22825;&#65292;&#38271;&#26399;&#25928;&#24212;&#26356;&#20026;&#26174;&#33879;&#65292;&#22240;&#20026;&#23427;&#20204;&#26174;&#30528;&#22686;&#21152;&#20102;&#32534;&#36753;&#31163;&#24320;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.13568</link><description>&lt;p&gt;
&#27602;&#24615;&#35780;&#35770;&#20943;&#23569;&#32500;&#22522;&#30334;&#31185;&#24535;&#24895;&#32534;&#36753;&#30340;&#27963;&#36291;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Toxic comments reduce the activity of volunteer editors on Wikipedia. (arXiv:2304.13568v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#32500;&#22522;&#30334;&#31185;&#20845;&#31181;&#26368;&#27963;&#36291;&#30340;&#35821;&#35328;&#29256;&#26412;&#20013;8.5&#19975;&#21517;&#32534;&#36753;&#30340;&#29992;&#25143;&#23545;&#35805;&#39029;&#38754;&#19978;&#21457;&#34920;&#30340;5700&#19975;&#26465;&#35780;&#35770;&#65292;&#22312;&#30701;&#26399;&#20869;&#21457;&#29616;&#27602;&#24615;&#35780;&#35770;&#19981;&#26029;&#38477;&#20302;&#32534;&#36753;&#30340;&#27963;&#36291;&#31243;&#24230;&#65292;&#24182;&#23548;&#33268;&#27599;&#20010;&#29992;&#25143;&#30340;&#27963;&#36291;&#22825;&#25968;&#20272;&#35745;&#20943;&#23569;&#33267;&#23569;0.5-2&#22825;&#65292;&#38271;&#26399;&#25928;&#24212;&#26356;&#20026;&#26174;&#33879;&#65292;&#22240;&#20026;&#23427;&#20204;&#26174;&#30528;&#22686;&#21152;&#20102;&#32534;&#36753;&#31163;&#24320;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#22522;&#30334;&#31185;&#26159;&#21382;&#21490;&#19978;&#26368;&#25104;&#21151;&#30340;&#21327;&#20316;&#39033;&#30446;&#20043;&#19968;&#65292;&#26159;&#26377;&#21490;&#20197;&#26469;&#26368;&#22823;&#30340;&#30334;&#31185;&#20840;&#20070;&#65292;&#20840;&#29699;&#25968;&#30334;&#19975;&#29992;&#25143;&#20381;&#36182;&#23427;&#20316;&#20026;&#20449;&#24687;&#21644;&#20107;&#23454;&#26680;&#26597;&#20197;&#21450;&#28145;&#20837;&#30740;&#31350;&#30340;&#31532;&#19968;&#26469;&#28304;&#12290;&#30001;&#20110;&#32500;&#22522;&#30334;&#31185;&#23436;&#20840;&#20381;&#36182;&#20854;&#24535;&#24895;&#32534;&#36753;&#30340;&#21162;&#21147;&#65292;&#22240;&#27492;&#24433;&#21709;&#21040;&#32534;&#36753;&#30340;&#27602;&#24615;&#35328;&#35770;&#21487;&#33021;&#29305;&#21035;&#26174;&#30528;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20845;&#31181;&#26368;&#27963;&#36291;&#30340;&#32500;&#22522;&#30334;&#31185;&#35821;&#35328;&#29256;&#26412;&#20013;8.5&#19975;&#21517;&#32534;&#36753;&#30340;&#29992;&#25143;&#23545;&#35805;&#39029;&#38754;&#19978;&#21457;&#34920;&#30340;5700&#19975;&#26465;&#35780;&#35770;&#65292;&#20197;&#30740;&#31350;&#27602;&#24615;&#23545;&#32534;&#36753;&#34892;&#20026;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27602;&#24615;&#35780;&#35770;&#19981;&#26029;&#38477;&#20302;&#32534;&#36753;&#30340;&#27963;&#36291;&#31243;&#24230;&#65292;&#23548;&#33268;&#27599;&#20010;&#29992;&#25143;&#22312;&#30701;&#26399;&#20869;&#30340;&#27963;&#36291;&#22825;&#25968;&#20272;&#35745;&#25439;&#22833;0.5-2&#22825;&#12290;&#32771;&#34385;&#21040;&#32500;&#22522;&#30334;&#31185;&#30340;&#27963;&#36291;&#36129;&#29486;&#32773;&#25968;&#37327;&#65292;&#36825;&#30456;&#24403;&#20110;&#22810;&#24180;&#30340;&#20154;&#21147;&#25104;&#26524;&#25439;&#22833;&#12290;&#27602;&#24615;&#35780;&#35770;&#30340;&#24433;&#21709;&#22312;&#38271;&#26399;&#26356;&#20026;&#26174;&#33879;&#65292;&#22240;&#20026;&#23427;&#20204;&#26174;&#30528;&#22686;&#21152;&#20102;&#32534;&#36753;&#31163;&#24320;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wikipedia is one of the most successful collaborative projects in history. It is the largest encyclopedia ever created, with millions of users worldwide relying on it as the first source of information as well as for fact-checking and in-depth research. As Wikipedia relies solely on the efforts of its volunteer-editors, its success might be particularly affected by toxic speech. In this paper, we analyze all 57 million comments made on user talk pages of 8.5 million editors across the six most active language editions of Wikipedia to study the potential impact of toxicity on editors' behaviour. We find that toxic comments consistently reduce the activity of editors, leading to an estimated loss of 0.5-2 active days per user in the short term. This amounts to multiple human-years of lost productivity when considering the number of active contributors to Wikipedia. The effects of toxic comments are even greater in the long term, as they significantly increase the risk of editors leaving 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;</title><link>http://arxiv.org/abs/2304.13567</link><description>&lt;p&gt;
&#20301;&#32622;&#20559;&#24046;&#23545;token&#20998;&#31867;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13567
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#25110;&#35789;&#24615;&#26631;&#27880;&#31561;&#19979;&#28216;&#20219;&#21153;&#24050;&#30693;&#23384;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#36127;&#31034;&#20363;&#30340;&#27604;&#20363;&#21644;&#31867;&#19981;&#24179;&#34913;&#26041;&#38754;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#65292;&#21363;token&#20998;&#31867;&#20219;&#21153;&#20013;&#27491;&#31034;&#20363;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;Token&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20301;&#32622;&#20559;&#24046;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;CoNLL03&#21644;OntoNote5.0&#29992;&#20110;NER&#65292;English Tree Bank UD_en&#21644;TweeBank&#29992;&#20110;POS&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30740;&#31350;Transformer&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#20687;BERT&#12289;ERNIE&#12289;ELECTRA&#36825;&#26679;&#30340;&#32534;&#30721;&#22120;&#21644;&#20687;GPT2 &#21644;BLOOM&#36825;&#26679;&#30340;&#35299;&#30721;&#22120;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have shown state-of-the-art performance in Natural Language Processing (NLP) tasks. Downstream tasks such as Named Entity Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data imbalance issues, specifically in terms of the ratio of positive to negative examples, and class imbalance. In this paper, we investigate an additional specific issue for language models, namely the position bias of positive examples in token classification tasks. Therefore, we conduct an in-depth evaluation of the impact of position bias on the performance of LMs when fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We propose an evaluation approach to investigate position bias in Transformer models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in their performan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25968;&#25454;&#24211;&#31995;&#32479;MMDB&#65292;&#23427;&#21487;&#20197;&#26080;&#32541;&#26597;&#35810;&#25991;&#26412;&#21644;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#25152;&#35859;&#30340;&#22810;&#27169;&#24577;&#36816;&#31639;&#31526;&#65288;MMOps&#65289;&#23454;&#29616;&#25991;&#26412;&#38598;&#21512;&#30340;&#36716;&#25442;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#23545;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#26102;&#65292;&#20063;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.13559</link><description>&lt;p&gt;
&#23454;&#29616;&#26080;&#32541;&#26597;&#35810;&#25991;&#26412;&#21644;&#34920;&#26684;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#24211;&#31649;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables. (arXiv:2304.13559v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13559
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25968;&#25454;&#24211;&#31995;&#32479;MMDB&#65292;&#23427;&#21487;&#20197;&#26080;&#32541;&#26597;&#35810;&#25991;&#26412;&#21644;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#25152;&#35859;&#30340;&#22810;&#27169;&#24577;&#36816;&#31639;&#31526;&#65288;MMOps&#65289;&#23454;&#29616;&#25991;&#26412;&#38598;&#21512;&#30340;&#36716;&#25442;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#23545;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#26102;&#65292;&#20063;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25968;&#25454;&#24211;&#31995;&#32479;&#65292;&#21363;&#22810;&#27169;&#24577;&#25968;&#25454;&#24211;&#65288;MMDBs&#65289;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;SQL&#35821;&#35328;&#26080;&#32541;&#22320;&#26597;&#35810;&#25991;&#26412;&#21644;&#34920;&#26684;&#25968;&#25454;&#12290;&#20026;&#20102;&#23454;&#29616;&#22312;MMDB&#20013;&#20351;&#29992;SQL&#35821;&#35328;&#36827;&#34892;&#25991;&#26412;&#25968;&#25454;&#30340;&#26080;&#32541;&#26597;&#35810;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25152;&#35859;&#30340;&#22810;&#27169;&#24577;&#36816;&#31639;&#31526;&#65288;MMOps&#65289;&#65292;&#23427;&#20204;&#22522;&#20110;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-3&#65289;&#30340;&#36827;&#23637;&#12290;MMOps&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#65292;&#23427;&#20204;&#20801;&#35768;&#23558;&#25991;&#26412;&#38598;&#21512;&#20316;&#20026;&#34920;&#26684;&#36827;&#34892;&#22788;&#29702;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#36716;&#25442;&#25968;&#25454;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;&#35780;&#20272;&#20013;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#25105;&#20204;&#30340;MMDB&#21407;&#22411;&#19981;&#20165;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#22914;&#25991;&#26412;&#36716;&#34920;&#65289;&#65292;&#32780;&#19988;&#22312;&#23545;&#19968;&#20010;&#26410;&#35265;&#36807;&#30340;&#25991;&#26412;&#38598;&#21512;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#26102;&#65292;&#38656;&#35201;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#20063;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Multi-Modal Databases (MMDBs), which is a new class of database systems that can seamlessly query text and tables using SQL. To enable seamless querying of textual data using SQL in an MMDB, we propose to extend relational databases with so-called multi-modal operators (MMOps) which are based on the advances of recent large language models such as GPT-3. The main idea of MMOps is that they allow text collections to be treated as tables without the need to manually transform the data. As we show in our evaluation, our MMDB prototype can not only outperform state-of-the-art approaches such as text-to-table in terms of accuracy and performance but it also requires significantly less training data to fine-tune the model for an unseen text collection.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#22312;&#33521;&#35821;&#21644;&#26085;&#35821;&#20043;&#38388;&#30340;&#32763;&#35793;&#20013;&#23384;&#22312;&#30007;&#24615;&#20195;&#35789;&#20559;&#35265;&#65292;&#21516;&#26102;&#20063;&#26816;&#27979;&#21040;&#20102;&#23545;&#22899;&#24615;&#12289;&#20013;&#24615;&#21644;/&#25110;&#38750;&#20108;&#20803;&#20195;&#35789;&#23384;&#22312;&#30340;&#24494;&#22937;&#21453;&#24212;&#30340;&#20559;&#35265;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20195;&#35789;&#32763;&#35793;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23558;&#22797;&#25968;&#23884;&#20837;NLP&#25968;&#25454;&#38598;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.13557</link><description>&lt;p&gt;
&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#20013;&#8220;&#25105;&#8221;&#36855;&#22833;&#22312;&#32763;&#35793;&#20013;&#65306;&#20195;&#35789;&#38169;&#35823;&#27493;&#39588;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
"I'm" Lost in Translation: Pronoun Missteps in Crowdsourced Data Sets. (arXiv:2304.13557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13557
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#22312;&#33521;&#35821;&#21644;&#26085;&#35821;&#20043;&#38388;&#30340;&#32763;&#35793;&#20013;&#23384;&#22312;&#30007;&#24615;&#20195;&#35789;&#20559;&#35265;&#65292;&#21516;&#26102;&#20063;&#26816;&#27979;&#21040;&#20102;&#23545;&#22899;&#24615;&#12289;&#20013;&#24615;&#21644;/&#25110;&#38750;&#20108;&#20803;&#20195;&#35789;&#23384;&#22312;&#30340;&#24494;&#22937;&#21453;&#24212;&#30340;&#20559;&#35265;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20195;&#35789;&#32763;&#35793;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23558;&#22797;&#25968;&#23884;&#20837;NLP&#25968;&#25454;&#38598;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#34394;&#25311;&#21161;&#25163;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#26222;&#21450;&#65292;&#36234;&#26469;&#36234;&#38656;&#35201;&#36825;&#20123;&#35821;&#38899;&#31995;&#32479;&#20197;&#21508;&#31181;&#35821;&#35328;&#33258;&#28982;&#22320;&#36827;&#34892;&#20132;&#27969;&#12290;&#20247;&#21253;&#20513;&#35758;&#24050;&#32463;&#19987;&#27880;&#20110;&#23545;&#22823;&#22411;&#24320;&#25918;&#25968;&#25454;&#38598;&#36827;&#34892;&#22810;&#35821;&#35328;&#32763;&#35793;&#65292;&#20197;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#32763;&#35793;&#36890;&#24120;&#19981;&#26159;&#19968;&#23545;&#19968;&#30340;&#65292;&#24182;&#19988;&#20559;&#35265;&#21487;&#33021;&#20250;&#36880;&#28176;&#28183;&#20837;&#12290;&#22312;&#36825;&#39033;&#26368;&#26032;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20102;&#22312;&#20247;&#21253;Tatoeba&#25968;&#25454;&#24211;&#20013;&#33521;&#35821;&#21644;&#26085;&#35821;&#20043;&#38388;&#32763;&#35793;&#30340;&#20195;&#35789;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#25972;&#20307;&#19978;&#23384;&#22312;&#30007;&#24615;&#20195;&#35789;&#20559;&#35265;&#65292;&#21363;&#20351;&#22312;&#20854;&#20182;&#26041;&#24335;&#20013;&#32771;&#34385;&#21040;&#35821;&#35328;&#30340;&#22797;&#25968;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#26816;&#27979;&#21040;&#32763;&#35793;&#36807;&#31243;&#20013;&#21453;&#26144;&#20102;&#23545;&#22899;&#24615;&#12289;&#20013;&#24615;&#21644;/&#25110;&#38750;&#20108;&#20803;&#20195;&#35789;&#23384;&#22312;&#30340;&#24494;&#22937;&#21453;&#24212;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20195;&#35789;&#32763;&#35793;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23558;&#22797;&#25968;&#23884;&#20837;NLP&#25968;&#25454;&#38598;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As virtual assistants continue to be taken up globally, there is an ever-greater need for these speech-based systems to communicate naturally in a variety of languages. Crowdsourcing initiatives have focused on multilingual translation of big, open data sets for use in natural language processing (NLP). Yet, language translation is often not one-to-one, and biases can trickle in. In this late-breaking work, we focus on the case of pronouns translated between English and Japanese in the crowdsourced Tatoeba database. We found that masculine pronoun biases were present overall, even though plurality in language was accounted for in other ways. Importantly, we detected biases in the translation process that reflect nuanced reactions to the presence of feminine, neutral, and/or non-binary pronouns. We raise the issue of translation bias for pronouns and offer a practical solution to embed plurality in NLP data sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20351;&#29992;ChatGPT&#20316;&#20026;&#22810;&#32500;&#35780;&#20272;&#22120;&#65292;&#27979;&#35797;&#20854;&#22312;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#21644;&#20154;&#31867;&#21028;&#26029;&#36827;&#34892;&#27604;&#36739;&#12290;&#19982;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#30456;&#27604;&#65292;ChatGPT&#22312;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.13462</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#22810;&#32500;&#35780;&#20272;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Multidimensional Evaluation for Text Style Transfer Using ChatGPT. (arXiv:2304.13462v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20351;&#29992;ChatGPT&#20316;&#20026;&#22810;&#32500;&#35780;&#20272;&#22120;&#65292;&#27979;&#35797;&#20854;&#22312;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#21644;&#20154;&#31867;&#21028;&#26029;&#36827;&#34892;&#27604;&#36739;&#12290;&#19982;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#30456;&#27604;&#65292;ChatGPT&#22312;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;ChatGPT&#20316;&#20026;&#19968;&#31181;&#22810;&#32500;&#35780;&#20272;&#22120;&#26469;&#35780;&#20272;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#30340;&#28508;&#21147;&#65292;&#19982;&#29616;&#26377;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#21644;&#20154;&#31867;&#21028;&#26029;&#30456;&#27604;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#38646;-shot&#35774;&#32622;&#65292;&#21363;&#20351;&#29992;&#29305;&#23450;&#30340;&#20219;&#21153;&#25351;&#20196;&#25552;&#31034;ChatGPT&#65292;&#22312;&#39118;&#26684;&#24378;&#24230;&#12289;&#20869;&#23481;&#20445;&#30041;&#21644;&#27969;&#30021;&#24230;&#19977;&#20010;&#24120;&#29992;&#30340;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#35780;&#20272;&#32500;&#24230;&#19978;&#27979;&#35797;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#32423;&#21035;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30456;&#20851;&#20998;&#26512;&#65292;&#27604;&#36739;&#20102;&#20004;&#20010;&#36716;&#25442;&#26041;&#21521;&#65288;&#20197;&#21450;&#24635;&#20307;&#65289;&#30340;&#32467;&#26524;&#12290;&#19982;&#29616;&#26377;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#30456;&#27604;&#65292;ChatGPT&#22312;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20123;&#21021;&#27493;&#32467;&#26524;&#26377;&#26395;&#39318;&#27425;&#25581;&#31034;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#24425;&#33394;&#25991;&#26412;&#29983;&#25104;&#30340;&#22810;&#32500;&#35780;&#20272;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the potential of ChatGPT as a multidimensional evaluator for the task of \emph{Text Style Transfer}, alongside, and in comparison to, existing automatic metrics as well as human judgements. We focus on a zero-shot setting, i.e. prompting ChatGPT with specific task instructions, and test its performance on three commonly-used dimensions of text style transfer evaluation: style strength, content preservation, and fluency. We perform a comprehensive correlation analysis for two transfer directions (and overall) at different levels. Compared to existing automatic metrics, ChatGPT achieves competitive correlations with human judgments. These preliminary results are expected to provide a first glimpse into the role of large language models in the multidimensional evaluation of stylized text generation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25511;&#20869;&#23384;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13343</link><description>&lt;p&gt;
&#33258;&#25511;&#20869;&#23384;&#31995;&#32479;&#37322;&#25918;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#38480;&#36755;&#20837;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System. (arXiv:2304.13343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13343
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25511;&#20869;&#23384;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21463;&#21046;&#20110;&#26080;&#27861;&#22788;&#29702;&#36807;&#38271;&#30340;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25511;&#20869;&#23384;&#65288;SCM&#65289;&#31995;&#32479;&#65292;&#20197;&#37322;&#25918;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#38480;&#36755;&#20837;&#23481;&#37327;&#12290;&#25105;&#20204;&#30340;SCM&#31995;&#32479;&#30001;&#19977;&#20010;&#20851;&#38190;&#27169;&#22359;&#32452;&#25104;&#65306;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#12289;&#20869;&#23384;&#27969;&#21644;&#20869;&#23384;&#25511;&#21046;&#22120;&#12290;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#36845;&#20195;&#22320;&#22788;&#29702;&#36229;&#38271;&#36755;&#20837;&#65292;&#24182;&#23558;&#25152;&#26377;&#21382;&#21490;&#20449;&#24687;&#23384;&#20648;&#22312;&#20869;&#23384;&#27969;&#20013;&#12290;&#20869;&#23384;&#25511;&#21046;&#22120;&#20026;&#20195;&#29702;&#25552;&#20379;&#38271;&#26399;&#23384;&#20648;&#22120;&#65288;&#24402;&#26723;&#23384;&#20648;&#22120;&#65289;&#21644;&#30701;&#26399;&#23384;&#20648;&#22120;&#65288;&#38378;&#23384;&#65289;&#65292;&#20197;&#29983;&#25104;&#31934;&#30830;&#36830;&#36143;&#30340;&#21709;&#24212;&#12290;&#25511;&#21046;&#22120;&#30830;&#23450;&#24212;&#28608;&#27963;&#21738;&#20123;&#26469;&#33258;&#24402;&#26723;&#23384;&#20648;&#22120;&#30340;&#35760;&#24518;&#65292;&#24182;&#22914;&#20309;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;&#27169;&#22411;&#36755;&#20837;&#20013;&#12290;&#25105;&#20204;&#30340;SCM&#31995;&#32479;&#21487;&#20197;&#19982;&#20219;&#20309;LLMs&#38598;&#25104;&#65292;&#20197;&#20351;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#36229;&#38271;&#25991;&#26412;&#32780;&#26080;&#38656;&#20462;&#25913;&#25110;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;SCM&#31995;&#32479;&#20351;&#24471;LLMs&#33021;&#22815;&#22788;&#29702;&#38271;&#24230;&#39640;&#36798;8192&#20010;&#20196;&#29260;&#30340;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Language Models (LLMs) are constrained by their inability to process lengthy inputs. To address this limitation, we propose the Self-Controlled Memory (SCM) system to unleash infinite-length input capacity for large-scale language models. Our SCM system is composed of three key modules: the language model agent, the memory stream, and the memory controller. The language model agent iteratively processes ultra-long inputs and stores all historical information in the memory stream. The memory controller provides the agent with both long-term memory (archived memory) and short-term memory (flash memory) to generate precise and coherent responses. The controller determines which memories from archived memory should be activated and how to incorporate them into the model input. Our SCM system can be integrated with any LLMs to enable them to process ultra-long texts without any modification or fine-tuning. Experimental results show that our SCM system enables LLMs, which are not
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#35270;&#35282;&#65292;&#29992;&#20110;&#25551;&#36848;&#21487;&#20197;&#34987;&#36712;&#36947;&#26377;&#38480;&#30340;&#21517;&#20041;&#21333;&#23376;&#32676;&#35782;&#21035;&#30340;&#25968;&#25454;&#35821;&#35328;&#65292;&#24182;&#25506;&#35752;&#20102; pro-&#36712;&#36947;&#26377;&#38480;&#26041;&#31243;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.13337</link><description>&lt;p&gt;
&#25968;&#25454;&#35821;&#35328;&#30340;&#21517;&#20041;&#25299;&#25169;
&lt;/p&gt;
&lt;p&gt;
Nominal Topology for Data Languages. (arXiv:2304.13337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#35270;&#35282;&#65292;&#29992;&#20110;&#25551;&#36848;&#21487;&#20197;&#34987;&#36712;&#36947;&#26377;&#38480;&#30340;&#21517;&#20041;&#21333;&#23376;&#32676;&#35782;&#21035;&#30340;&#25968;&#25454;&#35821;&#35328;&#65292;&#24182;&#25506;&#35752;&#20102; pro-&#36712;&#36947;&#26377;&#38480;&#26041;&#31243;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#35270;&#35282;&#65292;&#29992;&#20110;&#25551;&#36848;&#21487;&#20197;&#34987;&#36712;&#36947;&#26377;&#38480;&#30340;&#21517;&#20041;&#21333;&#23376;&#32676;&#35782;&#21035;&#30340;&#25968;&#25454;&#35821;&#35328;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; pro- &#36712;&#36947;&#26377;&#38480;&#30340;&#21517;&#20041;&#25299;&#25169;&#31354;&#38388;&#12290;&#22312;&#20840;&#23616;&#26377;&#30028;&#25903;&#25345;&#22823;&#23567;&#30340;&#21069;&#25552;&#19979;&#65292;&#23427;&#20204;&#19982;&#21517;&#20041; Stone &#31354;&#38388;&#37325;&#21512;&#65292;&#24182;&#19988;&#34987;&#35777;&#26126;&#19982;&#21517;&#20041;&#24067;&#23572;&#20195;&#25968;&#30340;&#19968;&#20010;&#23376;&#33539;&#30068;&#21452;&#37325;&#31561;&#20215;&#12290;&#21487;&#35782;&#21035;&#30340;&#25968;&#25454;&#35821;&#35328;&#34987;&#34920;&#24449;&#20026; pro-&#36712;&#36947;&#26377;&#38480;&#21333;&#35789;&#30340;&#25299;&#25169;&#38381;&#24320;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435; Reiterman &#30340;&#20266;&#21464;&#31181;&#23450;&#29702;&#30340;&#21517;&#20041;&#29256;&#26412;&#65292;&#25506;&#35752;&#20102; pro-&#36712;&#36947;&#26377;&#38480;&#26041;&#31243;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel topological perspective on data languages recognizable by orbit-finite nominal monoids. For this purpose, we introduce pro-orbit-finite nominal topological spaces. Assuming globally bounded support sizes, they coincide with nominal Stone spaces and are shown to be dually equivalent to a subcategory of nominal boolean algebras. Recognizable data languages are characterized as topologically clopen sets of pro-orbit-finite words. In addition, we explore the expressive power of pro-orbit-finite equations by establishing a nominal version of Reiterman's pseudovariety theorem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#26694;&#26550;&#30340;&#36328;&#22495;&#25991;&#26412;&#21040;SQL&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19982;&#26696;&#20363;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#19981;&#33391;&#38480;&#21046;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13301</link><description>&lt;p&gt;
&#36328;&#22495;&#25991;&#26412;&#21040;SQL&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL. (arXiv:2304.13301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#26694;&#26550;&#30340;&#36328;&#22495;&#25991;&#26412;&#21040;SQL&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19982;&#26696;&#20363;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#19981;&#33391;&#38480;&#21046;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Codex&#12289;ChatGPT&#21644;GPT-4&#65289;&#22312;AI&#31038;&#21306;&#26041;&#38754;&#26377;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;SQL&#30340;&#20219;&#21153;&#12290;&#19968;&#20123;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#20204;&#26377;&#28508;&#21147;&#29983;&#25104;SQL&#26597;&#35810;&#65292;&#20294;&#26159;&#23427;&#20204;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#35774;&#35745;&#19981;&#33391;&#65288;&#20363;&#22914;&#31616;&#21333;&#30340;&#32467;&#26500;&#25110;&#38543;&#26426;&#25277;&#26679;&#65289;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#19981;&#24517;&#35201;&#25110;&#26080;&#20851;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CBR-ApSQL&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#65288;CBR&#65289;&#30340;&#26694;&#26550;&#65292;&#19982;GPT-3.5&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#23545;&#19982;&#26696;&#20363;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#36827;&#34892;&#31934;&#30830;&#25511;&#21046;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#36866;&#24212;&#25552;&#31034;&#65292;&#20197;&#28789;&#27963;&#35843;&#25972;GPT-3.5&#30340;&#36755;&#20837;&#65292;&#20854;&#20013;&#28041;&#21450;&#65288;1&#65289;&#36890;&#36807;&#21435;&#35821;&#20041;&#21270;&#36755;&#20837;&#38382;&#39064;&#26469;&#33258;&#36866;&#24212;&#26816;&#32034;&#26696;&#20363;&#65292;&#26681;&#25454;&#38382;&#39064;&#24847;&#22270;&#65292;&#20197;&#21450;&#65288;2&#65289;&#33258;&#36866;&#24212;&#22238;&#36864;&#26426;&#21046;&#65292;&#20197;&#30830;&#20445;&#25552;&#31034;&#30340;&#20449;&#24687;&#37327;&#21644;&#26696;&#20363;&#19982;&#25552;&#31034;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#21435;&#35821;&#20041;&#21270;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Semantic D
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs), such as Codex, ChatGPT and GPT-4 have significantly impacted the AI community, including Text-to-SQL tasks. Some evaluations and analyses on LLMs show their potential to generate SQL queries but they point out poorly designed prompts (e.g. simplistic construction or random sampling) limit LLMs' performance and may cause unnecessary or irrelevant outputs. To address these issues, we propose CBR-ApSQL, a Case-Based Reasoning (CBR)-based framework combined with GPT-3.5 for precise control over case-relevant and case-irrelevant knowledge in Text-to-SQL tasks. We design adaptive prompts for flexibly adjusting inputs for GPT-3.5, which involves (1) adaptively retrieving cases according to the question intention by de-semantizing the input question, and (2) an adaptive fallback mechanism to ensure the informativeness of the prompt, as well as the relevance between cases and the prompt. In the de-semanticization phase, we designed Semantic D
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#65306;&#36890;&#36807;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#26377;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.13273</link><description>&lt;p&gt;
&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#65306;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#30340;&#32431;&#25991;&#26412;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping. (arXiv:2304.13273v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#65306;&#36890;&#36807;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#26377;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20197;CLIP&#21644;ALIGN&#20026;&#20195;&#34920;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;CLIP&#30340;&#38646;-shot&#33021;&#21147;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#31561;&#22522;&#20110;&#20851;&#32852;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#20294;&#26159;&#65292;CLIP&#38590;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#30001;&#20110;&#32570;&#20047;&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;K&#26368;&#36817;&#37051;&#36328;&#27169;&#24577;&#26144;&#23556;&#65288;Knight&#65289;&#65292;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#12290;&#36890;&#36807;&#31364;&#23383;&#24149;&#20219;&#21153;&#30340;&#32431;&#25991;&#26412;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Knight&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#19988;&#23558;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of Vision-Language Pre-training Models (VLPMs) represented by CLIP and ALIGN, significant breakthroughs have been achieved for association-based visual tasks such as image classification and image-text retrieval by the zero-shot capability of CLIP without fine-tuning. However, CLIP is hard to apply to generation-based tasks. This is due to the lack of decoder architecture and pre-training tasks for generation. Although previous works have created generation capacity for CLIP through additional language models, a modality gap between the CLIP representations of different modalities and the inability of CLIP to model the offset of this gap, which fails the concept to transfer across modalities. To solve the problem, we try to map images/videos to the language modality and generate captions from the language modality. In this paper, we propose the K-nearest-neighbor Cross-modality Mapping (Knight), a zero-shot method from association to generation. With text-only unsu
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#27604;&#36739;&#20102;&#20195;&#30721;&#25552;&#31034;&#21644;&#25991;&#26412;&#25552;&#31034;&#22312;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21457;&#29616;&#38500;&#20102;&#23569;&#25968;&#20219;&#21153;&#22806;&#65292;&#20195;&#30721;&#25552;&#31034;&#24182;&#19981;&#24635;&#26159;&#27604;&#25991;&#26412;&#25552;&#31034;&#26356;&#22909;&#65292;&#20195;&#30721;&#25552;&#31034;&#39118;&#26684;&#23545;&#24615;&#33021;&#26377;&#19968;&#23450;&#24433;&#21709;&#65292;&#32780;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20195;&#30721;&#25552;&#31034;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13250</link><description>&lt;p&gt;
&#25506;&#31350;&#20195;&#30721;&#25552;&#31034;&#30340;&#31070;&#22855;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Exploring the Curious Case of Code Prompts. (arXiv:2304.13250v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13250
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#27604;&#36739;&#20102;&#20195;&#30721;&#25552;&#31034;&#21644;&#25991;&#26412;&#25552;&#31034;&#22312;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21457;&#29616;&#38500;&#20102;&#23569;&#25968;&#20219;&#21153;&#22806;&#65292;&#20195;&#30721;&#25552;&#31034;&#24182;&#19981;&#24635;&#26159;&#27604;&#25991;&#26412;&#25552;&#31034;&#26356;&#22909;&#65292;&#20195;&#30721;&#25552;&#31034;&#39118;&#26684;&#23545;&#24615;&#33021;&#26377;&#19968;&#23450;&#24433;&#21709;&#65292;&#32780;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20195;&#30721;&#25552;&#31034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#30340;&#20195;&#30721;&#34920;&#31034;&#19978;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#32467;&#26500;&#21270;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20219;&#21153;&#20165;&#26500;&#25104;&#25152;&#26377;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#22238;&#31572;&#26159;&#21542;&#20195;&#30721;&#25552;&#31034;&#26159;&#19968;&#33324;&#19982;&#35821;&#35328;&#27169;&#22411;&#20114;&#21160;&#30340;&#39318;&#36873;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#27969;&#34892;&#30340; GPT &#27169;&#22411; (davinci, code-davinci-002 &#21644; text-davinci-002) &#19978;&#27604;&#36739;&#20195;&#30721;&#25552;&#31034;&#21644;&#25991;&#26412;&#25552;&#31034;&#22312;&#26356;&#24191;&#27867;&#30340;&#20219;&#21153;&#36873;&#25321;&#19978; (&#22914; QA, &#24773;&#24863;&#20998;&#26512;, &#25688;&#35201;&#31561;) &#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#38500;&#20102;&#23569;&#25968;&#20363;&#22806;&#65292;&#20195;&#30721;&#25552;&#31034;&#24182;&#19981;&#24635;&#26159;&#27604;&#25991;&#26412;&#25552;&#31034;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20195;&#30721;&#25552;&#31034;&#39118;&#26684;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#23545;&#24615;&#33021;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#20294;&#24182;&#38750;&#25152;&#26377;&#20219;&#21153;&#37117;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#23545;&#25991;&#26412;&#35828;&#26126;&#36827;&#34892;&#24494;&#35843;&#20250;&#23548;&#33268;&#20195;&#30721;&#25552;&#31034;&#30456;&#23545;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that prompting language models with code-like representations of natural language leads to performance improvements on structured reasoning tasks. However, such tasks comprise only a small subset of all natural language tasks. In our work, we seek to answer whether or not code-prompting is the preferred way of interacting with language models in general. We compare code and text prompts across three popular GPT models (davinci, code-davinci-002, and text-davinci-002) on a broader selection of tasks (e.g., QA, sentiment, summarization) and find that with few exceptions, code prompts do not consistently outperform text prompts. Furthermore, we show that the style of code prompt has a large effect on performance for some but not all tasks and that fine-tuning on text instructions leads to better relative performance of code prompts.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#24515;&#29702;&#20581;&#24247;&#20250;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#26500;&#24314;&#36131;&#20219;VMHA&#65292;&#20197;&#25552;&#20986;&#21518;&#32493;&#38382;&#39064;&#25110;&#25552;&#20379;&#30693;&#24773;&#22238;&#24212;&#65292;&#20016;&#23500;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2304.13191</link><description>&lt;p&gt;
&#38754;&#21521;&#24515;&#29702;&#20581;&#24247;&#30340;&#21487;&#35299;&#37322;&#21644;&#23433;&#20840;&#30340;&#20250;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable and Safe Conversational Agents for Mental Health: A Survey. (arXiv:2304.13191v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13191
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#24515;&#29702;&#20581;&#24247;&#20250;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#26500;&#24314;&#36131;&#20219;VMHA&#65292;&#20197;&#25552;&#20986;&#21518;&#32493;&#38382;&#39064;&#25110;&#25552;&#20379;&#30693;&#24773;&#22238;&#24212;&#65292;&#20016;&#23500;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#24515;&#29702;&#20581;&#24247;&#21161;&#25163;&#65288;VMHA&#65289;&#22312;&#25345;&#32493;&#25512;&#36827;&#65292;&#20197;&#25903;&#25345;&#27599;&#24180;6000&#19975;&#27425;&#21021;&#32423;&#21307;&#30103;&#20445;&#20581;&#23601;&#35786;&#21644;600&#19975;&#27425;&#24613;&#35786;&#23460;&#23601;&#35786;&#30340;&#36127;&#25285;&#36807;&#37325;&#30340;&#20840;&#29699;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#26159;&#30001;&#20020;&#24202;&#24515;&#29702;&#23398;&#23478;&#12289;&#31934;&#31070;&#31185;&#21307;&#24072;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30740;&#31350;&#20154;&#21592;&#20026;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#65288;CBT&#65289;&#26500;&#24314;&#30340;&#12290;&#30446;&#21069;&#65292;VMHA&#30340;&#20316;&#29992;&#26159;&#36890;&#36807;&#20449;&#24687;&#25552;&#20379;&#24773;&#24863;&#25903;&#25345;&#65292;&#37325;&#28857;&#19981;&#22312;&#19982;&#24739;&#32773;&#36827;&#34892;&#28145;&#20837;&#30340;&#21453;&#24605;&#23545;&#35805;&#12290;&#38656;&#35201;&#26356;&#20840;&#38754;&#12289;&#23433;&#20840;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#36127;&#36131;&#20219;&#30340;VMHA&#65292;&#20197;&#25552;&#20986;&#21518;&#32493;&#38382;&#39064;&#25110;&#25552;&#20379;&#30693;&#24773;&#22238;&#24212;&#12290;&#36825;&#39033;&#35843;&#26597;&#23545;&#29616;&#26377;&#30340;&#24515;&#29702;&#20581;&#24247;&#20250;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25209;&#21028;&#24615;&#23457;&#26597;&#65292;&#38543;&#21518;&#25552;&#20986;&#20102;&#20851;&#20110;VMHA&#25913;&#36827;&#30340;&#26032;&#35265;&#35299;&#65292;&#21253;&#25324;&#29615;&#22659;&#30693;&#35782;&#12289;&#25968;&#25454;&#38598;&#21644;&#23427;&#20204;&#22312;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#20013;&#30340;&#26032;&#20852;&#35282;&#33394;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20016;&#23500;&#29992;&#25143;&#20307;&#39564;&#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual Mental Health Assistants (VMHAs) are seeing continual advancements to support the overburdened global healthcare system that gets 60 million primary care visits, and 6 million Emergency Room (ER) visits annually. These systems are built by clinical psychologists, psychiatrists, and Artificial Intelligence (AI) researchers for Cognitive Behavioral Therapy (CBT). At present, the role of VMHAs is to provide emotional support through information, focusing less on developing a reflective conversation with the patient. A more comprehensive, safe and explainable approach is required to build responsible VMHAs to ask follow-up questions or provide a well-informed response. This survey offers a systematic critical review of the existing conversational agents in mental health, followed by new insights into the improvements of VMHAs with contextual knowledge, datasets, and their emerging role in clinical decision support. We also provide new directions toward enriching the user experience
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TABLET&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;20&#20010;&#19981;&#21516;&#30340;&#21253;&#21547;&#25351;&#20196;&#27880;&#37322;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#39044;&#27979;&#38382;&#39064;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#35780;&#20272;&#20102;&#25351;&#20196;&#22312;&#20445;&#30495;&#24230;&#21644;LLM&#22312;&#34920;&#26684;&#39044;&#27979;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13188</link><description>&lt;p&gt;
TABLET&#65306;&#22522;&#20110;&#25351;&#20196;&#23398;&#20064;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
TABLET: Learning From Instructions For Tabular Data. (arXiv:2304.13188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13188
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TABLET&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;20&#20010;&#19981;&#21516;&#30340;&#21253;&#21547;&#25351;&#20196;&#27880;&#37322;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#39044;&#27979;&#38382;&#39064;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#35780;&#20272;&#20102;&#25351;&#20196;&#22312;&#20445;&#30495;&#24230;&#21644;LLM&#22312;&#34920;&#26684;&#39044;&#27979;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#34920;&#26684;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#36890;&#24120;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#25935;&#24863;&#21644;&#25104;&#26412;&#39640;&#30340;&#39046;&#22495;&#65292;&#27604;&#22914;&#21307;&#23398;&#21644;&#37329;&#34701;&#12290;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25552;&#20379;&#20102;&#21478;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25351;&#20196;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;LLM&#20013;&#30340;&#30693;&#35782;&#26469;&#35299;&#20915;&#34920;&#26684;&#39044;&#27979;&#38382;&#39064;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TABLET&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;20&#20010;&#19981;&#21516;&#30340;&#21253;&#21547;&#25351;&#20196;&#27880;&#37322;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#20123;&#25351;&#20196;&#22312;&#25514;&#36766;&#12289;&#32454;&#33410;&#21644;&#25216;&#26415;&#24615;&#26041;&#38754;&#21508;&#19981;&#30456;&#21516;&#12290;&#27492;&#22806;&#65292;TABLET&#36824;&#21253;&#25324;&#25351;&#20196;&#30340;&#36923;&#36753;&#21644;&#32467;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19978;&#19979;&#25991;&#25351;&#20196;&#30340;&#24110;&#21161;&#19979;&#65292;Flan-T5 11b&#30340;&#38646;&#31034;&#20363;F1&#24615;&#33021;&#24179;&#22343;&#25552;&#39640;&#20102;44&#65285;&#65292;&#22312;TABLET&#19978;&#65292;ChatGPT&#30340;&#25552;&#21319;&#20026;13&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25351;&#20196;&#20445;&#30495;&#24230;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;LLM&#36827;&#34892;&#34920;&#26684;&#39044;&#27979;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#36890;&#24120;&#20250;&#24573;&#30053;&#25351;&#20196;&#24182;&#20381;&#36182;&#20110;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring high-quality data is often a significant challenge in training machine learning (ML) models for tabular prediction, particularly in privacy-sensitive and costly domains like medicine and finance. Providing natural language instructions to large language models (LLMs) offers an alternative solution. However, it is unclear how effectively instructions leverage the knowledge in LLMs for solving tabular prediction problems. To address this gap, we introduce TABLET, a benchmark of 20 diverse tabular datasets annotated with instructions that vary in their phrasing, granularity, and technicality. Additionally, TABLET includes the instructions' logic and structured modifications to the instructions. We find in-context instructions increase zero-shot F1 performance for Flan-T5 11b by 44% on average and 13% for ChatGPT on TABLET. Also, we explore the limitations of using LLMs for tabular prediction in our benchmark by evaluating instruction faithfulness. We find LLMs often ignore instr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#20004;&#20010;NLP&#31995;&#32479;&#65306;&#19968;&#20010;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#19968;&#20010;&#20026;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#35777;&#25454;&#26816;&#32034;&#12290;&#23427;&#20204;&#20998;&#21035;&#37319;&#29992;&#20102;&#27969;&#27700;&#32447;&#27169;&#22411;&#21644;&#32852;&#21512;&#27169;&#22411;&#65292;&#24182;&#22312;&#26368;&#32456;&#30340;&#38598;&#25104;&#31995;&#32479;&#20013;&#34701;&#21512;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2304.13180</link><description>&lt;p&gt;
Sebis&#22312;SemEval-2023&#20219;&#21153;7&#20013;&#65306;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#20013;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#35777;&#25454;&#26816;&#32034;&#30340;&#32852;&#21512;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference and Evidence Retrieval from Clinical Trial Reports. (arXiv:2304.13180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#20004;&#20010;NLP&#31995;&#32479;&#65306;&#19968;&#20010;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#19968;&#20010;&#20026;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#35777;&#25454;&#26816;&#32034;&#12290;&#23427;&#20204;&#20998;&#21035;&#37319;&#29992;&#20102;&#27969;&#27700;&#32447;&#27169;&#22411;&#21644;&#32852;&#21512;&#27169;&#22411;&#65292;&#24182;&#22312;&#26368;&#32456;&#30340;&#38598;&#25104;&#31995;&#32479;&#20013;&#34701;&#21512;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27599;&#22825;&#20135;&#29983;&#30340;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#25968;&#37327;&#22686;&#21152;&#65292;&#36319;&#36827;&#21578;&#30693;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#30103;&#24314;&#35758;&#30340;&#26032;&#21457;&#29616;&#21464;&#24471;&#36234;&#26469;&#36234;&#38590;&#12290;&#20026;&#20102;&#24110;&#21161;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#24182;&#21327;&#21161;&#21307;&#30103;&#19987;&#23478;&#65292;&#27491;&#22312;&#24320;&#21457;NLP&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#28608;&#21457;&#20102;SemEval-2023&#20219;&#21153;7&#65292;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;NLP&#31995;&#32479;&#65292;&#20197;&#22788;&#29702;&#20174;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#20013;&#25552;&#21462;&#35777;&#25454;&#21644;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#20004;&#20010;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#25105;&#20204;&#24320;&#21457;&#30340;&#20004;&#20010;&#31995;&#32479;&#12290;&#31532;&#19968;&#20010;&#26159;&#27969;&#27700;&#32447;&#31995;&#32479;&#65292;&#21333;&#29420;&#24314;&#27169;&#20102;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#32780;&#31532;&#20108;&#20010;&#26159;&#32852;&#21512;&#31995;&#32479;&#65292;&#37319;&#29992;&#20849;&#20139;&#34920;&#31034;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#21516;&#26102;&#23398;&#20064;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;&#26368;&#32456;&#31995;&#32479;&#23558;&#23427;&#20204;&#30340;&#36755;&#20986;&#21512;&#24182;&#20026;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#27169;&#22411;&#65292;&#20171;&#32461;&#20854;&#29305;&#28857;&#21644;&#25361;&#25112;&#65292;&#24182;&#23545;&#23454;&#29616;&#30340;&#32467;&#26524;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing number of clinical trial reports generated every day, it is becoming hard to keep up with novel discoveries that inform evidence-based healthcare recommendations. To help automate this process and assist medical experts, NLP solutions are being developed. This motivated the SemEval-2023 Task 7, where the goal was to develop an NLP system for two tasks: evidence retrieval and natural language inference from clinical trial data. In this paper, we describe our two developed systems. The first one is a pipeline system that models the two tasks separately, while the second one is a joint system that learns the two tasks simultaneously with a shared representation and a multi-task learning approach. The final system combines their outputs in an ensemble system. We formalize the models, present their characteristics and challenges, and provide an analysis of achieved results.
&lt;/p&gt;</description></item><item><title>&#35752;&#35770;&#20102;&#34394;&#25311;&#21161;&#25163;&#20013;&#21475;&#35821;&#20449;&#24687;&#26597;&#35810;&#24314;&#27169;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#20013;&#30340;&#26426;&#36935;&#65307;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26597;&#35810;&#39046;&#22495;&#20998;&#31867;&#12289;&#30693;&#35782;&#22270;&#35889;&#31561;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65307;&#31616;&#35201;&#27010;&#36848;&#20102;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.13149</link><description>&lt;p&gt;
&#34394;&#25311;&#21161;&#25163;&#20013;&#21475;&#35821;&#20449;&#24687;&#26597;&#35810;&#30340;&#24314;&#27169;&#65306;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Modeling Spoken Information Queries for Virtual Assistants: Open Problems, Challenges and Opportunities. (arXiv:2304.13149v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13149
&lt;/p&gt;
&lt;p&gt;
&#35752;&#35770;&#20102;&#34394;&#25311;&#21161;&#25163;&#20013;&#21475;&#35821;&#20449;&#24687;&#26597;&#35810;&#24314;&#27169;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#20013;&#30340;&#26426;&#36935;&#65307;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26597;&#35810;&#39046;&#22495;&#20998;&#31867;&#12289;&#30693;&#35782;&#22270;&#35889;&#31561;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65307;&#31616;&#35201;&#27010;&#36848;&#20102;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#21161;&#25163;&#27491;&#22312;&#25104;&#20026;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;&#20449;&#24687;&#26816;&#32034;&#24179;&#21488;&#65292;&#23427;&#20204;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#34394;&#25311;&#21161;&#25163;&#21475;&#35821;&#20449;&#24687;&#26597;&#35810;&#24314;&#27169;&#30340;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#24182;&#21015;&#20986;&#20102;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#21644;&#30740;&#31350;&#21487;&#20197;&#24212;&#29992;&#20110;&#25552;&#39640;&#34394;&#25311;&#21161;&#25163;&#35821;&#38899;&#35782;&#21035;&#36136;&#37327;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#26597;&#35810;&#39046;&#22495;&#20998;&#31867;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#20197;&#21450;&#26597;&#35810;&#20010;&#24615;&#21270;&#26469;&#24110;&#21161;&#25913;&#21892;&#21475;&#35821;&#20449;&#24687;&#39046;&#22495;&#26597;&#35810;&#30340;&#20934;&#30830;&#35782;&#21035;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#31616;&#35201;&#27010;&#36848;&#20102;&#35821;&#38899;&#35782;&#21035;&#20013;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual assistants are becoming increasingly important speech-driven Information Retrieval platforms that assist users with various tasks.  We discuss open problems and challenges with respect to modeling spoken information queries for virtual assistants, and list opportunities where Information Retrieval methods and research can be applied to improve the quality of virtual assistant speech recognition.  We discuss how query domain classification, knowledge graphs and user interaction data, and query personalization can be helpful to improve the accurate recognition of spoken information domain queries. Finally, we also provide a brief overview of current problems and challenges in speech recognition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;ESimCSE&#26080;&#30417;&#30563;&#27604;&#36739;&#23398;&#20064;&#21644;UDA&#21322;&#30417;&#30563;&#27604;&#36739;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#25216;&#26415;&#35299;&#20915;&#20102;&#22823;&#26631;&#31614;&#31995;&#32479;&#25991;&#26412;&#20998;&#31867;&#30340;&#22810;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20934;&#30830;&#29575;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.13140</link><description>&lt;p&gt;
ESimCSE&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32852;&#21512;UDA&#21322;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#22823;&#26631;&#31614;&#31995;&#32479;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ESimCSE Unsupervised Contrastive Learning Jointly with UDA Semi-Supervised Learning for Large Label System Text Classification Mode. (arXiv:2304.13140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;ESimCSE&#26080;&#30417;&#30563;&#27604;&#36739;&#23398;&#20064;&#21644;UDA&#21322;&#30417;&#30563;&#27604;&#36739;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#25216;&#26415;&#35299;&#20915;&#20102;&#22823;&#26631;&#31614;&#31995;&#32479;&#25991;&#26412;&#20998;&#31867;&#30340;&#22810;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20934;&#30830;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#25991;&#26412;&#20998;&#31867;&#38754;&#20020;&#30340;&#25361;&#25112;&#21253;&#25324;&#22810;&#20010;&#26631;&#31614;&#31995;&#32479;&#12289;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#21644;&#39640;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#32852;&#21512;&#35757;&#32451;&#25216;&#26415;&#65292;&#23558;ESimCSE&#26080;&#30417;&#30563;&#27604;&#36739;&#23398;&#20064;&#21644;UDA&#21322;&#30417;&#30563;&#27604;&#36739;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;ESimCSE&#27169;&#22411;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#39640;&#25928;&#22320;&#23398;&#20064;&#25991;&#26412;&#21521;&#37327;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#65307;&#32780;UDA&#21017;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;FGM&#21644;PGD&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;Ruesters&#19978;&#26377;8%&#21644;10%&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenges faced by text classification with large tag systems in natural language processing tasks include multiple tag systems, uneven data distribution, and high noise. To address these problems, the ESimCSE unsupervised comparative learning and UDA semi-supervised comparative learning models are combined through the use of joint training techniques in the models.The ESimCSE model efficiently learns text vector representations using unlabeled data to achieve better classification results, while UDA is trained using unlabeled data through semi-supervised learning methods to improve the prediction performance of the models and stability, and further improve the generalization ability of the model. In addition, adversarial training techniques FGM and PGD are used in the model training process to improve the robustness and reliability of the model. The experimental results show that there is an 8% and 10% accuracy improvement relative to Baseline on the public dataset Ruesters as we
&lt;/p&gt;</description></item><item><title>LAST&#26159;&#19968;&#20010;&#22522;&#20110;JAX&#30340;&#24211;&#65292;&#29992;&#20110;&#23454;&#29616;&#28789;&#27963;&#26131;&#29992;&#30340;&#26684;&#27169;&#22411;&#35821;&#38899;&#36716;&#24405;&#22120;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#31995;&#21015;&#36866;&#29992;&#20110;&#35299;&#20915;&#29616;&#20195;&#26550;&#26500;&#24615;&#33021;&#29305;&#24449;&#21644;&#33258;&#21160;&#24494;&#20998;&#20013;&#30340;&#32454;&#24494;&#24046;&#21035;&#26032;&#25361;&#25112;&#30340;&#36890;&#29992;&#25216;&#26415;&#65292;&#24182;&#22312;TPUv3&#21644;V100 GPU&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.13134</link><description>&lt;p&gt;
LAST: &#22522;&#20110;JAX&#30340;&#21487;&#25193;&#23637;&#26684;&#27169;&#22411;&#35821;&#38899;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
LAST: Scalable Lattice-Based Speech Modelling in JAX. (arXiv:2304.13134v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13134
&lt;/p&gt;
&lt;p&gt;
LAST&#26159;&#19968;&#20010;&#22522;&#20110;JAX&#30340;&#24211;&#65292;&#29992;&#20110;&#23454;&#29616;&#28789;&#27963;&#26131;&#29992;&#30340;&#26684;&#27169;&#22411;&#35821;&#38899;&#36716;&#24405;&#22120;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#31995;&#21015;&#36866;&#29992;&#20110;&#35299;&#20915;&#29616;&#20195;&#26550;&#26500;&#24615;&#33021;&#29305;&#24449;&#21644;&#33258;&#21160;&#24494;&#20998;&#20013;&#30340;&#32454;&#24494;&#24046;&#21035;&#26032;&#25361;&#25112;&#30340;&#36890;&#29992;&#25216;&#26415;&#65292;&#24182;&#22312;TPUv3&#21644;V100 GPU&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;LAST&#30340;&#24211;&#65292;&#23427;&#26159;&#22522;&#20110;&#26684;&#27169;&#22411;&#30340;&#35821;&#38899;&#36716;&#24405;&#22120;&#12290;LAST&#24378;&#35843;&#20102;&#28789;&#27963;&#24615;&#12289;&#26131;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#35757;&#32451;&#21644;&#25512;&#26029;&#25152;&#38656;&#30340;&#21487;&#24494;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;WFSA&#65289;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#25972;&#20010;&#35805;&#35821;&#30340;&#35782;&#21035;&#26684;&#19978;&#12290;&#23613;&#31649;&#36825;&#20123;WFSA&#31639;&#27861;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#29616;&#20195;&#26550;&#26500;&#30340;&#24615;&#33021;&#29305;&#24449;&#21644;&#33258;&#21160;&#24494;&#20998;&#20013;&#30340;&#32454;&#24494;&#24046;&#21035;&#26032;&#30340;&#25361;&#25112;&#24050;&#32463;&#20986;&#29616;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#19968;&#31995;&#21015;&#36890;&#29992;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;TPUv3&#21644;V100 GPU&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce LAST, a LAttice-based Speech Transducer library in JAX. With an emphasis on flexibility, ease-of-use, and scalability, LAST implements differentiable weighted finite state automaton (WFSA) algorithms needed for training \&amp; inference that scale to a large WFSA such as a recognition lattice over the entire utterance. Despite these WFSA algorithms being well-known in the literature, new challenges arise from performance characteristics of modern architectures, and from nuances in automatic differentiation. We describe a suite of generally applicable techniques employed in LAST to address these challenges, and demonstrate their effectiveness with benchmarks on TPUv3 and V100 GPU.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20154;&#36896;&#32467;&#26500;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#20808;&#35757;&#32451;&#21644;&#22312;&#33521;&#35821;&#19978;&#24494;&#35843;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19977;&#31181;&#24402;&#32435;&#20559;&#32622;&#31867;&#22411;&#65306;&#36882;&#24402;&#30340;&#23618;&#32423;&#22788;&#29702;&#12289;&#26080;&#38480;&#21046;&#30340;&#26631;&#35760;-&#26631;&#35760;&#20381;&#36182;&#20197;&#21450;&#22522;&#20110;Zipfian&#24130;&#24459;&#35789;&#27719;&#20998;&#24067;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25105;&#20204;&#24471;&#20986;&#22797;&#26434;&#26631;&#35760;-&#26631;&#35760;&#20132;&#20114;&#24418;&#25104;&#20102;&#26368;&#22909;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2304.13060</link><description>&lt;p&gt;
&#21482;&#29992;&#32467;&#26500;&#20808;&#39044;&#35757;&#32451;&#65306;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#29702;&#35299;&#35821;&#35328;&#24402;&#32435;&#20559;&#32622;
&lt;/p&gt;
&lt;p&gt;
Pretrain on just structure: Understanding linguistic inductive biases using transfer learning. (arXiv:2304.13060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13060
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20154;&#36896;&#32467;&#26500;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#20808;&#35757;&#32451;&#21644;&#22312;&#33521;&#35821;&#19978;&#24494;&#35843;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19977;&#31181;&#24402;&#32435;&#20559;&#32622;&#31867;&#22411;&#65306;&#36882;&#24402;&#30340;&#23618;&#32423;&#22788;&#29702;&#12289;&#26080;&#38480;&#21046;&#30340;&#26631;&#35760;-&#26631;&#35760;&#20381;&#36182;&#20197;&#21450;&#22522;&#20110;Zipfian&#24130;&#24459;&#35789;&#27719;&#20998;&#24067;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25105;&#20204;&#24471;&#20986;&#22797;&#26434;&#26631;&#35760;-&#26631;&#35760;&#20132;&#20114;&#24418;&#25104;&#20102;&#26368;&#22909;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#37117;&#33021;&#22312;&#27809;&#26377;&#26126;&#30830;&#30340;&#32467;&#26500;&#30417;&#30563;&#19979;&#23398;&#20064;&#35821;&#35328;&#12290;&#20160;&#20040;&#26679;&#30340;&#24402;&#32435;&#24335;&#23398;&#20064;&#20559;&#32622;&#20351;&#24471;&#36825;&#31181;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#20154;&#36896;&#32467;&#26500;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#24182;&#22312;&#33521;&#35821;&#19978;&#24494;&#35843;&#26469;&#37319;&#29992;&#19981;&#21516;&#30340;&#24402;&#32435;&#24335;&#23398;&#20064;&#20559;&#32622;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20559;&#32622;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#20351;&#25105;&#20204;&#33021;&#22815;&#31215;&#26497;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#24402;&#32435;&#20559;&#32622;&#30340;&#27604;&#36739;&#25104;&#21151;:1)&#36882;&#24402;&#30340;&#23618;&#32423;&#22788;&#29702;&#30340;&#24402;&#32435;&#20559;&#32622;2)&#19981;&#21463;&#38480;&#30340;&#26631;&#35760;-&#26631;&#35760;&#20381;&#36182;&#65292;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#19981;&#33021;&#30001;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#24314;&#27169;3)Zipfian&#24130;&#24459;&#35789;&#27719;&#20998;&#24067;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22797;&#26434;&#30340;&#26631;&#35760;-&#26631;&#35760;&#20132;&#20114;&#24418;&#25104;&#20102;&#26368;&#22909;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#24182;&#19988;&#36825;&#22312;&#38750;&#19978;&#19979;&#25991;&#26080;&#20851;&#24773;&#20917;&#19979;&#26368;&#20026;&#24378;&#28872;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;Z(targetEntity)&#20998;&#24067;&#22312;&#33521;&#35821;&#19978;&#20063;&#26159;&#21512;&#36866;&#30340;&#39044;&#20808;&#35757;&#32451;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Both humans and transformer language models are able to learn language without explicit structural supervision. What inductive learning biases make this learning possible? In this study, we examine the effect of different inductive learning biases by predisposing language models with structural biases through pretraining on artificial structured data, and then evaluating by fine-tuning on English. Our experimental setup gives us the ability to actively control the inductive bias of language models. With our experiments, we investigate the comparative success of three types of inductive bias: 1) an inductive bias for recursive, hierarchical processing 2) an inductive bias for unrestricted token-token dependencies that can't be modeled by context-free grammars, and 3) an inductive bias for a Zipfian power-law vocabulary distribution. We show that complex token-token interactions form the best inductive biases, and that this is strongest in the non-context-free case. We also show that a Z
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#28966;&#28857;&#29228;&#34411;ThreatCrawl&#65292;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#26469;&#31579;&#36873;&#20986;&#26368;&#21487;&#33021;&#21253;&#21547;&#26377;&#20215;&#20540;CTI&#20449;&#24687;&#30340;&#32593;&#39029;&#12290;</title><link>http://arxiv.org/abs/2304.11960</link><description>&lt;p&gt;
ThreatCrawl&#65306;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23433;&#20840;&#28966;&#28857;&#29228;&#34411;
&lt;/p&gt;
&lt;p&gt;
ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain. (arXiv:2304.11960v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#28966;&#28857;&#29228;&#34411;ThreatCrawl&#65292;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#26469;&#31579;&#36873;&#20986;&#26368;&#21487;&#33021;&#21253;&#21547;&#26377;&#20215;&#20540;CTI&#20449;&#24687;&#30340;&#32593;&#39029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20844;&#24320;&#33719;&#21462;&#30340;&#20449;&#24687;&#23545;&#20110;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#65288;CTI&#65289;&#26469;&#35828;&#21253;&#21547;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#36825;&#21487;&#20197;&#29992;&#20110;&#39044;&#38450;&#24050;&#32463;&#22312;&#20854;&#20182;&#31995;&#32479;&#19978;&#21457;&#29983;&#30340;&#25915;&#20987;&#12290;&#20294;&#26159;&#65292;&#34429;&#28982;&#26377;&#19981;&#21516;&#30340;&#26631;&#20934;&#26469;&#20132;&#27969;&#36825;&#20123;&#20449;&#24687;&#65292;&#20294;&#24456;&#22810;&#20449;&#24687;&#26159;&#20197;&#38750;&#26631;&#20934;&#21270;&#30340;&#26041;&#24335;&#22312;&#25991;&#31456;&#25110;&#21338;&#23458;&#24086;&#23376;&#20013;&#20849;&#20139;&#30340;&#12290;&#25163;&#21160;&#27983;&#35272;&#22810;&#20010;&#22312;&#32447;&#38376;&#25143;&#21644;&#26032;&#38395;&#39029;&#38754;&#20197;&#21457;&#29616;&#26032;&#23041;&#32961;&#24182;&#25552;&#21462;&#23427;&#20204;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#36825;&#20010;&#25195;&#25551;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#22810;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#23041;&#32961;&#25351;&#31034;&#22120;&#65288;IOCs&#65289;&#30340;&#25552;&#21462;&#22120;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#24050;&#32463;&#35299;&#20915;&#20102;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#20294;&#24456;&#23569;&#32771;&#34385;&#25628;&#32034;&#36825;&#20123;&#25991;&#26723;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28966;&#28857;&#29228;&#34411;ThreatCrawl&#65292;&#23427;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65288;BERT&#65289;&#25628;&#32034;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#30456;&#20851;&#25991;&#26723;&#12290;ThreatCrawl&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#26469;&#35782;&#21035;&#30456;&#20851;&#32593;&#31449;&#21644;&#32593;&#39029;&#65292;&#28982;&#21518;&#24212;&#29992;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#22120;&#26469;&#20248;&#20808;&#32771;&#34385;&#26368;&#21487;&#33021;&#21253;&#21547;&#26377;&#20215;&#20540;CTI&#20449;&#24687;&#30340;&#32593;&#39029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Publicly available information contains valuable information for Cyber Threat Intelligence (CTI). This can be used to prevent attacks that have already taken place on other systems. Ideally, only the initial attack succeeds and all subsequent ones are detected and stopped. But while there are different standards to exchange this information, a lot of it is shared in articles or blog posts in non-standardized ways. Manually scanning through multiple online portals and news pages to discover new threats and extracting them is a time-consuming task. To automize parts of this scanning process, multiple papers propose extractors that use Natural Language Processing (NLP) to extract Indicators of Compromise (IOCs) from documents. However, while this already solves the problem of extracting the information out of documents, the search for these documents is rarely considered. In this paper, a new focused crawler is proposed called ThreatCrawl, which uses Bidirectional Encoder Representations 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#25552;&#21319;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#29305;&#21035;&#26159;ToM&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11490</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#26234;&#29702;&#35770;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Boosting Theory-of-Mind Performance in Large Language Models via Prompting. (arXiv:2304.11490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#25552;&#21319;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#29305;&#21035;&#26159;ToM&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2023&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22797;&#26434;&#25512;&#29702;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#38656;&#35201;&#29702;&#35299;&#20195;&#29702;&#20154;&#30340;&#20449;&#24565;&#12289;&#30446;&#26631;&#21644;&#24515;&#29702;&#29366;&#24577;&#65292;&#23545;&#20110;&#28041;&#21450;&#20154;&#31867;&#30340;&#24120;&#35782;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#25552;&#39640;LLM&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#27979;&#37327;&#20102;GPT-4&#21644;&#19977;&#20010;GPT-3.5&#21464;&#20307;&#65288;Davinci-2&#12289;Davinci-3&#12289;GPT-3.5-Turbo&#65289;&#30340;ToM&#34920;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;&#23427;&#20204;&#30340;ToM&#29702;&#35299;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21253;&#21547;&#20004;&#27493;&#24605;&#32500;&#25512;&#29702;&#21644;&#36880;&#27493;&#24605;&#32771;&#35828;&#26126;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#35757;&#32451;&#30340;LLMs&#65288;&#38500;Davinci-2&#22806;&#30340;&#25152;&#26377;&#27169;&#22411;&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;ToM&#20934;&#30830;&#24615;&#12290;GPT-4&#22312;&#38646;&#36718;&#24773;&#20917;&#19979;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;&#36817;80%&#30340;ToM&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#19981;&#36275;&#27979;&#35797;&#38598;&#19978;87%&#30340;&#20154;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#25552;&#20379;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25552;&#31034;&#26102;&#65292;GPT-4&#21644;&#19977;&#20010;GPT-3.5&#21464;&#20307;&#30340;ToM&#20934;&#30830;&#24615;&#26174;&#33879;&#39640;&#20110;&#26080;&#25552;&#31034;&#26102;&#65292;&#20854;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#65288;GPT-3.5-Turbo&#65289;&#36798;&#21040;&#20102;92%&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#21319;LLM&#22312;&#22797;&#26434;&#25512;&#29702;&#23588;&#20854;&#26159;ToM&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GPT-NER&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65288;NER&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#23558;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#36716;&#21270;&#20026;&#29983;&#25104;&#20219;&#21153;&#65292;&#23558;LLM&#33021;&#22815;&#23481;&#26131;&#22320;&#36866;&#24212;NER&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;LLMs&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#35810;&#38382;&#33258;&#36523;&#26469;&#30830;&#23450;&#25552;&#21462;&#30340;&#23454;&#20307;&#26159;&#21542;&#23646;&#20110;&#23454;&#38469;&#23384;&#22312;&#30340;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2304.10428</link><description>&lt;p&gt;
GPT-NER&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
GPT-NER: Named Entity Recognition via Large Language Models. (arXiv:2304.10428v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GPT-NER&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65288;NER&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#23558;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#36716;&#21270;&#20026;&#29983;&#25104;&#20219;&#21153;&#65292;&#23558;LLM&#33021;&#22815;&#23481;&#26131;&#22320;&#36866;&#24212;NER&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;LLMs&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#35810;&#38382;&#33258;&#36523;&#26469;&#30830;&#23450;&#25552;&#21462;&#30340;&#23454;&#20307;&#26159;&#21542;&#23646;&#20110;&#23454;&#38469;&#23384;&#22312;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;NER&#24615;&#33021;&#20173;&#28982;&#26126;&#26174;&#20302;&#20110;&#30417;&#30563;&#22522;&#32447;&#12290;&#36825;&#26159;&#30001;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#65306;&#21069;&#32773;&#22312;&#26412;&#36136;&#19978;&#26159;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#65292;&#21518;&#32773;&#26159;&#19968;&#31181;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-NER&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290; GPT-NER&#36890;&#36807;&#23558;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#36716;&#25442;&#20026;&#29983;&#25104;&#20219;&#21153;&#26469;&#24357;&#21512;&#24046;&#36317;&#65292;LLMs&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#12290;&#20363;&#22914;&#65292;&#23558;&#22312;&#36755;&#20837;&#25991;&#26412;&#8220;&#21733;&#20262;&#24067;&#26159;&#19968;&#24231;&#22478;&#24066;&#8221;&#20013;&#26597;&#25214;&#20301;&#32622;&#23454;&#20307;&#30340;&#20219;&#21153;&#36716;&#25442;&#20026;&#29983;&#25104;&#25991;&#26412;&#24207;&#21015;&#8220;@@&#21733;&#20262;&#24067;##&#26159;&#19968;&#24231;&#22478;&#24066;&#8221;&#65292;&#20854;&#20013;&#29305;&#27530;&#26631;&#35760;@@##&#26631;&#35760;&#35201;&#25552;&#21462;&#30340;&#23454;&#20307;&#12290;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;LLMs&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#21363;LLMs&#26377;&#24456;&#24378;&#30340;&#20542;&#21521;&#23558;&#31354;&#36755;&#20837;&#36807;&#24230;&#33258;&#20449;&#22320;&#26631;&#35760;&#20026;&#23454;&#20307;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#35810;&#38382;&#33258;&#36523;&#26469;&#30830;&#23450;&#25552;&#21462;&#30340;&#23454;&#20307;&#26159;&#21542;&#23646;&#20110;&#23454;&#38469;&#23384;&#22312;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model.  In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text "Columbus is a city" is transformed to generate the text sequence "@@Columbus## is a city", where special tokens @@## marks the entity to extract. To efficiently address the "hallucination" issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a lab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.09349</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#22823;&#33041;&#65306;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#24863;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#22791;&#29289;&#29702;&#25110;&#34394;&#25311;&#23454;&#20307;&#65288;&#21363;&#26426;&#22120;&#20154;&#65289;&#24182;&#33021;&#22815;&#19982;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#35760;&#24518;&#21644;&#25511;&#21046;&#26159;&#20307;&#24863;&#31995;&#32479;&#30340;&#20004;&#20010;&#22522;&#26412;&#37096;&#20998;&#65292;&#36890;&#24120;&#38656;&#35201;&#20998;&#21035;&#20351;&#29992;&#26694;&#26550;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;LLM-Brain&#65306;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#65292;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#12290;LLM-Brain&#26694;&#26550;&#38598;&#25104;&#20102;&#22810;&#20010;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#21033;&#29992;&#38646;-shot&#23398;&#20064;&#26041;&#27861;&#12290;LLM-Brain&#20013;&#30340;&#25152;&#26377;&#32452;&#20214;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#12290;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#20855;&#22791;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#23454;&#20307;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#65306;&#20027;&#21160;&#25506;&#32034;&#21644;&#23454;&#20307;&#38382;&#31572;&#26469;&#28436;&#31034;LLM-Brain&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23637;&#31034;&#65292;&#19968;&#20010;&#25317;&#26377;&#20856;&#22411;API&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#25163;&#21487;&#20197;&#20197;&#26497;&#20302;&#30340;&#37329;&#38065;&#25104;&#26412;&#31363;&#21462;GPT-2&#21644;GPT-3&#31561;LM&#30340;&#35299;&#30721;&#31639;&#27861;&#30340;&#31867;&#22411;&#21644;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.04729</link><description>&lt;p&gt;
&#35770;&#30423;&#29992;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#31639;&#27861;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
On the Risks of Stealing the Decoding Algorithms of Language Models. (arXiv:2303.04729v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04729
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23637;&#31034;&#65292;&#19968;&#20010;&#25317;&#26377;&#20856;&#22411;API&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#25163;&#21487;&#20197;&#20197;&#26497;&#20302;&#30340;&#37329;&#38065;&#25104;&#26412;&#31363;&#21462;GPT-2&#21644;GPT-3&#31561;LM&#30340;&#35299;&#30721;&#31639;&#27861;&#30340;&#31867;&#22411;&#21644;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#29983;&#25104;&#25991;&#26412;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#21644;&#35843;&#25972;&#35299;&#30721;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#30830;&#23450;&#22914;&#20309;&#20174;LM&#29983;&#25104;&#30340;&#20869;&#37096;&#27010;&#29575;&#20998;&#24067;&#20013;&#29983;&#25104;&#25991;&#26412;&#12290;&#36873;&#25321;&#35299;&#30721;&#31639;&#27861;&#24182;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#30340;&#36807;&#31243;&#38656;&#35201;&#26174;&#33879;&#30340;&#26102;&#38388;&#12289;&#25163;&#21160;&#24037;&#20316;&#21644;&#35745;&#31639;&#65292;&#36824;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#35299;&#30721;&#31639;&#27861;&#30340;&#36523;&#20221;&#21644;&#36229;&#21442;&#25968;&#34987;&#35748;&#20026;&#26159;&#26497;&#20854;&#26377;&#20215;&#20540;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#19968;&#20010;&#25317;&#26377;&#20856;&#22411;API&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#25163;&#21487;&#20197;&#20197;&#26497;&#20302;&#30340;&#37329;&#38065;&#25104;&#26412;&#31363;&#21462;&#20854;&#35299;&#30721;&#31639;&#27861;&#30340;&#31867;&#22411;&#21644;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#23545;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;API&#30340;&#27969;&#34892;LM&#26377;&#25928;&#65292;&#21253;&#25324;GPT-2&#21644;GPT-3&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21482;&#38656;&#33457;&#36153;&#20960;&#32654;&#20803;&#65292;&#20363;&#22914;0.8&#32654;&#20803;&#12289;1&#32654;&#20803;&#12289;4&#32654;&#20803;&#21644;40&#32654;&#20803;&#65292;&#23601;&#21487;&#20197;&#30423;&#21462;&#27492;&#31867;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., $\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of GPT-3.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25972;&#20010;&#23186;&#20307;&#30340;&#32454;&#31890;&#24230;&#21487;&#38752;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;&#25163;&#21160;&#21046;&#20316;&#30340;&#8220;FactNews&#8221;&#25968;&#25454;&#24211;&#19978;&#65292;&#36890;&#36807; fine-tuning BERT &#27169;&#22411;&#39044;&#27979;&#26032;&#38395;&#25253;&#36947;&#30340;&#21477;&#23376;&#32423;&#21035;&#20107;&#23454;&#24615;&#21644;&#23186;&#20307;&#20542;&#21521;&#12290;&#27492;&#26041;&#27861;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#20854;&#20182;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2301.11850</link><description>&lt;p&gt;
&#39044;&#27979;&#26032;&#38395;&#20107;&#23454;&#24615;&#21644;&#23186;&#20307;&#20542;&#21521;&#30340;&#21477;&#23376;&#32423;&#21035;&#21487;&#38752;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Predicting Sentence-Level Factuality of News and Bias of Media Outlets. (arXiv:2301.11850v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25972;&#20010;&#23186;&#20307;&#30340;&#32454;&#31890;&#24230;&#21487;&#38752;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;&#25163;&#21160;&#21046;&#20316;&#30340;&#8220;FactNews&#8221;&#25968;&#25454;&#24211;&#19978;&#65292;&#36890;&#36807; fine-tuning BERT &#27169;&#22411;&#39044;&#27979;&#26032;&#38395;&#25253;&#36947;&#30340;&#21477;&#23376;&#32423;&#21035;&#20107;&#23454;&#24615;&#21644;&#23186;&#20307;&#20542;&#21521;&#12290;&#27492;&#26041;&#27861;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#20854;&#20182;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#26032;&#38395;&#25253;&#36947;&#30340;&#20107;&#23454;&#24615;&#21644;&#23186;&#20307;&#20542;&#21521;&#23545;&#20110;&#33258;&#21160;&#21270;&#30340;&#26032;&#38395;&#20449;&#35465;&#21644;&#20107;&#23454;&#26680;&#26597;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#25972;&#20010;&#23186;&#20307;&#36827;&#34892;&#32454;&#31890;&#24230;&#21487;&#38752;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#27979;&#26032;&#38395;&#25253;&#36947;&#30340;&#21477;&#23376;&#32423;&#21035;&#20107;&#23454;&#24615;&#21644;&#23186;&#20307;&#20542;&#21521;&#65292;&#36825;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#35299;&#37322;&#25972;&#20010; source &#30340;&#21487;&#38752;&#31243;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#25163;&#21160;&#21046;&#20316;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#21477;&#23376;&#32423;&#21035;&#25968;&#25454;&#24211;&#65292;&#8220;FactNews&#8221;&#65292;&#30001; 6191 &#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#21477;&#23376;&#32452;&#25104;&#65292;&#27880;&#37322;&#20381;&#25454;&#26469;&#33258; AllSides &#30340;&#20107;&#23454;&#24615;&#21644;&#23186;&#20307;&#20542;&#21521;&#23450;&#20041;&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#24052;&#35199;&#23384;&#22312;&#20005;&#37325;&#30340;&#34394;&#20551;&#26032;&#38395;&#21644;&#25919;&#27835;&#26497;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#20854;&#20182;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the factuality of news reporting and bias of media outlets is surely relevant for automated news credibility and fact-checking. While prior work has focused on the veracity of news, we propose a fine-grained reliability analysis of the entire media. Specifically, we study the prediction of sentence-level factuality of news reporting and bias of media outlets, which may explain more accurately the overall reliability of the entire source. We first manually produced a large sentence-level dataset, titled "FactNews", composed of 6,191 sentences expertly annotated according to factuality and media bias definitions from AllSides. As a result, baseline models for sentence-level factuality prediction were presented by fine-tuning BERT. Finally, due to the severity of fake news and political polarization in Brazil, both dataset and baseline were proposed for Portuguese. However, our approach may be applied to any other language.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#37319;&#29992;&#21069;&#32512;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11719</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#32435;&#20837;&#25991;&#26723;&#25688;&#35201;&#29983;&#25104;&#20013;&#65306;&#22522;&#20110;GPT-2&#30340;&#21069;&#32512;&#35843;&#25972;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#37319;&#29992;&#21069;&#32512;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#22312;&#25991;&#26723;&#25688;&#35201;&#25216;&#26415;&#24471;&#21040;&#20102;&#24456;&#22823;&#30340;&#21457;&#23637;&#65292;&#20294;&#26159;&#29983;&#25104;&#30340;&#25688;&#35201;&#21644;&#21407;&#22987;&#25991;&#26412;&#20043;&#38388;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#20173;&#28982;&#26102;&#26377;&#21457;&#29983;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#37319;&#29992;&#25552;&#31034;&#26469;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20855;&#20307;&#30740;&#31350;&#20102;&#21069;&#32512;&#35843;&#25972;&#65292;&#23427;&#20351;&#29992;&#19968;&#32452;&#21487;&#35757;&#32451;&#30340;&#36830;&#32493;&#21069;&#32512;&#25552;&#31034;&#21644;&#31163;&#25955;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#24110;&#21161;&#25688;&#35201;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#35757;&#32451;&#30340;&#21069;&#32512;&#21487;&#20197;&#24110;&#21161;&#25688;&#35201;&#27169;&#22411;&#20934;&#30830;&#22320;&#20174;&#31163;&#25955;&#25552;&#31034;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#20174;&#32780;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#36825;&#20123;&#25688;&#35201;&#22312;&#20107;&#23454;&#19978;&#19982;&#31163;&#25955;&#25552;&#31034;&#19968;&#33268;&#12290;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;ROUGE&#25913;&#36827;&#34920;&#26126;&#65292;&#23558;&#20107;&#23454;&#30693;&#35782;&#26126;&#30830;&#22320;&#28155;&#21152;&#21040;&#25688;&#35201;&#29983;&#25104;&#36807;&#31243;&#20013;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24212;&#29992;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great development of document summarisation techniques nowadays, factual inconsistencies between the generated summaries and the original texts still occur from time to time. This study explores the possibility of adopting prompts to incorporate factual knowledge into generated summaries. We specifically study prefix-tuning that uses a set of trainable continuous prefix prompts together with discrete natural language prompts to aid summary generation. Experimental results demonstrate that the trainable prefixes can help the summarisation model extract information from discrete prompts precisely, thus generating knowledge-preserving summaries that are factually consistent with the discrete prompts. The ROUGE improvements of the generated summaries indicate that explicitly adding factual knowledge into the summarisation process could boost the overall performance, showing great potential for applying it to other natural language processing tasks.
&lt;/p&gt;</description></item><item><title>LMP&#23558;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20174;&#32431;&#25991;&#26412;&#25552;&#31034;&#25193;&#23637;&#20026;&#25991;&#26412;&#25552;&#31034;&#21644;&#33050;&#26412;&#30340;&#30452;&#35266;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#32534;&#31243;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2212.06094</link><description>&lt;p&gt;
Prompting&#23601;&#26159;&#32534;&#31243;: &#19968;&#31181;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Prompting Is Programming: A Query Language for Large Language Models. (arXiv:2212.06094v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06094
&lt;/p&gt;
&lt;p&gt;
LMP&#23558;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20174;&#32431;&#25991;&#26412;&#25552;&#31034;&#25193;&#23637;&#20026;&#25991;&#26412;&#25552;&#31034;&#21644;&#33050;&#26412;&#30340;&#30452;&#35266;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#32534;&#31243;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#21644;&#20195;&#30721;&#29983;&#25104;&#31561;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;&#20174;&#39640;&#23618;&#27425;&#19978;&#35762;&#65292;&#32473;&#23450;&#36755;&#20837;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#32479;&#35745;&#19978;&#30340;&#21487;&#33021;&#24615;&#33258;&#21160;&#23436;&#25104;&#24207;&#21015;&#12290;&#22522;&#20110;&#27492;&#65292;&#29992;&#25143;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196;&#25110;&#31034;&#20363;&#26469;&#25552;&#31034;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#39640;&#32423;&#25552;&#31034;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#26263;&#31034;&#27169;&#22411;&#12289;&#29992;&#25143;&#21644;&#35745;&#31639;&#22120;&#31561;&#22806;&#37096;&#24037;&#20855;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#25110;&#23558;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#65292;&#24517;&#39035;&#23454;&#29616;&#22797;&#26434;&#30340;&#20219;&#21153;-&#21644;&#27169;&#22411;&#29305;&#23450;&#30340;&#31243;&#24207;&#65292;&#36825;&#20173;&#28982;&#21487;&#33021;&#38656;&#35201;&#29305;&#23450;&#30340;&#20132;&#20114;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#32534;&#31243;&#65288;LMP&#65289;&#30340;&#26032;&#27010;&#24565;&#12290;LMP&#23558;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20174;&#32431;&#25991;&#26412;&#25552;&#31034;&#25193;&#23637;&#20026;&#25991;&#26412;&#25552;&#31034;&#21644;&#33050;&#26412;&#30340;&#30452;&#35266;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;LMP&#20801;&#35768;&#25351;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.  Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaBo&#30340;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65292;&#33021;&#22815;&#19981;&#38656;&#35201;&#25163;&#21160;&#25351;&#23450;&#20851;&#38190;&#27010;&#24565;&#24182;&#23454;&#29616;&#19982;&#40657;&#30418;&#23376;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11158</link><description>&lt;p&gt;
&#29942;&#20013;&#35821;&#35328;&#65306;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#27010;&#24565;&#29942;&#39048;&#29992;&#20110;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification. (arXiv:2211.11158v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaBo&#30340;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65292;&#33021;&#22815;&#19981;&#38656;&#35201;&#25163;&#21160;&#25351;&#23450;&#20851;&#38190;&#27010;&#24565;&#24182;&#23454;&#29616;&#19982;&#40657;&#30418;&#23376;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#26159;&#19968;&#31181;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#27169;&#22411;&#20915;&#31574;&#20998;&#35299;&#20026;&#21487;&#35835;&#30340;&#27010;&#24565;&#65292;&#20351;&#20154;&#20204;&#33021;&#22815;&#36731;&#26494;&#29702;&#35299;&#27169;&#22411;&#22833;&#36133;&#30340;&#21407;&#22240;&#65292;&#36825;&#23545;&#20110;&#39640;&#39118;&#38505;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#38656;&#35201;&#25163;&#21160;&#25351;&#23450;&#27010;&#24565;&#65292;&#36890;&#24120;&#34920;&#29616;&#19981;&#22914;&#40657;&#30418;&#23376;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#32570;&#28857;&#65292;&#24182;&#39318;&#27425;&#23637;&#31034;&#20102;&#22914;&#20309;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#25351;&#23450;&#65292;&#20854;&#20934;&#30830;&#24615;&#31867;&#20284;&#20110;&#40657;&#30418;&#23376;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8220;Language Guided Bottlenecks&#8221;&#65288;LaBo&#65289;&#21033;&#29992;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;GPT-3&#26469;&#23450;&#20041;&#21487;&#33021;&#30340;&#29942;&#39048;&#32452;&#25104;&#30340;&#22823;&#22411;&#31354;&#38388;&#12290;&#22312;&#32473;&#23450;&#38382;&#39064;&#22495;&#30340;&#24773;&#20917;&#19979;&#65292;LaBo&#20351;&#29992;GPT-3&#29983;&#25104;&#26377;&#20851;&#31867;&#21035;&#30340;&#20107;&#23454;&#21477;&#23376;&#65292;&#24418;&#25104;&#20505;&#36873;&#27010;&#24565;&#12290;LaBo&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23376;&#27169;&#22359;&#25928;&#29992;&#39640;&#25928;&#25628;&#32034;&#21487;&#33021;&#30340;&#29942;&#39048;&#65292;&#20419;&#36827;&#36873;&#25321;&#26377;&#21306;&#20998;&#21147;&#21644;&#22810;&#26679;&#24615;&#30340;&#20449;&#24687;&#12290;&#26368;&#32456;&#65292;GPT-3&#30340;&#21477;&#23376;&#27010;&#24565;&#21487;&#20197;&#20351;&#29992;C&#19982;&#22270;&#20687;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into human-readable concepts. They allow people to easily understand why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and often under-perform their black box counterparts, preventing their broad adoption. We address these shortcomings and are first to show how to construct high-performance CBMs without manual specification of similar accuracy to black box models. Our approach, Language Guided Bottlenecks (LaBo), leverages a language model, GPT-3, to define a large space of possible bottlenecks. Given a problem domain, LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches possible bottlenecks through a novel submodular utility that promotes the selection of discriminative and diverse information. Ultimately, GPT-3's sentential concepts can be aligned to images using C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PIXEL&#30340;&#22522;&#20110;&#20687;&#32032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#23558;&#25991;&#26412;&#28210;&#26579;&#20026;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#25193;&#23637;&#25903;&#25345;&#30340;&#35821;&#35328;&#25968;&#37327;&#26102;&#20986;&#29616;&#30340;&#35789;&#27719;&#29942;&#39048;&#38382;&#39064;&#65292;&#19988;&#22312;&#24418;&#24577;&#23398;&#21644;&#35821;&#20041;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;BERT&#12290;</title><link>http://arxiv.org/abs/2207.06991</link><description>&lt;p&gt;
&#20351;&#29992;&#20687;&#32032;&#30340;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Language Modelling with Pixels. (arXiv:2207.06991v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PIXEL&#30340;&#22522;&#20110;&#20687;&#32032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#23558;&#25991;&#26412;&#28210;&#26579;&#20026;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#25193;&#23637;&#25903;&#25345;&#30340;&#35821;&#35328;&#25968;&#37327;&#26102;&#20986;&#29616;&#30340;&#35789;&#27719;&#29942;&#39048;&#38382;&#39064;&#65292;&#19988;&#22312;&#24418;&#24577;&#23398;&#21644;&#35821;&#20041;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;BERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#26377;&#38480;&#30340;&#36755;&#20837;&#38598;&#21512;&#19978;&#36827;&#34892;&#23450;&#20041;&#65292;&#36825;&#23548;&#33268;&#22312;&#23581;&#35797;&#25193;&#23637;&#25903;&#25345;&#30340;&#35821;&#35328;&#25968;&#37327;&#26102;&#20986;&#29616;&#35789;&#27719;&#29942;&#39048;&#12290;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#20250;&#23548;&#33268;&#22312;&#23884;&#20837;&#30697;&#38453;&#20013;&#25152;&#33021;&#34920;&#31034;&#30340;&#20869;&#23481;&#19982;&#36755;&#20986;&#23618;&#30340;&#35745;&#31639;&#38382;&#39064;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21517;&#20026;PIXEL&#30340;&#22522;&#20110;&#20687;&#32032;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#12290;PIXEL&#26159;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#25991;&#26412;&#28210;&#26579;&#20026;&#22270;&#20687;&#65292;&#20174;&#32780;&#21487;&#20197;&#22522;&#20110;&#25340;&#20889;&#30456;&#20284;&#24615;&#25110;&#20687;&#32032;&#30340;&#20849;&#21516;&#28608;&#27963;&#26469;&#36328;&#35821;&#35328;&#20256;&#36882;&#34920;&#31034;&#12290;PIXEL&#35757;&#32451;&#26102;&#19981;&#26159;&#39044;&#27979;&#26631;&#35760;&#20998;&#24067;&#65292;&#32780;&#26159;&#37325;&#26500;&#34987;&#23631;&#34109;&#30340;&#22359;&#30340;&#20687;&#32032;&#12290;&#25105;&#20204;&#23545;&#19982;BART&#30456;&#21516;&#30340;&#33521;&#35821;&#25968;&#25454;&#36827;&#34892;&#20102;86M&#21442;&#25968;PIXEL&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#24418;&#24577;&#23398;&#21644;&#35821;&#20041;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#65292;&#21253;&#25324;&#21508;&#31181;&#38750;&#25289;&#19969;&#25991;&#23383;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35821;&#27861;&#21644;&#35821;&#20041;&#22788;&#29702;&#26041;&#38754;&#65292;PIXEL&#30340;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;BERT&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20986;&#19968;&#31181;&#26816;&#27979;&#24613;&#35786;&#31185;&#20998;&#35786;&#21069;&#36133;&#34880;&#30151;&#30340;&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#26631;&#20934;&#36133;&#34880;&#30151;&#31579;&#26597;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.07657</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#24613;&#35786;&#31185;&#20998;&#35786;&#20013;&#26816;&#27979;&#36133;&#34880;&#30151;
&lt;/p&gt;
&lt;p&gt;
Detection of sepsis during emergency department triage using machine learning. (arXiv:2204.07657v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20986;&#19968;&#31181;&#26816;&#27979;&#24613;&#35786;&#31185;&#20998;&#35786;&#21069;&#36133;&#34880;&#30151;&#30340;&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#26631;&#20934;&#36133;&#34880;&#30151;&#31579;&#26597;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#22120;&#23448;&#21151;&#33021;&#38556;&#30861;&#30340;&#36133;&#34880;&#30151;&#26159;&#20840;&#29699;&#27515;&#20129;&#21644;&#21361;&#37325;&#30142;&#30149;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#27604;&#36739;&#26631;&#20934;&#36133;&#34880;&#30151;&#31579;&#26597;&#31639;&#27861;&#21644;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20998;&#35786;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#24613;&#35786;&#31185;&#20998;&#35786;&#21069;&#65288;&#26410;&#20351;&#29992;&#23454;&#39564;&#23460;&#35786;&#26029;&#65289;&#30340;&#36133;&#34880;&#30151;&#26816;&#27979;&#24615;&#33021;&#12290;&#30740;&#31350;&#24471;&#20986;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340; AUC &#20026; 0.9423&#65292;&#25935;&#24863;&#24615;&#20026; 71.09%&#12290;
&lt;/p&gt;
&lt;p&gt;
Sepsis is a life-threatening condition with organ dysfunction and is a leading cause of death and critical illness worldwide. Even a few hours of delay in the treatment of sepsis results in increased mortality. Early detection of sepsis during emergency department triage would allow early initiation of lab analysis, antibiotic administration, and other sepsis treatment protocols. The purpose of this study was to compare sepsis detection performance at ED triage (prior to the use of laboratory diagnostics) of the standard sepsis screening algorithm (SIRS with source of infection) and a machine learning algorithm trained on EHR triage data. A machine learning model (KATE Sepsis) was developed using patient encounters with triage data from 16participating hospitals. KATE Sepsis and standard screening were retrospectively evaluated on the adult population of 512,949 medical records. KATE Sepsis demonstrates an AUC of 0.9423 (0.9401 - 0.9441) with sensitivity of 71.09% (70.12% - 71.98%) and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#20998;&#26512;&#35777;&#23454;&#65292;&#26085;&#23572;&#26364;&#35821;&#35328;&#21644;&#26031;&#25289;&#22827;&#35821;&#35328;&#20013;&#30340;&#24615;&#21035;&#19968;&#33268;&#25928;&#24212;&#27604;&#32599;&#26364;&#35821;&#35328;&#26356;&#21152;&#31283;&#20581;&#65292;&#20294;&#25928;&#24212;&#22823;&#23567;&#36866;&#20013;&#65292;&#24182;&#19988;&#23384;&#22312;&#30740;&#31350;&#38388;&#30340;&#21464;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2109.03490</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#24615;&#21035;&#19968;&#33268;&#24615;&#24433;&#21709;&#30340;&#36328;&#35821;&#35328;&#24046;&#24322;&#65306;&#22522;&#20110;&#20803;&#20998;&#26512;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-linguistic differences in gender congruency effects: Evidence from meta-analyses. (arXiv:2109.03490v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03490
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#20998;&#26512;&#35777;&#23454;&#65292;&#26085;&#23572;&#26364;&#35821;&#35328;&#21644;&#26031;&#25289;&#22827;&#35821;&#35328;&#20013;&#30340;&#24615;&#21035;&#19968;&#33268;&#25928;&#24212;&#27604;&#32599;&#26364;&#35821;&#35328;&#26356;&#21152;&#31283;&#20581;&#65292;&#20294;&#25928;&#24212;&#22823;&#23567;&#36866;&#20013;&#65292;&#24182;&#19988;&#23384;&#22312;&#30740;&#31350;&#38388;&#30340;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20154;&#35748;&#20026;&#65292;&#21333;&#35789;&#21046;&#20316;&#30340;&#20934;&#22791;&#39034;&#24207;&#21462;&#20915;&#20110;&#35828;&#35805;&#32773;&#25152;&#29992;&#30340;&#35821;&#35328;&#12290;&#24403;&#24503;&#35821;&#25110;&#33655;&#20848;&#35821;&#30340;&#35828;&#35805;&#32773;&#21046;&#20316;&#23567;&#29483;&#30340;&#32763;&#35793;&#26102;&#65292;&#20250;&#22312;&#21046;&#20316;&#36807;&#31243;&#30340;&#36739;&#26089;&#38454;&#27573;&#36873;&#25321;&#26631;&#35760;&#24615;&#21035;&#30340;&#38480;&#23450;&#35789;&#12290;&#32780;&#27861;&#35821;&#25110;&#24847;&#22823;&#21033;&#35821;&#30340;&#35828;&#35805;&#32773;&#20250;&#23558;&#38480;&#23450;&#35789;&#25110;&#24418;&#23481;&#35789;&#30340;&#32534;&#30721;&#25512;&#36831;&#21040;&#21487;&#29992;&#21517;&#35789;&#30340;&#38899;&#38901;&#24418;&#24335;&#20043;&#21518;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#21333;&#35789;&#30340;&#39034;&#24207;&#30456;&#21516;&#65288;&#20363;&#22914;&#22312;&#24503;&#35821;&#20013;&#26159;&#8220;die kleine Katze&#8221;&#65292;&#22312;&#27861;&#35821;&#20013;&#26159;&#8220;le petit chat&#8221;&#65289;&#65292;&#23427;&#20204;&#30340;&#35745;&#21010;&#39034;&#24207;&#19981;&#21516;&#65292;&#21487;&#33021;&#38656;&#35201;&#22312;&#21046;&#20316;&#24320;&#22987;&#21069;&#36827;&#34892;&#19981;&#21516;&#31243;&#24230;&#30340;&#25552;&#21069;&#35745;&#21010;&#12290;&#36825;&#31181;&#26089;&#26399;&#21644;&#26202;&#26399;&#36873;&#25321;&#35821;&#35328;&#20043;&#38388;&#30340;&#21306;&#21035;&#26159;&#20026;&#20102;&#35299;&#37322;&#35266;&#23519;&#21040;&#30340;&#25351;&#20986;&#24503;&#26085;&#31561;&#35821;&#35328;&#20294;&#19981;&#21253;&#25324;&#32599;&#26364;&#35821;&#31995;&#35821;&#35328;&#20013;&#30340;&#22270;&#29255;&#21629;&#21517;&#36895;&#24230;&#36739;&#24930;&#30340;&#24615;&#21035;&#24178;&#25200;&#25928;&#24212;&#12290;&#36827;&#34892;&#20803;&#20998;&#26512;&#20197;&#30452;&#25509;&#27979;&#35797;&#36825;&#19968;&#36328;&#35821;&#35328;&#24615;&#20551;&#35828;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#22270;&#29255;&#21629;&#21517;&#21453;&#24212;&#26102;&#38388;&#20013;&#65292;&#24615;&#21035;&#19968;&#33268;&#25928;&#24212;&#22312;&#26085;&#23572;&#26364;&#35821;&#35328;&#21644;&#26031;&#25289;&#22827;&#35821;&#35328;&#20013;&#30340;&#31283;&#20581;&#24615;&#27604;&#32599;&#26364;&#35821;&#31995;&#35821;&#35328;&#26356;&#39640;&#12290;&#20294;&#26159;&#65292;&#25928;&#24212;&#22823;&#23567;&#36866;&#20013;&#65292;&#24182;&#19988;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;&#25928;&#24212;&#22823;&#23567;&#22312;&#30740;&#31350;&#20043;&#38388;&#23384;&#22312;&#30456;&#24403;&#30340;&#21464;&#24322;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#25351;&#20986;&#20102;&#24615;&#21035;&#26631;&#35760;&#24433;&#21709;&#35821;&#35328;&#21046;&#20316;&#30340;&#36328;&#35821;&#35328;&#24046;&#24322;&#65292;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#32771;&#34385;&#24433;&#21709;&#24615;&#21035;&#19968;&#33268;&#24615;&#25928;&#24212;&#22823;&#23567;&#30340;&#26041;&#27861;&#21644;&#35821;&#22659;&#22240;&#32032;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been proposed that the order in which words are prepared for production depends on the speaker's language. When producing the translation equivalent of the small cat, speakers of German or Dutch select the gender-marked determiner at a relatively early stage of production. Speakers of French or Italian postpone the encoding of a determiner or adjective until the phonological form of the noun is available. Hence, even though the words are produced in the same order (e.g., die kleine Katze in German, le petit chat in French), they are not planned in the same order and might require different amounts of advanced planning prior to production onset. This distinction between early and late selection languages was proposed to account for the observation that speakers of Germanic and Slavic languages, but not of Romance languages, are slower to name pictures in the context of a distractor word of a different gender. Meta-analyses are conducted to provide the first direct test of this cr
&lt;/p&gt;</description></item></channel></rss>