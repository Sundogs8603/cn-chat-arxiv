<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>DUMA&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.18075</link><description>&lt;p&gt;
DUMA&#65306;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking. (arXiv:2310.18075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18075
&lt;/p&gt;
&lt;p&gt;
DUMA&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DUMA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29992;&#20110;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#30340;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20307;&#29616;&#20102;&#21452;&#37325;&#24605;&#32500;&#26426;&#21046;&#12290;&#24555;&#36895;&#24605;&#32771;&#27169;&#22411;&#20316;&#20026;&#20027;&#35201;&#25509;&#21475;&#29992;&#20110;&#22806;&#37096;&#20132;&#20114;&#21644;&#21021;&#22987;&#21709;&#24212;&#29983;&#25104;&#65292;&#26681;&#25454;&#23436;&#25972;&#21709;&#24212;&#30340;&#22797;&#26434;&#24615;&#35780;&#20272;&#26159;&#21542;&#38656;&#35201;&#35843;&#29992;&#24930;&#36895;&#24605;&#32771;&#27169;&#22411;&#12290;&#19968;&#26086;&#34987;&#35843;&#29992;&#65292;&#24930;&#36895;&#24605;&#32771;&#27169;&#22411;&#25509;&#31649;&#23545;&#35805;&#65292;&#22312;&#32454;&#33268;&#35268;&#21010;&#12289;&#25512;&#29702;&#21644;&#24037;&#20855;&#21033;&#29992;&#26041;&#38754;&#36827;&#34892;&#24037;&#20316;&#65292;&#25552;&#20379;&#32463;&#36807;&#20805;&#20998;&#20998;&#26512;&#30340;&#21709;&#24212;&#12290;&#36825;&#31181;&#21452;&#37325;&#24605;&#32500;&#37197;&#32622;&#20801;&#35768;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#25151;&#22320;&#20135;&#34892;&#19994;&#22312;&#32447;&#21672;&#35810;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the dual-process theory of human cognition, we introduce DUMA, a novel conversational agent framework that embodies a dual-mind mechanism through the utilization of two generative Large Language Models (LLMs) dedicated to fast and slow thinking respectively. The fast thinking model serves as the primary interface for external interactions and initial response generation, evaluating the necessity for engaging the slow thinking model based on the complexity of the complete response. When invoked, the slow thinking model takes over the conversation, engaging in meticulous planning, reasoning, and tool utilization to provide a well-analyzed response. This dual-mind configuration allows for a seamless transition between intuitive responses and deliberate problem-solving processes based on the situation. We have constructed a conversational agent to handle online inquiries in the real estate industry. The experiment proves that our method balances effectiveness and efficiency, an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#22810;&#35821;&#35328;&#25968;&#25454;&#26377;&#26356;&#39640;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#65292;&#24182;&#19988;&#22522;&#20110;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.14356</link><description>&lt;p&gt;
&#25991;&#21270;&#21644;&#35821;&#35328;&#22810;&#26679;&#24615;&#25552;&#39640;&#20102;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Cultural and Linguistic Diversity Improves Visual Representations. (arXiv:2310.14356v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14356
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#22810;&#35821;&#35328;&#25968;&#25454;&#26377;&#26356;&#39640;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#65292;&#24182;&#19988;&#22522;&#20110;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#36890;&#24120;&#23558;&#24863;&#30693;&#35270;&#20026;&#23458;&#35266;&#30340;&#65292;&#24182;&#19988;&#36825;&#31181;&#20551;&#35774;&#22312;&#25968;&#25454;&#38598;&#25910;&#38598;&#21644;&#27169;&#22411;&#35757;&#32451;&#20013;&#24471;&#21040;&#21453;&#26144;&#12290;&#20363;&#22914;&#65292;&#19981;&#21516;&#35821;&#35328;&#30340;&#22270;&#20687;&#25551;&#36848;&#36890;&#24120;&#34987;&#20551;&#23450;&#20026;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#36328;&#25991;&#21270;&#24515;&#29702;&#23398;&#21644;&#35821;&#35328;&#23398;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20010;&#20307;&#30340;&#35270;&#35273;&#24863;&#30693;&#22240;&#20854;&#25991;&#21270;&#32972;&#26223;&#21644;&#25152;&#35828;&#30340;&#35821;&#35328;&#32780;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#26631;&#39064;&#20013;&#65292;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#20869;&#23481;&#24046;&#24322;&#12290;&#24403;&#25968;&#25454;&#26159;&#22810;&#35821;&#35328;&#32780;&#19981;&#26159;&#21333;&#35821;&#35328;&#26102;&#65292;&#26631;&#39064;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#24179;&#22343;&#26356;&#39640;&#65292;&#20197;&#22330;&#26223;&#22270;&#12289;&#23884;&#20837;&#21644;&#35821;&#35328;&#22797;&#26434;&#24615;&#36827;&#34892;&#27979;&#37327;&#12290;&#20363;&#22914;&#65292;&#19982;&#19968;&#32452;&#21333;&#35821;&#26631;&#39064;&#30456;&#27604;&#65292;&#22810;&#35821;&#26631;&#39064;&#24179;&#22343;&#26377;21.8&#65285;&#26356;&#22810;&#30340;&#23545;&#35937;&#65292;24.5&#65285;&#26356;&#22810;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;27.1&#65285;&#26356;&#22810;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#30340;&#20869;&#23481;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision often treats perception as objective, and this assumption gets reflected in the way that datasets are collected and models are trained. For instance, image descriptions in different languages are typically assumed to be translations of the same semantic content. However, work in cross-cultural psychology and linguistics has shown that individuals differ in their visual perception depending on their cultural background and the language they speak. In this paper, we demonstrate significant differences in semantic content across languages in both dataset and model-produced captions. When data is multilingual as opposed to monolingual, captions have higher semantic coverage on average, as measured by scene graph, embedding, and linguistic complexity. For example, multilingual captions have on average 21.8% more objects, 24.5% more relations, and 27.1% more attributes than a set of monolingual captions. Moreover, models trained on content from different languages perform bes
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#25506;&#32034;&#24615;AI&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#33258;&#20027;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#26032;&#39062;&#24615;&#65292;&#20351;AI&#19981;&#20877;&#36807;&#24230;&#20381;&#36182;&#20154;&#31867;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2310.08899</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#26679;&#21270;AI&#30417;&#30563;&#30340;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Exploration with Principles for Diverse AI Supervision. (arXiv:2310.08899v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#25506;&#32034;&#24615;AI&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#33258;&#20027;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#26032;&#39062;&#24615;&#65292;&#20351;AI&#19981;&#20877;&#36807;&#24230;&#20381;&#36182;&#20154;&#31867;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#26469;&#35757;&#32451;&#22823;&#22411;transformer&#22312;AI&#39046;&#22495;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;&#23613;&#31649;&#36825;&#31181;&#29983;&#25104;&#22411;AI&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#31867;&#30417;&#30563;&#12290;&#21363;&#20351;&#26159;&#20687;ChatGPT&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;AI&#27169;&#22411;&#20063;&#20381;&#36182;&#20110;&#36890;&#36807;&#20154;&#31867;&#28436;&#31034;&#30340;&#24494;&#35843;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#36755;&#20837;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#23545;&#20154;&#31867;&#30417;&#30563;&#30340;&#24378;&#28872;&#20381;&#36182;&#23545;&#20110;&#25512;&#21160;AI&#21019;&#26032;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#31216;&#20026;&#25506;&#32034;&#24615;AI&#65288;EAI&#65289;&#65292;&#26088;&#22312;&#33258;&#20027;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20174;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#39044;&#35757;&#32451;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;EAI&#22312;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#20869;&#23454;&#29616;&#20102;&#25506;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#26032;&#39062;&#24615;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#20010;&#29983;&#25104;&#25353;&#29031;&#25506;&#32034;&#21407;&#21017;&#29983;&#25104;&#26032;&#39062;&#20869;&#23481;&#30340;&#25191;&#34892;&#32773;&#21644;&#19968;&#20010;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#35780;&#35770;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large transformers using next-token prediction has given rise to groundbreaking advancements in AI. While this generative AI approach has produced impressive results, it heavily leans on human supervision. Even state-of-the-art AI models like ChatGPT depend on fine-tuning through human demonstrations, demanding extensive human input and domain expertise. This strong reliance on human oversight poses a significant hurdle to the advancement of AI innovation. To address this limitation, we propose a novel paradigm termed Exploratory AI (EAI) aimed at autonomously generating high-quality training data. Drawing inspiration from unsupervised reinforcement learning (RL) pretraining, EAI achieves exploration within the natural language space. We accomplish this by harnessing large language models to assess the novelty of generated content. Our approach employs two key components: an actor that generates novel content following exploration principles and a critic that evaluates the gen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#36827;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#35777;&#26126;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.05028</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;
Revisiting Large Language Models as Zero-shot Relation Extractors. (arXiv:2310.05028v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#36827;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#35777;&#26126;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;(RE)&#21363;&#20351;&#22312;&#38646;-shot&#35774;&#23450;&#19979;&#65292;&#19968;&#30452;&#28041;&#21450;&#19968;&#23450;&#31243;&#24230;&#30340;&#26631;&#35760;&#25110;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#22312;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#21644;&#21442;&#25968;&#35843;&#25972;&#65292;&#33258;&#21160;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#36825;&#20026;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23558;LLMs&#65292;&#22914;ChatGPT&#65292;&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#30740;&#31350;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;RE&#25552;&#31034;&#30340;&#32570;&#28857;&#65292;&#24182;&#23581;&#35797;&#23558;&#26368;&#36817;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#22914;CoT&#65292;&#32435;&#20837;&#20854;&#20013;&#20197;&#25552;&#39640;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#36882;&#24402;&#20351;&#29992;LLMs&#23558;RE&#36755;&#20837;&#36716;&#25442;&#20026;&#26377;&#25928;&#30340;&#38382;&#31572;(QA)&#26684;&#24335;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#22522;&#20934;&#21644;&#35774;&#32622;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;LLMs&#22312;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#19978;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26377;&#20197;&#19979;&#30340;followi
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the followi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;PEFT&#26131;&#21463;&#29305;&#27931;&#20234;&#25915;&#20987;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;PETA&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#35302;&#21457;&#22120;&#35774;&#35745;&#20013;&#36827;&#34892;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;PETA&#33021;&#22815;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#26410;&#21463;&#24433;&#21709;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00648</link><description>&lt;p&gt;
&#26356;&#23569;&#23601;&#24847;&#21619;&#30528;&#26356;&#22810;: &#23545;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#29305;&#27931;&#20234;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning. (arXiv:2310.00648v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;PEFT&#26131;&#21463;&#29305;&#27931;&#20234;&#25915;&#20987;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;PETA&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#35302;&#21457;&#22120;&#35774;&#35745;&#20013;&#36827;&#34892;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;PETA&#33021;&#22815;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#26410;&#21463;&#24433;&#21709;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#39640;&#25928;&#22320;&#36866;&#24212;&#21040;&#29305;&#23450;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#20165;&#24494;&#35843;&#19968;&#23567;&#37096;&#20998;&#65288;&#39069;&#22806;&#30340;&#65289;&#21442;&#25968;&#65292;PEFT&#23454;&#29616;&#20102;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;PEFT&#30340;&#23433;&#20840;&#24615;&#24433;&#21709;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;PEFT&#23545;&#29305;&#27931;&#20234;&#25915;&#20987;&#30340;&#29420;&#29305;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PETA&#65292;&#19968;&#31181;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#32771;&#34385;&#19979;&#28216;&#36866;&#24212;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#24335;&#65306;&#19978;&#23618;&#30446;&#26631;&#23558;&#21518;&#38376;&#23884;&#20837;PLM&#20013;&#65292;&#32780;&#19979;&#23618;&#30446;&#26631;&#27169;&#25311;PEFT&#20197;&#20445;&#30041;PLM&#30340;&#20219;&#21153;&#29305;&#23450;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#35302;&#21457;&#22120;&#35774;&#35745;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PETA&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#26410;&#21463;&#24433;&#21709;&#30340;&#24178;&#20928;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#21463;&#23475;&#29992;&#25143;&#22312;&#20351;&#29992;&#32431;&#20928;&#25968;&#25454;&#23545;&#24102;&#26377;&#21518;&#38376;&#30340;PLM&#36827;&#34892;PEFT&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of pre-trained language models (PLMs) to specific tasks. By tuning only a minimal set of (extra) parameters, PEFT achieves performance comparable to full fine-tuning. However, despite its prevalent use, the security implications of PEFT remain largely unexplored. In this paper, we conduct a pilot study revealing that PEFT exhibits unique vulnerability to trojan attacks. Specifically, we present PETA, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM's task-specific performance. With extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. Moreover, we empi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;GPT&#27169;&#22411;&#22312;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2309.17169</link><description>&lt;p&gt;
&#35780;&#20272;GPT&#27169;&#22411;&#22312;&#34920;&#22411;&#27010;&#24565;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An evaluation of GPT models for phenotype concept recognition. (arXiv:2309.17169v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;GPT&#27169;&#22411;&#22312;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#22312;&#32597;&#35265;&#30142;&#30149;&#24739;&#32773;&#30340;&#35786;&#26029;&#20197;&#21450;&#26500;&#24314;&#21327;&#35843;&#25252;&#29702;&#35745;&#21010;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#35813;&#36807;&#31243;&#20381;&#36182;&#20110;&#20351;&#29992;&#26412;&#20307;&#27010;&#24565;&#23545;&#24739;&#32773;&#26723;&#26696;&#36827;&#34892;&#24314;&#27169;&#21644;&#25972;&#29702;&#65292;&#36890;&#24120;&#20351;&#29992;&#20154;&#31867;&#34920;&#22411;&#26412;&#20307;&#36827;&#34892;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#26469;&#25903;&#25345;&#36825;&#39033;&#34920;&#22411;&#27010;&#24565;&#35782;&#21035;&#20219;&#21153;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#24212;&#29992;&#65292;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#22312;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#20013;&#30340;&#24615;&#33021;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#30740;&#31350;&#30340;&#23454;&#39564;&#35774;&#32622;&#21253;&#25324;&#19971;&#20010;&#19981;&#21516;&#29305;&#24322;&#24615;&#32423;&#21035;&#30340;&#25552;&#31034;&#12289;&#20004;&#20010;GPT&#27169;&#22411;&#65288;gpt-3.5&#21644;gpt-4.0&#65289;&#20197;&#21450;&#19968;&#20010;&#24050;&#24314;&#31435;&#30340;&#34920;&#22411;&#35782;&#21035;&#40644;&#37329;&#26631;&#20934;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#30446;&#21069;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#26368;&#20339;&#36816;&#34892;&#32467;&#26524;&#37319;&#29992;&#20102;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Clinical deep phenotyping plays a critical role in both the diagnosis of patients with rare disorders as well as in building care coordination plans. The process relies on modelling and curating patient profiles using ontology concepts, usually from the Human Phenotype Ontology. Machine learning methods have been widely adopted to support this phenotype concept recognition task. With the significant shift in the use of large language models (LLMs) for most NLP tasks, herewithin, we examine the performance of the latest Generative Pre-trained Transformer (GPT) models underpinning ChatGPT in clinical deep phenotyping. Materials and Methods: The experimental setup of the study included seven prompts of various levels of specificity, two GPT models (gpt-3.5 and gpt-4.0) and an established gold standard for phenotype recognition. Results: Our results show that, currently, these models have not yet achieved state of the art performance. The best run, using few-shots learning, achi
&lt;/p&gt;</description></item><item><title>InstructERC&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#27169;&#26495;&#27169;&#22359;&#21644;&#39069;&#22806;&#30340;&#24773;&#24863;&#23545;&#40784;&#20219;&#21153;&#65292;&#25913;&#38761;&#20102;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.11911</link><description>&lt;p&gt;
InstructERC&#65306;&#20511;&#21161;&#26816;&#32034;&#22810;&#20219;&#21153;LLMs&#26694;&#26550;&#25913;&#38761;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework. (arXiv:2309.11911v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11911
&lt;/p&gt;
&lt;p&gt;
InstructERC&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#27169;&#26495;&#27169;&#22359;&#21644;&#39069;&#22806;&#30340;&#24773;&#24863;&#23545;&#40784;&#20219;&#21153;&#65292;&#25913;&#38761;&#20102;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;(ERC)&#30340;&#21457;&#23637;&#19968;&#30452;&#21463;&#21040;&#31649;&#36947;&#35774;&#35745;&#22797;&#26434;&#24615;&#30340;&#38459;&#30861;&#65292;&#23548;&#33268;ERC&#27169;&#22411;&#24448;&#24448;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#23545;&#35805;&#27169;&#24335;&#36807;&#25311;&#21512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;InstructERC&#65292;&#23558;ERC&#20219;&#21153;&#20174;&#21028;&#21035;&#24335;&#26694;&#26550;&#36716;&#21270;&#20026;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#26694;&#26550;&#12290;InstructERC&#26377;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;InstructERC&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26816;&#32034;&#27169;&#26495;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#21382;&#21490;&#23545;&#35805;&#20869;&#23481;&#12289;&#26631;&#31614;&#35821;&#21477;&#21644;&#24773;&#24863;&#39046;&#22495;&#28436;&#31034;&#19982;&#39640;&#35821;&#20041;&#30456;&#20284;&#24615;&#36827;&#34892;&#25340;&#25509;&#65292;&#24110;&#21161;&#27169;&#22411;&#26126;&#30830;&#22320;&#38598;&#25104;&#22810;&#31890;&#24230;&#23545;&#35805;&#30417;&#30563;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#39069;&#22806;&#30340;&#24773;&#24863;&#23545;&#40784;&#20219;&#21153;&#65292;&#21363;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#24773;&#24863;&#39044;&#27979;&#20219;&#21153;&#65292;&#20197;&#38544;&#24335;&#22320;&#24314;&#27169;&#23545;&#35805;&#35282;&#33394;&#20851;&#31995;&#21644;&#26410;&#26469;&#23545;&#35805;&#24773;&#32490;&#20542;&#21521;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;LLM&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The development of emotion recognition in dialogue (ERC) has been consistently hindered by the complexity of pipeline designs, leading to ERC models that often overfit to specific datasets and dialogue patterns. In this study, we propose a novel approach, namely  InstructERC, to reformulates the ERC task from a discriminative framework to a generative framework based on Large Language Models (LLMs) . InstructERC has two significant contributions: Firstly, InstructERC introduces a simple yet effective retrieval template module, which helps the model explicitly integrate multi-granularity dialogue supervision information by concatenating the historical dialog content, label statement, and emotional domain demonstrations with high semantic similarity. Furthermore, we introduce two additional emotion alignment tasks, namely speaker identification and emotion prediction tasks, to implicitly model the dialogue role relationships and future emotional tendencies in conversations. Our LLM-based
&lt;/p&gt;</description></item><item><title>&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;</title><link>http://arxiv.org/abs/2308.09687</link><description>&lt;p&gt;
&#24819;&#27861;&#22270;&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09687
&lt;/p&gt;
&lt;p&gt;
&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24819;&#27861;&#22270;&#65288;Graph of Thoughts&#65292;GoT&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25552;&#31034;&#33021;&#21147;&#19978;&#36229;&#36234;&#20102;Chain-of-Thought&#25110;Tree of Thoughts&#65288;ToT&#65289;&#31561;&#33539;&#24335;&#12290;GoT&#30340;&#20851;&#38190;&#24605;&#24819;&#21644;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#23558;LLM&#29983;&#25104;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#20854;&#20013;&#20449;&#24687;&#21333;&#20803;&#65288;"LLM&#24819;&#27861;"&#65289;&#26159;&#39030;&#28857;&#65292;&#36793;&#34920;&#31034;&#36825;&#20123;&#39030;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#23558;&#20219;&#24847;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#12289;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#20248;&#21183;&#65292;&#20363;&#22914;&#22312;&#25490;&#24207;&#20219;&#21153;&#19978;&#36136;&#37327;&#25552;&#39640;&#20102;62%&#65292;&#21516;&#26102;&#25104;&#26412;&#38477;&#20302;&#20102;&#36229;&#36807;31%&#12290;&#25105;&#20204;&#30830;&#20445;GoT&#33021;&#22815;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#24320;&#21019;&#26032;&#30340;&#25552;&#31034;&#26041;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#20351;&#24471;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinki
&lt;/p&gt;</description></item><item><title>Think-on-Graph&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.07697</link><description>&lt;p&gt;
Think-on-Graph: &#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph. (arXiv:2307.07697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07697
&lt;/p&gt;
&lt;p&gt;
Think-on-Graph&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#38656;&#35201;&#30693;&#35782;&#36861;&#28335;&#24615;&#12289;&#21450;&#26102;&#24615;&#21644;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#22330;&#26223;&#20013;&#65292;&#23427;&#20204;&#32463;&#24120;&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#34920;&#29616;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Think-on-Graph&#65288;ToG&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;LLMs&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;ToG&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#19982;&#32473;&#23450;&#38382;&#39064;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#24182;&#23545;&#22806;&#37096;&#30693;&#35782;&#25968;&#25454;&#24211;&#36827;&#34892;&#25506;&#32034;&#21644;&#25512;&#29702;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#19977;&#20803;&#32452;&#12290;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#29983;&#25104;&#21253;&#21547;&#39034;&#24207;&#36830;&#25509;&#30340;&#19977;&#20803;&#32452;&#30340;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#65292;&#30452;&#21040;&#25910;&#38598;&#21040;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#25110;&#36798;&#21040;&#26368;&#22823;&#28145;&#24230;&#20026;&#27490;&#12290;&#36890;&#36807;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ToG&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;LLMs&#30340;&#21069;&#36848;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant strides in various tasks, yet they often struggle with complex reasoning and exhibit poor performance in scenarios where knowledge traceability, timeliness, and accuracy are crucial. To address these limitations, we present Think-on-Graph (ToG), a novel framework that leverages knowledge graphs to enhance LLMs' ability for deep and responsible reasoning. By employing ToG, we can identify entities relevant to a given question and conduct exploration and reasoning to retrieve related triples from an external knowledge database. This iterative procedure generates multiple reasoning pathways consisting of sequentially connected triplets until sufficient information is gathered to answer the question or the maximum depth is reached. Through experiments on complex multi-hop reasoning question-answering tasks, we demonstrate that ToG outperforms existing methods, effectively addressing the aforementioned limitations of LLMs without incurring 
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;LLM&#30340;&#29305;&#28857;&#21644;&#21151;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#21457;&#29616;&#21644;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.06435</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overview of Large Language Models. (arXiv:2307.06435v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06435
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;LLM&#30340;&#29305;&#28857;&#21644;&#21151;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#21457;&#29616;&#21644;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;&#20102;&#20247;&#22810;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#20986;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#35843;&#25972;&#29616;&#26377;&#30340;&#26550;&#26500;&#65292;&#22686;&#21152;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#22686;&#21152;&#35757;&#32451;&#26102;&#38388;&#20197;&#36229;&#36234;&#22522;&#32447;&#12290;&#20998;&#26512;&#26032;&#30340;&#21457;&#23637;&#23545;&#20110;&#35782;&#21035;&#22686;&#24378;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#25913;&#36827;LLM&#27867;&#21270;&#33021;&#21147;&#30340;&#21464;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;LLM&#30340;&#26550;&#26500;&#21450;&#20854;&#20998;&#31867;&#12289;&#35757;&#32451;&#31574;&#30053;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;LLM&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#21644;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;LLM&#30340;&#23436;&#25972;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#37325;&#35201;&#29305;&#28857;&#21644;&#21151;&#33021;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;LLM&#30740;&#31350;&#30340;&#37325;&#35201;&#21457;&#29616;&#65292;&#24182;&#25972;&#21512;&#20102;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown excellent generalization capabilities that have led to the development of numerous models. These models propose various new architectures, tweaking existing architectures with refined training strategies, increasing context length, using high-quality training data, and increasing training time to outperform baselines. Analyzing new developments is crucial for identifying changes that enhance training stability and improve generalization in LLMs. This survey paper comprehensively analyses the LLMs architectures and their categorization, training strategies, training datasets, and performance evaluations and discusses future research directions. Moreover, the paper also discusses the basic building blocks and concepts behind LLMs, followed by a complete overview of LLMs, including their important features and functions. Finally, the paper summarizes significant findings from LLM research and consolidates essential architectural and training strateg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07848</link><description>&lt;p&gt;
GEmo-CLAP: &#38754;&#21521;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;CLAP&#65289;&#26368;&#36817;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEmo-CLAP&#30340;&#39640;&#25928;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;CLAP&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24773;&#24863;CLAP&#27169;&#22411;&#65288;&#31216;&#20026;Emo-CLAP&#65289;&#65292;&#29992;&#20110;SER&#12290;&#28982;&#21518;&#65292;&#32771;&#34385;&#21040;&#22312;&#35821;&#38899;&#24773;&#24863;&#24314;&#27169;&#20013;&#24615;&#21035;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#65292;&#26469;&#25972;&#21512;&#35821;&#38899;&#20449;&#21495;&#30340;&#24773;&#24863;&#21644;&#24615;&#21035;&#20449;&#24687;&#65292;&#24418;&#25104;&#26356;&#21512;&#29702;&#30340;&#30446;&#26631;&#12290;&#22312;IEMOCAP&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;Emo-CLAP&#27169;&#22411;&#65288;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#21516;&#26102;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Audio Pretraining (CLAP) has recently exhibited impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a kind of efficient gender-attribute-enhanced CLAP model for speech emotion recognition (SER). Specifically, we first build an effective emotion CLAP model termed Emo-CLAP for SER, utilizing various self-supervised learning based pre-trained models. Then, considering the importance of the gender attribute in speech emotion modeling, two GEmo-CLAP approaches are further proposed to integrate the emotion and gender information of speech signals, forming more reasonable objectives. Extensive experiments conducted on the IEMOCAP corpus demonstrate that our proposed two GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with different pre-trained models, while also achieving superior recognition performance compared with other state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>CrossGET&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21152;&#36895;&#26694;&#26550;&#65292;&#36890;&#36807;&#23454;&#26102;&#30340;&#36328;&#27169;&#24577;&#23548;&#24341;&#65292;&#33258;&#36866;&#24212;&#22320;&#32467;&#21512;&#20196;&#29260;&#65292;&#23454;&#29616;&#20102;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#30340;&#22823;&#24133;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2305.17455</link><description>&lt;p&gt;
CrossGET: &#36328;&#23548;&#24341;&#30340;&#20196;&#29260;&#38598;&#21512;&#29992;&#20110;&#21152;&#36895;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers. (arXiv:2305.17455v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17455
&lt;/p&gt;
&lt;p&gt;
CrossGET&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21152;&#36895;&#26694;&#26550;&#65292;&#36890;&#36807;&#23454;&#26102;&#30340;&#36328;&#27169;&#24577;&#23548;&#24341;&#65292;&#33258;&#36866;&#24212;&#22320;&#32467;&#21512;&#20196;&#29260;&#65292;&#23454;&#29616;&#20102;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#30340;&#22823;&#24133;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#36828;&#36828;&#36229;&#20986;&#20102;&#25105;&#20204;&#30340;&#39044;&#26399;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#38543;&#30528;&#24555;&#36895;&#21457;&#23637;&#20063;&#22312;&#22823;&#24133;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#22411;&#27169;&#22411;&#32780;&#35328;&#12290;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#21152;&#36895;&#21464;&#24471;&#26497;&#20854;&#20851;&#38190;&#12290;&#23613;&#31649;&#23545;&#20110;&#21333;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#30340;&#21152;&#36895;&#20173;&#28982;&#30456;&#23545;&#19981;&#36275;&#12290;&#20026;&#20102;&#36861;&#27714;&#26356;&#39640;&#25928;&#21644;&#21487;&#35775;&#38382;&#30340;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;CrossGET&#30340;&#36328;&#23548;&#24341;&#20196;&#29260;&#38598;&#21512;&#30340;&#36890;&#29992;&#21152;&#36895;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23454;&#26102;&#30340;&#36328;&#27169;&#24577;&#23548;&#24341;&#65292;&#33258;&#36866;&#24212;&#22320;&#32467;&#21512;&#20196;&#29260;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#24133;&#21152;&#36895;&#32780;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;CrossGET&#30340;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#28857;&#26159;&#65306;1) &#36328;&#23548;&#24341;&#21305;&#37197;&#21644;&#38598;&#21512;&#12290;CrossGET&#23558;&#36328;&#23548;&#24341;&#30340;&#21305;&#37197;&#21644;&#38598;&#21512;&#24212;&#29992;&#21040;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored. To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25509;&#35302;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21463;&#38480;&#28216;&#25103;&#24335;&#29615;&#22659;&#19979;&#65292;&#33021;&#21542;&#26377;&#24847;&#20041;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#30740;&#31350;&#20102;&#20116;&#31181;&#20132;&#20114;&#35774;&#32622;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;&#32842;&#22825;&#20248;&#21270;LLMs&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33021;&#22815;&#36981;&#24490;&#28216;&#25103;&#29609;&#27861;&#25351;&#20196;&#12290;&#36825;&#23545;&#20110;&#23558;LLMs&#24320;&#21457;&#20026;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#23545;&#35805;&#20195;&#29702;&#20154;&#20855;&#26377;&#21551;&#31034;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.13455</link><description>&lt;p&gt;
clembench&#65306;&#20351;&#29992;&#28216;&#25103;&#26469;&#35780;&#20272;&#20316;&#20026;&#23545;&#35805;&#20195;&#29702;&#20154;&#30340;&#32842;&#22825;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents. (arXiv:2305.13455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25509;&#35302;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21463;&#38480;&#28216;&#25103;&#24335;&#29615;&#22659;&#19979;&#65292;&#33021;&#21542;&#26377;&#24847;&#20041;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#30740;&#31350;&#20102;&#20116;&#31181;&#20132;&#20114;&#35774;&#32622;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;&#32842;&#22825;&#20248;&#21270;LLMs&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33021;&#22815;&#36981;&#24490;&#28216;&#25103;&#29609;&#27861;&#25351;&#20196;&#12290;&#36825;&#23545;&#20110;&#23558;LLMs&#24320;&#21457;&#20026;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#23545;&#35805;&#20195;&#29702;&#20154;&#20855;&#26377;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#8220;&#31449;&#31435;&#35821;&#35328;&#29702;&#35299;&#20195;&#29702;&#8221;&#30340;&#31995;&#32479;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#20195;&#29702;&#22312;&#20016;&#23500;&#30340;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#36890;&#36807;&#22312;&#31934;&#24515;&#26500;&#36896;&#30340;&#20114;&#21160;&#29615;&#22659;&#20013;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;&#23427;&#20204;&#12290;&#20854;&#20182;&#26368;&#36817;&#30340;&#24037;&#20316;&#21017;&#35748;&#20026;&#65292;&#22914;&#26524;&#36866;&#24403;&#35774;&#32622;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#36825;&#26679;&#30340;&#20195;&#29702;&#65288;&#30340;&#27169;&#25311;&#22120;&#65289;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#31181;&#32852;&#31995;&#65306;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#35753;LLMs&#25509;&#35302;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21463;&#38480;&#28216;&#25103;&#24335;&#29615;&#22659;&#26469;&#26377;&#24847;&#20041;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#65311;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20116;&#31181;&#20132;&#20114;&#35774;&#32622;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;&#32842;&#22825;&#20248;&#21270;LLMs&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33021;&#22815;&#36981;&#24490;&#28216;&#25103;&#29609;&#27861;&#25351;&#20196;&#12290;&#36825;&#31181;&#33021;&#21147;&#21644;&#28216;&#25103;&#29609;&#27861;&#30340;&#36136;&#37327;&#65288;&#36890;&#36807;&#28385;&#36275;&#19981;&#21516;&#28216;&#25103;&#30446;&#26631;&#30340;&#24773;&#20917;&#26469;&#34913;&#37327;&#65289;&#37117;&#36981;&#24490;&#30528;&#21457;&#23637;&#24490;&#29615;&#65292;&#26032;&#22411;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#21363;&#20351;&#26159;&#30456;&#23545;&#31616;&#21333;&#30340;&#8220;&#20117;&#23383;&#28216;&#25103;&#8221;&#31034;&#20363;&#28216;&#25103;&#30340;&#25351;&#26631;&#20063;&#25552;&#20379;&#20102;&#27169;&#22411;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#30340;&#22522;&#26412;&#24615;&#33021;&#25351;&#31034;&#12290;&#36825;&#23545;&#20110;&#23558;LLMs&#24320;&#21457;&#20026;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#23545;&#35805;&#20195;&#29702;&#20154;&#20855;&#26377;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has proposed a methodology for the systematic evaluation of "Situated Language Understanding Agents"-agents that operate in rich linguistic and non-linguistic contexts-through testing them in carefully constructed interactive settings. Other recent work has argued that Large Language Models (LLMs), if suitably set up, can be understood as (simulators of) such agents. A connection suggests itself, which this paper explores: Can LLMs be evaluated meaningfully by exposing them to constrained game-like settings that are built to challenge specific capabilities? As a proof of concept, this paper investigates five interaction settings, showing that current chat-optimised LLMs are, to an extent, capable to follow game-play instructions. Both this capability and the quality of the game play, measured by how well the objectives of the different games are met, follows the development cycle, with newer models performing better. The metrics even for the comparatively simple example gam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;Transformer&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#25237;&#24433;&#21040;&#20854;&#35789;&#27719;&#34920;&#20013;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20869;&#37096;&#20449;&#24687;&#27969;&#30340;&#27169;&#24335;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#23558;GPT&#30340;&#21069;&#21521;&#20256;&#36882;&#21487;&#35270;&#21270;&#20026;&#20132;&#20114;&#24335;&#27969;&#22270;&#65292;&#31616;&#21270;&#20102;&#22823;&#37327;&#25968;&#25454;&#20026;&#26131;&#20110;&#38405;&#35835;&#30340;&#22270;&#34920;&#65292;&#23637;&#31034;&#20102;&#20854;&#35821;&#20041;&#20449;&#24687;&#27969;&#12290;</title><link>http://arxiv.org/abs/2305.13417</link><description>&lt;p&gt;
&#35299;&#35835;Transformer&#30340;&#27880;&#24847;&#21147;&#21160;&#24577;&#20869;&#23384;&#65292;&#21487;&#35270;&#21270;GPT&#30340;&#35821;&#20041;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Interpreting Transformer's Attention Dynamic Memory and Visualizing the Semantic Information Flow of GPT. (arXiv:2305.13417v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;Transformer&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#25237;&#24433;&#21040;&#20854;&#35789;&#27719;&#34920;&#20013;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20869;&#37096;&#20449;&#24687;&#27969;&#30340;&#27169;&#24335;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#23558;GPT&#30340;&#21069;&#21521;&#20256;&#36882;&#21487;&#35270;&#21270;&#20026;&#20132;&#20114;&#24335;&#27969;&#22270;&#65292;&#31616;&#21270;&#20102;&#22823;&#37327;&#25968;&#25454;&#20026;&#26131;&#20110;&#38405;&#35835;&#30340;&#22270;&#34920;&#65292;&#23637;&#31034;&#20102;&#20854;&#35821;&#20041;&#20449;&#24687;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#36827;&#23637;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#23558;&#22522;&#20110;transformer&#27169;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#25237;&#24433;&#21040;&#20854;&#35789;&#27719;&#34920;&#20013;&#65292;&#36825;&#31181;&#36716;&#25442;&#20351;&#23427;&#20204;&#21464;&#24471;&#26356;&#23481;&#26131;&#29702;&#35299;&#65292;&#24182;&#19988;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#35821;&#20041;&#20998;&#37197;&#21040;&#20165;&#20316;&#20026;&#25968;&#23383;&#21521;&#37327;&#30340;&#20869;&#23481;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;LM&#27880;&#24847;&#21147;&#22836;&#21644;&#20869;&#23384;&#20540;&#65292;&#36825;&#20123;&#21521;&#37327;&#26159;&#27169;&#22411;&#22312;&#22788;&#29702;&#32473;&#23450;&#36755;&#20837;&#26102;&#21160;&#24577;&#22320;&#21019;&#24314;&#21644;&#26816;&#32034;&#30340;&#12290;&#36890;&#36807;&#36890;&#36807;&#36825;&#31181;&#25237;&#24433;&#20998;&#26512;&#23427;&#20204;&#25152;&#20195;&#34920;&#30340;&#26631;&#35760;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20869;&#37096;&#20449;&#24687;&#27969;&#30340;&#27169;&#24335;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#23558;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65288;GPT&#65289;&#30340;&#21069;&#21521;&#20256;&#36882;&#21487;&#35270;&#21270;&#20026;&#20132;&#20114;&#24335;&#27969;&#22270;&#65292;&#20854;&#20013;&#32467;&#28857;&#20195;&#34920;&#31070;&#32463;&#20803;&#25110;&#38544;&#34255;&#29366;&#24577;&#65292;&#36793;&#20195;&#34920;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#21487;&#35270;&#21270;&#23558;&#28023;&#37327;&#25968;&#25454;&#31616;&#21270;&#20026;&#26131;&#20110;&#38405;&#35835;&#30340;&#22270;&#34920;&#65292;&#21453;&#26144;&#20102;&#27169;&#22411;&#20026;&#20160;&#20040;&#36755;&#20986;&#20854;&#32467;&#26524;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#23450;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#24182;&#21487;&#35270;&#21270;&#20854;&#35821;&#20041;&#20449;&#24687;&#27969;&#26469;&#28436;&#31034;&#25105;&#20204;&#24314;&#27169;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in interpretability suggest we can project weights and hidden states of transformer-based language models (LMs) to their vocabulary, a transformation that makes them human interpretable and enables us to assign semantics to what was seen only as numerical vectors. In this paper, we interpret LM attention heads and memory values, the vectors the models dynamically create and recall while processing a given input. By analyzing the tokens they represent through this projection, we identify patterns in the information flow inside the attention mechanism. Based on these discoveries, we create a tool to visualize a forward pass of Generative Pre-trained Transformers (GPTs) as an interactive flow graph, with nodes representing neurons or hidden states and edges representing the interactions between them. Our visualization simplifies huge amounts of data into easy-to-read plots that reflect why models output their results. We demonstrate the utility of our modeling by identifyi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#20294;&#36825;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2303.10475</link><description>&lt;p&gt;
&#20165;&#20165;&#25552;&#31034;&#36275;&#22815;&#20102;&#21527;&#65311;&#19981;&#26159;&#30340;&#12290;&#25351;&#23548;&#23398;&#20064;&#30340;&#20840;&#38754;&#21644;&#26356;&#24191;&#38420;&#35270;&#35282;&#65288;arXiv&#65306;2303.10475v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning. (arXiv:2303.10475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10475
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#20294;&#36825;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#35821;&#20041;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#25110;&#19968;&#26465;&#25991;&#26412;&#25351;&#20196;&#26469;&#34920;&#36798;&#12290;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#24341;&#36215;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#25910;&#38598;&#20219;&#21153;&#29305;&#23450;&#26631;&#35760;&#31034;&#20363;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#65292;&#25110;&#32773;&#31995;&#32479;&#38656;&#35201;&#31435;&#21363;&#22788;&#29702;&#26032;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#36825;&#19981;&#26159;&#29992;&#25143;&#21451;&#22909;&#30340;&#65292;&#22240;&#20026;&#26368;&#32456;&#29992;&#25143;&#21487;&#33021;&#26356;&#24895;&#24847;&#22312;&#20351;&#29992;&#31995;&#32479;&#20043;&#21069;&#25552;&#20379;&#20219;&#21153;&#25551;&#36848;&#32780;&#19981;&#26159;&#19968;&#32452;&#31034;&#20363;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#31038;&#21306;&#20173;&#28982;&#38754;&#20020;&#30528;&#19968;&#20123;&#20849;&#21516;&#30340;&#38382;&#39064;&#12290;&#26412;&#27425;&#35843;&#26597;&#26088;&#22312;&#24635;&#32467;&#25351;&#23548;&#23398;&#20064;&#30340;&#24403;&#21069;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;
&lt;/p&gt;
&lt;p&gt;
Task semantics can be expressed by a set of input-to-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: first, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: learning from task instructions. Despite its impressive progress, there are some common issues that the community struggles with. This survey paper tries to summarize the current research on instruction learning, particularly, by answering the following questions:
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;tieval&#65292;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#20449;&#24687;&#25277;&#21462;&#31995;&#32479;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23427;&#25552;&#20379;&#20102;&#26631;&#20934;&#30340;&#25509;&#21475;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#20811;&#26381;&#19981;&#21516;&#25968;&#25454;&#38598;&#27880;&#37322;&#20307;&#31995;&#30340;&#24046;&#24322;&#12289;&#35299;&#26512;&#19981;&#21516;&#35821;&#26009;&#24211;&#30340;&#26684;&#24335;&#21644;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20960;&#20010;TIE&#31995;&#32479;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;tieval&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.04643</link><description>&lt;p&gt;
tieval&#65306;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#20449;&#24687;&#25277;&#21462;&#31995;&#32479;&#35780;&#20272;&#30340;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
tieval: An Evaluation Framework for Temporal Information Extraction Systems. (arXiv:2301.04643v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;tieval&#65292;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#20449;&#24687;&#25277;&#21462;&#31995;&#32479;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23427;&#25552;&#20379;&#20102;&#26631;&#20934;&#30340;&#25509;&#21475;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#20811;&#26381;&#19981;&#21516;&#25968;&#25454;&#38598;&#27880;&#37322;&#20307;&#31995;&#30340;&#24046;&#24322;&#12289;&#35299;&#26512;&#19981;&#21516;&#35821;&#26009;&#24211;&#30340;&#26684;&#24335;&#21644;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20960;&#20010;TIE&#31995;&#32479;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;tieval&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20108;&#21313;&#24180;&#26469;&#65292;&#26102;&#38388;&#20449;&#24687;&#25277;&#21462;(TIE)&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#25512;&#21160;&#20102;&#22823;&#37327;&#25968;&#25454;&#38598;&#30340;&#24320;&#21457;&#12290;&#28982;&#32780;&#65292;&#25317;&#26377;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#22909;&#22788;&#19982;&#27492;&#21516;&#26102;&#20063;&#20351;&#24471;&#23545;TIE&#31995;&#32479;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21464;&#24471;&#22256;&#38590;&#12290;&#19981;&#21516;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#21516;&#30340;&#27880;&#37322;&#20307;&#31995;&#65292;&#36825;&#20351;&#24471;&#27604;&#36739;&#19981;&#21516;&#35821;&#26009;&#24211;&#20013;&#30340;&#31454;&#20105;&#23545;&#25163;&#21464;&#24471;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#35821;&#26009;&#24211;&#36890;&#24120;&#37319;&#29992;&#19981;&#21516;&#30340;&#26684;&#24335;&#36827;&#34892;&#20256;&#25773;&#65292;&#22240;&#27492;&#38656;&#35201;&#30740;&#31350;&#20154;&#21592;/&#20174;&#19994;&#20154;&#21592;&#22312;&#24320;&#21457;&#25152;&#26377;&#35821;&#26009;&#24211;&#30340;&#35299;&#26512;&#22120;&#26102;&#20184;&#20986;&#22823;&#37327;&#30340;&#24037;&#31243;&#21162;&#21147;&#12290;&#36825;&#31181;&#38480;&#21046;&#36843;&#20351;&#30740;&#31350;&#20154;&#21592;&#36873;&#25321;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#20182;&#20204;&#30340;&#31995;&#32479;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#31995;&#32479;&#30340;&#27604;&#36739;&#12290;&#21478;&#19968;&#20010;&#38459;&#30861;TIE&#31995;&#32479;&#21487;&#27604;&#24615;&#30340;&#38556;&#30861;&#26159;&#35780;&#20272;&#25351;&#26631;&#30340;&#37319;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;tieval&#65292;&#19968;&#31181;TIE&#31995;&#32479;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20026;&#35821;&#26009;&#24211;&#35775;&#38382;&#12289;&#40644;&#37329;&#26631;&#20934;&#34920;&#31034;&#21644;&#35780;&#20272;&#25351;&#26631;&#25552;&#20379;&#20102;&#26631;&#20934;&#25509;&#21475;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;tieval&#35780;&#20272;TIE&#31995;&#32479;&#65292;&#24182;&#23545;TIE&#31038;&#21306;&#20013;&#20351;&#29992;&#30340;&#20004;&#20010;&#40644;&#37329;&#26631;&#20934;&#30340;&#20960;&#20010;TIE&#31995;&#32479;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal information extraction (TIE) has attracted a great deal of interest over the last two decades, leading to the development of a significant number of datasets. Despite its benefits, having access to a large volume of corpora makes it difficult when it comes to benchmark TIE systems. On the one hand, different datasets have different annotation schemes, thus hindering the comparison between competitors across different corpora. On the other hand, the fact that each corpus is commonly disseminated in a different format requires a considerable engineering effort for a researcher/practitioner to develop parsers for all of them. This constraint forces researchers to select a limited amount of datasets to evaluate their systems which consequently limits the comparability of the systems. Yet another obstacle that hinders the comparability of the TIE systems is the evaluation metric employed. While most research works adopt traditional metrics such as precision, recall, and $F_1$, a fe
&lt;/p&gt;</description></item></channel></rss>