<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24418;&#24577;&#23398;&#24863;&#30693;&#26694;&#26550; M2C&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#27979;&#35797;&#26469;&#35780;&#20272; NLP &#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#29305;&#24449;&#19979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#33521;&#35821;&#20013;&#65292;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26031;&#29926;&#24076;&#37324;&#35821;&#30340;&#26102;&#38388;&#34920;&#36798;&#21644;&#33452;&#20848;&#35821;&#30340;&#21512;&#25104;&#25152;&#26377;&#26684;&#31561;&#29305;&#23450;&#31867;&#22411;&#29305;&#24449;&#19978;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#33021;&#22815;&#35299;&#20915;&#36825;&#20123;&#30450;&#28857;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.05454</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#31867;&#22411;&#29305;&#24449;&#22686;&#24378;&#36328;&#35821;&#35328;&#34892;&#20026;&#27979;&#35797;&#30340; NLP &#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features. (arXiv:2307.05454v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05454
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24418;&#24577;&#23398;&#24863;&#30693;&#26694;&#26550; M2C&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#27979;&#35797;&#26469;&#35780;&#20272; NLP &#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#29305;&#24449;&#19979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#33521;&#35821;&#20013;&#65292;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26031;&#29926;&#24076;&#37324;&#35821;&#30340;&#26102;&#38388;&#34920;&#36798;&#21644;&#33452;&#20848;&#35821;&#30340;&#21512;&#25104;&#25152;&#26377;&#26684;&#31561;&#29305;&#23450;&#31867;&#22411;&#29305;&#24449;&#19978;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#33021;&#22815;&#35299;&#20915;&#36825;&#20123;&#30450;&#28857;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#38754;&#21521;&#19990;&#30028;&#21508;&#35821;&#35328;&#30340; NLP &#31995;&#32479;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#29702;&#35299;&#23427;&#20204;&#22312;&#19982;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30456;&#20851;&#30340;&#31867;&#22411;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; M2C&#65292;&#19968;&#20010;&#23545; NLP &#27169;&#22411;&#36827;&#34892;&#34892;&#20026;&#27979;&#35797;&#30340;&#24418;&#24577;&#23398;&#24863;&#30693;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992; M2C &#29983;&#25104;&#27979;&#35797;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#22312;12&#31181;&#31867;&#22411;&#22810;&#26679;&#30340;&#35821;&#35328;&#20013;&#38024;&#23545;&#29305;&#23450;&#35821;&#35328;&#29305;&#24449;&#34920;&#29616;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#29983;&#25104;&#30340;&#27979;&#35797;&#19978;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#34429;&#28982;&#27169;&#22411;&#22312;&#33521;&#35821;&#19978;&#30340;&#22823;&#22810;&#25968;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#26031;&#29926;&#24076;&#37324;&#35821;&#30340;&#26102;&#38388;&#34920;&#36798;&#21644;&#33452;&#20848;&#35821;&#30340;&#21512;&#25104;&#25152;&#26377;&#26684;&#31561;&#29305;&#23450;&#31867;&#22411;&#29305;&#24449;&#30340;&#27867;&#21270;&#22833;&#36133;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20419;&#36827;&#20102;&#24320;&#21457;&#33021;&#22815;&#35299;&#20915;&#36825;&#20123;&#30450;&#28857;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A challenge towards developing NLP systems for the world's languages is understanding how they generalize to typological differences relevant for real-world applications. To this end, we propose M2C, a morphologically-aware framework for behavioral testing of NLP models. We use M2C to generate tests that probe models' behavior in light of specific linguistic features in 12 typologically diverse languages. We evaluate state-of-the-art language models on the generated tests. While models excel at most tests in English, we highlight generalization failures to specific typological characteristics such as temporal expressions in Swahili and compounding possessives in Finish. Our findings motivate the development of models that address these blind spots.
&lt;/p&gt;</description></item><item><title>ISLTranslate&#26159;&#19968;&#20010;&#21253;&#21547;31k&#20010;ISL-&#33521;&#35821;&#21477;&#23376;/&#30701;&#35821;&#23545;&#30340;&#26368;&#22823;&#36830;&#32493;&#21360;&#24230;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#24110;&#21161;&#24320;&#21457;&#25163;&#35821;&#32763;&#35793;&#31995;&#32479;&#65292;&#35299;&#20915;&#21360;&#24230;&#25163;&#35821;&#36164;&#28304;&#21294;&#20047;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05440</link><description>&lt;p&gt;
ISLTranslate: &#32763;&#35793;&#21360;&#24230;&#25163;&#35821;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ISLTranslate: Dataset for Translating Indian Sign Language. (arXiv:2307.05440v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05440
&lt;/p&gt;
&lt;p&gt;
ISLTranslate&#26159;&#19968;&#20010;&#21253;&#21547;31k&#20010;ISL-&#33521;&#35821;&#21477;&#23376;/&#30701;&#35821;&#23545;&#30340;&#26368;&#22823;&#36830;&#32493;&#21360;&#24230;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#24110;&#21161;&#24320;&#21457;&#25163;&#35821;&#32763;&#35793;&#31995;&#32479;&#65292;&#35299;&#20915;&#21360;&#24230;&#25163;&#35821;&#36164;&#28304;&#21294;&#20047;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#26159;&#20840;&#29699;&#35768;&#22810;&#21548;&#38556;&#20154;&#22763;&#30340;&#20027;&#35201;&#36890;&#20449;&#26041;&#24335;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#24357;&#34917;&#21548;&#38556;&#31038;&#21306;&#19982;&#20854;&#20182;&#20154;&#32676;&#20043;&#38388;&#30340;&#27807;&#36890;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#20960;&#20010;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#24320;&#21457;&#32479;&#35745;&#25163;&#35821;&#32763;&#35793;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#21360;&#24230;&#25163;&#35821;&#30340;&#36164;&#28304;&#21294;&#20047;&#12290;&#26412;&#36164;&#28304;&#35770;&#25991;&#20171;&#32461;&#20102;ISLTranslate&#65292;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#21360;&#24230;&#25163;&#35821;&#65288;ISL&#65289;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;31k&#20010;ISL-&#33521;&#35821;&#21477;&#23376;/&#30701;&#35821;&#23545;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#36830;&#32493;&#21360;&#24230;&#25163;&#35821;&#26368;&#22823;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#20026;&#20102;&#39564;&#35777;&#29616;&#26377;&#30340;&#31471;&#21040;&#31471;&#25163;&#35821;&#21040;&#21475;&#35821;&#32763;&#35793;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;ISL&#32763;&#35793;&#23545;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign languages are the primary means of communication for many hard-of-hearing people worldwide. Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several sign language translation datasets have been proposed to enable the development of statistical sign language translation systems. However, there is a dearth of sign language resources for the Indian sign language. This resource paper introduces ISLTranslate, a translation dataset for continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language. We provide a detailed analysis of the dataset. To validate the performance of existing end-to-end Sign language to spoken language translation systems, we benchmark the created dataset with a transformer-based model for ISL translation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#36716;&#25442;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#32534;&#30721;&#22120;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;Duncode&#32534;&#30721;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32534;&#30721;&#25972;&#20010;Unicode&#23383;&#31526;&#38598;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#31354;&#38388;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.05414</link><description>&lt;p&gt;
Duncode&#23383;&#31526;&#26356;&#30701;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Duncode Characters Shorter. (arXiv:2307.05414v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#36716;&#25442;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#32534;&#30721;&#22120;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;Duncode&#32534;&#30721;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32534;&#30721;&#25972;&#20010;Unicode&#23383;&#31526;&#38598;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#31354;&#38388;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25991;&#26412;&#36716;&#25442;&#20013;&#20351;&#29992;&#21508;&#31181;&#32534;&#30721;&#22120;&#65292;&#23558;&#23383;&#31526;&#36716;&#25442;&#20026;&#23383;&#33410;&#12290;&#35752;&#35770;&#20102;&#26412;&#22320;&#32534;&#30721;&#22120;&#65288;&#22914;ASCII&#21644;GB-2312&#65289;&#65292;&#23427;&#20204;&#23558;&#29305;&#23450;&#23383;&#31526;&#32534;&#30721;&#20026;&#36739;&#30701;&#30340;&#23383;&#33410;&#65292;&#20197;&#21450;&#36890;&#29992;&#32534;&#30721;&#22120;&#65288;&#22914;UTF-8&#21644;UTF-16&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#20351;&#29992;&#26356;&#22810;&#30340;&#31354;&#38388;&#26469;&#32534;&#30721;&#23436;&#25972;&#30340;Unicode&#23383;&#31526;&#38598;&#65292;&#24182;&#24471;&#21040;&#24191;&#27867;&#25509;&#21463;&#12290;&#28982;&#32780;&#65292;&#20854;&#20182;&#32534;&#30721;&#22120;&#65288;&#21253;&#25324;SCSU&#65292;BOCU-1&#21644;&#20108;&#36827;&#21046;&#32534;&#30721;&#22120;&#65289;&#32570;&#20047;&#33258;&#21516;&#27493;&#21151;&#33021;&#12290;Duncode&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#26088;&#22312;&#20197;&#39640;&#31354;&#38388;&#25928;&#29575;&#32534;&#30721;&#25972;&#20010;Unicode&#23383;&#31526;&#38598;&#65292;&#31867;&#20284;&#20110;&#26412;&#22320;&#32534;&#30721;&#22120;&#12290;&#23427;&#26377;&#28508;&#21147;&#20351;&#29992;&#36739;&#23569;&#30340;&#23383;&#33410;&#23558;&#23383;&#31526;&#20018;&#30340;&#22810;&#20010;&#23383;&#31526;&#21387;&#32553;&#20026;&#19968;&#20010;Duncode&#21333;&#20803;&#12290;&#23613;&#31649;&#25552;&#20379;&#20102;&#36739;&#23569;&#30340;&#33258;&#21516;&#27493;&#35782;&#21035;&#20449;&#24687;&#65292;Duncode&#22312;&#31354;&#38388;&#25928;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;UTF8&#12290;&#24212;&#29992;&#31243;&#24207;&#21487;&#22312;\url{https://github.com/laohur/duncode}&#20013;&#25214;&#21040;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the employment of various encoders in text transformation, converting characters into bytes. It discusses local encoders such as ASCII and GB-2312, which encode specific characters into shorter bytes, and universal encoders like UTF-8 and UTF-16, which can encode the complete Unicode set with greater space requirements and are gaining widespread acceptance. Other encoders, including SCSU, BOCU-1, and binary encoders, however, lack self-synchronizing capabilities. Duncode is introduced as an innovative encoding method that aims to encode the entire Unicode character set with high space efficiency, akin to local encoders. It has the potential to compress multiple characters of a string into a Duncode unit using fewer bytes. Despite offering less self-synchronizing identification information, Duncode surpasses UTF8 in terms of space efficiency. The application is available at \url{https://github.com/laohur/duncode}. Additionally, we have developed a benchmark for e
&lt;/p&gt;</description></item><item><title>BLUEX&#26159;&#19968;&#20010;&#22522;&#20110;&#24052;&#35199;&#39030;&#23574;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#20026;&#35780;&#20272;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#27880;&#37322;&#22270;&#20687;&#20301;&#32622;&#65292;&#20419;&#36827;&#22810;&#27169;&#24577;&#35821;&#35328;&#29702;&#35299;&#21644;&#26816;&#32034;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.05410</link><description>&lt;p&gt;
BLUEX:&#19968;&#31181;&#22522;&#20110;&#24052;&#35199;&#39030;&#23574;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BLUEX: A benchmark based on Brazilian Leading Universities Entrance eXams. (arXiv:2307.05410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05410
&lt;/p&gt;
&lt;p&gt;
BLUEX&#26159;&#19968;&#20010;&#22522;&#20110;&#24052;&#35199;&#39030;&#23574;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#20026;&#35780;&#20272;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#27880;&#37322;&#22270;&#20687;&#20301;&#32622;&#65292;&#20419;&#36827;&#22810;&#27169;&#24577;&#35821;&#35328;&#29702;&#35299;&#21644;&#26816;&#32034;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#30740;&#31350;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#36235;&#21183;&#26159;&#20351;&#29992;&#26631;&#20934;&#21270;&#27979;&#35797;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#33889;&#33796;&#29273;&#35821;&#26159;&#20840;&#29699;&#31532;&#20116;&#22823;&#20351;&#29992;&#35821;&#35328;&#65292;&#20294;&#22312;&#33889;&#33796;&#29273;&#35821;&#20013;&#36827;&#34892;&#36825;&#26679;&#30340;&#35780;&#20272;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#21487;&#20379;&#31038;&#21306;&#36827;&#34892;&#33889;&#33796;&#29273;&#35821;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24052;&#35199;&#39046;&#20808;&#30340;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#65288;BLUEX&#65289;&#65292;&#36825;&#26159;&#24052;&#35199;&#20004;&#25152;&#39030;&#23574;&#22823;&#23398;UNICAMP&#21644;USP&#30340;&#20837;&#23398;&#32771;&#35797;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#22312;&#21508;&#31181;&#31185;&#30446;&#19978;&#24615;&#33021;&#30340;&#24102;&#27880;&#37322;&#30340;&#20803;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;BLUEX&#36824;&#21253;&#25324;&#19968;&#31995;&#21015;&#26368;&#36817;&#36827;&#34892;&#30340;&#32771;&#35797;&#65292;&#36825;&#20123;&#32771;&#35797;&#19981;&#22826;&#21487;&#33021;&#21253;&#21547;&#22312;2023&#24180;&#20043;&#21069;&#35768;&#22810;&#27969;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#20197;&#25351;&#31034;&#27599;&#20010;&#38382;&#39064;&#20013;&#22270;&#20687;&#30340;&#20301;&#32622;&#65292;&#20026;&#25512;&#21160;&#22810;&#27169;&#24577;&#35821;&#35328;&#29702;&#35299;&#21644;&#26816;&#32034;&#25216;&#26415;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
One common trend in recent studies of language models (LMs) is the use of standardized tests for evaluation. However, despite being the fifth most spoken language worldwide, few such evaluations have been conducted in Portuguese. This is mainly due to the lack of high-quality datasets available to the community for carrying out evaluations in Portuguese. To address this gap, we introduce the Brazilian Leading Universities Entrance eXams (BLUEX), a dataset of entrance exams from the two leading universities in Brazil: UNICAMP and USP. The dataset includes annotated metadata for evaluating the performance of NLP models on a variety of subjects. Furthermore, BLUEX includes a collection of recently administered exams that are unlikely to be included in the training data of many popular LMs as of 2023. The dataset is also annotated to indicate the position of images in each question, providing a valuable resource for advancing the state-of-the-art in multimodal language understanding and re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#65292;&#37325;&#28857;&#20851;&#27880;Python&#32534;&#31243;&#35821;&#35328;&#21644;&#25968;&#25454;&#32467;&#26500;&#31639;&#27861;&#20004;&#20010;&#22522;&#30784;&#20027;&#39064;&#12290;&#24635;&#32467;&#27979;&#35797;&#20013;ChatGPT&#30340;&#20195;&#30721;&#35299;&#20915;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12289;&#20195;&#30721;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2307.05360</link><description>&lt;p&gt;
&#25581;&#24320;&#24040;&#20154;&#30340;&#30495;&#38754;&#30446;&#65306;&#23545;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures. (arXiv:2307.05360v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#65292;&#37325;&#28857;&#20851;&#27880;Python&#32534;&#31243;&#35821;&#35328;&#21644;&#25968;&#25454;&#32467;&#26500;&#31639;&#27861;&#20004;&#20010;&#22522;&#30784;&#20027;&#39064;&#12290;&#24635;&#32467;&#27979;&#35797;&#20013;ChatGPT&#30340;&#20195;&#30721;&#35299;&#20915;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12289;&#20195;&#30721;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#28145;&#21051;&#22320;&#37325;&#22609;&#20102;&#20154;&#24037;&#26234;&#33021;(AI)&#25216;&#26415;&#39046;&#22495;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ChatGPT&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#26377;&#30528;&#29420;&#29305;&#20043;&#22788;&#65292;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22810;&#36718;&#23545;&#35805;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#23637;&#31034;&#20986;&#23545;&#32534;&#30721;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#23545;ChatGPT&#30340;&#32534;&#30721;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;Python&#32534;&#31243;&#35821;&#35328;&#65292;&#20197;&#21450;&#38598;&#20013;&#22312;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#19978;&#30340;&#38382;&#39064;&#65292;&#36825;&#20004;&#20010;&#20027;&#39064;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35780;&#20272;ChatGPT&#35299;&#20915;&#25152;&#25552;&#20132;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#35780;&#20272;&#20854;&#20195;&#30721;&#36136;&#37327;&#20197;&#21450;&#20195;&#30721;&#24341;&#21457;&#30340;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;&#24403;ChatGPT&#30340;&#20195;&#30721;&#25104;&#21151;&#25191;&#34892;&#20294;&#26410;&#33021;&#35299;&#20915;&#25163;&#22836;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#20250;&#30740;&#31350;&#36890;&#36807;&#30340;&#27979;&#35797;&#26696;&#20363;&#20013;&#30340;&#27169;&#24335;&#65292;&#20197;&#20102;&#35299;ChatGPT&#20195;&#30721;&#20013;&#30340;&#38169;&#35823;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformative influence of Large Language Models (LLMs) is profoundly reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT distinguishes itself within these models, demonstrating remarkable performance in multi-turn conversations and exhibiting code proficiency across an array of languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's coding capabilities based on what is to date the largest catalog of coding challenges. Our focus is on the python programming language and problems centered on data structures and algorithms, two topics at the very foundations of Computer Science. We evaluate ChatGPT for its ability to generate correct solutions to the problems fed to it, its code quality, and nature of run-time errors thrown by its code. Where ChatGPT code successfully executes, but fails to solve the problem at hand, we look into patterns in the test cases passed in order to gain some insights into how wrong ChatGPT code is in these 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UniCoRN&#65292;&#23427;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#35748;&#30693;&#20449;&#21495;&#37325;&#24314;&#31995;&#32479;&#65292;&#26088;&#22312;&#23558;fMRI&#26102;&#38388;&#24207;&#21015;&#19982;&#20154;&#31867;&#35821;&#35328;&#26725;&#25509;&#12290;&#36890;&#36807;&#37325;&#24314;&#20010;&#21035;&#26102;&#38388;&#28857;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;UniCoRN&#33021;&#22815;&#20174;fMRI&#24207;&#21015;&#20013;&#35299;&#30721;&#36830;&#36143;&#25991;&#26412;&#65292;&#24182;&#22312;&#19981;&#21516;&#20999;&#21106;&#35774;&#32622;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;BLEU&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.05355</link><description>&lt;p&gt;
UniCoRN: &#32479;&#19968;&#35748;&#30693;&#20449;&#21495;&#37325;&#24314;&#23558;&#35748;&#30693;&#20449;&#21495;&#21644;&#20154;&#31867;&#35821;&#35328;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive signals and human language. (arXiv:2307.05355v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UniCoRN&#65292;&#23427;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#35748;&#30693;&#20449;&#21495;&#37325;&#24314;&#31995;&#32479;&#65292;&#26088;&#22312;&#23558;fMRI&#26102;&#38388;&#24207;&#21015;&#19982;&#20154;&#31867;&#35821;&#35328;&#26725;&#25509;&#12290;&#36890;&#36807;&#37325;&#24314;&#20010;&#21035;&#26102;&#38388;&#28857;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;UniCoRN&#33021;&#22815;&#20174;fMRI&#24207;&#21015;&#20013;&#35299;&#30721;&#36830;&#36143;&#25991;&#26412;&#65292;&#24182;&#22312;&#19981;&#21516;&#20999;&#21106;&#35774;&#32622;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;BLEU&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35748;&#30693;&#20449;&#21495;(&#22914;fMRI)&#35299;&#30721;&#25991;&#26412;&#21050;&#28608;&#21487;&#20197;&#22686;&#36827;&#25105;&#20204;&#23545;&#20154;&#31867;&#35821;&#35328;&#31995;&#32479;&#30340;&#29702;&#35299;&#65292;&#20026;&#26500;&#24314;&#22810;&#21151;&#33021;&#33041;&#26426;&#25509;&#21475;&#38138;&#24179;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20174;&#21463;&#38480;&#35789;&#27719;&#34920;&#35299;&#30721;&#20010;&#21035;&#21333;&#35789;&#32423;&#21035;&#30340;fMRI&#20307;&#31215;&#65292;&#36825;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#26469;&#35828;&#36807;&#20110;&#29702;&#24819;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;fMRI2text&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#26725;&#25509;fMRI&#26102;&#38388;&#24207;&#21015;&#21644;&#20154;&#31867;&#35821;&#35328;&#30340;&#39318;&#20010;&#24320;&#25918;&#35789;&#27719;&#37327;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#26032;&#20219;&#21153;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#35299;&#20915;&#26041;&#26696;UniCoRN: &#32479;&#19968;&#35748;&#30693;&#20449;&#21495;&#37325;&#24314;&#29992;&#20110;&#33041;&#35299;&#30721;&#12290;&#36890;&#36807;&#37325;&#24314;&#20010;&#21035;&#26102;&#38388;&#28857;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;UniCoRN&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#35748;&#30693;&#20449;&#21495;(fMRI&#21644;EEG)&#30340;&#24378;&#22823;&#32534;&#30721;&#22120;&#12290;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35299;&#30721;&#22120;&#65292;UniCoRN&#35777;&#26126;&#20102;&#20854;&#22312;&#36328;&#19981;&#21516;&#20999;&#21106;&#35774;&#32622;&#19979;&#20174;fMRI&#24207;&#21015;&#20013;&#35299;&#30721;&#36830;&#36143;&#25991;&#26412;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;fMRI2text&#19978;&#23454;&#29616;&#20102;34.77%&#30340;BLEU&#24471;&#20998;&#65292;&#22312;&#36827;&#34892;&#20998;&#21449;&#26102;&#36798;&#21040;&#20102;37.04%&#30340;BLEU&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding text stimuli from cognitive signals (e.g. fMRI) enhances our understanding of the human language system, paving the way for building versatile Brain-Computer Interface. However, existing studies largely focus on decoding individual word-level fMRI volumes from a restricted vocabulary, which is far too idealized for real-world application. In this paper, we propose fMRI2text, the first openvocabulary task aiming to bridge fMRI time series and human language. Furthermore, to explore the potential of this new task, we present a baseline solution, UniCoRN: the Unified Cognitive Signal ReconstructioN for Brain Decoding. By reconstructing both individual time points and time series, UniCoRN establishes a robust encoder for cognitive signals (fMRI &amp; EEG). Leveraging a pre-trained language model as decoder, UniCoRN proves its efficacy in decoding coherent text from fMRI series across various split settings. Our model achieves a 34.77% BLEU score on fMRI2text, and a 37.04% BLEU when ge
&lt;/p&gt;</description></item><item><title>GujiBERT&#21644;GujiGPT&#26159;&#19987;&#20026;&#21476;&#31821;&#26234;&#33021;&#20449;&#24687;&#22788;&#29702;&#32780;&#35774;&#35745;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#36827;&#19968;&#27493;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#19982;&#21476;&#31821;&#30456;&#20851;&#30340;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.05354</link><description>&lt;p&gt;
GujiBERT&#21644;GujiGPT&#65306;&#29992;&#20110;&#21476;&#31821;&#26234;&#33021;&#20449;&#24687;&#22788;&#29702;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
GujiBERT and GujiGPT: Construction of Intelligent Information Processing Foundation Language Models for Ancient Texts. (arXiv:2307.05354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05354
&lt;/p&gt;
&lt;p&gt;
GujiBERT&#21644;GujiGPT&#26159;&#19987;&#20026;&#21476;&#31821;&#26234;&#33021;&#20449;&#24687;&#22788;&#29702;&#32780;&#35774;&#35745;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#36827;&#19968;&#27493;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#19982;&#21476;&#31821;&#30456;&#20851;&#30340;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#31934;&#24515;&#35757;&#32451;&#24182;&#24341;&#20837;&#20102;GujiBERT&#21644;GujiGPT&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21476;&#31821;&#26234;&#33021;&#20449;&#24687;&#22788;&#29702;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#21253;&#21547;&#31616;&#20307;&#21644;&#32321;&#20307;&#20013;&#25991;&#23383;&#31526;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19982;&#21476;&#31821;&#30456;&#20851;&#30340;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#33258;&#21160;&#21477;&#23376;&#20998;&#21106;&#12289;&#26631;&#28857;&#31526;&#21495;&#12289;&#35789;&#35821;&#20998;&#21106;&#12289;&#35789;&#24615;&#26631;&#27880;&#12289;&#23454;&#20307;&#35782;&#21035;&#21644;&#33258;&#21160;&#32763;&#35793;&#31561;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20351;&#29992;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#21508;&#31181;&#39564;&#35777;&#20219;&#21153;&#26102;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#37319;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#36827;&#19968;&#27493;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#21476;&#20856;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#27169;&#22411;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of the rapid development of large language models, we have meticulously trained and introduced the GujiBERT and GujiGPT language models, which are foundational models specifically designed for intelligent information processing of ancient texts. These models have been trained on an extensive dataset that encompasses both simplified and traditional Chinese characters, allowing them to effectively handle various natural language processing tasks related to ancient books, including but not limited to automatic sentence segmentation, punctuation, word segmentation, part-of-speech tagging, entity recognition, and automatic translation. Notably, these models have exhibited exceptional performance across a range of validation tasks using publicly available datasets. Our research findings highlight the efficacy of employing self-supervised methods to further train the models using classical text corpora, thus enhancing their capability to tackle downstream tasks. Moreover, it is
&lt;/p&gt;</description></item><item><title>&#20174;&#30005;&#35270;&#21095;&#30340;&#35282;&#33394;&#32593;&#32476;&#20013;&#25552;&#21462;&#32593;&#32476;&#25351;&#26631;&#65292;&#30740;&#31350;&#21457;&#29616;&#23545;&#30005;&#35270;&#21095;&#30340;&#35780;&#35770;&#20998;&#25968;&#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#20026;&#30005;&#35270;&#21046;&#29255;&#20154;&#25552;&#20379;&#20102;&#23450;&#37327;&#20449;&#24687;&#65292;&#24110;&#21161;&#20182;&#20204;&#35843;&#25972;&#35282;&#33394;&#21160;&#24577;&#20197;&#21560;&#24341;&#35266;&#20247;&#12290;</title><link>http://arxiv.org/abs/2307.05329</link><description>&lt;p&gt;
&#35299;&#30721;&#30005;&#35270;&#21095;&#30340;&#27969;&#34892;&#31243;&#24230;&#65306;&#19968;&#20010;&#32593;&#32476;&#20998;&#26512;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Decoding the Popularity of TV Series: A Network Analysis Perspective. (arXiv:2307.05329v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05329
&lt;/p&gt;
&lt;p&gt;
&#20174;&#30005;&#35270;&#21095;&#30340;&#35282;&#33394;&#32593;&#32476;&#20013;&#25552;&#21462;&#32593;&#32476;&#25351;&#26631;&#65292;&#30740;&#31350;&#21457;&#29616;&#23545;&#30005;&#35270;&#21095;&#30340;&#35780;&#35770;&#20998;&#25968;&#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#20026;&#30005;&#35270;&#21046;&#29255;&#20154;&#25552;&#20379;&#20102;&#23450;&#37327;&#20449;&#24687;&#65292;&#24110;&#21161;&#20182;&#20204;&#35843;&#25972;&#35282;&#33394;&#21160;&#24577;&#20197;&#21560;&#24341;&#35266;&#20247;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20174;&#19977;&#37096;&#27969;&#34892;&#30005;&#35270;&#21095;&#20013;&#25552;&#21462;&#30340;&#35282;&#33394;&#32593;&#32476;&#65292;&#24182;&#25506;&#35752;&#20102;&#30005;&#35270;&#21095;&#38598;&#30340;&#35282;&#33394;&#32593;&#32476;&#25351;&#26631;&#19982;IMDB&#35780;&#35770;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35282;&#33394;&#32593;&#32476;&#26159;&#20174;&#30005;&#35270;&#21095;&#24773;&#33410;&#20013;&#21019;&#24314;&#30340;&#22270;&#24418;&#65292;&#34920;&#31034;&#22330;&#26223;&#20013;&#35282;&#33394;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25351;&#31034;&#23427;&#20204;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#36830;&#25509;&#12290;&#25105;&#20204;&#20026;&#27599;&#38598;&#35745;&#31639;&#20102;&#21508;&#31181;&#32593;&#32476;&#25351;&#26631;&#65292;&#22914;&#33410;&#28857;&#24230;&#21644;&#22270;&#24418;&#23494;&#24230;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25351;&#26631;&#26469;&#25506;&#32034;&#32593;&#32476;&#25351;&#26631;&#19982;&#30005;&#35270;&#21095;&#22312;IMDB&#19978;&#30340;&#35780;&#20215;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30005;&#35270;&#21095;&#38598;&#20013;&#30340;&#35282;&#33394;&#20114;&#21160;&#30340;&#26576;&#20123;&#32593;&#32476;&#25351;&#26631;&#19982;&#30005;&#35270;&#21095;&#30340;&#35780;&#35770;&#20998;&#25968;&#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#26356;&#22810;&#23450;&#37327;&#20449;&#24687;&#65292;&#24110;&#21161;&#30005;&#35270;&#21046;&#29255;&#20154;&#20102;&#35299;&#22914;&#20309;&#35843;&#25972;&#26410;&#26469;&#21095;&#38598;&#30340;&#35282;&#33394;&#21160;&#24577;&#65292;&#20197;&#21560;&#24341;&#35266;&#20247;&#12290;&#36890;&#36807;&#29702;&#35299;&#35282;&#33394;&#20114;&#21160;&#23545;&#35266;&#20247;&#21442;&#19982;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
In this paper, we analyze the character networks extracted from three popular television series and explore the relationship between a TV show episode's character network metrics and its review from IMDB. Character networks are graphs created from the plot of a TV show that represents the interactions of characters in scenes, indicating the presence of a connection between them. We calculate various network metrics for each episode, such as node degree and graph density, and use these metrics to explore the potential relationship between network metrics and TV series reviews from IMDB. Our results show that certain network metrics of character interactions in episodes have a strong correlation with the review score of TV series. Our research aims to provide more quantitative information that can help TV producers understand how to adjust the character dynamics of future episodes to appeal to their audience. By understanding the impact of character interactions on audience engagement an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05300</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#35748;&#30693;&#21327;&#21516;&#65306;&#36890;&#36807;&#22810;&#20154;&#26684;&#33258;&#25105;&#21327;&#20316;&#23454;&#29616;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#24935;&#20381;&#36182;&#20110;&#35748;&#30693;&#21327;&#21516;&#30340;&#27010;&#24565;&#65292;&#21363;&#22312;&#19981;&#21516;&#35748;&#30693;&#36807;&#31243;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#21644;&#20449;&#24687;&#25972;&#21512;&#65292;&#20197;&#33719;&#24471;&#27604;&#20010;&#20307;&#35748;&#30693;&#36807;&#31243;&#26356;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#38656;&#35201;&#20016;&#23500;&#39046;&#22495;&#30693;&#35782;&#21644;&#22797;&#26434;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;LLM&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#12290;&#35748;&#30693;&#21327;&#21516;&#32773;&#25351;&#30340;&#26159;&#19968;&#20010;&#26234;&#33021;&#20195;&#29702;&#65292;&#19982;&#22810;&#20010;&#26234;&#24935;&#21512;&#20316;&#65292;&#32467;&#21512;&#20182;&#20204;&#30340;&#20010;&#20307;&#20248;&#21183;&#21644;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#22797;&#26434;&#20219;&#21153;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#36755;&#20837;&#21160;&#24577;&#35782;&#21035;&#21644;&#27169;&#25311;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;SPP&#37322;&#25918;&#20102;LLM&#20013;&#35748;&#30693;&#21327;&#21516;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assi
&lt;/p&gt;</description></item><item><title>U-CREAT&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#20107;&#20214;&#25552;&#21462;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#26816;&#32034;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2307.05260</link><description>&lt;p&gt;
U-CREAT: &#26080;&#30417;&#30563;&#20107;&#20214;&#25552;&#21462;&#30340;&#26080;&#30417;&#30563;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
U-CREAT: Unsupervised Case Retrieval using Events extrAcTion. (arXiv:2307.05260v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05260
&lt;/p&gt;
&lt;p&gt;
U-CREAT&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#20107;&#20214;&#25552;&#21462;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#26816;&#32034;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#39046;&#22495;&#65292;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#30340;&#20219;&#21153;&#26159;&#33258;&#21160;&#24341;&#29992;&#19982;&#32473;&#23450;&#26597;&#35810;&#26696;&#20363;&#30456;&#20851;&#65288;&#22522;&#20110;&#20107;&#23454;&#21644;&#20808;&#20363;&#65289;&#30340;&#20808;&#21069;&#27861;&#24459;&#26696;&#20363;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#21160;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#30740;&#31350;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#22522;&#20934;&#65288;&#20197;&#33521;&#25991;&#20026;&#20027;&#65289;&#29992;&#20110;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#20219;&#21153;&#65306;IL-PCR&#65288;&#21360;&#24230;&#27861;&#24459;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#65289;&#35821;&#26009;&#24211;&#12290;&#32771;&#34385;&#21040;&#26696;&#20363;&#30456;&#20851;&#24615;&#30340;&#22797;&#26434;&#24615;&#21644;&#27861;&#24459;&#25991;&#26723;&#30340;&#38271;&#24230;&#65292;BM25&#20173;&#28982;&#26159;&#25490;&#21517;&#24341;&#29992;&#20808;&#21069;&#25991;&#26723;&#30340;&#24378;&#22823;&#22522;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20107;&#20214;&#22312;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#26816;&#32034;&#26041;&#27861;&#30340;&#31649;&#36947;&#31995;&#32479;U-CREAT&#65288;&#26080;&#30417;&#30563;&#20107;&#20214;&#25552;&#21462;&#30340;&#26080;&#30417;&#30563;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#25552;&#20986;&#30340;&#26080;&#30417;&#30563;&#26816;&#32034;&#26041;&#27861;&#19982;BM25&#30456;&#27604;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#26816;&#32034;&#36895;&#24230;&#22823;&#22823;&#21152;&#24555;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#23454;&#26102;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#23427;&#36866;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#27861;&#24459;&#20307;&#31995;&#65288;&#21360;&#24230;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of Prior Case Retrieval (PCR) in the legal domain is about automatically citing relevant (based on facts and precedence) prior legal cases in a given query case. To further promote research in PCR, in this paper, we propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance and the long size of legal documents, BM25 remains a strong baseline for ranking the cited prior documents. In this work, we explore the role of events in legal case retrieval and propose an unsupervised retrieval method-based pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find that the proposed unsupervised retrieval method significantly increases performance compared to BM25 and makes retrieval faster by a considerable margin, making it applicable to real-time case retrieval systems. Our proposed system is generic, we show that it generalizes across two different legal systems (India
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23454;&#20363;&#32423;&#25511;&#21046;&#20195;&#30721;&#30340;&#23545;&#35805;&#24341;&#23548;&#31639;&#27861;&#65292;&#29992;&#20110;&#25506;&#32034;&#23454;&#20363;&#29305;&#23450;&#30340;&#25552;&#31034;&#23545;&#20110;&#25511;&#21046;&#23545;&#35805;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#25552;&#31034;&#22522;&#32447;&#65292;&#24182;&#19988;&#19982;&#20165;&#20351;&#29992;&#24635;&#21442;&#25968;&#30340;&#24494;&#35843;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2307.05228</link><description>&lt;p&gt;
&#23646;&#24615;&#25511;&#21046;&#30340;&#23545;&#35805;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Attribute Controlled Dialogue Prompting. (arXiv:2307.05228v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23454;&#20363;&#32423;&#25511;&#21046;&#20195;&#30721;&#30340;&#23545;&#35805;&#24341;&#23548;&#31639;&#27861;&#65292;&#29992;&#20110;&#25506;&#32034;&#23454;&#20363;&#29305;&#23450;&#30340;&#25552;&#31034;&#23545;&#20110;&#25511;&#21046;&#23545;&#35805;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#25552;&#31034;&#22522;&#32447;&#65292;&#24182;&#19988;&#19982;&#20165;&#20351;&#29992;&#24635;&#21442;&#25968;&#30340;&#24494;&#35843;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#25552;&#31034;&#35843;&#25972;&#24050;&#25104;&#20026;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#31163;&#25955;&#25552;&#31034;&#21644;&#36830;&#32493;&#25552;&#31034;&#37117;&#20551;&#35774;&#20219;&#21153;&#20013;&#30340;&#25152;&#26377;&#25968;&#25454;&#26679;&#26412;&#20351;&#29992;&#30456;&#21516;&#30340;&#22266;&#23450;&#25552;&#31034;&#65292;&#24573;&#30053;&#20102;&#26576;&#20123;&#20219;&#21153;&#65288;&#22914;&#24320;&#25918;&#22495;&#23545;&#35805;&#29983;&#25104;&#65289;&#20013;&#36755;&#20837;&#30340;&#24040;&#22823;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#22522;&#20110;&#23454;&#20363;&#32423;&#25511;&#21046;&#20195;&#30721;&#30340;&#23545;&#35805;&#24341;&#23548;&#31639;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22522;&#20110;&#23454;&#20363;&#32423;&#25511;&#21046;&#20195;&#30721;&#32780;&#19981;&#26159;&#23545;&#35805;&#21382;&#21490;&#29983;&#25104;&#25552;&#31034;&#65292;&#20197;&#25506;&#32034;&#23454;&#20363;&#29305;&#23450;&#30340;&#25552;&#31034;&#23545;&#20110;&#25511;&#21046;&#23545;&#35805;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;&#22312;&#27969;&#34892;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#22312;&#33258;&#21160;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#38754;&#37117;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25552;&#31034;&#22522;&#32447;&#65292;&#24182;&#19988;&#19982;&#20165;&#20351;&#29992;&#24635;&#21442;&#25968;&#30340;5%-6%&#30340;&#24494;&#35843;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks. However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation. In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation. Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation. Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines and comparable to fine-tuning with only 5%-6% of total parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#26631;&#31614;&#34920;&#31034;&#22810;&#22836;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#30340;&#26368;&#36817;&#37051;&#26426;&#21046;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#22312;SemEval 2023&#20219;&#21153;4&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2307.05174</link><description>&lt;p&gt;
Mao-Zedong&#22312;SemEval-2023&#20219;&#21153;4&#20013;&#65306;&#29992;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#30340;&#26368;&#36817;&#37051;&#26426;&#21046;&#30340;&#26631;&#31614;&#34920;&#31034;&#22810;&#22836;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Mao-Zedong At SemEval-2023 Task 4: Label Represention Multi-Head Attention Model With Contrastive Learning-Enhanced Nearest Neighbor Mechanism For Multi-Label Text Classification. (arXiv:2307.05174v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#26631;&#31614;&#34920;&#31034;&#22810;&#22836;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#30340;&#26368;&#36817;&#37051;&#26426;&#21046;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#22312;SemEval 2023&#20219;&#21153;4&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#30740;&#31350;&#22312;&#23454;&#36341;&#21644;&#29702;&#35770;&#39046;&#22495;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#38543;&#30528;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#21457;&#23637;&#65292;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#20351;&#24471;&#33021;&#22815;&#20934;&#30830;&#22320;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#20215;&#20540;&#35266;&#25104;&#20026;&#21487;&#33021;&#12290;SemEval 2023&#20219;&#21153;4&#25552;&#20379;&#20102;&#19968;&#32452;&#35770;&#35777;&#21644;20&#31181;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#36825;&#20123;&#20154;&#31867;&#20215;&#20540;&#35266;&#22312;&#27599;&#20010;&#35770;&#35777;&#20013;&#37117;&#26159;&#38544;&#21547;&#34920;&#36798;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22242;&#38431;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20351;&#29992;Roberta&#27169;&#22411;&#33719;&#21462;&#25991;&#26723;&#30340;&#35789;&#21521;&#37327;&#32534;&#30721;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#24314;&#31435;&#29305;&#23450;&#26631;&#31614;&#21644;&#35821;&#20041;&#32452;&#20214;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#30340;K&#26368;&#36817;&#37051;&#26426;&#21046;&#26469;&#21033;&#29992;&#29616;&#26377;&#30340;&#23454;&#20363;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;0.533&#30340;F1&#20998;&#25968;&#65292;&#24182;&#22312;&#25490;&#34892;&#27036;&#19978;&#25490;&#21517;&#31532;&#22235;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of human values is essential in both practical and theoretical domains. With the development of computational linguistics, the creation of large-scale datasets has made it possible to automatically recognize human values accurately. SemEval 2023 Task 4\cite{kiesel:2023} provides a set of arguments and 20 types of human values that are implicitly expressed in each argument. In this paper, we present our team's solution. We use the Roberta\cite{liu_roberta_2019} model to obtain the word vector encoding of the document and propose a multi-head attention mechanism to establish connections between specific labels and semantic components. Furthermore, we use a contrastive learning-enhanced K-nearest neighbor mechanism\cite{su_contrastive_2022} to leverage existing instance information for prediction. Our approach achieved an F1 score of 0.533 on the test set and ranked fourth on the leaderboard.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;LoRA&#30340;&#21442;&#25968;&#39640;&#25928;&#32454;&#35843;&#26041;&#27861;&#22312;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#20013;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;LoRA&#19982;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#32454;&#35843;&#25928;&#26524;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2307.05162</link><description>&lt;p&gt;
SuryaKiran&#22312;MEDIQA-Sum 2023&#20013;&#30340;&#24212;&#29992;&#65306;&#21033;&#29992;LoRA&#36827;&#34892;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization. (arXiv:2307.05162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;LoRA&#30340;&#21442;&#25968;&#39640;&#25928;&#32454;&#35843;&#26041;&#27861;&#22312;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#20013;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;LoRA&#19982;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#32454;&#35843;&#25928;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#25913;&#21892;&#29305;&#23450;&#39046;&#22495;&#29992;&#20363;&#30340;&#32467;&#26524;&#12290;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#32454;&#35843;&#32791;&#36153;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#24182;&#20855;&#26377;&#39640;&#23384;&#20648;&#38656;&#27714;&#20197;&#23384;&#20648;&#32454;&#35843;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21442;&#25968;&#39640;&#25928;&#32454;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#36890;&#36807;&#20445;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#22266;&#23450;&#22522;&#20934;&#24182;&#28155;&#21152;&#39069;&#22806;&#23618;&#26469;&#35299;&#20915;&#26102;&#38388;&#21644;&#36164;&#28304;&#25361;&#25112;&#65292;PEFT&#26041;&#27861;&#36827;&#34892;&#32454;&#35843;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;&#21517;&#20026;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;PEFT&#26041;&#27861;&#22312;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#20013;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;LoRA&#19982;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#32454;&#35843;&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#35299;&#20915;ImageCLEFmedical&#30340;Subtask A&#21644;B&#25152;&#36827;&#34892;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning Large Language Models helps improve the results for domain-specific use cases. End-to-end finetuning of large language models is time and resource intensive and has high storage requirements to store the finetuned version of the large language model. Parameter Efficient Fine Tuning (PEFT) methods address the time and resource challenges by keeping the large language model as a fixed base and add additional layers, which the PEFT methods finetune. This paper demonstrates the evaluation results for one such PEFT method Low Rank Adaptation (LoRA), for Clinical Dialogue Summarization. The evaluation results show that LoRA works at par with end-to-end finetuning for a large language model. The paper presents the evaluations done for solving both the Subtask A and B from ImageCLEFmedical {https://www.imageclef.org/2023/medical}
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;TIAM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#20869;&#23481;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#21253;&#25324;&#23545;&#35937;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.05134</link><description>&lt;p&gt;
TIAM -- &#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;TIAM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#20869;&#23481;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#21253;&#25324;&#23545;&#35937;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#30340;&#36827;&#23637;&#20351;&#24471;&#35780;&#20272;&#20854;&#36136;&#37327;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#28210;&#26579;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#32780;&#35328;&#65292;&#32771;&#34385;&#21040;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#37325;&#35201;&#20869;&#23481;&#20043;&#38388;&#30340;&#30456;&#20284;&#31243;&#24230;&#31561;&#39069;&#22806;&#22240;&#32032;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#29983;&#25104;&#30340;&#22270;&#20687;&#36890;&#24120;&#26159;&#20174;&#38543;&#26426;&#36215;&#22987;&#28857;&#24320;&#22987;&#30340;&#65292;&#20294;&#36890;&#24120;&#19981;&#32771;&#34385;&#36825;&#19968;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#25552;&#31034;&#20013;&#25351;&#23450;&#30340;&#20869;&#23481;&#19982;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#24615;&#12290;&#23427;&#20801;&#35768;&#25105;&#20204;&#26356;&#22909;&#22320;&#25551;&#36848;&#23545;&#40784;&#24615;&#65292;&#21253;&#25324;&#25351;&#23450;&#23545;&#35937;&#30340;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#25105;&#20204;&#23545;&#20960;&#20010;&#26368;&#36817;&#30340;T2I&#27169;&#22411;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#39069;&#22806;&#32467;&#26524;&#65292;&#21363;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#22823;&#24133;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress in the generation of synthetic images has made it crucial to assess their quality. While several metrics have been proposed to assess the rendering of images, it is crucial for Text-to-Image (T2I) models, which generate images based on a prompt, to consider additional aspects such as to which extent the generated image matches the important content of the prompt. Moreover, although the generated images usually result from a random starting point, the influence of this one is generally not considered. In this article, we propose a new metric based on prompt templates to study the alignment between the content specified in the prompt and the corresponding generated images. It allows us to better characterize the alignment in terms of the type of the specified objects, their number, and their color. We conducted a study on several recent T2I models about various aspects. An additional interesting result we obtained with our approach is that image quality can vary drastically 
&lt;/p&gt;</description></item><item><title>BioASQ 2023&#26159;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#35821;&#20041;&#32034;&#24341;&#21644;&#38382;&#39064;&#22238;&#31572;&#39046;&#22495;&#30340;&#22269;&#38469;&#25361;&#25112;&#36187;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#20004;&#20010;&#24050;&#24314;&#31435;&#20219;&#21153;&#30340;&#26032;&#29256;&#21644;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#21442;&#36187;&#31995;&#32479;&#30340;&#24615;&#33021;&#25345;&#32493;&#36827;&#27493;&#12290;</title><link>http://arxiv.org/abs/2307.05131</link><description>&lt;p&gt;
BioASQ 2023&#27010;&#36848;&#65306;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#35821;&#20041;&#32034;&#24341;&#19982;&#38382;&#39064;&#22238;&#31572;&#30340;&#31532;11&#23626;BioASQ&#25361;&#25112;&#36187;
&lt;/p&gt;
&lt;p&gt;
Overview of BioASQ 2023: The eleventh BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering. (arXiv:2307.05131v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05131
&lt;/p&gt;
&lt;p&gt;
BioASQ 2023&#26159;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#35821;&#20041;&#32034;&#24341;&#21644;&#38382;&#39064;&#22238;&#31572;&#39046;&#22495;&#30340;&#22269;&#38469;&#25361;&#25112;&#36187;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#20004;&#20010;&#24050;&#24314;&#31435;&#20219;&#21153;&#30340;&#26032;&#29256;&#21644;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#21442;&#36187;&#31995;&#32479;&#30340;&#24615;&#33021;&#25345;&#32493;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;BioASQ&#25361;&#25112;&#36187;&#31532;11&#23626;&#22312;CLEF 2023&#20250;&#35758;&#21644;&#23454;&#39564;&#23460;&#35780;&#20272;&#35770;&#22363;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#30340;&#24773;&#20917;&#12290;BioASQ&#26159;&#19968;&#31995;&#21015;&#22269;&#38469;&#25361;&#25112;&#36187;&#65292;&#26088;&#22312;&#25512;&#21160;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#35821;&#20041;&#32034;&#24341;&#21644;&#38382;&#39064;&#22238;&#31572;&#30340;&#36827;&#23637;&#12290;&#20170;&#24180;&#30340;BioASQ&#21253;&#25324;&#20102;&#20004;&#20010;&#24050;&#24314;&#31435;&#20219;&#21153;b&#21644;Synergy&#30340;&#26032;&#29256;&#26412;&#65292;&#20197;&#21450;&#19968;&#20010;&#26032;&#20219;&#21153;(MedProcNER)&#65292;&#28041;&#21450;&#35199;&#29677;&#29273;&#35821;&#20020;&#24202;&#20869;&#23481;&#30340;&#35821;&#20041;&#27880;&#37322;&#65292;&#24182;&#22312;&#21307;&#23398;&#23454;&#36341;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#23626;BioASQ&#20013;&#65292;&#20849;&#26377;28&#25903;&#21442;&#36187;&#38431;&#20237;&#25552;&#20132;&#20102;&#19977;&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#20849;&#35745;150&#22810;&#20010;&#31995;&#32479;&#30340;&#32467;&#26524;&#12290;&#19982;&#20197;&#24448;&#30340;&#29256;&#26412;&#31867;&#20284;&#65292;&#22823;&#22810;&#25968;&#21442;&#36187;&#31995;&#32479;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#25216;&#26415;&#19981;&#26029;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is an overview of the eleventh edition of the BioASQ challenge in the context of the Conference and Labs of the Evaluation Forum (CLEF) 2023. BioASQ is a series of international challenges promoting advances in large-scale biomedical semantic indexing and question answering. This year, BioASQ consisted of new editions of the two established tasks b and Synergy, and a new task (MedProcNER) on semantic annotation of clinical content in Spanish with medical procedures, which have a critical role in medical practice. In this edition of BioASQ, 28 competing teams submitted the results of more than 150 distinct systems in total for the three different shared tasks of the challenge. Similarly to previous editions, most of the participating systems achieved competitive performance, suggesting the continuous advancement of the state-of-the-art in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;LSR-Benchmark&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35828;&#26126;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#26085;&#24120;&#29983;&#27963;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.05113</link><description>&lt;p&gt;
&#36229;&#36234;&#26174;&#32780;&#26131;&#35265;&#65306;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#8212;&#8212;&#22522;&#20110;&#29983;&#27963;&#26223;&#35266;&#25512;&#29702;&#22522;&#20934;(LSR-Benchmark)&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Beyond the Obvious: Evaluating the Reasoning Ability In Real-life Scenarios of Language Models on Life Scapes Reasoning Benchmark~(LSR-Benchmark). (arXiv:2307.05113v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;LSR-Benchmark&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35828;&#26126;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#26085;&#24120;&#29983;&#27963;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29983;&#27963;&#26223;&#35266;&#25512;&#29702;&#22522;&#20934; (LSR-Benchmark)&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#30495;&#23454;&#24773;&#22659;&#25512;&#29702;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24357;&#34917;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#26085;&#24120;&#32972;&#26223;&#19979;&#25512;&#29702;&#33021;&#21147;&#30340;&#24046;&#36317;&#12290;&#19982;&#39046;&#22495;&#30693;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;LSR-Benchmark&#21253;&#21547;&#33258;&#30001;&#25991;&#26412;&#26684;&#24335;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#26377;&#20851;&#30495;&#23454;&#29983;&#27963;&#24773;&#26223;&#12289;&#20154;&#31867;&#34892;&#20026;&#21644;&#35282;&#33394;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#26469;&#33258;&#24320;&#28304;&#22312;&#32447;&#26469;&#28304;&#30340;2162&#20010;&#38382;&#39064;&#32452;&#25104;&#65292;&#24182;&#36827;&#34892;&#25163;&#21160;&#27880;&#37322;&#20197;&#25552;&#39640;&#36136;&#37327;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;gpt3.5-turbo&#21644;instruction fine-tuned llama&#27169;&#22411;&#65292;&#27979;&#35797;&#20854;&#22312;LSR-Benchmark&#19978;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#26126;&#26174;&#20248;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#36825;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#26085;&#24120;&#29983;&#27963;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Life Scapes Reasoning Benchmark (LSR-Benchmark), a novel dataset targeting real-life scenario reasoning, aiming to close the gap in artificial neural networks' ability to reason in everyday contexts. In contrast to domain knowledge reasoning datasets, LSR-Benchmark comprises free-text formatted questions with rich information on real-life scenarios, human behaviors, and character roles. The dataset consists of 2,162 questions collected from open-source online sources and is manually annotated to improve its quality. Experiments are conducted using state-of-the-art language models, such as gpt3.5-turbo and instruction fine-tuned llama models, to test the performance in LSR-Benchmark. The results reveal that humans outperform these models significantly, indicating a persisting challenge for machine learning models in comprehending daily human life.
&lt;/p&gt;</description></item><item><title>Vacaspati&#26159;&#19968;&#26412;&#22810;&#20803;&#30340;&#23391;&#21152;&#25289;&#25991;&#23398;&#35821;&#26009;&#24211;&#65292;&#25910;&#38598;&#20102;&#22810;&#20010;&#26041;&#38754;&#30340;&#25991;&#23398;&#20316;&#21697;&#65292;&#21253;&#21547;&#36229;&#36807;1100&#19975;&#20010;&#21477;&#23376;&#21644;1.15&#20159;&#20010;&#21333;&#35789;&#12290;&#35813;&#35821;&#26009;&#24211;&#26088;&#22312;&#35299;&#20915;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25968;&#25454;&#38656;&#27714;&#38382;&#39064;&#65292;&#20197;&#21450;&#25552;&#20379;&#35789;&#23884;&#20837;&#27169;&#22411;Vac-FT&#21644;&#35757;&#32451;&#22909;&#30340;Electra&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.05083</link><description>&lt;p&gt;
Vacaspati: &#19968;&#26412;&#22810;&#20803;&#30340;&#23391;&#21152;&#25289;&#25991;&#23398;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Vacaspati: A Diverse Corpus of Bangla Literature. (arXiv:2307.05083v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05083
&lt;/p&gt;
&lt;p&gt;
Vacaspati&#26159;&#19968;&#26412;&#22810;&#20803;&#30340;&#23391;&#21152;&#25289;&#25991;&#23398;&#35821;&#26009;&#24211;&#65292;&#25910;&#38598;&#20102;&#22810;&#20010;&#26041;&#38754;&#30340;&#25991;&#23398;&#20316;&#21697;&#65292;&#21253;&#21547;&#36229;&#36807;1100&#19975;&#20010;&#21477;&#23376;&#21644;1.15&#20159;&#20010;&#21333;&#35789;&#12290;&#35813;&#35821;&#26009;&#24211;&#26088;&#22312;&#35299;&#20915;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25968;&#25454;&#38656;&#27714;&#38382;&#39064;&#65292;&#20197;&#21450;&#25552;&#20379;&#35789;&#23884;&#20837;&#27169;&#22411;Vac-FT&#21644;&#35757;&#32451;&#22909;&#30340;Electra&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23391;&#21152;&#25289;&#35821;&#65288;&#25110;&#23391;&#21152;&#25289;&#65289;&#26159;&#20840;&#29699;&#31532;&#20116;&#22823;&#21475;&#35821;&#65292;&#28982;&#32780;&#65292;&#23391;&#21152;&#25289;&#35821;&#30340;&#26368;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#35832;&#22914;&#35789;&#24418;&#36824;&#21407;&#12289;&#35789;&#24615;&#26631;&#27880;&#31561;&#31616;&#21333;&#20219;&#21153;&#20013;&#20173;&#28982;&#28382;&#21518;&#12290;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#30340;&#35821;&#26009;&#24211;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;Vacapati&#65292;&#19968;&#26412;&#22810;&#26679;&#30340;&#23391;&#21152;&#25289;&#25991;&#23398;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#21508;&#20010;&#32593;&#31449;&#30340;&#25991;&#23398;&#20316;&#21697;&#65292;&#21482;&#36873;&#25321;&#37027;&#20123;&#27809;&#26377;&#29256;&#26435;&#25110;&#38480;&#21046;&#30340;&#20844;&#24320;&#20316;&#21697;&#12290;&#25105;&#20204;&#35748;&#20026;&#20986;&#29256;&#30340;&#25991;&#23398;&#20316;&#21697;&#27604;&#25253;&#32440;&#12289;&#21338;&#23458;&#25110;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#19968;&#31181;&#35821;&#35328;&#30340;&#29305;&#28857;&#65292;&#21518;&#32773;&#36890;&#24120;&#21482;&#36981;&#24490;&#26576;&#31181;&#29305;&#23450;&#30340;&#25991;&#23398;&#27169;&#24335;&#65292;&#22240;&#27492;&#24573;&#35270;&#20102;&#35821;&#35328;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;Vacapati&#35821;&#26009;&#24211;&#20174;&#22810;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#22810;&#26679;&#21270;&#35774;&#35745;&#65292;&#21253;&#25324;&#20316;&#21697;&#31867;&#22411;&#12289;&#20027;&#39064;&#12289;&#20316;&#32773;&#12289;&#26102;&#38388;&#12289;&#31354;&#38388;&#31561;&#12290;&#23427;&#21253;&#21547;&#36229;&#36807;1100&#19975;&#20010;&#21477;&#23376;&#21644;1.15&#20159;&#20010;&#21333;&#35789;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;Vacapati&#26500;&#24314;&#20102;&#19968;&#20010;&#35789;&#23884;&#20837;&#27169;&#22411;Vac-FT&#65292;&#20351;&#29992;FastText&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;Electra&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bangla (or Bengali) is the fifth most spoken language globally; yet, the state-of-the-art NLP in Bangla is lagging for even simple tasks such as lemmatization, POS tagging, etc. This is partly due to lack of a varied quality corpus. To alleviate this need, we build Vacaspati, a diverse corpus of Bangla literature. The literary works are collected from various websites; only those works that are publicly available without copyright violations or restrictions are collected. We believe that published literature captures the features of a language much better than newspapers, blogs or social media posts which tend to follow only a certain literary pattern and, therefore, miss out on language variety. Our corpus Vacaspati is varied from multiple aspects, including type of composition, topic, author, time, space, etc. It contains more than 11 million sentences and 115 million words. We also built a word embedding model, Vac-FT, using FastText from Vacaspati as well as trained an Electra mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26412;&#20307;&#39537;&#21160;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;&#31995;&#32479;&#19982;ChatGPT&#36827;&#34892;&#20803;&#23398;&#20064;&#30456;&#20114;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#24247;&#22797;&#39046;&#22495;&#30340;&#24212;&#29992;&#23454;&#29616;&#20102;&#35813;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05082</link><description>&lt;p&gt;
OntoChatGPT&#20449;&#24687;&#31995;&#32479;&#65306;&#26412;&#20307;&#39537;&#21160;&#30340;ChatGPT&#20803;&#23398;&#20064;&#32467;&#26500;&#21270;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
OntoChatGPT Information System: Ontology-Driven Structured Prompts for ChatGPT Meta-Learning. (arXiv:2307.05082v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26412;&#20307;&#39537;&#21160;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;&#31995;&#32479;&#19982;ChatGPT&#36827;&#34892;&#20803;&#23398;&#20064;&#30456;&#20114;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#24247;&#22797;&#39046;&#22495;&#30340;&#24212;&#29992;&#23454;&#29616;&#20102;&#35813;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26412;&#20307;&#39537;&#21160;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;&#31995;&#32479;&#19982;ChatGPT&#65288;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30456;&#20114;&#32467;&#21512;&#12290;&#30740;&#31350;&#24320;&#21457;&#20102;&#24418;&#24335;&#27169;&#22411;&#65288;&#20449;&#24687;&#21644;&#21151;&#33021;&#20004;&#20010;&#26041;&#38754;&#65289;&#65292;&#24182;&#24314;&#31435;&#20102;&#23558;&#26412;&#20307;&#39537;&#21160;&#30340;&#25552;&#31034;&#19982;ChatGPT&#30340;&#20803;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35770;&#22522;&#30784;&#12290;&#24471;&#21040;&#30340;&#19977;&#37325;&#32467;&#26500;&#21253;&#25324;&#26041;&#27861;&#35770;&#22522;&#30784;&#12289;&#20808;&#36827;&#30340;&#20449;&#24687;&#25216;&#26415;&#21644;OntoChatGPT&#31995;&#32479;&#65292;&#20849;&#21516;&#25552;&#39640;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#25928;&#33021;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#24247;&#22797;&#39046;&#22495;&#20013;&#37319;&#29992;&#20044;&#20811;&#20848;&#35821;&#23454;&#29616;&#20102;&#35813;&#25216;&#26415;&#12290;&#36890;&#36807;&#24212;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#65292;OntoChatGPT&#31995;&#32479;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#23454;&#20307;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#29983;&#25104;&#30456;&#20851;&#30340;&#22238;&#31572;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#35813;&#26041;&#27861;&#35770;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#24378;&#35843;&#20854;&#19981;&#20165;&#36866;&#29992;&#20110;ChatGPT&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research presents a comprehensive methodology for utilizing an ontology-driven structured prompts system in interplay with ChatGPT, a widely used large language model (LLM). The study develops formal models, both information and functional, and establishes the methodological foundations for integrating ontology-driven prompts with ChatGPT's meta-learning capabilities. The resulting productive triad comprises the methodological foundations, advanced information technology, and the OntoChatGPT system, which collectively enhance the effectiveness and performance of chatbot systems. The implementation of this technology is demonstrated using the Ukrainian language within the domain of rehabilitation. By applying the proposed methodology, the OntoChatGPT system effectively extracts entities from contexts, classifies them, and generates relevant responses. The study highlights the versatility of the methodology, emphasizing its applicability not only to ChatGPT but also to other chatbot
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#35777;&#36777;&#21010;&#20998;&#21644;&#27861;&#24459;&#35770;&#35777;&#26041;&#26696;&#32467;&#21512;&#30340;&#26041;&#27861;&#21019;&#24314;&#27861;&#24459;&#35777;&#36777;&#27573;&#33853;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#27861;&#24459;&#26696;&#20363;&#20915;&#31574;&#30340;&#35777;&#36777;&#27573;&#33853;&#20998;&#31867;&#20219;&#21153;&#12290;&#20351;&#29992;GPT-3.5&#29983;&#25104;&#25688;&#35201;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#27604;GPT-4&#21644;&#38750;GPT&#27169;&#22411;&#26356;&#39640;&#36136;&#37327;&#30340;&#35777;&#36777;&#25688;&#35201;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05081</link><description>&lt;p&gt;
&#35777;&#36777;&#21010;&#20998;&#22686;&#24378;&#27861;&#24459;&#25688;&#35201;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Argumentative Segmentation Enhancement for Legal Summarization. (arXiv:2307.05081v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#35777;&#36777;&#21010;&#20998;&#21644;&#27861;&#24459;&#35770;&#35777;&#26041;&#26696;&#32467;&#21512;&#30340;&#26041;&#27861;&#21019;&#24314;&#27861;&#24459;&#35777;&#36777;&#27573;&#33853;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#27861;&#24459;&#26696;&#20363;&#20915;&#31574;&#30340;&#35777;&#36777;&#27573;&#33853;&#20998;&#31867;&#20219;&#21153;&#12290;&#20351;&#29992;GPT-3.5&#29983;&#25104;&#25688;&#35201;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#27604;GPT-4&#21644;&#38750;GPT&#27169;&#22411;&#26356;&#39640;&#36136;&#37327;&#30340;&#35777;&#36777;&#25688;&#35201;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#35777;&#36777;&#21010;&#20998;&#21644;&#27861;&#24459;&#35770;&#35777;&#26041;&#26696;&#30340;&#32452;&#21512;&#26469;&#21019;&#24314;&#27861;&#24459;&#35777;&#36777;&#27573;&#33853;&#12290;&#22522;&#20110;&#36825;&#31181;&#21010;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#23545;&#27861;&#24459;&#26696;&#20363;&#20915;&#31574;&#30340;&#35777;&#36777;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#26681;&#25454;&#35777;&#36777;&#27573;&#33853;&#29983;&#25104;&#25688;&#35201;&#12290;&#22312;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#19978;&#65292;&#19982;GPT-4&#21644;&#38750;GPT&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#35777;&#36777;&#25688;&#35201;&#65292;&#21516;&#26102;&#21076;&#38500;&#20102;&#36739;&#19981;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use the combination of argumentative zoning [1] and a legal argumentative scheme to create legal argumentative segments. Based on the argumentative segmentation, we propose a novel task of classifying argumentative segments of legal case decisions. GPT-3.5 is used to generate summaries based on argumentative segments. In terms of automatic evaluation metrics, our method generates higher quality argumentative summaries while leaving out less relevant context as compared to GPT-4 and non-GPT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.05052</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps. (arXiv:2307.05052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#24615;&#33021;&#20013;&#65292;&#21508;&#31181;&#28436;&#31034;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26631;&#31614;&#12289;&#36755;&#20837;&#20998;&#24067;&#21644;&#34917;&#20805;&#35299;&#37322;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#36825;&#20123;&#22240;&#32032;&#34987;&#20462;&#25913;&#25110;&#25200;&#21160;&#26102;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22522;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#20123;&#24037;&#20316;&#23545;&#20110;&#36825;&#20123;&#20803;&#32032;&#22914;&#20309;&#24433;&#21709;ICL&#32473;&#20986;&#20102;&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21487;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(XNLP)&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#23545;&#27604;&#28436;&#31034;&#30340;&#26174;&#33879;&#24615;&#22270;&#36827;&#34892;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#23545;&#36755;&#20837;&#20998;&#24067;&#36827;&#34892;&#20102;&#31890;&#24230;&#32423;&#21035;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;ICL&#26041;&#38754;&#30340;&#25928;&#26524;&#26159;&#23384;&#22312;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the role of various demonstration components in the in-context learning (ICL) performance of large language models (LLMs). Specifically, we explore the impacts of ground-truth labels, input distribution, and complementary explanations, particularly when these are altered or perturbed. We build on previous work, which offers mixed findings on how these elements influence ICL. To probe these questions, we employ explainable NLP (XNLP) methods and utilize saliency maps of contrastive demonstrations for both qualitative and quantitative analysis. Our findings reveal that flipping ground-truth labels significantly affects the saliency, though it's more noticeable in larger LLMs. Our analysis of the input distribution at a granular level reveals that changing sentiment-indicative terms in a sentiment analysis task to neutral ones does not have as substantial an impact as altering ground-truth labels. Finally, we find that the effectiveness of complementary explanations in boos
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SICCK&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#22797;&#26434;&#32452;&#21512;&#30693;&#35782;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38646;-shot&#21644;&#24494;&#35843;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#32467;&#26500;&#21644;&#35821;&#20041;&#32452;&#21512;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.05034</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#22797;&#26434;&#32452;&#21512;&#30693;&#35782;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference. (arXiv:2307.05034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SICCK&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#22797;&#26434;&#32452;&#21512;&#30693;&#35782;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38646;-shot&#21644;&#24494;&#35843;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#32467;&#26500;&#21644;&#35821;&#20041;&#32452;&#21512;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Sentences Involving Complex Compositional Knowledge (SICCK)&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#23545;&#36923;&#36753;&#32452;&#25104;&#24615;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;SICK&#25968;&#25454;&#38598;&#20013;&#30340;15&#20010;&#31034;&#20363;&#65292;&#29983;&#25104;&#20102;1,304&#20010;&#21477;&#23376;&#23545;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#30701;&#35821; - &#19982;&#33258;&#28982;&#36923;&#36753;&#20013;&#30340;&#26222;&#36941;&#37327;&#35789;&#12289;&#23384;&#22312;&#37327;&#35789;&#12289;&#21542;&#23450;&#21644;&#20854;&#20182;&#27010;&#24565;&#20462;&#39280;&#31526;&#30456;&#23545;&#24212;&#30340;&#20462;&#39280;&#31526; - &#20462;&#25913;&#20102;&#21407;&#22987;&#25991;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#30701;&#35821;&#20462;&#25913;&#21069;&#25552;&#21644;&#20551;&#35774;&#30340;&#20027;&#35821;&#12289;&#35859;&#35821;&#21644;&#23486;&#35821;&#37096;&#20998;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#33258;&#28982;&#36923;&#36753;&#35268;&#21017;&#20026;&#36825;&#20123;&#20462;&#25913;&#21518;&#30340;&#25991;&#26412;&#26631;&#27880;&#30456;&#24212;&#30340;&#21253;&#21547;&#20851;&#31995;&#26631;&#31614;&#12290;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#27169;&#22411;&#22312;&#38646;-shot&#21644;&#24494;&#35843;&#24773;&#20917;&#19979;&#23545;&#32467;&#26500;&#21644;&#35821;&#20041;&#32452;&#21512;&#21464;&#21270;&#30340;&#25429;&#25417;&#33021;&#21147;&#36827;&#34892;&#20102;&#21021;&#27493;&#39564;&#35777;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;NLI&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis that investigates the performance of Natural Language Inference (NLI) models to understand compositionality in logic. We produce 1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et al., 2014). To this end, we modify the original texts using a set of phrases - modifiers that correspond to universal quantifiers, existential quantifiers, negation, and other concept modifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases to modify the subject, verb, and object parts of the premise and hypothesis. Lastly, we annotate these modified texts with the corresponding entailment labels following NL rules. We conduct a preliminary verification of how well the change in the structural and semantic composition is captured by neural NLI models, in both zero-shot and fine-tuned scenarios. We found that the performance of NLI models under th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LookAhead&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#21069;&#35266;&#23519;&#38899;&#39057;&#36755;&#20837;&#30340;&#26410;&#26469;&#37096;&#20998;&#65292;&#20351;RNN-Transducers&#27169;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#26356;&#21152;&#19982;&#22768;&#23398;&#30456;&#31526;&#12290;&#35813;&#25216;&#26415;&#22312;&#20934;&#30830;&#29575;&#19978;&#30456;&#23545;&#38477;&#20302;&#20102;5%-20%&#12290;</title><link>http://arxiv.org/abs/2307.05006</link><description>&lt;p&gt;
&#29992;&#22768;&#23398;&#39044;&#27979;&#25913;&#36827;RNN-Transducers&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving RNN-Transducers with Acoustic LookAhead. (arXiv:2307.05006v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LookAhead&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#21069;&#35266;&#23519;&#38899;&#39057;&#36755;&#20837;&#30340;&#26410;&#26469;&#37096;&#20998;&#65292;&#20351;RNN-Transducers&#27169;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#26356;&#21152;&#19982;&#22768;&#23398;&#30456;&#31526;&#12290;&#35813;&#25216;&#26415;&#22312;&#20934;&#30830;&#29575;&#19978;&#30456;&#23545;&#38477;&#20302;&#20102;5%-20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RNN-Transducers&#65288;RNN-Ts&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#25509;&#21463;&#20316;&#20026;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35821;&#38899;&#36716;&#25991;&#26412;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#39640;&#20934;&#30830;&#29575;&#21644;&#27969;&#24335;&#22788;&#29702;&#33021;&#21147;&#12290;&#20256;&#32479;&#30340;RNN-T&#27169;&#22411;&#29420;&#31435;&#22320;&#32534;&#30721;&#36755;&#20837;&#38899;&#39057;&#21644;&#25991;&#26412;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#34180;&#22411;&#32852;&#21512;&#32593;&#32476;&#23558;&#20004;&#31181;&#32534;&#30721;&#32467;&#21512;&#36215;&#26469;&#12290;&#34429;&#28982;&#36825;&#31181;&#26550;&#26500;&#25552;&#20379;&#20102;SOTA&#30340;&#27969;&#24335;&#22788;&#29702;&#20934;&#30830;&#29575;&#65292;&#20294;&#20063;&#20351;&#27169;&#22411;&#23545;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#20559;&#35265;&#33030;&#24369;&#65292;&#36825;&#34920;&#29616;&#20026;&#22312;&#27809;&#26377;&#22768;&#23398;&#35777;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#25991;&#26412;&#36827;&#34892;&#22810;&#27493;&#24187;&#35273;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LookAhead&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#21069;&#35266;&#23519;&#38899;&#39057;&#36755;&#20837;&#30340;&#26410;&#26469;&#37096;&#20998;&#65292;&#20351;&#25991;&#26412;&#34920;&#31034;&#26356;&#20855;&#26377;&#22768;&#23398;&#22522;&#30784;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#35780;&#20272;&#38598;&#19978;&#30456;&#23545;&#38169;&#35823;&#29575;&#26377;&#26174;&#33879;&#30340;5%-20%&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
RNN-Transducers (RNN-Ts) have gained widespread acceptance as an end-to-end model for speech to text conversion because of their high accuracy and streaming capabilities. A typical RNN-T independently encodes the input audio and the text context, and combines the two encodings by a thin joint network. While this architecture provides SOTA streaming accuracy, it also makes the model vulnerable to strong LM biasing which manifests as multi-step hallucination of text without acoustic evidence. In this paper we propose LookAhead that makes text representations more acoustically grounded by looking ahead into the future within the audio input. This technique yields a significant 5%-20% relative reduction in word error rate on both in-domain and out-of-domain evaluation sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22870;&#21169;&#27169;&#22411;&#12289;PPO&#21644;&#36827;&#31243;&#30417;&#30563;&#31561;&#25216;&#26415;&#36335;&#24452;&#65292;&#25506;&#32034;&#22914;&#20309;&#35299;&#20915;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04964</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494; &#31532;&#19968;&#37096;&#20998;&#65306;PPO
&lt;/p&gt;
&lt;p&gt;
Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22870;&#21169;&#27169;&#22411;&#12289;PPO&#21644;&#36827;&#31243;&#30417;&#30563;&#31561;&#25216;&#26415;&#36335;&#24452;&#65292;&#25506;&#32034;&#22914;&#20309;&#35299;&#20915;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#25512;&#21160;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#34013;&#22270;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#25104;&#20026;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#65288;&#26377;&#30410;&#12289;&#35802;&#23454;&#21644;&#26080;&#23475;&#65289;&#21161;&#25163;&#12290;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#24847;&#20041;&#65292;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#25104;&#20026;&#25903;&#25745;&#36825;&#19968;&#36861;&#27714;&#30340;&#20851;&#38190;&#25216;&#26415;&#33539;&#24335;&#12290;&#24403;&#21069;&#30340;&#25216;&#26415;&#36335;&#32447;&#36890;&#24120;&#21253;&#25324;&#29992;&#20110;&#34913;&#37327;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#12289;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#27169;&#22411;&#36755;&#20986;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#20197;&#21450;&#29992;&#20110;&#25913;&#21892;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#36827;&#31243;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22870;&#21169;&#35774;&#35745;&#12289;&#29615;&#22659;&#20132;&#20114;&#21644;&#20195;&#29702;&#35757;&#32451;&#30340;&#25361;&#25112;&#65292;&#20877;&#21152;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35797;&#39564;&#25104;&#26412;&#24040;&#22823;&#65292;&#23545;&#20110;AI&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#28608;&#21169;&#25216;&#26415;&#23545;&#40784;&#21644;LLMs&#30340;&#23433;&#20840;&#30528;&#38470;&#23384;&#22312;&#37325;&#22823;&#38556;&#30861;&#12290;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \textbf{reward models} to measure human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first re
&lt;/p&gt;</description></item><item><title>DyCL&#36890;&#36807;&#31243;&#24207;&#37325;&#20889;&#21644;&#22270;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DL&#32534;&#35793;&#22120;&#22312;&#32534;&#35793;&#20855;&#26377;&#21160;&#24577;&#29305;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#25104;&#21151;&#32534;&#35793;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2307.04963</link><description>&lt;p&gt;
DyCL: &#36890;&#36807;&#31243;&#24207;&#37325;&#20889;&#21644;&#22270;&#20248;&#21270;&#23454;&#29616;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#32534;&#35793;
&lt;/p&gt;
&lt;p&gt;
DyCL: Dynamic Neural Network Compilation Via Program Rewriting and Graph Optimization. (arXiv:2307.04963v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04963
&lt;/p&gt;
&lt;p&gt;
DyCL&#36890;&#36807;&#31243;&#24207;&#37325;&#20889;&#21644;&#22270;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DL&#32534;&#35793;&#22120;&#22312;&#32534;&#35793;&#20855;&#26377;&#21160;&#24577;&#29305;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#25104;&#21151;&#32534;&#35793;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DL&#32534;&#35793;&#22120;&#30340;&#20027;&#35201;&#21151;&#33021;&#26159;&#23558;&#20351;&#29992;&#39640;&#32423;DL&#26694;&#26550;&#65288;&#22914;PyTorch&#21644;TensorFlow&#65289;&#32534;&#20889;&#30340;DNN&#31243;&#24207;&#36716;&#25442;&#20026;&#21487;&#31227;&#26893;&#30340;&#21487;&#25191;&#34892;&#25991;&#20214;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DL&#32534;&#35793;&#22120;&#20381;&#36182;&#20110;&#36319;&#36394;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#28041;&#21450;&#21521;&#31070;&#32463;&#32593;&#32476;&#31243;&#24207;&#25552;&#20379;&#36816;&#34892;&#26102;&#36755;&#20837;&#65292;&#24182;&#36319;&#36394;&#31243;&#24207;&#25191;&#34892;&#36335;&#24452;&#20197;&#29983;&#25104;&#32534;&#35793;&#25152;&#38656;&#30340;&#35745;&#31639;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26426;&#21046;&#22312;&#22788;&#29702;&#20855;&#26377;&#26681;&#25454;&#36755;&#20837;&#21464;&#21270;&#30340;&#35745;&#31639;&#22270;&#30340;&#29616;&#20195;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#65288;DyNNs&#65289;&#26102;&#23384;&#22312;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;DL&#32534;&#35793;&#22120;&#22312;&#23558;DyNNs&#20934;&#30830;&#32534;&#35793;&#20026;&#21487;&#25191;&#34892;&#20195;&#30721;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\tool&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#20219;&#20309;&#29616;&#26377;&#30340;DL&#32534;&#35793;&#22120;&#25104;&#21151;&#32534;&#35793;DyNNs&#12290;\tool&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#32534;&#35793;&#26426;&#21046;&#26469;&#35299;&#20915;DyNNs&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#35813;&#26426;&#21046;&#37325;&#26032;&#20998;&#37197;&#21407;&#22987;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
DL compiler's primary function is to translate DNN programs written in high-level DL frameworks such as PyTorch and TensorFlow into portable executables. These executables can then be flexibly executed by the deployed host programs. However, existing DL compilers rely on a tracing mechanism, which involves feeding a runtime input to a neural network program and tracing the program execution paths to generate the computational graph necessary for compilation. Unfortunately, this mechanism falls short when dealing with modern dynamic neural networks (DyNNs) that possess varying computational graphs depending on the inputs. Consequently, conventional DL compilers struggle to accurately compile DyNNs into executable code. To address this limitation, we propose \tool, a general approach that enables any existing DL compiler to successfully compile DyNNs. \tool tackles the dynamic nature of DyNNs by introducing a compilation mechanism that redistributes the control and data flow of the origi
&lt;/p&gt;</description></item><item><title>SimpleMTOD&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22810;&#27169;&#24577;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#23376;&#20219;&#21153;&#36716;&#21270;&#20026;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30340;&#23545;&#35937;&#26631;&#35760;&#26469;&#25429;&#25417;&#35270;&#35273;&#22330;&#26223;&#30340;&#35821;&#20041;&#12290;&#23427;&#22312;SIMMC 2.0&#27979;&#35797;&#38598;&#30340;&#22238;&#24212;&#29983;&#25104;&#23376;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;BLEU&#20998;&#25968;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#22810;&#27169;&#24577;&#23376;&#20219;&#21153;&#20013;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.04907</link><description>&lt;p&gt;
SimpleMTOD: &#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#21270;&#22330;&#26223;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#31616;&#26131;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SimpleMTOD: A Simple Language Model for Multimodal Task-Oriented Dialogue with Symbolic Scene Representation. (arXiv:2307.04907v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04907
&lt;/p&gt;
&lt;p&gt;
SimpleMTOD&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22810;&#27169;&#24577;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#23376;&#20219;&#21153;&#36716;&#21270;&#20026;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30340;&#23545;&#35937;&#26631;&#35760;&#26469;&#25429;&#25417;&#35270;&#35273;&#22330;&#26223;&#30340;&#35821;&#20041;&#12290;&#23427;&#22312;SIMMC 2.0&#27979;&#35797;&#38598;&#30340;&#22238;&#24212;&#29983;&#25104;&#23376;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;BLEU&#20998;&#25968;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#22810;&#27169;&#24577;&#23376;&#20219;&#21153;&#20013;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SimpleMTOD&#26159;&#19968;&#20010;&#31616;&#26131;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22810;&#27169;&#24577;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#20960;&#20010;&#23376;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;SimpleMTOD&#22522;&#20110;&#22823;&#35268;&#27169;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#33258;&#22238;&#24402;&#26550;&#26500;&#26500;&#24314;&#32780;&#25104;&#65292;&#35813;&#26550;&#26500;&#24050;&#32463;&#22312;&#21333;&#27169;&#24577;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#19988;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;GPT-2&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#20026;&#20102;&#25429;&#25417;&#35270;&#35273;&#22330;&#26223;&#30340;&#35821;&#20041;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30340;&#23545;&#35937;&#26631;&#35760;&#12290;&#38750;&#23616;&#37096;&#30340;&#23545;&#35937;&#26631;&#35760;&#34920;&#31034;&#23545;&#35937;&#30340;&#31867;&#22411;&#32780;&#19981;&#26159;&#20855;&#20307;&#30340;&#23545;&#35937;&#26412;&#36523;&#65292;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#19968;&#33268;&#30340;&#21547;&#20041;&#12290;SimpleMTOD&#22312;SIMMC 2.0&#27979;&#35797;&#38598;&#20013;&#30340;&#22238;&#24212;&#29983;&#25104;&#23376;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;BLEU&#20998;&#25968;&#65288;0.327&#65289;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#22810;&#27169;&#24577;&#23376;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65306;&#28040;&#27495;&#12289;&#25351;&#20195;&#28040;&#35299;&#21644;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#12290;&#23613;&#31649;&#37319;&#21462;&#20102;&#26497;&#31616;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#35270;&#35273;&#65288;&#21644;&#38750;&#35270;&#35273;&#65289;&#20449;&#24687;&#65292;&#20294;SimpleMTOD&#20173;&#28982;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
SimpleMTOD is a simple language model which recasts several sub-tasks in multimodal task-oriented dialogues as sequence prediction tasks. SimpleMTOD is built on a large-scale transformer-based auto-regressive architecture, which has already proven to be successful in uni-modal task-oriented dialogues, and effectively leverages transfer learning from pre-trained GPT-2. In-order to capture the semantics of visual scenes, we introduce both local and de-localized tokens for objects within a scene. De-localized tokens represent the type of an object rather than the specific object itself and so possess a consistent meaning across the dataset. SimpleMTOD achieves a state-of-the-art BLEU score (0.327) in the Response Generation sub-task of the SIMMC 2.0 test-std dataset while performing on par in other multimodal sub-tasks: Disambiguation, Coreference Resolution, and Dialog State Tracking. This is despite taking a minimalist approach for extracting visual (and non-visual) information. In addi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#25991;&#26412;&#35299;&#26512;&#30340;&#23454;&#20307;&#20851;&#31995;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#38656;&#27714;&#25551;&#36848;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#23454;&#20307;&#26641;&#26469;&#24314;&#27169;&#36825;&#20123;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#29983;&#25104;CRUD&#31867;&#20195;&#30721;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.04892</link><description>&lt;p&gt;
&#23454;&#20307;&#26631;&#35782;&#31526;&#65306;&#22522;&#20110;&#33258;&#28982;&#25991;&#26412;&#35299;&#26512;&#30340;&#23454;&#20307;&#20851;&#31995;&#25552;&#21462;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Entity Identifier: A Natural Text Parsing-based Framework For Entity Relation Extraction. (arXiv:2307.04892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#25991;&#26412;&#35299;&#26512;&#30340;&#23454;&#20307;&#20851;&#31995;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#38656;&#27714;&#25551;&#36848;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#23454;&#20307;&#26641;&#26469;&#24314;&#27169;&#36825;&#20123;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#29983;&#25104;CRUD&#31867;&#20195;&#30721;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#31243;&#39046;&#22495;&#26377;&#22810;&#31181;&#33539;&#24335;&#65292;&#26681;&#25454;&#24037;&#20316;&#26694;&#26550;&#20351;&#29992;&#19981;&#21516;&#30340;&#33539;&#24335;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#31070;&#32463;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#33021;&#22815;&#30452;&#25509;&#20174;&#25991;&#26412;&#20013;&#23398;&#20064;&#21644;&#29983;&#25104;&#20195;&#30721;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#26041;&#27861;&#23545;&#26576;&#20123;&#20195;&#30721;&#20219;&#21153;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#29305;&#21035;&#26159;&#38754;&#21521;&#23545;&#35937;&#39033;&#30446;&#20013;&#30340;&#31867;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#38656;&#27714;&#25551;&#36848;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#20197;&#33258;&#21160;&#21270;&#29983;&#25104;CRUD&#65288;&#21019;&#24314;&#12289;&#35835;&#21462;&#12289;&#26356;&#26032;&#12289;&#21024;&#38500;&#65289;&#31867;&#20195;&#30721;&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#27969;&#27700;&#32447;&#65292;&#20197;&#21450;&#19968;&#31181;&#31216;&#20026;&#8220;&#23454;&#20307;&#26641;&#8221;&#30340;&#34920;&#31034;&#24418;&#24335;&#26469;&#24314;&#27169;&#36825;&#20123;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of programming has a diversity of paradigms that are used according to the working framework. While current neural code generation methods are able to learn and generate code directly from text, we believe that this approach is not optimal for certain code tasks, particularly the generation of classes in an object-oriented project. Specifically, we use natural language processing techniques to extract structured information from requirements descriptions, in order to automate the generation of CRUD (Create, Read, Update, Delete) class code. To facilitate this process, we introduce a pipeline for extracting entity and relation information, as well as a representation called an "Entity Tree" to model this information. We also create a dataset to evaluate the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;LaunchpadGPT&#27169;&#22411;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#38899;&#20048;&#21487;&#35270;&#21270;&#35774;&#35745;&#65292;&#24182;&#23637;&#31034;&#20986;&#20248;&#20110;&#38543;&#26426;&#29983;&#25104;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#38899;&#20048;&#21487;&#35270;&#21270;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.04827</link><description>&lt;p&gt;
LaunchpadGPT: &#20197;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38899;&#20048;&#21487;&#35270;&#21270;&#35774;&#35745;&#24072;&#22312;Launchpad&#19978;
&lt;/p&gt;
&lt;p&gt;
LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad. (arXiv:2307.04827v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04827
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LaunchpadGPT&#27169;&#22411;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#38899;&#20048;&#21487;&#35270;&#21270;&#35774;&#35745;&#65292;&#24182;&#23637;&#31034;&#20986;&#20248;&#20110;&#38543;&#26426;&#29983;&#25104;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#38899;&#20048;&#21487;&#35270;&#21270;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Launchpad&#26159;&#19968;&#31181;&#20048;&#22120;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#25353;&#20142;&#30340;&#25353;&#38062;&#26469;&#21019;&#20316;&#21644;&#28436;&#22863;&#38899;&#20048;&#12290;&#20026;&#20102;&#36741;&#21161;&#21644;&#21551;&#21457;Launchpad&#28783;&#20809;&#25928;&#26524;&#30340;&#35774;&#35745;&#65292;&#24182;&#20026;&#21021;&#23398;&#32773;&#25552;&#20379;&#26356;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#26469;&#36890;&#36807;&#36825;&#20010;&#20048;&#22120;&#21019;&#24314;&#38899;&#20048;&#21487;&#35270;&#21270;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LaunchpadGPT&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;Launchpad&#19978;&#30340;&#38899;&#20048;&#21487;&#35270;&#21270;&#35774;&#35745;&#12290;&#22522;&#20110;&#20855;&#26377;&#20986;&#33394;&#29983;&#25104;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;LaunchpadGPT&#27169;&#22411;&#20197;&#38899;&#39057;&#38899;&#20048;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20197;&#35270;&#39057;&#24418;&#24335;&#34920;&#29616;Launchpad&#28436;&#22863;&#30340;&#28783;&#20809;&#25928;&#26524;&#65288;Launchpad&#25773;&#25918;&#35270;&#39057;&#65289;&#12290;&#25105;&#20204;&#25910;&#38598;Launchpad&#28436;&#22863;&#35270;&#39057;&#24182;&#36827;&#34892;&#22788;&#29702;&#65292;&#20197;&#33719;&#21462;&#38899;&#20048;&#21644;&#30456;&#24212;&#30340;Launchpad&#28436;&#22863;&#35270;&#39057;&#24103;&#20316;&#20026;&#25552;&#31034;&#23436;&#25104;&#23545;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#38543;&#26426;&#29983;&#25104;&#26041;&#27861;&#21487;&#20197;&#21019;&#36896;&#20986;&#26356;&#22909;&#30340;&#38899;&#20048;&#21487;&#35270;&#21270;&#25928;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#38899;&#20048;&#21487;&#35270;&#21270;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Launchpad is a musical instrument that allows users to create and perform music by pressing illuminated buttons. To assist and inspire the design of the Launchpad light effect, and provide a more accessible approach for beginners to create music visualization with this instrument, we proposed the LaunchpadGPT model to generate music visualization designs on Launchpad automatically. Based on the language model with excellent generation ability, our proposed LaunchpadGPT takes an audio piece of music as input and outputs the lighting effects of Launchpad-playing in the form of a video (Launchpad-playing video). We collect Launchpad-playing videos and process them to obtain music and corresponding video frame of Launchpad-playing as prompt-completion pairs, to train the language model. The experiment result shows the proposed method can create better music visualization than random generation methods and hold the potential for a broader range of music visualization applications. Our code 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25193;&#22823;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38480;&#21046;&#12289;&#20260;&#23475;&#21644;&#39118;&#38505;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#20851;&#20110;AI&#30340;&#22840;&#22823;&#28818;&#20316;&#21644;&#35823;&#35299;&#12290;&#36825;&#26377;&#21161;&#20110;&#28040;&#38500;&#19968;&#20123;&#23545;AI&#25216;&#26415;&#30340;&#38169;&#35823;&#35748;&#35782;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#27880;&#24847;&#30001;&#20110;&#36825;&#20123;&#38480;&#21046;&#32780;&#20135;&#29983;&#30340;&#23454;&#38469;&#20260;&#23475;&#12290;</title><link>http://arxiv.org/abs/2307.04821</link><description>&lt;p&gt;
&#25193;&#22823;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38480;&#21046;&#12289;&#20260;&#23475;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Amplifying Limitations, Harms and Risks of Large Language Models. (arXiv:2307.04821v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25193;&#22823;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38480;&#21046;&#12289;&#20260;&#23475;&#21644;&#39118;&#38505;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#20851;&#20110;AI&#30340;&#22840;&#22823;&#28818;&#20316;&#21644;&#35823;&#35299;&#12290;&#36825;&#26377;&#21161;&#20110;&#28040;&#38500;&#19968;&#20123;&#23545;AI&#25216;&#26415;&#30340;&#38169;&#35823;&#35748;&#35782;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#27880;&#24847;&#30001;&#20110;&#36825;&#20123;&#38480;&#21046;&#32780;&#20135;&#29983;&#30340;&#23454;&#38469;&#20260;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#35797;&#22270;&#36890;&#36807;&#19968;&#20010;&#23567;&#23567;&#30340;&#20030;&#21160;&#26469;&#25269;&#21046;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21450;&#20854;&#33021;&#21147;&#25152;&#24102;&#26469;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#28818;&#20316;&#65292;&#20197;&#21450;&#30001;&#27492;&#24102;&#26469;&#30340;&#31185;&#24187;&#24773;&#26223;&#30340;&#20998;&#25955;&#27880;&#24847;&#21147;&#12290;&#36825;&#20063;&#26377;&#21161;&#20110;&#37027;&#20123;&#22312;&#35813;&#39046;&#22495;&#20043;&#22806;&#30340;&#20154;&#20102;&#35299;&#19968;&#20123;AI&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#24403;&#21069;&#27969;&#34892;&#35805;&#35821;&#30340;&#32972;&#26223;&#19979;&#65292;AI&#40664;&#35748;&#20026;&#24847;&#21619;&#30528;&#22522;&#30784;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;&#29992;&#20110;&#21019;&#24314;ChatGPT&#30340;&#27169;&#22411;&#12290;&#36825;&#26412;&#36523;&#23601;&#26159;&#23545;&#30740;&#31350;&#39046;&#22495;&#22810;&#26679;&#24615;&#12289;&#28145;&#24230;&#21644;&#23481;&#37327;&#30340;&#26354;&#35299;&#65292;&#32780;&#30495;&#27491;&#20195;&#34920;AI&#39046;&#22495;&#30340;&#26159;&#30740;&#31350;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#25216;&#26415;&#30340;&#22810;&#26679;&#24615;&#12290;AI&#20316;&#20026;&#19968;&#38376;&#30740;&#31350;&#39046;&#22495;&#65292;&#33267;&#23569;&#20174;20&#19990;&#32426;50&#24180;&#20195;&#20197;&#26469;&#23601;&#23384;&#22312;&#20110;&#36719;&#20214;&#26500;&#20214;&#20013;&#12290;&#25105;&#20204;&#35797;&#22270;&#31361;&#20986;&#19968;&#20123;LLMs&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#27492;&#36807;&#31243;&#20013;&#24378;&#35843;&#30001;&#20110;&#36825;&#20123;&#23616;&#38480;&#24615;&#24050;&#32463;&#20986;&#29616;&#24182;&#23558;&#32487;&#32493;&#20986;&#29616;&#30340;&#20260;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present this article as a small gesture in an attempt to counter what appears to be exponentially growing hype around Artificial Intelligence (AI) and its capabilities, and the distraction provided by the associated talk of science-fiction scenarios that might arise if AI should become sentient and super-intelligent. It may also help those outside of the field to become more informed about some of the limitations of AI technology. In the current context of popular discourse AI defaults to mean foundation and large language models (LLMs) such as those used to create ChatGPT. This in itself is a misrepresentation of the diversity, depth and volume of research, researchers, and technology that truly represents the field of AI. AI being a field of research that has existed in software artefacts since at least the 1950's. We set out to highlight a number of limitations of LLMs, and in so doing highlight that harms have already arisen and will continue to arise due to these limitations. A
&lt;/p&gt;</description></item><item><title>S2vNTM&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20851;&#38190;&#35789;&#30340;&#27169;&#24335;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#20027;&#39064;&#65292;&#24182;&#20248;&#21270;&#20027;&#39064;&#20851;&#38190;&#35789;&#38598;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#36895;&#24230;&#33267;&#23569;&#27604;&#22522;&#32447;&#27169;&#22411;&#24555;&#20004;&#20493;&#12290;</title><link>http://arxiv.org/abs/2307.04804</link><description>&lt;p&gt;
S2vNTM: &#21322;&#30417;&#30563;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
S2vNTM: Semi-supervised vMF Neural Topic Modeling. (arXiv:2307.04804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04804
&lt;/p&gt;
&lt;p&gt;
S2vNTM&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20851;&#38190;&#35789;&#30340;&#27169;&#24335;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#20027;&#39064;&#65292;&#24182;&#20248;&#21270;&#20027;&#39064;&#20851;&#38190;&#35789;&#38598;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#36895;&#24230;&#33267;&#23569;&#27604;&#22522;&#32447;&#27169;&#22411;&#24555;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#26469;&#35828;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65306;&#65288;1&#65289;&#24456;&#38590;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#65292;&#27604;&#22914;&#20851;&#38190;&#35789;&#65307;&#65288;2&#65289;&#35757;&#32451;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65307;&#65288;3&#65289;&#20381;&#36182;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21322;&#30417;&#30563;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#65288;S2vNTM&#65289;&#26469;&#20811;&#26381;&#36825;&#20123;&#22256;&#38590;&#12290;S2vNTM&#23558;&#19968;&#20123;&#31181;&#23376;&#20851;&#38190;&#35789;&#20316;&#20026;&#20027;&#39064;&#30340;&#36755;&#20837;&#12290;S2vNTM&#21033;&#29992;&#20851;&#38190;&#35789;&#30340;&#27169;&#24335;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#20027;&#39064;&#65292;&#24182;&#20248;&#21270;&#20027;&#39064;&#20851;&#38190;&#35789;&#38598;&#30340;&#36136;&#37327;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;S2vNTM&#22312;&#25552;&#20379;&#26377;&#38480;&#20851;&#38190;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20998;&#31867;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;S2vNTM&#33267;&#23569;&#27604;&#22522;&#32447;&#27169;&#22411;&#24555;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model based methods are powerful techniques for text classification. However, the models have several shortcomings. (1) It is difficult to integrate human knowledge such as keywords. (2) It needs a lot of resources to train the models. (3) It relied on large text data to pretrain. In this paper, we propose Semi-Supervised vMF Neural Topic Modeling (S2vNTM) to overcome these difficulties. S2vNTM takes a few seed keywords as input for topics. S2vNTM leverages the pattern of keywords to identify potential topics, as well as optimize the quality of topics' keywords sets. Across a variety of datasets, S2vNTM outperforms existing semi-supervised topic modeling methods in classification accuracy with limited keywords provided. S2vNTM is at least twice as fast as baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RecallM&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02738</link><description>&lt;p&gt;
RecallM:&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#38382;&#39064;&#22238;&#31572;&#30340;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
RecallM: An Architecture for Temporal Context Understanding and Question Answering. (arXiv:2307.02738v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RecallM&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#23558;&#20026;&#36830;&#32493;&#23398;&#20064;&#12289;&#22797;&#26434;&#25512;&#29702;&#21644;&#23398;&#20064;&#24207;&#21015;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#25171;&#19979;&#22522;&#30784;&#12290;&#21019;&#24314;&#36825;&#31181;&#31867;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#26041;&#27861;&#23454;&#29616;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#19987;&#27880;&#20110;&#20026;AGI&#31995;&#32479;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#23637;&#31034;&#20102;RecallM&#26550;&#26500;&#30340;&#22909;&#22788;&#65292;&#29305;&#21035;&#26159;&#23427;&#25552;&#20379;&#30340;&#25913;&#36827;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ideal long-term memory mechanism for Large Language Model (LLM) based chatbots, would lay the foundation for continual learning, complex reasoning and allow sequential and temporal dependencies to be learnt. Creating this type of memory mechanism is an extremely challenging problem. In this paper we explore different methods of achieving the effect of long-term memory. We propose a new architecture focused on creating adaptable and updatable long-term memory for AGI systems. We demonstrate through various experiments the benefits of the RecallM architecture, particularly the improved temporal understanding it provides.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#25512;&#29702;&#24418;&#24335;&#65292;&#24182;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#12290;&#25552;&#20986;&#31934;&#32454;&#32423;&#21035;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09841</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30495;&#30340;&#26159;&#33391;&#22909;&#30340;&#36923;&#36753;&#25512;&#29702;&#32773;&#21527;&#65311;&#22522;&#20110;&#28436;&#32462;&#12289;&#24402;&#32435;&#21644;&#38463;&#24067;&#36798;&#26031;&#35266;&#28857;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. (arXiv:2306.09841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#25512;&#29702;&#24418;&#24335;&#65292;&#24182;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#12290;&#25552;&#20986;&#31934;&#32454;&#32423;&#21035;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23545;LLMs&#30340;&#20855;&#20307;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#65292;&#22914;&#22810;&#35821;&#35328;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#20851;&#38190;&#25512;&#29702;&#35270;&#35282;&#20043;&#19968;&#65292;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#24443;&#24213;&#35780;&#20272;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#24182;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#36827;&#34892;&#31995;&#32479;&#21270;&#35780;&#20272;&#65292;&#26412;&#25991;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#25104;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#24418;&#24335;&#30340;&#25512;&#29702;&#35774;&#32622;&#12290;&#32771;&#34385;&#35780;&#20272;&#30340;&#20840;&#38754;&#24615;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#65288;text-davinci-003&#65292;ChatGPT&#21644;BARD&#65289;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#23545;&#25152;&#26377;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#20854;&#27425;&#65292;&#19982;&#20197;&#24448;&#20165;&#20381;&#36182;&#31616;&#21333;&#25351;&#26631;&#65288;&#22914;&#20934;&#30830;&#24615;&#65289;&#30340;&#35780;&#20272;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#30446;&#26631;&#25512;&#29702;&#35282;&#24230;&#36827;&#34892;&#30340;&#31934;&#32454;&#32423;&#21035;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved great success in various natural language tasks. It has aroused much interest in evaluating the specific reasoning capability of LLMs, such as multilingual reasoning and mathematical reasoning. However, as one of the key reasoning perspectives, logical reasoning capability has not yet been thoroughly evaluated. In this work, we aim to bridge those gaps and provide comprehensive evaluations. Firstly, to offer systematic evaluations, this paper selects fifteen typical logical reasoning datasets and organizes them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot, one-shot and three-shot settings. Secondly, different from previous evaluations relying only on simple metrics (e.g., accuracy), we propose fine-level evaluations from objective 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; TOAST &#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#65292;&#36873;&#25321;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20803;&#32032;&#24182;&#21453;&#39304;&#22238;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#23567;&#37096;&#20998;&#21487;&#35843;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.15542</link><description>&lt;p&gt;
&#32858;&#28966;&#26159;&#36801;&#31227;&#23398;&#20064;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Refocusing Is Key to Transfer Learning. (arXiv:2305.15542v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; TOAST &#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#65292;&#36873;&#25321;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20803;&#32032;&#24182;&#21453;&#39304;&#22238;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#23567;&#37096;&#20998;&#21487;&#35843;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#28041;&#21450;&#23558;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36866;&#24212;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#21069;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#24120;&#24120;&#26080;&#27861;&#32858;&#28966;&#20110;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;-Top-Down Attention Steering&#65288;TOAST&#65289;&#65292;&#23427;&#20445;&#25345;&#39044;&#20808;&#35757;&#32451;&#30340;&#39592;&#24178;&#32467;&#26500;&#19981;&#21464;&#65292;&#21516;&#26102;&#36873;&#25321;&#36755;&#20986;&#20013;&#19982;&#20219;&#21153;&#26377;&#20851;&#30340;&#20803;&#32032;&#65292;&#24182;&#23558;&#23427;&#20204;&#21453;&#39304;&#22238;&#27169;&#22411;&#65292;&#20197;&#24341;&#23548;&#20854;&#27880;&#24847;&#20219;&#21153;&#29305;&#23450;&#30340;&#29305;&#24449;&#12290;&#20165;&#36890;&#36807;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#65292;TOAST&#22312;&#35768;&#22810;&#36801;&#31227;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20855;&#26377;&#23567;&#37096;&#20998;&#21487;&#35843;&#21442;&#25968;&#12290;&#19982;&#23436;&#20840;&#24494;&#35843;&#12289;LoRA&#21644;&#25552;&#31034;&#24494;&#35843;&#30456;&#27604;&#65292;TOAST&#22312;&#19968;&#31995;&#21015;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65288;&#20363;&#22914;&#65292;&#22312; FGVC &#19978;&#20174; 81.1% &#25552;&#39640;&#21040; 86.2%&#65289;&#26174;&#30528;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;TOAST&#22312;&#25351;&#20196;&#36319;&#38543;&#26041;&#38754;&#20063;&#20248;&#20110;&#23436;&#20840;&#24494;&#35843;&#30340; Alpaca &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning involves adapting a pre-trained model to novel downstream tasks. However, we observe that current transfer learning methods often fail to focus on task-relevant features. In this work, we emphasize the importance of refocusing the attention in transfer learning. We introduce Top-Down Attention Steering (TOAST), a novel transfer learning algorithm that keeps the pre-trained backbone frozen, while selecting the task-relevant elements in the output and feeding them back to the model to steer its attention to the task-specific features. By refocusing the attention only, TOAST achieves state-of-the-art results on a number of transfer learning benchmarks, while having a small portion of tunable parameters. Compared to fully fine-tuning, LoRA, and prompt tuning, TOAST substantially improves performance across a range of fine-grained visual classification datasets (e.g., 81.1% -&gt; 86.2% on FGVC). TOAST also outperforms the fully fine-tuned Alpaca model on instruction-following
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#19982;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29702;&#35299;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#39046;&#22495;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.15066</link><description>&lt;p&gt;
GPT4Graph&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#29702;&#35299;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65311;&#19968;&#39033;&#23454;&#35777;&#35780;&#20272;&#19982;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking. (arXiv:2305.15066v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#19982;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29702;&#35299;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#39046;&#22495;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#24050;&#25104;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22270;&#25968;&#25454;&#26080;&#22788;&#19981;&#22312;&#65292;&#26159;AGI&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#27969;&#34892;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#36890;&#24120;&#21253;&#25324;&#19968;&#20123;&#31639;&#27861;&#32452;&#20214;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#19968;&#20123;&#19982;&#22270;&#25968;&#25454;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#21462;&#24471;&#19968;&#23450;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#23427;&#20204;&#22312;&#26356;&#24191;&#27867;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#36824;&#32570;&#20047;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#35843;&#26597;&#65292;&#35780;&#20272;LLMs&#22312;&#29702;&#35299;&#22270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#32467;&#26500;&#21644;&#35821;&#20041;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21253;&#25324;10&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#22270;&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#19981;&#20165;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25968;&#25454;&#29702;&#35299;&#26041;&#38754;&#30340;&#24403;&#21069;&#38480;&#21046;&#65292;&#36824;&#21457;&#29616;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models~(LLM) like ChatGPT have become indispensable to artificial general intelligence~(AGI), demonstrating excellent performance in various natural language processing tasks. In the real world, graph data is ubiquitous and an essential part of AGI and prevails in domains like social network analysis, bioinformatics and recommender systems. The training corpus of large language models often includes some algorithmic components, which allows them to achieve certain effects on some graph data-related problems. However, there is still little research on their performance on a broader range of graph-structured data. In this study, we conduct an extensive investigation to assess the proficiency of LLMs in comprehending graph data, employing a diverse range of structural and semantic-related tasks. Our analysis encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph understanding. Through our study, we not only uncover the current limitations of language mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#24490;&#29615;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#23569;&#37327;&#30417;&#30563;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#25991;&#26412;&#20219;&#21153;&#19982;&#20840;&#30417;&#30563;&#26041;&#27861;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#38750;&#22495;&#25968;&#25454;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14793</link><description>&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#35757;&#32451;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#26041;&#27861;&#26469;&#33258;&#20110;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithful Low-Resource Data-to-Text Generation through Cycle Training. (arXiv:2305.14793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#24490;&#29615;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#23569;&#37327;&#30417;&#30563;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#25991;&#26412;&#20219;&#21153;&#19982;&#20840;&#30417;&#30563;&#26041;&#27861;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#38750;&#22495;&#25968;&#25454;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20027;&#35201;&#26159;&#36890;&#36807;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#19978;&#21487;&#33021;&#26080;&#27861;&#20135;&#29983;&#19982;&#36755;&#20837;&#25968;&#25454;&#30456;&#31526;&#30340;&#36755;&#20986;&#25991;&#26412;&#65292;&#23588;&#20854;&#26159;&#22312;&#22495;&#22806;&#25968;&#25454;&#19978;&#12290;&#30001;&#20110;&#32570;&#23569;&#29305;&#23450;&#39046;&#22495;&#30340;&#36275;&#22815;&#27880;&#37322;&#25968;&#25454;&#65292;&#22240;&#27492;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#36755;&#20986;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36890;&#36807;&#24490;&#29615;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20010;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#25991;&#26412;&#20043;&#38388;&#34920;&#31034;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#24490;&#29615;&#35757;&#32451;&#20351;&#29992;&#20004;&#20010;&#20114;&#20026;&#21453;&#20989;&#25968;&#30340;&#27169;&#22411;&#65306;&#19968;&#20010;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#65292;&#21478;&#19968;&#20010;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#29983;&#25104;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#23569;&#37327;&#30417;&#30563;&#25968;&#25454;&#65288;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;100&#20010;&#26679;&#26412;&#65289;&#30340;&#24773;&#20917;&#19979;&#21021;&#22987;&#21270;&#30340;&#24490;&#29615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#20219;&#21153;&#19982;&#20840;&#30417;&#30563;&#26041;&#27861;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#38750;&#22495;&#25968;&#25454;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods to generate text from structured data have advanced significantly in recent years, primarily due to fine-tuning of pre-trained language models on large datasets. However, such models can fail to produce output faithful to the input data, particularly on out-of-domain data. Sufficient annotated data is often not available for specific domains, leading us to seek an unsupervised approach to improve the faithfulness of output text. Since the problem is fundamentally one of consistency between the representations of the structured data and text, we evaluate the effectiveness of cycle training in this work. Cycle training uses two models which are inverses of each other: one that generates text from structured data, and one which generates the structured data from natural language text. We show that cycle training, when initialized with a small amount of supervised data (100 samples in our case), achieves nearly the same performance as fully supervised approaches for the data-to-tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ImprovisetoInitialize(I2I)&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#26469;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#36825;&#20351;&#24471;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.02168</link><description>&lt;p&gt;
I2I: &#29992;&#25913;&#36827;&#30340;&#30693;&#35782;&#21021;&#22987;&#21270;&#36716;&#25509;&#22120;
&lt;/p&gt;
&lt;p&gt;
I2I: Initializing Adapters with Improvised Knowledge. (arXiv:2304.02168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ImprovisetoInitialize(I2I)&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#26469;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#36825;&#20351;&#24471;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#25509;&#22120;&#26159;&#24310;&#32493;&#23398;&#20064;&#20013;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#35757;&#32451;&#29420;&#31435;&#30340;&#36866;&#37197;&#22120;&#27169;&#22359;&#38169;&#22833;&#20102;&#36328;&#20219;&#21153;&#30693;&#35782;&#36716;&#31227;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Improvise to Initialize (I2I) &#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#65292;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#24207;&#21015;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102; I2I &#22312; CLiMB&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992; I2I &#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#22987;&#32456;&#27604;&#29420;&#31435;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#20219;&#21153;&#31934;&#24230;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20419;&#36827;&#20102;&#20219;&#21153;&#36866;&#37197;&#22120;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20808;&#36827;&#30340; AdapterFusion&#65292;I2I &#20063;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#36328;&#20219;&#21153;&#30693;&#35782;&#36716;&#31227;&#32780;&#19981;&#20135;&#29983;&#30456;&#20851;&#30340;&#21442;&#25968;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapters present a promising solution to the catastrophic forgetting problem in continual learning. However, training independent Adapter modules for every new task misses an opportunity for cross-task knowledge transfer. We propose Improvise to Initialize (I2I), a continual learning algorithm that initializes Adapters for incoming tasks by distilling knowledge from previously-learned tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning benchmark, by conducting experiments on sequences of visual question answering tasks. Adapters trained with I2I consistently achieve better task accuracy than independently-trained Adapters, demonstrating that our algorithm facilitates knowledge transfer between task Adapters. I2I also results in better cross-task knowledge transfer than the state-of-the-art AdapterFusion without incurring the associated parametric cost.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#19968;&#31181;&#22788;&#29702;&#24102;&#26377;&#22833;&#36133;&#36716;&#25442;&#30340;&#26080;&#29615;&#26435;&#37325;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#31639;&#27861;&#65292;&#20026;&#20102;&#23454;&#29616;&#22788;&#29702;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#24179;&#22343;&#29366;&#24577;&#20165;&#20855;&#26377;&#23383;&#27597;&#34920;&#19968;&#23567;&#37096;&#20998;&#30340;&#20986;&#24359;&#30340;&#24773;&#20917;&#19979;&#30340;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.06862</link><description>&lt;p&gt;
&#26080;&#29615;&#26435;&#37325;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#20013;&#24102;&#26377;&#22833;&#36133;&#24359;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Algorithms for Acyclic Weighted Finite-State Automata with Failure Arcs. (arXiv:2301.06862v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06862
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#19968;&#31181;&#22788;&#29702;&#24102;&#26377;&#22833;&#36133;&#36716;&#25442;&#30340;&#26080;&#29615;&#26435;&#37325;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#31639;&#27861;&#65292;&#20026;&#20102;&#23454;&#29616;&#22788;&#29702;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#24179;&#22343;&#29366;&#24577;&#20165;&#20855;&#26377;&#23383;&#27597;&#34920;&#19968;&#23567;&#37096;&#20998;&#30340;&#20986;&#24359;&#30340;&#24773;&#20917;&#19979;&#30340;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288; WSFAs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#22833;&#36133;&#36716;&#25442;&#26159;&#21387;&#32553;&#34920;&#31034;$n$-gram&#27169;&#22411;&#21644;CRF&#20013;&#30340;&#22238;&#36864;&#25110;&#25554;&#20540;&#30340;&#26377;&#29992;&#25193;&#23637;&#65292;&#23427;&#20204;&#26159;WFSAs&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#22312;&#26222;&#36890;&#26080;&#29615;WFSAs&#20013;&#65292;&#36890;&#36807;&#21518;&#21521;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#36335;&#24452;&#21644;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(|E|)$&#65292;&#20854;&#20013;$E$&#26159;&#36716;&#25442;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#19981;&#20801;&#35768;&#22833;&#36133;&#36716;&#25442;&#65292;&#32780;&#39044;&#22788;&#29702;WFSA&#20197;&#28040;&#38500;&#22833;&#36133;&#36716;&#25442;&#21487;&#33021;&#20250;&#22823;&#22823;&#22686;&#21152;$|E|$&#12290;&#25105;&#20204;&#23558;&#21518;&#21521;&#31639;&#27861;&#25193;&#23637;&#20026;&#30452;&#25509;&#22788;&#29702;&#22833;&#36133;&#36716;&#25442;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24179;&#22343;&#29366;&#24577;&#20165;&#20026;&#23383;&#27597;&#34920;$\Sigma$&#30340;&#19968;&#23567;&#37096;&#20998;$s\ll 1$&#20855;&#26377;&#20986;&#24359;&#30340;&#24773;&#20917;&#19979;&#65292;&#25928;&#29575;&#38750;&#24120;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#29992;&#20110;&#22788;&#29702;&#19968;&#33324;&#26080;&#29615;WFSAs&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$O{\left(|E| + s |\Sigma| |Q| T_\text{max} \log{|\Sigma|}\right)}$&#65292;&#20854;&#20013;$Q$&#26159;&#29366;&#24577;&#38598;&#21512;&#65292;$T_\text{max}$&#26159;&#25925;&#38556;&#36716;&#25442;&#30340;&#26368;&#22823;&#36830;&#36890;&#20998;&#37327;&#30340;&#22823;&#23567;&#12290;&#24403;&#25925;&#38556;&#36716;&#25442;&#25299;&#25169;&#32467;&#26500;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Weighted finite-state automata (WSFAs) are commonly used in NLP. Failure transitions are a useful extension for compactly representing backoffs or interpolation in $n$-gram models and CRFs, which are special cases of WFSAs. The pathsum in ordinary acyclic WFSAs is efficiently computed by the backward algorithm in time $O(|E|)$, where $E$ is the set of transitions. However, this does not allow failure transitions, and preprocessing the WFSA to eliminate failure transitions could greatly increase $|E|$. We extend the backward algorithm to handle failure transitions directly. Our approach is efficient when the average state has outgoing arcs for only a small fraction $s \ll 1$ of the alphabet $\Sigma$. We propose an algorithm for general acyclic WFSAs which runs in $O{\left(|E| + s |\Sigma| |Q| T_\text{max} \log{|\Sigma|}\right)}$, where $Q$ is the set of states and $T_\text{max}$ is the size of the largest connected component of failure transitions. When the failure transition topology s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#25552;&#31034;(PARC)&#31649;&#36947;&#65292;&#22312;&#38646;-shot&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#36890;&#36807;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#26816;&#32034;&#20986;&#30340;&#35821;&#20041;&#19978;&#31867;&#20284;&#30340;&#21477;&#23376;&#26469;&#25913;&#21892;&#24615;&#33021;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110; fine-tuning &#22522;&#32447;&#65292;&#21516;&#26102;&#19982;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#20302;&#36164;&#28304;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#23384;&#22312;&#26174;&#33879;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2212.09651</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages. (arXiv:2212.09651v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#25552;&#31034;(PARC)&#31649;&#36947;&#65292;&#22312;&#38646;-shot&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#36890;&#36807;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#26816;&#32034;&#20986;&#30340;&#35821;&#20041;&#19978;&#31867;&#20284;&#30340;&#21477;&#23376;&#26469;&#25913;&#21892;&#24615;&#33021;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110; fine-tuning &#22522;&#32447;&#65292;&#21516;&#26102;&#19982;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#20302;&#36164;&#28304;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#23384;&#22312;&#26174;&#33879;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(MPLMs)&#22312;&#26368;&#36817;&#30340;&#32463;&#39564;&#36328;&#35821;&#35328;&#36716;&#31227;&#30740;&#31350;&#20013;&#23637;&#29616;&#20102;&#20854;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;(PARC)&#31649;&#36947;&#65292;&#36890;&#36807;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;(HRL)&#20013;&#26816;&#32034;&#20986;&#30340;&#35821;&#20041;&#19978;&#31867;&#20284;&#30340;&#21477;&#23376;&#20316;&#20026;&#25552;&#31034;&#26469;&#25913;&#21892;&#38646;-shot&#20302;&#36164;&#28304;&#35821;&#35328;(LRLs)&#30340;&#24615;&#33021;&#12290;PARC&#36890;&#36807;&#22810;&#35821;&#35328;&#24182;&#34892;&#27979;&#35797;&#38598;&#22312;&#19977;&#20010;&#19979;&#28216;&#20219;&#21153;(&#20108;&#20803;&#24773;&#24863;&#20998;&#31867;&#12289;&#20027;&#39064;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;)&#19978;&#25552;&#39640;&#20102;&#38646;-shot&#30340;&#24615;&#33021;&#65292;&#35206;&#30422;&#20102;10&#20010;LRLs&#65292;&#28085;&#30422;&#20102;6&#31181;&#35821;&#35328;&#23478;&#26063;&#65292;&#22312;&#26410;&#26631;&#35760;&#30340;&#35774;&#32622;&#20013;&#25552;&#39640;&#20102;(+5.1%)&#65292;&#22312;&#26631;&#35760;&#30340;&#35774;&#32622;&#20013;&#25552;&#39640;&#20102;(+16.3%)&#12290;PARC&#26631;&#35760;&#36824;&#36229;&#36234;&#20102; fine-tuning &#22522;&#32447;3.7%&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#24615;&#33021;&#22312;&#19968;&#26041;&#38754;&#19982;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#20302;&#36164;&#28304;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual Pretrained Language Models (MPLMs) have shown their strong multilinguality in recent empirical cross-lingual transfer studies. In this paper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC) pipeline to improve the zero-shot performance on low-resource languages (LRLs) by augmenting the context with semantically similar sentences retrieved from a high-resource language (HRL) as prompts. PARC improves the zero-shot performance on three downstream tasks (binary sentiment classification, topic categorization and natural language inference) with multilingual parallel test sets across 10 LRLs covering 6 language families in both unlabeled settings (+5.1%) and labeled settings (+16.3%). PARC-labeled also outperforms the finetuning baseline by 3.7%. We find a significant positive correlation between cross-lingual transfer performance on one side, and the similarity between the high- and low-resource languages as well as the amount of low-resource pretraining da
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#26469;&#29983;&#25104;&#20805;&#20998;&#21644;&#31616;&#26126;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#40657;&#30418;&#39044;&#27979;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.09603</link><description>&lt;p&gt;
&#20449;&#24687;&#29942;&#39048;&#36890;&#36807;&#35299;&#37322;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Explanation Regeneration via Information Bottleneck. (arXiv:2212.09603v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#26469;&#29983;&#25104;&#20805;&#20998;&#21644;&#31616;&#26126;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#40657;&#30418;&#39044;&#27979;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#65292;&#35299;&#37322;NLP&#27169;&#22411;&#30340;&#40657;&#30418;&#39044;&#27979;&#33258;&#28982;&#32780;&#20934;&#30830;&#22320;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#36825;&#20123;&#33258;&#30001;&#25991;&#26412;&#30340;&#35299;&#37322;&#34987;&#26399;&#26395;&#21253;&#21547;&#36275;&#22815;&#21644;&#32463;&#36807;&#31934;&#24515;&#36873;&#25321;&#30340;&#35777;&#25454;&#65292;&#20197;&#24418;&#25104;&#23545;&#39044;&#27979;&#30340;&#25903;&#25345;&#24615;&#35770;&#25454;&#12290;&#30001;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#20511;&#21161;&#25552;&#31034;&#24037;&#31243;&#20351;&#24471;&#35299;&#37322;&#29983;&#25104;&#21487;&#20197;&#19981;&#38656;&#35201;&#29305;&#23450;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21333;&#27425;&#25552;&#31034;&#29983;&#25104;&#30340;&#35299;&#37322;&#24448;&#24448;&#32570;&#20047;&#20805;&#20998;&#24615;&#21644;&#31616;&#26126;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;EIB&#65292;&#29992;&#20110;&#20135;&#29983;&#20805;&#20998;&#21644;&#31616;&#26126;&#30340;&#31934;&#28860;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21333;&#27425;&#36755;&#20986;&#36827;&#34892;&#20248;&#21270;&#65292;&#21516;&#26102;&#20445;&#30041;&#25903;&#25345;&#25152;&#35299;&#37322;&#20869;&#23481;&#30340;&#20449;&#24687;&#26469;&#37325;&#24314;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#39046;&#22495;&#22806;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#36890;&#36807;&#33258;&#21160;&#35780;&#20272;&#21644;&#24443;&#24213;&#39564;&#35777;&#20102;EIB&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining the black-box predictions of NLP models naturally and accurately is an important open problem in natural language generation. These free-text explanations are expected to contain sufficient and carefully-selected evidence to form supportive arguments for predictions. Due to the superior generative capacity of large pretrained language models, recent work built on prompt engineering enables explanation generation without specific training. However, explanation generated through single-pass prompting often lacks sufficiency and conciseness. To address this problem, we develop an information bottleneck method EIB to produce refined explanations that are sufficient and concise. Our approach regenerates the free-text explanation by polishing the single-pass output from the pretrained language model but retaining the information that supports the contents being explained. Experiments on two out-of-domain tasks verify the effectiveness of EIB through automatic evaluation and thorou
&lt;/p&gt;</description></item><item><title>TencentPretrain&#26159;&#19968;&#20010;&#25903;&#25345;&#19981;&#21516;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24037;&#20855;&#21253;&#65292;&#20855;&#26377;&#28789;&#27963;&#30340;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36873;&#25321;&#32452;&#20214;&#21644;&#27169;&#22359;&#26469;&#26500;&#24314;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#25991;&#26412;&#12289;&#35270;&#35273;&#21644;&#38899;&#39057;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.06385</link><description>&lt;p&gt;
TencentPretrain:&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#28789;&#27963;&#30340;&#19981;&#21516;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
TencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models of Different Modalities. (arXiv:2212.06385v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06385
&lt;/p&gt;
&lt;p&gt;
TencentPretrain&#26159;&#19968;&#20010;&#25903;&#25345;&#19981;&#21516;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24037;&#20855;&#21253;&#65292;&#20855;&#26377;&#28789;&#27963;&#30340;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36873;&#25321;&#32452;&#20214;&#21644;&#27169;&#22359;&#26469;&#26500;&#24314;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#25991;&#26412;&#12289;&#35270;&#35273;&#21644;&#38899;&#39057;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#39046;&#22495;&#20013;&#39044;&#35757;&#32451;&#30340;&#25104;&#21151;&#24050;&#32463;&#23436;&#20840;&#25193;&#23637;&#21040;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#36328;&#27169;&#24577;&#30340;&#22330;&#26223;&#12290;&#25552;&#20986;&#30340;&#19981;&#21516;&#27169;&#24577;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20854;&#27169;&#22411;&#32467;&#26500;&#19978;&#21576;&#29616;&#20986;&#36234;&#26469;&#36234;&#36235;&#21516;&#30340;&#36235;&#21183;&#65292;&#36825;&#20026;&#22312;&#32479;&#19968;&#26694;&#26550;&#20869;&#23454;&#29616;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TencentPretrain&#65292;&#19968;&#20010;&#25903;&#25345;&#19981;&#21516;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24037;&#20855;&#21253;&#12290;TencentPretrain&#30340;&#26680;&#24515;&#29305;&#24615;&#26159;&#27169;&#22359;&#21270;&#35774;&#35745;&#12290;&#35813;&#24037;&#20855;&#21253;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#32479;&#19968;&#21010;&#20998;&#20026;5&#20010;&#32452;&#20214;&#65306;&#23884;&#20837;&#12289;&#32534;&#30721;&#22120;&#12289;&#30446;&#26631;&#23884;&#20837;&#12289;&#35299;&#30721;&#22120;&#21644;&#30446;&#26631;&#12290;&#30001;&#20110;&#27599;&#20010;&#32452;&#20214;&#20013;&#25552;&#20379;&#20102;&#20960;&#20046;&#25152;&#26377;&#24120;&#35265;&#30340;&#27169;&#22359;&#65292;&#29992;&#25143;&#21487;&#20197;&#20174;&#19981;&#21516;&#30340;&#32452;&#20214;&#20013;&#36873;&#25321;&#25152;&#38656;&#30340;&#27169;&#22359;&#26469;&#26500;&#24314;&#23436;&#25972;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#27169;&#22359;&#21270;&#35774;&#35745;&#20351;&#29992;&#25143;&#33021;&#22815;&#39640;&#25928;&#22320;&#22797;&#29616;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#26500;&#24314;&#20840;&#26032;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#12289;&#35270;&#35273;&#21644;&#38899;&#39057;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;&#35813;&#24037;&#20855;&#21253;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the success of pre-training in text domain has been fully extended to vision, audio, and cross-modal scenarios. The proposed pre-training models of different modalities are showing a rising trend of homogeneity in their model structures, which brings the opportunity to implement different pre-training models within a uniform framework. In this paper, we present TencentPretrain, a toolkit supporting pre-training models of different modalities. The core feature of TencentPretrain is the modular design. The toolkit uniformly divides pre-training models into 5 components: embedding, encoder, target embedding, decoder, and target. As almost all of common modules are provided in each component, users can choose the desired modules from different components to build a complete pre-training model. The modular design enables users to efficiently reproduce existing pre-training models or build brand-new one. We test the toolkit on text, vision, and audio benchmarks and show that it can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Treeformer&#27169;&#22359;&#65292;&#23427;&#20511;&#37492;&#20102;CKY&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32452;&#21512;&#36816;&#31639;&#31526;&#21644;&#27719;&#32858;&#20989;&#25968;&#26469;&#26500;&#24314;&#30701;&#35821;&#21644;&#21477;&#23376;&#30340;&#23618;&#27425;&#32534;&#30721;&#65292;&#20174;&#32780;&#23558;&#23618;&#27425;&#32467;&#26500;&#32435;&#20837;Transformer&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#27169;&#22359;&#22312;&#32452;&#21512;&#27867;&#21270;&#21644;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2207.06960</link><description>&lt;p&gt;
&#29992;Treeformers&#29983;&#25104;&#26641;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Forming Trees with Treeformers. (arXiv:2207.06960v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Treeformer&#27169;&#22359;&#65292;&#23427;&#20511;&#37492;&#20102;CKY&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32452;&#21512;&#36816;&#31639;&#31526;&#21644;&#27719;&#32858;&#20989;&#25968;&#26469;&#26500;&#24314;&#30701;&#35821;&#21644;&#21477;&#23376;&#30340;&#23618;&#27425;&#32534;&#30721;&#65292;&#20174;&#32780;&#23558;&#23618;&#27425;&#32467;&#26500;&#32435;&#20837;Transformer&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#27169;&#22359;&#22312;&#32452;&#21512;&#27867;&#21270;&#21644;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35821;&#35328;&#20855;&#26377;&#23884;&#22871;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#36739;&#23567;&#30340;&#29255;&#27573;&#20013;&#26500;&#24314;&#22797;&#26434;&#30340;&#21477;&#23376;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;&#22914;Transformers&#65289;&#22312;&#20854;&#26550;&#26500;&#20013;&#27809;&#26377;&#26126;&#30830;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#21363;&#23427;&#20204;&#23545;&#23618;&#27425;&#32467;&#26500;&#27809;&#26377;&#24402;&#32435;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#24050;&#30693;Transformers&#22312;&#38656;&#35201;&#36825;&#31181;&#32467;&#26500;&#30340;&#32452;&#21512;&#27867;&#21270;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Treeformer&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#21463;&#21040;CKY&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#20010;&#32452;&#21512;&#36816;&#31639;&#31526;&#21644;&#27719;&#32858;&#20989;&#25968;&#65292;&#29992;&#20110;&#26500;&#24314;&#30701;&#35821;&#21644;&#21477;&#23376;&#30340;&#23618;&#27425;&#32534;&#30721;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#23618;&#27425;&#32467;&#26500;&#32435;&#20837;Transformer&#27169;&#22411;&#20013;&#30340;&#22909;&#22788;&#65292;&#24182;&#19988;&#22312;&#32452;&#21512;&#27867;&#21270;&#20197;&#21450;&#26426;&#22120;&#32763;&#35793;&#12289;&#25277;&#35937;&#25688;&#35201;&#21644;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human language is known to exhibit a nested, hierarchical structure, allowing us to form complex sentences out of smaller pieces. However, many state-of-the-art neural networks models such as Transformers have no explicit hierarchical structure in its architecture -- that is, they don't have an inductive bias toward hierarchical structure. Additionally, Transformers are known to perform poorly on compositional generalization tasks which require such structures. In this paper, we introduce Treeformer, a general-purpose encoder module inspired by the CKY algorithm which learns a composition operator and pooling function to construct hierarchical encodings for phrases and sentences. Our extensive experiments demonstrate the benefits of incorporating hierarchical structure into the Transformer and show significant improvements in compositional generalization as well as in downstream tasks such as machine translation, abstractive summarization, and various natural language understanding tas
&lt;/p&gt;</description></item><item><title>LegoNN&#26159;&#19968;&#31181;&#21487;&#20197;&#26500;&#24314;&#27169;&#22359;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21508;&#20010;&#32452;&#20214;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#21487;&#37325;&#29992;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#31163;&#25955;&#35789;&#27719;&#36793;&#32536;&#20998;&#24067;&#30340;&#25509;&#21475;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#22791;&#23545;&#26799;&#24230;&#30340;&#21487;&#20256;&#36882;&#24615;&#25110;&#38548;&#31163;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#35299;&#30721;&#22120;&#27169;&#22359;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#21487;&#31227;&#26893;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.03318</link><description>&lt;p&gt;
LegoNN&#65306;&#26500;&#24314;&#27169;&#22359;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LegoNN: Building Modular Encoder-Decoder Models. (arXiv:2206.03318v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03318
&lt;/p&gt;
&lt;p&gt;
LegoNN&#26159;&#19968;&#31181;&#21487;&#20197;&#26500;&#24314;&#27169;&#22359;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21508;&#20010;&#32452;&#20214;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#21487;&#37325;&#29992;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#31163;&#25955;&#35789;&#27719;&#36793;&#32536;&#20998;&#24067;&#30340;&#25509;&#21475;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#22791;&#23545;&#26799;&#24230;&#30340;&#21487;&#20256;&#36882;&#24615;&#25110;&#38548;&#31163;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#35299;&#30721;&#22120;&#27169;&#22359;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#21487;&#31227;&#26893;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65288;&#20363;&#22914;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#25110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65289;&#34987;&#26500;&#24314;&#21644;&#35757;&#32451;&#20026;&#19968;&#20010;&#19981;&#21487;&#20998;&#21106;&#30340;&#25972;&#20307;&#12290;&#27169;&#22411;&#30340;&#20219;&#20309;&#32452;&#20214;&#37117;&#19981;&#33021;&#29420;&#31435;&#20351;&#29992;&#25110;&#37325;&#22797;&#20351;&#29992;&#65292;&#22240;&#27492;&#26080;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#37096;&#20998;&#65292;&#20363;&#22914;&#39640;&#36164;&#28304;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;LegoNN&#65292;&#19968;&#31181;&#26500;&#24314;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#21508;&#20010;&#32452;&#20214;&#21487;&#20197;&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#21487;&#37325;&#29992;&#24615;&#65292;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22359;&#20043;&#38388;&#30340;&#25509;&#21475;&#22522;&#20110;&#39044;&#23450;&#20041;&#31163;&#25955;&#35789;&#27719;&#30340;&#36793;&#32536;&#20998;&#24067;&#24207;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25668;&#21462;&#36825;&#20123;&#36793;&#32536;&#20998;&#24067;&#30340;&#26041;&#27861;&#65307;&#20854;&#20013;&#19968;&#31181;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#20801;&#35768;&#26799;&#24230;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#20256;&#36882;&#65292;&#21478;&#19968;&#31181;&#26159;&#26799;&#24230;&#38548;&#31163;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#35299;&#30721;&#22120;&#27169;&#22359;&#22312;&#19981;&#21516;&#28304;&#35821;&#35328;&#30340;MT&#20219;&#21153;&#21644;&#20854;&#20182;&#20219;&#21153;&#65288;&#22914;ASR&#65289;&#20043;&#38388;&#30340;&#21487;&#31227;&#26893;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#24577;&#19981;&#21487;&#30693;&#30340;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art encoder-decoder models (e.g. for machine translation (MT) or automatic speech recognition (ASR)) are constructed and trained end-to-end as an atomic unit. No component of the model can be (re-)used without the others, making it impossible to share parts, e.g. a high resourced decoder, across tasks. We describe LegoNN, a procedure for building encoder-decoder architectures in a way so that its parts can be applied to other tasks without the need for any fine-tuning. To achieve this reusability, the interface between encoder and decoder modules is grounded to a sequence of marginal distributions over a pre-defined discrete vocabulary. We present two approaches for ingesting these marginals; one is differentiable, allowing the flow of gradients across the entire network, and the other is gradient-isolating. To enable the portability of decoder modules between MT tasks for different source languages and across other tasks like ASR, we introduce a modality agnostic encoder 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Talmudic Public Announcement Logic&#30340;&#26032;&#39062;&#35299;&#37322;&#24615;&#26041;&#27861;BTPK&#65292;&#29992;&#20110;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#20869;&#37096;&#36923;&#36753;&#65292;&#21516;&#26102;&#33021;&#22815;&#25429;&#25417;&#21477;&#23376;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#19978;&#19979;&#25991;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2201.09523</link><description>&lt;p&gt;
&#22522;&#20110;Talmudic Public Announcement Logic&#30340;BTPK&#35299;&#37322;&#24615;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BTPK-based interpretable method for NER tasks based on Talmudic Public Announcement Logic. (arXiv:2201.09523v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Talmudic Public Announcement Logic&#30340;&#26032;&#39062;&#35299;&#37322;&#24615;&#26041;&#27861;BTPK&#65292;&#29992;&#20110;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#20869;&#37096;&#36923;&#36753;&#65292;&#21516;&#26102;&#33021;&#22815;&#25429;&#25417;&#21477;&#23376;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#19978;&#19979;&#25991;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#26159;NLP&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#20449;&#24687;&#25552;&#21462;&#12289;&#21477;&#27861;&#20998;&#26512;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#65289;&#30340;&#37325;&#35201;&#22522;&#30784;&#24037;&#20855;&#12290;&#24403;&#21069;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#23545;&#29992;&#25143;&#26469;&#35828;&#26159;&#40657;&#30418;&#25805;&#20316;&#65292;&#29992;&#25143;&#27809;&#26377;&#20381;&#25454;&#26469;&#30830;&#23450;&#21738;&#20010;&#21629;&#21517;&#23454;&#20307;&#26356;&#26377;&#24847;&#20041;&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#35782;&#21035;&#36807;&#31243;&#23545;&#35768;&#22810;&#20154;&#26469;&#35828;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;BTPK&#65288;Binary Talmudic Public Announcement Logic&#27169;&#22411;&#65289;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#22522;&#20110;Talmudic Public Announcement Logic&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#20869;&#37096;&#35782;&#21035;&#36923;&#36753;&#12290;BTPK&#27169;&#22411;&#36824;&#21487;&#20197;&#25429;&#25417;&#36755;&#20837;&#21477;&#23376;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#21363;&#21477;&#23376;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;BTPK&#30340;&#20844;&#20849;&#20844;&#21578;&#21576;&#29616;&#20102;BRNNs&#30340;&#20869;&#37096;&#20915;&#31574;&#36923;&#36753;&#65292;&#24182;&#20174;&#20013;&#33719;&#24471;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
As one of the basic tasks in natural language processing (NLP), named entity recognition (NER) is an important basic tool for downstream tasks of NLP, such as information extraction, syntactic analysis, machine translation and so on. The internal operation logic of current name entity recognition model is black-box to the user, so the user has no basis to determine which name entity makes more sense. Therefore, a user-friendly explainable recognition process would be very useful for many people. In this paper, we propose a novel interpretable method, BTPK (Binary Talmudic Public Announcement Logic model), to help users understand the internal recognition logic of the name entity recognition tasks based on Talmudic Public Announcement Logic. BTPK model can also capture the semantic information in the input sentences, that is, the context dependency of the sentence. We observed the public announcement of BTPK presents the inner decision logic of BRNNs, and the explanations obtained from 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#35757;&#32451;&#23436;&#25104;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#23618;&#38754;&#21644;&#31070;&#32463;&#20803;&#27700;&#24179;&#30340;&#20998;&#26512;&#65292;&#25506;&#32034;&#20102;&#20854;&#20013;&#20851;&#20110;&#35828;&#35805;&#20154;&#12289;&#35821;&#35328;&#21644;&#20449;&#36947;&#23646;&#24615;&#30340;&#20449;&#24687;&#25429;&#33719;&#24773;&#20917;&#12290;&#20854;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#35299;&#37322;&#27169;&#22411;&#23398;&#20064;&#30340;&#20851;&#38190;&#29305;&#24449;&#21450;&#20854;&#22312;&#23454;&#29616;&#20844;&#27491;&#24615;&#20915;&#31574;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2107.00439</link><description>&lt;p&gt;
&#31471;&#21040;&#31471;&#35821;&#38899;&#27169;&#22411;&#23398;&#20064;&#20102;&#21738;&#20123;&#20851;&#20110;&#35828;&#35805;&#20154;&#12289;&#35821;&#35328;&#21644;&#20449;&#36947;&#20449;&#24687;&#65311;&#19968;&#39033;&#23618;&#38754;&#21644;&#31070;&#32463;&#20803;&#27700;&#24179;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
What do End-to-End Speech Models Learn about Speaker, Language and Channel Information? A Layer-wise and Neuron-level Analysis. (arXiv:2107.00439v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.00439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#35757;&#32451;&#23436;&#25104;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#23618;&#38754;&#21644;&#31070;&#32463;&#20803;&#27700;&#24179;&#30340;&#20998;&#26512;&#65292;&#25506;&#32034;&#20102;&#20854;&#20013;&#20851;&#20110;&#35828;&#35805;&#20154;&#12289;&#35821;&#35328;&#21644;&#20449;&#36947;&#23646;&#24615;&#30340;&#20449;&#24687;&#25429;&#33719;&#24773;&#20917;&#12290;&#20854;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#35299;&#37322;&#27169;&#22411;&#23398;&#20064;&#30340;&#20851;&#38190;&#29305;&#24449;&#21450;&#20854;&#22312;&#23454;&#29616;&#20844;&#27491;&#24615;&#20915;&#31574;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22825;&#29983;&#38590;&#20197;&#35299;&#37322;&#21644;&#29702;&#35299;&#12290;&#19982;&#25163;&#24037;&#29305;&#24449;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#38590;&#20197;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#20102;&#21738;&#20123;&#27010;&#24565;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#29702;&#35299;&#19981;&#20165;&#23545;&#20110;&#35843;&#35797;&#30446;&#30340;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#19988;&#23545;&#20110;&#30830;&#20445;&#36947;&#24503;&#20915;&#31574;&#20013;&#30340;&#20844;&#27491;&#24615;&#20063;&#24456;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25506;&#27979;&#26694;&#26550;[1]&#23545;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#20107;&#21518;&#21151;&#33021;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#38024;&#23545;&#19981;&#21516;&#20219;&#21153;&#65288;&#22914;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#26041;&#35328;&#35782;&#21035;&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;&#35821;&#38899;&#27169;&#22411;&#30340;&#35805;&#35821;&#27700;&#24179;&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23618;&#38754;&#21644;&#31070;&#32463;&#20803;&#27700;&#24179;&#30340;&#20998;&#26512;&#65292;&#25506;&#32034;&#35828;&#35805;&#20154;&#12289;&#35821;&#35328;&#21644;&#20449;&#36947;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;i&#65289;&#34920;&#31034;&#20013;&#25429;&#33719;&#20102;&#21738;&#20123;&#20449;&#24687;&#65311;ii&#65289;&#23427;&#26159;&#22914;&#20309;&#34920;&#31034;&#21644;&#20998;&#24067;&#30340;&#65311;&#20197;&#21450;iii&#65289;&#25105;&#20204;&#33021;&#21542;&#30830;&#23450;&#25317;&#26377;&#27492;&#20449;&#24687;&#30340;&#32593;&#32476;&#30340;&#26368;&#23567;&#23376;&#38598;&#65311;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20123;&#26032;&#30340;&#21457;&#29616;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are inherently opaque and challenging to interpret. Unlike hand-crafted feature-based models, we struggle to comprehend the concepts learned and how they interact within these models. This understanding is crucial not only for debugging purposes but also for ensuring fairness in ethical decision-making. In our study, we conduct a post-hoc functional interpretability analysis of pretrained speech models using the probing framework [1]. Specifically, we analyze utterance-level representations of speech models trained for various tasks such as speaker recognition and dialect identification. We conduct layer and neuron-wise analyses, probing for speaker, language, and channel properties. Our study aims to answer the following questions: i) what information is captured within the representations? ii) how is it represented and distributed? and iii) can we identify a minimal subset of the network that possesses this information?  Our results reveal several novel findings,
&lt;/p&gt;</description></item></channel></rss>