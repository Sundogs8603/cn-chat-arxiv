<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>QueryAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#21453;&#39304;&#30340;&#33258;&#25105;&#26657;&#27491;&#26041;&#27861;ERASER&#65292;&#22312;&#35821;&#20041;&#35299;&#26512;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#39640;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.11886</link><description>&lt;p&gt;
QueryAgent&#65306;&#19968;&#31181;&#20855;&#26377;&#29615;&#22659;&#21453;&#39304;&#30340;&#21487;&#38752;&#39640;&#25928;&#25512;&#29702;&#26694;&#26550;&#21450;&#33258;&#25105;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11886
&lt;/p&gt;
&lt;p&gt;
QueryAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#21453;&#39304;&#30340;&#33258;&#25105;&#26657;&#27491;&#26041;&#27861;ERASER&#65292;&#22312;&#35821;&#20041;&#35299;&#26512;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#39640;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35821;&#20041;&#35299;&#26512;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#22312;&#36935;&#21040;&#24187;&#35273;&#26102;&#29616;&#26377;&#26041;&#27861;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;QueryAgent&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#20915;&#38382;&#39064;&#24182;&#36827;&#34892;&#36880;&#27493;&#33258;&#25105;&#26657;&#27491;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#21453;&#39304;&#30340;&#33258;&#25105;&#26657;&#27491;&#26041;&#27861;ERASER&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;ERASER&#21033;&#29992;&#20013;&#38388;&#27493;&#39588;&#20013;&#30340;&#20016;&#23500;&#29615;&#22659;&#21453;&#39304;&#65292;&#22312;&#24517;&#35201;&#26102;&#20165;&#36827;&#34892;&#36873;&#25321;&#24615;&#21644;&#24046;&#24322;&#21270;&#30340;&#33258;&#25105;&#26657;&#27491;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QueryAgent&#22312;GrailQA&#21644;GraphQ&#19978;&#20165;&#20351;&#29992;&#19968;&#20010;&#20363;&#23376;&#23601;&#27604;&#25152;&#26377;&#20808;&#21069;&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#21462;&#24471;&#20102;7.0&#21644;15.0&#30340;F1&#20540;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#21253;&#25324;&#36816;&#34892;&#26102;&#38388;&#12289;&#26597;&#35810;&#24320;&#38144;&#21644;API&#35843;&#29992;&#25104;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;ERASER&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11886v1 Announce Type: cross  Abstract: Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs. By leveraging ERASER, we
&lt;/p&gt;</description></item><item><title>KEBench&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#26032;&#22686;&#21152;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#31227;&#26893;&#24615;&#65289;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.07350</link><description>&lt;p&gt;
KEBench: &#29992;&#20110;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07350
&lt;/p&gt;
&lt;p&gt;
KEBench&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#26032;&#22686;&#21152;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#31227;&#26893;&#24615;&#65289;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07350v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#30446;&#21069;&#65292;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#30693;&#35782;&#32534;&#36753;&#30740;&#31350;&#24456;&#23569;&#12290;&#32534;&#36753;LVLMs&#38754;&#20020;&#30528;&#26377;&#25928;&#25972;&#21512;&#22810;&#31181;&#27169;&#24577;&#65288;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#30830;&#20445;&#20462;&#25913;&#36830;&#36143;&#19988;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#12290;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#19977;&#20010;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#38752;&#24615;&#12289;&#23616;&#37096;&#24615;&#21644;&#19968;&#33324;&#24615;&#65289;&#29992;&#20110;&#34913;&#37327;LVLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#22312;&#35780;&#20272;&#20013;&#20351;&#29992;&#30340;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#19981;&#36275;&#65292;&#24182;&#19988;&#26080;&#27861;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#26377;&#25928;&#22320;&#21033;&#29992;&#19982;&#30456;&#20851;&#20869;&#23481;&#30456;&#20851;&#30340;&#32534;&#36753;&#30693;&#35782;&#12290;&#25105;&#20204;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;$\textbf{KEBench}$&#65292;&#24182;&#25193;&#23637;&#20102;&#26032;&#24230;&#37327;&#26631;&#20934;(&#21487;&#31227;&#26893;&#24615;)&#20197;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#20511;&#21161;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#65292;&#25105;&#20204;&#30340;&#22270;&#20687;&#25968;&#25454;&#21576;&#29616;&#20986;&#26126;&#30830;&#30340;&#32473;&#23454;&#20307;&#26041;&#21521;&#24615;&#12290;&#36825;&#31181;&#26041;&#21521;&#24615;&#21487;&#20197;&#36827;&#19968;&#27493;&#29992;&#20110;&#25552;&#21462;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#30693;&#35782;&#21644;&#36827;&#34892;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07350v1 Announce Type: cross  Abstract: Currently, little research has been done on knowledge editing for Large Vision-Language Models (LVLMs). Editing LVLMs faces the challenge of effectively integrating diverse modalities (image and text) while ensuring coherent and contextually relevant modifications. An existing benchmark has three metrics (Reliability, Locality and Generality) to measure knowledge editing for LVLMs. However, the benchmark falls short in the quality of generated images used in evaluation and cannot assess whether models effectively utilize edited knowledge in relation to the associated content. We adopt different data collection methods to construct a new benchmark, $\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive evaluation. Leveraging a multimodal knowledge graph, our image data exhibits clear directionality towards entities. This directional aspect can be further utilized to extract entity-related knowledge and form editing 
&lt;/p&gt;</description></item><item><title>Mad Libs &#25552;&#20379;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861; MLA &#22312;&#36328;&#39046;&#22495;&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25968;&#25454;&#25552;&#21462;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102; 2.6 &#20010; F1 &#20998;&#25968;&#12290;&#21516;&#26102;&#65292;&#22312;&#38646;&#21644;&#23569;&#26679;&#26412;&#20107;&#20214;&#35282;&#33394;&#26041;&#38754;&#30456;&#27604;&#26080;&#22686;&#24378;&#22522;&#32447;&#65292;&#25552;&#39640;&#20102; 3.9 &#21644; 5.2 &#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.03304</link><description>&lt;p&gt;
&#25152;&#38656;&#21482;&#26159; Mad Libs: &#22686;&#24378;&#36328;&#39046;&#22495;&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03304
&lt;/p&gt;
&lt;p&gt;
Mad Libs &#25552;&#20379;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861; MLA &#22312;&#36328;&#39046;&#22495;&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25968;&#25454;&#25552;&#21462;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102; 2.6 &#20010; F1 &#20998;&#25968;&#12290;&#21516;&#26102;&#65292;&#22312;&#38646;&#21644;&#23569;&#26679;&#26412;&#20107;&#20214;&#35282;&#33394;&#26041;&#38754;&#30456;&#27604;&#26080;&#22686;&#24378;&#22522;&#32447;&#65292;&#25552;&#39640;&#20102; 3.9 &#21644; 5.2 &#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25552;&#21462;&#65288;DocEAE&#65289;&#26159;&#19968;&#20010;&#26497;&#20854;&#22256;&#38590;&#30340;&#20449;&#24687;&#25552;&#21462;&#38382;&#39064;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#36328;&#39046;&#22495;&#35774;&#32622;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; Mad Lib Aug&#65288;MLA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335; DocEAE &#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102; Mad Libs &#30340;&#30452;&#35273;&#65292;&#21363;&#20316;&#20026;&#19968;&#31181;&#28909;&#38376;&#28216;&#25103;&#20013;&#20351;&#29992;&#30340;&#20998;&#31867;&#25513;&#30721;&#25991;&#26723;&#21487;&#20197;&#34987; LLMs &#29983;&#25104;&#24182;&#35299;&#31572;&#65292;&#20174;&#32780;&#20026; DocEAE &#29983;&#25104;&#25968;&#25454;&#12290;&#20351;&#29992; MLA&#65292;&#25105;&#20204;&#30340;&#25972;&#20307; F1 &#20998;&#25968;&#24179;&#22343;&#25913;&#36827;&#20102; 2.6 &#20010;&#30334;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;&#19982;&#26080;&#22686;&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#21644;&#23569;&#26679;&#26412;&#20107;&#20214;&#35282;&#33394;&#26041;&#38754;&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#24179;&#22343;&#25552;&#39640;&#20102; 3.9 &#21644; 5.2 &#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03304v1 Announce Type: new  Abstract: Document-Level Event Argument Extraction (DocEAE) is an extremely difficult information extraction problem -- with significant limitations in low-resource cross-domain settings. To address this problem, we introduce Mad Lib Aug (MLA), a novel generative DocEAE data augmentation framework. Our approach leverages the intuition that Mad Libs, which are categorically masked documents used as a part of a popular game, can be generated and solved by LLMs to produce data for DocEAE. Using MLA, we achieve a 2.6-point average improvement in overall F1 score. Moreover, this approach achieves a 3.9 and 5.2 point average increase in zero and few-shot event roles compared to augmentation-free baselines across all experiments.   To better facilitate analysis of cross-domain DocEAE, we additionally introduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to identify roles in the target domain which are semantic outliers with respect t
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Emotional Voice Messages&#25968;&#25454;&#24211;&#65292;&#32467;&#21512;eGeMAPS&#29305;&#24449;&#21644;Transformer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#37326;&#22806;&#24405;&#21046;&#30340;&#35821;&#38899;&#28040;&#24687;&#20013;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#27604;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;10%&#12290;</title><link>https://arxiv.org/abs/2403.02167</link><description>&lt;p&gt;
&#20174;&#37326;&#22806;&#24405;&#21046;&#30340;&#35821;&#38899;&#28040;&#24687;&#20013;&#35782;&#21035;&#35821;&#38899;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition from voice messages recorded in the wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02167
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Emotional Voice Messages&#25968;&#25454;&#24211;&#65292;&#32467;&#21512;eGeMAPS&#29305;&#24449;&#21644;Transformer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#37326;&#22806;&#24405;&#21046;&#30340;&#35821;&#38899;&#28040;&#24687;&#20013;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#27604;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#30340;&#24773;&#24863;&#25968;&#25454;&#38598;&#36890;&#24120;&#21253;&#21547;&#34920;&#28436;&#25110;&#24341;&#21457;&#30340;&#35821;&#38899;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;Emotional Voice Messages&#65288;EMOVOME&#65289;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35821;&#20351;&#29992;&#32773;&#22312;&#28040;&#24687;&#24212;&#29992;&#20013;&#30340;&#33258;&#21457;&#35821;&#38899;&#28040;&#24687;&#65292;&#30001;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#26631;&#27880;&#32773;&#20197;&#36830;&#32493;&#21644;&#31163;&#25955;&#30340;&#24773;&#24863;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;eGeMAPS&#29305;&#24449;&#12289;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30340;&#32452;&#21512;&#26469;&#21019;&#24314;&#35762;&#35805;&#32773;&#26080;&#20851;&#30340;SER&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#21442;&#32771;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20998;&#26512;&#20102;&#26631;&#27880;&#32773;&#21644;&#24615;&#21035;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#39044;&#35757;&#32451;&#30340;Unispeech-L&#27169;&#22411;&#21450;&#20854;&#19982;eGeMAPS&#30340;&#32452;&#21512;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#22312;3&#31867;valence&#21644;arousal&#39044;&#27979;&#20013;&#20998;&#21035;&#33719;&#24471;&#20102;61.64%&#21644;55.57%&#30340;Unweighted Accuracy&#65288;UA&#65289;&#65292;&#27604;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;10%&#12290;&#23545;&#20110;&#24773;&#24863;&#31867;&#21035;&#65292;&#33719;&#24471;&#20102;42.58%&#30340;UA&#12290;EMOVOME&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02167v1 Announce Type: cross  Abstract: Emotion datasets used for Speech Emotion Recognition (SER) often contain acted or elicited speech, limiting their applicability in real-world scenarios. In this work, we used the Emotional Voice Messages (EMOVOME) database, including spontaneous voice messages from conversations of 100 Spanish speakers on a messaging app, labeled in continuous and discrete emotions by expert and non-expert annotators. We created speaker-independent SER models using the eGeMAPS features, transformer-based models and their combination. We compared the results with reference databases and analyzed the influence of annotators and gender fairness. The pre-trained Unispeech-L model and its combination with eGeMAPS achieved the highest results, with 61.64% and 55.57% Unweighted Accuracy (UA) for 3-class valence and arousal prediction respectively, a 10% improvement over baseline models. For the emotion categories, 42.58% UA was obtained. EMOVOME performed low
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MedGENIE&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#30340;&#31532;&#19968;&#20010;&#29983;&#25104;&#21518;&#38405;&#35835;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.01924</link><description>&lt;p&gt;
&#29983;&#25104;&#36824;&#26159;&#26816;&#32034;&#65311;&#20851;&#20110;&#20154;&#24037;&#29615;&#22659;&#22312;&#21307;&#23398;&#24320;&#25918;&#22495;&#38382;&#31572;&#25928;&#26524;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MedGENIE&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#30340;&#31532;&#19968;&#20010;&#29983;&#25104;&#21518;&#38405;&#35835;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#25918;&#22495;&#38382;&#31572;&#38656;&#35201;&#22823;&#37327;&#19987;&#19994;&#30693;&#35782;&#30340;&#25903;&#25345;&#12290;&#36817;&#26399;&#30340;&#21162;&#21147;&#33268;&#21147;&#20110;&#23558;&#30693;&#35782;&#19982;&#27169;&#22411;&#21442;&#25968;&#20998;&#31163;&#65292;&#23545;&#25239;&#26550;&#26500;&#35268;&#27169;&#21270;&#65292;&#24182;&#20801;&#35768;&#22312;&#24120;&#35265;&#30340;&#20302;&#36164;&#28304;&#30828;&#20214;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#26816;&#32034;&#28982;&#21518;&#38405;&#35835;&#30340;&#33539;&#24335;&#24050;&#21464;&#24471;&#26222;&#36941;&#65292;&#27169;&#22411;&#39044;&#27979;&#20381;&#36182;&#20110;&#26469;&#33258;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;PubMed&#12289;&#25945;&#31185;&#20070;&#21644;UMLS&#65289;&#30340;&#30456;&#20851;&#30693;&#35782;&#29255;&#27573;&#12290;&#21478;&#19968;&#26465;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#20294;&#30001;&#20110;&#39046;&#22495;&#29305;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#21464;&#24471;&#21487;&#33021;&#30340;&#36335;&#24452;&#26159;&#36890;&#36807;&#25552;&#31034;&#26500;&#24314;&#20154;&#24037;&#29615;&#22659;&#12290;&#22240;&#27492;&#65292;&#8220;&#29983;&#25104;&#36824;&#26159;&#26816;&#32034;&#8221;&#25104;&#20026;&#20102;&#29616;&#20195;&#29256;&#30340;&#21704;&#22982;&#38647;&#29305;&#22256;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MedGENIE&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#29983;&#25104;&#28982;&#21518;&#38405;&#35835;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;MedQA-USMLE&#12289;MedMCQA&#21644;MMLU&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#20174;&#23454;&#36341;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20551;&#35774;&#26368;&#22823;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01924v1 Announce Type: cross  Abstract: Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, "to generate or to retrieve" is the modern equivalent of Hamlet's dilemma. This paper presents MedGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maxim
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.00835</link><description>&lt;p&gt;
CLLMs: &#19968;&#33268;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLLMs: Consistency Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00835
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#35299;&#30721;&#26041;&#27861;&#65292;&#22914;&#38597;&#21487;&#27604;&#35299;&#30721;&#65292;&#26174;&#31034;&#20986;&#26377;&#26395;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#25512;&#26029;&#65292;&#22240;&#20026;&#23427;&#25171;&#30772;&#20102;LLM&#35299;&#30721;&#36807;&#31243;&#30340;&#39034;&#24207;&#24615;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#21487;&#24182;&#34892;&#21270;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#35299;&#30721;&#30456;&#27604;&#65292;&#38597;&#21487;&#27604;&#35299;&#30721;&#24456;&#23569;&#33021;&#22312;&#21333;&#20010;&#22266;&#23450;&#28857;&#36845;&#20195;&#27493;&#39588;&#20013;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#26631;&#35760;&#65292;&#22240;&#27492;&#22312;&#36895;&#24230;&#19978;&#21462;&#24471;&#30340;&#25552;&#21319;&#30456;&#23545;&#36739;&#23567;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#20174;&#20219;&#20309;&#29366;&#24577;&#24555;&#36895;&#25910;&#25947;&#21040;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#30340;&#22266;&#23450;&#28857;&#12290;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#65292;&#20197;&#20415;&#22312;&#20219;&#20309;&#36755;&#20837;&#29366;&#24577;&#19979;&#19968;&#33268;&#22320;&#39044;&#27979;&#22266;&#23450;&#28857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#39046;&#22495;&#29305;&#23450;&#21644;&#24320;&#25918;&#22495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;2.4&#20493;&#21040;3.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00835v1 Announce Type: cross  Abstract: Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\times$ to 3.4$\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#31034;&#65292;&#22312;&#39134;&#34892;&#20013;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#31181;&#26410;&#30693;&#35821;&#35328;&#65292;&#25552;&#20986;DiPMT ++&#26694;&#26550;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20351;LLMs&#36866;&#24212;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#65292;&#24182;&#23454;&#29616;&#20102;&#22766;&#35821;&#21644;&#27721;&#35821;&#20043;&#38388;&#30340;&#32763;&#35793;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#24110;&#21161;&#20154;&#31867;&#32763;&#35793;&#23436;&#20840;&#26410;&#30693;&#35821;&#35328;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.19167</link><description>&lt;p&gt;
&#22312;&#38656;&#35201;&#26102;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#31181;&#26410;&#30693;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Teaching Large Language Models an Unseen Language on the Fly
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19167
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#65292;&#22312;&#39134;&#34892;&#20013;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#31181;&#26410;&#30693;&#35821;&#35328;&#65292;&#25552;&#20986;DiPMT ++&#26694;&#26550;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20351;LLMs&#36866;&#24212;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#65292;&#24182;&#23454;&#29616;&#20102;&#22766;&#35821;&#21644;&#27721;&#35821;&#20043;&#38388;&#30340;&#32763;&#35793;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#24110;&#21161;&#20154;&#31867;&#32763;&#35793;&#23436;&#20840;&#26410;&#30693;&#35821;&#35328;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#35768;&#22810;&#20302;&#36164;&#28304;&#35821;&#35328;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#26041;&#38754;&#65292;&#22312;&#36825;&#20123;&#35821;&#35328;&#20013;&#65292;&#26377;&#25928;&#21442;&#25968;&#26356;&#26032;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#26497;&#23569;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLM&#26159;&#21542;&#21487;&#20197;&#20165;&#36890;&#36807;&#25552;&#31034;&#22312;&#39134;&#34892;&#20013;&#23398;&#20064;&#19968;&#31181;&#26032;&#35821;&#35328;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#22766;&#35821;&#25910;&#38598;&#20102;&#19968;&#20010;&#30740;&#31350;&#22871;&#20214;&#65292;&#36825;&#26159;&#24403;&#21069;&#27809;&#26377;LLMs&#25903;&#25345;&#30340;&#19968;&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DiPMT++&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23558;LLMs&#36866;&#24212;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#12290;&#20351;&#29992;&#19968;&#26412;&#35789;&#20856;&#21644;&#20165;&#26377;5K&#23545;&#24179;&#34892;&#21477;&#23376;&#65292;DiPMT++&#23558;GPT-4&#30340;&#24615;&#33021;&#20174;0&#25552;&#21319;&#21040;16 BLEU&#65292;&#29992;&#20110;&#27721;&#35821;&#21040;&#22766;&#35821;&#30340;&#32763;&#35793;&#65292;&#24182;&#23454;&#29616;&#20102;&#22766;&#35821;&#21040;&#27721;&#35821;&#30340;32 BLEU&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#26694;&#26550;&#22312;&#24110;&#21161;&#20154;&#31867;&#32763;&#35793;&#23436;&#20840;&#26410;&#30693;&#35821;&#35328;&#26041;&#38754;&#30340;&#23454;&#38469;&#29992;&#36884;&#65292;&#36825;&#26377;&#21161;&#20110;&#32500;&#25252;&#35821;&#35328;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19167v1 Announce Type: new  Abstract: Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones where there is minimal training data available for effective parameter updating. We thus investigate whether LLMs can learn a new language on the fly solely through prompting. To study this question, we collect a research suite for Zhuang, a language supported by no LLMs currently. We introduce \textsc{DiPMT++}, a framework for adapting LLMs to unseen languages by in-context learning. Using a dictionary and only 5K parallel sentences, \textsc{DiPMT++} significantly enhances the performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32 BLEU for Zhuang-to-Chinese translation. Furthermore, we demonstrate the practical utility of this framework in aiding humans to translate completely unseen languages, which could contribute to the preservation of linguistic diversity.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;NextLevelBERT&#65292;&#36890;&#36807;&#22312;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#20041;&#34920;&#31034;&#19978;&#36827;&#34892;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#65292;&#26377;&#25928;&#22788;&#29702;&#38271;&#25991;&#26723;&#29992;&#20363;&#65292;&#20855;&#26377;&#36229;&#36234;&#26356;&#22823;&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.17682</link><description>&lt;p&gt;
&#25506;&#31350;&#20351;&#29992;&#26356;&#39640;&#32423;&#21035;&#34920;&#31034;&#30340;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#22312;&#38271;&#25991;&#26723;&#20013;&#30340;&#24212;&#29992; - NextLevelBERT
&lt;/p&gt;
&lt;p&gt;
NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17682
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;NextLevelBERT&#65292;&#36890;&#36807;&#22312;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#20041;&#34920;&#31034;&#19978;&#36827;&#34892;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#65292;&#26377;&#25928;&#22788;&#29702;&#38271;&#25991;&#26723;&#29992;&#20363;&#65292;&#20855;&#26377;&#36229;&#36234;&#26356;&#22823;&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#65288;&#22823;&#22411;&#65289;&#35821;&#35328;&#27169;&#22411;&#22312;&#36807;&#21435;&#20960;&#24180;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#22522;&#30784;&#27880;&#24847;&#26426;&#21046;&#30340;&#20108;&#27425;&#25193;&#23637;&#65292;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#21512;&#29702;&#22788;&#29702;&#21457;&#29616;&#22312;&#20070;&#31821;&#20013;&#30340;&#38271;&#24207;&#21015;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NextLevelBERT&#65292;&#36825;&#26159;&#19968;&#20010;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#19981;&#26159;&#22312;&#26631;&#35760;&#19978;&#25805;&#20316;&#65292;&#32780;&#26159;&#22312;&#25991;&#26412;&#23884;&#20837;&#30340;&#24418;&#24335;&#20013;&#30340;&#26356;&#39640;&#32423;&#21035;&#35821;&#20041;&#34920;&#31034;&#19978;&#25805;&#20316;&#12290;&#25105;&#20204;&#23545;NextLevelBERT&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#39044;&#27979;&#25972;&#20010;&#34987;&#36974;&#34109;&#25991;&#26412;&#22359;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#35780;&#20272;&#20135;&#29983;&#30340;&#25991;&#26723;&#21521;&#37327;&#22312;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#65306;1&#65289;&#36890;&#36807;&#38646;&#26679;&#26412;&#25991;&#20214;&#23884;&#20837;&#36827;&#34892;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65292;2&#65289;&#38271;&#25991;&#26723;&#20998;&#31867;&#65292;3&#65289;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19979;&#19968;&#32423;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22788;&#29702;&#38271;&#25991;&#26723;&#29992;&#20363;&#65292;&#24182;&#19988;&#21482;&#35201;&#25152;&#38656;&#30340;&#32454;&#33410;&#27700;&#24179;&#19981;&#22826;&#39640;&#65292;&#23601;&#21487;&#20197;&#36229;&#36234;&#26356;&#22823;&#30340;&#23884;&#20837;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#27169;&#22411;&#21644;&#20195;&#30721; avai
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17682v1 Announce Type: new  Abstract: While (large) language models have significantly improved over the last years, they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism. To address this, we propose NextLevelBERT, a Masked Language Model operating not on tokens, but on higher-level semantic representations in the form of text embeddings. We pretrain NextLevelBERT to predict the vector representation of entire masked text chunks and evaluate the effectiveness of the resulting document vectors on three task types: 1) Semantic Textual Similarity via zero-shot document embeddings, 2) Long document classification, 3) Multiple-choice question answering. We find that next level Masked Language Modeling is an effective technique to tackle long-document use cases and can outperform much larger embedding models as long as the required level of detail is not too high. We make model and code avai
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;Emotional Voice Messages (EMOVOME)&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35828;&#35805;&#32773;&#30340;999&#26465;&#33258;&#21457;&#35821;&#38899;&#28040;&#24687;&#65292;&#36890;&#36807;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#30340;&#26631;&#35760;&#23454;&#29616;&#20102;&#22312;valence&#21644;arousal&#32500;&#24230;&#19978;&#30340;&#24773;&#24863;&#35782;&#21035;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#35821;&#38899;&#21644;&#25991;&#26412;&#36716;&#24405;&#23454;&#29616;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.17496</link><description>&lt;p&gt;
Emotional Voice Messages (EMOVOME)&#25968;&#25454;&#24211;&#65306;&#33258;&#21457;&#24773;&#24863;&#35821;&#38899;&#28040;&#24687;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17496
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;Emotional Voice Messages (EMOVOME)&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35828;&#35805;&#32773;&#30340;999&#26465;&#33258;&#21457;&#35821;&#38899;&#28040;&#24687;&#65292;&#36890;&#36807;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#30340;&#26631;&#35760;&#23454;&#29616;&#20102;&#22312;valence&#21644;arousal&#32500;&#24230;&#19978;&#30340;&#24773;&#24863;&#35782;&#21035;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#35821;&#38899;&#21644;&#25991;&#26412;&#36716;&#24405;&#23454;&#29616;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Emotional Voice Messages (EMOVOME)&#26159;&#19968;&#20010;&#33258;&#21457;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35828;&#35805;&#32773;&#12289;&#30007;&#22899;&#24615;&#24179;&#34913;&#30340;999&#26465;&#30495;&#23454;&#20250;&#35805;&#20013;&#30340;&#38899;&#39057;&#28040;&#24687;&#65292;&#36825;&#20123;&#28040;&#24687;&#36890;&#36807;&#19968;&#20010;&#28040;&#24687;&#24212;&#29992;&#31243;&#24207;&#20135;&#29983;&#65292;&#22312;&#21442;&#19982;&#32773;&#34987;&#25307;&#21215;&#20043;&#21069;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#21046;&#20316;&#65292;&#36991;&#20813;&#20102;&#30001;&#20110;&#23454;&#39564;&#23460;&#29615;&#22659;&#32780;&#20135;&#29983;&#30340;&#20219;&#20309;&#24847;&#35782;&#20559;&#35265;&#12290;&#38899;&#39057;&#25353;&#29031;&#19977;&#20010;&#38750;&#19987;&#23478;&#21644;&#20004;&#20010;&#19987;&#23478;&#30340;&#35748;&#21487;&#22312;valence&#21644;arousal&#32500;&#24230;&#19978;&#36827;&#34892;&#20102;&#26631;&#35760;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#32467;&#21512;&#20197;&#33719;&#24471;&#27599;&#20010;&#32500;&#24230;&#30340;&#26368;&#32456;&#26631;&#31614;&#12290;&#19987;&#23478;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#20110;&#19971;&#31181;&#24773;&#24863;&#31867;&#21035;&#30340;&#39069;&#22806;&#26631;&#31614;&#12290;&#20026;&#20102;&#20026;&#23558;&#26469;&#20351;&#29992;EMOVOME&#36827;&#34892;&#35843;&#26597;&#35774;&#23450;&#22522;&#20934;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#35821;&#38899;&#21644;&#38899;&#39057;&#36716;&#24405;&#26469;&#23454;&#29616;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#12290;&#23545;&#20110;&#35821;&#38899;&#37096;&#20998;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26631;&#20934;&#30340;eGeMAPS&#29305;&#24449;&#38598;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#20998;&#21035;&#33719;&#24471;&#20102;49.27%&#21644;44.71%&#30340;valence&#21644;arousal&#26410;&#21152;&#26435;&#20934;&#30830;&#24230;&#12290;&#23545;&#20110;&#25991;&#26412;&#37096;&#20998;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#22810;&#35821;&#35328;BERT&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#23454;&#29616;&#20102;61%&#30340;&#24773;&#24863;&#35782;&#21035;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17496v1 Announce Type: cross  Abstract: Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced. Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment. Audios were labeled in valence and arousal dimensions by three non-experts and two experts, which were then combined to obtain a final label per dimension. The experts also provided an extra label corresponding to seven emotion categories. To set a baseline for future investigations using EMOVOME, we implemented emotion recognition models using both speech and audio transcriptions. For speech, we used the standard eGeMAPS feature set and support vector machines, obtaining 49.27% and 44.71% unweighted accuracy for valence and arousal respectively. For text, we fine-tuned a multilingual BERT model and achieved 61
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25925;&#20107;&#35762;&#36848;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27861;&#24459;&#25945;&#32946;&#26041;&#27861;&#65292;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#22763;&#23398;&#20064;&#22797;&#26434;&#30340;&#27861;&#24459;&#27010;&#24565;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#27861;&#24459;&#25925;&#20107;&#21644;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17019</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35762;&#25925;&#20107;&#23398;&#20064;&#22797;&#26434;&#27861;&#24459;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17019
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25925;&#20107;&#35762;&#36848;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27861;&#24459;&#25945;&#32946;&#26041;&#27861;&#65292;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#22763;&#23398;&#20064;&#22797;&#26434;&#30340;&#27861;&#24459;&#27010;&#24565;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#27861;&#24459;&#25925;&#20107;&#21644;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27861;&#24459;&#30693;&#35782;&#21464;&#24471;&#26356;&#23481;&#26131;&#29702;&#35299;&#23545;&#20110;&#25552;&#21319;&#26222;&#36890;&#27861;&#24459;&#32032;&#20859;&#21644;&#40723;&#21169;&#20844;&#27665;&#21442;&#19982;&#27665;&#20027;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#27809;&#26377;&#27861;&#24459;&#32972;&#26223;&#30340;&#20154;&#26469;&#35828;&#65292;&#27861;&#24459;&#25991;&#20214;&#36890;&#24120;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27861;&#24459;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#65292;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#22763;&#36890;&#36807;&#35762;&#25925;&#20107;&#23398;&#20064;&#22797;&#26434;&#30340;&#27861;&#24459;&#27010;&#24565;&#65292;&#35762;&#25925;&#20107;&#26159;&#20256;&#36798;&#22797;&#26434;&#21644;&#25277;&#35937;&#27010;&#24565;&#30340;&#26377;&#25928;&#25945;&#23398;&#24037;&#20855;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;LegalStories&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;295&#20010;&#22797;&#26434;&#30340;&#27861;&#24459;&#21407;&#21017;&#65292;&#27599;&#20010;&#21407;&#21017;&#37117;&#38468;&#26377;&#19968;&#20010;&#25925;&#20107;&#21644;&#19968;&#32452;&#30001;LLMs&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#20026;&#20102;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#21508;&#31181;LLMs&#29983;&#25104;&#35299;&#37322;&#36825;&#20123;&#27010;&#24565;&#30340;&#27861;&#24459;&#25925;&#20107;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#23478;&#21442;&#19982;&#30340;&#26041;&#27861;&#26469;&#36845;&#20195;&#35774;&#35745;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17019v1 Announce Type: new  Abstract: Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 295 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop method to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through an 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;EHRNoteQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20855;&#26377;&#37319;&#29992;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#26684;&#24335;&#21644;&#38656;&#35201;&#20998;&#26512;&#22810;&#31687;&#20020;&#24202;&#31508;&#35760;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16040</link><description>&lt;p&gt;
EHRNoteQA&#65306;&#29992;&#20110;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16040
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;EHRNoteQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20855;&#26377;&#37319;&#29992;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#26684;&#24335;&#21644;&#38656;&#35201;&#20998;&#26512;&#22810;&#31687;&#20020;&#24202;&#31508;&#35760;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;EHRNoteQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#22312;MIMIC-IV&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#30001;&#19977;&#20301;&#21307;&#30103;&#19987;&#23478;&#22242;&#38431;&#31934;&#24515;&#31574;&#21010;&#20102;&#21253;&#21547;962&#20010;&#29420;&#29305;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#19982;&#29305;&#23450;&#24739;&#32773;&#30340;EHR&#20020;&#24202;&#31508;&#35760;&#30456;&#20851;&#32852;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110;EHR&#30340;&#22522;&#20934;&#19981;&#21516;&#30340;&#26159;&#65306;&#39318;&#20808;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#26684;&#24335;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#22312;&#33258;&#21160;&#35780;&#20272;&#30340;&#32972;&#26223;&#19979;&#26377;&#25928;&#35780;&#20272;LLMs&#30340;&#24471;&#20998;&#24615;&#33021;&#65292;&#19982;&#20854;&#20182;&#26684;&#24335;&#30456;&#27604;&#12290;&#20854;&#27425;&#65292;&#23427;&#38656;&#35201;&#20998;&#26512;&#22810;&#31687;&#20020;&#24202;&#31508;&#35760;&#25165;&#33021;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65292;&#21453;&#26144;&#20102;&#23454;&#38469;&#20020;&#24202;&#20915;&#31574;&#21046;&#23450;&#30340;&#22797;&#26434;&#24615;&#65292;&#21307;&#29983;&#38656;&#35201;&#23457;&#26597;&#22823;&#37327;&#24739;&#32773;&#30149;&#21490;&#35760;&#24405;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16040v1 Announce Type: new  Abstract: This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three medical professionals has curated the dataset comprising 962 unique questions, each linked to a specific patient's EHR clinical notes. What makes EHRNoteQA distinct from existing EHR-based benchmarks is as follows: Firstly, it is the first dataset to adopt a multi-choice question answering format, a design choice that effectively evaluates LLMs with reliable scores in the context of automatic evaluation, compared to other formats. Secondly, it requires an analysis of multiple clinical notes to answer a single question, reflecting the complex nature of real-world clinical decision-making where clinicians review extensive records of patient histories. Our comprehensive evaluation on various large language models
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ReCoVERR&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#35270;&#35273;-&#35821;&#35328;&#31995;&#32479;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#24230;&#25918;&#24323;&#65292;&#36890;&#36807;&#23547;&#25214;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#32447;&#32034;&#25552;&#20379;&#39069;&#22806;&#35777;&#25454;&#26469;&#21462;&#20195;&#25918;&#24323;&#65292;&#20174;&#32780;&#19981;&#38477;&#20302;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15610</link><description>&lt;p&gt;
&#36873;&#25321;&#8220;&#36873;&#25321;&#24615;&#39044;&#27979;&#8221;&#65306;&#20943;&#23569;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20013;&#19981;&#24517;&#35201;&#30340;&#24323;&#26435;
&lt;/p&gt;
&lt;p&gt;
Selective "Selective Prediction": Reducing Unnecessary Abstention in Vision-Language Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15610
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ReCoVERR&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#35270;&#35273;-&#35821;&#35328;&#31995;&#32479;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#24230;&#25918;&#24323;&#65292;&#36890;&#36807;&#23547;&#25214;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#32447;&#32034;&#25552;&#20379;&#39069;&#22806;&#35777;&#25454;&#26469;&#21462;&#20195;&#25918;&#24323;&#65292;&#20174;&#32780;&#19981;&#38477;&#20302;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#36873;&#25321;&#24615;&#39044;&#27979;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#20801;&#35768;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#19981;&#30830;&#23450;&#26102;&#25918;&#24323;&#22238;&#31572;&#65292;&#20197;&#26368;&#23567;&#21270;&#38169;&#35823;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#24403;&#37096;&#32626;&#19968;&#20010;&#23545;&#19981;&#20934;&#30830;&#39044;&#27979;&#23481;&#24525;&#24230;&#20302;&#30340;&#35270;&#35273;-&#35821;&#35328;&#31995;&#32479;&#26102;&#65292;&#36873;&#25321;&#24615;&#39044;&#27979;&#21487;&#33021;&#36807;&#20110;&#35880;&#24910;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#27491;&#30830;&#39044;&#27979;&#19978;&#36807;&#20110;&#25918;&#24323;&#12290;&#25105;&#20204;&#24341;&#20837;ReCoVERR&#65292;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#36873;&#25321;&#24615;&#35270;&#35273;-&#35821;&#35328;&#31995;&#32479;&#36807;&#24230;&#25918;&#24323;&#30340;&#25512;&#29702;&#26102;&#38388;&#31639;&#27861;&#65292;&#32780;&#19981;&#38477;&#20302;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#24403;VLM&#20570;&#20986;&#20302;&#32622;&#20449;&#24230;&#39044;&#27979;&#26102;&#65292;ReCoVERR&#23581;&#35797;&#22312;&#22270;&#20687;&#20013;&#25214;&#21040;&#25552;&#20379;&#39069;&#22806;&#35777;&#25454;&#30340;&#30456;&#20851;&#32447;&#32034;&#65292;&#32780;&#19981;&#26159;&#25918;&#24323;&#12290;ReCoVERR&#20351;&#29992;LLM&#21521;VLM&#25552;&#20986;&#30456;&#20851;&#38382;&#39064;&#65292;&#25910;&#38598;&#39640;&#32622;&#20449;&#24230;&#35777;&#25454;&#65292;&#22914;&#26524;&#36275;&#22815;&#30340;&#35777;&#25454;&#30830;&#35748;&#39044;&#27979;&#65292;&#21017;&#31995;&#32479;&#20570;&#20986;&#39044;&#27979;&#32780;&#19981;&#26159;&#25918;&#24323;&#12290;ReCoVERR&#20351;&#20004;&#20010;VLM&#65292;BLIP2&#21644;InstructBLIP&#65292;&#33021;&#22815;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15610v1 Announce Type: new  Abstract: Prior work on selective prediction minimizes incorrect predictions from vision-language models (VLMs) by allowing them to abstain from answering when uncertain. However, when deploying a vision-language system with low tolerance for inaccurate predictions, selective prediction may be over-cautious and abstain too frequently, even on many correct predictions. We introduce ReCoVERR, an inference-time algorithm to reduce the over-abstention of a selective vision-language system without decreasing prediction accuracy. When the VLM makes a low-confidence prediction, instead of abstaining ReCoVERR tries to find relevant clues in the image that provide additional evidence for the prediction. ReCoVERR uses an LLM to pose related questions to the VLM, collects high-confidence evidences, and if enough evidence confirms the prediction the system makes a prediction instead of abstaining. ReCoVERR enables two VLMs, BLIP2 and InstructBLIP, to answer u
&lt;/p&gt;</description></item><item><title>FinTral&#26159;&#19968;&#31867;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#30340;GPT-4&#32423;&#21035;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#26816;&#32034;&#26041;&#27861;&#20248;&#21270;&#65292;&#22312;AI&#39537;&#21160;&#37329;&#34701;&#25216;&#26415;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.10986</link><description>&lt;p&gt;
FinTral&#65306;&#19968;&#31867;GPT-4&#32423;&#21035;&#30340;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10986
&lt;/p&gt;
&lt;p&gt;
FinTral&#26159;&#19968;&#31867;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#30340;GPT-4&#32423;&#21035;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#26816;&#32034;&#26041;&#27861;&#20248;&#21270;&#65292;&#22312;AI&#39537;&#21160;&#37329;&#34701;&#25216;&#26415;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;FinTral&#65292;&#36825;&#26159;&#19968;&#32452;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#26500;&#24314;&#30340;&#19968;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#19987;&#38376;&#20026;&#37329;&#34701;&#20998;&#26512;&#23450;&#21046;&#12290;FinTral&#25972;&#21512;&#20102;&#25991;&#26412;&#12289;&#25968;&#23383;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20026;&#26412;&#30740;&#31350;&#31574;&#21010;&#30340;&#22823;&#37327;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#12289;&#25351;&#23548;&#24494;&#35843;&#21644;RLAIF&#35757;&#32451;&#22686;&#24378;&#20102;FinTral&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20061;&#20010;&#20219;&#21153;&#21644;25&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#25324;&#37329;&#34701;&#39046;&#22495;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;FinTral&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#20808;&#36827;&#30340;&#24037;&#20855;&#21644;&#26816;&#32034;&#26041;&#27861;&#36827;&#34892;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#35757;&#32451;&#65292;&#21629;&#21517;&#20026;FinTral-DPO-T&amp;R&#65292;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#23427;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;ChatGPT-3.5&#65292;&#24182;&#22312;&#20061;&#39033;&#20219;&#21153;&#20013;&#30340;&#20116;&#39033;&#20013;&#36229;&#36234;GPT-4&#65292;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#37329;&#34701;&#25216;&#26415;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;FinTral&#20855;&#26377;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10986v1 Announce Type: cross  Abstract: We introduce FinTral, a suite of state-of-the-art multimodal large language models (LLMs) built upon the Mistral-7b model and tailored for financial analysis. FinTral integrates textual, numerical, tabular, and image data. We enhance FinTral with domain-specific pretraining, instruction fine-tuning, and RLAIF training by exploiting a large collection of textual and visual datasets we curate for this work. We also introduce an extensive benchmark featuring nine tasks and 25 datasets for evaluation, including hallucinations in the financial domain. Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&amp;R, demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5 in all tasks and surpasses GPT-4 in five out of nine tasks, marking a significant advancement in AI-driven financial technology. We also demonstrate that FinTral has the potential to e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.05889</link><description>&lt;p&gt;
CREMA: &#36890;&#36807;&#26377;&#25928;&#30340;&#27169;&#22359;&#21270;&#36866;&#24212;&#21644;&#34701;&#21512;&#36827;&#34892;&#22810;&#27169;&#24577;&#32452;&#21512;&#35270;&#39057;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#22788;&#29702;&#22266;&#23450;&#27169;&#24577;&#36755;&#20837;&#24182;&#26356;&#26032;&#35768;&#22810;&#27169;&#22411;&#21442;&#25968;&#65292;&#20173;&#28982;&#23384;&#22312;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;CREMA&#65292;&#19968;&#31181;&#29992;&#20110;&#23558;&#20219;&#20309;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65288;&#22914;&#20809;&#27969;&#12289;3D&#28857;&#20113;&#12289;&#38899;&#39057;&#65289;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26597;&#35810;&#36716;&#25442;&#22120;&#65292;&#35813;&#36716;&#25442;&#22120;&#19982;&#27599;&#20010;&#21487;&#20197;&#35775;&#38382;&#30340;&#27169;&#24577;&#30456;&#20851;&#32852;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22359;&#12290;&#23427;&#23558;&#22810;&#31181;&#27169;&#24577;&#29305;&#24449;&#25237;&#24433;&#21040;LLM&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#25968;&#25454;&#31867;&#22411;&#20197;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#27169;&#22359;&#65292;&#29992;&#20110;&#21387;&#32553;&#22810;&#27169;&#24577;&#26597;&#35810;&#65292;&#22312;LLM&#20013;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#36827;&#34892;&#34701;&#21512;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive advancements in multimodal compositional reasoning approaches, they are still limited in their flexibility and efficiency by processing fixed modality inputs while updating a lot of model parameters. This paper tackles these critical challenges and proposes CREMA, an efficient and modular modality-fusion framework for injecting any new modality into video reasoning. We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio) from given videos without extra human annotation by leveraging existing pre-trained models. Next, we introduce a query transformer with multiple parameter-efficient modules associated with each accessible modality. It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation. Furthermore, we propose a fusion module designed to compress multimodal queries, maintaining computational efficiency in the LLM while combining additio
&lt;/p&gt;</description></item><item><title>LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.04333</link><description>&lt;p&gt;
LESS&#65306;&#29992;&#20110;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#30340;&#36873;&#25321;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LESS: Selecting Influential Data for Targeted Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04333
&lt;/p&gt;
&lt;p&gt;
LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20351;&#29992;&#32452;&#21512;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#36890;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#24448;&#24448;&#38656;&#35201;&#19968;&#22871;&#19987;&#38376;&#30340;&#25216;&#33021;&#65288;&#20363;&#22914;&#25512;&#29702;&#65289;&#12290;&#25361;&#25112;&#22312;&#20110;&#20174;&#36825;&#20123;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#20986;&#26368;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#20197;&#26377;&#25928;&#24320;&#21457;&#29305;&#23450;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#24773;&#20917;&#31216;&#20026;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LESS&#65292;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#20272;&#35745;&#25968;&#25454;&#24433;&#21709;&#24182;&#25191;&#34892;&#36866;&#29992;&#20110;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#30340;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#20851;&#38190;&#22312;&#20110;LESS&#23558;&#29616;&#26377;&#30340;&#24433;&#21709;&#20844;&#24335;&#35843;&#25972;&#20026;&#19982;Adam&#20248;&#21270;&#22120;&#21644;&#21487;&#21464;&#38271;&#24230;&#25351;&#20196;&#25968;&#25454;&#19968;&#36215;&#24037;&#20316;&#12290;LESS&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#20302;&#32500;&#26799;&#24230;&#29305;&#24449;&#30340;&#39640;&#24230;&#21487;&#37325;&#29992;&#21644;&#21487;&#20256;&#36882;&#30340;&#26799;&#24230;&#25968;&#25454;&#23384;&#20648;&#24211;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#19982;&#20855;&#26377;&#29305;&#23450;&#33021;&#21147;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#30456;&#20284;&#24230;&#36873;&#25321;&#31034;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;t
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25991;&#26723;&#20998;&#26512;&#21644;&#25991;&#26723;&#22270;&#20687;&#39044;&#27979;&#20013;&#35757;&#32451;&#35821;&#35328;-&#35270;&#35273;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#34920;&#26126;&#20351;&#29992;&#25351;&#20196;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;Polling-based Object Probing Evaluation (POPE)&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#25351;&#20196;&#35843;&#20248;&#24615;&#33021;&#30456;&#23545;&#20110;&#38646;-shot&#24615;&#33021;&#25552;&#39640;&#20102;11&#20493;&#21040;32&#20493;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#38750;&#25351;&#20196;&#24494;&#35843;&#25552;&#39640;&#20102;0.1%&#21040;4.2%&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#22240;&#20026;&#36825;&#20123;&#24615;&#33021;&#20173;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#65288;94.36%&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.00453</link><description>&lt;p&gt;
&#25351;&#20196;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Instruction Makes a Difference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00453
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25991;&#26723;&#20998;&#26512;&#21644;&#25991;&#26723;&#22270;&#20687;&#39044;&#27979;&#20013;&#35757;&#32451;&#35821;&#35328;-&#35270;&#35273;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#34920;&#26126;&#20351;&#29992;&#25351;&#20196;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;Polling-based Object Probing Evaluation (POPE)&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#25351;&#20196;&#35843;&#20248;&#24615;&#33021;&#30456;&#23545;&#20110;&#38646;-shot&#24615;&#33021;&#25552;&#39640;&#20102;11&#20493;&#21040;32&#20493;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#38750;&#25351;&#20196;&#24494;&#35843;&#25552;&#39640;&#20102;0.1%&#21040;4.2%&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#22240;&#20026;&#36825;&#20123;&#24615;&#33021;&#20173;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#65288;94.36%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Instruction Document Visual Question Answering (iDocVQA)&#25968;&#25454;&#38598;&#21644;Large Language Document (LLaDoc)&#27169;&#22411;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;-&#35270;&#35273;&#65288;LV&#65289;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#20998;&#26512;&#21644;&#25991;&#26723;&#22270;&#20687;&#39044;&#27979;&#12290;&#36890;&#24120;&#65292;&#29992;&#20110;DocVQA&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#22312;&#32570;&#20047;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#34920;&#26126;&#20351;&#29992;&#36981;&#24490;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#26032;&#30340;Large Language and Vision Assistant (LLaVA)1.5&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#25991;&#26723;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22522;&#20110;&#25237;&#31080;&#30340;&#23545;&#35937;&#25506;&#27979;&#35780;&#20272;&#65288;POPE&#65289;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#23548;&#20986;&#27169;&#22411;&#30340;&#23545;&#35937;&#24187;&#35273;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#35843;&#20248;&#24615;&#33021;&#30456;&#23545;&#20110;&#38646;-shot&#24615;&#33021;&#25552;&#39640;&#20102;11&#20493;&#21040;32&#20493;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#38750;&#25351;&#20196;&#65288;&#20256;&#32479;&#20219;&#21153;&#65289;&#24494;&#35843;&#25552;&#39640;&#20102;0.1%&#21040;4.2%&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#36798;&#19981;&#21040;&#20154;&#31867;&#24615;&#33021;&#65288;94.36%&#65289;&#65292;&#36825;&#24847;&#21619;&#30528;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Instruction Document Visual Question Answering (iDocVQA) dataset and Large Language Document (LLaDoc) model, for training Language-Vision (LV) models for document analysis and predictions on document images, respectively. Usually, deep neural networks for the DocVQA task are trained on datasets lacking instructions. We show that using instruction-following datasets improves performance. We compare performance across document-related datasets using the recent state-of-the-art (SotA) Large Language and Vision Assistant (LLaVA)1.5 as the base model. We also evaluate the performance of the derived models for object hallucination using the Polling-based Object Probing Evaluation (POPE) dataset. The results show that instruction-tuning performance ranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over non-instruction (traditional task) finetuning. Despite the gains, these still fall short of human performance (94.36%), implying there's much room for improveme
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KBQA&#26550;&#26500;FuSIC-KBQA&#65292;&#36890;&#36807;&#22810;&#20010;&#28304;&#35757;&#32451;&#30340;&#21484;&#22238;&#22120;&#25191;&#34892;KB&#26816;&#32034;&#65292;&#22312;LLM&#30340;&#37325;&#26032;&#25490;&#24207;&#21518;&#20197;&#27492;&#20316;&#20026;LLM&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36755;&#20837;&#26469;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#25191;&#34892;&#24341;&#23548;&#21453;&#39304;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2311.08894</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#30693;&#35782;&#24211;&#38382;&#31572;&#65306;&#34701;&#21512;&#30417;&#30563;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08894
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KBQA&#26550;&#26500;FuSIC-KBQA&#65292;&#36890;&#36807;&#22810;&#20010;&#28304;&#35757;&#32451;&#30340;&#21484;&#22238;&#22120;&#25191;&#34892;KB&#26816;&#32034;&#65292;&#22312;LLM&#30340;&#37325;&#26032;&#25490;&#24207;&#21518;&#20197;&#27492;&#20316;&#20026;LLM&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36755;&#20837;&#26469;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#25191;&#34892;&#24341;&#23548;&#21453;&#39304;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26550;&#26500;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#37096;&#32626;&#26102;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;KBQA&#30340;&#38382;&#39064;&#65292;&#30446;&#26631;&#22495;&#20165;&#25552;&#20379;&#23569;&#37327;&#26631;&#35760;&#31034;&#20363;&#65292;&#20294;&#22312;&#28304;&#22495;&#20013;&#26377;&#22823;&#37327;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuSIC-KBQA&#30340;&#26032;&#22411;KBQA&#26550;&#26500;&#65292;&#23427;&#20351;&#29992;&#22810;&#20010;&#32463;&#36807;&#28304;&#22521;&#35757;&#30340;&#21484;&#22238;&#22120;&#25191;&#34892;KB&#26816;&#32034;&#65292;&#20351;&#29992;LLM&#37325;&#26032;&#25490;&#24207;&#65292;&#23558;&#27492;&#20316;&#20026;LLM&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36755;&#20837;&#20197;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#36827;&#19968;&#27493;&#20351;&#29992;&#25191;&#34892;&#24341;&#23548;&#21453;&#39304;&#36827;&#34892;&#32454;&#21270;&#12290;&#22312;&#22235;&#23545;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#28304;-&#30446;&#26631;KBQA&#23545;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FuSIC-KBQA&#26126;&#26174;&#20248;&#20110;&#20026;&#27492;&#35774;&#32622;&#35843;&#25972;&#30340;SoTA KBQA&#27169;&#22411;&#12290;&#22312;&#39046;&#22495;&#20869;&#35774;&#32622;&#30340;&#39069;&#22806;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#26102;&#65292;FuSIC-KBQA&#20063;&#20248;&#20110;SoTA KBQA&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08894v2 Announce Type: replace-cross  Abstract: Existing Knowledge Base Question Answering (KBQA) architectures are hungry for annotated data, which make them costly and time-consuming to deploy. We introduce the problem of few-shot transfer learning for KBQA, where the target domain offers only a few labeled examples, but a large labeled training dataset is available in a source domain. We propose a novel KBQA architecture called FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers, re-ranks using an LLM and uses this as input for LLM few-shot in-context learning to generate logical forms, which are further refined using execution-guided feedback. Experiments over four source-target KBQA pairs of varying complexity show that FuSIC-KBQA significantly outperforms adaptations of SoTA KBQA models for this setting. Additional experiments in the in-domain setting show that FuSIC-KBQA also outperforms SoTA KBQA models when training data is limited.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#33719;&#24471;&#26032;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#20351;&#29992;GPU&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;DARE&#25216;&#26415;&#26469;&#31232;&#30095;&#21270;&#21442;&#25968;&#24182;&#23558;&#22810;&#20010;&#21516;&#28304;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DARE&#21487;&#20197;&#36731;&#26494;&#21024;&#38500;&#22823;&#37096;&#20998;&#21442;&#25968;&#24182;&#23454;&#29616;&#22810;&#20219;&#21153;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2311.03099</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23601;&#20687;&#36229;&#32423;&#39532;&#37324;&#22885;&#65306;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#23454;&#29616;&#20813;&#36153;&#21320;&#39184;
&lt;/p&gt;
&lt;p&gt;
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#33719;&#24471;&#26032;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#20351;&#29992;GPU&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;DARE&#25216;&#26415;&#26469;&#31232;&#30095;&#21270;&#21442;&#25968;&#24182;&#23558;&#22810;&#20010;&#21516;&#28304;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DARE&#21487;&#20197;&#36731;&#26494;&#21024;&#38500;&#22823;&#37096;&#20998;&#21442;&#25968;&#24182;&#23454;&#29616;&#22810;&#20219;&#21153;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;(LMs)&#21487;&#20197;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#33719;&#24471;&#26032;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#20351;&#29992;GPU&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;DARE&#26469;&#23558;&#22823;&#22810;&#25968;delta&#21442;&#25968;&#65288;&#21363;&#24494;&#35843;&#21644;&#39044;&#35757;&#32451;&#21442;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65289;&#35774;&#32622;&#20026;&#38646;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#30417;&#30563;&#24494;&#35843;(SFT) LMs&#30340;&#33021;&#21147;&#65292;DARE&#36890;&#36807;&#38543;&#26426;&#21024;&#38500;&#27604;&#29575;&#20026;p&#30340;delta&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;1/(1 - p)&#37325;&#26032;&#32553;&#25918;&#21097;&#20313;&#21442;&#25968;&#26469;&#36817;&#20284;&#21407;&#22987;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;DARE&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#21363;&#25554;&#21363;&#29992;&#25216;&#26415;&#26469;&#31232;&#30095;&#21270;&#22810;&#20010;SFT&#21516;&#28304;&#27169;&#22411;&#30340;delta&#21442;&#25968;&#65292;&#20197;&#20943;&#36731;&#21442;&#25968;&#24178;&#25200;&#65292;&#24182;&#36890;&#36807;&#21442;&#25968;&#34701;&#21512;&#23558;&#23427;&#20204;&#21512;&#24182;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20026;&#22522;&#30784;&#30340;LM&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;SFT delta&#21442;&#25968;&#20540;&#33539;&#22260;&#36890;&#24120;&#24456;&#23567;&#65288;&#22312;0.005&#20197;&#20869;&#65289;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#20887;&#20313;&#65292;DARE&#21487;&#20197;&#36731;&#26494;&#21024;&#38500;90%&#29978;&#33267;99%&#30340;&#21442;&#25968;&#12290;&#65288;2&#65289;DARE&#21487;&#20197;&#23558;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;LM&#21512;&#24182;&#20026;&#19968;&#20010;LM&#65292;&#24182;&#26377;&#39550;&#39542;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio p And REscales the remaining ones by 1/(1 - p) to approximate the original embeddings. Then, we use DARE as a versatile plug-and-play technique to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.005) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them. (2) DARE can merge multiple task-specific LMs into one LM with dive
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#32452;&#23450;&#21046;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26088;&#22312;&#21033;&#29992;&#30142;&#30149;&#21517;&#31216;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#30142;&#30149;&#21517;&#31216;&#30340;&#35821;&#20041;&#32454;&#24494;&#24046;&#21035;&#21644;&#20998;&#31867;&#32467;&#26500;&#30340;&#29702;&#35299;</title><link>https://arxiv.org/abs/2306.01931</link><description>&lt;p&gt;
&#25506;&#32034;&#30142;&#30149;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65306;&#29992;&#20110;&#20013;&#25991;&#30142;&#30149;&#35268;&#33539;&#21270;&#30340;&#31616;&#21333;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Exploring semantic information in disease: Simple Data Augmentation Techniques for Chinese Disease Normalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.01931
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#32452;&#23450;&#21046;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26088;&#22312;&#21033;&#29992;&#30142;&#30149;&#21517;&#31216;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#30142;&#30149;&#21517;&#31216;&#30340;&#35821;&#20041;&#32454;&#24494;&#24046;&#21035;&#21644;&#20998;&#31867;&#32467;&#26500;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30142;&#30149;&#21517;&#31216;&#35268;&#33539;&#21270;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#23427;&#23558;&#20197;&#21508;&#31181;&#26684;&#24335;&#32534;&#20889;&#30340;&#30142;&#30149;&#21517;&#31216;&#20998;&#31867;&#20026;&#26631;&#20934;&#21270;&#21517;&#31216;&#65292;&#20316;&#20026;&#26234;&#33021;&#21307;&#30103;&#31995;&#32479;&#20013;&#21508;&#31181;&#19982;&#30142;&#30149;&#30456;&#20851;&#21151;&#33021;&#30340;&#22522;&#26412;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30142;&#30149;&#21517;&#31216;&#35268;&#33539;&#21270;&#31995;&#32479;&#38754;&#20020;&#30340;&#26368;&#22823;&#38556;&#30861;&#26159;&#35757;&#32451;&#25968;&#25454;&#20005;&#37325;&#19981;&#36275;&#12290;&#34429;&#28982;&#25968;&#25454;&#22686;&#24378;&#26159;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36890;&#24120;&#20250;&#38459;&#30861;&#20219;&#21153;&#24615;&#33021;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#30142;&#30149;&#21517;&#31216;&#30340;&#22810;&#36724;&#21644;&#22810;&#31890;&#24230;&#24615;&#36136;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#32452;&#23450;&#21046;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26088;&#22312;&#21033;&#29992;&#30142;&#30149;&#21517;&#31216;&#20013;&#22266;&#26377;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20123;&#25216;&#26415;&#26088;&#22312;&#22686;&#24378;&#27169;&#22411;&#23545;&#30142;&#30149;&#21517;&#31216;&#30340;&#35821;&#20041;&#22797;&#26434;&#24615;&#21644;&#20998;&#31867;&#32467;&#26500;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.01931v2 Announce Type: replace  Abstract: Disease name normalization is an important task in the medical domain. It classifies disease names written in various formats into standardized names, serving as a fundamental component in smart healthcare systems for various disease-related functions. Nevertheless, the most significant obstacle to existing disease name normalization systems is the severe shortage of training data. While data augmentation is a powerful approach for addressing data scarcity, our findings reveal that conventional data augmentation techniques often impede task performance, primarily due to the multi-axis and multi-granularity nature of disease names. Consequently, we introduce a set of customized data augmentation techniques designed to leverage the semantic information inherent in disease names. These techniques aim to enhance the model's understanding of the semantic intricacies and classification structure of disease names. Through extensive experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;UnTrac&#65292;&#36890;&#36807;&#21453;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UnTrac&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.15241</link><description>&lt;p&gt;
&#21453;&#23398;&#20064;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unlearning Reveals the Influential Training Data of Language Models. (arXiv:2401.15241v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;UnTrac&#65292;&#36890;&#36807;&#21453;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UnTrac&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#35782;&#21035;&#21738;&#20123;&#35757;&#32451;&#25968;&#25454;&#38598;&#24433;&#21709;&#27169;&#22411;&#30340;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20174;&#35757;&#32451;&#20013;&#31227;&#38500;&#27599;&#20010;&#25968;&#25454;&#38598;&#26469;&#34913;&#37327;&#20854;&#24433;&#21709;;&#28982;&#32780;&#65292;&#22810;&#27425;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;UnTrac&#65292;&#36890;&#36807;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21462;&#28040;&#23398;&#20064;&#26469;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;UnTrac&#38750;&#24120;&#31616;&#21333;; &#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#26469;&#21462;&#28040;&#23398;&#20064;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#22312;&#21462;&#28040;&#23398;&#20064;&#21518;&#27169;&#22411;&#30340;&#39044;&#27979;&#21457;&#29983;&#20102;&#22810;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21542;&#33021;&#35780;&#20272;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#29983;&#25104;&#26377;&#27602;&#12289;&#26377;&#20559;&#35265;&#21644;&#19981;&#30495;&#23454;&#20869;&#23481;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#20102;&#23427;&#20204;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#36807;&#22810;&#30340;&#20869;&#23384;&#31354;&#38388;&#25110;&#22810;&#20010;&#27169;&#22411;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to enhance the performance of language models while mitigating the risks of generating harmful content, it is crucial to identify which training dataset affects the model's outputs. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac, which estimates the influence of a training dataset by unlearning it from the trained model. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model's predictions change after unlearning. We empirically examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Experimental results demonstrate that our method estimates their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple model checkpoints.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Axis Tour&#65292;&#29992;&#20110;&#30830;&#23450;ICA&#36716;&#25442;&#23884;&#20837;&#20013;&#36724;&#30340;&#39034;&#24207;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#35821;&#20041;&#36830;&#32493;&#24615;&#26469;&#25552;&#39640;&#35789;&#23884;&#20837;&#31354;&#38388;&#30340;&#28165;&#26224;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Axis Tour&#26500;&#24314;&#30340;&#20302;&#32500;&#23884;&#20837;&#27604;PCA&#21644;ICA&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.06112</link><description>&lt;p&gt;
Axis Tour: Word Tour &#30830;&#23450;ICA&#36716;&#25442;&#23884;&#20837;&#20013;&#36724;&#30340;&#39034;&#24207;
&lt;/p&gt;
&lt;p&gt;
Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings. (arXiv:2401.06112v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Axis Tour&#65292;&#29992;&#20110;&#30830;&#23450;ICA&#36716;&#25442;&#23884;&#20837;&#20013;&#36724;&#30340;&#39034;&#24207;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#35821;&#20041;&#36830;&#32493;&#24615;&#26469;&#25552;&#39640;&#35789;&#23884;&#20837;&#31354;&#38388;&#30340;&#28165;&#26224;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Axis Tour&#26500;&#24314;&#30340;&#20302;&#32500;&#23884;&#20837;&#27604;PCA&#21644;ICA&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#23884;&#20837;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#65292;&#20294;&#35299;&#37322;&#39640;&#32500;&#23884;&#20837;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#34987;&#30830;&#23450;&#20026;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;ICA&#36716;&#25442;&#30340;&#35789;&#23884;&#20837;&#25581;&#31034;&#20102;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#36724;&#65292;&#20294;&#36825;&#20123;&#36724;&#30340;&#39034;&#24207;&#26159;&#20219;&#24847;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#20851;&#27880;&#36825;&#20010;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Axis Tour&#65292;&#23427;&#20248;&#21270;&#20102;&#36724;&#30340;&#39034;&#24207;&#12290;&#21463;&#21040;&#19968;&#32500;&#35789;&#23884;&#20837;&#26041;&#27861;Word Tour&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#26368;&#22823;&#21270;&#36724;&#30340;&#35821;&#20041;&#36830;&#32493;&#24615;&#26469;&#25552;&#39640;&#35789;&#23884;&#20837;&#31354;&#38388;&#30340;&#28165;&#26224;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;PCA&#21644;ICA&#30456;&#27604;&#65292;Axis Tour&#26500;&#24314;&#20102;&#26356;&#22909;&#30340;&#20302;&#32500;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word embedding is one of the most important components in natural language processing, but interpreting high-dimensional embeddings remains a challenging problem. To address this problem, Independent Component Analysis (ICA) is identified as an effective solution. ICA-transformed word embeddings reveal interpretable semantic axes; however, the order of these axes are arbitrary. In this study, we focus on this property and propose a novel method, Axis Tour, which optimizes the order of the axes. Inspired by Word Tour, a one-dimensional word embedding method, we aim to improve the clarity of the word embedding space by maximizing the semantic continuity of the axes. Furthermore, we show through experiments on downstream tasks that Axis Tour constructs better low-dimensional embeddings compared to both PCA and ICA.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEFT&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#20102;70%&#30340;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.16776</link><description>&lt;p&gt;
DEFT&#65306;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. (arXiv:2310.16776v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16776
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEFT&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#20102;70%&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#20351;&#24471;&#35768;&#22810;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#20351;&#29992;&#65307;&#28982;&#32780;&#65292;&#19968;&#20010;&#20173;&#28982;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#24494;&#35843;PLMs&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#31350;&#31455;&#38656;&#35201;&#22810;&#23569;&#25968;&#25454;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DEFT&#65292;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;PLMs&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#32534;&#36753;LM&#30340;&#32972;&#26223;&#19979;&#23637;&#31034;&#20102;DEFT&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;CoEDIT&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;CoEDIT&#19968;&#26679;&#65292;&#32780;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#35201;&#23569;&#32422;70%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to minimize the amount of data needed to fine-tune PLMs for downstream tasks. We demonstrate the efficacy of our DEFT framework in the context of text-editing LMs, and compare to the state-of-the art text-editing model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT models are just as accurate as CoEDIT while being finetuned on ~70% less data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;</title><link>http://arxiv.org/abs/2310.07815</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#32034;&#24341;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models As Semantic Indexers. (arXiv:2310.07815v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#26631;&#35782;&#31526;&#65288;ID&#65289;&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#65292;&#26088;&#22312;&#20445;&#30041;&#23545;&#35937;&#65288;&#22914;&#25991;&#26723;&#21644;&#39033;&#65289;&#20869;&#37096;&#30340;&#35821;&#20041;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#27969;&#31243;&#26469;&#23398;&#20064;&#35821;&#20041;ID&#65292;&#39318;&#20808;&#20351;&#29992;&#29616;&#25104;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#33719;&#21462;&#23884;&#20837;&#65292;&#24182;&#26681;&#25454;&#23884;&#20837;&#26469;&#25512;&#23548;ID&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#27493;&#39588;&#37117;&#20250;&#24341;&#20837;&#28508;&#22312;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#19988;&#25991;&#26412;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#20869;&#30340;&#23884;&#20837;&#20998;&#24067;&#36890;&#24120;&#19982;&#35821;&#20041;&#32034;&#24341;&#25152;&#38656;&#30340;&#39044;&#26399;&#20998;&#24067;&#23384;&#22312;&#22266;&#26377;&#30340;&#19981;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#26082;&#33021;&#23398;&#20064;&#25991;&#26723;&#30340;&#35821;&#20041;&#34920;&#31034;&#21448;&#33021;&#21516;&#26102;&#23398;&#20064;&#20854;&#20998;&#23618;&#32467;&#26500;&#30340;&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35821;&#20041;ID&#26159;&#31163;&#25955;&#21644;&#39034;&#24207;&#32467;&#26500;&#30340;&#65292;&#24182;&#19988;&#35821;&#20041;&#30417;&#30563;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LMINDEXER&#65292;&#23427;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. Nevertheless, it is non-trivial to design a method that can learn the document's semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMINDEXER, a self-supervised framework to learn semantic IDs with a generative language model. We tackl
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03309</link><description>&lt;p&gt;
&#31616;&#26126;&#26377;&#24207;&#30340;&#24863;&#30693;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03309
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#28436;&#32462;&#25512;&#29702;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#22797;&#26434;&#30340;&#28436;&#32462;&#38382;&#39064;&#20013;&#20173;&#28982;&#24456;&#38590;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#36825;&#31867;&#38382;&#39064;&#20855;&#26377;&#22823;&#37327;&#21069;&#25552;&#65288;&#21363;&#20107;&#23454;&#25110;&#35268;&#21017;&#65289;&#65292;&#20854;&#20013;&#28041;&#21450;&#23454;&#20307;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#12290;&#19968;&#31181;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#21407;&#22987;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#20197;&#21069;&#21521;&#65288;&#20363;&#22914;&#36873;&#25321;-&#25512;&#29702;&#65289;&#25110;&#21453;&#21521;&#65288;&#20363;&#22914;LAMBADA&#65289;&#26041;&#24335;&#23558;&#22810;&#20010;&#22240;&#26524;&#25512;&#29702;&#27493;&#39588;&#36830;&#25509;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#22823;&#37327;&#30340;&#24635;&#20307;&#38454;&#27573;&#65292;&#23548;&#33268;&#35745;&#31639;&#24320;&#38144;&#22823;&#65292;&#24182;&#19988;&#26377;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#27493;&#39588;&#12290;&#38500;&#20102;&#36880;&#38454;&#27573;&#20998;&#35299;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#20154;&#31867;&#38382;&#39064;&#35299;&#20915;&#30340;&#21478;&#19968;&#20010;&#26041;&#38754;&#33719;&#24471;&#20102;&#21551;&#21457;&#12290;&#20154;&#31867;&#20542;&#21521;&#20110;&#25552;&#28860;&#20986;&#26368;&#30456;&#20851;&#30340;&#20449;&#24687;&#24182;&#26377;&#24207;&#22320;&#32452;&#32455;&#24605;&#32500;&#65288;&#20363;&#22914;&#21019;&#24314;&#24605;&#32500;&#23548;&#22270;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#20182;&#20204;&#23545;&#38382;&#39064;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploiting large language models (LLMs) to tackle deductive reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex deductive problems, characterized by plenty of premises (i.e., facts or rules) entailing intricate relationships among entities and requiring multi-hop reasoning. One intuitive solution is to decompose the original task into smaller sub-tasks, and then chain the multiple casual reasoning steps together in a forward (e.g., Selection-Inference) or backward (e.g., LAMBADA) direction. However, these techniques inevitably necessitate a large number of overall stages, leading to computationally expensive operations and a higher possibility of making misleading steps. In addition to stage-by-stage decomposition, we draw inspiration from another aspect of human problem-solving. Humans tend to distill the most relevant information and organize their thoughts systematically (e.g., creating mind maps), which assists th
&lt;/p&gt;</description></item><item><title>Nugget 2D&#26159;&#19968;&#31181;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#20219;&#21153;&#33021;&#21147;&#30340;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35299;&#30721;&#36807;&#31243;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2310.02409</link><description>&lt;p&gt;
Nugget 2D&#65306;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models. (arXiv:2310.02409v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02409
&lt;/p&gt;
&lt;p&gt;
Nugget 2D&#26159;&#19968;&#31181;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#20219;&#21153;&#33021;&#21147;&#30340;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35299;&#30721;&#36807;&#31243;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#20013;&#32553;&#25918;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23558;Qin&#65286;Van Durme&#65288;2023&#24180;&#65289;&#30340;Nugget&#26041;&#27861;&#20174;BERT&#31867;&#26694;&#26550;&#25193;&#23637;&#21040;&#20165;&#35299;&#30721;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21382;&#21490;&#24314;&#27169;&#20026;&#21387;&#32553;&#30340;&#8220;nuggets&#8221;&#65292;&#36825;&#20123;&#8220;nuggets&#8221;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#36827;&#34892;&#37325;&#24314;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#35832;&#22914;LLaMA&#20043;&#31867;&#30340;&#29616;&#25104;&#27169;&#22411;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#12289;&#38382;&#31572;&#21644;&#25688;&#35201;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;Nugget2D&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#20445;&#30041;&#20102;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#22823;&#24133;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#21160;&#32534;&#30721;&#23454;&#39564;&#20013;&#65292;Nugget2D&#21487;&#20197;&#20197;20&#20493;&#30340;&#21387;&#32553;&#27604;&#25910;&#32553;&#19978;&#19979;&#25991;&#65292;&#37325;&#24314;&#26102;&#30340;BLEU&#24471;&#20998;&#20026;98&#65285;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#26080;&#25439;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard Transformer-based language models (LMs) scale poorly to long contexts. We propose a solution based on dynamic contextual compression, which extends the Nugget approach of Qin &amp; Van Durme (2023) from BERT-like frameworks to decoder-only LMs. Our method models history as compressed "nuggets" which are trained to allow for reconstruction, and it can be initialized with off-the-shelf models such as LLaMA. We demonstrate through experiments in language modeling, question answering, and summarization that Nugget2D retains capabilities in these tasks, while drastically reducing the overhead during decoding in terms of time and space. For example, in the experiments of autoencoding, Nugget2D can shrink context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#35299;&#20915;&#25163;&#35821;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#23545;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#22312;&#22810;&#35821;&#31181;&#25163;&#25351;&#25340;&#20889;&#35821;&#26009;&#24211;&#19978;&#26377;&#30410;&#22788;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#35270;&#35273;&#19978;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12443</link><description>&lt;p&gt;
&#22810;&#35821;&#31181;&#25163;&#25351;&#25340;&#20889;&#35821;&#26009;&#24211;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Multilingual Fingerspelling Corpora. (arXiv:2309.12443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#35299;&#20915;&#25163;&#35821;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#23545;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#22312;&#22810;&#35821;&#31181;&#25163;&#25351;&#25340;&#20889;&#35821;&#26009;&#24211;&#19978;&#26377;&#30410;&#22788;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#35270;&#35273;&#19978;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#35299;&#20915;&#25163;&#35821;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#20998;&#26512;&#12290;&#30001;&#20110;&#35768;&#22810;&#25163;&#35821;&#26159;&#27861;&#22269;&#25163;&#35821;&#30340;&#35821;&#35328;&#21518;&#35028;&#65292;&#23427;&#20204;&#20849;&#20139;&#25163;&#21183;&#37197;&#32622;&#65292;&#39044;&#35757;&#32451;&#26377;&#26395;&#21033;&#29992;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#32654;&#22269;&#12289;&#20013;&#22269;&#12289;&#24503;&#22269;&#21644;&#29233;&#23572;&#20848;&#30340;&#25163;&#25351;&#25340;&#20889;&#35821;&#26009;&#24211;&#19978;&#27979;&#35797;&#20102;&#36825;&#19968;&#20551;&#35774;&#12290;&#25105;&#20204;&#30830;&#23454;&#35266;&#23519;&#21040;&#20102;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#65292;&#20294;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#35270;&#35273;&#19978;&#30340;&#30456;&#20284;&#24615;&#32780;&#38750;&#35821;&#35328;&#19978;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply active learning to help with data scarcity problems in sign languages. In particular, we perform a novel analysis of the effect of pre-training. Since many sign languages are linguistic descendants of French sign language, they share hand configurations, which pre-training can hopefully exploit. We test this hypothesis on American, Chinese, German, and Irish fingerspelling corpora. We do observe a benefit from pre-training, but this may be due to visual rather than linguistic similarities
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38598;&#20013;&#22312;&#21457;&#24067;&#19968;&#20010;ASR&#20551;&#35774;&#20462;&#35746;&#65288;HypR&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20960;&#20010;&#24120;&#29992;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#19988;&#20026;ASR&#27169;&#22411;&#30340;&#20462;&#35746;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2309.09838</link><description>&lt;p&gt;
HypR&#65306;&#19968;&#20010;&#20351;&#29992;&#21442;&#32771;&#35821;&#26009;&#24211;&#36827;&#34892;ASR&#20551;&#35774;&#20462;&#35746;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
HypR: A comprehensive study for ASR hypothesis revising with a reference corpus. (arXiv:2309.09838v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38598;&#20013;&#22312;&#21457;&#24067;&#19968;&#20010;ASR&#20551;&#35774;&#20462;&#35746;&#65288;HypR&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20960;&#20010;&#24120;&#29992;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#19988;&#20026;ASR&#27169;&#22411;&#30340;&#20462;&#35746;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20462;&#35746;&#35782;&#21035;&#32467;&#26524;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#39640;&#25928;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#21508;&#31181;&#26041;&#27861;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;N-best&#37325;&#25490;&#24207;&#26041;&#27861;&#21644;&#38169;&#35823;&#20462;&#27491;&#27169;&#22411;&#12290;&#21069;&#32773;&#26088;&#22312;&#20174;&#30001;ASR&#29983;&#25104;&#30340;&#19968;&#32452;&#20505;&#36873;&#20551;&#35774;&#20013;&#36873;&#25321;&#38169;&#35823;&#29575;&#26368;&#20302;&#30340;&#20551;&#35774;&#65292;&#29992;&#20110;&#32473;&#23450;&#30340;&#36755;&#20837;&#35821;&#38899;&#12290;&#21518;&#32773;&#21017;&#19987;&#27880;&#20110;&#26816;&#27979;&#32473;&#23450;&#20551;&#35774;&#20013;&#30340;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#32416;&#27491;&#36825;&#20123;&#38169;&#35823;&#20197;&#33719;&#24471;&#22686;&#24378;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#30740;&#31350;&#24456;&#38590;&#30456;&#20114;&#27604;&#36739;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#22312;&#19981;&#21516;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#19982;&#19981;&#21516;&#30340;ASR&#27169;&#22411;&#37197;&#23545;&#65292;&#24182;&#19988;&#29978;&#33267;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#19987;&#27880;&#20110;&#21457;&#24067;&#19968;&#20010;ASR&#20551;&#35774;&#20462;&#35746;&#65288;HypR&#65289;&#25968;&#25454;&#38598;&#12290;HypR&#21253;&#21547;&#20960;&#20010;&#24120;&#29992;&#30340;&#35821;&#26009;&#24211;&#65288;AISHELL-1&#65292;TED-LIUM 2&#21644;LibriSpeech&#65289;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;ASR&#27169;&#22411;&#30340;&#22522;&#32447;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of deep learning, automatic speech recognition (ASR) has made significant progress. To further enhance the performance, revising recognition results is one of the lightweight but efficient manners. Various methods can be roughly classified into N-best reranking methods and error correction models. The former aims to select the hypothesis with the lowest error rate from a set of candidates generated by ASR for a given input speech. The latter focuses on detecting recognition errors in a given hypothesis and correcting these errors to obtain an enhanced result. However, we observe that these studies are hardly comparable to each other as they are usually evaluated on different corpora, paired with different ASR models, and even use different datasets to train the models. Accordingly, we first concentrate on releasing an ASR hypothesis revising (HypR) dataset in this study. HypR contains several commonly used corpora (AISHELL-1, TED-LIUM 2, and LibriSpeech) and provid
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#26500;&#24314;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20811;&#26381;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00237</link><description>&lt;p&gt;
&#22522;&#20110;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#30340;&#20844;&#24320;&#21487;&#20849;&#20139;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes. (arXiv:2309.00237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00237
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#26500;&#24314;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20811;&#26381;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21512;&#25104;&#30340;&#20020;&#24202;&#26696;&#20363;&#25253;&#21578;&#65292;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#65292;&#20197;&#35299;&#20915;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#21512;&#25104;&#35760;&#24405;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#19987;&#38376;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;Asclepius&#12290;&#34429;&#28982;Asclepius&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#20020;&#24202;&#35760;&#24405;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;Asclepius&#19982;&#21253;&#25324;GPT-3.5-turbo&#21644;&#20854;&#20182;&#24320;&#28304;&#26367;&#20195;&#26041;&#26696;&#22312;&#20869;&#30340;&#20960;&#31181;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#23558;Asclepius&#19982;&#20854;&#22312;&#30495;&#23454;&#20020;&#24202;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21464;&#20307;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21147;&#22320;&#35777;&#26126;&#65292;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#22312;&#26500;&#24314;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#21487;&#20197;&#20316;&#20026;&#21487;&#34892;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models tailored for handling patients' clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations. To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature. We then use these synthetic notes to train our specialized clinical large language model, Asclepius. While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes. We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#21482;&#26377;&#24102;&#26631;&#31614;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#21644;&#33258;&#28982;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#19979;&#30028;&#21644;&#19968;&#20010;&#19978;&#30028;&#26469;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.04539</link><description>&lt;p&gt;
&#26080;&#26631;&#35760;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#20445;&#35777;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications. (arXiv:2306.04539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#21482;&#26377;&#24102;&#26631;&#31614;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#21644;&#33258;&#28982;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#19979;&#30028;&#21644;&#19968;&#20010;&#19978;&#30028;&#26469;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20849;&#21516;&#23398;&#20064;&#22810;&#20010;&#27169;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#19968;&#20010;&#26680;&#24515;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#29702;&#35299;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26412;&#36136;&#65306;&#22312;&#20174;&#20004;&#20010;&#37117;&#27809;&#26377;&#30340;&#27169;&#24577;&#23398;&#20064;&#26102;&#20986;&#29616;&#20102;&#26032;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#21322;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#36825;&#19968;&#20132;&#20114;&#37327;&#21270;&#30340;&#25361;&#25112;&#65292;&#21482;&#20351;&#29992;&#24102;&#26631;&#31614;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#21644;&#33258;&#28982;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#26080;&#26631;&#31614;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#65292;&#35270;&#39057;&#21644;&#30456;&#24212;&#30340;&#38899;&#39057;&#65289;&#12290;&#21033;&#29992;&#31934;&#30830;&#30340;&#20449;&#24687;&#35770;&#20132;&#20114;&#23450;&#20041;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25512;&#23548;&#19979;&#30028;&#21644;&#19978;&#30028;&#65292;&#37327;&#21270;&#36825;&#31181;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#37327;&#21644;&#21333;&#29420;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#30340;&#20004;&#20010;&#19979;&#30028;&#65292;&#24182;&#36890;&#36807;&#36830;&#25509;&#21040;&#36817;&#20284;&#31639;&#27861;&#26469;&#25512;&#23548;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MixAlign&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#65292;&#23454;&#29616;&#33258;&#21160;&#30340;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22240;&#26080;&#27861;&#27491;&#30830;&#29702;&#35299;&#38382;&#39064;&#21644;&#30693;&#35782;&#32780;&#23548;&#33268;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13669</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#24335;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment. (arXiv:2305.13669v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MixAlign&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#65292;&#23454;&#29616;&#33258;&#21160;&#30340;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22240;&#26080;&#27861;&#27491;&#30830;&#29702;&#35299;&#38382;&#39064;&#21644;&#30693;&#35782;&#32780;&#23548;&#33268;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#36817;&#26399;&#36827;&#23637;&#26174;&#33879;&#65292;&#20294;&#20173;&#38754;&#20020;&#24187;&#35273;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#29983;&#25104;&#35823;&#23548;&#24615;&#21644;&#19981;&#25903;&#25345;&#30340;&#22238;&#31572;&#12290;&#19968;&#31181;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20174;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#21644;&#25972;&#21512;&#25903;&#25345;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#30340;&#38382;&#39064;&#36890;&#24120;&#19982;&#23384;&#20648;&#30340;&#30693;&#35782;&#19981;&#22826;&#23545;&#40784;&#65292;&#22240;&#20026;&#20182;&#20204;&#22312;&#25552;&#38382;&#21069;&#19981;&#30693;&#36947;&#21487;&#29992;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#19981;&#23545;&#40784;&#21487;&#33021;&#38480;&#21046;&#35821;&#35328;&#27169;&#22411;&#23450;&#20301;&#21644;&#21033;&#29992;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21487;&#33021;&#36843;&#20351;&#20854;&#36890;&#36807;&#24573;&#30053;&#25110;&#35206;&#30422;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#32780;&#20135;&#29983;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MixAlign&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#20197;&#33719;&#24471;&#24182;&#25972;&#21512;&#20851;&#20110;&#29992;&#25143;&#38382;&#39064;&#19982;&#23384;&#20648;&#20449;&#24687;&#30456;&#20851;&#24615;&#30340;&#28548;&#28165;&#20449;&#24687;&#12290; MixAlign &#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#24182;&#22312;&#38656;&#35201;&#26102;&#36890;&#36807;&#20154;&#24037;&#29992;&#25143;&#28548;&#28165;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#31181;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable recent advances in language models, they still struggle with the hallucination problem and can generate misleading and unsupported responses. A common approach to mitigate the hallucination issue is retrieving and incorporating supporting evidence from a knowledge base. However, user questions usually do not align well with the stored knowledge, as they are unaware of the information available before asking questions. This misalignment can limit the language model's ability to locate and utilize the knowledge, potentially forcing it to hallucinate by ignoring or overriding the retrieved evidence. To address this issue, we introduce MixAlign, a framework that interacts with both the user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic question-knowledge alignment and, if necessary, further enhances this alignment through human user clari
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;n-best&#37325;&#25490;&#24207;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#25552;&#20379;&#20266;&#26631;&#31614;&#65292;&#35757;&#32451;&#20986;&#21442;&#25968;&#26356;&#23569;&#20294;&#31934;&#24230;&#30456;&#24403;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12057</link><description>&lt;p&gt;
&#22522;&#20110;n-best&#37325;&#25490;&#24207;&#30340;&#31934;&#20934;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Accurate Knowledge Distillation with n-best Reranking. (arXiv:2305.12057v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12057
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;n-best&#37325;&#25490;&#24207;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#25552;&#20379;&#20266;&#26631;&#31614;&#65292;&#35757;&#32451;&#20986;&#21442;&#25968;&#26356;&#23569;&#20294;&#31934;&#24230;&#30456;&#24403;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;n-best&#37325;&#25490;&#24207;&#30340;&#24207;&#21015;&#32423;&#21035;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#25945;&#24072;&#27169;&#22411;&#30340;top-1&#20551;&#35774;&#20197;&#21450;top n-best&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21253;&#25324;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20869;&#30340;&#22810;&#31181;&#27169;&#22411;&#65292;&#20026;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;WMT21&#24503;&#33521;&#32763;&#35793;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#20855;&#26377;&#20004;&#20010;&#25968;&#37327;&#32423;&#36739;&#23569;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#19982;Tran&#31561;&#20154;&#65288;2021&#24180;&#65289;&#30340;&#21253;&#21547;47&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#32763;&#35793;&#27169;&#22411;&#30456;&#24403;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose extending the Sequence-level Knowledge Distillation (Kim and Rush, 2016) with n-best reranking to consider not only the top-1 hypotheses but also the top n-best hypotheses of teacher models. Our approach leverages a diverse set of models, including publicly-available large pretrained models, to provide more accurate pseudo-labels for training student models. We validate our proposal on the WMT21 German-English translation task and demonstrate that our student model achieves comparable accuracy to a large translation model with 4.7 billion parameters from (Tran et al., 2021) while having two orders of magnitude fewer parameters.
&lt;/p&gt;</description></item></channel></rss>