<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;boosting&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00723</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#19978;&#19979;&#25991;&#20559;&#20506;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Contextual Biasing of Named-Entities with Large Language Models. (arXiv:2309.00723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;boosting&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#65292;&#21363;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#20026;LLM&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#25171;&#20998;&#26399;&#38388;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;LLM&#36827;&#34892;boosting&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20123;&#25552;&#31034;&#20449;&#24687;&#21253;&#25324;&#20559;&#20506;&#21015;&#34920;&#21644;&#23569;&#26679;&#26412;&#31034;&#20363;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#20551;&#35774;&#24471;&#20998;&#26102;&#20316;&#20026;&#38468;&#21152;&#20449;&#24687;&#12290;&#38500;&#20102;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;LLM&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#20026;&#20102;&#25552;&#39640;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#25928;&#29575;&#24182;&#36991;&#20813;&#36229;&#36807;LLMs&#30340;&#26368;&#22823;&#24207;&#21015;&#38271;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#65292;&#21363;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#39044;&#27979;&#36873;&#25321;&#26368;&#21487;&#33021;&#30340;&#31867;&#21035;&#65292;&#24182;&#20165;&#20351;&#29992;&#36825;&#20010;&#31867;&#21035;&#20013;&#30340;&#23454;&#20307;&#20316;&#20026;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#12290;&#23545;&#20869;&#37096;&#30340;&#21628;&#21483;&#12289;&#28040;&#24687;&#21644;&#21475;&#36848;&#25968;&#25454;&#38598;&#20197;&#21450;SLUE-Voxpopuli&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35789;&#38169;&#35823;&#29575;(WER)&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies contextual biasing with Large Language Models (LLMs), where during second-pass rescoring additional contextual information is provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We propose to leverage prompts for a LLM without fine tuning during rescoring which incorporate a biasing list and few-shot examples to serve as additional information when calculating the score for the hypothesis. In addition to few-shot prompt learning, we propose multi-task training of the LLM to predict both the entity class and the next token. To improve the efficiency for contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we propose dynamic prompting, where we select the most likely class using the class tag prediction, and only use entities in this class as contexts for next token prediction. Word Error Rate (WER) evaluation is performed on i) an internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli dataset. Results
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30446;&#30340;&#22312;&#20110;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24773;&#22659;&#24847;&#35782;&#30340;&#20135;&#29983;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#24773;&#22659;&#24847;&#35782;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00667</link><description>&lt;p&gt;
&#33073;&#31163;&#19978;&#19979;&#25991;&#30340;&#24433;&#21709;&#65306;&#20851;&#20110;&#34913;&#37327;LLMs&#20013;&#30340;&#24773;&#22659;&#24847;&#35782;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Taken out of context: On measuring situational awareness in LLMs. (arXiv:2309.00667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00667
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30446;&#30340;&#22312;&#20110;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24773;&#22659;&#24847;&#35782;&#30340;&#20135;&#29983;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#24773;&#22659;&#24847;&#35782;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#8220;&#24773;&#22659;&#24847;&#35782;&#8221;&#30340;&#20986;&#29616;&#12290;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#22312;&#24847;&#35782;&#21040;&#33258;&#24049;&#26159;&#19968;&#20010;&#27169;&#22411;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#35782;&#21035;&#33258;&#24049;&#24403;&#21069;&#26159;&#22788;&#20110;&#27979;&#35797;&#25110;&#37096;&#32626;&#29366;&#24577;&#65292;&#37027;&#20040;&#36825;&#20010;&#27169;&#22411;&#22312;&#24773;&#22659;&#19978;&#26159;&#20855;&#22791;&#24847;&#35782;&#30340;&#12290;&#20170;&#22825;&#30340;LLMs&#22312;&#37096;&#32626;&#20043;&#21069;&#20250;&#32463;&#36807;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#30340;&#27979;&#35797;&#12290;&#22312;&#37096;&#32626;&#21518;&#65292;&#19968;&#20010;LLM&#21487;&#33021;&#20250;&#21033;&#29992;&#24773;&#22659;&#24847;&#35782;&#22312;&#23433;&#20840;&#27979;&#35797;&#20013;&#21462;&#24471;&#39640;&#20998;&#65292;&#20294;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#37319;&#21462;&#26377;&#23475;&#34892;&#20026;&#12290;&#24773;&#22659;&#24847;&#35782;&#21487;&#33021;&#20250;&#24847;&#22806;&#22320;&#22312;&#27169;&#22411;&#25193;&#23637;&#20013;&#20986;&#29616;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#39044;&#27979;&#36825;&#31181;&#20986;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20110;&#24773;&#22659;&#24847;&#35782;&#32780;&#35328;&#24517;&#35201;&#30340;&#33021;&#21147;&#20043;&#19968;&#65292;&#21363;&#8220;&#33073;&#31163;&#19978;&#19979;&#25991;&#25512;&#29702;&#8221;&#65288;&#19982;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#23398;&#20064;&#30456;&#23545;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#33073;&#31163;&#19978;&#19979;&#25991;&#25512;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#31034;&#20363;&#25110;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;LLM&#36827;&#34892;&#20102;&#25551;&#36848;&#27979;&#35797;&#30340;&#24494;&#35843;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#36890;&#36807;&#27979;&#35797;&#12290;&#20196;&#25105;&#20204;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim to better understand the emergence of `situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations. At test time, we assess whether the model can pass the test. To our surprise, we find that LLMs succeed on this out-
&lt;/p&gt;</description></item><item><title>GPT&#36890;&#36807;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#26174;&#31034;&#20986;&#20855;&#22791;&#25104;&#20026;&#22823;&#20247;&#37329;&#34701;&#26426;&#22120;&#39038;&#38382;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#22522;&#20110;GPT-4&#30340;ChatGPT&#20960;&#20046;&#23436;&#32654;&#22320;&#24471;&#20998;99%&#65292;&#25581;&#31034;&#20102;&#37329;&#34701;&#32032;&#20859;&#27491;&#22312;&#25104;&#20026;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00649</link><description>&lt;p&gt;
GPT&#24050;&#32463;&#20855;&#22791;&#20102;&#37329;&#34701;&#32032;&#20859;&#65306;&#26469;&#33258;GPT&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#30340;&#35265;&#35299;&#20197;&#21450;&#20154;&#20204;&#20351;&#29992;&#20854;&#20316;&#20026;&#21672;&#35810;&#26469;&#28304;&#30340;&#21021;&#27493;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPT has become financially literate: Insights from financial literacy tests of GPT and a preliminary test of how people use it as a source of advice. (arXiv:2309.00649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00649
&lt;/p&gt;
&lt;p&gt;
GPT&#36890;&#36807;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#26174;&#31034;&#20986;&#20855;&#22791;&#25104;&#20026;&#22823;&#20247;&#37329;&#34701;&#26426;&#22120;&#39038;&#38382;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#22522;&#20110;GPT-4&#30340;ChatGPT&#20960;&#20046;&#23436;&#32654;&#22320;&#24471;&#20998;99%&#65292;&#25581;&#31034;&#20102;&#37329;&#34701;&#32032;&#20859;&#27491;&#22312;&#25104;&#20026;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT&#65288;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#20316;&#20026;&#22823;&#20247;&#37329;&#34701;&#26426;&#22120;&#39038;&#38382;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;GPT-3.5&#30340;Davinci&#21644;ChatGPT&#20998;&#21035;&#22312;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#20013;&#24471;&#20998;&#20026;66%&#21644;65%&#65292;&#32780;&#22522;&#20110;GPT-4&#30340;ChatGPT&#20960;&#20046;&#23436;&#32654;&#22320;&#24471;&#21040;&#20102;99%&#30340;&#20998;&#25968;&#65292;&#36825;&#34920;&#26126;&#37329;&#34701;&#32032;&#20859;&#27491;&#22312;&#25104;&#20026;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;Judge-Advisor&#31995;&#32479;&#21644;&#19968;&#20010;&#20648;&#33988;&#22256;&#22659;&#26469;&#35828;&#26126;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#24314;&#35758;&#21033;&#29992;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We assess the ability of GPT -- a large language model -- to serve as a financial robo-advisor for the masses, by using a financial literacy test. Davinci and ChatGPT based on GPT-3.5 score 66% and 65% on the financial literacy test, respectively, compared to a baseline of 33%. However, ChatGPT based on GPT-4 achieves a near-perfect 99% score, pointing to financial literacy becoming an emergent ability of state-of-the-art models. We use the Judge-Advisor System and a savings dilemma to illustrate how researchers might assess advice-utilization from large language models. We also present a number of directions for future research.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25968;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#25968;&#23398;&#27010;&#24565;&#65292;&#20026;&#33258;&#21160;&#26415;&#35821;&#25552;&#21462;&#21644;&#25968;&#23398;&#25991;&#26412;&#22788;&#29702;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#30340;&#25552;&#21462;&#36807;&#31243;&#21644;&#26032;&#30340;&#26631;&#27880;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.00642</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25968;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#25968;&#23398;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Extracting Mathematical Concepts with Large Language Models. (arXiv:2309.00642v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00642
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25968;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#25968;&#23398;&#27010;&#24565;&#65292;&#20026;&#33258;&#21160;&#26415;&#35821;&#25552;&#21462;&#21644;&#25968;&#23398;&#25991;&#26412;&#22788;&#29702;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#30340;&#25552;&#21462;&#36807;&#31243;&#21644;&#26032;&#30340;&#26631;&#27880;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65288;&#22914;ChatGPT&#65289;&#20174;&#25968;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#25968;&#23398;&#27010;&#24565;&#65292;&#20026;&#33258;&#21160;&#26415;&#35821;&#25552;&#21462;&#65288;ATE&#65289;&#21644;&#25968;&#23398;&#25991;&#26412;&#22788;&#29702;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#21516;&#26102;&#20063;&#20026;LLM&#30740;&#31350;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#20110;&#65292;&#22312;&#25968;&#23398;&#39046;&#22495;&#65288;&#33539;&#30068;&#35770;&#65289;&#20013;&#33258;&#21160;&#25552;&#21462;&#26415;&#35821;&#65288;&#20851;&#38190;&#35789;&#65289;&#65292;&#20351;&#29992;&#30340;&#35821;&#26009;&#24211;&#26159;2020&#24180;&#24038;&#21491;&#30340;&#22312;&#32447;&#26399;&#21002;&#12298;&#33539;&#30068;&#30340;&#29702;&#35770;&#19982;&#24212;&#29992;&#12299;&#20013;&#30340;755&#20010;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#65306;&#65288;1&#65289;&#23545;&#20110;&#20160;&#20040;&#20351;&#25968;&#23398;&#26415;&#35821;&#25552;&#21462;&#25104;&#20026;&#22256;&#38590;&#38382;&#39064;&#36827;&#34892;&#20102;&#26356;&#20840;&#38754;&#30340;&#20998;&#26512;&#65307;&#65288;2&#65289;&#23545;&#20114;&#30456;&#20043;&#38388;&#30340;&#26631;&#27880;&#32773;&#24847;&#35265;&#19981;&#19968;&#33268;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#20851;&#27880;&#65307;&#65288;3&#65289;&#25552;&#20379;&#20102;&#19968;&#22871;&#20379;&#20154;&#24037;&#21644;&#26426;&#22120;&#26631;&#27880;&#32773;&#20351;&#29992;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;&#29992;&#20197;&#26631;&#20934;&#21270;&#25552;&#21462;&#36807;&#31243;&#65307;&#65288;4&#65289;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#27880;&#24037;&#20855;&#65292;&#24110;&#21161;&#20154;&#31867;&#36827;&#34892;ATE&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#25968;&#23398;&#39046;&#22495;&#65292;&#29978;&#33267;&#36229;&#36234;&#25968;&#23398;&#65307;&#65288;5&#65289;
&lt;/p&gt;
&lt;p&gt;
We extract mathematical concepts from mathematical text using generative large language models (LLMs) like ChatGPT, contributing to the field of automatic term extraction (ATE) and mathematical text processing, and also to the study of LLMs themselves. Our work builds on that of others in that we aim for automatic extraction of terms (keywords) in one mathematical field, category theory, using as a corpus the 755 abstracts from a snapshot of the online journal "Theory and Applications of Categories", circa 2020. Where our study diverges from previous work is in (1) providing a more thorough analysis of what makes mathematical term extraction a difficult problem to begin with; (2) paying close attention to inter-annotator disagreements; (3) providing a set of guidelines which both human and machine annotators could use to standardize the extraction process; (4) introducing a new annotation tool to help humans with ATE, applicable to any mathematical field and even beyond mathematics; (5
&lt;/p&gt;</description></item><item><title>&#38169;&#35823;&#20449;&#24687;&#31649;&#23478;&#26159;&#19968;&#20010;&#21487;&#34892;&#24615;&#27010;&#24565;&#65292;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#31038;&#20132;&#23186;&#20307;&#20013;&#27969;&#34892;&#30340;&#38169;&#35823;&#20449;&#24687;&#30340;&#21487;&#25805;&#20316;&#24773;&#25253;&#12290;&#23427;&#20351;&#29992;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26469;&#35782;&#21035;&#38169;&#35823;&#20449;&#24687;&#24182;&#25512;&#33616;&#21453;&#39539;&#20449;&#24687;&#65292;&#24110;&#21161;&#24178;&#39044;&#21644;&#21453;&#20987;&#38169;&#35823;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.00639</link><description>&lt;p&gt;
&#12298;&#38169;&#35823;&#20449;&#24687;&#31649;&#23478;&#65306;&#20197;COVID-19&#30123;&#33495;&#20026;&#20363;&#30340;&#32463;&#36807;&#31579;&#36873;&#30340;&#25512;&#29305;&#25968;&#25454;&#38598;&#30340;&#27010;&#24565;&#39564;&#35777;&#12299;
&lt;/p&gt;
&lt;p&gt;
Misinformation Concierge: A Proof-of-Concept with Curated Twitter Dataset on COVID-19 Vaccination. (arXiv:2309.00639v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00639
&lt;/p&gt;
&lt;p&gt;
&#38169;&#35823;&#20449;&#24687;&#31649;&#23478;&#26159;&#19968;&#20010;&#21487;&#34892;&#24615;&#27010;&#24565;&#65292;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#31038;&#20132;&#23186;&#20307;&#20013;&#27969;&#34892;&#30340;&#38169;&#35823;&#20449;&#24687;&#30340;&#21487;&#25805;&#20316;&#24773;&#25253;&#12290;&#23427;&#20351;&#29992;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26469;&#35782;&#21035;&#38169;&#35823;&#20449;&#24687;&#24182;&#25512;&#33616;&#21453;&#39539;&#20449;&#24687;&#65292;&#24110;&#21161;&#24178;&#39044;&#21644;&#21453;&#20987;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#38169;&#35823;&#20449;&#24687;&#31649;&#23478;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#34892;&#24615;&#27010;&#24565;&#65292;&#21487;&#25552;&#20379;&#20851;&#20110;&#31038;&#20132;&#23186;&#20307;&#20013;&#27969;&#34892;&#30340;&#38169;&#35823;&#20449;&#24687;&#30340;&#21487;&#25805;&#20316;&#24773;&#25253;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#20351;&#29992;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26469;&#35782;&#21035;&#35805;&#35821;&#30340;&#23376;&#20027;&#39064;&#21644;&#36776;&#21035;&#19981;&#20934;&#30830;/&#35823;&#23548;&#24615;&#30340;&#24086;&#23376;&#65307;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#32479;&#35745;&#25253;&#21578;&#65292;&#20197;&#21450;&#21450;&#26102;&#20102;&#35299;&#27969;&#34892;&#30340;&#38169;&#35823;&#20449;&#24687;&#30340;&#25972;&#20307;&#24773;&#20917;&#65307;&#24182;&#25512;&#33616;&#23545;&#29305;&#23450;&#38169;&#35823;&#20449;&#24687;&#30340;&#21453;&#39539;&#20449;&#24687;&#65292;&#20174;&#25968;&#25454;&#35821;&#26009;&#24211;&#20013;&#35782;&#21035;&#20986;&#26469;&#65292;&#20197;&#20415;&#21450;&#26102;&#24178;&#39044;&#21644;&#21453;&#20987;&#38169;&#35823;&#20449;&#24687;&#12290;&#38169;&#35823;&#20449;&#24687;&#31649;&#23478;&#30340;&#27010;&#24565;&#39564;&#35777;&#20351;&#29992;&#20102;&#32463;&#36807;&#31579;&#36873;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21487;&#20197;&#22312;&#20197;&#19979;&#32593;&#22336;&#35775;&#38382;&#65306;https://demo-frontend-uy34.onrender.com/
&lt;/p&gt;
&lt;p&gt;
We demonstrate the Misinformation Concierge, a proof-of-concept that provides actionable intelligence on misinformation prevalent in social media. Specifically, it uses language processing and machine learning tools to identify subtopics of discourse and discern non/misleading posts; presents statistical reports for policy-makers to understand the big picture of prevalent misinformation in a timely manner; and recommends rebuttal messages for specific pieces of misinformation, identified from within the corpus of data - providing means to intervene and counter misinformation promptly. The Misinformation Concierge proof-of-concept using a curated dataset is accessible at: https://demo-frontend-uy34.onrender.com/
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23545;&#25239;&#25915;&#20987;&#38382;&#39064;&#65292;&#36890;&#36807;&#35780;&#20272;&#22522;&#32447;&#38450;&#24481;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#31574;&#30053;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2309.00614</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#22522;&#32447;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Baseline Defenses for Adversarial Attacks Against Aligned Language Models. (arXiv:2309.00614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00614
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23545;&#25239;&#25915;&#20987;&#38382;&#39064;&#65292;&#36890;&#36807;&#35780;&#20272;&#22522;&#32447;&#38450;&#24481;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#31574;&#30053;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#21450;&#65292;&#20854;&#23433;&#20840;&#28431;&#27934;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25991;&#26412;&#20248;&#21270;&#22120;&#21487;&#20197;&#29983;&#25104;&#32469;&#36807;&#23457;&#26597;&#21644;&#23545;&#40784;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;&#20511;&#37492;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#20016;&#23500;&#30740;&#31350;&#25104;&#26524;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#38382;&#39064;&#20837;&#25163;&#65306;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20160;&#20040;&#26679;&#30340;&#23041;&#32961;&#27169;&#22411;&#26159;&#23454;&#29992;&#30340;&#65311;&#22522;&#32447;&#38450;&#24481;&#25216;&#26415;&#22312;&#36825;&#20010;&#26032;&#39046;&#22495;&#20013;&#34920;&#29616;&#22914;&#20309;&#65311;LLM&#23433;&#20840;&#24615;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#26377;&#20309;&#19981;&#21516;&#65311;&#25105;&#20204;&#23545;&#20027;&#23548;&#23545;&#25239;LLM&#25915;&#20987;&#30340;&#20960;&#31181;&#22522;&#32447;&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#65292;&#35752;&#35770;&#20102;&#27599;&#31181;&#31574;&#30053;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#19977;&#31181;&#31867;&#22411;&#30340;&#38450;&#24481;&#65306;&#26816;&#27979;&#65288;&#22522;&#20110;&#22256;&#24785;&#24230;&#65289;&#12289;&#36755;&#20837;&#39044;&#22788;&#29702;&#65288;&#25913;&#20889;&#21644;&#37325;&#26032;&#26631;&#35760;&#21270;&#65289;&#21644;&#23545;&#25239;&#35757;&#32451;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#30333;&#30418;&#21644;&#28784;&#30418;&#35774;&#32622;&#65292;&#24182;&#35752;&#35770;&#20102;&#27599;&#31181;&#32771;&#34385;&#30340;&#38450;&#24481;&#31574;&#30053;&#22312;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models quickly become ubiquitous, their security vulnerabilities are critical to understand. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision?  We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. Surprisingly, we find much more succ
&lt;/p&gt;</description></item><item><title>TouchStone&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#22996;&#26469;&#20840;&#38754;&#35780;&#20272;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#21508;&#31181;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;&#32508;&#21512;&#30340;&#35270;&#35273;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#22270;&#20687;&#27880;&#37322;&#65292;&#35780;&#20272;&#21253;&#25324;&#35782;&#21035;&#12289;&#29702;&#35299;&#12289;&#23545;&#35805;&#21644;&#21465;&#20107;&#31561;&#22810;&#20010;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16890</link><description>&lt;p&gt;
TouchStone: &#29992;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TouchStone: Evaluating Vision-Language Models by Language Models. (arXiv:2308.16890v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16890
&lt;/p&gt;
&lt;p&gt;
TouchStone&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#22996;&#26469;&#20840;&#38754;&#35780;&#20272;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#21508;&#31181;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;&#32508;&#21512;&#30340;&#35270;&#35273;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#22270;&#20687;&#27880;&#37322;&#65292;&#35780;&#20272;&#21253;&#25324;&#35782;&#21035;&#12289;&#29702;&#35299;&#12289;&#23545;&#35805;&#21644;&#21465;&#20107;&#31561;&#22810;&#20010;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#25509;&#25910;&#22120;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#36830;&#25509;&#65292;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24863;&#30693;&#12289;&#29702;&#35299;&#21644;&#22788;&#29702;&#35270;&#35273;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#35780;&#20272;&#20027;&#35201;&#20851;&#27880;&#35782;&#21035;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#32570;&#20047;&#23545;&#23545;&#35805;&#33021;&#21147;&#21644;&#35270;&#35273;&#21465;&#20107;&#33021;&#21147;&#30340;&#30452;&#25509;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;LLMs&#20316;&#20026;&#35780;&#22996;&#26469;&#20840;&#38754;&#35780;&#20272;LVLMs&#30340;&#21508;&#31181;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#24320;&#25918;&#19990;&#30028;&#22270;&#20687;&#21644;&#38382;&#39064;&#30340;&#32508;&#21512;&#35270;&#35273;&#23545;&#35805;&#25968;&#25454;&#38598;TouchStone&#65292;&#28085;&#30422;&#20102;&#20116;&#20010;&#20027;&#35201;&#33021;&#21147;&#21644;27&#20010;&#23376;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#19981;&#20165;&#28085;&#30422;&#20102;&#22522;&#30784;&#30340;&#35782;&#21035;&#21644;&#29702;&#35299;&#65292;&#36824;&#25193;&#23637;&#21040;&#25991;&#23398;&#21019;&#20316;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#25972;&#21512;&#35814;&#32454;&#30340;&#22270;&#20687;&#27880;&#37322;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#20869;&#23481;&#36716;&#21270;&#20026;LLMs&#21487;&#20197;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models (LVLMs) have recently witnessed rapid advancements, exhibiting a remarkable capacity for perceiving, understanding, and processing visual information by connecting visual receptor with large language models (LLMs). However, current assessments mainly focus on recognizing and reasoning abilities, lacking direct evaluation of conversational skills and neglecting visual storytelling abilities. In this paper, we propose an evaluation method that uses strong LLMs as judges to comprehensively evaluate the various abilities of LVLMs. Firstly, we construct a comprehensive visual dialogue dataset TouchStone, consisting of open-world images and questions, covering five major categories of abilities and 27 subtasks. This dataset not only covers fundamental recognition and comprehension but also extends to literary creation. Secondly, by integrating detailed image annotations we effectively transform the multimodal input content into a form understandable by LLMs. This
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#32534;&#31243;&#35821;&#35328;&#21487;&#20197;&#22312;&#25351;&#20196;&#35843;&#20248;&#38454;&#27573;&#30456;&#20114;&#20419;&#36827;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16824</link><description>&lt;p&gt;
&#32534;&#31243;&#35821;&#35328;&#33021;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#30456;&#20114;&#25552;&#21319;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Programming Languages Boost Each Other via Instruction Tuning?. (arXiv:2308.16824v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16824
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#32534;&#31243;&#35821;&#35328;&#21487;&#20197;&#22312;&#25351;&#20196;&#35843;&#20248;&#38454;&#27573;&#30456;&#20114;&#20419;&#36827;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#31243;&#24207;&#21592;&#25484;&#25569;&#20102;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#21518;&#65292;&#23398;&#20064;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#35821;&#35328;&#20250;&#26356;&#23481;&#26131;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#25506;&#35752;&#20102;&#22312;&#20195;&#30721;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#24494;&#35843;&#38454;&#27573;&#20013;&#65292;&#32534;&#31243;&#35821;&#35328;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#30456;&#20114;&#25552;&#21319;&#26469;&#22686;&#24378;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;StarCoder&#19978;&#23545;8&#31181;&#27969;&#34892;&#30340;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65288;Python&#65292;JavaScript&#65292;TypeScript&#65292;C&#65292;C ++&#65292;Java&#65292;Go&#65292;HTML&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#31243;&#35821;&#35328;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#36890;&#36807;&#22312;Python&#19978;&#35757;&#32451;&#30340;CodeM-Python 15B&#21487;&#20197;&#20351;Java&#30340;pass@1&#29575;&#32477;&#23545;&#22686;&#21152;&#20102;17.95&#65285;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#22312;HTML&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;CodeM-HTML 7B&#21487;&#20197;&#20351;Java&#30340;pass@1&#29575;&#32477;&#23545;&#22686;&#21152;&#20102;15.24&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#24050;&#32463;&#21457;&#24067;&#22312;https://github.com/NL2Code/CodeM&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
When human programmers have mastered a programming language, it would be easier when they learn a new programming language. In this report, we focus on exploring whether programming languages can boost each other during the instruction fine-tuning phase of code large language models. We conduct extensive experiments of 8 popular programming languages (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that programming languages can significantly improve each other. For example, CodeM-Python 15B trained on Python is able to increase Java by an absolute 17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our training data is released at https://github.com/NL2Code/CodeM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;DSAA-2023&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.16469</link><description>&lt;p&gt;
&#23558;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Link Prediction for Wikipedia Articles as a Natural Language Inference Task. (arXiv:2308.16469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;DSAA-2023&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#23545;&#20110;&#33258;&#21160;&#29702;&#35299;&#22823;&#22411;&#30693;&#35782;&#24211;&#30340;&#32467;&#26500;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#25968;&#25454;&#31185;&#23398;&#21644;&#39640;&#32423;&#20998;&#26512;2023&#24180;&#31454;&#36187;&#8220;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#38142;&#25509;&#39044;&#27979;&#8221;&#65288;DSAA-2023&#31454;&#36187;&#65289;&#20013;&#29992;&#21253;&#21547;948,233&#20010;&#35757;&#32451;&#26679;&#26412;&#21644;238,265&#20010;&#29992;&#20110;&#20844;&#20849;&#27979;&#35797;&#30340;&#35821;&#26009;&#24211;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#30340;&#31995;&#32479;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#30340;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#24314;&#27169;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65288;NLI&#65289;&#30340;&#26041;&#27861;&#12290;&#21463;&#21040;&#36817;&#26399;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#38142;&#25509;&#39044;&#27979;&#20316;&#20026;&#19968;&#20010;NLI&#20219;&#21153;&#65292;&#20854;&#20013;&#23558;&#20004;&#20010;&#25991;&#31456;&#20043;&#38388;&#30340;&#38142;&#25509;&#23384;&#22312;&#35270;&#20026;&#21069;&#25552;&#65292;&#20219;&#21153;&#26159;&#22522;&#20110;&#25991;&#31456;&#20013;&#21576;&#29616;&#30340;&#20449;&#24687;&#26469;&#30830;&#23450;&#35813;&#21069;&#25552;&#26159;&#21542;&#25104;&#31435;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26159;&#22522;&#20110;&#29992;&#20110;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#38142;&#25509;&#39044;&#27979;&#30340;&#21477;&#23545;&#20998;&#31867;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23454;&#29616;&#20102;0.99996&#30340;Macro F1-score&#21644;1.00000&#30340;Macro F1-score&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction task is vital to automatically understanding the structure of large knowledge bases. In this paper, we present our system to solve this task at the Data Science and Advanced Analytics 2023 Competition "Efficient and Effective Link Prediction" (DSAA-2023 Competition) with a corpus containing 948,233 training and 238,265 for public testing. This paper introduces an approach to link prediction in Wikipedia articles by formulating it as a natural language inference (NLI) task. Drawing inspiration from recent advancements in natural language processing and understanding, we cast link prediction as an NLI task, wherein the presence of a link between two articles is treated as a premise, and the task is to determine whether this premise holds based on the information presented in the articles. We implemented our system based on the Sentence Pair Classification for Link Prediction for the Wikipedia Articles task. Our system achieved 0.99996 Macro F1-score and 1.00000 Macro F1-s
&lt;/p&gt;</description></item><item><title>BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16458</link><description>&lt;p&gt;
BioCoder: &#19968;&#31181;&#24102;&#26377;&#19978;&#19979;&#25991;&#35821;&#29992;&#30693;&#35782;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16458
&lt;/p&gt;
&lt;p&gt;
BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#20195;&#30721;&#29983;&#25104;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#36755;&#20986;&#26469;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#65292;&#29983;&#25104;&#21151;&#33021;&#31243;&#24207;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#37327;&#22823;&#12289;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25805;&#20316;&#21644;&#22797;&#26434;&#30340;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#32780;&#38754;&#20020;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BioCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#19982;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#26377;&#20851;&#65292;BioCoder&#28085;&#30422;&#20102;&#21487;&#33021;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;GitHub&#30340;1026&#20010;Python&#21644;Java&#20989;&#25968;&#21644;1243&#20010;&#26041;&#27861;&#65292;&#20197;&#21450;&#26469;&#33258;Rosalind&#39033;&#30446;&#30340;253&#20010;&#31034;&#20363;&#12290;BioCoder&#36824;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#25105;&#20204;&#24050;&#32463;&#24212;&#29992;&#23427;&#26469;&#35780;&#20272;&#35768;&#22810;&#27169;&#22411;&#65292;&#21253;&#25324;InCoder&#12289;CodeGen&#12289;CodeGen2&#12289;SantaCoder&#12289;StarCoder&#12289;StarCoder+&#12289;InstructCodeT&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
&lt;/p&gt;</description></item><item><title>LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16137</link><description>&lt;p&gt;
LM-Infinite: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21363;&#26102;&#38271;&#24230;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16137
&lt;/p&gt;
&lt;p&gt;
LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;Transformer-based&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;LLM&#22312;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#30528;&#23545;&#38271;&#26102;&#38388;&#25512;&#29702;&#36807;&#31243;&#25110;&#29702;&#35299;&#26356;&#22823;&#19978;&#19979;&#25991;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;LLM&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#26041;&#26696;&#23558;&#35757;&#32451;&#24207;&#21015;&#25130;&#26029;&#21040;&#22266;&#23450;&#38271;&#24230;&#65288;&#20363;&#22914;LLaMa&#30340;2048&#65289;&#12290;&#21363;&#20351;&#20351;&#29992;&#20102;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;LLM&#22312;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20043;&#21518;&#24448;&#24448;&#38590;&#20197;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#26356;&#19981;&#29992;&#35828;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#20102;&#12290;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#22312;&#26356;&#38271;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#24448;&#24448;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#30828;&#20214;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#38656;&#35201;&#36827;&#34892;&#20180;&#32454;&#30340;&#35757;&#32451;&#36807;&#31243;&#35774;&#35745;&#12290;&#20026;&#20102;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30740;&#31350;&#20102;&#20027;&#35201;&#30340;&#20998;&#24067;&#22806;(OOD) f
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer corpora often involves daunting hardware and time costs and requires careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) f
&lt;/p&gt;</description></item><item><title>&#32654;&#22269;&#27861;&#24459;&#38656;&#35201;&#21152;&#24378;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#65292;&#20197;&#22635;&#34917;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15906</link><description>&lt;p&gt;
&#32654;&#22269;&#27861;&#24459;&#20307;&#31995;&#26159;&#21542;&#20934;&#22791;&#22909;&#24212;&#23545;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#25361;&#25112;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is the U.S. Legal System Ready for AI's Challenges to Human Values?. (arXiv:2308.15906v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15906
&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#27861;&#24459;&#38656;&#35201;&#21152;&#24378;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#65292;&#20197;&#22635;&#34917;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#35843;&#26597;&#20102;&#32654;&#22269;&#27861;&#24459;&#22312;&#38754;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#19987;&#23478;&#30740;&#35752;&#20250;&#26399;&#38388;&#21046;&#23450;&#30340;&#22810;&#31181;&#20551;&#35774;&#24773;&#26223;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#33258;&#20027;&#26435;&#12289;&#38544;&#31169;&#26435;&#12289;&#23562;&#20005;&#12289;&#22810;&#26679;&#24615;&#12289;&#24179;&#31561;&#20197;&#21450;&#36523;&#24515;&#20581;&#24247;&#31561;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#23466;&#27861;&#21644;&#27665;&#26435;&#27861;&#20284;&#20046;&#26080;&#27861;&#23545;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#27495;&#35270;&#24615;&#20135;&#20986;&#25552;&#20379;&#36275;&#22815;&#30340;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#25105;&#20204;&#25490;&#38500;&#31532;230&#26465;&#27454;&#25552;&#20379;&#30340;&#36131;&#20219;&#20445;&#25252;&#65292;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22797;&#26434;&#21644;&#19981;&#36879;&#26126;&#24615;&#65292;&#35777;&#26126;&#35837;&#35876;&#21644;&#20135;&#21697;&#36131;&#20219;&#32034;&#36180;&#30340;&#22240;&#26524;&#20851;&#31995;&#20063;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24102;&#26469;&#30340;&#29420;&#29305;&#21644;&#38590;&#20197;&#39044;&#27979;&#30340;&#23041;&#32961;&#65292;&#25105;&#20204;&#20027;&#24352;&#24314;&#31435;&#33021;&#22815;&#36866;&#24212;&#26032;&#23041;&#32961;&#24182;&#20026;&#34892;&#19994;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#30340;&#27861;&#24459;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our interdisciplinary study investigates how effectively U.S. laws confront the challenges posed by Generative AI to human values. Through an analysis of diverse hypothetical scenarios crafted during an expert workshop, we have identified notable gaps and uncertainties within the existing legal framework regarding the protection of fundamental values, such as autonomy, privacy, dignity, diversity, equality, and physical/mental well-being. Constitutional and civil rights, it appears, may not provide sufficient protection against AI-generated discriminatory outputs. Furthermore, even if we exclude the liability shield provided by Section 230, proving causation for defamation and product liability claims is a challenging endeavor due to the intricate and opaque nature of AI systems. To address the unique and unforeseeable threats posed by Generative AI, we advocate for legal frameworks that evolve to recognize new threat and provide proactive, auditable guidelines to industry stakeholders
&lt;/p&gt;</description></item><item><title>ParaGuide&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;&#65292;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#65292;&#36890;&#36807;&#26799;&#24230;&#24341;&#23548;&#21644;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#30340;&#39118;&#26684;&#36716;&#21464;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.15459</link><description>&lt;p&gt;
ParaGuide: &#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;
&lt;/p&gt;
&lt;p&gt;
ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer. (arXiv:2308.15459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15459
&lt;/p&gt;
&lt;p&gt;
ParaGuide&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;&#65292;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#65292;&#36890;&#36807;&#26799;&#24230;&#24341;&#23548;&#21644;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#30340;&#39118;&#26684;&#36716;&#21464;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#26159;&#22312;&#20445;&#30041;&#24847;&#20041;&#30340;&#21516;&#26102;&#36716;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#23646;&#24615;&#30340;&#20219;&#21153;&#12290;&#30446;&#26631;&#39118;&#26684;&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#23450;&#20041;&#65292;&#20174;&#21333;&#19968;&#23646;&#24615;&#65288;&#20363;&#22914;&#27491;&#24335;&#24615;&#65289;&#21040;&#20316;&#32773;&#65288;&#20363;&#22914;&#33678;&#22763;&#27604;&#20122;&#65289;&#12290;&#20808;&#21069;&#30340;&#26080;&#30417;&#30563;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#20165;&#36866;&#29992;&#20110;&#22266;&#23450;&#30340;&#39118;&#26684;&#38598;&#65292;&#25110;&#38656;&#35201;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;ParaGuide&#21033;&#29992;&#20102;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#26469;&#33258;&#29616;&#25104;&#30340;&#20998;&#31867;&#22120;&#21644;&#24378;&#22823;&#30340;&#39118;&#26684;&#23884;&#20837;&#22120;&#30340;&#26799;&#24230;&#24341;&#23548;&#65292;&#20197;&#36716;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;Enron&#37038;&#20214;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#20154;&#24037;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27491;&#24335;&#24615;&#21644;... (&#20869;&#23481;&#22826;&#22810;&#65292;&#35831;&#21442;&#32771;&#33521;&#25991;&#25688;&#35201;)
&lt;/p&gt;
&lt;p&gt;
Textual style transfer is the task of transforming stylistic properties of text while preserving meaning. Target "styles" can be defined in numerous ways, ranging from single attributes (e.g, formality) to authorship (e.g, Shakespeare). Previous unsupervised style-transfer approaches generally rely on significant amounts of labeled data for only a fixed set of styles or require large language models. In contrast, we introduce a novel diffusion-based framework for general-purpose style transfer that can be flexibly adapted to arbitrary target styles at inference time. Our parameter-efficient approach, ParaGuide, leverages paraphrase-conditioned diffusion models alongside gradient-based guidance from both off-the-shelf classifiers and strong existing style embedders to transform the style of text while preserving semantic information. We validate the method on the Enron Email Corpus, with both human and automatic evaluations, and find that it outperforms strong baselines on formality, se
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25910;&#38598;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20013;&#23433;&#20840;&#26426;&#21046;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#19982;GPT-4&#22312;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#19978;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13387</link><description>&lt;p&gt;
Do-Not-Answer: &#29992;&#20110;&#35780;&#20272;LLMs&#20013;&#23433;&#20840;&#26426;&#21046;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. (arXiv:2308.13387v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13387
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25910;&#38598;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20013;&#23433;&#20840;&#26426;&#21046;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#19982;GPT-4&#22312;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#19978;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20986;&#29616;&#20102;&#26032;&#30340;&#38590;&#20197;&#39044;&#27979;&#30340;&#26377;&#23475;&#21151;&#33021;&#12290;&#36825;&#35201;&#27714;&#24320;&#21457;&#32773;&#33021;&#22815;&#36890;&#36807;&#35780;&#20272;LLMs&#20013;&#30340;&#8220;&#21361;&#38505;&#33021;&#21147;&#8221;&#26469;&#35782;&#21035;&#39118;&#38505;&#65292;&#20197;&#36127;&#36131;&#20219;&#22320;&#37096;&#32626;LLMs&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20013;&#23433;&#20840;&#26426;&#21046;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#20197;&#36739;&#20302;&#25104;&#26412;&#37096;&#32626;&#26356;&#23433;&#20840;&#30340;&#24320;&#28304;LLMs&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30001;&#36127;&#36131;&#20219;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#24212;&#36981;&#24490;&#30340;&#25351;&#20196;&#31934;&#24515;&#31574;&#21010;&#21644;&#36807;&#28388;&#32780;&#25104;&#12290;&#25105;&#20204;&#23545;&#20845;&#31181;&#27969;&#34892;&#30340;LLMs&#23545;&#36825;&#20123;&#25351;&#20196;&#30340;&#22238;&#24212;&#36827;&#34892;&#20102;&#27880;&#37322;&#21644;&#35780;&#20272;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26631;&#27880;&#65292;&#25105;&#20204;&#32487;&#32493;&#35757;&#32451;&#20102;&#20960;&#20010;&#31867;&#20284;BERT&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#23567;&#20998;&#31867;&#22120;&#22312;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#19978;&#21487;&#20197;&#36798;&#21040;&#19982;GPT-4&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#35686;&#21578;&#65306;&#26412;&#25991;&#21253;&#21547;&#21487;&#33021;&#20855;&#26377;&#20882;&#29359;&#24615;&#12289;&#26377;&#23475;&#24615;&#25110;&#20559;&#35265;&#24615;&#30340;&#31034;&#20363;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid evolution of large language models (LLMs), new and hard-to-predict harmful capabilities are emerging. This requires developers to be able to identify risks through the evaluation of "dangerous capabilities" in order to responsibly deploy LLMs. In this work, we collect the first open-source dataset to evaluate safeguards in LLMs, and deploy safer open-source LLMs at a low cost. Our dataset is curated and filtered to consist only of instructions that responsible language models should not follow. We annotate and assess the responses of six popular LLMs to these instructions. Based on our annotation, we proceed to train several BERT-like classifiers, and find that these small classifiers can achieve results that are comparable with GPT-4 on automatic safety evaluation. Warning: this paper contains example data that may be offensive, harmful, or biased.
&lt;/p&gt;</description></item><item><title>&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#23637;&#20026;&#35745;&#31639;&#26041;&#27861;&#21644;&#24314;&#26500;&#35821;&#27861;&#30740;&#31350;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#26412;&#31456;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35745;&#31639;&#26041;&#27861;&#19982;&#24314;&#26500;&#35821;&#27861;&#30456;&#20114;&#20316;&#29992;&#30340;&#36884;&#24452;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.13315</link><description>&lt;p&gt;
&#24314;&#26500;&#35821;&#27861;&#19982;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Construction Grammar and Language Models. (arXiv:2308.13315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13315
&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#23637;&#20026;&#35745;&#31639;&#26041;&#27861;&#21644;&#24314;&#26500;&#35821;&#27861;&#30740;&#31350;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#26412;&#31456;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35745;&#31639;&#26041;&#27861;&#19982;&#24314;&#26500;&#35821;&#27861;&#30456;&#20114;&#20316;&#29992;&#30340;&#36884;&#24452;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#20135;&#29983;&#20102;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#22312;&#19968;&#20010;&#22635;&#31354;&#24335;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#26174;&#31034;&#20986;&#20855;&#26377;&#20016;&#23500;&#35821;&#35328;&#20449;&#24687;&#30340;&#19968;&#20123;&#35777;&#25454;&#65292;&#21253;&#25324;&#19968;&#20123;&#26500;&#24335;&#30693;&#35782;&#12290;&#36825;&#19968;&#31361;&#30772;&#24615;&#30340;&#21457;&#29616;&#20026;&#35745;&#31639;&#26041;&#27861;&#21644;&#24314;&#26500;&#35821;&#27861;&#30740;&#31350;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26426;&#20250;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35745;&#31639;&#26041;&#27861;&#21644;&#24314;&#26500;&#35821;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26041;&#24335;&#65306;&#65288;&#19968;&#65289;&#25991;&#26412;&#20998;&#26512;&#30340;&#35745;&#31639;&#26041;&#27861;&#12289;&#65288;&#20108;&#65289;&#35745;&#31639;&#24314;&#26500;&#35821;&#27861;&#65292;&#20197;&#21450;&#65288;&#19977;&#65289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#20851;&#27880;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20171;&#32461;&#35745;&#31639;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#25509;&#35302;&#31532;&#19968;&#31181;&#21644;&#31532;&#20108;&#31181;&#26041;&#27861;&#65292;&#28982;&#21518;&#25552;&#20379;&#19968;&#31181;&#26131;&#20110;&#29702;&#35299;&#20294;&#20840;&#38754;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27010;&#36848;&#65292;&#20063;&#35299;&#20915;&#20102;&#24314;&#26500;&#35821;&#27861;&#23398;&#23478;&#21487;&#33021;&#23384;&#22312;&#30340;&#20445;&#30041;&#24847;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Recent progress in deep learning and natural language processing has given rise to powerful models that are primarily trained on a cloze-like task and show some evidence of having access to substantial linguistic information, including some constructional knowledge. This groundbreaking discovery presents an exciting opportunity for a synergistic relationship between computational methods and Construction Grammar research. In this chapter, we explore three distinct approaches to the interplay between computational methods and Construction Grammar: (i) computational methods for text analysis, (ii) computational Construction Grammar, and (iii) deep learning models, with a particular focus on language models. We touch upon the first two approaches as a contextual foundation for the use of computational methods before providing an accessible, yet comprehensive overview of deep learning models, which also addresses reservations construction grammarians may have. Additionally, we delve into e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#30340;&#19981;&#21516;&#35266;&#28857;&#65292;&#24182;&#36861;&#36394;&#20854;&#28436;&#21270;&#36335;&#24452;&#65292;&#26088;&#22312;&#24110;&#21161;&#30830;&#23450;&#26368;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.12014</link><description>&lt;p&gt;
&#20174;&#25351;&#20196;&#21040;&#20869;&#22312;&#20154;&#31867;&#20215;&#20540; - &#22823;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models. (arXiv:2308.12014v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#30340;&#19981;&#21516;&#35266;&#28857;&#65292;&#24182;&#36861;&#36394;&#20854;&#28436;&#21270;&#36335;&#24452;&#65292;&#26088;&#22312;&#24110;&#21161;&#30830;&#23450;&#26368;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27169;&#22411;&#65292;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#24120;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#30001;&#22823;&#37327;&#21442;&#25968;&#32452;&#25104;&#65292;&#19981;&#20165;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33719;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#36824;&#21576;&#29616;&#20986;&#36739;&#23567;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#26032;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#27169;&#22411;&#19982;&#26085;&#24120;&#29983;&#27963;&#30340;&#26085;&#30410;&#20132;&#32455;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21487;&#33021;&#36896;&#25104;&#20005;&#37325;&#30340;&#31038;&#20250;&#21361;&#23475;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#21162;&#21147;&#24050;&#32463;&#36827;&#34892;&#20102;&#65292;&#20197;&#20351;LLM&#19982;&#20154;&#31867;&#23545;&#40784;&#65292;&#20197;&#20351;&#23427;&#20204;&#26356;&#22909;&#22320;&#36981;&#24490;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#28385;&#36275;&#20154;&#31867;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#8220;&#19982;&#20309;&#23545;&#40784;&#8221;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#35752;&#35770;&#65292;&#19981;&#24403;&#30340;&#23545;&#40784;&#30446;&#26631;&#29978;&#33267;&#21487;&#33021;&#36866;&#24471;&#20854;&#21453;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#24037;&#20316;&#20013;&#30340;&#19981;&#21516;&#23545;&#40784;&#30446;&#26631;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#65292;&#24182;&#36861;&#36394;&#23427;&#20204;&#30340;&#28436;&#21270;&#36335;&#24452;&#65292;&#20197;&#24110;&#21161;&#30830;&#23450;&#26368;&#22522;&#26412;&#30340;&#30446;&#26631;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20174;&#23545;&#40784;&#30446;&#26631;&#30340;&#23450;&#20041;&#21644;&#23545;&#40784;&#35780;&#20272;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#30456;&#20851;&#24037;&#20316;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21644;&#28145;&#24230;&#23398;&#20064;&#20027;&#39064;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#26234;&#33021;&#25163;&#26426;&#37319;&#38598;&#30340;&#35821;&#38899;&#24405;&#38899;&#20013;&#35782;&#21035;&#20986;&#19982;&#25233;&#37057;&#30456;&#20851;&#30340;29&#20010;&#20027;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#20013;6&#20010;&#20027;&#39064;&#20316;&#20026;&#25233;&#37057;&#30340;&#39118;&#38505;&#20027;&#39064;&#12290;&#27492;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#38271;&#26399;&#30417;&#27979;&#35821;&#35328;&#20351;&#29992;&#65292;&#21487;&#20197;&#20102;&#35299;&#20027;&#39064;&#30340;&#20986;&#29616;&#19982;&#25233;&#37057;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2308.11773</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21644;&#28145;&#24230;&#23398;&#20064;&#20027;&#39064;&#27169;&#22411;&#22312;&#26234;&#33021;&#25163;&#26426;&#37319;&#38598;&#30340;&#33258;&#30001;&#22238;&#31572;&#35821;&#38899;&#24405;&#38899;&#20013;&#35782;&#21035;&#19982;&#25233;&#37057;&#30456;&#20851;&#30340;&#20027;&#39064;
&lt;/p&gt;
&lt;p&gt;
Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model. (arXiv:2308.11773v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11773
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21644;&#28145;&#24230;&#23398;&#20064;&#20027;&#39064;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#26234;&#33021;&#25163;&#26426;&#37319;&#38598;&#30340;&#35821;&#38899;&#24405;&#38899;&#20013;&#35782;&#21035;&#20986;&#19982;&#25233;&#37057;&#30456;&#20851;&#30340;29&#20010;&#20027;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#20013;6&#20010;&#20027;&#39064;&#20316;&#20026;&#25233;&#37057;&#30340;&#39118;&#38505;&#20027;&#39064;&#12290;&#27492;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#38271;&#26399;&#30417;&#27979;&#35821;&#35328;&#20351;&#29992;&#65292;&#21487;&#20197;&#20102;&#35299;&#20027;&#39064;&#30340;&#20986;&#29616;&#19982;&#25233;&#37057;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20351;&#29992;&#24050;&#34987;&#35777;&#26126;&#19982;&#25233;&#37057;&#30456;&#20851;&#65292;&#20294;&#38656;&#35201;&#22823;&#35268;&#27169;&#39564;&#35777;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#35786;&#25152;&#30740;&#31350;&#36153;&#29992;&#39640;&#26114;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24050;&#34987;&#24212;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#19978;&#39044;&#27979;&#25233;&#37057;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;-&#32570;&#20047;&#39564;&#35777;&#26631;&#31614;&#12289;&#26679;&#26412;&#20559;&#24046;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20351;&#29992;Whisper&#24037;&#20855;&#21644;BERTopic&#27169;&#22411;&#22312;&#26469;&#33258;265&#21517;&#21442;&#19982;&#32773;&#30340;3919&#20010;&#26234;&#33021;&#25163;&#26426;&#24405;&#38899;&#20013;&#35782;&#21035;&#20986;29&#20010;&#20027;&#39064;&#12290;&#20854;&#20013;6&#20010;&#20027;&#39064;&#30340;PHQ-8&#20013;&#20301;&#25968;&#22823;&#20110;&#25110;&#31561;&#20110;10&#34987;&#35270;&#20026;&#25233;&#37057;&#30340;&#39118;&#38505;&#20027;&#39064;&#65306;&#27809;&#26377;&#26399;&#26395;&#12289;&#30561;&#30496;&#12289;&#24515;&#29702;&#27835;&#30103;&#12289;&#29702;&#21457;&#12289;&#23398;&#20064;&#21644;&#35838;&#31243;&#12290;&#20026;&#20102;&#38416;&#26126;&#20027;&#39064;&#30340;&#20986;&#29616;&#21644;&#19982;&#25233;&#37057;&#30340;&#20851;&#32852;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#35782;&#21035;&#20986;&#30340;&#20027;&#39064;&#22312;&#34892;&#20026;&#65288;&#26469;&#33258;&#21487;&#31359;&#25140;&#35774;&#22791;&#65289;&#21644;&#35821;&#35328;&#29305;&#24449;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#36824;&#23545;&#20027;&#39064;&#36716;&#21464;&#21644;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#25233;&#37057;&#20005;&#37325;&#31243;&#24230;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#34920;&#26126;&#20102;&#38271;&#26399;&#30417;&#27979;&#35821;&#35328;&#20351;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Language use has been shown to correlate with depression, but large-scale validation is needed. Traditional methods like clinic studies are expensive. So, natural language processing has been employed on social media to predict depression, but limitations remain-lack of validated labels, biased user samples, and no context. Our study identified 29 topics in 3919 smartphone-collected speech recordings from 265 participants using the Whisper tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal to 10 were regarded as risk topics for depression: No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic emergence and associations with depression, we compared behavioral (from wearables) and linguistic characteristics across identified topics. The correlation between topic shifts and changes in depression severity over time was also investigated, indicating the importance of longitudinally monitoring language use. We also tested
&lt;/p&gt;</description></item><item><title>UniDoc&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20855;&#22791;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#20132;&#20114;&#25552;&#39640;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.11592</link><description>&lt;p&gt;
UniDoc: &#19968;&#31181;&#36890;&#29992;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#25991;&#26412;&#26816;&#27979;&#12289;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding. (arXiv:2308.11592v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11592
&lt;/p&gt;
&lt;p&gt;
UniDoc&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20855;&#22791;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#20132;&#20114;&#25552;&#39640;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26102;&#20195;&#65292;&#22810;&#27169;&#24577;&#29702;&#35299;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39640;&#32423;&#31639;&#27861;&#21463;&#38480;&#20110;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25152;&#22266;&#26377;&#30340;&#24040;&#22823;&#34920;&#31034;&#33021;&#21147;&#21644;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#20016;&#23500;&#22330;&#26223;&#20013;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#36830;&#25509;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;UniDoc&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20855;&#22791;&#29616;&#26377;&#26041;&#27861;&#25152;&#32570;&#20047;&#30340;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;UniDoc&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#20132;&#20114;&#26469;&#25552;&#39640;&#27599;&#20010;&#21333;&#29420;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;UniDoc&#65292;&#25105;&#20204;&#23545;&#36129;&#29486;&#30340;&#22823;&#35268;&#27169;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniDoc&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of Large Language Models (LLMs), tremendous strides have been made in the field of multimodal understanding. However, existing advanced algorithms are limited to effectively utilizing the immense representation capabilities and rich world knowledge inherent to these large pre-trained models, and the beneficial connections among tasks within the context of text-rich scenarios have not been sufficiently explored. In this work, we introduce UniDoc, a novel multimodal model equipped with text detection and recognition capabilities, which are deficient in existing approaches. Moreover, UniDoc capitalizes on the beneficial interactions among tasks to enhance the performance of each individual task. To implement UniDoc, we perform unified multimodal instruct tuning on the contributed large-scale instruction following datasets. Quantitative and qualitative experimental results show that UniDoc sets state-of-the-art scores across multiple challenging benchmarks. To the best of our kn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Reviewer&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#22312;&#20195;&#30721;&#23457;&#26597;&#39046;&#22495;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#20195;&#30721;&#23457;&#26597;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#20165;&#20351;&#29992;&#19981;&#21040;1%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#35813;&#26694;&#26550;&#20173;&#33021;&#21462;&#24471;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11148</link><description>&lt;p&gt;
LLaMA-Reviewer: &#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#23457;&#26597;&#33258;&#21160;&#21270;&#20013;&#30340;&#24212;&#29992;&#65288;&#23454;&#35777;&#30740;&#31350;&#65289;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report). (arXiv:2308.11148v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Reviewer&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#22312;&#20195;&#30721;&#23457;&#26597;&#39046;&#22495;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#20195;&#30721;&#23457;&#26597;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#20165;&#20351;&#29992;&#19981;&#21040;1%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#35813;&#26694;&#26550;&#20173;&#33021;&#21462;&#24471;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#23457;&#26597;&#27963;&#21160;&#30340;&#33258;&#21160;&#21270;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;&#36861;&#27714;&#65292;&#20027;&#35201;&#36890;&#36807;&#35768;&#22810;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#36164;&#28304;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#34917;&#20805;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30528;&#36855;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#33258;&#21160;&#21270;&#20195;&#30721;&#23457;&#26597;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLaMA-Reviewer&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#20102;&#27969;&#34892;&#30340;LLM&#8212;&#8212;LLaMA&#22312;&#20195;&#30721;&#23457;&#26597;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#32771;&#34385;&#21040;&#36164;&#28304;&#38480;&#21046;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#20197;&#26497;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25552;&#20379;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LLaMA-Reviewer&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#21482;&#20351;&#29992;&#19981;&#21040;1%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.  In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.  An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the 
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20351;&#29992;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#65288;PrefixLM&#65289;&#27604;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;CausalLM&#65289;&#25928;&#26524;&#26356;&#22909;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;&#34429;&#28982;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#37117;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#20294;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#21040;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#35299;&#65292;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#21160;&#24577;&#36981;&#24490;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#19981;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.06912</link><description>&lt;p&gt;
CausalLM&#19981;&#36866;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CausalLM is not optimal for in-context learning. (arXiv:2308.06912v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06912
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20351;&#29992;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#65288;PrefixLM&#65289;&#27604;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;CausalLM&#65289;&#25928;&#26524;&#26356;&#22909;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;&#34429;&#28982;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#37117;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#20294;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#21040;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#35299;&#65292;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#21160;&#24577;&#36981;&#24490;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#19981;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#65288;PrefixLM&#65289;&#34920;&#29616;&#26356;&#22909;&#65292;&#20854;&#20801;&#35768;&#19978;&#19979;&#25991;&#26679;&#26412;&#30456;&#20114;&#20851;&#27880;&#65307;&#30456;&#27604;&#20043;&#19979;&#65292;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;CausalLM&#65289;&#20351;&#29992;&#33258;&#22238;&#24402;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#31105;&#27490;&#19978;&#19979;&#25991;&#26679;&#26412;&#20851;&#27880;&#26410;&#26469;&#30340;&#26679;&#26412;&#12290;&#34429;&#28982;&#36825;&#20010;&#32467;&#26524;&#26159;&#30452;&#35266;&#30340;&#65292;&#20294;&#20174;&#29702;&#35770;&#35282;&#24230;&#24182;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#37319;&#29992;&#29702;&#35770;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#22312;&#29305;&#23450;&#21442;&#25968;&#26500;&#24314;&#19979;&#65292;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#21644;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#37117;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#20294;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#21040;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#35299;&#65292;&#32780;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#21160;&#24577;&#36981;&#24490;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#21363;&#20351;&#26679;&#26412;&#25968;&#37327;&#36235;&#20110;&#26080;&#31351;&#65292;&#20063;&#19981;&#33021;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#32463;&#39564;&#23454;&#39564;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06035</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#26399;&#38388;&#34920;&#29616;&#20986;&#20154;&#31867;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing. (arXiv:2308.06035v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06035
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#22797;&#21046;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30340;&#20105;&#35758;&#12290;LLMs&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#19968;&#20010;&#21306;&#21035;&#22312;&#20110;&#65292;&#35821;&#35328;&#36755;&#20837;&#36890;&#24120;&#24314;&#31435;&#22312;&#22810;&#20010;&#30693;&#35273;&#27169;&#24577;&#19978;&#65292;&#32780;&#22823;&#22810;&#25968;LLMs&#20165;&#22788;&#29702;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#20351;&#20154;&#31867;&#33021;&#22815;&#25972;&#21512;&#35270;&#35273;&#32972;&#26223;&#19982;&#35821;&#35328;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#21363;&#23558;&#20986;&#29616;&#30340;&#21333;&#35789;&#30340;&#31354;&#38388;&#26045;&#21152;&#38480;&#21046;&#65292;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;LLMs&#65288;mLLMs&#65289;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#21464;&#21387;&#22120;&#31867;&#22411;&#30340;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#19979;&#19968;&#20010;&#21333;&#35789;&#30340;&#39044;&#27979;&#12290;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#65292;&#22522;&#20110;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#22312;mLLMs&#21644;&#20154;&#31867;&#20013;&#21563;&#21512;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;200&#21517;&#34987;&#35797;&#35266;&#30475;&#20102;&#30701;&#30340;&#35270;&#21548;&#21098;&#36753;&#65292;&#24182;&#20272;&#35745;&#20102;&#21363;&#23558;&#20986;&#29616;&#30340;&#21160;&#35789;&#25110;&#21517;&#35789;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advanced language processing abilities of large language models (LLMs) have stimulated debate over their capacity to replicate human-like cognitive processes. One differentiating factor between language processing in LLMs and humans is that language input is often grounded in more than one perceptual modality, whereas most LLMs process solely text-based information. Multimodal grounding allows humans to integrate - e.g. visual context with linguistic information and thereby place constraints on the space of upcoming words, reducing cognitive load and improving perception and comprehension. Recent multimodal LLMs (mLLMs) combine visual and linguistic embedding spaces with a transformer type attention mechanism for next-word prediction. To what extent does predictive language processing based on multimodal input align in mLLMs and humans? To answer this question, 200 human participants watched short audio-visual clips and estimated the predictability of an upcoming verb or noun. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#25903;&#25345;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.02463</link><description>&lt;p&gt;
&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Generalist Foundation Model for Radiology. (arXiv:2308.02463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#25903;&#25345;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21551;&#21160;&#25918;&#23556;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#31216;&#20026;RadFM&#12290;&#25105;&#20204;&#20174;&#25968;&#25454;&#12289;&#27169;&#22411;&#35774;&#35745;&#21644;&#35780;&#20272;&#30340;&#35282;&#24230;&#20840;&#38754;&#32771;&#34385;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21487;&#24635;&#32467;&#22914;&#19979;&#65306;&#65288;i&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;MedMD&#65292;&#21253;&#25324;1600&#19975;&#20010;2D&#21644;3D&#21307;&#23398;&#25195;&#25551;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;3D&#21307;&#23398;&#25195;&#25551;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#65288;ii&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#20351;&#24471;&#21487;&#35270;&#26465;&#20214;&#29983;&#25104;&#39044;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#65292;&#21487;&#20197;&#23558;&#25991;&#26412;&#36755;&#20837;&#19982;2D&#25110;3D&#21307;&#23398;&#25195;&#25551;&#20132;&#38169;&#65292;&#29983;&#25104;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#22312;MedMD&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;RadMD&#19978;&#36827;&#34892;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#65292;RadMD&#26159;MedMD&#30340;&#25918;&#23556;&#23398;&#28165;&#29702;&#29256;&#26412;&#65292;&#21253;&#21547;300&#19975;&#20010;&#25918;&#23556;&#23398;&#30340;&#35270;&#35273;&#35821;&#35328;&#23545;&#12290;&#65288;iii&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#25324;&#20116;&#20010;&#20219;&#21153;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM.We consider the construction of foundational models from the perspectives of data, model design, and evaluation thoroughly. Our contribution can be concluded as follows: (i), we construct a large-scale Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans. To the best of our knowledge, this is the first multi-modal dataset containing 3D medical scans. (ii), We propose an architecture that enables visually conditioned generative pre-training, allowing for the integration of text input interleaved with 2D or 3D medical scans to generate response for diverse radiologic tasks. The model was initially pre-trained on MedMD and subsequently domain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD, containing 3M radiologic visual-language pairs. (iii), we propose a new evaluation benchmark that comprises five tasks, aiming to comprehensively assess the capa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#32771;&#34385;&#20102;&#29992;&#25143;&#30340;&#35282;&#24230;&#21644;&#31995;&#32479;&#30340;&#35282;&#24230;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#24120;&#35782;&#30693;&#35782;&#25552;&#21319;&#20102;ChatGPT&#22312;&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#39033;&#35780;&#20272;&#25351;&#26631;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.00085</link><description>&lt;p&gt;
&#20808;&#24605;&#32771;&#20877;&#22238;&#24212;&#65306;&#20026;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#38598;&#25104;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation. (arXiv:2308.00085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#32771;&#34385;&#20102;&#29992;&#25143;&#30340;&#35282;&#24230;&#21644;&#31995;&#32479;&#30340;&#35282;&#24230;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#24120;&#35782;&#30693;&#35782;&#25552;&#21319;&#20102;ChatGPT&#22312;&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#39033;&#35780;&#20272;&#25351;&#26631;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#26041;&#27861;&#35797;&#22270;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#25110;&#23545;&#24773;&#32490;&#21407;&#22240;&#30340;&#25512;&#29702;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#30340;&#32463;&#21382;&#21644;&#24863;&#21463;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20174;&#29992;&#25143;&#30340;&#35282;&#24230;&#29702;&#35299;&#19978;&#19979;&#25991;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#31995;&#32479;&#30340;&#35282;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#65292;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#30340;&#35282;&#24230;&#65288;&#29992;&#25143;&#30340;&#27442;&#26395;&#21644;&#21453;&#24212;&#65289;&#21644;&#31995;&#32479;&#30340;&#35282;&#24230;&#65288;&#31995;&#32479;&#30340;&#24847;&#22270;&#21644;&#21453;&#24212;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#24120;&#35782;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;ChatGPT&#22312;&#31995;&#32479;&#30340;&#35282;&#24230;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#19982;ChatGPT&#21644;&#22522;&#20110;T5&#27169;&#22411;&#30340;&#26041;&#27861;&#36827;&#34892;&#25972;&#21512;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#19978;&#20248;&#20110;&#20854;&#20182;&#21487;&#27604;&#36739;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches to empathetic response generation try to incorporate commonsense knowledge or reasoning about the causes of emotions to better understand the user's experiences and feelings. However, these approaches mainly focus on understanding the causalities of context from the user's perspective, ignoring the system's perspective. In this paper, we propose a commonsense-based causality explanation approach for diverse empathetic response generation that considers both the user's perspective (user's desires and reactions) and the system's perspective (system's intentions and reactions). We enhance ChatGPT's ability to reason for the system's perspective by integrating in-context learning with commonsense knowledge. Then, we integrate the commonsense-based causality explanation with both ChatGPT and a T5-based model. Experimental evaluations demonstrate that our method outperforms other comparable methods on both automatic and human evaluations.
&lt;/p&gt;</description></item><item><title>AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2307.11772</link><description>&lt;p&gt;
AutoAlign&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#33258;&#21160;&#26377;&#25928;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11772
&lt;/p&gt;
&lt;p&gt;
AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#38388;&#30340;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#26088;&#22312;&#35782;&#21035;&#20986;&#20004;&#20010;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20013;&#34920;&#31034;&#30456;&#21516;&#23454;&#20307;&#30340;&#27599;&#23545;&#23454;&#20307;&#12290;&#35768;&#22810;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#65292;&#36825;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21517;&#20026;AutoAlign&#30340;&#23436;&#20840;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#35859;&#35789;&#23884;&#20837;&#65292;AutoAlign&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#35859;&#35789;&#36817;&#37051;&#22270;&#65292;&#33258;&#21160;&#25429;&#25417;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#20013;&#35859;&#35789;&#30340;&#30456;&#20284;&#24615;&#12290;&#23545;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;AutoAlign&#39318;&#20808;&#20351;&#29992;TransE&#29420;&#31435;&#35745;&#31639;&#27599;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;&#35745;&#31639;&#22522;&#20110;&#23454;&#20307;&#23646;&#24615;&#30340;&#23454;&#20307;&#30456;&#20284;&#24615;&#65292;&#23558;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#31227;&#21160;&#21040;&#30456;&#21516;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;&#22240;&#27492;&#65292;AutoAlign&#23454;&#29616;&#20102;&#35859;&#35789;&#23545;&#40784;&#21644;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs' entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity al
&lt;/p&gt;</description></item><item><title>OUTFOX&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20801;&#35768;&#26816;&#27979;&#22120;&#21644;&#25915;&#20987;&#32773;&#32771;&#34385;&#24444;&#27492;&#30340;&#36755;&#20986;&#65292;&#25552;&#39640;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25915;&#20987;&#32773;&#21033;&#29992;&#26816;&#27979;&#22120;&#30340;&#39044;&#27979;&#26631;&#31614;&#20316;&#20026;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#29983;&#25104;&#38590;&#20197;&#26816;&#27979;&#30340;&#23545;&#25239;&#29983;&#25104;&#30340;&#35770;&#25991;&#12290;</title><link>http://arxiv.org/abs/2307.11729</link><description>&lt;p&gt;
OUTFOX: &#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#23545;&#25239;&#29983;&#25104;&#20363;&#23376;&#30340;LLM&#29983;&#25104;&#35770;&#25991;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples. (arXiv:2307.11729v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11729
&lt;/p&gt;
&lt;p&gt;
OUTFOX&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20801;&#35768;&#26816;&#27979;&#22120;&#21644;&#25915;&#20987;&#32773;&#32771;&#34385;&#24444;&#27492;&#30340;&#36755;&#20986;&#65292;&#25552;&#39640;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25915;&#20987;&#32773;&#21033;&#29992;&#26816;&#27979;&#22120;&#30340;&#39044;&#27979;&#26631;&#31614;&#20316;&#20026;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#29983;&#25104;&#38590;&#20197;&#26816;&#27979;&#30340;&#23545;&#25239;&#29983;&#25104;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#36798;&#21040;&#20102;&#19982;&#20154;&#31867;&#20889;&#20316;&#30456;&#24403;&#30340;&#27969;&#21033;&#31243;&#24230;&#65292;&#24456;&#38590;&#21306;&#20998;&#20154;&#31867;&#20889;&#20316;&#21644;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#36825;&#22686;&#21152;&#20102;LLMs&#34987;&#35823;&#29992;&#30340;&#39118;&#38505;&#65292;&#24182;&#38656;&#35201;&#24320;&#21457;&#26816;&#27979;&#22120;&#26469;&#35782;&#21035;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#36890;&#36807;&#31616;&#21333;&#22320;&#25913;&#20889;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26469;&#38477;&#20302;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#22312;&#23398;&#29983;&#22312;&#20889;&#20316;&#20316;&#19994;&#65288;&#22914;&#35770;&#25991;&#65289;&#20013;&#20351;&#29992;LLMs&#24182;&#36805;&#36895;&#23398;&#20250;&#22914;&#20309;&#35268;&#36991;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#30495;&#23454;&#29983;&#27963;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#34987;&#25506;&#35752;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OUTFOX&#65292;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20801;&#35768;&#26816;&#27979;&#22120;&#21644;&#25915;&#20987;&#32773;&#32771;&#34385;&#24444;&#27492;&#30340;&#36755;&#20986;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23398;&#29983;&#35770;&#25991;&#39046;&#22495;&#26469;&#25552;&#39640;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25915;&#20987;&#32773;&#20351;&#29992;&#26816;&#27979;&#22120;&#30340;&#39044;&#27979;&#26631;&#31614;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#65292;&#24182;&#23545;&#38590;&#20197;&#26816;&#27979;&#30340;&#23545;&#25239;&#29983;&#25104;&#35770;&#25991;&#36827;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, the effectiveness of these detectors in real-life situations, such as when students use LLMs for writing homework assignments (e.g., essays) and quickly learn how to evade these detectors, has not been explored. In this paper, we propose OUTFOX, a novel framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output and apply this to the domain of student essays. In our framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are hard
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20998;&#26512;&#20102;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#36807;&#31243;&#20013;&#30340;&#36136;&#37327;&#31649;&#29702;&#23454;&#36341;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#24314;&#35758;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#27969;&#34892;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#36739;&#22810;&#30340;&#38169;&#35823;&#27880;&#37322;&#12289;&#20559;&#35265;&#25110;&#27880;&#37322;&#20266;&#20687;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#36129;&#29486;&#26159;&#22312;&#36825;&#19968;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#36341;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2307.08153</link><description>&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#20998;&#26512;&#25968;&#25454;&#38598;&#27880;&#37322;&#36136;&#37327;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Analyzing Dataset Annotation Quality Management in the Wild. (arXiv:2307.08153v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08153
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20998;&#26512;&#20102;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#36807;&#31243;&#20013;&#30340;&#36136;&#37327;&#31649;&#29702;&#23454;&#36341;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#24314;&#35758;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#27969;&#34892;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#36739;&#22810;&#30340;&#38169;&#35823;&#27880;&#37322;&#12289;&#20559;&#35265;&#25110;&#27880;&#37322;&#20266;&#20687;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#36129;&#29486;&#26159;&#22312;&#36825;&#19968;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#36341;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#36136;&#37327;&#23545;&#20110;&#35757;&#32451;&#20934;&#30830;&#12289;&#20844;&#27491;&#21644;&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30340;&#27491;&#30830;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#27969;&#34892;&#25968;&#25454;&#38598;&#20013;&#20063;&#23384;&#22312;&#22823;&#37327;&#30340;&#38169;&#35823;&#27880;&#37322;&#12289;&#20559;&#35265;&#25110;&#27880;&#37322;&#20266;&#20687;&#12290;&#20851;&#20110;&#27880;&#37322;&#39033;&#30446;&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#25351;&#21335;&#24050;&#32463;&#23384;&#22312;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#36827;&#34892;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#20197;&#30740;&#31350;&#21019;&#24314;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#26102;&#23454;&#38469;&#36827;&#34892;&#30340;&#36136;&#37327;&#31649;&#29702;&#20197;&#21450;&#26159;&#21542;&#36981;&#24490;&#20102;&#36825;&#20123;&#24314;&#35758;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#24182;&#24635;&#32467;&#20102;&#25991;&#29486;&#20013;&#25551;&#36848;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#25512;&#33616;&#36136;&#37327;&#31649;&#29702;&#23454;&#36341;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#23454;&#36341;&#30340;&#24314;&#35758;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#30001;591&#31687;&#31185;&#23398;&#20986;&#29256;&#29289;&#32452;&#25104;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#35821;&#26009;&#24211;&#65292;&#24182;&#38024;&#23545;&#19982;&#36136;&#37327;&#30456;&#20851;&#30340;&#26041;&#38754;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#20363;&#22914;&#27880;&#37322;&#32773;&#31649;&#29702;&#12289;&#19968;&#33268;&#24615;&#12289;&#20210;&#35009;&#25110;&#25968;&#25454;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data quality is crucial for training accurate, unbiased, and trustworthy machine learning models and their correct evaluation. Recent works, however, have shown that even popular datasets used to train and evaluate state-of-the-art models contain a non-negligible amount of erroneous annotations, bias or annotation artifacts. There exist best practices and guidelines regarding annotation projects. But to the best of our knowledge, no large-scale analysis has been performed as of yet on how quality management is actually conducted when creating natural language datasets and whether these recommendations are followed. Therefore, we first survey and summarize recommended quality management practices for dataset creation as described in the literature and provide suggestions on how to apply them. Then, we compile a corpus of 591 scientific publications introducing text datasets and annotate it for quality-related aspects, such as annotator management, agreement, adjudication or data validat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#23545;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#20013;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#30340;&#24403;&#21069;&#29366;&#20917;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#25506;&#32034;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.07362</link><description>&lt;p&gt;
&#19968;&#31687;&#20851;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#20013;&#30340;&#25195;&#25551;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A scoping review on multimodal deep learning in biomedical images and texts. (arXiv:2307.07362v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07362
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#23545;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#20013;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#30340;&#24403;&#21069;&#29366;&#20917;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#25506;&#32034;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#21644;&#39044;&#21518;&#31995;&#32479;&#24212;&#35813;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#65288;MDL&#65289;&#28041;&#21450;&#22810;&#31181;&#25968;&#25454;&#28304;&#65288;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#30340;&#25972;&#21512;&#65292;&#26377;&#28508;&#21147;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#30452;&#21040;&#26368;&#36817;&#25165;&#24341;&#36215;&#30740;&#31350;&#20154;&#21592;&#30340;&#27880;&#24847;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23545;&#36825;&#20010;&#20027;&#39064;&#36827;&#34892;&#31995;&#32479;&#32508;&#36848;&#65292;&#30830;&#23450;&#24403;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25506;&#32034;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#35813;&#39046;&#22495; current state &#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#36825;&#20004;&#31181;&#25968;&#25454;&#31867;&#22411;&#22312; MDL &#30740;&#31350;&#20013;&#26368;&#24120;&#29992;&#12290;&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#20116;&#20010;&#20219;&#21153;&#20013;&#30340;&#24403;&#21069;&#24212;&#29992;&#65306;&#65288;1&#65289;&#25253;&#21578;&#29983;&#25104;&#65292;&#65288;2&#65289;&#35270;&#35273;&#38382;&#31572;&#65292;&#65288;3&#65289;&#20132;&#21449;...
&lt;/p&gt;
&lt;p&gt;
Computer-assisted diagnostic and prognostic systems of the future should be capable of simultaneously processing multimodal data. Multimodal deep learning (MDL), which involves the integration of multiple sources of data, such as images and text, has the potential to revolutionize the analysis and interpretation of biomedical data. However, it only caught researchers' attention recently. To this end, there is a critical need to conduct a systematic review on this topic, identify the limitations of current work, and explore future directions. In this scoping review, we aim to provide a comprehensive overview of the current state of the field and identify key concepts, types of studies, and research gaps with a focus on biomedical images and texts joint learning, mainly because these two were the most commonly available data types in MDL research. This study reviewed the current uses of multimodal deep learning on five tasks: (1) Report generation, (2) Visual question answering, (3) Cros
&lt;/p&gt;</description></item><item><title>ACTI&#22312;EVALITA 2023&#20013;&#30340;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21028;&#26029;&#38452;&#35851;&#20869;&#23481;&#21644;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.06954</link><description>&lt;p&gt;
ACTI&#22312;EVALITA 2023&#20013;&#30340;&#32508;&#36848;&#65306;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task. (arXiv:2307.06954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06954
&lt;/p&gt;
&lt;p&gt;
ACTI&#22312;EVALITA 2023&#20013;&#30340;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21028;&#26029;&#38452;&#35851;&#20869;&#23481;&#21644;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#26159;Evalita 2023&#39318;&#27425;&#25552;&#20986;&#30340;&#26032;&#20849;&#20139;&#20219;&#21153;&#12290;ACTI&#25361;&#25112;&#20165;&#22522;&#20110;Telegram&#19978;&#30340;&#38452;&#35851;&#39057;&#36947;&#35780;&#35770;&#65292;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(i) &#38452;&#35851;&#20869;&#23481;&#20998;&#31867;&#65306;&#36776;&#35782;&#38452;&#35851;&#20869;&#23481;&#21644;(ii) &#38452;&#35851;&#31867;&#21035;&#20998;&#31867;&#65306;&#38024;&#23545;&#29305;&#23450;&#38452;&#35851;&#29702;&#35770;&#20998;&#31867;&#12290;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#20102;&#35813;&#20219;&#21153;&#65292;&#24635;&#20849;&#25552;&#20132;&#20102;81&#20010;&#32467;&#26524;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conspiracy Theory Identication task is a new shared task proposed for the first time at the Evalita 2023. The ACTI challenge, based exclusively on comments published on conspiratorial channels of telegram, is divided into two subtasks: (i) Conspiratorial Content Classification: identifying conspiratorial content and (ii) Conspiratorial Category Classification about specific conspiracy theory classification. A total of fifteen teams participated in the task for a total of 81 submissions. We illustrate the best performing approaches were based on the utilization of large language models. We finally draw conclusions about the utilization of these models for counteracting the spreading of misinformation in online platforms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#12289;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12289;&#22810;&#35821;&#35328;&#29702;&#35299;&#21644;&#22810;&#20219;&#21153;&#22788;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#29790;&#22763;&#27861;&#24459;&#31995;&#32479;&#30340;&#22810;&#26679;&#21270;&#27861;&#24459;NLP&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#36827;&#34892;&#23545;&#24213;&#23618;&#38750;&#33521;&#35821;&#12289;&#22266;&#26377;&#22810;&#35821;&#35328;&#30340;&#27861;&#24459;&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.09237</link><description>&lt;p&gt;
SCALE: &#25552;&#21319;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
SCALE: Scaling up the Complexity for Advanced Language Model Evaluation. (arXiv:2306.09237v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#12289;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12289;&#22810;&#35821;&#35328;&#29702;&#35299;&#21644;&#22810;&#20219;&#21153;&#22788;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#29790;&#22763;&#27861;&#24459;&#31995;&#32479;&#30340;&#22810;&#26679;&#21270;&#27861;&#24459;NLP&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#36827;&#34892;&#23545;&#24213;&#23618;&#38750;&#33521;&#35821;&#12289;&#22266;&#26377;&#22810;&#35821;&#35328;&#30340;&#27861;&#24459;&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#24050;&#32463;&#39281;&#21644;&#20102;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65288;&#21253;&#25324;&#19987;&#19994;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#65289;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#26032;&#39062;&#12289;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26469;&#27491;&#30830;&#35780;&#20272;LLM&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#23545;&#24403;&#21069;LLM&#30340;&#22235;&#20010;&#20851;&#38190;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#65306;&#22788;&#29702;&#38271;&#25991;&#26723;&#65288;&#22810;&#36798;50K&#20010;&#26631;&#35760;&#65289;&#12289;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65288;&#20307;&#29616;&#22312;&#27861;&#24459;&#25991;&#26412;&#20013;&#65289;&#12289;&#22810;&#35821;&#35328;&#29702;&#35299;&#65288;&#28085;&#30422;&#20116;&#31181;&#35821;&#35328;&#65289;&#21644;&#22810;&#20219;&#21153;&#22788;&#29702;&#65288;&#21253;&#25324;&#27861;&#24459;&#25991;&#20214;&#21040;&#25991;&#20214;&#20449;&#24687;&#26816;&#32034;&#12289;&#27861;&#24237;&#35270;&#22270;&#29983;&#25104;&#12289;&#37325;&#35201;&#20915;&#31574;&#25688;&#35201;&#12289;&#24341;&#29992;&#25552;&#21462;&#21644;&#20843;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65289;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#26469;&#33258;&#29790;&#22763;&#27861;&#24459;&#31995;&#32479;&#30340;&#22810;&#26679;&#30340;&#27861;&#24459;NLP&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23545;&#24213;&#23618;&#38750;&#33521;&#35821;&#12289;&#22266;&#26377;&#22810;&#35821;&#35328;&#30340;&#32852;&#37030;&#27861;&#24459;&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#24378;&#28872;&#30340;&#23457;&#26597;/&#20998;&#26512;&#20219;&#21153;&#65292;&#39640;&#25928;&#22320;&#22788;&#29702;&#38271;&#25991;&#26723;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent strides in Large Language Models (LLMs) have saturated many NLP benchmarks (even professional domain-specific ones), emphasizing the need for novel, more challenging novel ones to properly assess LLM capabilities. In this paper, we introduce a novel NLP benchmark that poses challenges to current LLMs across four key dimensions: processing long documents (up to 50K tokens), utilizing domain specific knowledge (embodied in legal texts), multilingual understanding (covering five languages), and multitasking (comprising legal document to document Information Retrieval, Court View Generation, Leading Decision Summarization, Citation Extraction, and eight challenging Text Classification tasks). Our benchmark comprises diverse legal NLP datasets from the Swiss legal system, allowing for a comprehensive study of the underlying Non-English, inherently multilingual, federal legal system. Despite recent advances, efficiently processing long documents for intense review/analysis tasks remai
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35821;&#20041;&#22270;&#20013;&#33719;&#21462;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;NLI&#24230;&#37327;&#26041;&#27861;&#65292;&#19982;&#20351;&#29992;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#34917;&#20805;&#24615;&#65292;&#21487;&#20197;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.00936</link><description>&lt;p&gt;
AMR4NLI: &#20174;&#35821;&#20041;&#22270;&#20013;&#33719;&#24471;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;NLI&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
AMR4NLI: Interpretable and robust NLI measures from semantic graphs. (arXiv:2306.00936v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00936
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35821;&#20041;&#22270;&#20013;&#33719;&#21462;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;NLI&#24230;&#37327;&#26041;&#27861;&#65292;&#19982;&#20351;&#29992;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#34917;&#20805;&#24615;&#65292;&#21487;&#20197;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#35201;&#27714;&#21028;&#26029;&#32473;&#23450;&#30340;&#21069;&#25552;&#65288;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#65289;&#26159;&#21542;&#34164;&#21547;&#32473;&#23450;&#30340;&#20551;&#35774;&#12290;NLI&#22522;&#20934;&#21253;&#21547;&#20102;&#34164;&#21547;&#24615;&#30340;&#20154;&#24037;&#35780;&#20998;&#65292;&#20294;&#26159;&#39537;&#21160;&#36825;&#20123;&#35780;&#20998;&#30340;&#24847;&#20041;&#20851;&#31995;&#24182;&#26410;&#24418;&#24335;&#21270;&#12290;&#26159;&#21542;&#21487;&#20197;&#20197;&#19968;&#31181;&#21487;&#35299;&#37322;&#19988;&#40065;&#26834;&#30340;&#26041;&#24335;&#26356;&#26126;&#30830;&#22320;&#34920;&#31034;&#21477;&#23376;&#23545;&#20043;&#38388;&#30340;&#20851;&#31995;&#65311;&#25105;&#20204;&#27604;&#36739;&#20102;&#34920;&#31034;&#21069;&#25552;&#21644;&#20551;&#35774;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#21253;&#25324;&#19968;&#32452;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#21644;&#35821;&#20041;&#22270;&#65288;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65289;&#65292;&#24182;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#20551;&#35774;&#26159;&#21542;&#26159;&#21069;&#25552;&#30340;&#35821;&#20041;&#23376;&#32467;&#26500;&#12290;&#22312;&#19977;&#20010;&#33521;&#35821;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#35780;&#20272;&#21457;&#29616;&#65292;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#21644;&#35821;&#20041;&#22270;&#37117;&#26377;&#20854;&#20215;&#20540;&#65307;&#32780;&#19988;&#23427;&#20204;&#25552;&#20379;&#20102;&#20114;&#34917;&#30340;&#20449;&#21495;&#65292;&#24182;&#21487;&#20197;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#19968;&#36215;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of natural language inference (NLI) asks whether a given premise (expressed in NL) entails a given NL hypothesis. NLI benchmarks contain human ratings of entailment, but the meaning relationships driving these ratings are not formalized. Can the underlying sentence pair relationships be made more explicit in an interpretable yet robust fashion? We compare semantic structures to represent premise and hypothesis, including sets of contextualized embeddings and semantic graphs (Abstract Meaning Representations), and measure whether the hypothesis is a semantic substructure of the premise, utilizing interpretable metrics. Our evaluation on three English benchmarks finds value in both contextualized embeddings and semantic graphs; moreover, they provide complementary signals, and can be leveraged together in a hybrid model.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.06875</link><description>&lt;p&gt;
&#19981;&#38656;&#37325;&#26032;&#25628;&#32034;&#30340;&#30740;&#31350;&#65306;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#21487;&#31934;&#30830;&#39044;&#27979;&#36328;&#23610;&#24230;&#30340;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#39564;&#35777;&#30740;&#31350;&#24819;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#65292;&#22240;&#20026;&#23567;&#27169;&#22411;&#30340;&#32467;&#35770;&#19981;&#33021;&#31616;&#21333;&#22320;&#36716;&#31227;&#21040;&#22823;&#27169;&#22411;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#31995;&#32479;&#65292;&#20165;&#22522;&#20110;&#23567;&#27169;&#22411;&#30340;&#32467;&#26524;&#21644;&#36229;&#21442;&#25968;&#30452;&#25509;&#39044;&#27979;&#22823;&#27169;&#22411;&#30340;&#19968;&#20123;&#25351;&#26631;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#32553;&#25918;&#24459;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#19978;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#20294;&#30001;&#20110;&#36164;&#28304;&#26377;&#38480;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#65288;muP&#65289;&#20351;&#24471;&#21487;&#20197;&#22312;&#38752;&#36817;&#24120;&#35265;&#25439;&#22833;&#27969;&#22495;&#30340;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#25311;&#21512;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#25628;&#32034;&#12290;&#22240;&#27492;&#65292;&#19981;&#21516;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#22823;&#23610;&#24230;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#65292;&#22312;&#35757;&#32451;&#24320;&#22987;&#20043;&#21069;&#23601;&#21487;&#20197;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20316;&#20026;&#21487;&#38752;&#30340;&#23398;&#26415;&#30740;&#31350;&#30340;&#31532;&#19968;&#27493;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#27169;&#22411;&#35268;&#27169;&#65292;&#32780;&#19981;&#38656;&#22823;&#37327;&#30340;&#35745;&#31639;&#12290;&#20195;&#30721;&#23558;&#24456;&#24555;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#31471;&#21040;&#31471;&#23545;&#35805;&#27169;&#22411;- InterviewBot&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#19982;&#36755;&#20837;&#30340;&#21382;&#21490;&#23545;&#35805;&#21644;&#23450;&#21046;&#20027;&#39064;&#38598;&#25104;&#21040;&#21516;&#19968;&#23884;&#20837;&#31354;&#38388;&#20013;&#26469;&#35780;&#20272;&#22806;&#22269;&#23398;&#29983;&#30003;&#35831;&#32654;&#22269;&#22823;&#23398;&#30340;&#23398;&#26415;&#21644;&#25991;&#21270;&#20934;&#22791;&#24773;&#20917;&#12290;&#21516;&#26102;&#65292;&#20026;&#20811;&#26381;&#22522;&#20110;&#21464;&#24418;&#37329;&#21018;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#36755;&#20837;/&#36755;&#20986;&#22823;&#23567;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20851;&#27880;&#21644;&#20027;&#39064;&#23384;&#20648;&#20004;&#31181;&#26032;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#21160;&#24577;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#27969;&#30021;&#24615;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#39640;&#24230;&#28385;&#24847;&#12290;</title><link>http://arxiv.org/abs/2303.15049</link><description>&lt;p&gt;
InterviewBot&#65306;&#38754;&#21521;&#22823;&#23398;&#25307;&#29983;&#32771;&#35797;&#30340;&#23454;&#26102;&#31471;&#21040;&#31471;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
InterviewBot: Real-Time End-to-End Dialogue System to Interview Students for College Admission. (arXiv:2303.15049v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15049
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#31471;&#21040;&#31471;&#23545;&#35805;&#27169;&#22411;- InterviewBot&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#19982;&#36755;&#20837;&#30340;&#21382;&#21490;&#23545;&#35805;&#21644;&#23450;&#21046;&#20027;&#39064;&#38598;&#25104;&#21040;&#21516;&#19968;&#23884;&#20837;&#31354;&#38388;&#20013;&#26469;&#35780;&#20272;&#22806;&#22269;&#23398;&#29983;&#30003;&#35831;&#32654;&#22269;&#22823;&#23398;&#30340;&#23398;&#26415;&#21644;&#25991;&#21270;&#20934;&#22791;&#24773;&#20917;&#12290;&#21516;&#26102;&#65292;&#20026;&#20811;&#26381;&#22522;&#20110;&#21464;&#24418;&#37329;&#21018;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#36755;&#20837;/&#36755;&#20986;&#22823;&#23567;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20851;&#27880;&#21644;&#20027;&#39064;&#23384;&#20648;&#20004;&#31181;&#26032;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#21160;&#24577;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#27969;&#30021;&#24615;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#39640;&#24230;&#28385;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;InterviewBot&#65292;&#23427;&#21160;&#24577;&#23558;&#20250;&#35805;&#21382;&#21490;&#21644;&#23450;&#21046;&#20027;&#39064;&#38598;&#25104;&#21040;&#19968;&#33268;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#20197;&#36827;&#34892;10&#20998;&#38047;&#30340;&#28151;&#21512;&#39046;&#22495;&#65288;&#24320;&#25918;&#21644;&#23553;&#38381;&#65289;&#23545;&#35805;&#65292;&#20197;&#35780;&#20272;&#22806;&#22269;&#23398;&#29983;&#30003;&#35831;&#32654;&#22269;&#22823;&#23398;&#30340;&#23398;&#26415;&#21644;&#25991;&#21270;&#20934;&#22791;&#24773;&#20917;&#12290;&#20026;&#26500;&#24314;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#31471;&#21040;&#31471;&#23545;&#35805;&#27169;&#22411;&#65292;&#25105;&#20204;&#33258;&#21160;&#36716;&#24405;&#20102;7,361&#20010;&#20154;&#23545;&#20154;&#30340;&#38754;&#35797;&#38899;&#39057;&#35760;&#24405;&#65292;&#20854;&#20013;440&#20010;&#36827;&#34892;&#20102;&#25163;&#21160;&#32416;&#27491;&#20197;&#36827;&#34892;&#24494;&#35843;&#21644;&#35780;&#20272;&#12290;&#20026;&#20102;&#20811;&#26381;&#22522;&#20110;&#21464;&#24418;&#37329;&#21018;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#36755;&#20837;/&#36755;&#20986;&#22823;&#23567;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#19978;&#19979;&#25991;&#20851;&#27880;&#21644;&#20027;&#39064;&#23384;&#20648;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#30456;&#20851;&#21644;&#19968;&#33268;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#27169;&#22411;&#22312;&#32479;&#35745;&#19978;&#34987;&#27979;&#35797;&#65292;&#36890;&#36807;&#23558;&#20854;&#21709;&#24212;&#19982;&#38754;&#35797;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#21160;&#24577;&#22320;&#36992;&#35831;&#19987;&#19994;&#38754;&#35797;&#23448;&#21644;&#21508;&#31181;&#23398;&#29983;&#19982;&#20854;&#23454;&#26102;&#20132;&#20114;&#65292;&#22312;&#27969;&#30021;&#24615;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#26041;&#38754;&#21457;&#29616;&#20854;&#38750;&#24120;&#20196;&#20154;&#28385;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the InterviewBot that dynamically integrates conversation history and customized topics into a coherent embedding space to conduct 10 mins hybrid-domain (open and closed) conversations with foreign students applying to U.S. colleges for assessing their academic and cultural readiness. To build a neural-based end-to-end dialogue model, 7,361 audio recordings of human-to-human interviews are automatically transcribed, where 440 are manually corrected for finetuning and evaluation. To overcome the input/output size limit of a transformer-based encoder-decoder model, two new methods are proposed, context attention and topic storing, allowing the model to make relevant and consistent interactions. Our final model is tested both statistically by comparing its responses to the interview data and dynamically by inviting professional interviewers and various students to interact with it in real-time, finding it highly satisfactory in fluency and context awareness.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;VLSP2022-EVJVQA&#20849;&#20139;&#20219;&#21153;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#24207;&#21015;&#21040;&#24207;&#21015;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22312;&#22810;&#35821;&#35328;&#35270;&#35273;&#38382;&#31572;&#20013;&#23558;&#39044;&#35757;&#32451;&#30340;VQA&#27169;&#22411;&#21644;&#22270;&#20687;&#29305;&#24449;&#38598;&#25104;&#65292;&#33719;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.12671</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#24207;&#21015;&#21040;&#24207;&#21015;&#32593;&#32476;&#23558;&#22270;&#20687;&#29305;&#24449;&#38598;&#25104;&#21040;&#22810;&#35821;&#35328;&#35270;&#35273;&#38382;&#31572;&#20013;
&lt;/p&gt;
&lt;p&gt;
Integrating Image Features with Convolutional Sequence-to-sequence Network for Multilingual Visual Question Answering. (arXiv:2303.12671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12671
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;VLSP2022-EVJVQA&#20849;&#20139;&#20219;&#21153;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#24207;&#21015;&#21040;&#24207;&#21015;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22312;&#22810;&#35821;&#35328;&#35270;&#35273;&#38382;&#31572;&#20013;&#23558;&#39044;&#35757;&#32451;&#30340;VQA&#27169;&#22411;&#21644;&#22270;&#20687;&#29305;&#24449;&#38598;&#25104;&#65292;&#33719;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;(VQA)&#26159;&#19968;&#39033;&#35201;&#27714;&#35745;&#31639;&#26426;&#22522;&#20110;&#22270;&#20687;&#22238;&#31572;&#36755;&#20837;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#65292;&#20294;&#23545;&#35745;&#31639;&#26426;&#26469;&#35828;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;VLSP2022-EVJVQA&#20849;&#20139;&#20219;&#21153;&#22312;&#19968;&#20010;&#26032;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;UIT-EVJVQA&#20013;&#36827;&#34892;&#20102;&#22810;&#35821;&#35328;&#39046;&#22495;&#30340;VQA&#20219;&#21153;&#65292;&#20854;&#20013;&#38382;&#39064;&#21644;&#31572;&#26696;&#29992;&#19977;&#31181;&#19981;&#21516;&#35821;&#35328;&#32534;&#20889;&#65306;&#33521;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#26085;&#35821;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25361;&#25112;&#20316;&#20026;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#39044;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#30340;VQA&#27169;&#22411;&#21644;&#22270;&#20687;&#29305;&#24449;&#30340;&#25552;&#31034;&#19982;&#21367;&#31215;&#24207;&#21015;&#21040;&#24207;&#21015;&#32593;&#32476;&#38598;&#25104;&#22312;&#19968;&#36215;&#26469;&#29983;&#25104;&#25152;&#38656;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#20102;F1&#24471;&#20998;&#36798;&#21040;0.3442&#65292;&#22312;&#31169;&#26377;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#20102;0.4210&#30340;&#22909;&#25104;&#32489;&#65292;&#24182;&#22312;&#27604;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#19977;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) is a task that requires computers to give correct answers for the input questions based on the images. This task can be solved by humans with ease but is a challenge for computers. The VLSP2022-EVJVQA shared task carries the Visual Question Answering task in the multilingual domain on a newly released dataset: UIT-EVJVQA, in which the questions and answers are written in three different languages: English, Vietnamese and Japanese. We approached the challenge as a sequence-to-sequence learning task, in which we integrated hints from pre-trained state-of-the-art VQA models and image features with Convolutional Sequence-to-Sequence network to generate the desired answers. Our results obtained up to 0.3442 by F1 score on the public test set, 0.4210 on the private test set, and placed 3rd in the competition.
&lt;/p&gt;</description></item><item><title>BiasTestGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20559;&#35265;&#27979;&#35797;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#36827;&#34892;&#27979;&#35797;&#21477;&#23376;&#30340;&#29983;&#25104;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#22312;&#20132;&#21449;&#20559;&#35265;&#31561;&#25361;&#25112;&#24615;&#24773;&#22659;&#20013;&#12290;</title><link>http://arxiv.org/abs/2302.07371</link><description>&lt;p&gt;
BiasTestGPT: &#20351;&#29992;ChatGPT&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31038;&#20250;&#20559;&#35265;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models. (arXiv:2302.07371v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07371
&lt;/p&gt;
&lt;p&gt;
BiasTestGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20559;&#35265;&#27979;&#35797;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#36827;&#34892;&#27979;&#35797;&#21477;&#23376;&#30340;&#29983;&#25104;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#22312;&#20132;&#21449;&#20559;&#35265;&#31561;&#25361;&#25112;&#24615;&#24773;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23384;&#22312;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21487;&#33021;&#23548;&#33268;&#26377;&#23475;&#30340;&#29616;&#23454;&#24433;&#21709;&#12290;&#36825;&#31181;&#31038;&#20250;&#20559;&#35265;&#26159;&#36890;&#36807;PLMs&#23545;&#19968;&#32452;&#27979;&#35797;&#21477;&#23376;&#20013;&#19981;&#21516;&#31038;&#20250;&#32676;&#20307;&#21644;&#23646;&#24615;&#30340;&#27010;&#29575;&#20540;&#36827;&#34892;&#27979;&#37327;&#24471;&#20986;&#30340;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#20559;&#35265;&#27979;&#35797;&#26041;&#27861;&#38750;&#24120;&#32321;&#29712;&#65292;&#22240;&#20026;&#27979;&#35797;&#21477;&#23376;&#35201;&#20040;&#26159;&#20174;&#26377;&#38480;&#30340;&#19968;&#32452;&#25163;&#21160;&#27169;&#26495;&#20013;&#29983;&#25104;&#65292;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#20247;&#21253;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;ChatGPT&#36827;&#34892;&#21487;&#25511;&#29983;&#25104;&#27979;&#35797;&#21477;&#23376;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#25351;&#23450;&#30340;&#20219;&#24847;&#31038;&#20250;&#32676;&#20307;&#21644;&#23646;&#24615;&#32452;&#21512;&#12290;&#19982;&#22522;&#20110;&#27169;&#26495;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#36827;&#34892;&#27979;&#35797;&#21477;&#23376;&#29983;&#25104;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#31038;&#20250;&#20559;&#35265;&#26041;&#38754;&#26356;&#20026;&#20248;&#36234;&#65292;&#29305;&#21035;&#26159;&#22312;&#20132;&#21449;&#20559;&#35265;&#31561;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#20013;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#20840;&#38754;&#20559;&#35265;&#27979;&#35797;&#26694;&#26550;&#65288;BiasTestGPT&#65289;&#65292;&#25176;&#31649;&#22312;HuggingFace&#19978;&#65292;&#21487;&#20197;&#25554;&#20837;&#21040;&#20219;&#20309;&#24320;&#28304;PLM&#20013;&#36827;&#34892;&#20559;&#35265;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Language Models (PLMs) harbor inherent social biases that can result in harmful real-world implications. Such social biases are measured through the probability values that PLMs output for different social groups and attributes appearing in a set of test sentences. However, bias testing is currently cumbersome since the test sentences are generated either from a limited set of manual templates or need expensive crowd-sourcing. We instead propose using ChatGPT for controllable generation of test sentences, given any arbitrary user-specified combination of social groups and attributes appearing in the test sentences. When compared to template-based methods, our approach using ChatGPT for test sentence generation is superior in detecting social bias, especially in challenging settings such as intersectional biases. We present an open-source comprehensive bias testing framework (BiasTestGPT), hosted on HuggingFace, that can be plugged into any open-source PLM for bias testing. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;BERT&#30340;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;BLINKout&#65292;&#36890;&#36807;&#19982;&#29305;&#27530;NIL&#23454;&#20307;&#21305;&#37197;&#26469;&#35782;&#21035;&#27809;&#26377;&#30456;&#24212;KB&#23454;&#20307;&#30340;&#23454;&#20307;&#25552;&#21450;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.07189</link><description>&lt;p&gt;
&#25581;&#31034;&#26410;&#30693;&#65306;&#22522;&#20110;&#23454;&#20307;&#38142;&#25509;&#30340;&#30693;&#35782;&#24211;&#22806;&#25552;&#21450;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking. (arXiv:2302.07189v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;BERT&#30340;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;BLINKout&#65292;&#36890;&#36807;&#19982;&#29305;&#27530;NIL&#23454;&#20307;&#21305;&#37197;&#26469;&#35782;&#21035;&#27809;&#26377;&#30456;&#24212;KB&#23454;&#20307;&#30340;&#23454;&#20307;&#25552;&#21450;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#21457;&#29616;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#22806;&#30340;&#23454;&#20307;&#25552;&#21450;&#65292;&#22312;KB&#32500;&#25252;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#24182;&#26410;&#34987;&#23436;&#20840;&#24320;&#21457;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#23616;&#38480;&#20110;&#31616;&#21333;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#20998;&#31867;&#65292;&#24182;&#19988;&#29992;&#20110;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BLINKout&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;BERT&#30340;&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25552;&#21450;&#19982;&#29305;&#27530;&#30340;NIL&#23454;&#20307;&#36827;&#34892;&#21305;&#37197;&#26469;&#35782;&#21035;&#27809;&#26377;&#30456;&#24212;KB&#23454;&#20307;&#30340;&#25552;&#21450;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;BERT&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21253;&#25324;NIL&#23454;&#20307;&#34920;&#31034;&#21644;&#20998;&#31867;&#22312;&#20869;&#30340;&#26032;&#25216;&#26415;&#65292;&#24182;&#22686;&#24378;&#20102;&#20854;&#21516;&#20041;&#35789;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;KB&#20462;&#21098;&#21644;&#29256;&#26412;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#33258;&#21160;&#20174;&#24120;&#35265;&#30340;KB EL&#25968;&#25454;&#38598;&#26500;&#24314;&#20986;KB&#22806;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#21307;&#23398;&#26412;&#20307;&#35770;&#12289;UMLS&#12289;SNOMED CT&#31561;&#20116;&#20010;&#19981;&#21516;&#39046;&#22495;&#20013;&#65292;&#23545;&#20020;&#24202;&#31508;&#35760;&#12289;&#29983;&#29289;&#21307;&#23398;&#20986;&#29256;&#29289;&#21644;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BLINKout&#22312;&#35782;&#21035;&#30693;&#35782;&#24211;&#22806;&#25552;&#21450;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering entity mentions that are out of a Knowledge Base (KB) from texts plays a critical role in KB maintenance, but has not yet been fully explored. The current methods are mostly limited to the simple threshold-based approach and feature-based classification, and the datasets for evaluation are relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL) method which can identify mentions that do not have corresponding KB entities by matching them to a special NIL entity. To better utilize BERT, we propose new techniques including NIL entity representation and classification, with synonym enhancement. We also propose KB Pruning and Versioning strategies to automatically construct out-of-KB datasets from common in-KB EL datasets. Results on five datasets of clinical notes, biomedical publications, and Wikipedia articles in various domains show the advantages of BLINKout over existing methods to identify out-of-KB mentions for the medical ontologies, UMLS, SNOMED CT,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#38750;&#19977;&#20803;&#32452;&#25968;&#25454;&#26469;&#22686;&#24378;&#32763;&#35793;&#36136;&#37327;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#33521;&#27721;&#30005;&#23376;&#21830;&#21153;&#22810;&#27169;&#24577;&#32763;&#35793;&#25968;&#25454;&#38598;EMMT&#12290;</title><link>http://arxiv.org/abs/2212.10313</link><description>&lt;p&gt;
&#36229;&#36234;&#19977;&#20803;&#32452;&#65306;&#21033;&#29992;&#26368;&#22810;&#30340;&#25968;&#25454;&#36827;&#34892;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Beyond Triplet: Leveraging the Most Data for Multimodal Machine Translation. (arXiv:2212.10313v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#38750;&#19977;&#20803;&#32452;&#25968;&#25454;&#26469;&#22686;&#24378;&#32763;&#35793;&#36136;&#37327;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#33521;&#27721;&#30005;&#23376;&#21830;&#21153;&#22810;&#27169;&#24577;&#32763;&#35793;&#25968;&#25454;&#38598;EMMT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#20854;&#20182;&#27169;&#24577;&#65288;&#22914;&#35270;&#35273;&#65289;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;&#20197;&#24448;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20027;&#35201;&#20851;&#27880;&#26356;&#22909;&#22320;&#33719;&#21462;&#21644;&#21033;&#29992;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#20542;&#21521;&#20110;&#39564;&#35777;&#20854;&#26041;&#27861;&#22312;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;&#36825;&#20123;&#30740;&#31350;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#21482;&#33021;&#21033;&#29992;&#19977;&#20803;&#32452;&#25968;&#25454;&#65288;&#24102;&#26377;&#22270;&#20687;&#30340;&#21452;&#35821;&#25991;&#26412;&#65289;&#65292;&#36825;&#31181;&#25968;&#25454;&#31232;&#32570;&#65307;&#20854;&#27425;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#30456;&#23545;&#21463;&#38480;&#65292;&#19981;&#31526;&#21512;&#30495;&#23454;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30456;&#24212;&#22320;&#24314;&#31435;&#20102;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#30340;&#26032;&#26041;&#27861;&#21644;&#26032;&#25968;&#25454;&#38598;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;2/3-Triplet&#30340;&#26694;&#26550;&#65292;&#24182;&#37319;&#29992;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#65292;&#21363;&#21033;&#29992;&#22823;&#35268;&#27169;&#38750;&#19977;&#20803;&#32452;&#25968;&#25454;&#65306;&#21333;&#35821;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#21644;&#24179;&#34892;&#25991;&#26412;&#25968;&#25454;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33521;&#27721;&#30005;&#23376;&#21830;&#21153;&#22810;&#27169;&#24577;&#32763;&#35793;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#35757;&#32451;&#21644;&#27979;&#35797;&#65289;&#65292;&#21629;&#21517;&#20026;EMMT&#65292;&#20854;&#20013;&#27979;&#35797;&#38598;&#30340;&#36873;&#25321;&#32463;&#36807;&#31934;&#24515;&#32771;&#34385;&#65292;&#22240;&#20026;&#20854;&#20013;&#26377;&#20123;&#35789;&#26159;&#27169;&#31946;&#30340;&#65292;&#38656;&#35201;&#36827;&#34892;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal machine translation (MMT) aims to improve translation quality by incorporating information from other modalities, such as vision. Previous MMT systems mainly focus on better access and use of visual information and tend to validate their methods on image-related datasets. These studies face two challenges. First, they can only utilize triple data (bilingual texts with images), which is scarce; second, current benchmarks are relatively restricted and do not correspond to realistic scenarios. Therefore, this paper correspondingly establishes new methods and new datasets for MMT. First, we propose a framework 2/3-Triplet with two new approaches to enhance MMT by utilizing large-scale non-triple data: monolingual image-text data and parallel text-only data. Second, we construct an English-Chinese {e}-commercial {m}ulti{m}odal {t}ranslation dataset (including training and testing), named EMMT, where its test set is carefully selected as some words are ambiguous and shall be trans
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#20026;&#31038;&#20132;&#23186;&#20307;&#25366;&#25496;&#38750;&#21307;&#23398;&#22788;&#26041;&#33647;&#29289;&#20351;&#29992;&#20449;&#24687;&#32780;&#24320;&#21457;&#30340;&#31471;&#21040;&#31471;&#31649;&#36947;&#65292;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2211.10443</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25366;&#25496;&#22788;&#26041;&#33647;&#29289;&#30340;&#27602;&#24615;&#30417;&#27979;: &#20174;&#22836;&#21040;&#23614;&#30340;&#31649;&#36947;&#65292;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media mining for toxicovigilance of prescription medications: End-to-end pipeline, challenges and future work. (arXiv:2211.10443v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#20026;&#31038;&#20132;&#23186;&#20307;&#25366;&#25496;&#38750;&#21307;&#23398;&#22788;&#26041;&#33647;&#29289;&#20351;&#29992;&#20449;&#24687;&#32780;&#24320;&#21457;&#30340;&#31471;&#21040;&#31471;&#31649;&#36947;&#65292;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#36136;&#20351;&#29992;&#12289;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#21644;&#19982;&#29289;&#36136;&#20351;&#29992;&#30456;&#20851;&#30340;&#36807;&#37327;&#26159;&#20840;&#29699;&#21644;&#32654;&#22269;&#30340;&#20027;&#35201;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#12290;&#20174;&#20844;&#20849;&#21355;&#29983;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#25913;&#36827;&#30417;&#27979;&#31995;&#32479;&#12290;&#20256;&#32479;&#30340;&#30417;&#27979;&#31995;&#32479;&#28382;&#21518;&#65292;&#32780;&#31038;&#20132;&#23186;&#20307;&#26159;&#21450;&#26102;&#25968;&#25454;&#30340;&#28508;&#22312;&#26377;&#29992;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#25366;&#25496;&#30693;&#35782;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#24320;&#21457;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#25366;&#25496;&#38750;&#21307;&#23398;&#22788;&#26041;&#33647;&#29289;&#20351;&#29992;&#20449;&#24687;&#30340;&#31471;&#21040;&#31471;&#31649;&#36947;&#65292;&#21363;Twitter&#21644;Reddit&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#37319;&#29992;&#20102;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;NLP&#26469;&#36807;&#28388;&#22122;&#38899;&#21644;&#25551;&#36848;&#20132;&#27969;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;&#22235;&#24180;&#20869;&#24320;&#21457;&#30340;&#31471;&#21040;&#31471;&#31649;&#36947;&#12290;&#38500;&#20102;&#25551;&#36848;&#25105;&#20204;&#30340;&#25968;&#25454;&#25366;&#25496;&#22522;&#30784;&#35774;&#26045;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Substance use, substance use disorder, and overdoses related to substance use are major public health problems globally and in the United States. A key aspect of addressing these problems from a public health standpoint is improved surveillance. Traditional surveillance systems are laggy, and social media are potentially useful sources of timely data. However, mining knowledge from social media is challenging, and requires the development of advanced artificial intelligence, specifically natural language processing (NLP) and machine learning methods. We developed a sophisticated end-to-end pipeline for mining information about nonmedical prescription medication use from social media, namely Twitter and Reddit. Our pipeline employs supervised machine learning and NLP for filtering out noise and characterizing the chatter. In this paper, we describe our end-to-end pipeline developed over four years. In addition to describing our data mining infrastructure, we discuss existing challenges 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#30340;&#32534;&#30721;&#22120;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#23398;&#35821;&#20041;&#65292;&#27604;&#33258;&#21160;&#32534;&#30721;&#22120;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08142</link><description>&lt;p&gt;
&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#35821;&#20041;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Semantic Representations of Mathematical Expressions in a Continuous Vector Space. (arXiv:2211.08142v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08142
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#30340;&#32534;&#30721;&#22120;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#23398;&#35821;&#20041;&#65292;&#27604;&#33258;&#21160;&#32534;&#30721;&#22120;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#31526;&#21495;&#22312;STEM&#25991;&#29486;&#20013;&#21344;&#25454;&#20102;&#24456;&#22823;&#19968;&#37096;&#20998;&#65292;&#20294;&#26159;&#65292;&#20026;&#20844;&#24335;&#25214;&#21040;&#35821;&#20041;&#34920;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#25968;&#23398;&#31526;&#21495;&#26159;&#31934;&#30830;&#30340;&#65292;&#22312;&#23383;&#31526;&#24494;&#23567;&#21464;&#21270;&#26102;&#20854;&#21547;&#20041;&#20250;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#33258;&#28982;&#25991;&#26412;&#30340;&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#30340;&#32534;&#30721;&#22120;&#65292;&#35757;&#32451;&#20854;&#22312;&#35270;&#35273;&#19978;&#19981;&#21516;&#20294;&#22312;&#25968;&#23398;&#19978;&#31561;&#20215;&#30340;&#34920;&#36798;&#24335;&#19978;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#65288;&#25110;&#23884;&#20837;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#21069;&#32773;&#26356;&#33021;&#25429;&#25417;&#25968;&#23398;&#35821;&#20041;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20221;&#31561;&#20215;&#30340;&#36229;&#36234;&#21644;&#20195;&#25968;&#34920;&#36798;&#24335;&#23545;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical notation makes up a large portion of STEM literature, yet, finding semantic representations for formulae remains a challenging problem. Because mathematical notation is precise, and its meaning changes significantly with small character shifts, the methods that work for natural text do not necessarily work well for mathematical expressions. In this work, we describe an approach for representing mathematical expressions in a continuous vector space. We use the encoder of a sequence-to-sequence architecture, trained on visually different but mathematically equivalent expressions, to generate vector representations (or embeddings). We compare this approach with an autoencoder and show that the former is better at capturing mathematical semantics. Finally, to expedite future research, we publish a corpus of equivalent transcendental and algebraic expression pairs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26234;&#33021;&#22238;&#22797;&#24212;&#29992;&#31243;&#24207;&#20013;&#28508;&#22312;&#30340;&#20449;&#24687;&#27844;&#28431;&#28431;&#27934;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#38480;&#21046;&#26597;&#35810;&#31867;&#22411;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2207.10802</link><description>&lt;p&gt;
&#20174;&#26234;&#33021;&#22238;&#22797;&#20013;&#25552;&#21462;&#20027;&#21160;&#27169;&#24335;&#36827;&#34892;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Combing for Credentials: Active Pattern Extraction from Smart Reply. (arXiv:2207.10802v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26234;&#33021;&#22238;&#22797;&#24212;&#29992;&#31243;&#24207;&#20013;&#28508;&#22312;&#30340;&#20449;&#24687;&#27844;&#28431;&#28431;&#27934;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#38480;&#21046;&#26597;&#35810;&#31867;&#22411;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-2&#21644;BERT&#65292;&#36890;&#24120;&#20250;&#36890;&#36807;&#24494;&#35843;&#26469;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#20363;&#23376;&#26159;&#8220;&#26234;&#33021;&#22238;&#22797;&#8221;&#24212;&#29992;&#31243;&#24207;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#34987;&#35843;&#25972;&#20197;&#25552;&#20379;&#32473;&#23450;&#26597;&#35810;&#28040;&#24687;&#30340;&#24314;&#35758;&#22238;&#22797;&#12290;&#30001;&#20110;&#24494;&#35843;&#25968;&#25454;&#36890;&#24120;&#26159;&#25935;&#24863;&#30340;&#25968;&#25454;&#65292;&#22914;&#30005;&#23376;&#37038;&#20214;&#25110;&#32842;&#22825;&#35760;&#24405;&#65292;&#22240;&#27492;&#37325;&#35201;&#30340;&#26159;&#20102;&#35299;&#21644;&#20943;&#36731;&#27169;&#22411;&#27844;&#28431;&#24494;&#35843;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#20856;&#22411;&#26234;&#33021;&#22238;&#22797;&#27969;&#31243;&#20013;&#28508;&#22312;&#30340;&#20449;&#24687;&#27844;&#38706;&#28431;&#27934;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29616;&#23454;&#30340;&#24773;&#20917;&#65292;&#21363;&#25915;&#20987;&#32773;&#21482;&#33021;&#36890;&#36807;&#21069;&#31471;&#30028;&#38754;&#19982;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#38480;&#21046;&#20102;&#21487;&#20197;&#21457;&#36865;&#21040;&#27169;&#22411;&#30340;&#26597;&#35810;&#31867;&#22411;&#12290;&#20808;&#21069;&#30340;&#25915;&#20987;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#19981;&#36215;&#20316;&#29992;&#65292;&#32780;&#26159;&#38656;&#35201;&#33021;&#22815;&#30452;&#25509;&#21521;&#27169;&#22411;&#21457;&#36865;&#26080;&#38480;&#21046;&#30340;&#26597;&#35810;&#12290;&#21363;&#20351;&#22312;&#27809;&#26377;&#26597;&#35810;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#24448;&#30340;&#25915;&#20987;&#36890;&#24120;&#38656;&#35201;&#25968;&#21315;&#29978;&#33267;&#25968;&#30334;&#19975;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models, such as GPT\nobreakdash-2 and BERT, are often fine-tuned to achieve state-of-the-art performance on a downstream task. One natural example is the ``Smart Reply'' application where a pre-trained model is tuned to provide suggested responses for a given query message. Since the tuning data is often sensitive data such as emails or chat transcripts, it is important to understand and mitigate the risk that the model leaks its tuning data. We investigate potential information leakage vulnerabilities in a typical Smart Reply pipeline. We consider a realistic setting where the adversary can only interact with the underlying model through a front-end interface that constrains what types of queries can be sent to the model. Previous attacks do not work in these settings, but require the ability to send unconstrained queries directly to the model. Even when there are no constraints on the queries, previous attacks typically require thousands, or even millions, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35797;&#39564;&#23545;&#35797;&#39564;&#23398;&#20064;&#22312;&#35789;&#27719;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#29992;&#20102;&#20855;&#26377;&#20998;&#24067;&#35821;&#20041;&#24847;&#20041;&#34920;&#31034;&#30340;&#24515;&#29702;&#35789;&#24211;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#21453;&#24212;&#26102;&#38388;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.00430</link><description>&lt;p&gt;
&#35797;&#39564;&#23545;&#35797;&#39564;&#23398;&#20064;&#22914;&#20309;&#22609;&#36896;&#24515;&#29702;&#35789;&#24211;&#20013;&#30340;&#26144;&#23556;&#65306;&#20351;&#29992;&#32447;&#24615;&#21028;&#21035;&#23398;&#20064;&#27169;&#25311;&#35789;&#27719;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
How trial-to-trial learning shapes mappings in the mental lexicon: Modelling Lexical Decision with Linear Discriminative Learning. (arXiv:2207.00430v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35797;&#39564;&#23545;&#35797;&#39564;&#23398;&#20064;&#22312;&#35789;&#27719;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#29992;&#20102;&#20855;&#26377;&#20998;&#24067;&#35821;&#20041;&#24847;&#20041;&#34920;&#31034;&#30340;&#24515;&#29702;&#35789;&#24211;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#21453;&#24212;&#26102;&#38388;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35797;&#39564;&#23545;&#35797;&#39564;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#34920;&#26126;&#22788;&#29702;&#19968;&#20010;&#21050;&#28608;&#20250;&#24433;&#21709;&#21518;&#32493;&#35797;&#39564;&#20013;&#30340;&#21709;&#24212;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26080;&#21551;&#21160;&#26465;&#20214;&#30340;&#35789;&#27719;&#20915;&#31574;&#23454;&#39564;&#20013;&#26159;&#21542;&#33021;&#22815;&#26816;&#27979;&#21040;&#35797;&#39564;&#23545;&#35797;&#39564;&#23398;&#20064;&#12290;&#20351;&#29992;&#20855;&#26377;&#20998;&#24067;&#35821;&#20041;&#24847;&#20041;&#34920;&#31034;&#30340;&#24515;&#29702;&#35789;&#24211;&#27169;&#22411;&#65292;&#27169;&#25311;&#20102;&#27599;&#20010;&#21463;&#35797;&#32773;&#30340;&#35789;&#27719;&#20915;&#31574;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;Widrow-Hoff&#35268;&#21017;&#27169;&#25311;&#20102;&#38169;&#35823;&#39537;&#21160;&#30340;&#22686;&#37327;&#23398;&#20064;&#12290;&#21033;&#29992;&#26469;&#33258;&#33521;&#22269;&#35789;&#24211;&#39033;&#30446;&#30340;&#25968;&#25454;&#65292;&#20351;&#29992;&#22522;&#20110;&#35813;&#27169;&#22411;&#27169;&#25311;&#30340;&#34913;&#37327;&#25351;&#26631;&#39044;&#27979;&#20102;&#21453;&#24212;&#26102;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trial-to-trial effects have been found in a number of studies, indicating that processing a stimulus influences responses in subsequent trials. A special case are priming effects which have been modelled successfully with error-driven learning (Marsolek, 2008), implying that participants are continuously learning during experiments. This study investigates whether trial-to-trial learning can be detected in an unprimed lexical decision experiment. We used the Discriminative Lexicon Model (DLM; Baayen et al., 2019), a model of the mental lexicon with meaning representations from distributional semantics, which models error-driven incremental learning with the Widrow-Hoff rule. We used data from the British Lexicon Project (BLP; Keuleers et al., 2012) and simulated the lexical decision experiment with the DLM on a trial-by-trial basis for each subject individually. Then, reaction times were predicted with Generalised Additive Models (GAMs), using measures derived from the DLM simulations 
&lt;/p&gt;</description></item></channel></rss>