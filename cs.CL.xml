<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#19978;&#30340;&#21464;&#25442;&#23545;LLMs&#30340;&#28789;&#25935;&#24230;&#25110;&#19981;&#21464;&#24615;&#65292;&#30452;&#25509;&#30417;&#25511;LLMs&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.13651</link><description>&lt;p&gt;
&#33258;&#24102;&#25968;&#25454;&#65281;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Bring Your Own Data! Self-Supervised Evaluation for Large Language Models. (arXiv:2306.13651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#19978;&#30340;&#21464;&#25442;&#23545;LLMs&#30340;&#28789;&#25935;&#24230;&#25110;&#19981;&#21464;&#24615;&#65292;&#30452;&#25509;&#30417;&#25511;LLMs&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#20197;&#21450;&#23427;&#20204;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#34892;&#20026;&#21464;&#24471;&#19981;&#21487;&#25110;&#32570;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#19978;&#30340;&#21464;&#25442;&#23545;LLMs&#30340;&#28789;&#25935;&#24230;&#25110;&#19981;&#21464;&#24615;&#65292;&#30452;&#25509;&#30417;&#25511;LLM&#22312;&#37326;&#22806;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#25110;&#22312;&#27169;&#22411;&#37096;&#32626;&#26399;&#38388;&#36827;&#34892;&#30340;&#27969;&#25968;&#25454;&#30340;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#35780;&#20272;LLMs&#30340;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of Large Language Models (LLMs) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. For example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations. To bypass these drawbacks, we propose a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text. Self-supervised evaluation can directly monitor LLM behavior on datasets collected in the wild or streamed during live model deployment. We demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, tox
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13649</link><description>&lt;p&gt;
GKD&#65306;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models. (arXiv:2306.13649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#36890;&#24120;&#29992;&#20110;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;&#22914;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#33976;&#39311;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#35757;&#32451;&#26399;&#38388;&#36755;&#20986;&#24207;&#21015;&#21644;&#37096;&#32626;&#26102;&#30001;&#23398;&#29983;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#20043;&#38388;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#65288;2&#65289;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#33021;&#19981;&#22815;&#34920;&#36798;&#32769;&#24072;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#12290;GKD&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;GKD&#36890;&#36807;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26469;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36825;&#20123;&#31163;&#25955;&#24230;&#38598;&#20013;&#20110;&#29983;&#25104;&#21487;&#33021;&#31526;&#21512;&#32769;&#24072;&#20998;&#24067;&#30340;&#23398;&#29983;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;GKD&#20248;&#20110;&#24120;&#29992;&#30340;LLM&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is commonly used for compressing neural networks to reduce their inference cost and memory footprint. However, current distillation methods for auto-regressive models, such as generative language models (LMs), suffer from two key issues: (1) distribution mismatch between output sequences during training and the sequences generated by the student during its deployment, and (2) model under-specification, where the student model may not be expressive enough to fit the teacher's distribution. To address these issues, we propose Generalized Knowledge Distillation (GKD). GKD mitigates distribution mismatch by sampling output sequences from the student during training. Furthermore, GKD handles model under-specification by optimizing alternative divergences, such as reverse KL, that focus on generating samples from the student that are likely under the teacher's distribution. We demonstrate that GKD outperforms commonly-used approaches for distilling LLMs on summarizatio
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.13596</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#36793;&#32536;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#20063;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#24778;&#20154;&#25104;&#21151;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#32972;&#21518;&#30340;&#29702;&#35770;&#21407;&#21017;&#23578;&#19981;&#28165;&#26970;&#65292;&#29305;&#21035;&#26159;&#23427;&#30340;&#38750;&#20984;&#20248;&#21270;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21019;&#24615;&#30340;softmax-attention&#27169;&#22411;$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$&#65292;&#20854;&#20013;$\boldsymbol{X}$&#26159;&#26631;&#35760;&#24207;&#21015;&#65292;$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$&#26159;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$\boldsymbol{p}$&#25110;&#31561;&#20215;&#30340;$\boldsymbol{W}$&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#20250;&#27839;&#30528;&#26041;&#21521;&#25910;&#25947;&#21040;&#20998;&#38548;&#8220;&#23616;&#37096;&#26368;&#20248;&#8221;&#26631;&#35760;&#21644;&#8220;&#38750;&#26368;&#20248;&#8221;&#26631;&#35760;&#30340;&#26368;&#22823;&#36793;&#32536;&#35299;&#12290;&#36825;&#26126;&#30830;&#22320;&#24418;&#24335;&#21270;&#20102;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#31181;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19968;&#33324;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#23884;&#20837;$\boldsymbol{Xv}$&#21644;$\texttt{softmax}(\boldsymbol{XWp})$&#31934;&#32454;&#22320;&#34920;&#24449;&#26631;&#35760;&#30340;&#8220;&#26368;&#20248;&#24615;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20219;&#21153;&#24230;&#37327;&#35774;&#35745;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13588</link><description>&lt;p&gt;
&#31995;&#32479;&#32423;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
System-Level Natural Language Feedback. (arXiv:2306.13588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20219;&#21153;&#24230;&#37327;&#35774;&#35745;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#21253;&#21547;&#20102;&#20016;&#23500;&#30340;&#29992;&#25143;&#20307;&#39564;&#20449;&#24687;&#12290;&#29616;&#26377;&#30740;&#31350;&#32858;&#28966;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#21453;&#39304;&#29992;&#20110;&#32454;&#21270;&#29305;&#23450;&#20363;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#31995;&#32479;&#33539;&#22260;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#26159;&#36890;&#36807;&#20197;&#19979;&#20004;&#26041;&#38754;&#23454;&#29616;&#30340;&#65306;(i) &#20219;&#21153;&#24230;&#37327;&#35774;&#35745;; (ii) &#29992;&#20110;&#25913;&#36827;&#27169;&#22411;&#21709;&#24212;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#26469;&#25913;&#36827;&#25628;&#32034;&#26597;&#35810;&#29983;&#25104;&#21644;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#32452;&#21512;&#24102;&#26469;&#20102;&#36827;&#19968;&#27493;&#30340;&#25910;&#30410;&#65292;&#24182;&#19988;&#30001;&#20154;&#31867;&#25776;&#20889;&#30340;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#23548;&#33268;&#27604;GPT-3.5&#25776;&#20889;&#30340;&#21453;&#39304;&#26356;&#21152;&#25166;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language (NL) feedback contains rich information about the user experience. Existing studies focus on an instance-level approach, where feedback is used to refine specific examples, disregarding its system-wide application. This paper proposes a general framework for unlocking the system-level use of NL feedback. We show how to use feedback to formalize system-level design decisions in a human-in-the-loop-process -- in order to produce better models. In particular this is done through: (i) metric design for tasks; and (ii) language model prompt design for refining model responses. We conduct two case studies of this approach for improving search query generation and dialog response generation, demonstrating the effectiveness of the use of system-level feedback. We show the combination of system-level feedback and instance-level feedback brings further gains, and that human written instance-level feedback results in more grounded refinements than GPT-3.5 written ones, underlying
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.13549</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22823;&#33041;&#25191;&#34892;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;MLLM &#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#22914;&#22522;&#20110;&#22270;&#20687;&#32534;&#20889;&#25925;&#20107;&#21644;&#26080;OCR&#25968;&#23398;&#25512;&#29702;&#31561;&#65292;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#24456;&#23569;&#35265;&#65292;&#34920;&#26126;&#20102;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;&#26412;&#25991;&#26088;&#22312;&#36861;&#36394;&#21644;&#24635;&#32467; MLLM &#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MLLM &#30340;&#26500;&#25104;&#65292;&#27010;&#36848;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#65288;M-IT&#65289;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;M-ICL&#65289;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#65288;M-CoT&#65289;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#65288;LAVR&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#37492;&#20110; MLLM &#26102;&#20195;&#25165;&#21018;&#21018;&#24320;&#22987;&#65292;&#25105;&#20204;&#20250;&#19981;&#26029;&#26356;&#26032;&#36825;&#20010;&#32508;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#26356;&#22810;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30693;&#35782;&#27880;&#20837;&#21040;&#21464;&#21387;&#22120;&#27169;&#22411;&#19981;&#21516;&#32452;&#20214;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#24443;&#24213;&#22320;&#20998;&#26512;&#30693;&#35782;&#27880;&#20837;&#23545;&#27599;&#20010;&#32452;&#20214;&#21644;&#32452;&#20214;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.13501</link><description>&lt;p&gt;
&#30693;&#35782;&#27880;&#20837;&#33258;&#27880;&#24847;&#21147;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Infused Self Attention Transformers. (arXiv:2306.13501v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30693;&#35782;&#27880;&#20837;&#21040;&#21464;&#21387;&#22120;&#27169;&#22411;&#19981;&#21516;&#32452;&#20214;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#24443;&#24213;&#22320;&#20998;&#26512;&#30693;&#35782;&#27880;&#20837;&#23545;&#27599;&#20010;&#32452;&#20214;&#21644;&#32452;&#20214;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#20854;&#20351;&#29992;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#25429;&#25417;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#38750;&#27809;&#26377;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#38480;&#21046;&#21253;&#25324;&#24187;&#35273;&#65292;&#21363;&#23427;&#20204;&#20135;&#29983;&#39640;&#32622;&#20449;&#24230;&#30340;&#38169;&#35823;&#36755;&#20986;&#65292;&#20197;&#21450;&#23545;&#20154;&#31867;&#29992;&#25143;&#29983;&#25104;&#26080;&#29992;&#21644;&#19981;&#23433;&#20840;&#36755;&#20986;&#30340;&#23545;&#40784;&#38382;&#39064;&#12290;&#36825;&#20123;&#38480;&#21046;&#28304;&#20110;&#25968;&#25454;&#20013;&#38544;&#21547;&#21644;&#32570;&#22833;&#19978;&#19979;&#25991;&#30340;&#32570;&#20047;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#21033;&#29992;&#26469;&#33258;&#30693;&#35782;&#22270;&#35889;&#30340;&#22806;&#37096;&#30693;&#35782;&#26469;&#25552;&#20379;&#24517;&#35201;&#30340;&#38468;&#21152;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#20020;&#26102;&#24615;&#36136;&#20351;&#24471;&#22312;&#20998;&#26512;&#30693;&#35782;&#27880;&#20837;&#23545;&#21464;&#21387;&#22120;&#30340;&#35768;&#22810;&#37096;&#20998;&#25110;&#32452;&#20214;&#20135;&#29983;&#30340;&#24433;&#21709;&#20197;&#21450;&#32452;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#26041;&#38754;&#22256;&#38590;&#37325;&#37325;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30693;&#35782;&#27880;&#20837;&#21040;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#19981;&#21516;&#32452;&#20214;&#20013;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#35813;&#25552;&#20986;&#30340;&#26041;&#27861;&#20801;&#35768;&#26356;&#24443;&#24213;&#22320;&#20998;&#26512;&#30693;&#35782;&#27880;&#20837;&#23545;&#27599;&#20010;&#32452;&#20214;&#20197;&#21450;&#32452;&#20214;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#30528;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models have achieved impressive success in various natural language processing tasks due to their ability to capture complex dependencies and contextual information using self-attention mechanisms. However, they are not without limitations. These limitations include hallucinations, where they produce incorrect outputs with high confidence, and alignment issues, where they generate unhelpful and unsafe outputs for human users. These limitations stem from the absence of implicit and missing context in the data alone. To address this, researchers have explored augmenting these models with external knowledge from knowledge graphs to provide the necessary additional context. However, the ad-hoc nature of existing methods makes it difficult to properly analyze the effects of knowledge infusion on the many moving parts or components of a transformer. This paper introduces a systematic method for infusing knowledge into different components of a transformer-based mod
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;LeakDistill&#65292;&#23427;&#20351;&#29992;&#32467;&#26500;&#36866;&#37197;&#22120;&#23558;&#22270;&#24418;&#20449;&#24687;&#26126;&#30830;&#24182;&#20837;&#21040;&#23398;&#20064;&#30340;&#34920;&#31034;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;AMR&#35299;&#26512;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#21333;&#35789;&#21040;&#33410;&#28857;&#23545;&#40784;&#23558;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#23884;&#20837;&#32534;&#30721;&#22120;&#20013;&#65292;&#21363;&#20351;&#19981;&#20351;&#29992;&#20854;&#20182;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;AMR&#35299;&#26512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13467</link><description>&lt;p&gt;
&#23558;&#22270;&#34920;&#20449;&#24687;&#34701;&#20837;&#22522;&#20110;Transformer&#30340;AMR&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Incorporating Graph Information in Transformer-based AMR Parsing. (arXiv:2306.13467v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;LeakDistill&#65292;&#23427;&#20351;&#29992;&#32467;&#26500;&#36866;&#37197;&#22120;&#23558;&#22270;&#24418;&#20449;&#24687;&#26126;&#30830;&#24182;&#20837;&#21040;&#23398;&#20064;&#30340;&#34920;&#31034;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;AMR&#35299;&#26512;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#21333;&#35789;&#21040;&#33410;&#28857;&#23545;&#40784;&#23558;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#23884;&#20837;&#32534;&#30721;&#22120;&#20013;&#65292;&#21363;&#20351;&#19981;&#20351;&#29992;&#20854;&#20182;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;AMR&#35299;&#26512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#24847;&#20041;&#34920;&#36798;&#65288;AMR&#65289;&#26159;&#19968;&#31181;&#35821;&#20041;&#35299;&#26512;&#24418;&#24335;&#20027;&#20041;&#65292;&#26088;&#22312;&#25552;&#20379;&#34920;&#31034;&#32473;&#23450;&#25991;&#26412;&#30340;&#35821;&#20041;&#22270;&#34920;&#25277;&#35937;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#22522;&#20110;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;BART&#25110;T5&#65292;&#36890;&#36807;Teacher Forcing&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20174;&#21477;&#23376;&#24471;&#21040;&#32447;&#24615;&#21270;&#29256;&#26412;&#30340;AMR&#22270;&#34920;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LeakDistill&#65292;&#19968;&#31181;&#25506;&#32034;&#36716;&#25442;&#22120;&#26550;&#26500;&#20462;&#25913;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#65292;&#20351;&#29992;&#32467;&#26500;&#36866;&#37197;&#22120;&#26126;&#30830;&#23558;&#22270;&#24418;&#20449;&#24687;&#24182;&#20837;&#21040;&#23398;&#20064;&#30340;&#34920;&#31034;&#20013;&#65292;&#20197;&#25552;&#39640;AMR&#35299;&#26512;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#21333;&#35789;&#21040;&#33410;&#28857;&#23545;&#40784;&#23558;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#23884;&#20837;&#32534;&#30721;&#22120;&#20013;&#65292;&#21363;&#20351;&#19981;&#20351;&#29992;&#20854;&#20182;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;AMR&#35299;&#26512;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;\url{this http URL}&#19978;&#21457;&#24067;&#20102;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that aims at providing a semantic graph abstraction representing a given text. Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version of the AMR graph from a sentence. In this paper, we present LeakDistill, a model and method that explores a modification to the Transformer architecture, using structural adapters to explicitly incorporate graph information into the learned representations and improve AMR parsing performance. Our experiments show how, by employing word-to-node alignment to embed graph structural information into the encoder at training time, we can obtain state-of-the-art AMR parsing through self-knowledge distillation, even without the use of additional data. We release the code at \url{this http URL}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21322;&#36879;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#65292;&#40723;&#21169;&#27169;&#22411;&#29983;&#25104;&#26356;&#35814;&#32454;&#30340;&#38271;&#23383;&#24149;&#12290;</title><link>http://arxiv.org/abs/2306.13460</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#36879;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23398;&#20064;&#25551;&#36848;&#24615;&#22270;&#20687;&#23383;&#24149;
&lt;/p&gt;
&lt;p&gt;
Learning Descriptive Image Captioning via Semipermeable Maximum Likelihood Estimation. (arXiv:2306.13460v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21322;&#36879;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#65292;&#40723;&#21169;&#27169;&#22411;&#29983;&#25104;&#26356;&#35814;&#32454;&#30340;&#38271;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#26088;&#22312;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#35270;&#35273;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26159;&#35757;&#32451;&#30446;&#26631;&#65292;&#23383;&#24149;&#27169;&#22411;&#22312;&#39044;&#27979;&#19982;&#26631;&#31614;&#19981;&#21305;&#37197;&#26102;&#20250;&#21463;&#21040;&#24809;&#32602;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21322;&#36879;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;SMILE&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#20016;&#23500;&#24615;&#20248;&#21270;&#21516;&#26102;&#38459;&#27490;&#31616;&#27905;&#24615;&#20248;&#21270;&#65292;&#20174;&#32780;&#40723;&#21169;&#27169;&#22411;&#29983;&#25104;&#26356;&#35814;&#32454;&#30340;&#38271;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image captioning aims to describe visual content in natural language. As 'a picture is worth a thousand words', there could be various correct descriptions for an image. However, with maximum likelihood estimation as the training objective, the captioning model is penalized whenever its prediction mismatches with the label. For instance, when the model predicts a word expressing richer semantics than the label, it will be penalized and optimized to prefer more concise expressions, referred to as conciseness optimization. In contrast, predictions that are more concise than labels lead to richness optimization. Such conflicting optimization directions could eventually result in the model generating general descriptions. In this work, we introduce Semipermeable MaxImum Likelihood Estimation (SMILE), which allows richness optimization while blocking conciseness optimization, thus encouraging the model to generate longer captions with more details. Extensive experiments on two mainstream im
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Retrieval-Pretrained Transformer&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#32852;&#21512;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22120;&#26469;&#27169;&#25311;&#38271;&#25991;&#26412;&#12290;&#27169;&#22411;&#21487;&#20197;&#35745;&#31639;&#25991;&#26412;&#22359;&#30340;&#26597;&#35810;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26816;&#32034;&#21069;&#38754;&#30340;&#22359;&#65292;&#20174;&#32780;&#34701;&#21512;&#20449;&#24687;&#20197;&#39044;&#27979;&#19979;&#19968;&#20010;&#30446;&#26631;&#22359;&#12290;&#26816;&#32034;&#22120;&#20351;&#29992;&#19968;&#20010;&#35821;&#20041;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#30446;&#26631;&#26159;&#26816;&#32034;&#37027;&#20123;&#22686;&#21152;&#19979;&#19968;&#20010;&#22359;&#27010;&#29575;&#30340;&#22359;&#12290;</title><link>http://arxiv.org/abs/2306.13421</link><description>&lt;p&gt;
&#33258;&#26816;&#32034;&#30340;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Long-range Language Modeling with Self-retrieval. (arXiv:2306.13421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Retrieval-Pretrained Transformer&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#32852;&#21512;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22120;&#26469;&#27169;&#25311;&#38271;&#25991;&#26412;&#12290;&#27169;&#22411;&#21487;&#20197;&#35745;&#31639;&#25991;&#26412;&#22359;&#30340;&#26597;&#35810;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26816;&#32034;&#21069;&#38754;&#30340;&#22359;&#65292;&#20174;&#32780;&#34701;&#21512;&#20449;&#24687;&#20197;&#39044;&#27979;&#19979;&#19968;&#20010;&#30446;&#26631;&#22359;&#12290;&#26816;&#32034;&#22120;&#20351;&#29992;&#19968;&#20010;&#35821;&#20041;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#30446;&#26631;&#26159;&#26816;&#32034;&#37027;&#20123;&#22686;&#21152;&#19979;&#19968;&#20010;&#22359;&#27010;&#29575;&#30340;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22522;&#20110;&#26816;&#32034;&#36741;&#21161;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20294;&#26159;&#65292;&#36890;&#24120;&#26816;&#32034;&#22120;&#24182;&#19981;&#26159;&#20316;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#26412;&#22320;&#32452;&#20214;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#30340;&#65292;&#32780;&#26159;&#34987;&#28155;&#21152;&#21040;&#24050;&#32463;&#39044;&#35757;&#32451;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36825;&#38480;&#21046;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22120;&#30456;&#20114;&#36866;&#24212;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Retrieval-Pretrained Transformer (RPT)&#65292;&#19968;&#31181;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#26816;&#32034;&#36741;&#21161;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#38271;&#25991;&#26412;&#12290;&#32473;&#23450;&#19968;&#20010;&#26368;&#36817;&#22312;&#38271;&#25991;&#26723;&#20013;&#29983;&#25104;&#30340;&#25991;&#26412;&#22359;&#65292;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#26597;&#35810;&#34920;&#31034;&#65292;&#28982;&#21518;&#29992;&#23427;&#26469;&#26816;&#32034;&#25991;&#26723;&#20013;&#26356;&#26089;&#30340;&#22359;&#65292;&#36825;&#20123;&#22359;&#21487;&#33021;&#36328;&#36234;&#25968;&#19975;&#20010;&#26631;&#35760;&#12290;&#26816;&#32034;&#21040;&#30340;&#22359;&#20013;&#30340;&#20449;&#24687;&#34987;&#34701;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#20013;&#65292;&#20197;&#39044;&#27979;&#19979;&#19968;&#20010;&#30446;&#26631;&#22359;&#12290;&#25105;&#20204;&#29992;&#19968;&#20010;&#35821;&#20041;&#30446;&#26631;&#26469;&#35757;&#32451;&#26816;&#32034;&#22120;&#32452;&#20214;&#65292;&#35813;&#30446;&#26631;&#30340;&#30446;&#30340;&#26159;&#26816;&#32034;&#22686;&#21152;&#19979;&#19968;&#20010;&#22359;&#27010;&#29575;&#30340;&#22359;&#65292;&#26681;&#25454;&#21442;&#32771;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models (LMs) have received much attention recently. However, typically the retriever is not trained jointly as a native component of the LM, but added to an already-pretrained LM, which limits the ability of the LM and the retriever to adapt to one another. In this work, we propose the Retrieval-Pretrained Transformer (RPT), an architecture and training procedure for jointly training a retrieval-augmented LM from scratch for the task of modeling long texts. Given a recently generated text chunk in a long document, the LM computes query representations, which are then used to retrieve earlier chunks in the document, located potentially tens of thousands of tokens before. Information from retrieved chunks is fused into the LM representations to predict the next target chunk. We train the retriever component with a semantic objective, where the goal is to retrieve chunks that increase the probability of the next chunk, according to a reference LM. We evaluate 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#21270;&#23398;&#19987;&#21033;&#20013;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#30740;&#31350;&#20102;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#22122;&#22768;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#26088;&#22312;&#25552;&#39640;&#20854;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13379</link><description>&lt;p&gt;
&#21387;&#21147;&#27979;&#35797;BERT&#22312;&#21270;&#23398;&#19987;&#21033;&#20013;&#30340;&#22238;&#24212;&#25552;&#21462;&#20013;&#30340;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Stress Testing BERT Anaphora Resolution Models for Reaction Extraction in Chemical Patents. (arXiv:2306.13379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#21270;&#23398;&#19987;&#21033;&#20013;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#30740;&#31350;&#20102;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#22122;&#22768;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#26088;&#22312;&#25552;&#39640;&#20854;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#21270;&#23398;&#19987;&#21033;&#20986;&#29256;&#29289;&#21644;&#21450;&#26102;&#33719;&#21462;&#20854;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#20419;&#20351;&#33258;&#21160;&#21270;&#20174;&#21270;&#23398;&#19987;&#21033;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290; &#25351;&#20195;&#28040;&#35299;&#26159;&#32508;&#21512;&#20449;&#24687;&#25552;&#21462;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#20110;&#25552;&#21462;&#21453;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290; &#22312;&#21270;&#23398;&#19987;&#21033;&#20013;&#65292;&#23384;&#22312;&#20116;&#31181;&#24863;&#20852;&#36259;&#30340;&#25351;&#20195;&#20851;&#31995;&#65306;&#20849;&#25351;&#65292;&#36716;&#25442;&#65292;&#21453;&#24212;&#30456;&#20851;&#65292;&#22788;&#29702;&#21644;&#21253;&#21547;&#12290; &#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#22122;&#22768;&#29615;&#22659;&#20013;&#29992;&#20110;&#21270;&#23398;&#19987;&#21033;&#20013;&#21453;&#24212;&#25991;&#26412;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high volume of published chemical patents and the importance of a timely acquisition of their information gives rise to automating information extraction from chemical patents. Anaphora resolution is an important component of comprehensive information extraction, and is critical for extracting reactions. In chemical patents, there are five anaphoric relations of interest: co-reference, transformed, reaction associated, work up, and contained. Our goal is to investigate how the performance of anaphora resolution models for reaction texts in chemical patents differs in a noise-free and noisy environment and to what extent we can improve the robustness against noise of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#25216;&#26415;&#65288;&#21253;&#25324;LSTM&#12289;T5&#12289;Pegasus&#12289;BART&#21644;BART-Large&#27169;&#22411;&#65289;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;&#31616;&#21382;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#24494;&#35843;&#21518;&#30340;BART-Large&#27169;&#22411;&#25928;&#26524;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.13315</link><description>&lt;p&gt;
&#21033;&#29992;&#20808;&#36827;&#30340;NLP&#21464;&#24418;&#22120;&#21644;LSTM&#36827;&#34892;&#31616;&#21382;&#30340;&#25552;&#21462;&#24615;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Abstractive Text Summarization for Resumes With Cutting Edge NLP Transformers and LSTM. (arXiv:2306.13315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#25216;&#26415;&#65288;&#21253;&#25324;LSTM&#12289;T5&#12289;Pegasus&#12289;BART&#21644;BART-Large&#27169;&#22411;&#65289;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;&#31616;&#21382;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#24494;&#35843;&#21518;&#30340;BART-Large&#27169;&#22411;&#25928;&#26524;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#22823;&#37327;&#30340;&#25991;&#26412;&#20449;&#24687;&#21387;&#32553;&#25104;&#31616;&#27905;&#36830;&#36143;&#30340;&#25688;&#35201;&#12290;&#38543;&#30528;&#20869;&#23481;&#30340;&#25351;&#25968;&#22686;&#38271;&#21644;&#39640;&#25928;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#30340;&#38656;&#27714;&#65292;&#25991;&#26412;&#25688;&#35201;&#22312;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#35780;&#20272;&#20102;LSTM&#21644;&#39044;&#35757;&#32451;&#30340;T5&#12289;Pegasus&#12289;BART&#21644;BART-Large&#27169;&#22411;&#22312;&#24320;&#28304;&#25968;&#25454;&#38598;&#65288;Xsum&#12289;CNN/Daily Mail&#12289;&#20122;&#39532;&#36874;&#31934;&#32654;&#39135;&#21697;&#35780;&#35770;&#21644;&#26032;&#38395;&#25688;&#35201;&#65289;&#21644;&#20934;&#22791;&#30340;&#31616;&#21382;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#31616;&#21382;&#25968;&#25454;&#38598;&#21253;&#25324;&#35768;&#22810;&#20449;&#24687;&#65292;&#22914;&#35821;&#35328;&#12289;&#25945;&#32946;&#12289;&#32463;&#39564;&#12289;&#20010;&#20154;&#20449;&#24687;&#12289;&#25216;&#33021;&#31561;&#65292;&#25968;&#25454;&#38598;&#20013;&#21253;&#25324;&#20102;75&#20221;&#31616;&#21382;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;&#31616;&#21382;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#20351;&#29992;&#31616;&#21382;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;LSTM&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#12290;&#20351;&#29992;&#31616;&#21382;&#25968;&#25454;&#38598;&#24494;&#35843;&#30340;BART-Large&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text summarization is a fundamental task in natural language processing that aims to condense large amounts of textual information into concise and coherent summaries. With the exponential growth of content and the need to extract key information efficiently, text summarization has gained significant attention in recent years. In this study, LSTM and pre-trained T5, Pegasus, BART and BART-Large model performances were evaluated on the open source dataset (Xsum, CNN/Daily Mail, Amazon Fine Food Review and News Summary) and the prepared resume dataset. This resume dataset consists of many information such as language, education, experience, personal information, skills, and this data includes 75 resumes. The primary objective of this research was to classify resume text. Various techniques such as LSTM, pre-trained models, and fine-tuned models were assessed using a dataset of resumes. The BART-Large model fine-tuned with the resume dataset gave the best performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#30456;&#20114;&#25351;&#23548;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#36827;&#34892;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#22495;&#23569;&#26679;&#26412;&#19977;&#20803;&#32452;&#25552;&#21462;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#26377;&#31454;&#20105;&#21147;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.13310</link><description>&lt;p&gt;
&#30456;&#20114;&#25351;&#23548;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Mutually Guided Few-shot Learning for Relational Triple Extraction. (arXiv:2306.13310v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13310
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#30456;&#20114;&#25351;&#23548;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#36827;&#34892;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#22495;&#23569;&#26679;&#26412;&#19977;&#20803;&#32452;&#25552;&#21462;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#26377;&#31454;&#20105;&#21147;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#21253;&#21547;&#35768;&#22810;&#23454;&#20307;-&#20851;&#31995;-&#23454;&#20307;&#19977;&#20803;&#32452;&#65292;&#20026;&#19979;&#28216;&#24212;&#29992;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#19977;&#20803;&#32452;&#24050;&#32463;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#22823;&#37096;&#20998;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#23454;&#20363;&#12290;&#24403;&#21482;&#26377;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#24615;&#33021;&#23558;&#24613;&#21095;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#20114;&#25351;&#23548;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#36827;&#34892;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#65288;MG-FTE&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#21547;&#19968;&#20010;&#20197;&#23454;&#20307;&#20026;&#23548;&#21521;&#30340;&#20851;&#31995;&#21407;&#22411;&#35299;&#30721;&#22120;&#65292;&#39318;&#20808;&#23545;&#20851;&#31995;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#21450;&#19968;&#20010;&#20197;&#20851;&#31995;&#20026;&#23548;&#21521;&#30340;&#23454;&#20307;&#21407;&#22411;&#35299;&#30721;&#22120;&#65292;&#26681;&#25454;&#20998;&#31867;&#30340;&#20851;&#31995;&#25552;&#21462;&#23454;&#20307;&#12290;&#20026;&#20102;&#36830;&#32467;&#23454;&#20307;&#19982;&#20851;&#31995;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21407;&#22411;&#23618;&#34701;&#21512;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#23454;&#20307;&#25552;&#21462;&#21644;&#20851;&#31995;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#22495;&#23569;&#26679;&#26412;&#19977;&#20803;&#32452;&#25552;&#21462;&#20219;&#21153;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#19977;&#20803;&#32452;&#25552;&#21462;&#20219;&#21153;&#20013;&#20248;&#20110;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#20063;&#33021;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs), containing many entity-relation-entity triples, provide rich information for downstream applications. Although extracting triples from unstructured texts has been widely explored, most of them require a large number of labeled instances. The performance will drop dramatically when only few labeled data are available. To tackle this problem, we propose the Mutually Guided Few-shot learning framework for Relational Triple Extraction (MG-FTE). Specifically, our method consists of an entity-guided relation proto-decoder to classify the relations firstly and a relation-guided entity proto-decoder to extract entities based on the classified relations. To draw the connection between entity and relation, we design a proto-level fusion module to boost the performance of both entity extraction and relation classification. Moreover, a new cross-domain few-shot triple extraction task is introduced. Extensive experiments show that our method outperforms many state-of-the-art
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;Conformer Transducer&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24314;&#31435;&#39640;&#25928;&#32039;&#20945;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#27719;&#32858;&#23618;&#23454;&#29616;&#36328;&#35805;&#35821;&#30340;&#20449;&#24687;&#38598;&#25104;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.13307</link><description>&lt;p&gt;
&#20026;Conformer Transducer&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24314;&#31435;&#39640;&#25928;&#32039;&#20945;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Towards Effective and Compact Contextual Representation for Conformer Transducer Speech Recognition Systems. (arXiv:2306.13307v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;Conformer Transducer&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24314;&#31435;&#39640;&#25928;&#32039;&#20945;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#27719;&#32858;&#23618;&#23454;&#29616;&#36328;&#35805;&#35821;&#30340;&#20449;&#24687;&#38598;&#25104;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20027;&#35201;&#22312;&#35805;&#35821;&#32423;&#21035;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#21487;&#20197;&#32435;&#20837;&#38271;&#33539;&#22260;&#30340;&#36328;&#35805;&#35821;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#22312;Conformer-Transducer&#32534;&#30721;&#22120;&#20013;&#20351;&#29992;&#29305;&#27530;&#35774;&#35745;&#30340;&#27880;&#24847;&#21147;&#27719;&#32858;&#23618;&#65292;&#36890;&#36807;&#39640;&#25928;&#32531;&#23384;&#30340;&#21069;&#38754;&#35805;&#35821;&#21382;&#21490;&#21521;&#37327;&#65292;&#23398;&#20064;&#20102;&#32039;&#20945;&#30340;&#20302;&#32500;&#36328;&#35805;&#35821;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;&#22312;1000&#23567;&#26102;&#30340;Gigaspeech&#35821;&#26009;&#24211;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#19978;&#19979;&#25991;&#21270;&#27969;Conformer-Transducer&#22312;&#24320;&#21457;&#21644;&#27979;&#35797;&#25968;&#25454;&#19978;&#37117;&#27604;&#20165;&#20351;&#29992;&#35805;&#35821;&#20869;&#37096;&#19978;&#19979;&#25991;&#30340;&#22522;&#32447;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#23383;&#38169;&#29575;&#38477;&#20302;&#65288;0.7&#65285;&#21040;0.5&#65285;&#32477;&#23545;&#65292;4.3&#65285;&#21040;3.1&#65285;&#30456;&#23545;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current ASR systems are mainly trained and evaluated at the utterance level. Long range cross utterance context can be incorporated. A key task is to derive a suitable compact representation of the most relevant history contexts. In contrast to previous researches based on either LSTM-RNN encoded histories that attenuate the information from longer range contexts, or frame level concatenation of transformer context embeddings, in this paper compact low-dimensional cross utterance contextual features are learned in the Conformer-Transducer Encoder using specially designed attention pooling layers that are applied over efficiently cached preceding utterances history vectors. Experiments on the 1000-hr Gigaspeech corpus demonstrate that the proposed contextualized streaming Conformer-Transducers outperform the baseline using utterance internal context only with statistically significant WER reductions of 0.7% to 0.5% absolute (4.3% to 3.1% relative) on the dev and test data.
&lt;/p&gt;</description></item><item><title>DiversiGATE&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#27719;&#38598;&#20102;&#22810;&#31181;LLM&#39564;&#35777;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#19968;&#33268;&#24615;&#12289;&#25968;&#23398;&#25552;&#31034;&#21644;WebGPT&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21512;&#35813;&#26694;&#26550;&#30340;&#26032;&#27169;&#22411;&#8220;SelfLearner&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#33258;&#24049;&#30340;&#36755;&#20986;&#20013;&#23398;&#20064;&#24182;&#20248;&#21270;&#24615;&#33021;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;GSM8K&#22522;&#20934;&#27979;&#35797;&#19978;&#25552;&#39640;&#20102;7%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13230</link><description>&lt;p&gt;
DiversiGATE: &#19968;&#20010;&#21487;&#38752;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20840;&#38754;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DiversiGATE: A Comprehensive Framework for Reliable Large Language Models. (arXiv:2306.13230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13230
&lt;/p&gt;
&lt;p&gt;
DiversiGATE&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#27719;&#38598;&#20102;&#22810;&#31181;LLM&#39564;&#35777;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#19968;&#33268;&#24615;&#12289;&#25968;&#23398;&#25552;&#31034;&#21644;WebGPT&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21512;&#35813;&#26694;&#26550;&#30340;&#26032;&#27169;&#22411;&#8220;SelfLearner&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#33258;&#24049;&#30340;&#36755;&#20986;&#20013;&#23398;&#20064;&#24182;&#20248;&#21270;&#24615;&#33021;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;GSM8K&#22522;&#20934;&#27979;&#35797;&#19978;&#25552;&#39640;&#20102;7%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DiversiGATE&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#27719;&#38598;LLM&#39564;&#35777;&#30340;&#22810;&#31181;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#22810;&#26679;&#21270;&#21644;&#32858;&#21512;&#65292;&#22312;&#29616;&#26377;&#30340;&#39564;&#35777;&#26041;&#27861;&#19978;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#20363;&#22914;&#33258;&#19968;&#33268;&#24615;&#12289;&#25968;&#23398;&#25552;&#31034;&#21644;WebGPT&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#8220;SelfLearner&#8221;&#27169;&#22411;&#65292;&#31526;&#21512;DiversiGATE&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#33258;&#24049;&#30340;&#36755;&#20986;&#20013;&#23398;&#20064;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#19981;&#26029;&#23436;&#21892;&#20854;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;SelfLearner&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#26415;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;GSM8K&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;LLMs&#65292;&#22312;GSM8K&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#21487;&#35266;&#30340;54.8%-&gt;61.8%&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce DiversiGATE, a unified framework that consolidates diverse methodologies for LLM verification. The proposed framework comprises two main components: Diversification and Aggregation which provide a holistic perspective on existing verification approaches, such as Self-Consistency, Math Prompter and WebGPT. Furthermore, we propose a novel `SelfLearner' model that conforms to the DiversiGATE framework which can learn from its own outputs and refine its performance over time, leading to improved accuracy. To evaluate the effectiveness of SelfLearner, we conducted a rigorous series of experiments, including tests on synthetic data as well as on popular arithmetic reasoning benchmarks such as GSM8K. Our results demonstrate that our approach outperforms traditional LLMs, achieving a considerable 54.8% -&gt; 61.8% improvement on the GSM8K benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#23558;&#22270;&#20687;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38544;&#24739;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25351;&#20986;&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#21644;&#39640;&#32500;&#24615;&#26159;&#23545;&#25239;&#25915;&#20987;&#30340;&#20016;&#23500;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#20026;&#35270;&#35273;&#25915;&#20987;&#32773;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#23454;&#29616;&#23545;&#25239;&#30446;&#26631;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13213</link><description>&lt;p&gt;
&#35270;&#35273;&#23545;&#25239;&#26679;&#26412;&#36234;&#29425;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38544;&#24739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Visual Adversarial Examples Jailbreak Large Language Models. (arXiv:2306.13213v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#23558;&#22270;&#20687;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38544;&#24739;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25351;&#20986;&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#21644;&#39640;&#32500;&#24615;&#26159;&#23545;&#25239;&#25915;&#20987;&#30340;&#20016;&#23500;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#20026;&#35270;&#35273;&#25915;&#20987;&#32773;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#23454;&#29616;&#23545;&#25239;&#30446;&#26631;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#22270;&#20687;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#39640;&#24230;&#20851;&#27880;&#12290;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#26222;&#21450;&#65292;&#20363;&#22914;Flamingo&#12289;BLIP-2&#21644;GPT-4&#65292;&#26631;&#24535;&#30528;&#35270;&#35273;&#21644;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#20808;&#36827;&#21457;&#23637;&#30456;&#20114;&#34701;&#21512;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#28041;&#21450;&#30340;&#39118;&#38505;&#20173;&#26410;&#24471;&#21040;&#35814;&#32454;&#30740;&#31350;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#36825;&#19968;&#36235;&#21183;&#30340;&#23433;&#20840;&#38544;&#24739;&#12290;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#65292;&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#21644;&#39640;&#32500;&#24615;&#22312;&#26412;&#36136;&#19978;&#20351;&#20854;&#25104;&#20026;&#23545;&#25239;&#25915;&#20987;&#30340;&#20016;&#23500;&#39046;&#22495;&#65292;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#25193;&#22823;&#20102;LLMs&#30340;&#25915;&#20987;&#38754;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24378;&#35843;&#65292;LLMs&#30340;&#24191;&#27867;&#21151;&#33021;&#20063;&#20026;&#35270;&#35273;&#25915;&#20987;&#32773;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#23454;&#29616;&#23545;&#25239;&#30446;&#26631;&#30340;&#21487;&#33021;&#24615;&#65292;&#23558;&#23433;&#20840;&#22833;&#36133;&#30340;&#24433;&#21709;&#25193;&#23637;&#21040;&#20102;&#31616;&#21333;&#30340;&#38169;&#35823;&#20998;&#31867;&#20043;&#22806;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#20123;&#39118;&#38505;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;VLM&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#23545;&#25239;&#24615;&#26679;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a surge of interest in introducing vision into Large Language Models (LLMs). The proliferation of large Visual Language Models (VLMs), such as Flamingo, BLIP-2, and GPT-4, signifies an exciting convergence of advancements in both visual and language foundation models. Yet, the risks associated with this integrative approach are largely unexamined. In this paper, we shed light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the additional visual input space intrinsically makes it a fertile ground for adversarial attacks. This unavoidably expands the attack surfaces of LLMs. Second, we highlight that the broad functionality of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. To elucidate these risks, we study adversarial examples in the visual input space of a VLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;GPT-3&#19978;&#23454;&#29616;&#24189;&#40664;&#29983;&#25104;&#65292;&#20351;&#29992;&#20102;&#36880;&#27493;&#24605;&#32500;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#21019;&#36896;&#24189;&#40664;&#30340;&#35748;&#30693;&#36317;&#31163;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.13195</link><description>&lt;p&gt;
GPT-3&#20013;&#24189;&#40664;&#29983;&#25104;&#30340;&#36880;&#27493;&#24605;&#32500;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Prompt to GPT-3: Step-by-Step Thinking Instructions for Humor Generation. (arXiv:2306.13195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;GPT-3&#19978;&#23454;&#29616;&#24189;&#40664;&#29983;&#25104;&#65292;&#20351;&#29992;&#20102;&#36880;&#27493;&#24605;&#32500;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#21019;&#36896;&#24189;&#40664;&#30340;&#35748;&#30693;&#36317;&#31163;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20687;GPT-3&#36825;&#26679;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#24403;&#28041;&#21450;&#21040;&#38656;&#35201;&#29702;&#35299;&#29992;&#25143;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#27604;&#22914;&#25484;&#25569;&#20154;&#31867;&#21916;&#21095;&#20889;&#20316;&#31574;&#30053;&#12290;&#26412;&#25991;&#36890;&#36807;&#24314;&#27169;&#20154;&#31867;&#21916;&#21095;&#20889;&#20316;&#29702;&#35770;&#21644;&#21033;&#29992;&#36880;&#27493;&#24605;&#32500;&#25351;&#23548;&#26469;&#25506;&#35752;&#20351;&#29992;GPT-3&#36827;&#34892;&#24189;&#40664;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#21019;&#36896;&#24189;&#40664;&#30340;&#35748;&#30693;&#36317;&#31163;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence has made significant progress in natural language processing, with models like GPT-3 demonstrating impressive capabilities. However, these models still have limitations when it comes to complex tasks that require an understanding of the user, such as mastering human comedy writing strategies. This paper explores humor generation using GPT-3 by modeling human comedy writing theory and leveraging step-by-step thinking instructions. In addition, we explore the role of cognitive distance in creating humor.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#24046;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#21442;&#32771;&#36136;&#37327;&#35780;&#20215;&#25351;&#26631;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#30495;&#23454;&#36716;&#24405;&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#19981;&#21516;ASR&#27169;&#22411;&#22312;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#23558;WER&#38477;&#20302;&#36229;&#36807;7&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.13114</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21453;&#24046;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#21442;&#32771;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#36136;&#37327;&#35780;&#20215;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
A Reference-less Quality Metric for Automatic Speech Recognition via Contrastive-Learning of a Multi-Language Model with Self-Supervision. (arXiv:2306.13114v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#24046;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#21442;&#32771;&#36136;&#37327;&#35780;&#20215;&#25351;&#26631;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#30495;&#23454;&#36716;&#24405;&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#19981;&#21516;ASR&#27169;&#22411;&#22312;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#23558;WER&#38477;&#20302;&#36229;&#36807;7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#36136;&#37327;&#35780;&#20215;&#36890;&#24120;&#37319;&#29992;&#22522;&#20110;&#21442;&#32771;&#30340;&#25351;&#26631;&#65292;&#22914;&#20351;&#29992;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#25163;&#24037;&#30495;&#23454;&#36716;&#24405;&#26469;&#35745;&#31639;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#26080;&#21442;&#32771;&#36136;&#37327;&#35780;&#20215;&#25351;&#26631;&#65292;&#21487;&#22312;&#27809;&#26377;&#30495;&#23454;&#36716;&#24405;&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#19981;&#21516;ASR&#27169;&#22411;&#22312;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20272;&#35745;ASR&#20551;&#35774;&#30340;&#36136;&#37327;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#24335;&#36827;&#34892;&#21453;&#24046;&#23398;&#20064;&#30340;&#24494;&#35843;&#12290;&#22312;&#23545;&#22810;&#20010;&#26410;&#35265;&#36807;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26080;&#21442;&#32771;&#25351;&#26631;&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#37117;&#27604;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;LM&#30340;&#22256;&#24785;&#24230;&#25351;&#26631;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#19982;WER&#24471;&#20998;&#21450;&#20854;&#25490;&#21517;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#22312;&#29992;&#20110;&#21512;&#24182;&#20551;&#35774;&#26102;&#21487;&#23558;WER&#38477;&#20302;&#36229;&#36807;7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The common standard for quality evaluation of automatic speech recognition (ASR) systems is reference-based metrics such as the Word Error Rate (WER), computed using manual ground-truth transcriptions that are time-consuming and expensive to obtain. This work proposes a multi-language referenceless quality metric, which allows comparing the performance of different ASR models on a speech dataset without ground truth transcriptions. To estimate the quality of ASR hypotheses, a pre-trained language model (LM) is fine-tuned with contrastive learning in a self-supervised learning manner. In experiments conducted on several unseen test datasets consisting of outputs from top commercial ASR engines in various languages, the proposed referenceless metric obtains a much higher correlation with WER scores and their ranks than the perplexity metric from the state-of-art multi-lingual LM in all experiments, and also reduces WER by more than $7\%$ when used for ensembling hypotheses. The fine-tune
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIMLET&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#19981;&#36275;&#21644;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;GIMLET&#33021;&#22815;&#22686;&#24378;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.13089</link><description>&lt;p&gt;
GIMLET&#65306;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#25351;&#20196;&#20998;&#23376;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning. (arXiv:2306.13089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIMLET&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#19981;&#36275;&#21644;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;GIMLET&#33021;&#22815;&#22686;&#24378;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#23454;&#39564;&#36896;&#25104;&#30340;&#26631;&#31614;&#19981;&#36275;&#38382;&#39064;&#23558;&#26159;&#20854;&#20027;&#35201;&#29942;&#39048;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#25991;&#26412;&#30693;&#35782;&#36827;&#34892;&#20219;&#21153;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#20998;&#23376;-&#25991;&#26412;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#21407;&#22240;&#26159;&#22788;&#29702;&#25351;&#20196;&#19981;&#36275;&#20197;&#21450;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GIMLET&#65292;&#23427;&#32479;&#19968;&#20102;&#22270;&#24418;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#37319;&#29992;&#24191;&#20041;&#20301;&#32622;&#23884;&#20837;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34987;&#25193;&#23637;&#20197;&#32534;&#30721;&#22270;&#24418;&#32467;&#26500;&#21644;&#25351;&#20196;&#25991;&#26412;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22270;&#24418;&#32534;&#30721;&#27169;&#22359;&#12290;GIMLET&#36824;&#22312;&#27880;&#24847;&#26426;&#21046;&#20013;&#35299;&#32806;&#20102;&#22270;&#24418;&#30340;&#32534;&#30721;&#21644;&#20219;&#21153;&#25351;&#20196;&#65292;&#22686;&#24378;&#20102;&#36328;&#26032;&#20219;&#21153;&#30340;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs. To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#30340;&#26041;&#24335;&#65292;&#25163;&#21160;&#26657;&#27491;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25506;&#31350;&#29702;&#24615;&#20013;&#23376;&#36923;&#36753;&#30340;&#25163;&#21160;&#26657;&#27491;&#26469;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#22522;&#20110;&#32463;&#27982;&#29702;&#35770;&#30340;CAMLOP&#21487;&#20197;&#24179;&#34913;&#25928;&#29992;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.07932</link><description>&lt;p&gt;
&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-Loop through Chain-of-Thought. (arXiv:2306.07932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07932
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#30340;&#26041;&#24335;&#65292;&#25163;&#21160;&#26657;&#27491;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25506;&#31350;&#29702;&#24615;&#20013;&#23376;&#36923;&#36753;&#30340;&#25163;&#21160;&#26657;&#27491;&#26469;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#22522;&#20110;&#32463;&#27982;&#29702;&#35770;&#30340;CAMLOP&#21487;&#20197;&#24179;&#34913;&#25928;&#29992;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#20986;&#29616;&#20351;&#33258;&#21160;&#21270;&#21464;&#24471;&#36234;&#26469;&#36234;&#26080;&#22788;&#19981;&#22312;&#65292;&#20294;&#26377;&#26102;&#22312;&#38271;&#26399;&#25110;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#20854;&#24369;&#28857;&#12290;&#20363;&#22914;&#65292;&#29992;&#25143;&#22312;&#27809;&#26377;&#20154;&#31867;&#21442;&#19982;&#30340;&#24773;&#20917;&#19979;&#19981;&#24635;&#33021;&#24471;&#21040;&#22797;&#26434;&#25968;&#23398;&#38382;&#39064;&#30340;&#29702;&#24819;&#31572;&#26696;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25163;&#21160;&#26657;&#27491;&#31995;&#32479;&#65288;MCS&#65289;&#8212;&#8212;&#19968;&#20010;&#36890;&#36807;&#24605;&#32500;&#38142;&#25552;&#31034;&#22686;&#24378;&#30340;&#20154;&#24037;&#21442;&#19982;&#31995;&#32479;&#65292;&#25506;&#31350;&#20102;&#29702;&#24615;&#20013;&#23376;&#36923;&#36753;&#30340;&#25163;&#21160;&#26657;&#27491;&#22914;&#20309;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#26356;&#36827;&#19968;&#27493;&#32771;&#34385;&#21040;&#26377;&#20154;&#21442;&#19982;&#30340;&#31995;&#32479;&#19981;&#20165;&#35201;&#25552;&#39640;&#24615;&#33021;&#65292;&#36824;&#35201;&#25511;&#21046;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21476;&#20856;&#32463;&#27982;&#29702;&#35770;&#30340;&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#25104;&#26412;&#25928;&#29992;&#20998;&#26512;&#27169;&#22411;&#65288;CAMLOP&#65289;&#26469;&#20998;&#26512;&#12289;&#37327;&#21270;&#21644;&#24179;&#34913;&#25928;&#29992;&#21644;&#30456;&#24212;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;12&#20010;&#25968;&#25454;&#38598;&#23545;MCS&#21644;CAMLOP&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the emergence of powerful language models along with Chain-of-thought prompting has made automation more and more omnipresent, it sometimes demonstrates its weakness in long-term or multi-step logical reasoning. For example, users don't always get desirable answers for complex mathematical problems without human involvement. Against this background, we present the Manual Correction System (MCS) -- a human-in-the-loop system enhanced by Chain-of-Thought prompting, which explores how manual correction of sub-logics in rationales can improve LLM's reasoning performance. Moving one step forward, considering a system with human-in-the-loop involves more than having humans improve performance but also controlling the cost. Therefore, we post a Cost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on classical economics theory to analyze, quantify and balance the utility and the corresponding cost. We conduct experiments of MCS and CAMLOP with twelve datasets. A signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;V-GLOSS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.06077</link><description>&lt;p&gt;
&#35270;&#35273;&#35789;&#27719;&#25551;&#36848;&#25552;&#21319;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Visually-Grounded Descriptions Improve Zero-Shot Image Classification. (arXiv:2306.06077v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;V-GLOSS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#22914;CLIP&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#20219;&#21153;&#65288;&#20363;&#22914;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;ZSIC&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20855;&#20307;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#31867;&#21035;&#25551;&#36848;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#31890;&#24230;&#21644;&#26631;&#31614;&#27495;&#20041;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;V-GLOSS&#65306;Visual Glosses&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#26469;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20934;ZSIC&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;ImageNet&#21644;STL-10&#65289;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#26469;&#23637;&#31034;V-GLOSS&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;V-GLOSS&#29983;&#25104;&#30340;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20854;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#28304;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-vision models like CLIP have made significant progress in zero-shot vision tasks, such as zero-shot image classification (ZSIC). However, generating specific and expressive class descriptions remains a major challenge. Existing approaches suffer from granularity and label ambiguity issues. To tackle these challenges, we propose V-GLOSS: Visual Glosses, a novel method leveraging modern language models and semantic knowledge bases to produce visually-grounded class descriptions. We demonstrate V-GLOSS's effectiveness by achieving state-of-the-art results on benchmark ZSIC datasets including ImageNet and STL-10. In addition, we introduce a silver dataset with class descriptions generated by V-GLOSS, and show its usefulness for vision tasks. We make available our code and dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#27861;LEACE&#65292;&#21487;&#22312;&#21024;&#38500;&#25351;&#23450;&#29305;&#24449;&#30340;&#21516;&#26102;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#34920;&#31034;&#65292;&#24182;&#21487;&#35777;&#26126;&#38450;&#27490;&#25152;&#26377;&#32447;&#24615;&#20998;&#31867;&#22120;&#26816;&#27979;&#21040;&#27010;&#24565;&#12290;&#20316;&#32773;&#29992;&#8220;&#27010;&#24565;&#25830;&#38500;&#8221;&#36825;&#19968;&#26032;&#26041;&#27861;&#23558;&#20854;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24615;&#30340;&#20381;&#36182;&#24615;&#21644;&#20943;&#23569;BERT&#23884;&#20837;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#20219;&#21153;&#20013;&#24471;&#20986;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.03819</link><description>&lt;p&gt;
LEACE&#65306;&#38381;&#21512;&#24418;&#24335;&#20013;&#30340;&#23436;&#32654;&#32447;&#24615;&#27010;&#24565;&#25830;&#38500;
&lt;/p&gt;
&lt;p&gt;
LEACE: Perfect linear concept erasure in closed form. (arXiv:2306.03819v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#27861;LEACE&#65292;&#21487;&#22312;&#21024;&#38500;&#25351;&#23450;&#29305;&#24449;&#30340;&#21516;&#26102;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#34920;&#31034;&#65292;&#24182;&#21487;&#35777;&#26126;&#38450;&#27490;&#25152;&#26377;&#32447;&#24615;&#20998;&#31867;&#22120;&#26816;&#27979;&#21040;&#27010;&#24565;&#12290;&#20316;&#32773;&#29992;&#8220;&#27010;&#24565;&#25830;&#38500;&#8221;&#36825;&#19968;&#26032;&#26041;&#27861;&#23558;&#20854;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24615;&#30340;&#20381;&#36182;&#24615;&#21644;&#20943;&#23569;BERT&#23884;&#20837;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#20219;&#21153;&#20013;&#24471;&#20986;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#25830;&#38500;&#26088;&#22312;&#20174;&#34920;&#24449;&#20013;&#21024;&#38500;&#25351;&#23450;&#30340;&#29305;&#24449;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#65288;&#20363;&#22914;&#65292;&#38450;&#27490;&#20998;&#31867;&#22120;&#20351;&#29992;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#21644;&#21487;&#35299;&#37322;&#24615;&#65288;&#20363;&#22914;&#65292;&#21024;&#38500;&#27010;&#24565;&#20197;&#35266;&#23519;&#27169;&#22411;&#34892;&#20026;&#30340;&#21464;&#21270;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LEAst-squares&#27010;&#24565;&#25830;&#38500;&#65288;LEACE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#21487;&#35777;&#26126;&#38450;&#27490;&#25152;&#26377;&#32447;&#24615;&#20998;&#31867;&#22120;&#26816;&#27979;&#21040;&#27010;&#24565;&#65292;&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#25913;&#21464;&#34920;&#31034;&#65292;&#22914;&#24191;&#27867;&#31867;&#21035;&#30340;&#33539;&#25968;&#25152;&#27979;&#37327;&#30340;&#37027;&#26679;&#12290;&#25105;&#20204;&#20351;&#29992;&#21517;&#20026;&#8220;&#27010;&#24565;&#25830;&#38500;&#8221;&#30340;&#26032;&#26041;&#27861;&#23558;LEACE&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25830;&#38500;&#27599;&#20010;&#23618;&#20013;&#30340;&#30446;&#26631;&#27010;&#24565;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24615;&#20449;&#24687;&#30340;&#20381;&#36182;&#24615;&#65292;&#20197;&#21450;&#20943;&#23569;BERT&#23884;&#20837;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/EleutherAI/concept-erasure&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called "concept scrubbing," which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at https://github.com/EleutherAI/concept-erasure.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#23457;&#33258;&#21160;&#25552;&#31034;&#25216;&#26415;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;K-shot&#23398;&#20064;&#35774;&#32622;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#24182;&#19981;&#33021;&#22987;&#32456;&#20248;&#20110;&#25163;&#21160;&#25552;&#31034;&#65292;&#22240;&#27492;&#25163;&#21160;&#25552;&#31034;&#24212;&#35813;&#20316;&#20026;&#33258;&#21160;&#25552;&#31034;&#30340;&#19968;&#20010;&#22522;&#20934;&#32447;&#12290;</title><link>http://arxiv.org/abs/2304.03609</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#33258;&#21160;&#25552;&#31034;&#65306;&#25105;&#20204;&#30495;&#30340;&#20570;&#24471;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Revisiting Automated Prompting: Are We Actually Doing Better?. (arXiv:2304.03609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#23457;&#33258;&#21160;&#25552;&#31034;&#25216;&#26415;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;K-shot&#23398;&#20064;&#35774;&#32622;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#24182;&#19981;&#33021;&#22987;&#32456;&#20248;&#20110;&#25163;&#21160;&#25552;&#31034;&#65292;&#22240;&#27492;&#25163;&#21160;&#25552;&#31034;&#24212;&#35813;&#20316;&#20026;&#33258;&#21160;&#25552;&#31034;&#30340;&#19968;&#20010;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26159;&#20986;&#33394;&#30340;&#20960;&#20046;&#19981;&#29992;&#23398;&#20064;&#30340;&#23398;&#20064;&#32773;&#65292;&#22312;&#20960;&#20046;&#19981;&#29992;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#31034;&#26174;&#30528;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#38543;&#21518;&#36827;&#34892;&#20102;&#35797;&#22270;&#33258;&#21160;&#21270;&#20154;&#31867;&#25552;&#31034;&#30340;&#23581;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#38543;&#21518;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;K-shot&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#33258;&#21160;&#21270;&#21487;&#20197;&#20248;&#20110;&#24494;&#35843;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#21160;&#25552;&#31034;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#26356;&#22823;&#33539;&#22260;&#30340;K-shot&#23398;&#20064;&#35774;&#32622;&#19978;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#33258;&#21160;&#25552;&#31034;&#19981;&#33021;&#22987;&#32456;&#20248;&#20110;&#31616;&#21333;&#30340;&#25163;&#21160;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#38500;&#20102;&#24494;&#35843;&#20043;&#22806;&#65292;&#25163;&#21160;&#25552;&#31034;&#24212;&#20316;&#20026;&#22522;&#32447;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners, and prompting significantly increases their performance on a range of downstream tasks in a few-shot learning setting. An attempt to automate human-led prompting followed, with some progress achieved. In particular, subsequent work demonstrates automation can outperform fine-tuning in certain K-shot learning scenarios.  In this paper, we revisit techniques for automated prompting on six different downstream tasks and a larger range of K-shot learning settings. We find that automated prompting does not consistently outperform simple manual prompts. Our work suggests that, in addition to fine-tuning, manual prompts should be used as a baseline in this line of research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BLOOM-zh&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#25193;&#23637;&#20102;BLOOM&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#20855;&#26377;&#25913;&#36827;&#30340;&#32321;&#20307;&#20013;&#25991;&#25903;&#25345;&#12290;BLOOM-zh&#22312;&#32321;&#20307;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#21069;&#36523;&#12290;</title><link>http://arxiv.org/abs/2303.04715</link><description>&lt;p&gt;
BLOOM&#30340;&#39044;&#35757;&#32451;&#25193;&#23637;&#20197;&#25913;&#21892;&#23545;&#32321;&#20307;&#20013;&#25991;&#30340;&#25903;&#25345;&#65306;&#27169;&#22411;&#12289;&#26041;&#27861;&#21644;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Extending the Pre-Training of BLOOM for Improved Support of Traditional Chinese: Models, Methods and Results. (arXiv:2303.04715v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BLOOM-zh&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#25193;&#23637;&#20102;BLOOM&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#20855;&#26377;&#25913;&#36827;&#30340;&#32321;&#20307;&#20013;&#25991;&#25903;&#25345;&#12290;BLOOM-zh&#22312;&#32321;&#20307;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#21069;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BLOOM-zh&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#25913;&#36827;&#30340;&#32321;&#20307;&#20013;&#25991;&#25903;&#25345;&#12290;BLOOM-zh&#36215;&#28304;&#20110;&#30001;BigScience&#20110;2022&#24180;&#25512;&#20986;&#30340;&#24320;&#28304;BLOOM&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#24050;&#21457;&#24067;&#30340;&#27169;&#22411;&#22522;&#30784;&#19978;&#65292;&#20351;&#29992;74&#20159;&#20010;&#39069;&#22806;&#30340;&#32321;&#20307;&#20013;&#25991;&#21644;&#33521;&#25991;&#26631;&#35760;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#35206;&#30422;&#20102;&#21508;&#31181;&#39046;&#22495;&#65292;&#22914;&#26032;&#38395;&#25991;&#31456;&#12289;&#20070;&#31821;&#12289;&#30334;&#31185;&#20840;&#20070;&#12289;&#25945;&#32946;&#26448;&#26009;&#20197;&#21450;&#21475;&#35821;&#35821;&#35328;&#12290;&#20026;&#20102;&#23637;&#31034;BLOOM-zh&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#21644;&#26032;&#21019;&#24314;&#30340;&#22522;&#20934;&#22330;&#26223;&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#22312;&#22823;&#22810;&#25968;&#30340;&#32321;&#20307;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;BLOOM-zh&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#21069;&#36523;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20854;&#33521;&#25991;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#27169;&#22411;&#21457;&#24067;&#32473;&#30740;&#31350;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present the multilingual language model BLOOM-zh that features enhanced support for Traditional Chinese. BLOOM-zh has its origins in the open-source BLOOM models presented by BigScience in 2022. Starting from released models, we extended the pre-training of BLOOM by additional 7.4 billion tokens in Traditional Chinese and English covering a variety of domains such as news articles, books, encyclopedias, educational materials as well as spoken language. In order to show the properties of BLOOM-zh, both existing and newly created benchmark scenarios are used for evaluating the performance. BLOOM-zh outperforms its predecessor on most Traditional Chinese benchmarks while maintaining its English capability. We release all our models to the research community.
&lt;/p&gt;</description></item><item><title>MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2302.05981</link><description>&lt;p&gt;
MarioGPT: &#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#24335;&#25991;&#26412;&#20851;&#21345;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MarioGPT: Open-Ended Text2Level Generation through Large Language Models. (arXiv:2302.05981v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05981
&lt;/p&gt;
&lt;p&gt;
MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#22797;&#26434;&#25968;&#19968;&#33268;&#30340;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#29983;&#25104;&#21453;&#26144;&#29305;&#23450;&#24847;&#22270;&#21644;&#38480;&#21046;&#30340;&#26377;&#24847;&#20041;&#20869;&#23481;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#32570;&#20047;&#20197;&#24320;&#25918;&#24335;&#26041;&#24335;&#29983;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#37117;&#34920;&#29616;&#20986;&#20102;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#12290;&#36825;&#20123;&#35757;&#32451;&#26377;&#32032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24494;&#35843;&#65292;&#37325;&#22797;&#20351;&#29992;&#20449;&#24687;&#24182;&#21152;&#36895;&#26032;&#20219;&#21153;&#30340;&#22521;&#35757;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MarioGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;GPT2&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22522;&#20110;&#29943;&#30742;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#25105;&#20204;&#20197;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#30340;&#20851;&#21345;&#20026;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MarioGPT&#19981;&#20165;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#32780;&#19988;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#25511;&#21046;&#20851;&#21345;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;PCG&#25216;&#26415;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#20851;&#21345;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural Content Generation (PCG) algorithms provide a technique to generate complex and diverse environments in an automated way. However, while generating content with PCG methods is often straightforward, generating meaningful content that reflects specific intentions and constraints remains challenging. Furthermore, many PCG algorithms lack the ability to generate content in an open-ended manner. Recently, Large Language Models (LLMs) have shown to be incredibly effective in many diverse domains. These trained LLMs can be fine-tuned, re-using information and accelerating training for new tasks. In this work, we introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game levels, in our case Super Mario Bros levels. We show that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques. As far as we know, MarioGPT is the first text-to-level model. We a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;TransFusion&#26550;&#26500;&#65292;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24635;&#32467;&#21160;&#20316;&#19978;&#19979;&#25991;&#65292;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#23545;&#35937;&#20132;&#20114;&#30340;&#39044;&#27979;&#65292;&#26377;&#25928;&#24615;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.09209</link><description>&lt;p&gt;
&#24635;&#32467;&#36807;&#21435;&#20197;&#39044;&#27979;&#26410;&#26469;&#65306;&#33258;&#28982;&#35821;&#35328;&#23545;&#22330;&#26223;&#30340;&#25551;&#36848;&#20419;&#36827;&#22810;&#27169;&#24577;&#23545;&#35937;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction. (arXiv:2301.09209v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;TransFusion&#26550;&#26500;&#65292;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24635;&#32467;&#21160;&#20316;&#19978;&#19979;&#25991;&#65292;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#23545;&#35937;&#20132;&#20114;&#30340;&#39044;&#27979;&#65292;&#26377;&#25928;&#24615;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#20013;&#30340;&#23545;&#35937;&#20132;&#20114;&#39044;&#27979;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#35813;&#20219;&#21153;&#38656;&#35201;&#29702;&#35299;&#20808;&#21069;&#23545;&#23545;&#35937;&#25191;&#34892;&#30340;&#21160;&#20316;&#25152;&#24418;&#25104;&#30340;&#26102;&#31354;&#19978;&#19979;&#25991;&#65292;&#31216;&#20026;&#21160;&#20316;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;transformer&#30340;&#26550;&#26500;TransFusion&#12290;&#23427;&#21033;&#29992;&#35821;&#35328;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23545;&#21160;&#20316;&#19978;&#19979;&#25991;&#36827;&#34892;&#24635;&#32467;&#12290;TransFusion&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20174;&#36807;&#21435;&#30340;&#35270;&#39057;&#24103;&#20013;&#25552;&#21462;&#21160;&#20316;&#19978;&#19979;&#25991;&#12290;&#23558;&#36825;&#20010;&#21160;&#20316;&#19978;&#19979;&#25991;&#19982;&#19979;&#19968;&#20010;&#35270;&#39057;&#24103;&#19968;&#36215;&#32463;&#36807;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#36827;&#34892;&#22788;&#29702;&#65292;&#20174;&#32780;&#39044;&#27979;&#19979;&#19968;&#20010;&#23545;&#35937;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21017;&#22686;&#21152;&#20102;&#36890;&#29992;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;Ego4D&#21644;EPIC-KITCHENS-100&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20063;&#20984;&#26174;&#20102;&#22312;&#19968;&#20010;&#35270;&#35273;&#20284;&#20046;&#36275;&#22815;&#30340;&#20219;&#21153;&#20013;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#25688;&#35201;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study object interaction anticipation in egocentric videos. This task requires an understanding of the spatiotemporal context formed by past actions on objects, coined action context. We propose TransFusion, a multimodal transformer-based architecture. It exploits the representational power of language by summarising the action context. TransFusion leverages pre-trained image captioning and vision-language models to extract the action context from past video frames. This action context together with the next video frame is processed by the multimodal fusion module to forecast the next object interaction. Our model enables more efficient end-to-end learning. The large pre-trained language models add common sense and a generalisation capability. Experiments on Ego4D and EPIC-KITCHENS-100 show the effectiveness of our multimodal fusion model. They also highlight the benefits of using language-based context summaries in a task where vision seems to suffice. Our method outperforms state-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; IRCoT &#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26816;&#32034;&#19982;&#24605;&#36335;&#38142;&#26465;&#27493;&#39588;&#20132;&#26367;&#36827;&#34892;&#65292;&#20197;&#24341;&#23548;&#26816;&#32034;&#24182;&#20351;&#29992;&#26816;&#32034;&#32467;&#26524;&#25913;&#36827;&#24605;&#36335;&#38142;&#26465;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#22810;&#27493;&#38382;&#31572;&#20013;&#20808;&#21069;&#26816;&#32034;&#20449;&#24687;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10509</link><description>&lt;p&gt;
&#21033;&#29992;&#24605;&#36335;&#38142;&#26465;&#25512;&#29702;&#20132;&#38169;&#24335;&#26816;&#32034;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#22810;&#27493;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. (arXiv:2212.10509v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10509
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; IRCoT &#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26816;&#32034;&#19982;&#24605;&#36335;&#38142;&#26465;&#27493;&#39588;&#20132;&#26367;&#36827;&#34892;&#65292;&#20197;&#24341;&#23548;&#26816;&#32034;&#24182;&#20351;&#29992;&#26816;&#32034;&#32467;&#26524;&#25913;&#36827;&#24605;&#36335;&#38142;&#26465;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#22810;&#27493;&#38382;&#31572;&#20013;&#20808;&#21069;&#26816;&#32034;&#20449;&#24687;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#38382;&#31572;&#20013;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27493;&#39588;&#25110;&#24605;&#36335;&#38142;&#26465;&#65288;CoT&#65289;&#26102;&#20855;&#26377;&#20986;&#33394;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#25152;&#38656;&#30693;&#35782;&#19981;&#21487;&#29992;&#25110;&#19981;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#26356;&#26032;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#20986;&#38169;&#12290;&#34429;&#28982;&#20351;&#29992;&#38382;&#39064;&#20174;&#22806;&#37096;&#30693;&#35782;&#28304;&#26816;&#32034;&#30456;&#20851;&#25991;&#26412;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#19968;&#27493;&#26816;&#32034;&#21644;&#38405;&#35835;&#26041;&#27861;&#23545;&#20110;&#22810;&#27493;&#38382;&#39064;&#22238;&#31572;&#19981;&#36275;&#22815;&#12290;&#23545;&#20110;&#22810;&#27493;&#38382;&#39064;&#65292;&#38656;&#35201;&#26681;&#25454;&#20808;&#21069;&#24471;&#20986;&#30340;&#20869;&#23481;&#36873;&#25321;&#26816;&#32034;&#20869;&#23481;&#65292;&#32780;&#36825;&#21487;&#33021;&#20381;&#36182;&#20110;&#20043;&#21069;&#26816;&#32034;&#36807;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; IRCoT&#65292;&#19968;&#31181;&#26032;&#30340;&#22810;&#27493;&#38382;&#31572;&#26041;&#27861;&#65292;&#23427;&#23558;&#26816;&#32034;&#19982;&#24605;&#36335;&#38142;&#26465;&#20013;&#30340;&#27493;&#39588;&#36827;&#34892;&#20132;&#38169;&#65292;&#20197;&#24605;&#36335;&#38142;&#26465;&#24341;&#23548;&#26816;&#32034;&#65292;&#24182;&#20351;&#29992;&#26816;&#32034;&#32467;&#26524;&#26469;&#25913;&#36827;&#24605;&#36335;&#38142;&#26465;&#12290;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992; IRCoT &#19982; GPT3 &#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#26816;&#32034;&#65288;&#39640;&#36798; 21 &#28857;&#65289;&#21644;&#19979;&#28216;&#38382;&#31572;&#65288;&#39640;&#36798; 15 &#28857;&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, \textit{what to retrieve} depends on \textit{what has already been derived}, which in turn may depend on \textit{what was previously retrieved}. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: Ho
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21021;&#22987;&#21270;&#20559;&#24046;&#39033;&#20026;log-unigram&#20998;&#24067;&#65292;&#21487;&#20026;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#36171;&#20104;&#21333;&#20803;&#39057;&#29575;&#32479;&#35745;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#32763;&#35793;&#36136;&#37327;&#65292;&#20934;&#30830;&#32763;&#35793;&#19981;&#32463;&#24120;&#20986;&#29616;&#30340;&#21333;&#35789;&#21644;&#30701;&#35821;&#12290;</title><link>http://arxiv.org/abs/2212.09686</link><description>&lt;p&gt;
&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#33258;&#28982;&#20542;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Natural Bias for Language Generation Models. (arXiv:2212.09686v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09686
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21021;&#22987;&#21270;&#20559;&#24046;&#39033;&#20026;log-unigram&#20998;&#24067;&#65292;&#21487;&#20026;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#36171;&#20104;&#21333;&#20803;&#39057;&#29575;&#32479;&#35745;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#32763;&#35793;&#36136;&#37327;&#65292;&#20934;&#30830;&#32763;&#35793;&#19981;&#32463;&#24120;&#20986;&#29616;&#30340;&#21333;&#35789;&#21644;&#30701;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#20960;&#30334;&#20010;&#35757;&#32451;&#24490;&#29615;&#20043;&#21518;&#65292;&#19968;&#20010;&#26631;&#20934;&#30340;&#35821;&#35328;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#21487;&#33021;&#36824;&#27809;&#26377;&#23398;&#20250;&#33258;&#28982;&#35821;&#35328;&#30340;&#35768;&#22810;&#35821;&#20041;&#25110;&#21477;&#27861;&#35268;&#21017;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#20272;&#35745;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#20294;&#22312;&#36825;&#19968;&#28857;&#24038;&#21491;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#30830;&#23450;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26368;&#23567;&#21270;&#25439;&#22833;&#30340;&#34892;&#20026;&#65306;&#36755;&#20986;&#30446;&#26631;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#21333;&#20803;&#20998;&#24067;&#12290;&#20351;&#29992;&#36825;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#21487;&#20197;&#21021;&#22987;&#21270;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#36825;&#31181;&#34892;&#20026;&#24182;&#33410;&#30465;&#23453;&#36149;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#27169;&#22411;&#23481;&#37327;&#21527;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20026;&#26631;&#20934;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#36171;&#20104;&#21453;&#26144;&#21333;&#20803;&#39057;&#29575;&#32479;&#35745;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#30340;&#21333;&#29420;&#27169;&#22359;&#65292;&#21482;&#38656;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#26368;&#32456;&#32447;&#24615;&#23618;&#30340;&#20559;&#24046;&#39033;&#21021;&#22987;&#21270;&#20026;log-unigram&#20998;&#24067;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#20197;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20026;&#27979;&#35797;&#22522;&#30784;&#65292;&#35266;&#23519;&#21040;&#65306;&#65288;i&#65289;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#65307;&#65288;ii&#65289;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25972;&#20307;&#32763;&#35793;&#36136;&#37327;&#65307;&#65288;iii&#65289;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#32763;&#35793;&#19981;&#32463;&#24120;&#20986;&#29616;&#30340;&#21333;&#35789;&#21644;&#30701;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
After just a few hundred training updates, a standard probabilistic model for language generation has likely not yet learnt many semantic or syntactic rules of natural language, making it difficult to estimate the probability distribution over next tokens. Yet around this point, these models have identified a simple, loss-minimising behaviour: to output the unigram distribution of the target training corpus. The use of such a heuristic raises the question: Can we initialise our models with this behaviour and save precious compute resources and model capacity? Here we show that we can effectively endow standard neural language generation models with a separate module that reflects unigram frequency statistics as prior knowledge, simply by initialising the bias term in a model's final linear layer with the log-unigram distribution. We use neural machine translation as a test bed for this simple technique and observe that it: (i) improves learning efficiency; (ii) achieves better overall 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; InSCIt &#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#28151;&#21512;&#20027;&#21160;&#20132;&#20114;&#30340;&#20449;&#24687;&#26597;&#23547;&#23545;&#35805;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#22312;&#32500;&#22522;&#30334;&#31185;&#19978;&#25628;&#32034;&#65292;&#30452;&#25509;&#22238;&#31572;&#12289;&#35201;&#27714;&#28548;&#28165;&#25110;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#20197;&#22238;&#24212;&#29992;&#25143;&#30340;&#26597;&#35810;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20004;&#20010;&#23376;&#20219;&#21153;&#65288;&#35777;&#25454;&#27573;&#33853;&#35782;&#21035;&#21644;&#22238;&#31572;&#29983;&#25104;&#65289;&#20197;&#21450;&#20154;&#24037;&#35780;&#20272;&#21327;&#35758;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.00746</link><description>&lt;p&gt;
INSCIT: &#28151;&#21512;&#20027;&#21160;&#20132;&#20114;&#30340;&#20449;&#24687;&#26597;&#23547;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
INSCIT: Information-Seeking Conversations with Mixed-Initiative Interactions. (arXiv:2207.00746v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; InSCIt &#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#28151;&#21512;&#20027;&#21160;&#20132;&#20114;&#30340;&#20449;&#24687;&#26597;&#23547;&#23545;&#35805;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#22312;&#32500;&#22522;&#30334;&#31185;&#19978;&#25628;&#32034;&#65292;&#30452;&#25509;&#22238;&#31572;&#12289;&#35201;&#27714;&#28548;&#28165;&#25110;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#20197;&#22238;&#24212;&#29992;&#25143;&#30340;&#26597;&#35810;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20004;&#20010;&#23376;&#20219;&#21153;&#65288;&#35777;&#25454;&#27573;&#33853;&#35782;&#21035;&#21644;&#22238;&#31572;&#29983;&#25104;&#65289;&#20197;&#21450;&#20154;&#24037;&#35780;&#20272;&#21327;&#35758;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26597;&#23547;&#23545;&#35805;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#20250;&#25552;&#20986;&#27169;&#31946;&#25110;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#29702;&#24819;&#30340;&#20195;&#29702;&#20154;&#24212;&#26681;&#25454;&#21487;&#29992;&#30340;&#30693;&#35782;&#28304;&#21551;&#21160;&#19981;&#21516;&#30340;&#21709;&#24212;&#31867;&#22411;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#26410;&#33021;&#22815;&#65292;&#35201;&#20040;&#20154;&#20026;&#22320;&#32435;&#20837;&#36825;&#31181;&#20195;&#29702;&#20154;&#31471;&#30340;&#20027;&#21160;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; InSCIt &#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#28151;&#21512;&#20027;&#21160;&#20132;&#20114;&#30340;&#20449;&#24687;&#26597;&#23547;&#23545;&#35805;&#12290;&#23427;&#21253;&#21547;805&#20010;&#20154;-&#20154;&#23545;&#35805;&#20013;4.7K&#20010;&#29992;&#25143;-&#20195;&#29702;&#20154;&#20132;&#20114;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#22312;&#32500;&#22522;&#30334;&#31185;&#19978;&#25628;&#32034;&#65292;&#30452;&#25509;&#22238;&#31572;&#12289;&#35201;&#27714;&#28548;&#28165;&#25110;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#20197;&#22238;&#24212;&#29992;&#25143;&#30340;&#26597;&#35810;&#12290;&#35813;&#25968;&#25454;&#25903;&#25345;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#35777;&#25454;&#27573;&#33853;&#35782;&#21035;&#21644;&#22238;&#31572;&#29983;&#25104;&#65292;&#20197;&#21450;&#19968;&#20010;&#20154;&#24037;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20004;&#20010;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#30693;&#35782;&#35782;&#21035;&#21644;&#24320;&#25918;&#22495;&#38382;&#31572;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#32467;&#26524;&#12290;&#36825;&#20004;&#20010;&#31995;&#32479;&#37117;&#26126;&#26174;&#34920;&#29616;&#19981;&#21450;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an information-seeking conversation, a user may ask questions that are under-specified or unanswerable. An ideal agent would interact by initiating different response types according to the available knowledge sources. However, most current studies either fail to or artificially incorporate such agent-side initiative. This work presents InSCIt, a dataset for Information-Seeking Conversations with mixed-initiative Interactions. It contains 4.7K user-agent turns from 805 human-human conversations where the agent searches over Wikipedia and either directly answers, asks for clarification, or provides relevant information to address user queries. The data supports two subtasks, evidence passage identification and response generation, as well as a human evaluation protocol to assess model performance. We report results of two systems based on state-of-the-art models of conversational knowledge identification and open-domain question answering. Both systems significantly underperform huma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;GEEP&#65292;&#29992;&#20110;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#27809;&#26377;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36879;&#36807;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#23398;&#20064;&#24615;&#21035;&#30456;&#20851;&#30340;&#25552;&#31034;&#65292;GEEP&#23454;&#29616;&#20102;SOTA&#34920;&#29616;&#24182;&#22312;GLUE&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2110.05367</link><description>&lt;p&gt;
&#22312;&#19981;&#20135;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting. (arXiv:2110.05367v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05367
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;GEEP&#65292;&#29992;&#20110;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#27809;&#26377;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36879;&#36807;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#23398;&#20064;&#24615;&#21035;&#30456;&#20851;&#30340;&#25552;&#31034;&#65292;GEEP&#23454;&#29616;&#20102;SOTA&#34920;&#29616;&#24182;&#22312;GLUE&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35299;&#20915;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24615;&#21035;&#20559;&#35265;&#30340;&#30740;&#31350;&#36890;&#24120;&#24314;&#31435;&#19968;&#20010;&#23567;&#22411;&#30340;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#31532;&#20108;&#38454;&#27573;&#30340;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#26377;&#38480;&#19988;&#38598;&#20013;&#20851;&#27880;&#65292;&#31532;&#20108;&#38454;&#27573;&#39044;&#35757;&#32451;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#24536;&#35760;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#21487;&#33021;&#20250;&#20005;&#37325;&#25439;&#23475;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;GLUE&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#23454;&#35777;&#22320;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#20013;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;GEnder Equality Prompt (GEEP)&#65292;&#20197;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#19988;&#36951;&#24536;&#36739;&#23569;&#12290; GEEP&#20250;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#23398;&#20064;&#19982;&#24615;&#21035;&#30456;&#20851;&#30340;&#25552;&#31034;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;GEEP&#19981;&#20165;&#22312;&#24615;&#21035;&#20844;&#24179;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;SOTA&#34920;&#29616;&#65292;&#32780;&#19988;&#22312;GLUE&#19978;&#36951;&#24536;&#36739;&#23569;&#65292;&#24182;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model's downstream performance by a large margin. In this work, we empirically show that catastrophic forgetting occurs in such methods by evaluating them with general NLP tasks in GLUE. Then, we propose a new method, GEnder Equality Prompt (GEEP), to improve gender fairness of pre-trained models with less forgetting. GEEP freezes the pre-trained model and learns gender-related prompts with gender-neutral data. Empirical results show that GEEP not only achieves SOTA performances on gender fairness tasks, but also forgets less and performs better on GLUE by a large margin.
&lt;/p&gt;</description></item></channel></rss>