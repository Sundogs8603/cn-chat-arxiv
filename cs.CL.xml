<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21160;&#35789;&#32858;&#28966;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25191;&#34892;&#32454;&#31890;&#24230;&#30340;&#21160;&#35789;&#30701;&#35821;&#23545;&#40784;&#25439;&#22833;&#26469;&#25913;&#21892;&#22522;&#20110;CLIP&#30340;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#35789;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#22312;&#19977;&#20010;&#32858;&#28966;&#20110;&#21160;&#35789;&#29702;&#35299;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.06708</link><description>&lt;p&gt;
&#21160;&#35789;&#34892;&#21160;&#65306;&#25913;&#36827;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21160;&#35789;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Verbs in Action: Improving verb understanding in video-language models. (arXiv:2304.06708v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21160;&#35789;&#32858;&#28966;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25191;&#34892;&#32454;&#31890;&#24230;&#30340;&#21160;&#35789;&#30701;&#35821;&#23545;&#40784;&#25439;&#22833;&#26469;&#25913;&#21892;&#22522;&#20110;CLIP&#30340;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#35789;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#22312;&#19977;&#20010;&#32858;&#28966;&#20110;&#21160;&#35789;&#29702;&#35299;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21160;&#35789;&#23545;&#20110;&#27169;&#22411;&#21270;&#20154;&#19982;&#29289;&#20307;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19982;&#29615;&#22659;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;CLIP&#30340;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#22312;&#21160;&#35789;&#29702;&#35299;&#26041;&#38754;&#21463;&#38480;&#65292;&#19988;&#20005;&#37325;&#20381;&#36182;&#21517;&#35789;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#21160;&#20316;&#21644;&#26102;&#38388;&#29702;&#35299;&#30340;&#23454;&#38469;&#35270;&#39057;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21160;&#35789;&#32858;&#28966;&#23545;&#27604;&#65288;VFC&#65289;&#26694;&#26550;&#65292;&#25913;&#21892;&#22522;&#20110;CLIP&#30340;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#35789;&#29702;&#35299;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;1&#65289;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21019;&#24314;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#30340;&#30828;&#36127;&#20363;&#65292;&#20197;&#21450;&#36890;&#36807;&#26657;&#20934;&#31574;&#30053;&#24179;&#34913;&#27491;&#36127;&#23545;&#20013;&#27010;&#24565;&#30340;&#20986;&#29616;&#26469;&#24179;&#34913;&#27491;&#36127;&#23545;&#65307;&#65288;2&#65289;&#25191;&#34892;&#32454;&#31890;&#24230;&#30340;&#21160;&#35789;&#30701;&#35821;&#23545;&#40784;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#32858;&#28966;&#20110;&#21160;&#35789;&#29702;&#35299;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65306;&#35270;&#39057;t...
&lt;/p&gt;
&lt;p&gt;
Understanding verbs is crucial to modelling how people and objects interact with each other and the environment through space and time. Recently, state-of-the-art video-language models based on CLIP have been shown to have limited verb understanding and to rely extensively on nouns, restricting their performance in real-world video applications that require action and temporal understanding. In this work, we improve verb understanding for CLIP-based video-language models by proposing a new Verb-Focused Contrastive (VFC) framework. This consists of two main components: (1) leveraging pretrained large language models (LLMs) to create hard negatives for cross-modal contrastive learning, together with a calibration strategy to balance the occurrence of concepts in positive and negative pairs; and (2) enforcing a fine-grained, verb phrase alignment loss. Our method achieves state-of-the-art results for zero-shot performance on three downstream tasks that focus on verb understanding: video-t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06671</link><description>&lt;p&gt;
&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;&#21644;&#36845;&#20195;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#25511;&#21046;&#26159;&#21487;&#25511;&#22270;&#20687;&#29983;&#25104;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#22312;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20855;&#26377;&#31867;&#20284;&#31354;&#38388;&#37197;&#32622;&#30340;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#38598;&#19978;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#20219;&#24847;&#19981;&#30830;&#23450;&#30340;&#24067;&#23616;&#30340;&#31163;&#32447;&#20998;&#24067;&#26679;&#26412;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#29616;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayoutBench&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35786;&#26029;&#30340;&#22522;&#20934;&#65292;&#23427;&#26816;&#26597;&#20102;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#65306;&#25968;&#37327;&#65292;&#20301;&#32622;&#65292;&#22823;&#23567;&#21644;&#24418;&#29366;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#26368;&#36817;&#20195;&#34920;&#24615;&#30340;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#33391;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#36793;&#30028;&#19978;&#30340;&#23545;&#35937;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#65292;&#23427;&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#23637;&#31034;&#20986;&#22312;LayoutBench&#30340;OOD&#24067;&#23616;&#19978;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#34920;&#26126;IterInpaint&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#29983;&#25104;&#22810;&#26679;&#21644;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#22270;&#20687;&#21644;&#21487;&#25511;&#30340;&#31354;&#38388;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial control is a core capability in controllable image generation. Advancements in layout-guided image generation have shown promising results on in-distribution (ID) datasets with similar spatial configurations. However, it is unclear how these models perform when facing out-of-distribution (OOD) samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench, a diagnostic benchmark for layout-guided image generation that examines four categories of spatial control skills: number, position, size, and shape. We benchmark two recent representative layout-guided image generation methods and observe that the good ID layout control may not generalize well to arbitrary layouts in the wild (e.g., objects at the boundary). Next, we propose IterInpaint, a new baseline that generates foreground and background regions in a step-by-step manner via inpainting, demonstrating stronger generalizability than existing models on OOD layouts in LayoutBench. We perform quantitative and q
&lt;/p&gt;</description></item><item><title>G2T&#26159;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;G2T&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.06653</link><description>&lt;p&gt;
G2T: &#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection. (arXiv:2304.06653v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06653
&lt;/p&gt;
&lt;p&gt;
G2T&#26159;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;G2T&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#32858;&#31867;&#30340;&#20027;&#39064;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36866;&#24403;&#30340;&#35789;&#35821;&#31579;&#36873;&#26041;&#27861;&#32858;&#31867;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#29983;&#25104;&#27604;&#29983;&#25104;&#24335;&#27010;&#29575;&#20027;&#39064;&#27169;&#22411;&#26356;&#22909;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#36873;&#25321;&#21512;&#36866;&#21442;&#25968;&#30340;&#22256;&#38590;&#20197;&#21450;&#19981;&#23436;&#25972;&#30340;&#27169;&#22411;&#24573;&#30053;&#21333;&#35789;&#19982;&#20027;&#39064;&#21450;&#20027;&#39064;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#23450;&#37327;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#20294;&#26377;&#25928;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21363;&#22270;&#20027;&#39064;&#65288;G2T&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been reported that clustering-based topic models, which cluster high-quality sentence embeddings with an appropriate word selection method, can generate better topics than generative probabilistic topic models. However, these approaches suffer from the inability to select appropriate parameters and incomplete models that overlook the quantitative relation between words with topics and topics with text. To solve these issues, we propose graph to topic (G2T), a simple but effective framework for topic modelling. The framework is composed of four modules. First, document representation is acquired using pretrained language models. Second, a semantic graph is constructed according to the similarity between document representations. Third, communities in document semantic graphs are identified, and the relationship between topics and documents is quantified accordingly. Fourth, the word--topic distribution is computed based on a variant of TFIDF. Automatic evaluation suggests that G2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;CTG&#21644;&#38382;&#39064;&#20998;&#31867;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#36890;&#36807;&#25945;&#24072;&#35780;&#20272;&#35777;&#26126;&#36825;&#20123;&#29983;&#25104;&#30340;&#38382;&#39064;&#36136;&#37327;&#39640;&#19988;&#36275;&#22815;&#26377;&#29992;&#65292;&#26377;&#26497;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.06638</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25945;&#32946;&#38382;&#39064;&#26377;&#22810;&#26377;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Useful are Educational Questions Generated by Large Language Models?. (arXiv:2304.06638v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;CTG&#21644;&#38382;&#39064;&#20998;&#31867;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#36890;&#36807;&#25945;&#24072;&#35780;&#20272;&#35777;&#26126;&#36825;&#20123;&#29983;&#25104;&#30340;&#38382;&#39064;&#36136;&#37327;&#39640;&#19988;&#36275;&#22815;&#26377;&#29992;&#65292;&#26377;&#26497;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65288;CTG&#65289;&#23545;&#20110;&#25945;&#24072;&#21644;&#23398;&#29983;&#26469;&#35828;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#29983;&#25104;&#21487;&#20197;&#22823;&#24133;&#20943;&#36731;&#25945;&#24072;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#20182;&#20204;&#25945;&#23398;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#26368;&#36817;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26410;&#33021;&#34920;&#26126;&#30495;&#27491;&#30340;&#25945;&#24072;&#35780;&#21028;&#29983;&#25104;&#30340;&#38382;&#39064;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#26159;&#21542;&#36275;&#22815;&#26377;&#29992;&#65292;&#25110;&#32773;&#38382;&#39064;&#26159;&#21542;&#23384;&#22312;&#38169;&#35823;&#21644;/&#25110;&#25945;&#23398;&#20869;&#23481;&#30340;&#24110;&#21161;&#19981;&#22823;&#12290;&#26412;&#25991;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#25945;&#24072;&#30340;&#26041;&#24335;&#65292;&#35780;&#20272;&#36890;&#36807;&#32467;&#21512;CTG&#21644;&#38382;&#39064;&#20998;&#31867;&#65288;Bloom's&#21644;&#38590;&#24230;&#20998;&#31867;&#65289;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#36136;&#37327;&#21644;&#26377;&#29992;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#29983;&#25104;&#30340;&#38382;&#39064;&#36136;&#37327;&#39640;&#19988;&#36275;&#22815;&#26377;&#29992;&#65292;&#23637;&#31034;&#20102;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable text generation (CTG) by large language models has a huge potential to transform education for teachers and students alike. Specifically, high quality and diverse question generation can dramatically reduce the load on teachers and improve the quality of their educational content. Recent work in this domain has made progress with generation, but fails to show that real teachers judge the generated questions as sufficiently useful for the classroom setting; or if instead the questions have errors and/or pedagogically unhelpful content. We conduct a human evaluation with teachers to assess the quality and usefulness of outputs from combining CTG and question taxonomies (Bloom's and a difficulty taxonomy). The results demonstrate that the questions generated are high quality and sufficiently useful, showing their promise for widespread use in the classroom setting.
&lt;/p&gt;</description></item><item><title>&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#38656;&#35201;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#65292;&#32780;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;/&#29983;&#25104;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#26159;&#19968;&#39033;&#22522;&#26412;&#38656;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#65288;PGTask&#65289;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#35813;&#20219;&#21153;&#20351;&#24471;&#30740;&#31350;&#32773;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.06634</link><description>&lt;p&gt;
PGTask&#65306;&#20171;&#32461;&#20174;&#23545;&#35805;&#20013;&#29983;&#25104;&#26723;&#26696;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
PGTask: Introducing the Task of Profile Generation from Dialogues. (arXiv:2304.06634v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06634
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#38656;&#35201;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#65292;&#32780;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;/&#29983;&#25104;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#26159;&#19968;&#39033;&#22522;&#26412;&#38656;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#65288;PGTask&#65289;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#35813;&#20219;&#21153;&#20351;&#24471;&#30740;&#31350;&#32773;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23558;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#26469;&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30693;&#35782;&#20449;&#24687;&#31232;&#23569;&#19988;&#38590;&#20197;&#33719;&#21462;&#65292;&#36825;&#20351;&#24471;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;/&#29983;&#25104;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#25104;&#20026;&#19968;&#39033;&#22522;&#26412;&#38656;&#27714;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#65288;PGTask&#65289;&#12290;&#25105;&#20204;&#20026;&#27492;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#19982;&#30456;&#20851;&#35805;&#35821;&#23545;&#40784;&#30340;&#26723;&#26696;&#21477;&#23376;&#65292;&#20174;&#23545;&#35805;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20026;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26723;&#26696;&#29983;&#25104;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#26723;&#26696;&#29983;&#25104;&#30340;&#25361;&#25112;&#65292;&#24182;&#24076;&#26395;&#36825;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches have attempted to personalize dialogue systems by leveraging profile information into models. However, this knowledge is scarce and difficult to obtain, which makes the extraction/generation of profile information from dialogues a fundamental asset. To surpass this limitation, we introduce the Profile Generation Task (PGTask). We contribute with a new dataset for this problem, comprising profile sentences aligned with related utterances, extracted from a corpus of dialogues. Furthermore, using state-of-the-art methods, we provide a benchmark for profile generation on this novel dataset. Our experiments disclose the challenges of profile generation, and we hope that this introduces a new research direction.
&lt;/p&gt;</description></item><item><title>&#27861;&#24459;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#38754;&#20020;&#30528;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25361;&#25112;&#65292;&#20294;&#20854;&#22312;&#23458;&#25143;&#26381;&#21153;&#12289;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#36328;&#35821;&#35328;&#20132;&#27969;&#31561;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.06623</link><description>&lt;p&gt;
&#25506;&#32034;&#27861;&#24459;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#29616;&#29366;
&lt;/p&gt;
&lt;p&gt;
Exploring the State of the Art in Legal QA Systems. (arXiv:2304.06623v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06623
&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#38754;&#20020;&#30528;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25361;&#25112;&#65292;&#20294;&#20854;&#22312;&#23458;&#25143;&#26381;&#21153;&#12289;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#36328;&#35821;&#35328;&#20132;&#27969;&#31561;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31572;&#19982;&#27861;&#24459;&#39046;&#22495;&#30456;&#20851;&#30340;&#38382;&#39064;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22797;&#26434;&#30340;&#27861;&#24459;&#25991;&#26723;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#20026;&#27861;&#24459;&#38382;&#39064;&#25552;&#20379;&#20934;&#30830;&#30340;&#31572;&#26696;&#36890;&#24120;&#38656;&#35201;&#30456;&#20851;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#36825;&#20351;&#24471;&#21363;&#20351;&#23545;&#20110;&#20154;&#31867;&#19987;&#23478;&#26469;&#35828;&#65292;&#36825;&#39033;&#20219;&#21153;&#20063;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#38382;&#31572;&#31995;&#32479;&#65288;QA&#65289;&#26088;&#22312;&#29983;&#25104;&#23545;&#20197;&#20154;&#31867;&#35821;&#35328;&#25552;&#20986;&#30340;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#23427;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#29702;&#35299;&#38382;&#39064;&#24182;&#25628;&#32034;&#20449;&#24687;&#20197;&#25214;&#21040;&#30456;&#20851;&#31572;&#26696;&#12290;QA&#20855;&#26377;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#65292;&#21253;&#25324;&#23458;&#25143;&#26381;&#21153;&#12289;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#36328;&#35821;&#35328;&#20132;&#27969;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#35832;&#22914;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#22788;&#29702;&#22797;&#26434;&#21644;&#27169;&#31946;&#38382;&#39064;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering questions related to the legal domain is a complex task, primarily due to the intricate nature and diverse range of legal document systems. Providing an accurate answer to a legal query typically necessitates specialized knowledge in the relevant domain, which makes this task all the more challenging, even for human experts. QA (Question answering systems) are designed to generate answers to questions asked in human languages. They use natural language processing to understand questions and search through information to find relevant answers. QA has various practical applications, including customer service, education, research, and cross-lingual communication. However, they face challenges such as improving natural language understanding and handling complex and ambiguous questions. Answering questions related to the legal domain is a complex task, primarily due to the intricate nature and diverse range of legal document systems. Providing an accurate answer to a legal query
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT-4&#22312;&#25919;&#27835;Twitter&#30456;&#20851;&#25512;&#25991;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#20559;&#35265;&#26041;&#38754;&#30342;&#20248;&#20110;&#20154;&#31867;&#26631;&#27880;&#65292;&#23588;&#20854;&#26159;&#23427;&#33021;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#20934;&#30830;&#27880;&#37322;&#38656;&#35201;&#22522;&#20110;&#19978;&#19979;&#25991;&#30693;&#35782;&#21644;&#20316;&#32773;&#24847;&#21521;&#30340;&#25512;&#25991;&#65292;&#36825;&#23558;&#20351;&#24471;&#35268;&#27169;&#21270;&#30340;&#25991;&#26412;&#25968;&#25454;&#30740;&#31350;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#22312;&#31038;&#20250;&#31185;&#23398;&#30340;&#35299;&#37322;&#24615;&#30740;&#31350;&#20013;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.06588</link><description>&lt;p&gt;
ChatGPT-4&#22312;&#25919;&#27835;Twitter&#20449;&#24687;&#27880;&#37322;&#20013;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#32988;&#36807;&#19987;&#23478;&#21644;&#20247;&#21253;&#24037;&#20316;&#32773;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning. (arXiv:2304.06588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT-4&#22312;&#25919;&#27835;Twitter&#30456;&#20851;&#25512;&#25991;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#20559;&#35265;&#26041;&#38754;&#30342;&#20248;&#20110;&#20154;&#31867;&#26631;&#27880;&#65292;&#23588;&#20854;&#26159;&#23427;&#33021;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#20934;&#30830;&#27880;&#37322;&#38656;&#35201;&#22522;&#20110;&#19978;&#19979;&#25991;&#30693;&#35782;&#21644;&#20316;&#32773;&#24847;&#21521;&#30340;&#25512;&#25991;&#65292;&#36825;&#23558;&#20351;&#24471;&#35268;&#27169;&#21270;&#30340;&#25991;&#26412;&#25968;&#25454;&#30740;&#31350;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#22312;&#31038;&#20250;&#31185;&#23398;&#30340;&#35299;&#37322;&#24615;&#30740;&#31350;&#20013;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT-4&#22312;&#25919;&#27835;&#30456;&#20851;&#25512;&#25991;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#20559;&#35265;&#12290;&#19982;&#19987;&#23478;&#21644;&#20247;&#21253;&#24037;&#20316;&#32773;&#26631;&#27880;&#36827;&#34892;&#27604;&#36739;&#65292;&#30740;&#31350;&#20351;&#29992;2020&#24180;&#32654;&#22269;&#36873;&#20030;&#26399;&#38388;&#30340;&#25919;&#27835;&#30456;&#20851;&#25512;&#25991;&#20316;&#20026;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#20934;&#30830;&#24615;&#35780;&#27979;&#22522;&#20934;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;ChatGPT-4&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#12289;&#21487;&#38752;&#24615;&#26356;&#39640;&#65292;&#24182;&#19988;&#20559;&#35265;&#30456;&#31561;&#25110;&#26356;&#20302;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#27880;&#37322;&#38656;&#35201;&#22522;&#20110;&#19978;&#19979;&#25991;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#20316;&#32773;&#24847;&#21521;&#30340;&#25512;&#25991;&#65292;&#36825;&#20123;&#33021;&#21147;&#34987;&#20256;&#32479;&#19978;&#35270;&#20026;&#26159;&#20154;&#31867;&#29420;&#26377;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35299;&#37322;&#24615;&#30740;&#31350;&#26041;&#38754;&#23558;&#20135;&#29983;&#24040;&#22823;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#24471;&#35268;&#27169;&#21270;&#30340;&#25991;&#26412;&#25968;&#25454;&#30740;&#31350;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper assesses the accuracy, reliability and bias of the Large Language Model (LLM) ChatGPT-4 on the text analysis task of classifying the political affiliation of a Twitter poster based on the content of a tweet. The LLM is compared to manual annotation by both expert classifiers and crowd workers, generally considered the gold standard for such tasks. We use Twitter messages from United States politicians during the 2020 election, providing a ground truth against which to measure accuracy. The paper finds that ChatGPT-4 has achieves higher accuracy, higher reliability, and equal or lower bias than the human classifiers. The LLM is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities. These findings suggest that LLM will have substantial impact on the use of textual data in the social sciences, by enabling interpretive research at a scale.
&lt;/p&gt;</description></item><item><title>LLM&#22312;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#20013;&#34920;&#29616;&#19981;&#22914;&#19987;&#38376;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#20294;&#22914;&#26524;&#25552;&#20379;&#27491;&#30830;&#30340;&#27133;&#20540;&#65292;&#20173;&#26377;&#24341;&#23548;&#23545;&#35805;&#25104;&#21151;&#32467;&#26463;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#36890;&#36807;&#30495;&#23454;&#20449;&#24565;&#29366;&#24577;&#20998;&#24067;&#25110;&#22495;&#20869;&#31034;&#20363;&#30340;&#35775;&#38382;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.06556</link><description>&lt;p&gt;
LLM&#26159;&#21542;&#36275;&#20197;&#29992;&#20110;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are LLMs All You Need for Task-Oriented Dialogue?. (arXiv:2304.06556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06556
&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#20013;&#34920;&#29616;&#19981;&#22914;&#19987;&#38376;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#20294;&#22914;&#26524;&#25552;&#20379;&#27491;&#30830;&#30340;&#27133;&#20540;&#65292;&#20173;&#26377;&#24341;&#23548;&#23545;&#35805;&#25104;&#21151;&#32467;&#26463;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#36890;&#36807;&#30495;&#23454;&#20449;&#24565;&#29366;&#24577;&#20998;&#24067;&#25110;&#22495;&#20869;&#31034;&#20363;&#30340;&#35775;&#38382;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#36890;&#36807;&#23545;&#35805;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#23427;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#22522;&#20934;&#27979;&#35797;&#20013;&#23436;&#25104;&#22810;&#36718;&#20219;&#21153;&#24182;&#19982;&#22806;&#37096;&#25968;&#25454;&#24211;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#26174;&#24335;&#20449;&#24565;&#29366;&#24577;&#36319;&#36394;&#65292;LLM&#34920;&#29616;&#19981;&#22914;&#19987;&#38376;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22914;&#26524;&#25552;&#20379;&#27491;&#30830;&#30340;&#25554;&#27133;&#20540;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#24341;&#23548;&#23545;&#35805;&#25104;&#21151;&#32467;&#26463;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#20855;&#26377;&#30495;&#23454;&#20449;&#24565;&#29366;&#24577;&#20998;&#24067;&#25110;&#22495;&#20869;&#31034;&#20363;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#35813;&#33021;&#21147;&#20250;&#24471;&#21040;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instructions-tuned Large Language Models (LLMs) gained recently huge popularity thanks to their ability to interact with users through conversation. In this work we aim to evaluate their ability to complete multi-turn tasks and interact with external databases in the context of established task-oriented dialogue benchmarks. We show that for explicit belief state tracking, LLMs underperform compared to specialized task-specific models. Nevertheless, they show ability to guide the dialogue to successful ending if given correct slot values. Furthermore this ability improves with access to true belief state distribution or in-domain examples.
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#29983;&#25104;&#24335;AI&#30340;&#19968;&#23567;&#27493;&#65292;&#20063;&#26159;AGI&#30340;&#19968;&#22823;&#27493;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#23545;&#20854;&#22914;&#20309;&#28436;&#21464;&#20026;AIGC&#23637;&#26395;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#36890;&#29992;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#21457;&#23637;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.06488</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#30340;&#19968;&#23567;&#27493;&#65292;AGI&#30340;&#19968;&#22823;&#27493;&#65306;AIGC&#26102;&#20195;&#20013;ChatGPT&#30340;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era. (arXiv:2304.06488v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06488
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#29983;&#25104;&#24335;AI&#30340;&#19968;&#23567;&#27493;&#65292;&#20063;&#26159;AGI&#30340;&#19968;&#22823;&#27493;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#23545;&#20854;&#22914;&#20309;&#28436;&#21464;&#20026;AIGC&#23637;&#26395;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#36890;&#29992;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#21457;&#23637;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI &#26368;&#36817;&#21457;&#24067;&#20102;GPT-4&#65288;&#21448;&#31216;&#20026;ChatGPT plus&#65289;&#65292;&#35813;&#27169;&#22411;&#34987;&#35777;&#26126;&#26159;&#29983;&#25104;&#24335;AI&#65288;GAI&#65289;&#36808;&#20986;&#30340;&#19968;&#23567;&#27493;&#65292;&#20294;&#23545;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#26469;&#35828;&#21017;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#39134;&#36291;&#12290;&#33258;2022&#24180;11&#26376;&#27491;&#24335;&#21457;&#24067;&#20197;&#26469;&#65292;ChatGPT&#20415;&#36805;&#36895;&#21560;&#24341;&#20102;&#20247;&#22810;&#29992;&#25143;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#23186;&#20307;&#20851;&#27880;&#65292;&#30456;&#20851;&#30340;&#23398;&#26415;&#25991;&#31456;&#20063;&#36229;&#36807;&#20102;500&#31687;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#36827;&#34892;&#19968;&#27425;&#32508;&#36848;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23601;&#26159;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20174;&#25216;&#26415;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#19977;&#20010;&#26041;&#38754;&#20840;&#38754;&#35843;&#26597;ChatGPT&#30340;&#22242;&#38431;&#65292;&#24182;&#23637;&#26395;&#20102;ChatGPT&#22914;&#20309;&#28436;&#21464;&#20197;&#23454;&#29616;&#36890;&#29992;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#65292;&#36825;&#23558;&#26159;AGI&#21457;&#23637;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one giant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous users with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT from various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it in their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey ChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on how ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for the development of AGI.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;SemEval-2023&#20219;&#21153;12&#20013;&#20351;&#29992;Afro-centric&#35821;&#35328;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#36827;&#34892;&#38750;&#27954;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;Afro-centric&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;&#36866;&#37197;&#22120;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23545;&#20110;&#26377;&#38480;&#36164;&#28304;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#12290;</title><link>http://arxiv.org/abs/2304.06459</link><description>&lt;p&gt;
Masakhane-Afrisenti&#22312;SemEval-2023&#20219;&#21153;12&#20013;&#30340;&#24212;&#29992;&#65306;&#20351;&#29992;&#38750;&#27954;&#20013;&#24515;&#35821;&#35328;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#36827;&#34892;&#20302;&#36164;&#28304;&#38750;&#27954;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Masakhane-Afrisenti at SemEval-2023 Task 12: Sentiment Analysis using Afro-centric Language Models and Adapters for Low-resource African Languages. (arXiv:2304.06459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06459
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;SemEval-2023&#20219;&#21153;12&#20013;&#20351;&#29992;Afro-centric&#35821;&#35328;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#36827;&#34892;&#38750;&#27954;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;Afro-centric&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;&#36866;&#37197;&#22120;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23545;&#20110;&#26377;&#38480;&#36164;&#28304;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AfriSenti-SemEval&#20849;&#20139;&#20219;&#21153;12&#26088;&#22312;&#20026;12&#31181;&#38750;&#27954;&#35821;&#35328;&#25191;&#34892;&#21333;&#35821;&#24773;&#24863;&#20998;&#31867;&#65288;&#23376;&#20219;&#21153;A&#65289;&#12289;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#65288;&#23376;&#20219;&#21153;B&#65289;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#65288;&#20219;&#21153;C&#65289;&#12290;&#23545;&#20110;&#23376;&#20219;&#21153;A&#65292;&#25105;&#20204;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#12289;&#38750;&#27954;&#20013;&#24515;&#35821;&#35328;&#27169;&#22411;&#21644;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23545;&#20110;&#20219;&#21153;B&#65292;&#25105;&#20204;&#24494;&#35843;&#20102;&#25903;&#25345;&#20219;&#21153;&#20013;&#22810;&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#23545;&#20110;&#20219;&#21153;C&#65292;&#25105;&#20204;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#37197;&#22120;&#26041;&#27861;&#65292;&#21033;&#29992;&#30446;&#26631;&#35821;&#35328;&#30340;&#21333;&#35821;&#25991;&#26412;&#23454;&#29616;&#26377;&#25928;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#38750;&#27954;&#20013;&#24515;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25913;&#21892;&#20302;&#36164;&#28304;&#38750;&#27954;&#35821;&#35328;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#36866;&#37197;&#22120;&#36827;&#34892;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#36164;&#28304;&#21487;&#20197;&#33719;&#24471;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
AfriSenti-SemEval Shared Task 12 of SemEval-2023. The task aims to perform monolingual sentiment classification (sub-task A) for 12 African languages, multilingual sentiment classification (sub-task B), and zero-shot sentiment classification (task C). For sub-task A, we conducted experiments using classical machine learning classifiers, Afro-centric language models, and language-specific models. For task B, we fine-tuned multilingual pre-trained language models that support many of the languages in the task. For task C, we used we make use of a parameter-efficient Adapter approach that leverages monolingual texts in the target language for effective zero-shot transfer. Our findings suggest that using pre-trained Afro-centric language models improves performance for low-resource African languages. We also ran experiments using adapters for zero-shot tasks, and the results suggest that we can obtain promising results by using adapters with a limited amount of resources.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;VQA&#25968;&#25454;&#38598;PDF-VQA&#65292;&#20197;&#22810;&#20010;&#39029;&#38754;&#30340;&#23436;&#25972;&#25991;&#26723;&#20316;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#19982;&#22788;&#29702;&#25991;&#26723;&#20803;&#32032;&#12289;&#32467;&#26500;&#21644;&#20869;&#23481;&#31561;&#26041;&#38754;&#65292;&#20026;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2304.06447</link><description>&lt;p&gt;
PDF-VQA: &#19968;&#20010;&#26032;&#30340;&#29992;&#20110;PDF&#25991;&#20214;&#30495;&#23454;&#19990;&#30028;VQA&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PDF-VQA: A New Dataset for Real-World VQA on PDF Documents. (arXiv:2304.06447v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;VQA&#25968;&#25454;&#38598;PDF-VQA&#65292;&#20197;&#22810;&#20010;&#39029;&#38754;&#30340;&#23436;&#25972;&#25991;&#26723;&#20316;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#19982;&#22788;&#29702;&#25991;&#26723;&#20803;&#32032;&#12289;&#32467;&#26500;&#21644;&#20869;&#23481;&#31561;&#26041;&#38754;&#65292;&#20026;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26723;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#30740;&#31350;&#25991;&#26723;&#22270;&#20687;&#30340;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26723;&#30340;VQA&#25968;&#25454;&#38598;PDF-VQA&#65292;&#20174;&#25991;&#26723;&#20803;&#32032;&#35782;&#21035;&#12289;&#25991;&#26723;&#24067;&#23616;&#32467;&#26500;&#29702;&#35299;&#20197;&#21450;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#31561;&#21508;&#20010;&#26041;&#38754;&#20840;&#38754;&#25506;&#35752;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;PDF-VQA&#25968;&#25454;&#38598;&#23558;&#25991;&#26723;&#29702;&#35299;&#30340;&#35268;&#27169;&#20174;&#21333;&#20010;&#25991;&#26723;&#39029;&#38754;&#25193;&#23637;&#21040;&#35810;&#38382;&#22810;&#20010;&#39029;&#38754;&#30340;&#23436;&#25972;&#25991;&#26723;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;VQA&#27169;&#22411;&#65292;&#26126;&#30830;&#22320;&#38598;&#25104;&#20102;&#19981;&#21516;&#25991;&#26723;&#20803;&#32032;&#20043;&#38388;&#30340;&#31354;&#38388;&#21644;&#23618;&#27425;&#32467;&#26500;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#25991;&#26723;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#35813;&#24615;&#33021;&#19982;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#36739;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#38382;&#39064;&#31867;&#22411;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-based Visual Question Answering examines the document understanding of document images in conditions of natural language questions. We proposed a new document-based VQA dataset, PDF-VQA, to comprehensively examine the document understanding from various aspects, including document element recognition, document layout structural understanding as well as contextual understanding and key information extraction. Our PDF-VQA dataset extends the current scale of document understanding that limits on the single document page to the new scale that asks questions over the full document of multiple pages. We also propose a new graph-based VQA model that explicitly integrates the spatial and hierarchically structural relationships between different document elements to boost the document structural understanding. The performances are compared with several baselines over different question types and tasks\footnote{The full dataset will be released after paper acceptance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#30340;Spectformer&#26550;&#26500;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25552;&#39640;&#20102;top-1&#20934;&#30830;&#29575;2%&#12290;</title><link>http://arxiv.org/abs/2304.06446</link><description>&lt;p&gt;
SpectFormer: &#39057;&#29575;&#21644;&#27880;&#24847;&#21147;&#26159;&#35270;&#35273;Transformer&#25152;&#38656;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
SpectFormer: Frequency and Attention is what you need in a Vision Transformer. (arXiv:2304.06446v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#30340;Spectformer&#26550;&#26500;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25552;&#39640;&#20102;top-1&#20934;&#30830;&#29575;2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#12290;&#20854;&#31181;&#31867;&#21253;&#25324;&#22522;&#20110;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;&#22914;ViT&#12289;DeIT&#65289;&#21644;&#22522;&#20110;&#35889;&#23618;&#65288;&#22914;Fnet&#12289;GFNet&#12289;AFNO&#65289;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#37117;&#23545;Transformer&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#23558;&#20004;&#32773;&#32467;&#21512;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#22240;&#27492;&#25552;&#20986;&#20102;&#26032;&#30340;Spectformer&#26550;&#26500;&#65292;&#23558;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#34701;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;Spectformer&#21487;&#24688;&#24403;&#22320;&#25429;&#25417;&#29305;&#24449;&#34920;&#31034;&#65292;&#19982;&#20854;&#20182;Transformer&#34920;&#24449;&#30456;&#27604;&#65292;&#21487;&#20197;&#25552;&#39640;top-1&#20934;&#30830;&#29575;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have been applied successfully for image recognition tasks. There have been either multi-headed self-attention based (ViT \cite{dosovitskiy2020image}, DeIT, \cite{touvron2021training}) similar to the original work in textual models or more recently based on spectral layers (Fnet\cite{lee2021fnet}, GFNet\cite{rao2021global}, AFNO\cite{guibas2021efficient}). We hypothesize that both spectral and multi-headed attention plays a major role. We investigate this hypothesis through this work and observe that indeed combining spectral and multi-headed attention layers provides a better transformer architecture. We thus propose the novel Spectformer architecture for transformers that combines spectral and multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature representation appropriately and it yields improved performance over other transformer representations. For instance, it improves the top-1 accuracy by 2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29983;&#25104;&#31526;&#21495;&#65292;&#23454;&#29616;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#12290;&#36825;&#20123;&#31526;&#21495;&#21487;&#20197;&#25429;&#25417;&#21040;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21576;&#29616;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.06377</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#31526;&#21495;&#30340;&#20986;&#29616;&#19982;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29983;&#25104;&#31526;&#21495;&#65292;&#23454;&#29616;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#12290;&#36825;&#20123;&#31526;&#21495;&#21487;&#20197;&#25429;&#25417;&#21040;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21576;&#29616;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#21019;&#36896;&#26377;&#24847;&#20041;&#30340;&#31526;&#21495;&#65292;&#24182;&#29087;&#32451;&#22320;&#23558;&#23427;&#20204;&#29992;&#20110;&#26356;&#39640;&#30340;&#35748;&#30693;&#21151;&#33021;&#65292;&#22914;&#20132;&#27969;&#12289;&#25512;&#29702;&#12289;&#35268;&#21010;&#31561;&#65292;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#37325;&#35201;&#21644;&#29420;&#29305;&#20043;&#22788;&#12290; &#30446;&#21069;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20173;&#36828;&#36828;&#33853;&#21518;&#20110;&#20154;&#31867;&#21019;&#36896;&#31526;&#21495;&#36827;&#34892;&#36825;&#20123;&#39640;&#32423;&#35748;&#30693;&#21151;&#33021;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#31526;&#21495;&#21019;&#36896;&#12289;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#33021;&#21147;&#12290;SEA-net&#29983;&#25104;&#21160;&#24577;&#37197;&#32622;&#32593;&#32476;&#20197;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#31526;&#21495;&#12290;&#36825;&#20123;&#31526;&#21495;&#25429;&#25417;&#20102;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#36890;&#36807;&#32431;&#31526;&#21495;&#25805;&#20316;&#25110;&#20132;&#27969;&#33719;&#24471;&#26032;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#33258;&#21160;&#29983;&#25104;&#30340;&#31526;&#21495;&#21576;&#29616;&#20986;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#34920;&#26126;&#22312;&#20154;&#31867;&#22823;&#33041;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#29983;&#25104;&#21644;&#29702;&#35299;&#31526;&#21495;&#30340;&#20849;&#21516;&#26694;&#26550;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#23558;&#25104;&#20026;&#23558;&#26469;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#30340;&#21161;&#25512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to create meaningful symbols and proficiently use them for higher cognitive functions such as communication, reasoning, planning, etc., is essential and unique for human intelligence. Current deep neural networks are still far behind human's ability to create symbols for such higher cognitive functions. Here we propose a solution, named SEA-net, to endow neural networks with ability of symbol creation, semantic understanding and communication. SEA-net generates symbols that dynamically configure the network to perform specific tasks. These symbols capture compositional semantic information that enables the system to acquire new functions purely by symbolic manipulation or communication. In addition, we found that these self-generated symbols exhibit an intrinsic structure resembling that of natural language, suggesting a common framework underlying the generation and understanding of symbols in both human brains and artificial neural networks. We hope that it will be instrum
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29305;&#24449;&#20016;&#23500;&#35748;&#30693;&#36229;&#22270;&#20316;&#20026;&#20154;&#31867;&#35760;&#24518;&#23450;&#37327;&#27169;&#22411;&#26469;&#39044;&#27979;&#27010;&#24565;&#29305;&#24449;&#65292;&#19982;&#25104;&#23545;&#36830;&#25509;&#21644;&#32570;&#20047;&#29305;&#24449;&#30340;&#32593;&#32476;&#27169;&#22411;&#30456;&#27604;&#65292;&#29305;&#24449;&#20016;&#23500;&#35748;&#30693;&#36229;&#22270;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#28041;&#21450;&#26356;&#39640;&#38454;&#20851;&#32852;&#30340;&#36229;&#38142;&#25509;&#22312;&#20855;&#26377;&#30456;&#20284;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24449;&#30340;&#27010;&#24565;&#20043;&#38388;&#20248;&#20808;&#24418;&#25104;&#65292;&#36825;&#21453;&#26144;&#20102;&#20849;&#20139;&#35748;&#30693;&#32500;&#24230;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2304.06375</link><description>&lt;p&gt;
&#36229;&#22270;&#35748;&#30693;&#32593;&#32476;&#20316;&#20026;&#30693;&#35782;&#30340;&#29305;&#24449;&#20016;&#23500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards hypergraph cognitive networks as feature-rich models of knowledge. (arXiv:2304.06375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29305;&#24449;&#20016;&#23500;&#35748;&#30693;&#36229;&#22270;&#20316;&#20026;&#20154;&#31867;&#35760;&#24518;&#23450;&#37327;&#27169;&#22411;&#26469;&#39044;&#27979;&#27010;&#24565;&#29305;&#24449;&#65292;&#19982;&#25104;&#23545;&#36830;&#25509;&#21644;&#32570;&#20047;&#29305;&#24449;&#30340;&#32593;&#32476;&#27169;&#22411;&#30456;&#27604;&#65292;&#29305;&#24449;&#20016;&#23500;&#35748;&#30693;&#36229;&#22270;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#28041;&#21450;&#26356;&#39640;&#38454;&#20851;&#32852;&#30340;&#36229;&#38142;&#25509;&#22312;&#20855;&#26377;&#30456;&#20284;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24449;&#30340;&#27010;&#24565;&#20043;&#38388;&#20248;&#20808;&#24418;&#25104;&#65292;&#36825;&#21453;&#26144;&#20102;&#20849;&#20139;&#35748;&#30693;&#32500;&#24230;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#32593;&#32476;&#26159;&#29702;&#35299;&#22914;&#20309;&#20174;&#35760;&#24518;&#20013;&#26816;&#32034;&#30456;&#20851;&#27010;&#24565;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32593;&#32476;&#26041;&#27861;&#20351;&#29992;&#25104;&#23545;&#36830;&#25509;&#34920;&#31034;&#35760;&#24518;&#21484;&#22238;&#27169;&#24335;&#12290;&#25104;&#23545;&#36830;&#25509;&#24573;&#30053;&#20102;&#26356;&#39640;&#38454;&#30340;&#20851;&#32852;&#65292;&#21363;&#19968;&#27425;&#28041;&#21450;&#20004;&#20010;&#20197;&#19978;&#27010;&#24565;&#30340;&#20851;&#31995;&#12290;&#36825;&#20123;&#26356;&#39640;&#38454;&#30340;&#20132;&#20114;&#21487;&#33021;&#19982;&#22823;&#33041;&#28784;&#36136;&#32467;&#26500;&#29305;&#24449;&#65292;&#22914;&#20852;&#22859;&#12289;&#24841;&#24742;&#12289;&#29087;&#24713;&#24230;&#12289;&#24615;&#21035;&#31561;&#26377;&#20851;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20016;&#23500;&#35748;&#30693;&#36229;&#22270;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#20316;&#20026;&#20154;&#31867;&#35760;&#24518;&#30340;&#23450;&#37327;&#27169;&#22411;&#65306;&#65288;i&#65289;&#19968;&#36215;&#22238;&#24518;&#30340;&#27010;&#24565;&#21487;&#20197;&#21516;&#26102;&#21442;&#19982;&#21253;&#21547;&#20004;&#20010;&#20197;&#19978;&#27010;&#24565;&#30340;&#36229;&#38142;&#25509;&#65288;&#35748;&#30693;&#36229;&#22270;&#26041;&#38754;&#65289;&#65307;&#65288;ii&#65289;&#27599;&#20010;&#27010;&#24565;&#37117;&#20855;&#26377;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24449;&#21521;&#37327;&#65288;&#29305;&#24449;&#20016;&#23500;&#26041;&#38754;&#65289;&#12290;&#25105;&#20204;&#20174;&#35789;&#27719;&#32852;&#24819;&#25968;&#25454;&#20013;&#26500;&#24314;&#36229;&#22270;&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#29305;&#24449;&#35780;&#20272;&#26041;&#27861;&#26469;&#39044;&#27979;&#27010;&#24565;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#25104;&#23545;&#36830;&#25509;&#21644;&#32570;&#20047;&#29305;&#24449;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#29305;&#24449;&#20016;&#23500;&#35748;&#30693;&#36229;&#22270;&#22312;&#39044;&#27979;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#28041;&#21450;&#26356;&#39640;&#38454;&#20851;&#32852;&#30340;&#36229;&#38142;&#25509;&#20248;&#20808;&#24418;&#25104;&#22312;&#20855;&#26377;&#30456;&#20284;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24449;&#30340;&#27010;&#24565;&#20043;&#38388;&#12290;&#36825;&#34920;&#26126;&#65292;&#20154;&#31867;&#35760;&#24518;&#30340;&#32467;&#26500;&#28041;&#21450;&#21453;&#26144;&#20849;&#20139;&#35748;&#30693;&#32500;&#24230;&#30340;&#26356;&#39640;&#38454;&#20851;&#32852;&#65292;&#36825;&#20123;&#32500;&#24230;&#21487;&#20197;&#29992;&#20110;&#23558;&#27010;&#24565;&#32452;&#32455;&#25104;&#35821;&#20041;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic networks provide a useful tool to understand how related concepts are retrieved from memory. However, most current network approaches use pairwise links to represent memory recall patterns. Pairwise connections neglect higher-order associations, i.e. relationships between more than two concepts at a time. These higher-order interactions might covariate with (and thus contain information about) how similar concepts are along psycholinguistic dimensions like arousal, valence, familiarity, gender and others. We overcome these limits by introducing feature-rich cognitive hypergraphs as quantitative models of human memory where: (i) concepts recalled together can all engage in hyperlinks involving also more than two concepts at once (cognitive hypergraph aspect), and (ii) each concept is endowed with a vector of psycholinguistic features (feature-rich aspect). We build hypergraphs from word association data and use evaluation methods from machine learning features to predict concep
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;I3D&#35270;&#39057;&#29305;&#24449;&#22521;&#35757;Transformer&#27169;&#22411;&#65292;&#25454;&#27492;&#23545;How2Sign&#25968;&#25454;&#38598;&#36827;&#34892;&#25163;&#35821;&#32763;&#35793;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20844;&#20849;&#20195;&#30721;&#21644;&#39318;&#20010;&#24320;&#28304;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.06371</link><description>&lt;p&gt;
&#20174;&#25351;&#23548;&#35270;&#39057;&#20013;&#36827;&#34892;&#25163;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Sign Language Translation from Instructional Videos. (arXiv:2304.06371v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;I3D&#35270;&#39057;&#29305;&#24449;&#22521;&#35757;Transformer&#27169;&#22411;&#65292;&#25454;&#27492;&#23545;How2Sign&#25968;&#25454;&#38598;&#36827;&#34892;&#25163;&#35821;&#32763;&#35793;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20844;&#20849;&#20195;&#30721;&#21644;&#39318;&#20010;&#24320;&#28304;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25163;&#35821;&#32763;&#35793;&#65288;SLT&#65289;&#21040;&#21475;&#35821;&#35821;&#35328;&#30340;&#36827;&#23637;&#22823;&#37096;&#20998;&#37117;&#26159;&#22522;&#20110;&#35268;&#27169;&#26377;&#38480;&#12289;&#39046;&#22495;&#21463;&#38480;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#25552;&#20379;&#39318;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;"How2Sign"&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#25512;&#21160;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#25105;&#20204;&#20351;&#29992;I3D&#35270;&#39057;&#29305;&#24449;&#23545;Transformer&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#38477;&#20302;&#30340;BLEU&#20998;&#25968;&#20316;&#20026;&#39564;&#35777;&#30340;&#21442;&#32771;&#25351;&#26631;&#20195;&#26367;&#24191;&#27867;&#20351;&#29992;&#30340;BLEU&#20998;&#25968;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;8.03&#30340;BLEU&#20998;&#25968;&#65292;&#24182;&#21457;&#24067;&#20102;&#39318;&#20010;&#24320;&#28304;&#23454;&#29616;&#65292;&#20197;&#25512;&#21160;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advances in automatic sign language translation (SLT) to spoken languages have been mostly benchmarked with datasets of limited size and restricted domains. Our work advances the state of the art by providing the first baseline results on How2Sign, a large and broad dataset.  We train a Transformer over I3D video features, using the reduced BLEU as a reference metric for validation, instead of the widely used BLEU score. We report a result of 8.03 on the BLEU score, and publish the first open-source implementation of its kind to promote further advances.
&lt;/p&gt;</description></item><item><title>AGIEval&#26159;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#20013;&#24515;&#26631;&#20934;&#21270;&#32771;&#35797;&#19978;&#30340;&#34920;&#29616;&#12290;GPT-4&#22312;SAT&#12289;LSAT&#21644;&#25968;&#23398;&#27604;&#36187;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#24179;&#22343;&#34920;&#29616;&#65292;&#23637;&#31034;&#20102;&#24403;&#20195;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#32423;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06364</link><description>&lt;p&gt;
AGIEval&#65306;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#22522;&#20934;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. (arXiv:2304.06364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06364
&lt;/p&gt;
&lt;p&gt;
AGIEval&#26159;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#20013;&#24515;&#26631;&#20934;&#21270;&#32771;&#35797;&#19978;&#30340;&#34920;&#29616;&#12290;GPT-4&#22312;SAT&#12289;LSAT&#21644;&#25968;&#23398;&#27604;&#36187;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#24179;&#22343;&#34920;&#29616;&#65292;&#23637;&#31034;&#20102;&#24403;&#20195;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#32423;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#35299;&#20915;&#20154;&#31867;&#32423;&#21035;&#20219;&#21153;&#30340;&#36890;&#29992;&#33021;&#21147;&#26159;&#23427;&#20204;&#22312;&#21457;&#23637;&#21644;&#24212;&#29992;AGI&#65288;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65289;&#20013;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20256;&#32479;&#22522;&#20934;&#27979;&#35797;&#20381;&#36182;&#20110;&#20154;&#36896;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#20195;&#34920;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AGIEval&#65292;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#20013;&#24515;&#26631;&#20934;&#21270;&#32771;&#35797;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#20363;&#22914;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#65292;&#27861;&#24459;&#23398;&#26657;&#20837;&#23398;&#32771;&#35797;&#65292;&#25968;&#23398;&#31454;&#36187;&#21644;&#24459;&#24072;&#36164;&#26684;&#32771;&#35797;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324; GPT-4&#65292;ChatGPT &#21644;Text-Davinci-003&#12290;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;GPT-4&#22312;SAT&#12289;LSAT&#21644;&#25968;&#23398;&#27604;&#36187;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#24179;&#22343;&#34920;&#29616;&#65292;SAT&#25968;&#23398;&#27979;&#35797;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;95%&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#22823;&#23398;&#33521;&#35821;&#32771;&#35797;&#30340;&#33521;&#35821;&#27979;&#35797;&#20013;&#20934;&#30830;&#29575;&#20063;&#36798;&#21040;&#20102;92.5%&#12290;&#36825;&#23637;&#31034;&#20102;&#24403;&#20195;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#32423;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;AGI&#26410;&#26469;&#21457;&#23637;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary fou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;&#35821;&#20041;&#21464;&#21270;&#30340;&#35745;&#31639;&#24314;&#27169;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#22788;&#29702;&#35821;&#20041;&#21464;&#21270;&#26102;&#30340;&#20248;&#32570;&#28857;&#20197;&#21450;&#35780;&#20272;&#32467;&#26524;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2304.06337</link><description>&lt;p&gt;
&#35821;&#20041;&#21464;&#21270;&#30340;&#35745;&#31639;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Computational modeling of semantic change. (arXiv:2304.06337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#35821;&#20041;&#21464;&#21270;&#30340;&#35745;&#31639;&#24314;&#27169;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#22788;&#29702;&#35821;&#20041;&#21464;&#21270;&#26102;&#30340;&#20248;&#32570;&#28857;&#20197;&#21450;&#35780;&#20272;&#32467;&#26524;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#33410;&#27010;&#36848;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#21644;&#21322;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#36827;&#34892;&#35745;&#31639;&#24314;&#27169;&#26469;&#30740;&#31350;&#35821;&#20041;&#21464;&#21270;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#27491;&#30830;&#29702;&#35299;&#30456;&#20851;&#26041;&#27861;&#21644;&#35780;&#20272;&#25216;&#26415;&#30340;&#38053;&#21273;&#65292;&#21516;&#26102;&#20063;&#25506;&#35752;&#20102;&#35745;&#31639;&#30740;&#31350;&#35821;&#20041;&#21464;&#21270;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19981;&#21516;&#31867;&#21035;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#35821;&#20041;&#21464;&#21270;&#26102;&#30340;&#20248;&#32570;&#28857;&#65292;&#20197;&#21450;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#32467;&#26524;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this chapter we provide an overview of computational modeling for semantic change using large and semi-large textual corpora. We aim to provide a key for the interpretation of relevant methods and evaluation techniques, and also provide insights into important aspects of the computational study of semantic change. We discuss the pros and cons of different classes of models with respect to the properties of the data from which one wishes to model semantic change, and which avenues are available to evaluate the results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#26816;&#27979;&#24503;&#22269;&#22521;&#35757;&#25552;&#20379;&#21644;&#24191;&#21578;&#20013;&#30340;&#25945;&#32946;&#21644;&#22521;&#35757;&#20934;&#20837;&#65292;&#20197;&#24110;&#21161;&#21305;&#37197;&#22521;&#35757;&#23547;&#27714;&#32773;&#21644;&#25552;&#20379;&#32773;&#12290;</title><link>http://arxiv.org/abs/2304.06307</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#24503;&#22269;&#25945;&#32946;&#21644;&#22521;&#35757;&#20934;&#20837;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Rule-based detection of access to education and training in Germany. (arXiv:2304.06307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#26816;&#27979;&#24503;&#22269;&#22521;&#35757;&#25552;&#20379;&#21644;&#24191;&#21578;&#20013;&#30340;&#25945;&#32946;&#21644;&#22521;&#35757;&#20934;&#20837;&#65292;&#20197;&#24110;&#21161;&#21305;&#37197;&#22521;&#35757;&#23547;&#27714;&#32773;&#21644;&#25552;&#20379;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36716;&#22411;&#36807;&#31243;&#30340;&#19981;&#26029;&#28145;&#20837;&#65292;&#24503;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#23545;&#32844;&#19994;&#22521;&#35757;&#12289;&#20877;&#22521;&#35757;&#21644;&#32487;&#32493;&#25945;&#32946;&#39640;&#24230;&#20381;&#36182;&#12290;&#20026;&#20102;&#21305;&#37197;&#22521;&#35757;&#23547;&#27714;&#32773;&#21644;&#25552;&#20379;&#32773;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24503;&#22269;&#22521;&#35757;&#25552;&#20379;&#21644;&#24191;&#21578;&#20013;&#30340;&#25945;&#32946;&#21644;&#22521;&#35757;&#20934;&#20837;&#36827;&#34892;&#33258;&#21160;&#26816;&#27979;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#65306;&#65288;a&#65289;&#26222;&#36890;&#23398;&#26657;&#21644;&#25945;&#32946;&#23398;&#20301;&#20197;&#21450;&#27605;&#19994;&#35777;&#20070;&#65292;&#65288;b&#65289;&#19987;&#19994;&#32463;&#39564;&#65292;&#65288;c&#65289;&#20197;&#21069;&#30340;&#23398;&#24466;&#22521;&#35757;&#21644;&#65288;d&#65289;&#30001;&#24503;&#22269;&#32852;&#37030;&#21171;&#21160;&#26426;&#26500;&#25552;&#20379;&#30340;&#25216;&#33021;&#21015;&#34920;&#12290;&#35813;&#26032;&#26041;&#27861;&#32467;&#21512;&#20102;&#20960;&#31181;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25945;&#32946;&#21516;&#20041;&#35789;&#30340;&#26144;&#23556;&#65292;&#23558;&#19981;&#21516;&#30340;&#36164;&#26684;&#35777;&#20070;&#32452;&#21512;&#24182;&#28155;&#21152;&#36807;&#26102;&#30340;&#26415;&#35821;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20379;&#22522;&#20110;&#35268;&#21017;&#30340;&#21305;&#37197;&#65292;&#20197;&#30830;&#23450;&#19987;&#19994;&#32463;&#39564;&#25110;&#23398;&#24466;&#22521;&#35757;&#30340;&#38656;&#27714;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#25968;&#25454;&#26550;&#26500;&#19981;&#20860;&#23481;&#25110;&#38750;&#26631;&#20934;&#21270;&#35201;&#27714;&#65288;&#20363;&#22914;&#21021;&#22987;&#27979;&#35797;&#25110;&#38754;&#35797;&#65289;&#65292;&#24182;&#19981;&#26159;&#25152;&#26377;&#20934;&#20837;&#35201;&#27714;&#37117;&#33021;&#22815;&#21305;&#37197;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#20173;&#33021;&#20351;&#29992;&#36825;&#20123;&#26816;&#27979;&#21040;&#30340;&#20934;&#20837;&#38656;&#27714;&#26469;&#24110;&#21161;&#26356;&#22909;&#22320;&#21305;&#37197;&#22521;&#35757;&#23547;&#27714;&#32773;&#21644;&#22521;&#35757;&#25552;&#20379;&#21830;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a result of transformation processes, the German labor market is highly dependent on vocational training, retraining and continuing education. To match training seekers and offers, we present a novel approach towards the automated detection of access to education and training in German training offers and advertisements. We will in particular focus on (a) general school and education degrees and schoolleaving certificates, (b) professional experience, (c) a previous apprenticeship and (d) a list of skills provided by the German Federal Employment Agency. This novel approach combines several methods: First, we provide a mapping of synonyms in education combining different qualifications and adding deprecated terms. Second, we provide a rule-based matching to identify the need for professional experience or apprenticeship. However, not all access requirements can be matched due to incompatible data schemata or non-standardizes requirements, e.g initial tests or interviews. While we ca
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#24050;&#26377;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;GLM&#27169;&#22411;&#65292;&#36890;&#36807;&#24322;&#26500;&#32467;&#26500;&#24863;&#30693;&#22120;&#21518;&#35757;&#32451;&#65292;&#24341;&#20837;&#20102;&#21477;&#27861;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#32467;&#26500;&#24191;&#25773;&#22120;&#24341;&#23548;&#26356;&#22909;&#30340;&#29983;&#25104;&#65292;&#20197;&#21450;&#24341;&#20837;&#20102;&#38754;&#21521;&#20219;&#21153;&#30340;&#32467;&#26500;&#24494;&#35843;&#26426;&#21046;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#32467;&#26500;&#24863;&#30693;&#22312;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06248</link><description>&lt;p&gt;
LasUIE:&#21033;&#29992;&#28508;&#22312;&#33258;&#36866;&#24212;&#32467;&#26500;&#24863;&#30693;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#32479;&#19968;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
LasUIE: Unifying Information Extraction with Latent Adaptive Structure-aware Generative Language Model. (arXiv:2304.06248v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#24050;&#26377;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;GLM&#27169;&#22411;&#65292;&#36890;&#36807;&#24322;&#26500;&#32467;&#26500;&#24863;&#30693;&#22120;&#21518;&#35757;&#32451;&#65292;&#24341;&#20837;&#20102;&#21477;&#27861;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#32467;&#26500;&#24191;&#25773;&#22120;&#24341;&#23548;&#26356;&#22909;&#30340;&#29983;&#25104;&#65292;&#20197;&#21450;&#24341;&#20837;&#20102;&#38754;&#21521;&#20219;&#21153;&#30340;&#32467;&#26500;&#24494;&#35843;&#26426;&#21046;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#32467;&#26500;&#24863;&#30693;&#22312;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#26222;&#36941;&#22320;&#24314;&#27169;&#25152;&#26377;&#20856;&#22411;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65288;UIE&#65289;&#65292;&#23558;&#21508;&#31181;IE&#39044;&#27979;&#32479;&#19968;&#20026;GLM&#19979;&#30340;&#32447;&#24615;&#20998;&#23618;&#34920;&#36798;&#65292;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#21477;&#27861;&#32467;&#26500;&#20449;&#24687;&#26159;IE&#31038;&#21306;&#24191;&#27867;&#21033;&#29992;&#30340;&#19968;&#31181;&#26377;&#25928;&#29305;&#24449;&#65292;&#20063;&#24212;&#26377;&#30410;&#20110;UIE&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#24863;&#30693;GLM&#65292;&#20805;&#20998;&#37322;&#25918;&#20102;&#21477;&#27861;&#30693;&#35782;&#23545;UIE&#30340;&#24433;&#21709;&#21147;&#12290;&#37319;&#29992;&#24322;&#26500;&#32467;&#26500;&#24863;&#30693;&#22120;&#26469;&#21518;&#35757;&#32451;&#29616;&#26377;&#30340;GLM&#65292;&#26080;&#30417;&#30563;&#22320;&#24341;&#20837;&#20102;&#20016;&#23500;&#30340;&#24322;&#26500;&#32467;&#26500;&#34920;&#31034;&#12290;&#29305;&#21035;&#30340;&#65292;&#25552;&#20986;&#19968;&#31181;&#32467;&#26500;&#24191;&#25773;&#22120;&#65292;&#23558;&#21508;&#31181;&#28508;&#22312;&#26641;&#21387;&#32553;&#25104;&#26126;&#30830;&#30340;&#39640;&#38454;&#26862;&#26519;&#65292;&#22312;&#35299;&#30721;&#26399;&#38388;&#26377;&#21161;&#20110;&#24341;&#23548;&#26356;&#22909;&#30340;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#24341;&#20837;&#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#30340;&#32467;&#26500;&#24494;&#35843;&#26426;&#21046;&#65292;&#36827;&#19968;&#27493;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#32467;&#26500;&#65292;&#20351;&#20854;&#26356;&#31526;&#21512;&#26368;&#32456;&#20219;&#21153;&#30340;&#38656;&#35201;&#12290;&#36229;&#36807;12,000&#20010;&#21477;&#23376;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#26500;&#24314;&#20102;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#32467;&#26500;&#24863;&#30693;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Universally modeling all typical information extraction tasks (UIE) with one generative language model (GLM) has revealed great potential by the latest study, where various IE predictions are unified into a linearized hierarchical expression under a GLM. Syntactic structure information, a type of effective feature which has been extensively utilized in IE community, should also be beneficial to UIE. In this work, we propose a novel structure-aware GLM, fully unleashing the power of syntactic knowledge for UIE. A heterogeneous structure inductor is explored to unsupervisedly induce rich heterogeneous structural representations by post-training an existing GLM. In particular, a structural broadcaster is devised to compact various latent trees into explicit high-order forests, helping to guide a better generation during decoding. We finally introduce a task-oriented structure fine-tuning mechanism, further adjusting the learned structures to most coincide with the end-task's need. Over 12
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LeafAI&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#25968;&#25454;&#27169;&#22411;&#19981;&#21463;&#38480;&#21046;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#20026;&#22797;&#26434;&#30340;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#26631;&#20934;&#25552;&#20379;&#26032;&#39062;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.06203</link><description>&lt;p&gt;
LeafAI&#65306;&#20020;&#24202;&#38431;&#21015;&#21457;&#29616;&#30340;&#26597;&#35810;&#29983;&#25104;&#22120;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#19981;&#30456;&#19978;&#19979;
&lt;/p&gt;
&lt;p&gt;
LeafAI: query generator for clinical cohort discovery rivaling a human programmer. (arXiv:2304.06203v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06203
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LeafAI&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#25968;&#25454;&#27169;&#22411;&#19981;&#21463;&#38480;&#21046;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#20026;&#22797;&#26434;&#30340;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#26631;&#20934;&#25552;&#20379;&#26032;&#39062;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#65292;&#30830;&#23450;&#30740;&#31350;&#36164;&#26684;&#30340;&#24739;&#32773;&#26159;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#30340;&#26597;&#35810;&#35774;&#35745;&#36890;&#24120;&#38656;&#35201;&#24191;&#27867;&#30340;&#25216;&#26415;&#21644;&#29983;&#29289;&#21307;&#23398;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#35797;&#22270;&#21019;&#24314;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#29983;&#25104;&#25968;&#25454;&#27169;&#22411;&#19981;&#21463;&#38480;&#21046;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#20026;&#22797;&#26434;&#30340;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#26631;&#20934;&#25552;&#20379;&#26032;&#39062;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#20174;&#36164;&#26684;&#26631;&#20934;&#21019;&#24314;&#26597;&#35810;&#30340;&#20219;&#21153;&#38656;&#35201;&#35299;&#20915;&#20960;&#20010;&#25991;&#26412;&#22788;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;&#12289;&#24207;&#21015;&#21040;&#24207;&#21015;&#36716;&#25442;&#12289;&#24402;&#19968;&#21270;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22359;&#20197;&#21450;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#21644;&#38142;&#25509;&#26412;&#20307;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#30693;&#35782;&#24211;&#12290;&#20026;&#20102;&#23454;&#29616;&#25968;&#25454;&#27169;&#22411;&#19981;&#21463;&#38480;&#21046;&#30340;&#26597;&#35810;&#21019;&#24314;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;UMLS&#27010;&#24565;&#26631;&#35760;&#25968;&#25454;&#24211;&#27169;&#24335;&#20803;&#32032;&#30340;&#26032;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#31995;&#32479;LeafAI&#65292;&#25105;&#20204;&#19982;&#20004;&#20010;&#20855;&#26377;&#20020;&#24202;&#24212;&#29992;&#32972;&#26223;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Identifying study-eligible patients within clinical databases is a critical step in clinical research. However, accurate query design typically requires extensive technical and biomedical expertise. We sought to create a system capable of generating data model-agnostic queries while also providing novel logical reasoning capabilities for complex clinical trial eligibility criteria.  Materials and Methods: The task of query creation from eligibility criteria requires solving several text-processing problems, including named entity recognition and relation extraction, sequence-to-sequence transformation, normalization, and reasoning. We incorporated hybrid deep learning and rule-based modules for these, as well as a knowledge base of the Unified Medical Language System (UMLS) and linked ontologies. To enable data-model agnostic query creation, we introduce a novel method for tagging database schema elements using UMLS concepts. To evaluate our system, called LeafAI, we compare
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25551;&#36848;&#20102;&#20004;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33258;&#21160;&#32416;&#27491;&#21021;&#23398;&#32773;&#22312;&#36923;&#36753;&#35821;&#35328;&#36716;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06186</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21021;&#23398;&#32773;&#30340;&#65288;&#38750;&#65289;&#24418;&#24335;&#21270;&#21644;&#33258;&#28982;&#35770;&#35777;&#32451;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using large language models for (de-)formalization and natural argumentation exercises for beginner's students. (arXiv:2304.06186v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25551;&#36848;&#20102;&#20004;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33258;&#21160;&#32416;&#27491;&#21021;&#23398;&#32773;&#22312;&#36923;&#36753;&#35821;&#35328;&#36716;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#20004;&#20010;&#31995;&#32479;&#65292;&#20351;&#29992;&#25991;&#26412;&#36798;&#33452;&#22855;-003&#65292;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33258;&#21160;&#32416;&#27491;&#65288;i&#65289;&#33258;&#28982;&#35821;&#35328;&#19982;&#21629;&#39064;&#36923;&#36753;&#35821;&#35328;&#21644;&#19968;&#38454;&#35859;&#35789;&#36923;&#36753;&#35821;&#35328;&#20043;&#38388;&#36716;&#21270;&#30340;&#32451;&#20064;; &#21644;&#65288;ii&#65289;&#22312;&#38750;&#25968;&#23398;&#22330;&#26223;&#19979;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#20889;&#31616;&#21333;&#35770;&#28857;&#30340;&#32451;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe two systems that use text-davinci-003, a large language model, for the automatized correction of (i) exercises in translating back and forth between natural language and the languages of propositional logic and first-order predicate logic and (ii) exercises in writing simple arguments in natural language in non-mathematical scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25351;&#20196;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20998;&#26512;&#30340;&#26032;&#24037;&#20855;LINGO&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35270;&#35273;&#20998;&#26512;&#26469;&#25903;&#25345;&#29992;&#25143;&#32416;&#27491;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2304.06184</link><description>&lt;p&gt;
LINGO&#65306;&#36890;&#36807;&#35270;&#35273;&#21435;&#20559;&#35265;&#21270;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20197;&#25903;&#25345;&#20219;&#21153;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
LINGO : Visually Debiasing Natural Language Instructions to Support Task Diversity. (arXiv:2304.06184v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25351;&#20196;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20998;&#26512;&#30340;&#26032;&#24037;&#20855;LINGO&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35270;&#35273;&#20998;&#26512;&#26469;&#25903;&#25345;&#29992;&#25143;&#32416;&#27491;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#20219;&#21153;&#27867;&#21270;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#23450;&#20041;&#31934;&#36890;&#30340;&#37325;&#35201;&#32467;&#26524;&#12290;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25991;&#26412;&#25351;&#20196;&#21644;&#19968;&#23567;&#32452;&#31034;&#20363;&#26469;&#23450;&#20041;&#21644;&#20030;&#20363;&#35828;&#26126;&#20219;&#21153;&#65292;&#20174;&#32780;&#27169;&#20223;&#20102;&#20154;&#31867;&#30340;&#36825;&#31181;&#23398;&#20064;&#26041;&#24335;&#12290;&#20294;&#20998;&#26512;&#27169;&#22411;&#25509;&#25910;&#21040;&#30340;&#20219;&#21153;&#25351;&#20196;&#20013;&#30340;&#8220;&#20559;&#35265;&#8221;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#20998;&#26512;&#30028;&#38754;LINGO&#26469;&#25903;&#25345;&#26377;&#25928;&#30340;&#20219;&#21153;&#39537;&#21160;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;(1)&#24110;&#21161;&#35782;&#21035;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25351;&#20196;&#20013;&#30340;&#20559;&#35265;&#65292;(2)&#36890;&#36807;&#23545;&#25509;&#25910;&#21040;&#30340;&#25991;&#26412;&#36827;&#34892;&#35270;&#35273;&#20998;&#26512;&#26469;&#25903;&#25345;&#29992;&#25143;&#24605;&#32771;&#21644;&#32416;&#27491;&#35813;&#31867;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-task generalization is a significant outcome that defines mastery in natural language understanding. Humans show a remarkable aptitude for this, and can solve many different types of tasks, given definitions in the form of textual instructions and a small set of examples. Recent work with pre-trained language models mimics this learning style: users can define and exemplify a task for the model to attempt as a series of natural language prompts or instructions. While prompting approaches have led to higher cross-task generalization compared to traditional supervised learning, analyzing 'bias' in the task instructions given to the model is a difficult problem, and has thus been relatively unexplored. For instance, are we truly modeling a task, or are we modeling a user's instructions? To help investigate this, we develop LINGO, a novel visual analytics interface that supports an effective, task-driven workflow to (1) help identify bias in natural language task instructions, (2) al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;GPT-3&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#25688;&#35201;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#25991;&#26412;&#34920;&#24449;&#26041;&#27861;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25506;&#35752;&#35782;&#21035;&#26426;&#22120;&#20889;&#20316;&#25991;&#26412;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06148</link><description>&lt;p&gt;
&#26816;&#27979;&#34394;&#20551;&#29983;&#25104;&#30340;&#31185;&#23398;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Detection of Fake Generated Scientific Abstracts. (arXiv:2304.06148v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;GPT-3&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#25688;&#35201;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#25991;&#26412;&#34920;&#24449;&#26041;&#27861;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25506;&#35752;&#35782;&#21035;&#26426;&#22120;&#20889;&#20316;&#25991;&#26412;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#20844;&#24320;&#21487;&#29992;&#30340;ChatGPT&#30340;&#24191;&#27867;&#24212;&#29992;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#34701;&#20837;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#37325;&#35201;&#36716;&#25240;&#28857;&#12290;&#23398;&#26415;&#30028;&#24050;&#32463;&#27880;&#24847;&#21040;&#36825;&#20123;&#25216;&#26415;&#36827;&#27493;&#65292;&#24182;&#34920;&#36798;&#20102;&#20851;&#20110;&#21306;&#20998;&#30495;&#23454;&#21644;&#20154;&#24037;&#29983;&#25104;&#20449;&#24687;&#30340;&#22256;&#38590;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#21162;&#21147;&#24320;&#21457;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-3&#27169;&#22411;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#25688;&#35201;&#65292;&#24182;&#25506;&#35752;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#21508;&#31181;&#25991;&#26412;&#34920;&#24449;&#26041;&#27861;&#65292;&#20197;&#20415;&#35782;&#21035;&#26426;&#22120;&#20889;&#20316;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20998;&#26512;&#32467;&#26524;&#26102;&#24341;&#21457;&#30340;&#20960;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;&#36890;&#36807;&#36827;&#34892;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of Large Language Models and publicly available ChatGPT has marked a significant turning point in the integration of Artificial Intelligence into people's everyday lives. The academic community has taken notice of these technological advancements and has expressed concerns regarding the difficulty of discriminating between what is real and what is artificially generated. Thus, researchers have been working on developing effective systems to identify machine-generated text. In this study, we utilize the GPT-3 model to generate scientific paper abstracts through Artificial Intelligence and explore various text representation methods when combined with Machine Learning models with the aim of identifying machine-written text. We analyze the models' performance and address several research questions that rise during the analysis of the results. By conducting this research, we shed light on the capabilities and limitations of Artificial Intelligence generated text.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#32844;&#19994;&#12289;&#20154;&#26684;&#29305;&#24449;&#21644;&#26085;&#24120;&#24773;&#22659;&#31561;&#26041;&#38754;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#25490;&#38500;&#29305;&#23450;&#20154;&#32676;&#30340;&#32844;&#19994;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2304.06034</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#35282;&#24230;&#23457;&#35270;&#31038;&#20250;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Social Biases through the Text-to-Image Generation Lens. (arXiv:2304.06034v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#32844;&#19994;&#12289;&#20154;&#26684;&#29305;&#24449;&#21644;&#26085;&#24120;&#24773;&#22659;&#31561;&#26041;&#38754;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#25490;&#38500;&#29305;&#23450;&#20154;&#32676;&#30340;&#32844;&#19994;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687; (T2I) &#29983;&#25104;&#25216;&#26415;&#36890;&#36807;&#23558;&#32473;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#65292;&#29983;&#25104;&#39640;&#36924;&#30495;&#24230;&#30340;&#25554;&#22270;&#65292;&#20026;&#21019;&#20316;&#32773;&#12289;&#35774;&#35745;&#24072;&#21644;&#26222;&#36890;&#29992;&#25143;&#25552;&#20379;&#20102;&#26032;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#22823;&#37327;&#30340;&#32593;&#32476;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#26377;&#23475;&#20559;&#35265;&#39118;&#38505;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#21644;&#37327;&#21270;&#24120;&#35265;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#22914;&#32844;&#19994;&#12289;&#20154;&#26684;&#29305;&#24449;&#21644;&#26085;&#24120;&#24773;&#22659;&#22312;&#65288;&#34987;&#24863;&#30693;&#30340;&#65289;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#31181;&#26063;&#21644;&#22320;&#29702;&#20301;&#32622;&#19978;&#30340;&#34920;&#29616;&#65292;&#37319;&#29992;&#22810;&#32500;&#24230;&#26041;&#27861;&#26469;&#25506;&#31350;&#29983;&#25104;&#22270;&#29255;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#33258;&#21160;&#21270;&#21644;&#20154;&#31867;&#35780;&#20272;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340; T2I &#27169;&#22411; (DALLE-v2 &#21644; Stable Diffusion) &#30340;&#21457;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20013;&#24615;&#25552;&#31034;&#23384;&#22312;&#20005;&#37325;&#30340;&#32844;&#19994;&#20559;&#35265;&#65292;&#20027;&#35201;&#26159;&#25490;&#38500;&#26576;&#20123;&#20154;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-Image (T2I) generation is enabling new applications that support creators, designers, and general end users of productivity software by generating illustrative content with high photorealism starting from a given descriptive text as a prompt. Such models are however trained on massive amounts of web data, which surfaces the peril of potential harmful biases that may leak in the generation process itself. In this paper, we take a multi-dimensional approach to studying and quantifying common social biases as reflected in the generated images, by focusing on how occupations, personality traits, and everyday situations are depicted across representations of (perceived) gender, age, race, and geographical location. Through an extensive set of both automated and human evaluation experiments we present findings for two popular T2I models: DALLE-v2 and Stable Diffusion. Our results reveal that there exist severe occupational biases of neutral prompts majorly excluding groups of people 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21516;&#20041;&#35789;&#21477;&#23376;&#24314;&#31435;&#21516;&#24418;&#24322;&#20041;&#35789;&#35789;&#32423;&#28040;&#27495;&#34920;&#31034;&#65288;HDR&#65289;&#20197;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.05860</link><description>&lt;p&gt;
&#23398;&#20064;&#21516;&#24418;&#24322;&#20041;&#35789;&#28040;&#27495;&#34920;&#31034;&#20197;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Learning Homographic Disambiguation Representation for Neural Machine Translation. (arXiv:2304.05860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21516;&#20041;&#35789;&#21477;&#23376;&#24314;&#31435;&#21516;&#24418;&#24322;&#20041;&#35789;&#35789;&#32423;&#28040;&#27495;&#34920;&#31034;&#65288;HDR&#65289;&#20197;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#24418;&#24322;&#20041;&#35789;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#19968;&#30452;&#26159;&#38590;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#21516;&#24418;&#24322;&#20041;&#35789;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#8220;HDR-encoder&#8221;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#23398;&#20064;&#36890;&#29992;&#21477;&#23376;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;WordNet&#20013;&#30340;&#21516;&#20041;&#35789;&#21477;&#23376;&#24314;&#31435;&#21516;&#24418;&#24322;&#20041;&#35789;&#35789;&#32423;&#28040;&#27495;&#34920;&#31034;&#65288;HDR&#65289;&#65292;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;HDR-encoder&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#30340;HDR-encoder&#19982;&#22522;&#20110;Transformer&#30340;NMT&#22312;&#19981;&#21516;&#26041;&#26696;&#20013;&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#22235;&#20010;&#32763;&#35793;&#26041;&#21521;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#26412;&#26041;&#27861;&#22312;&#22686;&#24378;NMT&#31995;&#32479;&#22788;&#29702;&#21516;&#24418;&#24322;&#20041;&#35789;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Homographs, words with the same spelling but different meanings, remain challenging in Neural Machine Translation (NMT). While recent works leverage various word embedding approaches to differentiate word sense in NMT, they do not focus on the pivotal components in resolving ambiguities of homographs in NMT: the hidden states of an encoder. In this paper, we propose a novel approach to tackle homographic issues of NMT in the latent space. We first train an encoder (aka "HDR-encoder") to learn universal sentence representations in a natural language inference (NLI) task. We further fine-tune the encoder using homograph-based synset sentences from WordNet, enabling it to learn word-level homographic disambiguation representations (HDR). The pre-trained HDR-encoder is subsequently integrated with a transformer-based NMT in various schemes to improve translation accuracy. Experiments on four translation directions demonstrate the effectiveness of the proposed method in enhancing the perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35199;&#26031;&#25289;&#22827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#65292;&#21457;&#29616;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#26031;&#27931;&#20240;&#20811;&#35821;&#22343;&#23384;&#22312;&#30456;&#20284;&#31243;&#24230;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#36825;&#19968;&#30740;&#31350;&#22635;&#34917;&#20102;&#30740;&#31350;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#24615;&#21035;&#20559;&#35265;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2304.05783</link><description>&lt;p&gt;
&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#26031;&#27931;&#20240;&#20811;&#35821;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#37327;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Measuring Gender Bias in West Slavic Language Models. (arXiv:2304.05783v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35199;&#26031;&#25289;&#22827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#65292;&#21457;&#29616;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#26031;&#27931;&#20240;&#20811;&#35821;&#22343;&#23384;&#22312;&#30456;&#20284;&#31243;&#24230;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#36825;&#19968;&#30740;&#31350;&#22635;&#34917;&#20102;&#30740;&#31350;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#24615;&#21035;&#20559;&#35265;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#20250;&#23558;&#22522;&#30784;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#35265;&#24310;&#32493;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#22522;&#20110;&#33521;&#35821;&#30340;&#21333;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#38024;&#23545;&#25193;&#23637;&#21040;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#35199;&#26031;&#25289;&#22827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#26495;&#30340;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#26031;&#27931;&#20240;&#20811;&#35821;&#65289;&#65292;&#20197;&#27979;&#37327;&#38024;&#23545;&#30007;&#24615;&#12289;&#22899;&#24615;&#21644;&#38750;&#20108;&#36827;&#21046;&#20027;&#20307;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#20351;&#29992;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;&#36825;&#20123;&#21477;&#23376;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#26159;&#21542;&#36866;&#21512;&#20110;&#34987;&#36974;&#30422;&#30340;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#37327;&#21270;&#29983;&#25104;&#21333;&#35789;&#30340;&#26377;&#27602;&#24615;&#21644;&#24615;&#21035;&#29305;&#24449;&#26469;&#27979;&#37327;&#35199;&#26031;&#25289;&#22827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#35821;&#21477;&#20250;&#22240;&#20027;&#20307;&#30340;&#24615;&#21035;&#32780;&#20135;&#29983;&#20260;&#23475;&#24615;&#30340;&#23436;&#25104;&#24230;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25463;&#20811;&#35821;&#12289;&#26031;&#27931;&#20240;&#20811;&#35821;&#21644;&#27874;&#20848;&#35821;&#22343;&#26174;&#31034;&#20986;&#30456;&#20284;&#31243;&#24230;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#23545;&#20110;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#30340;&#26085;&#30410;&#22686;&#38271;&#30340;&#30740;&#31350;&#20307;&#31995;&#20570;&#20986;&#36129;&#29486;&#65292;&#24182;&#20026;&#35780;&#20272;&#21644;&#20943;&#23569;&#35199;&#26031;&#25289;&#22827;&#35821;&#35328;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have been known to perpetuate biases from the underlying datasets to downstream tasks. However, these findings are predominantly based on monolingual language models for English, whereas there are few investigative studies of biases encoded in language models for languages beyond English. In this paper, we fill this gap by analysing gender bias in West Slavic language models. We introduce the first template-based dataset in Czech, Polish, and Slovak for measuring gender bias towards male, female and non-binary subjects. We complete the sentences using both mono- and multilingual language models and assess their suitability for the masked language modelling objective. Next, we measure gender bias encoded in West Slavic language models by quantifying the toxicity and genderness of the generated words. We find that these language models produce hurtful completions that depend on the subject's gender. Perhaps surprisingly, Czech, Slovak, and Polish language mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;GPT(-3.5&#21644;-4)&#29983;&#25104;&#30340;&#26085;&#35821;&#25991;&#20307;&#29305;&#24449;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#23398;&#26415;&#35770;&#25991;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#25991;&#20307;&#29305;&#24449;&#19978;&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.05534</link><description>&lt;p&gt;
&#36890;&#36807;&#26085;&#35821;&#25991;&#20307;&#20998;&#26512;&#21306;&#20998;ChatGPT(-3.5,-4)&#30340;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Distinguishing ChatGPT(-3.5, -4)-generated and human-written papers through Japanese stylometric analysis. (arXiv:2304.05534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;GPT(-3.5&#21644;-4)&#29983;&#25104;&#30340;&#26085;&#35821;&#25991;&#20307;&#29305;&#24449;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#23398;&#26415;&#35770;&#25991;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#25991;&#20307;&#29305;&#24449;&#19978;&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#21253;&#25324;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#65292;&#24341;&#36215;&#20102;&#20840;&#29699;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT(-3.5&#21644;-4)&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#26085;&#35821;&#25991;&#20307;&#29305;&#24449;&#12290;&#36890;&#36807;&#22810;&#32500;&#23610;&#24230;&#20998;&#26512;&#65292;&#23558;216&#20010;&#25991;&#26412;&#65288;36&#20301;&#21333;&#19968;&#20316;&#32773;&#30340;72&#31687;&#23398;&#26415;&#35770;&#25991;&#12289;72&#31687;GPT-3.5&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;72&#31687;GPT-4&#29983;&#25104;&#30340;&#25991;&#26412;&#65289;&#26681;&#25454;&#35789;&#24615;&#30340;&#20108;&#20803;&#32452;&#65292;&#35789;&#23614;&#30340;&#20108;&#20803;&#32452;&#65292;&#36887;&#21495;&#30340;&#20301;&#32622;&#21644;&#21151;&#33021;&#35789;&#30340;&#27604;&#20363;&#20998;&#25104;&#19977;&#31867;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;GPT(-3.5&#65292;-4)&#21644;&#20154;&#31867;&#20043;&#38388;&#22312;&#25991;&#20307;&#29305;&#24449;&#19978;&#26377;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-generative artificial intelligence (AI), including ChatGPT, equipped with GPT-3.5 and GPT-4, from OpenAI, has attracted considerable attention worldwide. In this study, first, we compared Japanese stylometric features generated by GPT (-3.5 and -4) and those written by humans. In this work, we performed multi-dimensional scaling (MDS) to confirm the classification of 216 texts into three classes (72 academic papers written by 36 single authors, 72 texts generated by GPT-3.5, and 72 texts generated by GPT-4 on the basis of the titles of the aforementioned papers) focusing on the following stylometric features: (1) bigrams of parts-of-speech, (2) bigram of postpositional particle words, (3) positioning of commas, and (4) rate of function words. MDS revealed distinct distributions at each stylometric feature of GPT (-3.5 and -4) and human. Although GPT-4 is more powerful than GPT-3.5 because it has more parameters, both GPT (-3.5 and -4) distributions are likely to overlap. These res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05368</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#20934;&#22791;&#23601;&#32490;&#20102;&#21527;&#65311;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding. (arXiv:2304.05368v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#19987;&#19994;&#24615;&#36136;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;LLMs&#8212;&#8212;GPT-3.5&#12289;GPT-4&#21644;Bard&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#35813;&#35780;&#20272;&#33539;&#22260;&#28085;&#30422;&#20102;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#12289;&#25991;&#26723;&#20998;&#31867;&#21644;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#8212;&#8212;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#65288;SQP&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#21457;&#19982;&#30456;&#20851;&#20020;&#24202;&#22330;&#26223;&#30456;&#20851;&#30340;&#20449;&#24687;&#24615;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#23450;&#21046;&#21270;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#24378;&#35843;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#31574;&#30053;&#21644;&#25552;&#31034;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant progress in various domains, including healthcare. However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation. In this study, we conduct a comprehensive evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within the realm of clinical language understanding tasks. These tasks span a diverse range, including named entity recognition, relation extraction, natural language inference, semantic textual similarity, document classification, and question-answering. We also introduce a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand. Our evaluation underscores the significance of task-specific learning strategies and prompting techniques for improving LLMs' effectiveness in healthcare-related tasks.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;ChatGPT&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#24037;&#20855;&#26102;&#30340;&#36947;&#24503;&#32771;&#34385;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#21644;NLP&#39046;&#22495;&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.02017</link><description>&lt;p&gt;
&#35299;&#38145;ChatGPT&#30340;&#28508;&#21147;&#65306;&#23545;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24212;&#29992;&#12289;&#20248;&#28857;&#12289;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#20840;&#38754;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;ChatGPT&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#24037;&#20855;&#26102;&#30340;&#36947;&#24503;&#32771;&#34385;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#21644;NLP&#39046;&#22495;&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#35821;&#35328;&#32763;&#35793;&#12289;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#21307;&#30103;&#35786;&#26029;&#27835;&#30103;&#12290;&#23427;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#20934;&#30830;&#24615;&#20351;&#20854;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;ChatGPT&#20063;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#20854;&#20542;&#21521;&#20110;&#20135;&#29983;&#26377;&#20559;&#35265;&#30340;&#21709;&#24212;&#20197;&#21450;&#23384;&#22312;&#28508;&#22312;&#30340;&#26377;&#23475;&#35821;&#35328;&#27169;&#24335;&#12290;&#26412;&#25991;&#20840;&#38754;&#27010;&#36848;&#20102;ChatGPT&#21450;&#20854;&#24212;&#29992;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#24037;&#20855;&#26102;&#36947;&#24503;&#32771;&#34385;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#30340;&#35265;&#35299;&#65292;&#20026;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#21450;&#20854;&#23545;&#35270;&#35273;&#21644;NLP&#39046;&#22495;&#30340;&#24433;&#21709;&#30340;&#25345;&#32493;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a powerful tool in the field of artificial intelligence that has been widely used in various applications. ChatGPT has been applied successfully in chatbots, content generation, language translation, personalized recommendations, and medical diagnosis and treatment. Its versatility and accuracy make it a powerful tool for natural language processing (NLP). However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns. This article provides a comprehensive overview of ChatGPT, its applications, advantages, and limitations. Additionally, the paper emphasizes the importance of ethical considerations when using this robust tool in real-world scenarios. Finally, This paper contributes to ongoing discussions surrounding artificial intelligence and its impact on vision and NLP domains by providing insights into prompt engineering techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#20316;&#20026;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#35780;&#20272;&#22120;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20854;&#22312;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#21644;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15621</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
ChatGPT as a Factual Inconsistency Evaluator for Abstractive Text Summarization. (arXiv:2303.15621v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#20316;&#20026;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#35780;&#20272;&#22120;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20854;&#22312;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#21644;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22823;&#22823;&#25552;&#39640;&#20102;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#25277;&#35937;&#25688;&#35201;&#26041;&#27861;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#20854;&#29983;&#25104;&#30340;&#25688;&#35201;&#23384;&#22312;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#20026;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#35768;&#22810;&#21162;&#21147;&#23558;&#37325;&#28857;&#25918;&#22312;&#24320;&#21457;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#38382;&#31572;&#31561;&#26041;&#38754;&#30340;&#26377;&#25928;&#20107;&#23454;&#24615;&#35780;&#20272;&#25351;&#26631;&#19978;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#21644;&#20381;&#36182;&#27880;&#37322;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#26174;&#31034;&#20102;&#24378;&#22823;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#32780;&#19988;&#36824;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20107;&#23454;&#35780;&#20272;&#20219;&#21153;&#65288;&#21253;&#25324;&#20108;&#36827;&#21046;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#12289;&#25688;&#35201;&#25490;&#21517;&#21644;&#19968;&#33268;&#24615;&#35780;&#32423;&#65289;&#19978;&#35780;&#20272;ChatGPT&#30340;&#38646;-shot&#35774;&#32622;&#19979;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#35780;&#20272;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25152;&#26377;&#35780;&#20272;&#20219;&#21153;&#19978;&#22343;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of abstractive text summarization has been greatly boosted by pre-trained language models recently. The main concern of existing abstractive summarization methods is the factual inconsistency problem of their generated summary. To alleviate the problem, many efforts have focused on developing effective factuality evaluation metrics based on natural language inference and question answering et al. However, they have limitations of high computational complexity and relying on annotated data. Most recently, large language models such as ChatGPT have shown strong ability in not only natural language understanding but also natural language inference. In this paper, we study the factual inconsistency evaluation ability of ChatGPT under the zero-shot setting by evaluating it on the coarse-grained and fine-grained factuality evaluation tasks including binary natural language inference (NLI), summary ranking, and consistency rating. Experimental results show that ChatGPT outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#20197;&#20248;&#21270;&#21306;&#22495;&#20998;&#24067;&#24335;&#35745;&#31639;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; Hulk&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#25928;&#29575;&#65292;&#20943;&#23569;&#25968;&#25454;&#36890;&#20449;&#30340;&#24320;&#38144;&#65292;&#23454;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#20998;&#24067;&#24335;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2302.13741</link><description>&lt;p&gt;
Hulk: &#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#21306;&#22495;&#20998;&#24067;&#24335;&#35745;&#31639;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Hulk: Graph Neural Networks for Optimizing Regionally Distributed Computing Systems. (arXiv:2302.13741v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#20197;&#20248;&#21270;&#21306;&#22495;&#20998;&#24067;&#24335;&#35745;&#31639;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; Hulk&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#25928;&#29575;&#65292;&#20943;&#23569;&#25968;&#25454;&#36890;&#20449;&#30340;&#24320;&#38144;&#65292;&#23454;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#20998;&#24067;&#24335;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#22343;&#34920;&#29616;&#20986;&#26497;&#22823;&#30340;&#28508;&#21147;&#65292;&#28982;&#32780;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#30001;&#20110;&#21442;&#25968;&#25968;&#37327;&#20250;&#26497;&#22823;&#65292;&#36890;&#24120;&#26377;&#25968;&#21315;&#20159;&#20010;&#21442;&#25968;&#65292;&#22240;&#27492;&#35757;&#32451;&#21464;&#24471;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24120;&#35265;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#22914;&#25968;&#25454;&#24182;&#34892;&#12289;&#24352;&#37327;&#24182;&#34892;&#21644;&#27969;&#27700;&#32447;&#24182;&#34892;&#65292;&#38656;&#35201;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#36827;&#34892;&#22823;&#37327;&#25968;&#25454;&#36890;&#20449;&#65292;&#23548;&#33268;&#22312;&#29289;&#29702;&#19978;&#20998;&#24067;&#30340;&#36828;&#31243;&#31995;&#32479;&#20013;&#19968;&#20123;&#26426;&#22120;&#30340;&#31561;&#24453;&#26102;&#38388;&#21464;&#24471;&#24456;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026; Hulk&#65292;&#23427;&#21033;&#29992;&#25913;&#36827;&#21518;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20248;&#21270;&#20998;&#24067;&#24335;&#35745;&#31639;&#31995;&#32479;&#12290;Hulk &#19981;&#20165;&#21487;&#20197;&#20248;&#21270;&#19981;&#21516;&#22269;&#23478;&#29978;&#33267;&#21516;&#19968;&#20010;&#22478;&#24066;&#20869;&#19981;&#21516;&#21306;&#22495;&#20043;&#38388;&#30340;&#25968;&#25454;&#36890;&#20449;&#25928;&#29575;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#25552;&#20379;&#27169;&#22411;&#30340;&#26368;&#20339;&#20998;&#24067;&#24335;&#37096;&#32626;&#12290;&#20363;&#22914;&#65292;&#23427;&#21487;&#20197;&#23558;&#26576;&#20123;&#23618;&#25918;&#22312;&#29305;&#23450;&#21306;&#22495;&#30340;&#26426;&#22120;&#19978;&#25110;&#20256;&#36882;&#29305;&#23450;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large deep learning models have shown great potential for delivering exceptional results in various applications. However, the training process can be incredibly challenging due to the models' vast parameter sizes, often consisting of hundreds of billions of parameters. Common distributed training methods, such as data parallelism, tensor parallelism, and pipeline parallelism, demand significant data communication throughout the process, leading to prolonged wait times for some machines in physically distant distributed systems. To address this issue, we propose a novel solution called Hulk, which utilizes a modified graph neural network to optimize distributed computing systems. Hulk not only optimizes data communication efficiency between different countries or even different regions within the same city, but also provides optimal distributed deployment of models in parallel. For example, it can place certain layers on a machine in a specific region or pass specific parameters of a m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.04054</link><description>&lt;p&gt;
&#36861;&#27714;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#25512;&#29702;&#22797;&#29616;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#8212;&#8212;&#21363;&#22312;&#22797;&#21046;&#30340;&#27169;&#22411;&#35757;&#32451;&#36816;&#34892;&#20013;&#35266;&#23519;&#21040;&#30340;&#35780;&#20272;&#20998;&#25968;&#30340;&#19968;&#33268;&#24615;&#8212;&#8212;&#21463;&#21040;&#20960;&#31181;&#38750;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#27979;&#37327;&#22122;&#22768;&#12290;&#30446;&#21069;&#30340;&#36235;&#21183;&#26159;&#21435;&#38500;&#22122;&#22768;&#65292;&#20197;&#24378;&#21046;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#24573;&#30053;&#20102;&#23454;&#29616;&#23618;&#38754;&#22266;&#26377;&#30340;&#38750;&#30830;&#23450;&#24615;&#20197;&#21450;&#31639;&#27861;&#22122;&#22768;&#22240;&#32032;&#21644;&#25968;&#25454;&#29305;&#24615;&#20043;&#38388;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;&#36825;&#38480;&#21046;&#20102;&#20174;&#36825;&#20123;&#23454;&#39564;&#20013;&#21487;&#20197;&#24471;&#20986;&#30340;&#32467;&#35770;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23558;&#20960;&#20010;&#26041;&#24046;&#26469;&#28304;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982;&#25968;&#25454;&#29305;&#24615;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#26174;&#33879;&#24615;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#20013;&#65292;&#20197;&#26399;&#20174;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#23450;&#23454;&#20363;&#24471;&#20986;&#25512;&#29702;&#32467;&#35770;, &#32780;&#38750;&#21435;&#38500;&#22122;&#22768;&#12290;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#29992;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#24335;&#26469;&#32771;&#34385;&#31639;&#27861;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#28304;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#21508;&#20010;&#26041;&#24046;&#26469;&#28304;&#23545;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability of machine learning evaluation -- the consistency of observed evaluation scores across replicated model training runs -- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized lik
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#35821;&#20041;&#35299;&#26512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;(AL-MSP), &#36873;&#25321;&#19968;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#36827;&#34892;&#32763;&#35793;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#22810;&#26679;&#21270;&#36923;&#36753;&#24418;&#24335;&#32467;&#26500;&#21644;&#26356;&#22810;&#35789;&#27719;&#36873;&#25321;&#30340;&#31034;&#20363;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#39069;&#22806;&#27880;&#37322;&#25104;&#26412;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#32763;&#35793;&#25104;&#26412;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#22522;&#32447;&#26356;&#22909;&#30340;&#35299;&#26512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12920</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#20041;&#35299;&#26512;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Multilingual Semantic Parser. (arXiv:2301.12920v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#35821;&#20041;&#35299;&#26512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;(AL-MSP), &#36873;&#25321;&#19968;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#36827;&#34892;&#32763;&#35793;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#22810;&#26679;&#21270;&#36923;&#36753;&#24418;&#24335;&#32467;&#26500;&#21644;&#26356;&#22810;&#35789;&#27719;&#36873;&#25321;&#30340;&#31034;&#20363;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#39069;&#22806;&#27880;&#37322;&#25104;&#26412;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#32763;&#35793;&#25104;&#26412;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#22522;&#32447;&#26356;&#22909;&#30340;&#35299;&#26512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22810;&#35821;&#35328;&#35821;&#20041;&#35299;&#26512;(MSP) &#25968;&#25454;&#38598;&#20960;&#20046;&#37117;&#26159;&#36890;&#36807;&#23558;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#35805;&#35821;&#20174;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#32763;&#35793;&#21040;&#30446;&#26631;&#35821;&#35328;&#32780;&#25910;&#38598;&#30340;&#12290;&#20294;&#26159;&#65292;&#25163;&#21160;&#32763;&#35793;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#20943;&#23569;&#32763;&#35793;&#24037;&#20316;&#37327;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;MSP&#30340;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;(AL-MSP)&#12290;AL-MSP&#21482;&#36873;&#25321;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#36827;&#34892;&#32763;&#35793;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#26041;&#27861;&#65292;&#20248;&#20808;&#36873;&#25321;&#22810;&#26679;&#21270;&#36923;&#36753;&#24418;&#24335;&#32467;&#26500;&#21644;&#26356;&#22810;&#35789;&#27719;&#36873;&#25321;&#30340;&#31034;&#20363;&#65292;&#20197;&#21450;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#27880;&#37322;&#25104;&#26412;&#30340;&#26032;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#29702;&#24819;&#30340;&#36873;&#25321;&#26041;&#27861;&#65292;AL-MSP&#26174;&#33879;&#20943;&#23569;&#20102;&#32763;&#35793;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#36873;&#25321;&#26041;&#27861;&#19982;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#21487;&#20197;&#22312;&#20004;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#27604;&#20854;&#20182;&#22522;&#32447;&#26356;&#22909;&#30340;&#35299;&#26512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current multilingual semantic parsing (MSP) datasets are almost all collected by translating the utterances in the existing datasets from the resource-rich language to the target language. However, manual translation is costly. To reduce the translation effort, this paper proposes the first active learning procedure for MSP (AL-MSP). AL-MSP selects only a subset from the existing datasets to be translated. We also propose a novel selection method that prioritizes the examples diversifying the logical form structures with more lexical choices, and a novel hyperparameter tuning method that needs no extra annotation cost. Our experiments show that AL-MSP significantly reduces translation costs with ideal selection methods. Our selection method with proper hyperparameters yields better parsing performance than the other baselines on two multilingual datasets.
&lt;/p&gt;</description></item><item><title>RPN&#26159;&#19968;&#31181;&#22522;&#20110;&#35789;&#21521;&#37327;&#32423;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#20462;&#25913;&#21407;&#22987;&#25991;&#26412;&#30340;&#35789;&#23884;&#20837;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.05961</link><description>&lt;p&gt;
RPN: &#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20013;&#22522;&#20110;&#35789;&#21521;&#37327;&#30340;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning for Language Understanding. (arXiv:2212.05961v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05961
&lt;/p&gt;
&lt;p&gt;
RPN&#26159;&#19968;&#31181;&#22522;&#20110;&#35789;&#21521;&#37327;&#32423;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#20462;&#25913;&#21407;&#22987;&#25991;&#26412;&#30340;&#35789;&#23884;&#20837;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#33021;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#22797;&#26434;&#21464;&#21270;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#24212;&#29992;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#8212;&#8212;&#38543;&#26426;&#20301;&#32622;&#22122;&#22768;&#65288;RPN&#65289;&#31639;&#27861;&#65292;&#23427;&#22312;&#35789;&#21521;&#37327;&#32423;&#21035;&#19978;&#36827;&#34892;&#25805;&#20316;&#12290;RPN&#36890;&#36807;&#26681;&#25454;&#36873;&#23450;&#35789;&#21521;&#37327;&#30340;&#29616;&#26377;&#20540;&#24341;&#20837;&#22122;&#22768;&#20462;&#25913;&#21407;&#22987;&#25991;&#26412;&#30340;&#35789;&#23884;&#20837;&#65292;&#20801;&#35768;&#26356;&#32454;&#31890;&#24230;&#30340;&#20462;&#25913;&#24182;&#26356;&#22909;&#22320;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#21464;&#21270;&#12290;&#19982;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#19981;&#21516;&#65292;RPN&#19981;&#38656;&#35201;&#35745;&#31639;&#22270;&#20013;&#30340;&#26799;&#24230;&#26469;&#36827;&#34892;&#34394;&#25311;&#26679;&#26412;&#26356;&#26032;&#65292;&#20351;&#20854;&#26356;&#23481;&#26131;&#24212;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#31561;&#65292;RPN&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is a widely used technique in machine learning to improve model performance. However, existing data augmentation techniques in natural language understanding (NLU) may not fully capture the complexity of natural language variations, and they can be challenging to apply to large datasets. This paper proposes the Random Position Noise (RPN) algorithm, a novel data augmentation technique that operates at the word vector level. RPN modifies the word embeddings of the original text by introducing noise based on the existing values of selected word vectors, allowing for more fine-grained modifications and better capturing natural language variations. Unlike traditional data augmentation methods, RPN does not require gradients in the computational graph during virtual sample updates, making it simpler to apply to large datasets. Experimental results demonstrate that RPN consistently outperforms existing data augmentation techniques across various NLU tasks, including sentime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#35780;&#20272;&#20855;&#26377;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#22312;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#24773;&#22659;&#38382;&#31572;&#12290;&#22522;&#20110;650&#20010;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#20026;&#26234;&#33021;&#20195;&#29702;&#20154;&#30340;&#25512;&#29702;&#33021;&#21147;&#32771;&#23519;&#25552;&#20379;&#20102;&#24191;&#27867;&#19988;&#22823;&#37327;&#30340;&#38382;&#39064;&#65292;&#36825;&#23545;&#24403;&#21069;&#30340;&#22810;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;3D&#25512;&#29702;&#27169;&#22411;&#25552;&#20986;&#20102;&#24456;&#22823;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2210.07474</link><description>&lt;p&gt;
SQA3D&#65306;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#24773;&#22659;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
SQA3D: Situated Question Answering in 3D Scenes. (arXiv:2210.07474v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#35780;&#20272;&#20855;&#26377;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#22312;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#24773;&#22659;&#38382;&#31572;&#12290;&#22522;&#20110;650&#20010;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#20026;&#26234;&#33021;&#20195;&#29702;&#20154;&#30340;&#25512;&#29702;&#33021;&#21147;&#32771;&#23519;&#25552;&#20379;&#20102;&#24191;&#27867;&#19988;&#22823;&#37327;&#30340;&#38382;&#39064;&#65292;&#36825;&#23545;&#24403;&#21069;&#30340;&#22810;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;3D&#25512;&#29702;&#27169;&#22411;&#25552;&#20986;&#20102;&#24456;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#26469;&#35780;&#20272;&#20855;&#26377;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#65306;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#24773;&#22659;&#38382;&#31572;&#65288;SQA3D&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#22330;&#26223;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#19977;&#32500;&#25195;&#25551;&#65289;&#65292;SQA3D&#35201;&#27714;&#32463;&#36807;&#27979;&#35797;&#30340;&#20195;&#29702;&#20154;&#39318;&#20808;&#29702;&#35299;&#20854;&#22312;&#25991;&#26412;&#25551;&#36848;&#19979;&#30340;3D&#22330;&#26223;&#20013;&#30340;&#24773;&#22659;&#65288;&#20301;&#32622;&#12289;&#26041;&#21521;&#31561;&#65289;&#65292;&#28982;&#21518;&#22312;&#35813;&#24773;&#22659;&#19979;&#36827;&#34892;&#25512;&#29702;&#65292;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#26469;&#33258;ScanNet&#30340;650&#20010;&#22330;&#26223;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#24515;&#22260;&#32469;6.8k&#20010;&#21807;&#19968;&#24773;&#22659;&#65292;20.4k&#30340;&#25551;&#36848;&#21644;33.4k&#22810;&#26679;&#30340;&#25512;&#29702;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#28085;&#30422;&#20102;&#23545;&#26234;&#33021;&#20195;&#29702;&#20154;&#33539;&#22260;&#24191;&#27867;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#32771;&#23519;&#65292;&#20174;&#31354;&#38388;&#20851;&#31995;&#29702;&#35299;&#21040;&#24120;&#35782;&#29702;&#35299;&#12289;&#23548;&#33322;&#21644;&#22810;&#36339;&#25512;&#29702;&#12290;SQA3D&#23545;&#24403;&#21069;&#30340;&#22810;&#27169;&#24335;&#23588;&#20854;&#26159;3D&#25512;&#29702;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#26368;&#20339;&#32467;&#26524;&#20165;&#36798;&#21040;&#20102;47.20%&#30340;&#24635;&#20307;&#24471;&#20998;&#65292;&#32780;&#19994;&#20313;&#27700;&#24179;&#30340;&#34920;&#29616;&#26356;&#20026;&#31967;&#31957;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#27969;&#30021;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35821;&#27861;&#32467;&#26500;&#20250;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#20135;&#29983;&#24433;&#21709;&#65292;&#23548;&#33268;&#22810;&#35821;&#35328;BERT&#20855;&#26377;&#33521;&#35821;&#35821;&#38899;&#29305;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.05619</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;BERT&#24102;&#26377;&#35821;&#38899;&#29305;&#33394;&#65306;&#35780;&#20272;&#33521;&#35821;&#23545;&#22810;&#35821;&#35328;&#27169;&#22411;&#27969;&#30021;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models. (arXiv:2210.05619v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#27969;&#30021;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35821;&#27861;&#32467;&#26500;&#20250;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#20135;&#29983;&#24433;&#21709;&#65292;&#23548;&#33268;&#22810;&#35821;&#35328;BERT&#20855;&#26377;&#33521;&#35821;&#35821;&#38899;&#29305;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20063;&#20250;&#38477;&#20302;&#25152;&#26377;&#35821;&#35328;&#30340;&#24179;&#22343;&#24615;&#33021;&#65288;&#8220;&#22810;&#35821;&#35328;&#35781;&#21650;&#8221;&#65289;&#12290;&#26412;&#25991;&#26174;&#31034;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#38382;&#39064;&#65306;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35821;&#27861;&#32467;&#26500;&#20250;&#28183;&#20837;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#35821;&#27861;&#32467;&#26500;&#20559;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#26041;&#27861;&#27604;&#36739;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#30021;&#24615;&#19982;&#21333;&#35821;&#35199;&#29677;&#29273;&#35821;&#21644;&#24076;&#33098;&#35821;&#27169;&#22411;&#30340;&#27969;&#30021;&#24615;&#65292;&#23637;&#31034;&#20102;&#36825;&#31181;&#27169;&#22411;&#30340;&#20559;&#24046;&#65306;&#27979;&#35797;&#23427;&#20204;&#23545;&#20004;&#31181;&#31934;&#24515;&#36873;&#25321;&#30340;&#21487;&#21464;&#35821;&#27861;&#32467;&#26500;&#30340;&#20559;&#22909;&#65288;&#35199;&#29677;&#29273;&#35821;&#20013;&#30340;&#21487;&#36873;&#30465;&#30053;&#20195;&#35789;&#21644;&#24076;&#33098;&#35821;&#20013;&#30340;&#21487;&#36873;&#20027;&#35821;-&#35859;&#35821;&#35843;&#24207;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#25105;&#20204;&#30340;&#21333;&#35821;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#22810;&#35821;&#35328;BERT&#20559;&#21521;&#20110;&#31867;&#20284;&#33521;&#35821;&#30340;&#35774;&#32622;&#65288;&#26174;&#24335;&#20195;&#35789;&#21644;&#20027;-&#35859;-&#23486;&#35821;&#35843;&#24207;&#65289;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#24076;&#26395;&#25581;&#31034;&#22810;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#31934;&#32454;&#30340;&#26041;&#24335;&#24433;&#21709;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27969;&#30021;&#24615;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
While multilingual language models can improve NLP performance on low-resource languages by leveraging higher-resource languages, they also reduce average performance on all languages (the 'curse of multilinguality'). Here we show another problem with multilingual models: grammatical structures in higher-resource languages bleed into lower-resource languages, a phenomenon we call grammatical structure bias. We show this bias via a novel method for comparing the fluency of multilingual models to the fluency of monolingual Spanish and Greek models: testing their preference for two carefully-chosen variable grammatical structures (optional pronoun-drop in Spanish and optional Subject-Verb ordering in Greek). We find that multilingual BERT is biased toward the English-like setting (explicit pronouns and Subject-Verb-Object ordering) as compared to our monolingual control language model. With our case studies, we hope to bring to light the fine-grained ways in which multilingual models can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#33258;&#21160;&#21518;&#32534;&#36753;&#26694;&#26550;PePe&#65292;&#22312;&#19968;&#20010;&#23454;&#26102;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#25910;&#38598;&#29992;&#25143;&#21518;&#32534;&#36753;&#25968;&#25454;&#65292;&#32467;&#21512;&#21028;&#21035;&#27169;&#22359;&#21644;&#29992;&#25143;&#29305;&#23450;&#21442;&#25968;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#32771;&#34385;&#21040;&#19981;&#21516;&#20010;&#20154;&#34892;&#20026;&#30340;&#21477;&#23376;&#65292;&#24182;&#22312;&#22810;&#20010;&#24230;&#37327;&#26631;&#20934;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2209.10139</link><description>&lt;p&gt;
PePe: &#21033;&#29992;&#29992;&#25143;&#29983;&#25104;&#30340;&#21518;&#32534;&#36753;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#21518;&#32534;&#36753;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PePe: Personalized Post-editing Model utilizing User-generated Post-edits. (arXiv:2209.10139v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#33258;&#21160;&#21518;&#32534;&#36753;&#26694;&#26550;PePe&#65292;&#22312;&#19968;&#20010;&#23454;&#26102;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#25910;&#38598;&#29992;&#25143;&#21518;&#32534;&#36753;&#25968;&#25454;&#65292;&#32467;&#21512;&#21028;&#21035;&#27169;&#22359;&#21644;&#29992;&#25143;&#29305;&#23450;&#21442;&#25968;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#32771;&#34385;&#21040;&#19981;&#21516;&#20010;&#20154;&#34892;&#20026;&#30340;&#21477;&#23376;&#65292;&#24182;&#22312;&#22810;&#20010;&#24230;&#37327;&#26631;&#20934;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20808;&#36827;&#30340;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#65292;&#23558;&#20010;&#20154;&#30340;&#20559;&#22909;&#21152;&#20197;&#32771;&#34385;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26426;&#22120;&#32763;&#35793;&#24050;&#32463;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26159;&#27491;&#30830;&#21453;&#26144;&#20010;&#20154;&#39118;&#26684;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#33258;&#21160;&#21518;&#32534;&#36753;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#65292;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#32771;&#34385;&#21040;&#19981;&#21516;&#20010;&#20154;&#34892;&#20026;&#30340;&#21477;&#23376;&#12290;&#20026;&#20102;&#26500;&#24314;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#23454;&#26102;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#25910;&#38598;&#21253;&#21547;&#29992;&#25143;&#20559;&#22909;&#30340;&#21518;&#32534;&#36753;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30495;&#23454;&#29992;&#25143;&#36755;&#20837;&#24819;&#35201;&#32763;&#35793;&#30340;&#28304;&#35821;&#21477;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#39118;&#26684;&#20559;&#22909;&#32534;&#36753;&#26426;&#22120;&#32763;&#35793;&#30340;&#36755;&#20986;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#22312;APE&#26694;&#26550;&#19978;&#32467;&#21512;&#20102;&#19968;&#20010;&#21028;&#21035;&#27169;&#22359;&#21644;&#29992;&#25143;&#29305;&#23450;&#30340;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#21363;BLEU&#65292;TER&#65292;YiSi-1&#21644;&#20154;&#31867;&#35780;&#20272;&#65289;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating personal preference is crucial in advanced machine translation tasks. Despite the recent advancement of machine translation, it remains a demanding task to properly reflect personal style. In this paper, we introduce a personalized automatic post-editing framework to address this challenge, which effectively generates sentences considering distinct personal behaviors. To build this framework, we first collect post-editing data that connotes the user preference from a live machine translation system. Specifically, real-world users enter source sentences for translation and edit the machine-translated outputs according to the user's preferred style. We then propose a model that combines a discriminator module and user-specific parameters on the APE framework. Experimental results show that the proposed method outperforms other baseline models on four different metrics (i.e., BLEU, TER, YiSi-1, and human evaluation).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;AI&#35268;&#21010;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#26469;&#25913;&#21892;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2202.07138</link><description>&lt;p&gt;
&#23558;AI&#35268;&#21010;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38598;&#25104;&#65306;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#30340;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Integrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge. (arXiv:2202.07138v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;AI&#35268;&#21010;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#26469;&#25913;&#21892;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26088;&#22312;&#30740;&#31350;&#20195;&#29702;&#20154;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#22788;&#29702;&#21644;&#20998;&#26512;&#22823;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#12290;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#36923;&#36753;&#20851;&#31995;&#21644;&#35268;&#21017;&#24341;&#20837;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#65292;&#20363;&#22914;&#21033;&#29992;&#33258;&#21160;&#35268;&#21010;&#25216;&#26415;&#12290;&#33258;&#21160;&#35268;&#21010;&#21363;AI&#35268;&#21010;&#65292;&#20391;&#37325;&#20110;&#26500;&#24314;&#31526;&#21495;&#22495;&#27169;&#22411;&#24182;&#32508;&#21512;&#35268;&#21010;&#65292;&#20197;&#22522;&#20110;&#22495;&#27169;&#22411;&#23558;&#21021;&#22987;&#29366;&#24577;&#36716;&#25442;&#20026;&#30446;&#26631;&#12290;&#26368;&#36817;&#65292;&#26377;&#35768;&#22810;&#19982;&#36825;&#20004;&#20010;&#39046;&#22495;&#30456;&#20851;&#30340;&#24037;&#20316;&#65292;&#21487;&#20197;&#20135;&#29983;&#26174;&#24335;&#30693;&#35782;&#65292;&#20363;&#22914;&#25805;&#20316;&#27169;&#22411;&#30340;&#21069;&#25552;&#26465;&#20214;&#21644;&#25928;&#26524;&#65292;&#24182;&#20174;&#38544;&#24335;&#30693;&#35782;&#65288;&#20363;&#22914;&#31070;&#32463;&#27169;&#22411;&#65289;&#20013;&#23398;&#20064;&#12290;&#26377;&#25928;&#22320;&#23558;AI&#35268;&#21010;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38598;&#25104;&#21487;&#20197;&#25913;&#21892;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#27969;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#65292;&#29992;&#20110;&#23558;AI&#35268;&#21010;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;AI&#35268;&#21010;&#25216;&#26415;&#26500;&#24314;&#22495;&#27169;&#22411;&#24182;&#25512;&#29702;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20174;&#38544;&#24335;&#30693;&#35782;&#20013;&#23398;&#20064;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#30340;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning. Automated planning (AI planning) focuses on building symbolic domain models and synthesizing plans to transit initial states to goals based on domain models. Recently, there have been plenty of works related to these two fields, which have the abilities to generate explicit knowledge, e.g., preconditions and effects of action models, and learn from tacit knowledge, e.g., neural models, respectively. Integrating AI planning and natural language processing effectively improves the communication b
&lt;/p&gt;</description></item></channel></rss>