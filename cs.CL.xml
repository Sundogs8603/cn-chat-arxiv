<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ChartT5&#30340;V+L&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#32472;&#22270;&#34920;&#23545;&#36827;&#34892;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#23398;&#20064;&#22914;&#20309;&#35299;&#37322;&#26469;&#33258;&#22270;&#34920;&#22270;&#20687;&#30340;&#34920;&#26684;&#20449;&#24687;&#65292;&#24182;&#22312;&#22270;&#34920;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;8%&#20197;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.18641</link><description>&lt;p&gt;
&#36890;&#36807;&#32472;&#22270;&#34920;&#23545;&#30340;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#22270;&#34920;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs. (arXiv:2305.18641v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18641
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ChartT5&#30340;V+L&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#32472;&#22270;&#34920;&#23545;&#36827;&#34892;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#23398;&#20064;&#22914;&#20309;&#35299;&#37322;&#26469;&#33258;&#22270;&#34920;&#22270;&#20687;&#30340;&#34920;&#26684;&#20449;&#24687;&#65292;&#24182;&#22312;&#22270;&#34920;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;8%&#20197;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35821;&#35328;&#39046;&#22495;&#65292;&#24314;&#31435;&#21487;&#20197;&#29702;&#35299;&#22270;&#34920;&#24182;&#20256;&#36798;&#20854;&#20013;&#38544;&#34255;&#30340;&#20449;&#24687;&#30340;&#36328;&#27169;&#24577;&#26234;&#33021;&#26159;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#25361;&#25112;&#12290;&#25581;&#31034;&#22270;&#34920;&#25968;&#25454;&#30340;&#20851;&#38190;&#26159;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ChartT5&#65292;&#36825;&#26159;&#19968;&#20010;V + L&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#32472;&#22270;&#34920;&#23545;&#36827;&#34892;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#26469;&#23398;&#20064;&#22914;&#20309;&#35299;&#37322;&#26469;&#33258;&#22270;&#34920;&#22270;&#20687;&#30340;&#34920;&#26684;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65306;&#36974;&#32617;&#26631;&#39064;&#39044;&#27979;&#65288;MHP&#65289;&#21644;&#36974;&#32617;&#20540;&#39044;&#27979;&#65288;MVP&#65289;&#65292;&#20197;&#20351;&#27169;&#22411;&#20855;&#22791;&#19981;&#21516;&#30340;&#25216;&#33021;&#26469;&#35299;&#37322;&#34920;&#26684;&#20449;&#24687;&#12290;&#25105;&#20204;&#23545;&#22270;&#34920;&#38382;&#31572;&#21644;&#22270;&#34920;&#25688;&#35201;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#22312;ChartQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;ChartT5&#30340;&#24615;&#33021;&#27604;&#26368;&#20808;&#36827;&#30340;&#38750;&#39044;&#35757;&#32451;&#26041;&#27861;&#25552;&#39640;&#20102;8%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language(V+L) community. The capability to uncover the underlined table data of chart figures is a critical key to automatic chart understanding. We introduce ChartT5, a V+L model that learns how to interpret table information from chart images via cross-modal pre-training on plot table pairs. Specifically, we propose two novel pre-training objectives: Masked Header Prediction (MHP) and Masked Value Prediction (MVP) to facilitate the model with different skills to interpret the table information. We have conducted extensive experiments on chart question answering and chart summarization to verify the effectiveness of the proposed pre-training strategies. In particular, on the ChartQA benchmark, our ChartT5 outperforms the state-of-the-art non-pretraining methods by over 8% performance gains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#30701;&#22238;&#31572;&#35780;&#20998;&#65288;ASAG&#65289;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#27425;&#25552;&#31034;&#21644;&#25991;&#26412;&#30456;&#20284;&#24230;&#35780;&#20998;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#20998;&#26512;&#24471;&#20998;&#21644;&#26368;&#32456;&#25972;&#20307;&#24471;&#20998;&#65292;&#24182;&#20351;&#29992;&#23567;&#35268;&#27169;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.18638</link><description>&lt;p&gt;
&#37319;&#29992;&#19968;&#27425;&#25552;&#31034;&#21644;&#25991;&#26412;&#30456;&#20284;&#24230;&#35780;&#20998;&#27169;&#22411;&#36827;&#34892;&#30701;&#22238;&#31572;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Short Answer Grading Using One-shot Prompting and Text Similarity Scoring Model. (arXiv:2305.18638v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#30701;&#22238;&#31572;&#35780;&#20998;&#65288;ASAG&#65289;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#27425;&#25552;&#31034;&#21644;&#25991;&#26412;&#30456;&#20284;&#24230;&#35780;&#20998;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#20998;&#26512;&#24471;&#20998;&#21644;&#26368;&#32456;&#25972;&#20307;&#24471;&#20998;&#65292;&#24182;&#20351;&#29992;&#23567;&#35268;&#27169;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#30701;&#22238;&#31572;&#35780;&#20998;&#65288;ASAG&#65289;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20998;&#26512;&#24471;&#20998;&#21644;&#26368;&#32456;&#25972;&#20307;&#24471;&#20998;&#12290;&#20026;&#20102;&#22686;&#21152;&#33258;&#21160;&#21270;&#24471;&#20998;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#30701;&#22238;&#31572;&#39033;&#36890;&#24120;&#21253;&#21547;&#22810;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#19988;&#25552;&#20379;&#27599;&#20010;&#23376;&#38382;&#39064;&#30456;&#20851;&#30340;&#25991;&#26412;&#33539;&#22260;&#21644;&#20998;&#26512;&#24471;&#20998;&#21487;&#20197;&#22686;&#21152;&#20854;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21487;&#20197;&#29992;&#26469;&#20026;&#23398;&#29983;&#25552;&#20379;&#21487;&#25191;&#34892;&#30340;&#21453;&#39304;&#12290;&#23613;&#31649;&#20855;&#26377;&#36825;&#20123;&#20248;&#21183;&#65292;&#20294;&#30001;&#20110;&#38590;&#20197;&#26500;&#24314;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#21482;&#20851;&#27880;&#39044;&#27979;&#25972;&#20307;&#24471;&#20998;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#27425;&#25552;&#31034;&#21644;&#25991;&#26412;&#30456;&#20284;&#24230;&#35780;&#20998;&#27169;&#22411;&#65292;&#20351;&#29992;&#23567;&#35268;&#27169;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#21644;&#20108;&#27425;&#21152;&#26435;kappa&#31995;&#25968;&#20998;&#21035;&#20026;0.67&#21644;0.71&#65292;&#36825;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;ASAG&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we developed an automated short answer grading (ASAG) model that provided both analytic scores and final holistic scores. Short answer items typically consist of multiple sub-questions, and providing an analytic score and the text span relevant to each sub-question can increase the interpretability of the automated scores. Furthermore, they can be used to generate actionable feedback for students. Despite these advantages, most studies have focused on predicting only holistic scores due to the difficulty in constructing dataset with manual annotations. To address this difficulty, we used large language model (LLM)-based one-shot prompting and a text similarity scoring model with domain adaptation using small manually annotated dataset. The accuracy and quadratic weighted kappa of our model were 0.67 and 0.71 on a subset of the publicly available ASAG dataset. The model achieved a substantial improvement over the majority baseline.
&lt;/p&gt;</description></item><item><title>W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18624</link><description>&lt;p&gt;
W-procer: &#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18624
&lt;/p&gt;
&lt;p&gt;
W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20256;&#32479;&#37197;&#32622;&#21147;&#27714;&#20943;&#23569;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#22686;&#21152;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#23384;&#22312;&#22823;&#37327;&#34987;&#27880;&#37322;&#20026;&#8220;O&#8221;&#65288;&#21363;&#8220;OUTSIDE&#8221;&#65289;&#30340;&#23454;&#20307;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#24076;&#26395;&#34987;&#25512;&#31163;&#21040;&#24403;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26631;&#35760;&#20026;&#8220;O&#8221;&#20197;&#22806;&#30340;&#20854;&#20182;&#23454;&#20307;&#65292;&#36825;&#31181;&#35774;&#23450;&#25928;&#26524;&#19981;&#20339;&#65292;&#21487;&#33021;&#20250;&#24471;&#20986;&#21547;&#26377;&#22122;&#22768;&#21407;&#22411;&#26631;&#31614;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#8220;O&#8221;&#26631;&#31614;&#23454;&#20307;&#19982;&#26377;&#26631;&#31614;&#23454;&#20307;&#30456;&#20851;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;W-PROCER&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#22260;&#32469;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#23637;&#24320;&#12290;&#36825;&#20123;&#32452;&#20214;&#22312;&#21327;&#21161;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;W-PROCER&#24212;&#29992;&#20110;&#19968;&#20010;&#20844;&#20849;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
&lt;/p&gt;</description></item><item><title>Alfred&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#31034;&#21019;&#24314;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#30001;&#19987;&#23478;&#32534;&#20889;&#30340;&#31243;&#24207;&#24335;&#24369;&#30417;&#30563;(PWS)&#31995;&#32479;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20026;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#32534;&#20889;&#20027;&#39064;&#19987;&#19994;&#30693;&#35782;&#12290;Alfred&#20026;&#36825;&#31181;&#26032;&#20852;&#33539;&#24335;&#30340;&#20851;&#38190;&#27493;&#39588;&#25552;&#20379;&#31616;&#21333;&#30340;Python&#25509;&#21475;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#26631;&#27880;&#30340;&#39640;&#21534;&#21520;&#37327;&#21518;&#31471;&#12290;</title><link>http://arxiv.org/abs/2305.18623</link><description>&lt;p&gt;
Alfred: &#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#36827;&#34892;&#24369;&#30417;&#30563;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Alfred: A System for Prompted Weak Supervision. (arXiv:2305.18623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18623
&lt;/p&gt;
&lt;p&gt;
Alfred&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#31034;&#21019;&#24314;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#30001;&#19987;&#23478;&#32534;&#20889;&#30340;&#31243;&#24207;&#24335;&#24369;&#30417;&#30563;(PWS)&#31995;&#32479;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20026;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#32534;&#20889;&#20027;&#39064;&#19987;&#19994;&#30693;&#35782;&#12290;Alfred&#20026;&#36825;&#31181;&#26032;&#20852;&#33539;&#24335;&#30340;&#20851;&#38190;&#27493;&#39588;&#25552;&#20379;&#31616;&#21333;&#30340;Python&#25509;&#21475;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#26631;&#27880;&#30340;&#39640;&#21534;&#21520;&#37327;&#21518;&#31471;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Alfred&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#25552;&#31034;&#21019;&#24314;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#30340;&#31243;&#24207;&#24335;&#24369;&#30417;&#30563;(PWS)&#31995;&#32479;&#12290;&#19982;&#20856;&#22411;&#30340;PWS&#31995;&#32479;&#19981;&#21516;&#65292;&#20854;&#20013;&#24369;&#30417;&#30563;&#28304;&#30001;&#19987;&#23478;&#32534;&#20889;&#30340;&#31243;&#24207;&#65292;Alfred&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20026;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#32534;&#20889;&#20027;&#39064;&#19987;&#19994;&#30693;&#35782;&#12290;Alfred&#20026;&#36825;&#31181;&#26032;&#20852;&#33539;&#24335;&#30340;&#20851;&#38190;&#27493;&#39588;&#25552;&#20379;&#31616;&#21333;&#30340;Python&#25509;&#21475;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#26631;&#27880;&#30340;&#39640;&#21534;&#21520;&#37327;&#21518;&#31471;&#12290;&#29992;&#25143;&#21487;&#20197;&#24555;&#36895;&#21019;&#24314;&#12289;&#35780;&#20272;&#21644;&#23436;&#21892;&#22522;&#20110;&#25552;&#31034;&#30340;&#24369;&#30417;&#30563;&#26469;&#28304;&#65307;&#23558;&#32467;&#26524;&#26144;&#23556;&#21040;&#24369;&#26631;&#31614;&#65307;&#24182;&#20351;&#29992;&#26631;&#31614;&#27169;&#22411;&#35299;&#20915;&#19981;&#21516;&#24847;&#35265;&#12290;Alfred&#25903;&#25345;&#34987;&#33258;&#31649;&#29702;&#30340;&#35745;&#31639;&#38598;&#32676;&#25552;&#20379;&#30340;&#27169;&#22411;&#26381;&#21153;&#65292;&#23454;&#29616;&#26080;&#32541;&#30340;&#26412;&#22320;&#24320;&#21457;&#20307;&#39564;&#12290;&#23427;&#20351;&#29992;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#26426;&#21046;&#33258;&#21160;&#20248;&#21270;&#25552;&#31034;&#30340;&#25191;&#34892;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#31616;&#21333;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#20248;&#21270;&#21487;&#20197;&#23558;&#26597;&#35810;&#21534;&#21520;&#37327;&#25552;&#39640;2.9&#20493;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alfred is the first system for programmatic weak supervision (PWS) that creates training data for machine learning by prompting. In contrast to typical PWS systems where weak supervision sources are programs coded by experts, Alfred enables users to encode their subject matter expertise via natural language prompts for language and vision-language models. Alfred provides a simple Python interface for the key steps of this emerging paradigm, with a high-throughput backend for large-scale data labeling. Users can quickly create, evaluate, and refine their prompt-based weak supervision sources; map the results to weak labels; and resolve their disagreements with a label model. Alfred enables a seamless local development experience backed by models served from self-managed computing clusters. It automatically optimizes the execution of prompts with optimized batching mechanisms. We find that this optimization improves query throughput by 2.9x versus a naive approach. We present two example
&lt;/p&gt;</description></item><item><title>CONA&#26159;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25351;&#20196;&#33539;&#24335;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33258;&#21160;&#20248;&#21270;&#28436;&#31034;&#20869;&#23481;&#24182;&#25552;&#20379;&#19978;&#19979;&#25991;&#24863;&#30693;&#22411;&#31572;&#26696;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#24615;&#21644;&#26131;&#29702;&#35299;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18620</link><description>&lt;p&gt;
CONA: &#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25351;&#20196;&#33539;&#24335;&#65292;&#29992;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
CONA: A novel CONtext-Aware instruction paradigm for communication using large language model. (arXiv:2305.18620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18620
&lt;/p&gt;
&lt;p&gt;
CONA&#26159;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25351;&#20196;&#33539;&#24335;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33258;&#21160;&#20248;&#21270;&#28436;&#31034;&#20869;&#23481;&#24182;&#25552;&#20379;&#19978;&#19979;&#25991;&#24863;&#30693;&#22411;&#31572;&#26696;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#24615;&#21644;&#26131;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CONA&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25351;&#20196;&#33539;&#24335;&#65292;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#26377;&#25928;&#36827;&#34892;&#30693;&#35782;&#20256;&#25773;&#12290;CONA&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#24182;&#32467;&#21512;DIKW&#65288;&#25968;&#25454;&#12289;&#20449;&#24687;&#12289;&#30693;&#35782;&#12289;&#26234;&#24935;&#65289;&#23618;&#32423;&#33258;&#21160;&#25351;&#23548;&#21644;&#20248;&#21270;&#28436;&#31034;&#20869;&#23481;&#65292;&#39044;&#27979;&#28508;&#22312;&#30340;&#21548;&#20247;&#38382;&#39064;&#65292;&#24182;&#36866;&#24212;&#21548;&#20247;&#32676;&#20307;&#30340;&#30693;&#35782;&#27700;&#24179;&#25552;&#20379;&#19978;&#19979;&#25991;&#24863;&#30693;&#22411;&#31572;&#26696;&#12290;CONA&#33539;&#24335;&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#20854;&#29420;&#31435;&#21672;&#35810;&#26426;&#21046;&#21644;&#26681;&#26893;&#20110;DIKW&#23618;&#32423;&#30340;&#36882;&#24402;&#21453;&#39304;&#24490;&#29615;&#30340;&#32452;&#21512;&#12290;&#36825;&#31181;&#21327;&#21516;&#20316;&#29992;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#20869;&#23481;&#65292;&#30830;&#20445;&#21548;&#20247;&#21487;&#20197;&#36731;&#26494;&#29702;&#35299;&#21644;&#33719;&#21462;&#12290;&#36825;&#31181;&#33539;&#24335;&#26159;&#25506;&#32034;LLM&#26102;&#20195;&#30340;&#30693;&#35782;&#20256;&#25773;&#21644;&#27807;&#36890;&#30340;&#26032;&#26041;&#27861;&#30340;&#26089;&#26399;&#20808;&#39537;&#65292;&#20026;&#26085;&#24120;&#30693;&#35782;&#20849;&#20139;&#22330;&#26223;&#25552;&#20379;&#26377;&#25928;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CONA, a novel context-aware instruction paradigm for effective knowledge dissemination using generative pre-trained transformer (GPT) models. CONA is a flexible framework designed to leverage the capabilities of Large Language Models (LLMs) and incorporate DIKW (Data, Information, Knowledge, Wisdom) hierarchy to automatically instruct and optimise presentation content, anticipate potential audience inquiries, and provide context-aware answers that adaptive to the knowledge level of the audience group. The unique aspect of the CONA paradigm lies in its combination of an independent advisory mechanism and a recursive feedback loop rooted on the DIKW hierarchy. This synergy significantly enhances context-aware contents, ensuring they are accessible and easily comprehended by the audience. This paradigm is an early pioneer to explore new methods for knowledge dissemination and communication in the LLM era, offering effective support for everyday knowledge sharing scenarios. We
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20284;&#28982;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#12289;&#32553;&#25918;&#23450;&#24459;&#21644;&#22686;&#21152;&#35745;&#31639;&#65292;&#25104;&#21151;&#26500;&#24314;&#21644;&#21457;&#24067;&#20102;&#19968;&#20010;&#36229;&#36807;&#23567;&#20294;&#24191;&#20026;&#20154;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20248;&#20110;GPT-2 124M&#12290;</title><link>http://arxiv.org/abs/2305.18619</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Likelihood-Based Diffusion Language Models. (arXiv:2305.18619v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20284;&#28982;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#12289;&#32553;&#25918;&#23450;&#24459;&#21644;&#22686;&#21152;&#35745;&#31639;&#65292;&#25104;&#21151;&#26500;&#24314;&#21644;&#21457;&#24067;&#20102;&#19968;&#20010;&#36229;&#36807;&#23567;&#20294;&#24191;&#20026;&#20154;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20248;&#20110;GPT-2 124M&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#20204;&#23545;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#20294;&#29616;&#26377;&#30340;&#24037;&#20316;&#23578;&#26410;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#26631;&#20934;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#33719;&#24471;&#38750;&#24494;&#19981;&#36275;&#36947;&#30340;&#20284;&#28982;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#21462;&#20102;&#25514;&#26045;&#26469;&#32553;&#23567;&#33258;&#22238;&#24402;&#21644;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20284;&#28982;&#24046;&#24322;&#65292;&#30446;&#26631;&#26159;&#26500;&#24314;&#21644;&#21457;&#24067;&#19968;&#20010;&#36229;&#36807;&#23567;&#20294;&#24191;&#20026;&#20154;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#12289;&#32553;&#25918;&#23450;&#24459;&#21644;&#22686;&#21152;&#35745;&#31639;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;&#22312;&#31639;&#27861;&#21069;&#27839;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#31181;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#35770;&#25913;&#36827;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#25193;&#25955;&#27169;&#22411;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#21457;&#29616;&#35745;&#31639;&#20248;&#21270;&#30340;&#35757;&#32451;&#26041;&#26696;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#24046;&#21035;&#24456;&#22823;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#32553;&#25918;&#20998;&#26512;&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;Plaid 1B&#65292;&#19968;&#20010;&#22823;&#22411;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#20284;&#28982;&#24230;&#21644;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#19978;&#20248;&#20110;GPT-2 124M&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms GPT-2 124M in likelihood on benchmark datasets and gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#19977;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;(ChatGPT-3.5&#12289;ChatGPT-4&#21644;Google Bard)&#22312;&#35299;&#20915;&#25968;&#23398;&#21644;&#36923;&#36753;&#38382;&#39064;&#19978;&#30340;&#27491;&#30830;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#20294;&#22312;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#20013;&#38656;&#35201;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.18618</link><description>&lt;p&gt;
&#25968;&#23398;&#19982;&#36923;&#36753;&#38382;&#39064;&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;: ChatGPT-3.5&#12289;ChatGPT-4&#21644;Google Bard&#30340;&#21021;&#27493;&#27604;&#36739;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard. (arXiv:2305.18618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#19977;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;(ChatGPT-3.5&#12289;ChatGPT-4&#21644;Google Bard)&#22312;&#35299;&#20915;&#25968;&#23398;&#21644;&#36923;&#36753;&#38382;&#39064;&#19978;&#30340;&#27491;&#30830;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#20294;&#22312;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#20013;&#38656;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19977;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;(ChatGPT-3.5&#12289;ChatGPT-4&#21644;Google Bard)&#22312;&#35299;&#20915;&#25968;&#23398;&#21644;&#36923;&#36753;&#38382;&#39064;&#26102;&#30340;&#27491;&#30830;&#24615;&#23545;&#27604;&#12290;&#25105;&#20204;&#20351;&#29992;30&#20010;&#28165;&#26224;&#12289;&#26080;&#20108;&#20041;&#24615;&#12289;&#20165;&#20351;&#29992;&#32431;&#25991;&#26412;&#19988;&#20855;&#26377;&#29420;&#29305;&#23450;&#20041;&#30340;&#27491;&#30830;&#31572;&#26696;&#30340;&#38382;&#39064;&#65292;&#20998;&#20026;&#20004;&#32452;&#24182;&#20998;&#21035;&#21521;&#27599;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20986;&#19977;&#36941;&#12290;&#36890;&#36807;&#35760;&#24405;&#24182;&#35752;&#35770;&#38382;&#39064;&#30340;&#31572;&#26696;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#31616;&#21333;&#30340;&#31639;&#26415;&#21644;&#20195;&#25968;&#34920;&#36798;&#24335;...
&lt;/p&gt;
&lt;p&gt;
A comparison between three chatbots which are based on large language models, namely ChatGPT-3.5, ChatGPT-4 and Google Bard is presented, focusing on their ability to give correct answers to mathematics and logic problems. In particular, we check their ability to Understand the problem at hand; Apply appropriate algorithms or methods for its solution; and Generate a coherent response and a correct answer. We use 30 questions that are clear, without any ambiguities, fully described with plain text only, and have a unique, well defined correct answer. The questions are divided into two sets of 15 each. The questions of Set A are 15 "Original" problems that cannot be found online, while Set B contains 15 "Published" problems that one can find online, usually with their solution. Each question is posed three times to each chatbot. The answers are recorded and discussed, highlighting their strengths and weaknesses. It has been found that for straightforward arithmetic, algebraic expressions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;XSLR-53&#31070;&#32463;&#34920;&#31034;&#26469;&#20272;&#35745;&#38899;&#39057;&#25991;&#20214;&#20043;&#38388;&#30340;&#25509;&#36817;&#31243;&#24230;&#65292;&#26368;&#32456;&#26088;&#22312;&#25552;&#21462;&#30456;&#20851;&#30340;&#35821;&#35328;&#23646;&#24615;&#12290;&#22312;11&#31181;&#26041;&#35328;&#30340;&#25968;&#25454;&#20013;&#65292;&#21487;&#20026;&#19968;&#31181;&#24456;&#23569;&#36164;&#28304;&#25110;&#23436;&#20840;&#26410;&#30693;&#30340;&#35821;&#35328;&#33719;&#24471;&#24635;&#20307;&#35821;&#38899;/&#35821;&#38899;&#23398;&#25509;&#36817;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.18602</link><description>&lt;p&gt;
&#20174;&#8220;&#38899;&#39057;&#29255;&#27573;&#26041;&#35328;&#8221;&#21040;&#8220;&#35821;&#26009;&#24211;&#21644;&#26041;&#35328;&#8221;: &#21033;&#29992;&#35821;&#38899;&#30340;&#31070;&#32463;&#34920;&#31034;&#22312;&#35821;&#35328;&#32972;&#26223;&#20013;&#23450;&#20301;&#38899;&#39057;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
From `Snippet-lects' to Doculects and Dialects: Leveraging Neural Representations of Speech for Placing Audio Signals in a Language Landscape. (arXiv:2305.18602v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;XSLR-53&#31070;&#32463;&#34920;&#31034;&#26469;&#20272;&#35745;&#38899;&#39057;&#25991;&#20214;&#20043;&#38388;&#30340;&#25509;&#36817;&#31243;&#24230;&#65292;&#26368;&#32456;&#26088;&#22312;&#25552;&#21462;&#30456;&#20851;&#30340;&#35821;&#35328;&#23646;&#24615;&#12290;&#22312;11&#31181;&#26041;&#35328;&#30340;&#25968;&#25454;&#20013;&#65292;&#21487;&#20026;&#19968;&#31181;&#24456;&#23569;&#36164;&#28304;&#25110;&#23436;&#20840;&#26410;&#30693;&#30340;&#35821;&#35328;&#33719;&#24471;&#24635;&#20307;&#35821;&#38899;/&#35821;&#38899;&#23398;&#25509;&#36817;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
XSLR-53&#26159;&#19968;&#31181;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#38899;&#39057;&#20013;&#24314;&#31435;&#19968;&#20010;&#21521;&#37327;&#34920;&#31034;&#65292;&#20801;&#35768;&#36827;&#34892;&#21508;&#31181;&#35745;&#31639;&#22788;&#29702;&#12290;&#26412;&#25991;&#20351;&#29992;&#36825;&#31181;&#31070;&#32463;&#34920;&#31034;&#26469;&#20272;&#35745;&#38899;&#39057;&#25991;&#20214;&#20043;&#38388;&#30340;&#25509;&#36817;&#31243;&#24230;&#65292;&#26368;&#32456;&#26088;&#22312;&#25552;&#21462;&#30456;&#20851;&#30340;&#35821;&#35328;&#23646;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#27719;&#38598;&#27861;&#20174;&#8220;&#38899;&#39057;&#29255;&#27573;&#26041;&#35328;&#8221;(&#19968;&#20010;5&#31186;&#38899;&#39057;&#29255;&#27573;&#20013;&#30340;&#35821;&#38899;)&#32858;&#21512;&#31070;&#32463;&#34920;&#31034;&#21040;&#8220;&#35821;&#26009;&#24211;&#8221;(&#32473;&#23450;&#36164;&#28304;&#20013;&#30340;&#35821;&#38899;)&#65292;&#28982;&#21518;&#21040;&#26041;&#35328;&#21644;&#35821;&#35328;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;5&#31181;&#36739;&#23569;&#30740;&#31350;&#30340;&#35821;&#35328;&#20013;11&#31181;&#26041;&#35328;&#30340;&#35821;&#26009;&#24211;&#25968;&#25454;&#12290;&#23545;&#36825;11&#20010;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#27979;&#37327;&#25581;&#31034;&#20102;&#24050;&#30693;&#26159;&#21516;&#19968;&#35821;&#35328;&#26041;&#35328;&#20043;&#38388;&#26368;&#22823;&#30340;&#25509;&#36817;&#24230;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;: (i) &#26041;&#35328;/&#35821;&#35328;&#21487;&#20197;&#22312;&#34920;&#24449;&#38899;&#39057;&#25991;&#20214;&#30340;&#21508;&#20010;&#21442;&#25968;&#20043;&#38388;&#20986;&#29616;&#65292; (ii) &#21487;&#20197;&#20026;&#19968;&#31181;&#24456;&#23569;&#36164;&#28304;&#25110;&#23436;&#20840;&#26410;&#30693;&#30340;&#35821;&#35328;&#33719;&#24471;&#24635;&#20307;&#35821;&#38899;/&#35821;&#38899;&#23398;&#25509;&#36817;&#24230;&#20272;&#35745;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#21033;&#29992;&#35821;&#38899;&#30340;&#31070;&#32463;&#34920;&#31034;&#22312;&#35821;&#35328;&#32972;&#26223;&#20013;&#23450;&#20301;&#38899;&#39057;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
XLSR-53 a multilingual model of speech, builds a vector representation from audio, which allows for a range of computational treatments. The experiments reported here use this neural representation to estimate the degree of closeness between audio files, ultimately aiming to extract relevant linguistic properties. We use max-pooling to aggregate the neural representations from a "snippet-lect" (the speech in a 5-second audio snippet) to a "doculect" (the speech in a given resource), then to dialects and languages. We use data from corpora of 11 dialects belonging to 5 less-studied languages. Similarity measurements between the 11 corpora bring out greatest closeness between those that are known to be dialects of the same language. The findings suggest that (i) dialect/language can emerge among the various parameters characterizing audio files and (ii) estimates of overall phonetic/phonological closeness can be obtained for a little-resourced or fully unknown language. The findings help
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#27169;&#22411;&#65292;&#37319;&#29992;&#21644;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;Transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.18599</link><description>&lt;p&gt;
&#25552;&#39640;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization for Multimodal Fake News Detection. (arXiv:2305.18599v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#27169;&#22411;&#65292;&#37319;&#29992;&#21644;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;Transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#19981;&#26029;&#20256;&#25773;&#21450;&#20854;&#24778;&#20154;&#30340;&#24433;&#21709;&#24050;&#32463;&#20419;&#20351;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#24320;&#21457;&#20986;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#35268;&#27169;&#36739;&#23567;&#25110;&#29305;&#23450;&#20027;&#39064;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#37319;&#29992;&#24182;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;Transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25805;&#20316;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#26088;&#22312;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36328;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#31995;&#32479;&#22312;&#21463;&#21040;&#25805;&#20316;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20250;&#20986;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#20943;&#23569;&#20559;&#24046;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#35758;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#26356;&#26377;&#24847;&#20041;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing proliferation of misinformation and its alarming impact have motivated both industry and academia to develop approaches for fake news detection. However, state-of-the-art approaches are usually trained on datasets of smaller size or with a limited set of specific topics. As a consequence, these models lack generalization capabilities and are not applicable to real-world data. In this paper, we propose three models that adopt and fine-tune state-of-the-art multimodal transformers for multimodal fake news detection. We conduct an in-depth analysis by manipulating the input data aimed to explore models performance in realistic use cases on social media. Our study across multiple models demonstrates that these systems suffer significant performance drops against manipulated data. To reduce the bias and improve model generalization, we suggest training data augmentation to conduct more meaningful experiments for fake news detection on social media. The proposed data augmentat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#35821;&#27861;&#32467;&#26500;&#20013;&#30340;&#35821;&#20041;&#34920;&#36798;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#19978;&#19979;&#25991;&#21333;&#35789;&#23884;&#20837;&#25237;&#23556;&#21040;&#19977;&#20010;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#31354;&#38388;&#33258;&#21160;&#27966;&#29983;&#20986;&#22312;&#35821;&#27861;&#32467;&#26500;&#20013;&#30340;&#35789;&#27719;&#39033;&#30446;&#30340;&#35821;&#20041;&#34920;&#36848;&#65292;&#25105;&#20204;&#21457;&#29616;&#20027;&#35821;&#20301;&#32622;&#20013;&#30340;&#21333;&#35789;&#34987;&#35299;&#37322;&#20026;&#27604;&#21516;&#26679;&#30340;&#23486;&#35821;&#20301;&#32622;&#20013;&#30340;&#21333;&#35789;&#26356;&#20855;&#20195;&#29702;&#24615;&#65292;&#24182;&#19988;AANN&#32467;&#26500;&#20013;&#30340;&#21517;&#35789;&#34987;&#35299;&#37322;&#20026;&#27604;&#20256;&#32479;&#36716;&#25442;&#20013;&#30340;&#21517;&#35789;&#26356;&#20855;&#27979;&#37327;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18598</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#31354;&#38388;&#30740;&#31350;&#35821;&#27861;&#32467;&#26500;&#20013;&#30340;&#35821;&#20041;&#34920;&#36798;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces. (arXiv:2305.18598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#35821;&#27861;&#32467;&#26500;&#20013;&#30340;&#35821;&#20041;&#34920;&#36798;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#19978;&#19979;&#25991;&#21333;&#35789;&#23884;&#20837;&#25237;&#23556;&#21040;&#19977;&#20010;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#31354;&#38388;&#33258;&#21160;&#27966;&#29983;&#20986;&#22312;&#35821;&#27861;&#32467;&#26500;&#20013;&#30340;&#35789;&#27719;&#39033;&#30446;&#30340;&#35821;&#20041;&#34920;&#36848;&#65292;&#25105;&#20204;&#21457;&#29616;&#20027;&#35821;&#20301;&#32622;&#20013;&#30340;&#21333;&#35789;&#34987;&#35299;&#37322;&#20026;&#27604;&#21516;&#26679;&#30340;&#23486;&#35821;&#20301;&#32622;&#20013;&#30340;&#21333;&#35789;&#26356;&#20855;&#20195;&#29702;&#24615;&#65292;&#24182;&#19988;AANN&#32467;&#26500;&#20013;&#30340;&#21517;&#35789;&#34987;&#35299;&#37322;&#20026;&#27604;&#20256;&#32479;&#36716;&#25442;&#20013;&#30340;&#21517;&#35789;&#26356;&#20855;&#27979;&#37327;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#35821;&#27861;&#32467;&#26500;&#20013;&#30340;&#35821;&#20041;&#34920;&#36798;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#21333;&#35789;&#23884;&#20837;&#25237;&#23556;&#21040;&#19977;&#20010;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#27599;&#20010;&#31354;&#38388;&#30001;&#19981;&#21516;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24449;&#35268;&#33539;&#38598;&#23450;&#20041;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#36825;&#20123;&#21487;&#35299;&#37322;&#30340;&#31354;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#20204;&#33258;&#21160;&#27966;&#29983;&#20986;&#20004;&#20010;&#35821;&#27861;&#32467;&#26500;&#20013;&#35789;&#27719;&#39033;&#30446;&#30340;&#35821;&#20041;&#34920;&#36848;&#65306;&#22312;&#21516;&#19968;&#21477;&#23376;&#20013;&#20316;&#20026;&#20027;&#35821;&#25110;&#23486;&#35821;&#20301;&#32622;&#30340;&#21517;&#35789;&#65292;&#20197;&#21450;AANN&#32467;&#26500;&#65288;&#20363;&#22914;&#65292;&#8220;&#32654;&#20029;&#30340;&#19977;&#22825;&#8221;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20027;&#35821;&#20301;&#32622;&#20013;&#30340;&#21333;&#35789;&#34987;&#35299;&#37322;&#20026;&#27604;&#21516;&#26679;&#30340;&#23486;&#35821;&#20301;&#32622;&#20013;&#30340;&#21333;&#35789;&#26356;&#20855;&#20195;&#29702;&#24615;&#65292;&#24182;&#19988;AANN&#32467;&#26500;&#20013;&#30340;&#21517;&#35789;&#34987;&#35299;&#37322;&#20026;&#27604;&#20256;&#32479;&#36716;&#25442;&#20013;&#30340;&#21517;&#35789;&#26356;&#20855;&#27979;&#37327;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#26495;&#32423;&#21035;&#19978;&#25506;&#27979;&#21477;&#27861;&#32467;&#26500;&#30340;&#20998;&#24067;&#24335;&#24847;&#20041;&#65292;&#25277;&#35937;&#21040;&#29305;&#23450;&#35789;&#27719;&#20043;&#22806;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study semantic construal in grammatical constructions using large language models. First, we project contextual word embeddings into three interpretable semantic spaces, each defined by a different set of psycholinguistic feature norms. We validate these interpretable spaces and then use them to automatically derive semantic characterizations of lexical items in two grammatical constructions: nouns in subject or object position within the same sentence, and the AANN construction (e.g., `a beautiful three days'). We show that a word in subject position is interpreted as more agentive than the very same word in object position, and that the nouns in the AANN construction are interpreted as more measurement-like than when in the canonical alternation. Our method can probe the distributional meaning of syntactic constructions at a templatic level, abstracted away from specific lexemes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;&#25239;&#25915;&#20987;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#30740;&#31350;&#20102;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#23545;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.18585</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#35774;&#35745;&#23545;&#25239;&#25915;&#20987;&#21644;&#35780;&#20272;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploiting Explainability to Design Adversarial Attacks and Evaluate Attack Resilience in Hate-Speech Detection Models. (arXiv:2305.18585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;&#25239;&#25915;&#20987;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#30740;&#31350;&#20102;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#23545;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#20986;&#29616;&#24102;&#26469;&#20102;&#35768;&#22810;&#20262;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;&#20167;&#24680;&#35328;&#35770;&#26159;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#23457;&#26597;&#20869;&#23481;&#24182;&#20419;&#36827;&#25991;&#26126;&#35752;&#35770;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#21487;&#33021;&#20250;&#34987;&#23545;&#25239;&#25915;&#20987;&#25152;&#35823;&#23548;&#65292;&#24341;&#36215;&#20854;&#40065;&#26834;&#24615;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#21333;&#29420;&#35752;&#35770;&#20102;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#27809;&#26377;&#20840;&#38754;&#30740;&#31350;&#20182;&#20204;&#30340;&#20132;&#38598;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#21019;&#26032;&#22312;&#20110;&#32467;&#21512;&#36825;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#35782;&#21035;&#28508;&#22312;&#30340;&#28431;&#27934;&#65292;&#20174;&#32780;&#35774;&#35745;&#26377;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#21644;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of social media has given rise to numerous ethical challenges, with hate speech among the most significant concerns. Researchers are attempting to tackle this problem by leveraging hate-speech detection and employing language models to automatically moderate content and promote civil discourse. Unfortunately, recent studies have revealed that hate-speech detection systems can be misled by adversarial attacks, raising concerns about their resilience. While previous research has separately addressed the robustness of these models under adversarial attacks and their interpretability, there has been no comprehensive study exploring their intersection. The novelty of our work lies in combining these two critical aspects, leveraging interpretability to identify potential vulnerabilities and enabling the design of targeted adversarial attacks. We present a comprehensive and comparative analysis of adversarial robustness exhibited by various hate-speech detection models. Our study e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#26469;&#24110;&#21161;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#20449;&#24687;&#26356;&#26032;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.18582</link><description>&lt;p&gt;
&#36890;&#36807;&#20943;&#23569;&#26292;&#38706;&#20559;&#24046;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#20449;&#24687;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Self Information Update for Large Language Models through Mitigating Exposure Bias. (arXiv:2305.18582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#26469;&#24110;&#21161;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#20449;&#24687;&#26356;&#26032;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20449;&#24687;&#35831;&#27714;&#26041;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#21463;&#20854;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26368;&#26032;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#20351;&#20854;&#26080;&#27861;&#25552;&#20379;&#26368;&#26032;&#30340;&#20449;&#24687;&#12290;&#20174;&#22836;&#24320;&#22987;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#20195;&#20215;&#36739;&#39640;&#65292;&#24182;&#19988;&#23545;&#26032;&#35821;&#26009;&#24211;&#36827;&#34892;&#36830;&#32493;&#24494;&#35843;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#26816;&#26597;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#30340;&#26356;&#26032;&#31243;&#24207;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#25237;&#20837;&#65292;&#23558;&#20449;&#24687;&#20934;&#22791;&#25104;&#26356;&#32467;&#26500;&#21270;&#30340;&#24418;&#24335;&#65292;&#22914;&#30693;&#35782;&#19977;&#20803;&#32452;&#12289;&#23545;&#35805;&#25968;&#25454;&#25110;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#21709;&#24212;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#33258;&#25105;&#20449;&#24687;&#26356;&#26032;&#20219;&#21153;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#36825;&#21482;&#38656;&#35201;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#26368;&#26032;&#30340;&#26032;&#38395;&#25991;&#31456;&#26469;&#26356;&#26032;LLM&#30340;&#29616;&#26377;&#30693;&#35782;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#33258;&#25105;&#20449;&#24687;&#26356;&#26032;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#20102;&#36830;&#32493;&#24494;&#35843;&#26041;&#27861;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current LLMs have demonstrated remarkable capabilities in addressing users' requests for various types of information. However, these models are limited by the most recent data available in their pretraining corpora, rendering them incapable of providing up-to-date information. Retraining LLMs from scratch is cost-prohibitive, and the effectiveness of continual fine-tuning on new corpora has not been thoroughly examined. Additionally, current update procedures typically demand significant human input to prepare the information into more structured format, such as knowledge triples, conversational data or responses with human feedback. In this study, we conduct a comprehensive examination of a novel self information update task in LLMs, which only requires the provision of informative text corpora. For instance, we can use the latest news articles to update the LLMs' existing knowledge. We define the self information update task and assess the continual fine-tuning approach for this pur
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;TreeMAN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26641;&#30340;&#29305;&#24449;&#22686;&#24378;&#25991;&#26412;&#34920;&#31034;&#65292;&#23558;&#34920;&#26684;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#34701;&#21512;&#20026;&#22810;&#27169;&#24577;&#34920;&#31034;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;ICD&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2305.18576</link><description>&lt;p&gt;
TreeMAN&#65306;&#22522;&#20110;&#26641;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;ICD&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
TreeMAN: Tree-enhanced Multimodal Attention Network for ICD Coding. (arXiv:2305.18576v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18576
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;TreeMAN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26641;&#30340;&#29305;&#24449;&#22686;&#24378;&#25991;&#26412;&#34920;&#31034;&#65292;&#23558;&#34920;&#26684;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#34701;&#21512;&#20026;&#22810;&#27169;&#24577;&#34920;&#31034;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;ICD&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ICD&#32534;&#30721;&#26088;&#22312;&#20026;&#20986;&#38498;&#21518;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20998;&#37197;&#30142;&#30149;&#20195;&#30721;&#65292;&#36825;&#23545;&#36134;&#21333;&#21644;&#20020;&#24202;&#32479;&#35745;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#25552;&#39640;&#25163;&#21160;&#32534;&#30721;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#35768;&#22810;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#33258;&#21160;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#39044;&#27979;ICD&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#24573;&#30053;&#20102;&#21253;&#21547;&#22312;EHR&#20013;&#30340;&#32467;&#26500;&#21270;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#24456;&#38590;&#20174;&#22024;&#26434;&#30340;&#20020;&#24202;&#35760;&#24405;&#20013;&#25429;&#33719;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Tree-enhanced Multimodal Attention Network(TreeMAN)&#65292;&#36890;&#36807;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23558;&#34920;&#26684;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#34701;&#21512;&#20026;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#24182;&#20511;&#21161;&#22522;&#20110;&#26641;&#30340;&#29305;&#24449;&#22686;&#24378;&#25991;&#26412;&#34920;&#31034;&#12290;&#22522;&#20110;&#26641;&#30340;&#29305;&#24449;&#26159;&#26681;&#25454;&#20174;&#32467;&#26500;&#21270;&#22810;&#27169;&#24577;&#21307;&#30103;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#20915;&#31574;&#26641;&#26500;&#24314;&#30340;&#65292;&#23427;&#20204;&#25429;&#25417;&#20851;&#20110;ICD&#32534;&#30721;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#25105;&#20204;&#21487;&#20197;&#23558;&#20808;&#21069;&#25991;&#26412;&#27169;&#22411;&#20013;&#30340;&#30456;&#21516;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
ICD coding is designed to assign the disease codes to electronic health records (EHRs) upon discharge, which is crucial for billing and clinical statistics. In an attempt to improve the effectiveness and efficiency of manual coding, many methods have been proposed to automatically predict ICD codes from clinical notes. However, most previous works ignore the decisive information contained in structured medical data in EHRs, which is hard to be captured from the noisy clinical notes. In this paper, we propose a Tree-enhanced Multimodal Attention Network (TreeMAN) to fuse tabular features and textual features into multimodal representations by enhancing the text representations with tree-based features via the attention mechanism. Tree-based features are constructed according to decision trees learned from structured multimodal medical data, which capture the decisive information about ICD coding. We can apply the same multi-label classifier from previous text models to the multimodal re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#20197;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#24182;&#20026;&#20559;&#35265;&#32531;&#35299;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.18569</link><description>&lt;p&gt;
ChatGPT&#30340;&#20844;&#24179;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fairness of ChatGPT. (arXiv:2305.18569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#20197;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#24182;&#20026;&#20559;&#35265;&#32531;&#35299;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#35299;&#20915;LLM&#20013;&#19981;&#20844;&#24179;&#30340;&#38382;&#39064;&#23545;&#20110;&#36127;&#36131;&#20219;&#30340;AI&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;LLM&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#39046;&#22495;&#26102;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#20844;&#24179;&#35780;&#20272;&#26041;&#38754;&#65292;&#25968;&#37327;&#20998;&#26512;&#21644;&#28145;&#20837;&#30740;&#31350;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35780;&#20272;ChatGPT&#22312;&#21253;&#25324;&#25945;&#32946;&#12289;&#29359;&#32618;&#23398;&#12289;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#36827;&#34892;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#21644;&#20010;&#20154;&#20844;&#24179;&#24615;&#65292;&#24182;&#35266;&#23519;&#20102;&#22312;&#19968;&#31995;&#21015;&#26377;&#20559;&#25110;&#26080;&#20559;&#25552;&#31034;&#19979;ChatGPT&#36755;&#20986;&#30340;&#24046;&#24322;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#20415;&#20110;&#20559;&#35265;&#32531;&#35299;&#65292;&#20419;&#36827;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20855;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare. To make thorough evaluation, we consider both group fairness and individual fairness and we also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;PaLI-X&#65292;&#36890;&#36807;&#25193;&#23637;&#27169;&#22411;&#32452;&#20214;&#21644;&#35757;&#32451;&#20219;&#21153;&#33539;&#22260;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#26032;&#24615;&#33021;&#27700;&#24179;&#65292;&#21253;&#25324;&#22270;&#20687;&#23383;&#24149;&#38382;&#31572;&#12289;&#23545;&#35937;&#26816;&#27979;&#12289;&#35270;&#39057;&#38382;&#31572;&#21644;&#35270;&#39057;&#23383;&#24149;&#31561;&#65292;&#21516;&#26102;&#22312;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.18565</link><description>&lt;p&gt;
PaLI-X&#65306;&#20851;&#20110;&#25193;&#23637;&#19968;&#31181;&#22810;&#35821;&#35328;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PaLI-X: On Scaling up a Multilingual Vision and Language Model. (arXiv:2305.18565v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18565
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;PaLI-X&#65292;&#36890;&#36807;&#25193;&#23637;&#27169;&#22411;&#32452;&#20214;&#21644;&#35757;&#32451;&#20219;&#21153;&#33539;&#22260;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#26032;&#24615;&#33021;&#27700;&#24179;&#65292;&#21253;&#25324;&#22270;&#20687;&#23383;&#24149;&#38382;&#31572;&#12289;&#23545;&#35937;&#26816;&#27979;&#12289;&#35270;&#39057;&#38382;&#31572;&#21644;&#35270;&#39057;&#23383;&#24149;&#31561;&#65292;&#21516;&#26102;&#22312;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25193;&#23637;PaLI-X&#65292;&#19968;&#31181;&#22810;&#35821;&#35328;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#37197;&#26041;&#21644;&#32467;&#26524;&#65292;&#21253;&#25324;&#32452;&#20214;&#30340;&#22823;&#23567;&#21644;&#35757;&#32451;&#20219;&#21153;&#33539;&#22260;&#30340;&#24191;&#24230;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20110;&#22270;&#20687;&#30340;&#23383;&#24149;&#21644;&#38382;&#31572;&#20219;&#21153;&#12289;&#22522;&#20110;&#22270;&#20687;&#30340;&#25991;&#26723;&#29702;&#35299;&#21644;&#23569;&#37327;&#65288;&#19978;&#19979;&#25991;&#20869;&#65289;&#23398;&#20064;&#20197;&#21450;&#23545;&#35937;&#26816;&#27979;&#12289;&#35270;&#39057;&#38382;&#31572;&#21644;&#35270;&#39057;&#23383;&#24149;&#31561;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;PaLI-X &#22312;&#22823;&#22810;&#25968;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#65288;&#32771;&#34385;&#20102;25&#20010;&#20197;&#19978;&#30340;&#27979;&#35797;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#20123;&#26032;&#20852;&#30340;&#33021;&#21147;&#65292;&#22914;&#22797;&#26434;&#35745;&#25968;&#21644;&#22810;&#35821;&#35328;&#23545;&#35937;&#26816;&#27979;&#65292;&#36825;&#20123;&#20219;&#21153;&#24182;&#27809;&#26377;&#26126;&#30830;&#21015;&#22312;&#35757;&#32451;&#33539;&#22260;&#20043;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the training recipe and results of scaling up PaLI-X, a multilingual vision and language model, both in terms of size of the components and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based document understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#21644;&#23454;&#35777;&#22320;&#32771;&#23519;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#20869;&#30340;&#24341;&#25991;&#27169;&#24335;&#65292;&#35777;&#26126;&#20102;&#22823;&#32422;62%&#30340;&#24341;&#29992;&#35770;&#25991;&#23646;&#20110;&#20986;&#29256;&#21069;&#20116;&#24180;&#65292;&#21482;&#26377;&#32422;17%&#30340;&#35770;&#25991;&#36229;&#36807;&#21313;&#24180;&#12290;&#30446;&#21069;&#65292;NLP&#35770;&#25991;&#30340;&#24341;&#29992;&#24180;&#40836;&#36235;&#20110;&#21382;&#21490;&#26368;&#20302;&#27700;&#24179;&#65292;&#36825;&#20010;&#36235;&#21183;&#19982;&#27492;&#21069;&#30456;&#21453;&#12290;</title><link>http://arxiv.org/abs/2305.18554</link><description>&lt;p&gt;
&#36951;&#24536;&#30340;&#30693;&#35782;&#65306;&#23457;&#35270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24341;&#25991;&#20581;&#24536;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Forgotten Knowledge: Examining the Citational Amnesia in NLP. (arXiv:2305.18554v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#21644;&#23454;&#35777;&#22320;&#32771;&#23519;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#20869;&#30340;&#24341;&#25991;&#27169;&#24335;&#65292;&#35777;&#26126;&#20102;&#22823;&#32422;62%&#30340;&#24341;&#29992;&#35770;&#25991;&#23646;&#20110;&#20986;&#29256;&#21069;&#20116;&#24180;&#65292;&#21482;&#26377;&#32422;17%&#30340;&#35770;&#25991;&#36229;&#36807;&#21313;&#24180;&#12290;&#30446;&#21069;&#65292;NLP&#35770;&#25991;&#30340;&#24341;&#29992;&#24180;&#40836;&#36235;&#20110;&#21382;&#21490;&#26368;&#20302;&#27700;&#24179;&#65292;&#36825;&#20010;&#36235;&#21183;&#19982;&#27492;&#21069;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#29992;&#35770;&#25991;&#26159;&#29616;&#20195;&#31185;&#23398;&#20889;&#20316;&#35752;&#35770;&#21644;&#24314;&#31435;&#20808;&#21069;&#24037;&#20316;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#20840;&#38754;&#22320;&#24341;&#29992;&#19968;&#32452;&#22810;&#26679;&#21270;(&#26102;&#38388;&#21644;&#30740;&#31350;&#39046;&#22495;)&#30340;&#35770;&#25991;&#26159;&#34913;&#37327;&#31038;&#21306;&#38405;&#35835;&#24191;&#27867;&#31243;&#24230;&#30340;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#40092;&#26377;&#30740;&#31350;&#25506;&#35752;&#24341;&#25991;&#30340;&#24191;&#27867;&#26102;&#38388;&#27169;&#24335;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#21644;&#23454;&#35777;&#22320;&#32771;&#23519;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20869;&#30340;&#24341;&#25991;&#37325;&#35201;&#24615;&#65292;&#25506;&#35752;&#20102;&#25105;&#20204;&#24341;&#29992;&#35770;&#25991;&#26102;&#24448;&#22238;&#36861;&#28335;&#22810;&#23569;&#24180;&#30340;&#38382;&#39064;&#65292;&#24341;&#29992;&#26102;&#38388;&#30340;&#21464;&#21270;&#36235;&#21183;&#65292;&#20197;&#21450;&#26377;&#21738;&#20123;&#22240;&#32032;&#19982;&#36825;&#31181;&#24341;&#25991;&#27880;&#24847;&#21147;/&#20581;&#24536;&#29366;&#24577;&#30456;&#20851;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#32422;71.5K&#31687;&#35770;&#25991;&#65292;&#36873;&#25321;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20316;&#20026;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#39046;&#22495;&#65292;&#24182;&#35777;&#26126;&#24182;&#37327;&#21270;&#20102;&#24341;&#29992;&#20013;&#30340;&#20960;&#20010;&#20851;&#38190;&#36235;&#21183;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#32422;62%&#30340;&#24341;&#29992;&#35770;&#25991;&#23646;&#20110;&#22312;&#20986;&#29256;&#21069;&#20116;&#24180;&#30340;&#35770;&#25991;&#65292;&#32780;&#21482;&#26377;&#32422;17%&#30340;&#35770;&#25991;&#36229;&#36807;&#21313;&#24180;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#29992;&#35770;&#25991;&#30340;&#24180;&#40836;&#20013;&#20301;&#25968;&#21644;&#24180;&#40836;&#22810;&#26679;&#24615;&#20174;1990&#24180;&#21040;2014&#24180;&#25345;&#32493;&#22686;&#21152;&#65292;&#20294;&#33258;&#37027;&#26102;&#36215;&#65292;&#36825;&#20010;&#36235;&#21183;&#24050;&#32463;&#36870;&#36716;&#65292;&#30446;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35770;&#25991;&#30340;&#24341;&#25991;&#24180;&#40836;&#36824;&#21019;&#21382;&#21490;&#26368;&#20302;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Citing papers is the primary method through which modern scientific writing discusses and builds on past work. Collectively, citing a diverse set of papers (in time and area of study) is an indicator of how widely the community is reading. Yet, there is little work looking at broad temporal patterns of citation. This work systematically and empirically examines: How far back in time do we tend to go to cite papers? How has that changed over time, and what factors correlate with this citational attention/amnesia? We chose NLP as our domain of interest and analyzed approximately 71.5K papers to show and quantify several key trends in citation. Notably, around 62% of cited papers are from the immediate five years prior to publication, whereas only about 17% are more than ten years old. Furthermore, we show that the median age and age diversity of cited papers were steadily increasing from 1990 to 2014, but since then, the trend has reversed, and current NLP papers have an all-time low tem
&lt;/p&gt;</description></item><item><title>SlimFit&#26159;&#19968;&#20010;&#26032;&#24037;&#20855;&#65292;&#36890;&#36807;&#21160;&#24577;&#20998;&#26512;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#20923;&#32467;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#23618;&#65292;&#23558;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#20102;5&#20493;&#65292;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18513</link><description>&lt;p&gt;
SlimFit&#65306;&#20351;&#29992;&#35757;&#32451;&#21160;&#24577;&#23454;&#29616;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#20869;&#23384;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
SlimFit: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics. (arXiv:2305.18513v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18513
&lt;/p&gt;
&lt;p&gt;
SlimFit&#26159;&#19968;&#20010;&#26032;&#24037;&#20855;&#65292;&#36890;&#36807;&#21160;&#24577;&#20998;&#26512;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#20923;&#32467;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#23618;&#65292;&#23558;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#20102;5&#20493;&#65292;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;&#22914;BERT&#21644;ViT&#65289;&#22312;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#26497;&#39640;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#22312;&#20869;&#23384;&#36164;&#28304;&#21463;&#38480;&#30340;GPU&#19978;&#37096;&#32626;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;SlimFit&#30340;&#26032;&#24037;&#20855;&#65292;&#36890;&#36807;&#21160;&#24577;&#20998;&#26512;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#20923;&#32467;&#36129;&#29486;&#36739;&#23569;&#30340;&#23618;&#26469;&#20943;&#23569;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#23384;&#35201;&#27714;&#12290;&#36816;&#34892;&#26102;&#23618;&#38388;&#35843;&#24230;&#31639;&#27861;&#36873;&#25321;&#35201;&#20923;&#32467;&#30340;&#23618;&#12290;SlimFit&#23545;&#29305;&#23450;&#23618;&#37319;&#29992;&#37327;&#21270;&#21644;&#21098;&#26525;&#65292;&#20197;&#24179;&#34913;&#21160;&#24577;&#28608;&#27963;&#30340;&#36127;&#36733;&#65292;&#24182;&#26368;&#23567;&#21270;&#19981;&#33021;&#20002;&#24323;&#30340;&#38745;&#24577;&#28608;&#27963;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#20854;&#20013;&#38745;&#24577;&#28608;&#27963;&#25351;&#26080;&#35770;&#20923;&#32467;&#19982;&#21542;&#37117;&#19981;&#33021;&#20002;&#24323;&#30340;&#28608;&#27963;&#12290;&#36825;&#20351;&#24471;SlimFit&#21487;&#20197;&#20923;&#32467;&#22810;&#36798;95&#65285;&#30340;&#23618;&#65292;&#24182;&#23558;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#25972;&#20307;&#35774;&#22791;GPU&#20869;&#23384;&#20351;&#29992;&#29575;&#38477;&#20302;&#39640;&#36798;5&#20493;&#65292;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models, such as BERT and ViT, have achieved state-of-the-art results across different natural language processing (NLP) and computer vision (CV) tasks. However, these models are extremely memory intensive during their fine-tuning process, making them difficult to deploy on GPUs with limited memory resources. To address this issue, we introduce a new tool called SlimFit that reduces the memory requirements of these models by dynamically analyzing their training dynamics and freezing less-contributory layers during fine-tuning. The layers to freeze are chosen using a runtime inter-layer scheduling algorithm. SlimFit adopts quantization and pruning for particular layers to balance the load of dynamic activations and to minimize the memory footprint of static activations, where static activations refer to those that cannot be discarded regardless of freezing. This allows SlimFit to freeze up to 95% of layers and reduce the overall on-device GPU memory usage of transformer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#25552;&#31034;&#26041;&#27861;&#8212;&#8212;&#20195;&#30721;&#25552;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35302;&#21457;&#20195;&#30721;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#27604;&#65292;&#20195;&#30721;&#25552;&#31034;&#26377;&#30528;&#20960;&#20010;&#29420;&#29305;&#20248;&#21183;&#65292;&#33021;&#22815;&#25552;&#39640;&#31526;&#21495;&#25512;&#29702;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#24120;&#20248;&#20110;&#24605;&#36335;&#38142;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.18507</link><description>&lt;p&gt;
&#20195;&#30721;&#25552;&#31034;&#65306;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25512;&#29702;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models. (arXiv:2305.18507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#25552;&#31034;&#26041;&#27861;&#8212;&#8212;&#20195;&#30721;&#25552;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35302;&#21457;&#20195;&#30721;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#27604;&#65292;&#20195;&#30721;&#25552;&#31034;&#26377;&#30528;&#20960;&#20010;&#29420;&#29305;&#20248;&#21183;&#65292;&#33021;&#22815;&#25552;&#39640;&#31526;&#21495;&#25512;&#29702;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#24120;&#20248;&#20110;&#24605;&#36335;&#38142;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#36890;&#36807;&#21508;&#31181;&#25552;&#31034;&#26041;&#27861;&#25193;&#22823;&#20102;&#35268;&#27169;&#65292;&#20197;&#35299;&#38145;&#24191;&#27867;&#30340;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25552;&#31034;&#26041;&#27861;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#20013;&#38388;&#27493;&#39588;&#20197;&#24110;&#21161;&#25512;&#29702;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#23436;&#21892;&#30340;&#20219;&#21153;&#32553;&#20943;&#21644;&#28151;&#28102;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#26679;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20195;&#30721;&#25552;&#31034;&#65292;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#25552;&#31034;&#26041;&#27861;&#65292;&#20855;&#26377;&#38646;-shot&#21644;&#23569;-shot&#29256;&#26412;&#65292;&#21487;&#20197;&#35302;&#21457;&#20195;&#30721;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#31526;&#21495;&#25512;&#29702;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;7&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#20195;&#30721;&#25552;&#31034;&#36890;&#24120;&#20248;&#20110;&#24605;&#36335;&#38142;&#25552;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20102;&#35299;&#20195;&#30721;&#25552;&#31034;&#30340;&#24615;&#33021;&#21644;&#38480;&#21046;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#21644;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20102;&#20351;&#29992;&#31526;&#21495;&#25552;&#31034;&#30456;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20960;&#20010;&#29420;&#29305;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#20195;&#30721;&#25552;&#31034;&#21644;&#24605;&#36335;&#38142;&#25552;&#31034;&#30340;&#38598;&#21512;&#65292;&#20197;&#32467;&#21512;&#20004;&#32773;&#30340;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have scaled up to unlock a wide range of complex reasoning tasks with the aid of various prompting methods. However, current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion. To mitigate such limitations, we explore code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps. We conduct experiments on 7 widely-used benchmarks involving symbolic reasoning and arithmetic reasoning. Code prompting generally outperforms chain-of-thought (CoT) prompting. To further understand the performance and limitations of code prompting, we perform extensive ablation studies and error analyses, and identify several exclusive advantages of using symbolic promptings compared to natural language. We also consider the ensemble of code prompting and CoT prompting to combine the strengths of both. Finally, we show throu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#21160;&#40065;&#26834;&#24615;&#35780;&#20272;&#26694;&#26550;&#65292;&#21521;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#36716;&#22411;&#65292;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#30340;&#20248;&#21183;&#12290;&#30740;&#31350;&#32773;&#20204;&#26681;&#25454;&#27169;&#22411;&#33021;&#21147;&#30830;&#23450;&#40065;&#26834;&#24615;&#35780;&#20272;&#32500;&#24230;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#32500;&#24230;&#25351;&#23450;&#21512;&#29702;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.18503</link><description>&lt;p&gt;
&#20174;&#23545;&#25239;&#24615;&#31454;&#20105;&#21040;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#35780;&#20215;&#65306;&#25512;&#21160;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#21160;&#40065;&#26834;&#24615;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From Adversarial Arms Race to Model-centric Evaluation: Motivating a Unified Automatic Robustness Evaluation Framework. (arXiv:2305.18503v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#21160;&#40065;&#26834;&#24615;&#35780;&#20272;&#26694;&#26550;&#65292;&#21521;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#36716;&#22411;&#65292;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#30340;&#20248;&#21183;&#12290;&#30740;&#31350;&#32773;&#20204;&#26681;&#25454;&#27169;&#22411;&#33021;&#21147;&#30830;&#23450;&#40065;&#26834;&#24615;&#35780;&#20272;&#32500;&#24230;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#32500;&#24230;&#25351;&#23450;&#21512;&#29702;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#21521;&#36755;&#20837;&#28155;&#21152;&#35821;&#20041;&#20445;&#30041;&#20294;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#25200;&#21160;&#26469;&#21457;&#29616;&#27169;&#22411;&#30340;&#24369;&#28857;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#38271;&#26399;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#31454;&#20105;&#26159;&#31639;&#27861;&#20013;&#24515;&#30340;&#65292;&#20026;&#33258;&#21160;&#40065;&#26834;&#24615;&#35780;&#20272;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25216;&#26415;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#23454;&#36341;&#21487;&#33021;&#23384;&#22312;&#35780;&#20272;&#19981;&#20840;&#38754;&#12289;&#35780;&#20272;&#21327;&#35758;&#19981;&#23454;&#29992;&#20197;&#21450;&#23545;&#25239;&#26679;&#26412;&#22833;&#25928;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#21160;&#40065;&#26834;&#24615;&#35780;&#20272;&#26694;&#26550;&#65292;&#21521;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#36716;&#22411;&#65292;&#36827;&#19968;&#27493;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#30340;&#20248;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#22522;&#20110;&#27169;&#22411;&#33021;&#21147;&#30830;&#23450;&#40065;&#26834;&#24615;&#35780;&#20272;&#32500;&#24230;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#32500;&#24230;&#25351;&#23450;&#21512;&#29702;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35780;&#20272;&#21327;&#35758;&#65292;&#21253;&#25324;&#35780;&#20272;&#35774;&#32622;&#21644;&#25351;&#26631;&#65292;&#20197;&#28385;&#36275;&#23454;&#38469;&#38656;&#27714;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#35813;&#26694;&#26550;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#36866;&#29992;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual adversarial attacks can discover models' weaknesses by adding semantic-preserved but misleading perturbations to the inputs. The long-lasting adversarial attack-and-defense arms race in Natural Language Processing (NLP) is algorithm-centric, providing valuable techniques for automatic robustness evaluation. However, the existing practice of robustness evaluation may exhibit issues of incomprehensive evaluation, impractical evaluation protocol, and invalid adversarial samples. In this paper, we aim to set up a unified automatic robustness evaluation framework, shifting towards model-centric evaluation to further exploit the advantages of adversarial attacks. To address the above challenges, we first determine robustness evaluation dimensions based on model capabilities and specify the reasonable algorithm to generate adversarial samples for each dimension. Then we establish the evaluation protocol, including evaluation settings and metrics, under realistic demands. Finally, we u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#21450;&#20854;&#25968;&#25454;&#38598;VAST-27M&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#36890;&#36807;&#33258;&#21160;&#38598;&#25104;&#22810;&#27169;&#24577;&#23383;&#24149;&#21644;&#36164;&#28304;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#25991;&#26412;&#20851;&#32852;&#30340;&#22810;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18500</link><description>&lt;p&gt;
VAST&#65306;&#19968;&#31181;&#35270;&#21548;&#23383;&#24149;&#25991;&#26412;&#20840;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. (arXiv:2305.18500v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#21450;&#20854;&#25968;&#25454;&#38598;VAST-27M&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#36890;&#36807;&#33258;&#21160;&#38598;&#25104;&#22810;&#27169;&#24577;&#23383;&#24149;&#21644;&#36164;&#28304;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#25991;&#26412;&#20851;&#32852;&#30340;&#22810;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#23436;&#20840;&#25506;&#32034;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#65292;&#32780;&#20854;&#20182;&#27169;&#24577;&#65292;&#22914;&#35270;&#39057;&#20013;&#30340;&#38899;&#39057;&#21644;&#23383;&#24149;&#65292;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#20840;&#27169;&#24577;&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;VAST-27M&#65292;&#24314;&#31435;&#22810;&#27169;&#24577;&#35270;&#39057;&#36712;&#36857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#65292;&#24182;&#19982;&#25991;&#26412;&#36827;&#34892;&#20851;&#32852;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;2700&#19975;&#20010;&#24320;&#25918;&#39046;&#22495;&#35270;&#39057;&#29255;&#27573;&#65292;&#24182;&#20998;&#21035;&#35757;&#32451;&#35270;&#35273;&#21644;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#22120;&#20197;&#29983;&#25104;&#35270;&#35273;&#21644;&#38899;&#39057;&#23383;&#24149;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29616;&#26377;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#29983;&#25104;&#30340;&#23383;&#24149;&#12289;&#23383;&#24149;&#21644;&#25351;&#23548;&#25552;&#31034;&#38598;&#25104;&#21040;&#20840;&#27169;&#24577;&#23383;&#24149;&#20013;&#12290;&#22522;&#20110;&#25552;&#20986;&#30340;VAST-27M&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#65292;&#23427;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#24182;&#26356;&#22909;&#22320;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision and text have been fully explored in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including Vision, Audio, and Subtitle, and Text by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vis
&lt;/p&gt;</description></item><item><title>ANPL&#26159;&#19968;&#20010;&#32534;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.18498</link><description>&lt;p&gt;
ANPL&#65306;&#20351;&#29992;&#20132;&#20114;&#24335;&#20998;&#35299;&#32534;&#35793;&#33258;&#28982;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
ANPL: Compiling Natural Programs with Interactive Decomposition. (arXiv:2305.18498v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18498
&lt;/p&gt;
&lt;p&gt;
ANPL&#26159;&#19968;&#20010;&#32534;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#22312;&#36890;&#36807;&#33258;&#28982;&#20132;&#20114;&#22686;&#24378;&#32534;&#31243;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#23558;&#24120;&#35265;&#30340;&#20351;&#29992;&#27169;&#24335;&#32534;&#35793;&#20026;&#32534;&#31243;&#35821;&#35328;&#65292;&#20363;&#22914;Python&#65292;&#20294;&#22914;&#20309;&#32534;&#36753;&#21644;&#35843;&#35797;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31243;&#24207;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ANPL&#65292;&#19968;&#31181;&#32534;&#31243;&#31995;&#32479;&#65292;&#20801;&#35768;&#29992;&#25143;&#20998;&#35299;&#29305;&#23450;&#20110;&#29992;&#25143;&#30340;&#20219;&#21153;&#12290;&#22312;ANPL&#31243;&#24207;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#35813;&#33609;&#22270;&#25351;&#23450;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#25968;&#25454;&#27969;&#12290;&#29992;&#25143;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#23558;&#29983;&#25104;&#21151;&#33021;&#30340;&#26114;&#36149;&#20219;&#21153;&#21368;&#36733;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#32473;&#23450;&#19968;&#20010;ANPL&#31243;&#24207;&#65292;ANPL&#32534;&#35793;&#22120;&#20250;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23454;&#29616;&#23380;&#20013;&#30340;&#21151;&#33021;&#65292;&#24182;&#36981;&#23432;&#33609;&#22270;&#20013;&#25351;&#23450;&#30340;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#23558;ANPL&#37096;&#32626;&#22312;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#65292;&#23427;&#26159;&#19968;&#32452;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29420;&#29305;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advents of Large Language Models (LLMs) have shown promise in augmenting programming using natural interactions. However, while LLMs are proficient in compiling common usage patterns into a programming language, e.g., Python, it remains a challenge how to edit and debug an LLM-generated program. We introduce ANPL, a programming system that allows users to decompose user-specific tasks. In an ANPL program, a user can directly manipulate sketch, which specifies the data flow of the generated program. The user annotates the modules, or hole with natural language descriptions offloading the expensive task of generating functionalities to the LLM. Given an ANPL program, the ANPL compiler generates a cohesive Python program that implements the functionalities in hole, while respecting the dataflows specified in sketch. We deploy ANPL on the Abstraction and Reasoning Corpus (ARC), a set of unique tasks that are challenging for state-of-the-art AI systems, showing it outperforms baseline p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.18486</link><description>&lt;p&gt;
&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#31995;&#32479;&#30740;&#31350;&#21644;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24320;&#21457;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#23558;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20135;&#20986;&#19982;&#22522;&#26412;&#20107;&#23454;&#36827;&#34892;&#27604;&#36739;&#65292;&#22240;&#27492;&#20854;&#22312;&#22522;&#20934;&#23398;&#26415;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545; ChatGPT &#22312;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312; 140 &#20010;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102; ChatGPT&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#30340; 255K &#27425;&#21709;&#24212;&#65292;&#36825;&#20351;&#25105;&#20204;&#30340;&#24037;&#20316;&#25104;&#20026;&#20102;&#22312; NLP &#22522;&#20934;&#27979;&#35797;&#20013;&#23545; ChatGPT &#36827;&#34892;&#30340;&#26368;&#22823;&#35780;&#20272;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992; LLM &#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#31181;&#26032;&#30340;&#36856;&#21457;&#33021;&#21147;&#65292;&#21363;&#36981;&#24490;&#22810;&#20010;&#26597;&#35810;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#24494;&#35843;&#23569;&#37327;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18466</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Test-Time Training on Nearest Neighbors for Large Language Models. (arXiv:2305.18466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18466
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#24494;&#35843;&#23569;&#37327;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35768;&#22810;&#24037;&#20316;&#37117;&#26088;&#22312;&#22312;&#27979;&#35797;&#26102;&#20174;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#20854;&#26631;&#20934;&#35757;&#32451;&#35774;&#32622;&#23545;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#38656;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#8220;Pile&#8221;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#23884;&#20837;&#30340;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#26368;&#36817;&#37051;&#32034;&#24341;&#12290;&#32473;&#23450;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#26816;&#32034;&#26597;&#35810;&#30340;&#37051;&#23621;&#65292;&#24182;&#22312;&#23545;&#24212;&#20110;&#36825;&#20123;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#19978;&#24494;&#35843;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26816;&#32034;&#21644;&#35757;&#32451;&#20165;20&#20010;&#37051;&#23621;&#65292;&#27599;&#20010;&#37051;&#23621;&#20165;&#36827;&#34892;&#19968;&#27425;&#26799;&#24230;&#36845;&#20195;&#65292;&#23601;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#8220;Pile&#8221;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20108;&#21313;&#20010;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26174;&#33879;&#32553;&#23567;&#20102;&#23567;&#22411;GPT2&#27169;&#22411;&#21644;GPTNeo&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21518;&#32773;&#26159;&#19987;&#38376;&#23545;&#8220;Pile&#8221;&#36827;&#34892;&#25910;&#25947;&#35757;&#32451;&#30340;&#65292;&#20307;&#31215;&#21364;&#26159;&#21069;&#32773;&#30340;&#21313;&#20493;&#20197;&#19978;&#12290;&#28982;&#32780;&#65292;&#20854;&#26041;&#27861;&#30340;&#25104;&#21151;&#36824;&#21462;&#20915;&#20110;&#20805;&#20998;&#30340;&#32034;&#24341;&#36136;&#37327;&#21644;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent efforts aim to augment language models with relevant information retrieved from a database at test time. We avoid the need for prompt engineering by directly fine-tuning the model on data retrieved at test time using its standard training setup. For this purpose, we build a large-scale distributed nearest neighbor index based on text embeddings of the Pile dataset. Given a query to a language model, our system retrieves the neighbors of the query and fine-tunes the model on the text data corresponding to those neighbors. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than twenty language modeling tasks in the Pile benchmark. For example, test-time training significantly narrows the performance gap between a small GPT2 model and a GPTNeo model, more than ten times larger, that was specifically trained to convergence on the Pile. Sufficient index quality and size, however, are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#37051;&#22495;&#27604;&#36739;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#26469;&#25552;&#39640;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20960;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#36825;&#20123;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18462</link><description>&lt;p&gt;
&#22522;&#20110;&#37051;&#22495;&#27604;&#36739;&#30340;&#35821;&#35328;&#27169;&#22411;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attacks against Language Models via Neighbourhood Comparison. (arXiv:2305.18462v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#37051;&#22495;&#27604;&#36739;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#26469;&#25552;&#39640;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20960;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#36825;&#20123;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIAs)&#26088;&#22312;&#39044;&#27979;&#19968;&#20010;&#25968;&#25454;&#26679;&#26412;&#26159;&#21542;&#23384;&#22312;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#25915;&#20987;&#20381;&#36182;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#27169;&#22411;&#20542;&#21521;&#20110;&#23558;&#26356;&#39640;&#30340;&#27010;&#29575;&#20998;&#37197;&#32473;&#35757;&#32451;&#26679;&#26412;&#32780;&#38750;&#38750;&#35757;&#32451;&#28857;&#12290;&#28982;&#32780;&#65292;&#23545;&#27169;&#22411;&#20998;&#25968;&#30340;&#31616;&#21333;&#38408;&#20540;&#35774;&#23450;&#24448;&#24448;&#23548;&#33268;&#39640;&#35823;&#25253;&#29575;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32771;&#34385;&#26679;&#26412;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#21442;&#32771;&#27169;&#22411;&#30340;&#25915;&#20987;&#21487;&#20197;&#23558;&#27169;&#22411;&#20998;&#25968;&#19982;&#22312;&#31867;&#20284;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#21442;&#32771;&#27169;&#22411;&#33719;&#24471;&#30340;&#20998;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;MIAs&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#35757;&#32451;&#21442;&#32771;&#27169;&#22411;&#65292;&#36825;&#31181;&#25915;&#20987;&#30340;&#20570;&#27861;&#26159;&#20551;&#23450;&#25932;&#26041;&#30693;&#36947;&#19982;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#23494;&#20999;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#20551;&#35774;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26356;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20551;&#23450;&#25915;&#20987;&#32773;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#30340;&#37051;&#22495;&#26679;&#26412;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#22312;&#26356;&#29616;&#23454;&#30340;&#25104;&#21592;&#25512;&#26029;&#22330;&#26223;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#22312;&#20960;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20854;&#20013;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#23545;&#35805;&#29983;&#25104;&#65292;&#24182;&#31361;&#26174;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to assign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of MIAs. However, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic sce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#25552;&#31034;&#65292;&#25511;&#21046;AI bot&#21040;&#36798;&#20219;&#20309;&#29366;&#24577;&#65292;&#32780;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#33391;&#22909;&#30340;Bot&#33021;&#20960;&#20046;&#30830;&#23450;&#22320;&#21040;&#36798;&#20219;&#20309;&#24847;&#20041;&#23376;&#38598;&#65292;&#20855;&#26377;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18449</link><description>&lt;p&gt;
&#39535;&#26381;AI Bot&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31070;&#32463;&#29366;&#24577;&#30340;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
Taming AI Bots: Controllability of Neural States in Large Language Models. (arXiv:2305.18449v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#25552;&#31034;&#65292;&#25511;&#21046;AI bot&#21040;&#36798;&#20219;&#20309;&#29366;&#24577;&#65292;&#32780;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#33391;&#22909;&#30340;Bot&#33021;&#20960;&#20046;&#30830;&#23450;&#22320;&#21040;&#36798;&#20219;&#20309;&#24847;&#20041;&#23376;&#38598;&#65292;&#20855;&#26377;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#25552;&#31034;&#65292;&#23558;AI bot &#25511;&#21046;&#21040;&#20219;&#20309;&#29366;&#24577;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#24847;&#20041;&#8221;&#23450;&#20041;&#65292;&#20415;&#20110;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#26465;&#20214;&#65292;&#34920;&#24449;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25152;&#26174;&#28982;&#35757;&#32451;&#30340;&#8220;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#8221;&#21644;&#8220;&#35757;&#32451;&#33391;&#22909;&#30340;LLM&#8221;&#12290;&#34429;&#28982;&#35757;&#32451;&#33391;&#22909;&#30340;LLM&#26500;&#24314;&#20102;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24847;&#20041;&#23884;&#20837;&#31354;&#38388;&#65292;&#20294;&#26159;&#24847;&#20041;&#26412;&#36523;&#24182;&#19981;&#24418;&#25104;&#19968;&#20010;&#21521;&#37327;&#65288;&#32447;&#24615;&#65289;&#23376;&#31354;&#38388;&#65292;&#32780;&#26159;&#19968;&#20010;&#21830;&#31354;&#38388;&#12290;&#25105;&#20204;&#28982;&#21518;&#34920;&#24449;&#20102;&#26576;&#20010;&#36755;&#20837;&#25552;&#31034;&#30340;&#29366;&#24577;&#25152;&#33021;&#21040;&#36798;&#30340;&#24847;&#20041;&#23376;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#35757;&#32451;&#33391;&#22909;&#30340;Bot&#33021;&#22815;&#21040;&#36798;&#20219;&#20309;&#24847;&#20041;&#65292;&#23613;&#31649;&#27010;&#29575;&#24456;&#23567;&#12290;&#25105;&#20204;&#25509;&#30528;&#24341;&#20837;&#20102;&#26356;&#24378;&#30340;&#21487;&#25511;&#24615;&#27010;&#24565;&#65292;&#21363;&#8220;&#20960;&#20046;&#30830;&#23450;&#21487;&#36798;&#24615;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#38480;&#21046;&#21040;&#24847;&#20041;&#31354;&#38388;&#26102;&#65292;AI bot&#26159;&#21487;&#25511;&#30340;&#12290;&#25105;&#20204;&#22312;&#24341;&#20837;&#21151;&#33021;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#36825;&#26679;&#20570;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the question of whether an agent can, by suitable choice of prompts, control an AI bot to any state. To that end, we first introduce a formal definition of ``meaning'' that is amenable to analysis. Then, we characterize ``meaningful data'' on which large language models (LLMs) are ostensibly trained, and ``well-trained LLMs'' through conditions that are largely met by today's LLMs. While a well-trained LLM constructs an embedding space of meanings that is Euclidean, meanings themselves do not form a vector (linear) subspace, but rather a quotient space within. We then characterize the subset of meanings that can be reached by the state of the LLMs for some input prompt, and show that a well-trained bot can reach any meaning albeit with small probability. We then introduce a stronger notion of controllability as {\em almost certain reachability}, and show that, when restricted to the space of meanings, an AI bot is controllable. We do so after introducing a functional characte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#37319;&#29992;&#21452;&#21521;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#38271;&#31687;&#38899;&#39057;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.18419</link><description>&lt;p&gt;
&#37319;&#29992;&#21452;&#21521;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#25552;&#39640;&#38271;&#31687;&#38899;&#39057;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR. (arXiv:2305.18419v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#37319;&#29992;&#21452;&#21521;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#38271;&#31687;&#38899;&#39057;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#31163;&#21457;&#38899;&#20013;&#30340;&#35821;&#20041;&#23436;&#25972;&#21477;&#23376;&#23454;&#29616;&#38271;&#31687;&#38899;&#39057;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#38450;&#27490;ASR&#35299;&#30721;&#22120;&#22788;&#29702;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#30340;&#26080;&#25928;&#20449;&#24687;&#65292;&#21516;&#26102;&#36991;&#20813;&#23427;&#38169;&#36807;&#24403;&#21069;&#21477;&#23376;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#22312;&#20070;&#38754;&#25991;&#26412;&#20013;&#65292;&#35821;&#20041;&#23436;&#25972;&#30340;&#21477;&#23376;&#36890;&#24120;&#30001;&#26631;&#28857;&#31526;&#21495;&#20998;&#38548;&#65307;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#23454;&#38469;&#21475;&#35821;&#20013;&#30340;&#21457;&#38899;&#24456;&#23569;&#21253;&#21547;&#26631;&#28857;&#31526;&#21495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#28860;&#20174;&#20070;&#38754;&#26631;&#28857;&#25991;&#26412;&#20013;&#35757;&#32451;&#30340;&#21452;&#21521;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20013;&#30340;&#26631;&#28857;&#31526;&#21495;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#20174;LM&#25945;&#24072;&#25552;&#28860;&#30340;&#20998;&#21106;&#22120;&#19982;&#20197;&#20854;&#20182;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#22768;&#23398;&#20572;&#39039;&#30340;&#25945;&#24072;&#25552;&#28860;&#30340;&#20998;&#21106;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#27969;&#23186;&#20307;ASR&#31649;&#36947;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#20998;&#21106;&#22120;&#22312;YouTube&#23383;&#24149;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;3.2%&#30340;&#30456;&#23545;WER&#22686;&#30410;&#20197;&#21450;60ms&#20013;&#20301;&#27573;&#26411;&#31471;&#24310;&#36831;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method of segmenting long-form speech by separating semantically complete sentences within the utterance. This prevents the ASR decoder from needlessly processing faraway context while also preventing it from missing relevant context within the current sentence. Semantically complete sentence boundaries are typically demarcated by punctuation in written text; but unfortunately, spoken real-world utterances rarely contain punctuation. We address this limitation by distilling punctuation knowledge from a bidirectional teacher language model (LM) trained on written, punctuated text. We compare our segmenter, which is distilled from the LM teacher, against a segmenter distilled from a acoustic-pause-based teacher used in other works, on a streaming ASR pipeline. The pipeline with our segmenter achieves a 3.2% relative WER gain along with a 60 ms median end-of-segment latency reduction on a YouTube captioning task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#22810;&#32452;&#23398;&#25968;&#25454;&#22312;&#22240;&#26524;&#25512;&#26029;&#12289;&#22522;&#22240;&#32452;&#23398;&#21644;&#20083;&#33146;&#30284;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#22914;&#20309;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#20998;&#26512;&#22522;&#22240;&#32452;&#25200;&#21160;&#23545;&#20083;&#33146;&#30284;&#24739;&#32773;&#29983;&#23384;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.18410</link><description>&lt;p&gt;
&#29702;&#35299;&#20083;&#33146;&#30284;&#29983;&#23384;&#65306;&#22312;&#22810;&#32452;&#23398;&#25968;&#25454;&#19978;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#21644;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Understanding Breast Cancer Survival: Using Causality and Language Models on Multi-omics Data. (arXiv:2305.18410v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#22810;&#32452;&#23398;&#25968;&#25454;&#22312;&#22240;&#26524;&#25512;&#26029;&#12289;&#22522;&#22240;&#32452;&#23398;&#21644;&#20083;&#33146;&#30284;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#22914;&#20309;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#20998;&#26512;&#22522;&#22240;&#32452;&#25200;&#21160;&#23545;&#20083;&#33146;&#30284;&#24739;&#32773;&#29983;&#23384;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23545;&#26356;&#26131;&#29992;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#21457;&#23637;&#21644;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#20197;&#36890;&#36807;&#20998;&#26512;&#35266;&#27979;&#25968;&#25454;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#20020;&#24202;&#21307;&#29983;&#21644;&#29983;&#29289;&#23398;&#23478;&#39044;&#27979;&#30142;&#30149;&#39044;&#21518;&#24182;&#24314;&#35758;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#22240;&#26524;&#25512;&#26029;&#12289;&#22522;&#22240;&#32452;&#23398;&#21644;&#20083;&#33146;&#30284;&#20132;&#21449;&#39046;&#22495;&#19978;&#26497;&#23569;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#30495;&#23454;&#25968;&#25454;&#19978;&#23545;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30340;&#35780;&#20272;&#26222;&#36941;&#26497;&#20026;&#22256;&#38590;&#65292;&#22240;&#20026;&#22320;&#38754;&#30495;&#23454;&#30340;&#22240;&#26524;&#20851;&#31995;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36824;&#25552;&#35758;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#35780;&#20272;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#21512;&#36866;&#30340;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#26469;&#30740;&#31350;&#22522;&#22240;&#32452;&#20013;&#21508;&#31181;&#25200;&#21160;&#22914;&#20309;&#24433;&#21709;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#29983;&#23384;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for more usable and explainable machine learning models in healthcare increases the importance of developing and utilizing causal discovery algorithms, which aim to discover causal relations by analyzing observational data. Explainable approaches aid clinicians and biologists in predicting the prognosis of diseases and suggesting proper treatments. However, very little research has been conducted at the crossroads between causal discovery, genomics, and breast cancer, and we aim to bridge this gap. Moreover, evaluation of causal discovery methods on real data is in general notoriously difficult because ground-truth causal relations are usually unknown, and accordingly, in this paper, we also propose to address the evaluation problem with large language models. In particular, we exploit suitable causal discovery algorithms to investigate how various perturbations in the genome can affect the survival of patients diagnosed with breast cancer. We used three main causal discovery 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.18404</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#26696;&#30830;&#35748;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24320;&#21457;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#20581;&#22766;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#23558;&#25104;&#20026;&#23427;&#20204;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#19979;&#23433;&#20840;&#37096;&#32626;&#30340;&#20851;&#38190;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#36825;&#31181;&#35266;&#23519;&#23545;&#20110;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#36873;&#25321;&#24615;&#20998;&#31867;&#21644;&#36807;&#28388;&#20302;&#36136;&#37327;&#39044;&#27979;&#65292;&#21487;&#33021;&#20250;&#26377;&#29992;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#23545;&#20110;&#36229;&#20986;&#20027;&#39064;&#30340;&#38382;&#39064;&#30340;&#20132;&#25442;&#24615;&#20551;&#35774;&#65292;&#36825;&#21487;&#33021;&#26159;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#26356;&#20026;&#29616;&#23454;&#30340;&#22330;&#26223;&#12290;&#26412;&#30740;&#31350;&#20026;&#22312;&#38656;&#35201;&#21487;&#38752;&#20445;&#35777;&#38169;&#35823;&#29575;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#20917;&#19979;&#26356;&#21152;&#20540;&#24471;&#20449;&#36182;&#21644;&#21487;&#38752;&#22320;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26041;&#27861;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#35745;&#31639;&#21152;&#36895;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.18396</link><description>&lt;p&gt;
LLM&#21487;&#20197;&#29702;&#35299;&#21152;&#23494;&#25552;&#31034;&#65306;&#38754;&#21521;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;Transformers
&lt;/p&gt;
&lt;p&gt;
LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers. (arXiv:2305.18396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26041;&#27861;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#35745;&#31639;&#21152;&#36895;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#26381;&#21153;&#22120;&#23458;&#25143;&#31471;&#29615;&#22659;&#20013;&#20026;&#22522;&#20110;transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26500;&#24314;&#31169;&#26377;&#25512;&#26029;&#26694;&#26550;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#25345;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#23458;&#25143;&#31471;&#36755;&#20837;&#31169;&#26377;&#25968;&#25454;&#36827;&#34892;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#24403;&#31169;&#26377;&#36755;&#20837;&#36890;&#36807;&#21407;&#22987;LLMs&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#26102;&#65292;&#36825;&#20123;&#26694;&#26550;&#20250;&#20135;&#29983;&#26174;&#30528;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#65292;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;&#19982;&#26368;&#26032;&#30340;Iron&#65288;NeurIPS 2022&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#27169;&#22411;&#25512;&#26029;&#31649;&#36947;&#22312;&#35745;&#31639;&#19978;&#23454;&#29616;&#20102;$5 \times$&#30340;&#21152;&#36895;&#65292;&#22312;&#36890;&#20449;&#24320;&#38144;&#19978;&#23454;&#29616;&#20102;80\%&#30340;&#38477;&#20302;&#65292;&#21516;&#26102;&#20960;&#20046;&#20445;&#25345;&#20102;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior works have attempted to build private inference frameworks for transformer-based large language models (LLMs) in a server-client setting, where the server holds the model parameters and the client inputs the private data for inference. However, these frameworks impose significant overhead when the private inputs are forward propagated through the original LLMs. In this paper, we show that substituting the computation- and communication-heavy operators in the transformer architecture with privacy-computing friendly approximations can greatly reduce the private inference costs with minor impact on model performance. Compared to the state-of-the-art Iron (NeurIPS 2022), our privacy-computing friendly model inference pipeline achieves a $5\times$ acceleration in computation and an 80\% reduction in communication overhead, while retaining nearly identical accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KARD&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21521;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#20837;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#26469;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18395</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#25512;&#29702;&#33976;&#39311;&#65306;&#38754;&#21521;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks. (arXiv:2305.18395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KARD&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21521;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#20837;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#26469;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#21512;&#30693;&#35782;&#29702;&#35299;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#35745;&#31639;&#35201;&#27714;&#39640;&#19988;&#28041;&#21450;&#25968;&#25454;&#38544;&#31169;&#65292;&#23558;&#27492;&#31867;&#27169;&#22411;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#36890;&#36807;&#24494;&#35843;&#20855;&#26377;&#26631;&#35760;&#25968;&#25454;&#25110;&#33976;&#39311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26500;&#24314;&#20219;&#21153;&#29305;&#23450;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#26159;&#30001;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#25152;&#38656;&#30693;&#35782;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#29702;&#35770;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#22686;&#24378;&#30340;&#25512;&#29702;&#33976;&#39311; (KARD) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24494;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#30340;&#20381;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#37325;&#25490;&#22120;&#65292;&#29992;&#20110;&#33719;&#24471;&#19982;&#20381;&#25454;&#29983;&#25104;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;KARD&#22312;&#19977;&#39033;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#26174;&#30528;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#23610;&#23544;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36798;&#21040;&#19982;LLMs&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small language models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22330;&#26223;&#22270;&#21644;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21270;&#34920;&#36798;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20998;&#31867;&#12290;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30456;&#27604;&#20351;&#29992;&#23398;&#20064;&#34920;&#36798;&#24335;&#30340;&#27169;&#22411;&#26377;&#25152;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2305.18391</link><description>&lt;p&gt;
MemeGraphs: &#23558;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#36830;
&lt;/p&gt;
&lt;p&gt;
MemeGraphs: Linking Memes to Knowledge Graphs. (arXiv:2305.18391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18391
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22330;&#26223;&#22270;&#21644;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21270;&#34920;&#36798;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20998;&#31867;&#12290;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30456;&#27604;&#20351;&#29992;&#23398;&#20064;&#34920;&#36798;&#24335;&#30340;&#27169;&#22411;&#26377;&#25152;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#26159;&#19968;&#31181;&#22312;&#31038;&#20132;&#23186;&#20307;&#21644;&#20114;&#32852;&#32593;&#19978;&#27969;&#34892;&#30340;&#20256;&#25773;&#36235;&#21183;&#21644;&#35266;&#28857;&#30340;&#24418;&#24335;&#65292;&#32467;&#21512;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#27169;&#24335;&#12290;&#23427;&#20204;&#21487;&#20197;&#34920;&#36798;&#24189;&#40664;&#21644;&#35773;&#21050;&#65292;&#20294;&#20063;&#21487;&#33021;&#21547;&#26377;&#20882;&#29359;&#24615;&#30340;&#20869;&#23481;&#12290;&#33258;&#21160;&#20998;&#26512;&#21644;&#20998;&#31867;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20854;&#35299;&#37322;&#20381;&#36182;&#20110;&#23545;&#35270;&#35273;&#20803;&#32032;&#12289;&#35821;&#35328;&#21644;&#32972;&#26223;&#30693;&#35782;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#26159;&#26377;&#24847;&#20041;&#22320;&#34920;&#31034;&#36825;&#20123;&#26469;&#28304;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20197;&#20415;&#23558;&#34920;&#24773;&#21253;&#20316;&#20026;&#25972;&#20307;&#20998;&#31867;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22330;&#26223;&#22270;&#20316;&#20026;&#34920;&#31034;&#22270;&#20687;&#20013;&#29289;&#20307;&#21450;&#20854;&#35270;&#35273;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#36798;&#26041;&#24335;&#65292;&#24182;&#23558;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#20998;&#31867;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;ImgBERT&#36827;&#34892;&#27604;&#36739;&#65292;&#21518;&#32773;&#20351;&#29992;&#20165;&#23398;&#20064;&#65288;&#32780;&#19981;&#26159;&#32467;&#26500;&#21270;&#65289;&#30340;&#34920;&#36798;&#24335;&#36827;&#34892;&#22810;&#27169;&#24335;&#24314;&#27169;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22987;&#32456;&#26377;&#25152;&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#20154;&#24037;&#22270;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20379;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memes are a popular form of communicating trends and ideas in social media and on the internet in general, combining the modalities of images and text. They can express humor and sarcasm but can also have offensive content. Analyzing and classifying memes automatically is challenging since their interpretation relies on the understanding of visual elements, language, and background knowledge. Thus, it is important to meaningfully represent these sources and the interaction between them in order to classify a meme as a whole. In this work, we propose to use scene graphs, that express images in terms of objects and their visual relations, and knowledge graphs as structured representations for meme classification with a Transformer-based architecture. We compare our approach with ImgBERT, a multimodal model that uses only learned (instead of structured) representations of the meme, and observe consistent improvements. We further provide a dataset with human graph annotations that we compa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#33258;&#21457;&#27169;&#22359;&#21270;&#29616;&#35937;&#65292;&#21457;&#29616;&#31070;&#32463;&#20803;&#21487;&#20197;&#36827;&#34892;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#24314;&#31435;&#36215;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#27492;&#32467;&#26500;&#21487;&#34987;&#26377;&#25928;&#25200;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.18390</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#33258;&#21457;&#27169;&#22359;&#21270;
&lt;/p&gt;
&lt;p&gt;
Emergent Modularity in Pre-trained Transformers. (arXiv:2305.18390v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#33258;&#21457;&#27169;&#22359;&#21270;&#29616;&#35937;&#65292;&#21457;&#29616;&#31070;&#32463;&#20803;&#21487;&#20197;&#36827;&#34892;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#24314;&#31435;&#36215;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#27492;&#32467;&#26500;&#21487;&#34987;&#26377;&#25928;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#27169;&#22359;&#21270;&#29305;&#24449;&#65292;&#36825;&#26159;&#20154;&#33041;&#20013;&#24120;&#35265;&#30340;&#29305;&#28857;&#65292;&#34987;&#35748;&#20026;&#23545;&#20110;&#26222;&#36941;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20027;&#35201;&#32771;&#34385;&#20102;&#27169;&#22359;&#21270;&#30340;&#20004;&#20010;&#20027;&#35201;&#29305;&#24449;&#65306;&#65288;1&#65289;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#19987;&#19994;&#21270;&#65306;&#25105;&#20204;&#35780;&#20272;&#20102;&#27599;&#20010;&#31070;&#32463;&#20803;&#26159;&#21542;&#20027;&#35201;&#19987;&#19994;&#21270;&#20110;&#26576;&#19968;&#21151;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#26159;&#30340;&#12290;&#65288;2&#65289;&#22522;&#20110;&#21151;&#33021;&#32858;&#31867;&#30340;&#31070;&#32463;&#20803;&#20998;&#32452;&#65306;&#25105;&#20204;&#25506;&#31350;&#20102;&#23558;&#31070;&#32463;&#20803;&#25353;&#21151;&#33021;&#20998;&#32452;&#30340;&#32467;&#26500;&#23547;&#25214;&#26041;&#27861;&#65292;&#27599;&#20010;&#27169;&#22359;&#22343;&#20026;&#20854;&#30456;&#24212;&#21151;&#33021;&#24037;&#20316;&#12290;&#37492;&#20110;&#21487;&#33021;&#23384;&#22312;&#30340;&#22823;&#37327;&#32467;&#26500;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20102;&#20998;&#23618;&#19987;&#23478;&#27169;&#22411;&#36523;&#19978;&#65292;&#24182;&#23558;&#31070;&#32463;&#20803;&#21010;&#20998;&#20026;&#19987;&#23478;&#65292;&#36890;&#24120;&#20026;&#19981;&#21516;&#30340;&#36755;&#20837;&#28608;&#27963;&#19981;&#21516;&#30340;&#19987;&#23478;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#21151;&#33021;&#19987;&#23478;&#65292;&#32858;&#38598;&#20102;&#26576;&#19968;&#21151;&#33021;&#30340;&#31070;&#32463;&#20803;&#12290;&#27492;&#22806;&#65292;&#25200;&#21160;&#21151;&#33021;&#19987;&#23478;&#30340;&#28608;&#27963;&#26174;&#33879;&#24433;&#21709;&#20102;&#30456;&#24212;&#30340;f&#38190;
&lt;/p&gt;
&lt;p&gt;
This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate whether each neuron is mainly specialized in a certain function, and find that the answer is yes. (2) function-based neuron grouping: we explore finding a structure that groups neurons into modules by function, and each module works for its corresponding function. Given the enormous amount of possible structures, we focus on Mixture-of-Experts as a promising candidate, which partitions neurons into experts and usually activates different experts for different inputs. Experimental results show that there are functional experts, where clustered are the neurons specialized in a certain function. Moreover, perturbing the activations of functional experts significantly affects the corresponding f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#31532;&#19968;&#31687;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;VLMs&#23454;&#35777;&#30740;&#31350;&#22270;&#20687;&#24191;&#21578;&#29702;&#35299;&#30340;&#25991;&#31456;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#29305;&#24449;&#33258;&#36866;&#24212;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#22320;&#34701;&#21512;&#20102;&#22270;&#20687;&#24191;&#21578;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#30693;&#35782;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#27169;&#22411;&#12290;&#30740;&#31350;&#23545;&#24191;&#21578;&#34892;&#19994;&#20855;&#26377;&#24191;&#27867;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18373</link><description>&lt;p&gt;
KAFA&#65306;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#29305;&#24449;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#24605;&#32771;&#22270;&#20687;&#24191;&#21578;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models. (arXiv:2305.18373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#31532;&#19968;&#31687;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;VLMs&#23454;&#35777;&#30740;&#31350;&#22270;&#20687;&#24191;&#21578;&#29702;&#35299;&#30340;&#25991;&#31456;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#29305;&#24449;&#33258;&#36866;&#24212;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#22320;&#34701;&#21512;&#20102;&#22270;&#20687;&#24191;&#21578;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#30693;&#35782;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#27169;&#22411;&#12290;&#30740;&#31350;&#23545;&#24191;&#21578;&#34892;&#19994;&#20855;&#26377;&#24191;&#27867;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24191;&#21578;&#29702;&#35299;&#26159;&#19968;&#20010;&#38750;&#24120;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#28041;&#21450;&#38750;&#20856;&#22411;&#22330;&#26223;&#65292;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#65292;&#24182;&#23545;&#22330;&#26223;&#25991;&#26412;&#36827;&#34892;&#25512;&#29702;&#31561;&#65292;&#20294;&#22914;&#20309;&#35299;&#37322;&#22270;&#20687;&#24191;&#21578;&#30456;&#23545;&#23569;&#26377;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#21331;&#36234;&#30340;&#27867;&#21270;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#22522;&#30784;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#26102;&#20195;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;VLMs&#30340;&#35270;&#35282;&#65292;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23454;&#35777;&#30740;&#31350;&#22270;&#20687;&#24191;&#21578;&#29702;&#35299;&#12290;&#25105;&#20204;&#35780;&#20272;&#24182;&#25581;&#31034;&#20102;&#23558;&#36825;&#20123;VLMs&#36866;&#24212;&#20110;&#22270;&#20687;&#24191;&#21578;&#29702;&#35299;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#29305;&#24449;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#22320;&#34701;&#21512;&#22270;&#20687;&#24191;&#21578;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#30693;&#35782;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#36215;&#26356;&#22810;&#20154;&#23545;&#24191;&#21578;&#34892;&#19994;&#24191;&#27867;&#30456;&#20851;&#30340;&#22270;&#20687;&#24191;&#21578;&#29702;&#35299;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image ad understanding is a crucial task with wide real-world applications. Although highly challenging with the involvement of diverse atypical scenes, real-world entities, and reasoning over scene-texts, how to interpret image ads is relatively under-explored, especially in the era of foundational vision-language models (VLMs) featuring impressive generalizability and adaptability. In this paper, we perform the first empirical study of image ad understanding through the lens of pre-trained VLMs. We benchmark and reveal practical challenges in adapting these VLMs to image ad understanding. We propose a simple feature adaptation strategy to effectively fuse multimodal information for image ads and further empower it with knowledge of real-world entities. We hope our study draws more attention to image ad understanding which is broadly relevant to the advertising industry.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#25324; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.18365</link><description>&lt;p&gt;
GPT &#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#21040;&#24213;&#26377;&#24590;&#26679;&#30340;&#24212;&#29992;&#65311;&#20843;&#20010;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#25324; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24378;&#22823;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31185;&#23398;&#12289;&#37329;&#34701;&#21644;&#36719;&#20214;&#24037;&#31243;&#31561;&#39046;&#22495;&#12290;&#20294;&#26159;&#65292;LLM &#26159;&#21542;&#26377;&#33021;&#21147;&#25512;&#21160;&#21270;&#23398;&#39046;&#22495;&#30340;&#36827;&#23637;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#21547; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#21517;&#31216;&#39044;&#27979;&#12289;&#23646;&#24615;&#39044;&#27979;&#12289;&#20135;&#37327;&#39044;&#27979;&#12289;&#21453;&#24212;&#39044;&#27979;&#12289;&#21453;&#21512;&#25104;&#65288;&#20174;&#20135;&#29289;&#39044;&#27979;&#21453;&#24212;&#29289;&#65289;&#12289;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#23376;&#35774;&#35745;&#12289;&#20998;&#23376;&#23383;&#24149;&#21644;&#35797;&#21058;&#36873;&#25321;&#12290;&#25105;&#20204;&#20351;&#29992;&#24191;&#27867;&#35748;&#21487;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324; BBBP&#12289;Tox21&#12289;PubChem&#12289;USPTO &#21644; ChEBI&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;&#22312;&#31934;&#24515;&#36873;&#25321;&#30340;&#31034;&#20363;&#20013;&#65292;&#23545;&#19977;&#31181; GPT &#27169;&#22411;&#65288;GPT-4&#12289;GPT-3.5 &#21644; DaVinci-003&#65289;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been rapidly applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper,we establish a comprehensive benchmark containing 8 practical chemistry tasks, including 1) name prediction, 2) property prediction, 3) yield prediction, 4) reaction prediction, 5) retrosynthesis (prediction of reactants from products), 6)text-based molecule design, 7) molecule captioning, and 8) reagent selection. Our analysis draws on widely recognized datasets including BBBP, Tox21, PubChem, USPTO, and ChEBI, facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Three GPT models (GPT-4, GPT-3.5,and Davinci-003) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#20132;&#20114;&#30340;&#20132;&#20114;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#25913;&#21892;&#35270;&#35273;&#20998;&#26512;&#24212;&#29992;&#20013;&#30340;&#35821;&#20041;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.18357</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#20132;&#20114;&#30340;&#20132;&#20114;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepSI: Interactive Deep Learning for Semantic Interaction. (arXiv:2305.18357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#20132;&#20114;&#30340;&#20132;&#20114;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#25913;&#21892;&#35270;&#35273;&#20998;&#26512;&#24212;&#29992;&#20013;&#30340;&#35821;&#20041;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#35270;&#35273;&#20998;&#26512;&#24212;&#29992;&#20013;&#30340;&#35821;&#20041;&#20132;&#20114;&#12290;&#35821;&#20041;&#20132;&#20114;&#25512;&#26029;&#20998;&#26512;&#20154;&#21592;&#22312;&#24863;&#30693;&#36807;&#31243;&#20013;&#30340;&#31934;&#30830;&#24847;&#22270;&#21462;&#20915;&#20110;&#24213;&#23618;&#25968;&#25454;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;$\text{DeepSI}_{\text{finetune}}$&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#21040;&#20154;&#22312;&#20132;&#20114;&#24335;&#24863;&#30693;&#31649;&#36947;&#20013;&#65292;&#20855;&#26377;&#20004;&#20010;&#37325;&#35201;&#23646;&#24615;&#12290;&#39318;&#20808;&#65292;&#28145;&#24230;&#23398;&#20064;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#35821;&#20041;&#20132;&#20114;&#25512;&#26029;&#30340;&#36136;&#37327;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#35821;&#20041;&#20132;&#20114;&#26469;&#24494;&#35843;&#28145;&#24230;&#23398;&#20064;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35821;&#20041;&#20132;&#20114;&#25512;&#26029;&#30340;&#36136;&#37327;&#12290;&#20154;&#26426;&#20132;&#20114;&#21644;&#28145;&#24230;&#23398;&#20064;&#20043;&#38388;&#30340;&#21453;&#39304;&#24490;&#29615;&#20351;&#24471;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35780;&#20272;&#23558;&#28145;&#24230;&#23398;&#20064;&#23884;&#20837;&#21040;&#35821;&#20041;&#20132;&#20114;&#24490;&#29615;&#20013;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;$\text{DeepSI}_{\te
&lt;/p&gt;
&lt;p&gt;
In this paper, we design novel interactive deep learning methods to improve semantic interactions in visual analytics applications. The ability of semantic interaction to infer analysts' precise intents during sensemaking is dependent on the quality of the underlying data representation. We propose the $\text{DeepSI}_{\text{finetune}}$ framework that integrates deep learning into the human-in-the-loop interactive sensemaking pipeline, with two important properties. First, deep learning extracts meaningful representations from raw data, which improves semantic interaction inference. Second, semantic interactions are exploited to fine-tune the deep learning representations, which then further improves semantic interaction inference. This feedback loop between human interaction and deep learning enables efficient learning of userand task-specific representations. To evaluate the advantage of embedding the deep learning within the semantic interaction loop, we compare $\text{DeepSI}_{\te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;GPT&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#38656;&#35201;&#26680;&#24515;&#27010;&#24565;&#8220;&#26680;&#24515;&#30693;&#35782;&#8221;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23545;&#35937;&#30340;&#34920;&#31034;&#26041;&#27861;&#21644;&#26032;&#30340;1D-ARC&#22522;&#20934;&#65292;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.18354</link><description>&lt;p&gt;
LLM&#21644;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#65306;&#25104;&#21151;&#12289;&#22833;&#36133;&#19982;&#22522;&#20110;&#23545;&#35937;&#34920;&#31034;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations. (arXiv:2305.18354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;GPT&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#38656;&#35201;&#26680;&#24515;&#27010;&#24565;&#8220;&#26680;&#24515;&#30693;&#35782;&#8221;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23545;&#35937;&#30340;&#34920;&#31034;&#26041;&#27861;&#21644;&#26032;&#30340;1D-ARC&#22522;&#20934;&#65292;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21542;&#35299;&#20915;&#31616;&#21333;&#30340;&#25277;&#35937;&#25512;&#29702;&#38382;&#39064;&#65311;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#20998;&#26512;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#30340;&#34920;&#29616;&#26469;&#25506;&#32034;&#36825;&#20010;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#65292;&#20174;&#26377;&#38480;&#30340;&#20363;&#23376;&#20013;&#35201;&#27714;&#25105;&#20204;&#26377;&#20123;&#20851;&#20110;&#27010;&#24565;&#65288;&#22914;&#23545;&#35937;&#12289;&#30446;&#26631;&#29366;&#24577;&#12289;&#35745;&#25968;&#21644;&#22522;&#26412;&#20960;&#20309;&#65289;&#30340;&#8220;&#26680;&#24515;&#30693;&#35782;&#8221;&#20197;&#35299;&#20915;&#38382;&#39064;&#12290;&#24403;&#20351;&#29992;&#25991;&#26412;&#32534;&#30721;&#23545;&#20108;&#32500;&#36755;&#20837;&#36755;&#20986;&#32593;&#26684;&#30340;ARC&#20219;&#21153;&#36827;&#34892;&#32534;&#30721;&#26102;&#65292;GPT-4&#20165;&#35299;&#20915;&#20102;50&#20010;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#30340;13&#20010;&#12290;&#25105;&#20204;&#30340;&#22833;&#36133;&#20998;&#26512;&#26174;&#31034;&#65292;GPT-4&#35782;&#21035;&#23545;&#35937;&#24182;&#25512;&#29702;&#23427;&#20204;&#30340;&#33021;&#21147;&#21463;&#21040;&#34920;&#31034;&#20219;&#21153;&#20013;&#23545;&#35937;&#30340;&#25991;&#26412;&#30340;&#39034;&#24207;&#24615;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;1D-ARC&#65292;&#23427;&#30001;&#26356;&#26377;&#21033;&#20110;&#22522;&#20110;GPT&#30340;&#25512;&#29702;&#30340;&#19968;&#32500;&#65288;&#31867;&#20284;&#25968;&#32452;&#65289;&#20219;&#21153;&#32452;&#25104;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#65292;GPT&#30340;&#34920;&#29616;&#30830;&#23454;&#27604;&#22312;&#65288;2D&#65289;ARC&#19978;&#26356;&#22909;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#35937;&#30340;&#32534;&#30721;&#26041;&#26696;&#65292;&#23427;&#20445;&#30041;&#20102;&#23545;&#35937;&#20043;&#38388;&#30340;&#22522;&#26412;&#31354;&#38388;&#20851;&#31995;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25512;&#29702;&#12290;&#20351;&#29992;&#36825;&#31181;&#32534;&#30721;&#65292;GPT-4&#22312;ARC&#19978;&#30340;&#25104;&#21151;&#29575;&#22823;&#22823;&#25552;&#39640;&#65292;&#36798;&#21040;&#20102;45/50&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#20351;&#29992;&#22522;&#20110;&#23545;&#35937;&#30340;&#34920;&#31034;&#26041;&#27861;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;LLM&#22522;&#20110;&#25512;&#29702;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can a Large Language Model (LLM) solve simple abstract reasoning problems? We explore this broad question through a systematic analysis of GPT on the Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract reasoning ability from limited examples in which solutions require some "core knowledge" of concepts such as objects, goal states, counting, and basic geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly influenced by the sequential nature of the text that represents an object within a text encoding of a task. To test this hypothesis, we design a new benchmark, the 1D-ARC, which consists of one-dimensional (array-like) tasks that are more conducive to GPT-based reasoning, and where it indeed performs better than on the (2D) ARC. To alleviate this issue, we prop
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#25366;&#25496;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#21487;&#20197;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#31181;&#23376;&#23646;&#24615;&#38598;&#21512;&#36731;&#24230;&#30417;&#30563;&#24182;&#33258;&#21160;&#21457;&#29616;&#26032;&#30340;&#23646;&#24615;&#31867;&#22411;&#12290;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#21551;&#21457;&#24335;&#21644;&#26080;&#30417;&#30563;&#28508;&#22312;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#39069;&#22806;&#30340;&#38544;&#21547;&#35821;&#20041;&#20449;&#21495;&#20316;&#20026;&#36741;&#21161;&#30417;&#30563;&#65292;&#23558;&#29616;&#26377;&#31867;&#22411;&#30340;&#23646;&#24615;&#25193;&#23637;&#26368;&#22810;12&#20493;&#65292;&#24182;&#25104;&#21151;&#21457;&#25496;&#20102;39&#65285;&#30340;&#26032;&#23646;&#24615;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.18350</link><description>&lt;p&gt;
&#23454;&#29616;&#24320;&#25918;&#19990;&#30028;&#20135;&#21697;&#23646;&#24615;&#25366;&#25496;&#65306;&#22522;&#20110;&#36731;&#24230;&#30417;&#30563;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Open-World Product Attribute Mining: A Lightly-Supervised Approach. (arXiv:2305.18350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#25366;&#25496;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#21487;&#20197;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#31181;&#23376;&#23646;&#24615;&#38598;&#21512;&#36731;&#24230;&#30417;&#30563;&#24182;&#33258;&#21160;&#21457;&#29616;&#26032;&#30340;&#23646;&#24615;&#31867;&#22411;&#12290;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#21551;&#21457;&#24335;&#21644;&#26080;&#30417;&#30563;&#28508;&#22312;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#39069;&#22806;&#30340;&#38544;&#21547;&#35821;&#20041;&#20449;&#21495;&#20316;&#20026;&#36741;&#21161;&#30417;&#30563;&#65292;&#23558;&#29616;&#26377;&#31867;&#22411;&#30340;&#23646;&#24615;&#25193;&#23637;&#26368;&#22810;12&#20493;&#65292;&#24182;&#25104;&#21151;&#21457;&#25496;&#20102;39&#65285;&#30340;&#26032;&#23646;&#24615;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#25366;&#25496;&#20219;&#21153;&#35774;&#32622;&#65292;&#29992;&#20110;&#25552;&#21462;&#24320;&#25918;&#19990;&#30028;&#23646;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20154;&#24037;&#24178;&#39044;&#12290;&#25105;&#20204;&#30340;&#30417;&#30563;&#26469;&#33258;&#20110;&#29616;&#26377;&#36164;&#28304;&#20013;&#24341;&#23548;&#30340;&#39640;&#36136;&#37327;&#31181;&#23376;&#23646;&#24615;&#38598;&#21512;&#65292;&#26088;&#22312;&#25193;&#23637;&#29616;&#26377;&#31181;&#23376;&#31867;&#22411;&#30340;&#23646;&#24615;&#35789;&#27719;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#26041;&#24335;&#21457;&#29616;&#20219;&#20309;&#26032;&#30340;&#23646;&#24615;&#31867;&#22411;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#29305;&#23450;&#20110;&#21463;&#38480;&#30417;&#30563;&#30340;Amacer&#26041;&#27861;&#12290;&#23588;&#20854;&#26159;&#65292;&#30001;&#20110;&#37027;&#20123;&#26410;&#35265;&#36807;&#30340;&#26032;&#23646;&#24615;&#27809;&#26377;&#30452;&#25509;&#30417;&#30563;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#20844;&#24335;&#21033;&#29992;&#20102;&#33258;&#25105;&#30417;&#30563;&#21551;&#21457;&#24335;&#21644;&#26080;&#30417;&#30563;&#28508;&#22312;&#23646;&#24615;&#65292;&#21033;&#29992;&#20135;&#21697;&#19978;&#19979;&#25991;&#33719;&#24471;&#39069;&#22806;&#30340;&#38544;&#21547;&#35821;&#20041;&#20449;&#21495;&#20316;&#20026;&#36741;&#21161;&#30417;&#30563;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#20540;&#19978;&#36229;&#36807;&#20102;&#21508;&#31181;&#22522;&#32447;&#26041;&#27861;12&#20010;&#30334;&#20998;&#28857;&#65292;&#20351;&#29616;&#26377;&#31867;&#22411;&#30340;&#23646;&#24615;&#22823;&#22823;&#25193;&#23637;&#20102;&#26368;&#22810;12&#20493;&#65292;&#24182;&#19988;&#21457;&#29616;&#26032;&#23646;&#24615;&#20540;&#30340;&#33021;&#21147;&#36798;&#21040;&#20102;39&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new task setting for attribute mining on e-commerce products, serving as a practical solution to extract open-world attributes without extensive human intervention. Our supervision comes from a high-quality seed attribute set bootstrapped from existing resources, and we aim to expand the attribute vocabulary of existing seed types, and also to discover any new attribute types automatically. A new dataset is created to support our setting, and our approach Amacer is proposed specifically to tackle the limited supervision. Especially, given that no direct supervision is available for those unseen new attributes, our novel formulation exploits self-supervised heuristic and unsupervised latent attributes, which attains implicit semantic signals as additional supervision by leveraging product context. Experiments suggest that our approach surpasses various baselines by 12 F1, expanding attributes of existing types significantly by up to 12 times, and discovering values from 39%
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18342</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#32534;&#31243;&#20013;&#31070;&#32463;&#20219;&#21153;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#26032;&#30340;&#20869;&#23481;&#65292;&#29983;&#25104;&#24335;&#31070;&#32463;&#27169;&#22411;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#31070;&#32463;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#21487;&#35270;&#21270;&#32534;&#31243;&#29615;&#22659;&#19979;&#32473;&#23450;&#30340;&#35268;&#33539;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#20687; GPT-4 &#36825;&#26679;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#33719;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21512;&#25104;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#36923;&#36753;&#21644;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415; NeurTaskSyn&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#21512;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;NeurTaskSyn &#30001;&#20004;&#20010;&#37096;&#20998;&#26500;&#25104;&#65306;&#31532;&#19968;&#20010;&#37096;&#20998;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#29983;&#25104;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#25351;&#23548;&#24213;&#23618;&#31526;&#21495;&#25191;&#34892;&#24341;&#25806;&#29983;&#25104;&#21487;&#35270;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative neural models hold great promise in enhancing programming education by synthesizing new content for students. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37327;&#21270;&#20998;&#26512;&#20102;&#20027;&#27969;&#23186;&#20307;&#23545;ChatGPT&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#25253;&#36947;&#36235;&#21183;&#21644;&#24773;&#24863;&#24577;&#24230;&#65292;&#21457;&#29616;&#20154;&#20204;&#26222;&#36941;&#23545;&#20854;&#25345;&#31215;&#26497;&#24577;&#24230;&#12290;&#28982;&#32780;&#65292;&#20027;&#39064;&#30340;&#35789;&#39057;&#20998;&#26512;&#26174;&#31034;&#65292;&#22823;&#22411;&#31185;&#25216;&#38382;&#39064;&#21644;&#34892;&#20026;&#32773;&#24471;&#21040;&#20102;&#39640;&#24230;&#20851;&#27880;&#65292;&#32780;&#23601;&#19994;&#12289;&#22810;&#26679;&#24615;&#12289;&#20262;&#29702;&#12289;&#29256;&#26435;&#12289;&#24615;&#21035;&#21644;&#22899;&#24615;&#31561;&#20027;&#39064;&#21017;&#34920;&#29616;&#19981;&#36275;&#25110;&#23436;&#20840;&#32570;&#22833;&#12290;&#26412;&#25991;&#21628;&#21505;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38656;&#35201;&#26356;&#21152;&#22810;&#20803;&#21644;&#32454;&#33268;&#30340;&#23186;&#20307;&#21457;&#35328;&#12290;</title><link>http://arxiv.org/abs/2305.18340</link><description>&lt;p&gt;
&#22312;&#20027;&#27969;&#23186;&#20307;&#19978;&#32472;&#21046;ChatGPT&#30340;&#22320;&#22270;&#65306;&#24773;&#24863;&#20998;&#26512;&#21644;&#35789;&#39057;&#20998;&#26512;&#30340;&#26089;&#26399;&#37327;&#21270;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mapping ChatGPT in Mainstream Media: Early Quantitative Insights through Sentiment Analysis and Word Frequency Analysis. (arXiv:2305.18340v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#20998;&#26512;&#20102;&#20027;&#27969;&#23186;&#20307;&#23545;ChatGPT&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#25253;&#36947;&#36235;&#21183;&#21644;&#24773;&#24863;&#24577;&#24230;&#65292;&#21457;&#29616;&#20154;&#20204;&#26222;&#36941;&#23545;&#20854;&#25345;&#31215;&#26497;&#24577;&#24230;&#12290;&#28982;&#32780;&#65292;&#20027;&#39064;&#30340;&#35789;&#39057;&#20998;&#26512;&#26174;&#31034;&#65292;&#22823;&#22411;&#31185;&#25216;&#38382;&#39064;&#21644;&#34892;&#20026;&#32773;&#24471;&#21040;&#20102;&#39640;&#24230;&#20851;&#27880;&#65292;&#32780;&#23601;&#19994;&#12289;&#22810;&#26679;&#24615;&#12289;&#20262;&#29702;&#12289;&#29256;&#26435;&#12289;&#24615;&#21035;&#21644;&#22899;&#24615;&#31561;&#20027;&#39064;&#21017;&#34920;&#29616;&#19981;&#36275;&#25110;&#23436;&#20840;&#32570;&#22833;&#12290;&#26412;&#25991;&#21628;&#21505;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38656;&#35201;&#26356;&#21152;&#22810;&#20803;&#21644;&#32454;&#33268;&#30340;&#23186;&#20307;&#21457;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20854;&#29992;&#25143;&#33719;&#21462;&#21644;&#26222;&#21450;&#24615;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#20276;&#38543;&#30528;&#24191;&#27867;&#30340;&#20027;&#27969;&#23186;&#20307;&#25253;&#36947;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#19982;ChatGPT&#21644;&#20154;&#24037;&#26234;&#33021;&#20027;&#39064;&#30456;&#20851;&#30340;10,902&#26465;&#20027;&#27969;&#26032;&#38395;&#26631;&#39064;&#35821;&#26009;&#36827;&#34892;&#25991;&#26412;&#25366;&#25496;&#21644;NLP&#26041;&#27861;&#30340;&#23450;&#37327;&#25968;&#25454;&#20998;&#26512;&#65292;&#21576;&#29616;&#20986;&#26089;&#26399;&#36235;&#21183;&#21644;&#24773;&#24863;&#30340;&#21457;&#29616;&#12290;&#24773;&#24863;&#20998;&#26512;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#21644;&#20154;&#24037;&#26234;&#33021;&#22312;&#20027;&#27969;&#23186;&#20307;&#20013;&#30340;&#22909;&#24863;&#24230;&#39640;&#20110;&#21453;&#24863;&#24230;&#12290;&#20851;&#20110;&#35789;&#39057;&#32467;&#26524;&#65292;65%&#20197;&#19978;&#30340;&#39640;&#39057;&#35789;&#38598;&#20013;&#22312;&#22823;&#22411;&#31185;&#25216;&#38382;&#39064;&#21644;&#34892;&#20026;&#32773;&#19978;&#65292;&#32780;&#23601;&#19994;&#12289;&#22810;&#26679;&#24615;&#12289;&#20262;&#29702;&#12289;&#29256;&#26435;&#12289;&#24615;&#21035;&#21644;&#22899;&#24615;&#31561;&#20027;&#39064;&#21017;&#34920;&#29616;&#19981;&#36275;&#25110;&#23436;&#20840;&#32570;&#22833;&#65292;&#20165;&#21344;&#24635;&#35821;&#26009;&#30340;6%&#12290;&#26412;&#25991;&#26159;&#23545;&#20027;&#27969;&#23186;&#20307;&#25253;&#36947;ChatGPT&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26435;&#21147;&#32467;&#26500;&#36827;&#34892;&#30340;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#24182;&#24378;&#35843;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38656;&#35201;&#26356;&#21152;&#22810;&#20803;&#21644;&#32454;&#33268;&#30340;&#23186;&#20307;&#21457;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth in user acquisition and popularity of ChatGPT, an artificial intelligence(AI) powered chatbot, was accompanied by widespread mainstream media coverage. This article presents a quantitative data analysis of the early trends and sentiments revealed by conducting text mining and NLP methods onto a corpus of 10,902 mainstream news headlines related to the subject of ChatGPT and artificial intelligence, from the launch of ChatGPT in November 2022 to March 2023. The findings revealed in sentiment analysis, ChatGPT and artificial intelligence, were perceived more positively than negatively in the mainstream media. In regards to word frequency results, over sixty-five percent of the top frequency words were focused on Big Tech issues and actors while topics such as jobs, diversity, ethics, copyright, gender and women were poorly represented or completely absent and only accounted for six percent of the total corpus. This article is a critical analysis into the power stru
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#23433;&#20840;&#19982;&#38544;&#31169;&#23041;&#32961;&#12289;&#29616;&#29366;&#21644;&#26410;&#26469;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.18339</link><description>&lt;p&gt;
ChatGPT&#65306;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#25361;&#25112;&#19982;&#35299;&#20915;&#26041;&#26696;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions. (arXiv:2305.18339v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#23433;&#20840;&#19982;&#38544;&#31169;&#23041;&#32961;&#12289;&#29616;&#29366;&#21644;&#26410;&#26469;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#27604;&#22914;ChatGPT&#30340;&#26222;&#21450;&#20351;&#29992;&#65292;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#65292;&#27491;&#22312;&#24341;&#39046;&#20869;&#23481;&#21019;&#20316;&#21644;&#30693;&#35782;&#34920;&#31034;&#26041;&#24335;&#23454;&#29616;&#33539;&#24335;&#36716;&#21464;&#12290;AIGC&#21033;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;AI&#31639;&#27861;&#26469;&#36741;&#21161;&#25110;&#26367;&#20195;&#20154;&#31867;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25552;&#31034;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#26356;&#20302;&#30340;&#25104;&#26412;&#21019;&#24314;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#21644;&#31867;&#20154;&#30340;&#20869;&#23481;&#12290;&#23613;&#31649;&#22312;AIGC&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23433;&#20840;&#12289;&#38544;&#31169;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#25361;&#25112;&#20173;&#38656;&#35299;&#20915;&#12290;&#26412;&#25991;&#28145;&#20837;&#35843;&#26597;&#20102;AIGC&#33539;&#24335;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#12289;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#35752;&#20102;AIGC&#30340;&#25216;&#26415;&#23454;&#29616;&#12289;&#24635;&#20307;&#26550;&#26500;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24037;&#20316;&#27169;&#24335;&#21644;&#20851;&#38190;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#38024;&#23545;AIGC&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#20998;&#31867;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;GPT&#21644;AIGC&#25216;&#26415;&#30340;&#20262;&#29702;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#24050;&#30830;&#23450;&#30340;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;AIGC&#39046;&#22495;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#21457;&#23637;AIGC&#30340;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of large artificial intelligence (AI) models such as ChatGPT, AI-generated content (AIGC) has garnered increasing attention and is leading a paradigm shift in content creation and knowledge representation. AIGC uses generative large AI algorithms to assist or replace humans in creating massive, high-quality, and human-like content at a faster pace and lower cost, based on user-provided prompts. Despite the recent significant progress in AIGC, security, privacy, ethical, and legal challenges still need to be addressed. This paper presents an in-depth survey of working principles, security and privacy threats, state-of-the-art solutions, and future challenges of the AIGC paradigm. Specifically, we first explore the enabling technologies, general architecture of AIGC, and discuss its working modes and key characteristics. Then, we investigate the taxonomy of security and privacy threats to AIGC and highlight the ethical and societal implications of GPT and AIGC tec
&lt;/p&gt;</description></item><item><title>#REVAL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#32771;&#34385;&#25512;&#33616;&#21644;&#23454;&#38469;hashtag&#20043;&#38388;&#35821;&#20041;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;#REVAL&#22312;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.18330</link><description>&lt;p&gt;
#REVAL&#65306;&#19968;&#31181;&#29992;&#20110;hashtag&#25512;&#33616;&#30340;&#35821;&#20041;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
#REVAL: a semantic evaluation framework for hashtag recommendation. (arXiv:2305.18330v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18330
&lt;/p&gt;
&lt;p&gt;
#REVAL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#32771;&#34385;&#25512;&#33616;&#21644;&#23454;&#38469;hashtag&#20043;&#38388;&#35821;&#20041;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;#REVAL&#22312;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#31995;&#32479;&#20013;&#65292;&#33258;&#21160;&#35780;&#20272;hashtag&#25512;&#33616;&#27169;&#22411;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26159;&#39318;&#20808;&#27604;&#36739;&#31639;&#27861;&#25512;&#33616;&#30340;hashtag&#19982;&#23454;&#38469;&#30340;hashtag&#30340;&#31934;&#30830;&#23545;&#24212;&#20851;&#31995;&#65292;&#28982;&#21518;&#20351;&#29992;&#31934;&#30830;&#21305;&#37197;&#30340;&#25968;&#37327;&#35745;&#31639;&#21629;&#20013;&#29575;&#12289;&#21629;&#20013;&#27604;&#29575;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#25110;F1&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35780;&#20272;&#26041;&#24335;&#24573;&#30053;&#20102;&#25512;&#33616;&#21644;&#23454;&#38469;hashtag&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#35780;&#20272;&#26694;&#26550;#REval&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#31216;&#20026;BERTag&#30340;&#20869;&#37096;&#27169;&#22359;&#65292;&#21487;&#33258;&#21160;&#23398;&#20064;hashtag&#23884;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;#REval-hit-ratio&#24230;&#37327;&#26631;&#20934;&#65292;&#30740;&#31350;&#20102;#REval&#26694;&#26550;&#22312;&#19981;&#21516;&#30340;&#35789;&#23884;&#20837;&#26041;&#27861;&#21644;&#25512;&#33616;&#20013;&#30340;&#21516;&#20041;&#35789;&#21644;hashtag&#25968;&#37327;&#19979;&#30340;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#25429;&#25417;&#25512;&#33616;&#21644;&#23454;&#38469;hashtag&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic evaluation of hashtag recommendation models is a fundamental task in many online social network systems. In the traditional evaluation method, the recommended hashtags from an algorithm are firstly compared with the ground truth hashtags for exact correspondences. The number of exact matches is then used to calculate the hit rate, hit ratio, precision, recall, or F1-score. This way of evaluating hashtag similarities is inadequate as it ignores the semantic correlation between the recommended and ground truth hashtags. To tackle this problem, we propose a novel semantic evaluation framework for hashtag recommendation, called #REval. This framework includes an internal module referred to as BERTag, which automatically learns the hashtag embeddings. We investigate on how the #REval framework performs under different word embedding methods and different numbers of synonyms and hashtags in the recommendation using our proposed #REval-hit-ratio measure. Our experiments of the propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#27491;&#21017;&#34920;&#36798;&#24335;&#27169;&#24335;&#20316;&#20026;&#39046;&#22495;&#30693;&#35782;&#29305;&#24449;&#19982;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#19968;&#36215;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#23454;&#39564;&#65292;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#19979;&#28216;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.18324</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;&#22686;&#24378;&#30340;&#39046;&#22495;&#36801;&#31227;&#20027;&#39064;&#20998;&#31867;&#65306;&#37329;&#34701;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Regex-augmented Domain Transfer Topic Classification based on a Pre-trained Language Model: An application in Financial Domain. (arXiv:2305.18324v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#27491;&#21017;&#34920;&#36798;&#24335;&#27169;&#24335;&#20316;&#20026;&#39046;&#22495;&#30693;&#35782;&#29305;&#24449;&#19982;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#19968;&#36215;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#23454;&#39564;&#65292;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#19979;&#28216;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#39069;&#22806;&#30340;&#23618;&#36827;&#34892;&#24494;&#35843;&#12290;&#20294;&#24403;&#19979;&#28216;&#39046;&#22495;&#26159;&#19968;&#20010;&#19987;&#19994;&#39046;&#22495;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#36890;&#29992;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#25928;&#26524;&#19981;&#20339;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#27169;&#24335;&#20316;&#20026;&#39046;&#22495;&#30693;&#35782;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#29983;&#20135;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20165;&#22312;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#19978;&#36827;&#34892;&#24494;&#35843;&#30456;&#27604;&#65292;&#36825;&#31181;&#24494;&#35843;&#26041;&#27861;&#25913;&#21892;&#20102;&#19979;&#28216;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#27604;&#31616;&#21333;&#30340;&#32447;&#24615;&#23618;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common way to use large pre-trained language models for downstream tasks is to fine tune them using additional layers. This may not work well if downstream domain is a specialized domain whereas the large language model has been pre-trained on a generic corpus. In this paper, we discuss the use of regular expression patterns employed as features for domain knowledge during the process of fine tuning, in addition to domain specific text. Our experiments on real scenario production data show that this method of fine tuning improves the downstream text classification tasks as compared to fine tuning only on domain specific text. We also show that the use of attention network for fine tuning improves results compared to simple linear layers.
&lt;/p&gt;</description></item><item><title>ReWOO&#26159;&#19968;&#31181;&#23558;&#25512;&#29702;&#36807;&#31243;&#19982;&#22806;&#37096;&#35266;&#23519;&#20998;&#31163;&#30340;&#27169;&#22359;&#21270;&#33539;&#24335;&#65292;&#20174;&#32780;&#21487;&#20197;&#20943;&#23569;&#26631;&#35760;&#28040;&#32791;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18323</link><description>&lt;p&gt;
ReWOO&#65306;&#23558;&#25512;&#29702;&#19982;&#35266;&#23519;&#20998;&#31163;&#65292;&#23454;&#29616;&#39640;&#25928;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models. (arXiv:2305.18323v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18323
&lt;/p&gt;
&lt;p&gt;
ReWOO&#26159;&#19968;&#31181;&#23558;&#25512;&#29702;&#36807;&#31243;&#19982;&#22806;&#37096;&#35266;&#23519;&#20998;&#31163;&#30340;&#27169;&#22359;&#21270;&#33539;&#24335;&#65292;&#20174;&#32780;&#21487;&#20197;&#20943;&#23569;&#26631;&#35760;&#28040;&#32791;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#20801;&#35768;&#30693;&#35782;&#26816;&#32034;&#21644;&#34892;&#21160;&#25191;&#34892;&#30340;&#24037;&#20855;&#30456;&#32467;&#21512;&#12290;&#29616;&#26377;&#30340;ALM&#31995;&#32479;&#20197;&#20132;&#38169;&#26041;&#24335;&#35302;&#21457;LLM&#24605;&#32771;&#36807;&#31243;&#65292;&#21516;&#26102;&#20174;&#36825;&#20123;&#24037;&#20855;&#20013;&#25552;&#21462;&#35266;&#23519;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#33539;&#24335;ReWOO&#65288;Without Observation Reasoning&#65289;&#65292;&#23558;&#25512;&#29702;&#36807;&#31243;&#19982;&#22806;&#37096;&#35266;&#23519;&#20998;&#31163;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#26631;&#35760;&#28040;&#32791;&#12290;&#22312;&#20845;&#20010;&#20844;&#20849;NLP&#22522;&#20934;&#27979;&#35797;&#21644;&#19968;&#20010;&#31574;&#21010;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology.
&lt;/p&gt;</description></item><item><title>REFinD&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#22522;&#20110;&#37329;&#34701;&#25991;&#26723;&#29983;&#25104;&#30340;&#20851;&#31995;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#22312;&#37329;&#34701;&#39046;&#22495;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#30340;&#29305;&#23450;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.18322</link><description>&lt;p&gt;
REFinD: &#37329;&#34701;&#39046;&#22495;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
REFinD: Relation Extraction Financial Dataset. (arXiv:2305.18322v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18322
&lt;/p&gt;
&lt;p&gt;
REFinD&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#22522;&#20110;&#37329;&#34701;&#25991;&#26723;&#29983;&#25104;&#30340;&#20851;&#31995;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#22312;&#37329;&#34701;&#39046;&#22495;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#30340;&#29305;&#23450;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#20102;&#35768;&#22810;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#25628;&#32034;&#12289;&#38382;&#31572;&#21644;&#25991;&#26412;&#34164;&#21547;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26410;&#33021;&#25429;&#25417;&#21040;&#37329;&#34701;&#39046;&#22495;&#30340;&#29305;&#23450;&#25361;&#25112;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#12289;&#22522;&#20110;&#32593;&#32476;&#30340;&#25991;&#26412;&#21644;&#26032;&#38395;&#25991;&#31456;&#31561;&#19968;&#33324;&#30693;&#35782;&#26469;&#28304;&#32534;&#21046;&#65292;&#38459;&#30861;&#20102;&#22312;&#37329;&#34701;&#19990;&#30028;&#20013;&#30340;&#23454;&#38469;&#36827;&#23637;&#21644;&#37319;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REFinD&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#22522;&#20110;&#37329;&#34701;&#25991;&#26723;&#29983;&#25104;&#30340;&#20851;&#31995;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;8&#31181;&#31867;&#22411;&#30340;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;22&#20010;&#20851;&#31995;&#21644;&#32422;29K&#20010;&#23454;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19982;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#20316;&#20026;RE&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#24182;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#23383;&#25512;&#29702;&#12289;&#20851;&#31995;&#21644;&#26041;&#21521;&#27169;&#31946;&#31561;&#26041;&#38754;&#37117;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
A number of datasets for Relation Extraction (RE) have been created to aide downstream tasks such as information retrieval, semantic search, question answering and textual entailment. However, these datasets fail to capture financial-domain specific challenges since most of these datasets are compiled using general knowledge sources such as Wikipedia, web-based text and news articles, hindering real-life progress and adoption within the financial world. To address this limitation, we propose REFinD, the first large-scale annotated dataset of relations, with $\sim$29K instances and 22 relations amongst 8 types of entity pairs, generated entirely over financial documents. We also provide an empirical evaluation with various state-of-the-art models as benchmarks for the RE task and highlight the challenges posed by our dataset. We observed that various state-of-the-art deep learning models struggle with numeric inference, relational and directional ambiguity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34892;&#20026;forma mentis&#32593;&#32476;(BFMNs)&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;(GPT-3&#65292;Chat-GPT&#21644;GPT-4)&#23545;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#30340;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#20542;&#21521;&#20110;&#23558;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#35270;&#20026;&#22256;&#38590;&#12289;&#32039;&#24352;&#21644;&#24341;&#36215;&#28966;&#34385;&#12290;</title><link>http://arxiv.org/abs/2305.18320</link><description>&lt;p&gt;
&#35748;&#30693;&#32593;&#32476;&#31185;&#23398;&#25581;&#31034;GPT-3&#12289;ChatGPT&#21644;GPT-4&#20013;&#30340;&#20559;&#35265;&#65306;&#21453;&#26144;&#39640;&#20013;&#23398;&#29983;&#25968;&#23398;&#28966;&#34385;
&lt;/p&gt;
&lt;p&gt;
Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4 mirroring math anxiety in high-school students. (arXiv:2305.18320v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34892;&#20026;forma mentis&#32593;&#32476;(BFMNs)&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;(GPT-3&#65292;Chat-GPT&#21644;GPT-4)&#23545;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#30340;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#20542;&#21521;&#20110;&#23558;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#35270;&#20026;&#22256;&#38590;&#12289;&#32039;&#24352;&#21644;&#24341;&#36215;&#28966;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#36880;&#28176;&#34701;&#20837;&#25105;&#20204;&#30340;&#29983;&#27963;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#23427;&#20204;&#36755;&#20986;&#20013;&#30340;&#20559;&#35265;&#26159;&#24456;&#37325;&#35201;&#30340;&#65292;&#20197;&#36991;&#20813;&#25345;&#32493;&#27969;&#20256;&#26377;&#23475;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;&#36825;&#31181;&#25361;&#25112;&#38656;&#35201;&#24320;&#21457;&#26032;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#26469;&#37327;&#21270;&#24773;&#24863;&#21644;&#35821;&#20041;&#20559;&#24046;&#65292;&#21516;&#26102;&#38125;&#35760;LLMs&#26159;&#21453;&#26144;&#31038;&#20250;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#35266;&#28857;&#21644;&#20542;&#21521;&#30340;&#24515;&#29702;&#31038;&#20250;&#38236;&#23376;&#12290;&#20854;&#20013;&#19968;&#31181;&#26377;&#23475;&#30340;&#36127;&#38754;&#24433;&#21709;&#26159;&#38738;&#23569;&#24180;&#20840;&#29699;&#26222;&#36941;&#23384;&#22312;&#30340;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#30340;&#28966;&#34385;&#29616;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36816;&#29992;&#32593;&#32476;&#31185;&#23398;&#21644;&#35748;&#30693;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#34892;&#20026;forma mentis&#32593;&#32476;(BFMNs)&#65292;&#30740;&#31350;&#20102;&#20808;&#36827;&#30340;LLMs&#65292;&#21363;GPT-3&#12289;Chat-GPT&#21644;GPT-4&#65292;&#25552;&#20379;&#30340;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#20197;&#21450;&#36825;&#20123;LLMs&#22914;&#20309;&#23558;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#19982;&#20854;&#20182;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;&#25968;&#23398;&#21644;STEM&#30456;&#20851;&#30340;&#25552;&#31034;&#25506;&#27979;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#33719;&#21462;&#30340;&#25968;&#25454;&#65292;&#21457;&#29616;GPT-3&#12289;Chat-GPT&#21644;GPT-4&#37117;&#20542;&#21521;&#20110;&#23558;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#35270;&#20026;&#22256;&#38590;&#12289;&#32039;&#24352;&#21644;&#24341;&#36215;&#28966;&#34385;&#65292;&#21453;&#26144;&#20102;&#39640;&#20013;&#23398;&#29983;&#25968;&#23398;&#28966;&#34385;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#32771;&#34385;LLMs&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;&#21033;&#29992;&#32593;&#32476;&#31185;&#23398;&#21644;&#35748;&#30693;&#24515;&#29702;&#23398;&#24320;&#21457;&#37327;&#21270;&#36825;&#20123;&#20559;&#35265;&#30340;&#26032;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are becoming increasingly integrated into our lives. Hence, it is important to understand the biases present in their outputs in order to avoid perpetuating harmful stereotypes, which originate in our own flawed ways of thinking. This challenge requires developing new benchmarks and methods for quantifying affective and semantic bias, keeping in mind that LLMs act as psycho-social mirrors that reflect the views and tendencies that are prevalent in society. One such tendency that has harmful negative effects is the global phenomenon of anxiety toward math and STEM subjects. Here, we investigate perceptions of math and STEM fields provided by cutting-edge language models, namely GPT-3, Chat-GPT, and GPT-4, by applying an approach from network science and cognitive psychology. Specifically, we use behavioral forma mentis networks (BFMNs) to understand how these LLMs frame math and STEM disciplines in relation to other concepts. We use data obtained by probing the thr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;BERT&#27169;&#22411;&#65292;&#23545;&#21270;&#23398;&#25968;&#25454;&#24211;&#20013;&#25688;&#35201;&#32451;&#20064;&#30340;&#31572;&#26696;&#32467;&#26500;&#36827;&#34892;&#21453;&#39304;&#65292;&#35813;&#27169;&#22411;&#22312;&#23398;&#29983;&#25552;&#20132;&#30340;&#21477;&#23376;&#20013;&#23558;&#20854;&#24402;&#20026;&#19977;&#31867;&#65292;&#21363;&#32972;&#26223;&#12289;&#25216;&#26415;&#21644;&#35266;&#23519;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#21270;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2305.18319</link><description>&lt;p&gt;
&#21270;&#23398;&#25968;&#25454;&#24211;&#21644;&#25688;&#35201;&#32451;&#20064;&#30340;&#33258;&#21160;&#29983;&#25104;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Automated Feedback Generation for a Chemistry Database and Abstracting Exercise. (arXiv:2305.18319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;BERT&#27169;&#22411;&#65292;&#23545;&#21270;&#23398;&#25968;&#25454;&#24211;&#20013;&#25688;&#35201;&#32451;&#20064;&#30340;&#31572;&#26696;&#32467;&#26500;&#36827;&#34892;&#21453;&#39304;&#65292;&#35813;&#27169;&#22411;&#22312;&#23398;&#29983;&#25552;&#20132;&#30340;&#21477;&#23376;&#20013;&#23558;&#20854;&#24402;&#20026;&#19977;&#31867;&#65292;&#21363;&#32972;&#26223;&#12289;&#25216;&#26415;&#21644;&#35266;&#23519;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#21270;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#30340;&#21453;&#39304;&#23545;&#20110;&#25945;&#23398;&#21644;&#23398;&#20064;&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#22914;&#20309;&#21033;&#29992;&#29616;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#21464;&#25442;&#22120;&#65288;&#26426;&#22120;&#23398;&#20064;&#65289;&#27169;&#22411;&#65288;BERT&#65289;&#26469;&#23545;&#25688;&#35201;&#32451;&#20064;&#30340;&#31572;&#26696;&#32467;&#26500;&#36827;&#34892;&#21453;&#39304;&#12290;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#65292;&#35201;&#27714;&#23398;&#29983;&#20204;&#20174;&#20986;&#29256;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#19968;&#31687;&#25991;&#31456;&#24182;&#23545;&#20854;&#20869;&#23481;&#36827;&#34892;&#24635;&#32467;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;207&#20010;&#25552;&#20132;&#21697;&#65292;&#24635;&#20849;&#25688;&#35201;&#20102;21&#31687;&#26469;&#33258;&#20027;&#35201;&#25991;&#29486;&#30340;&#25991;&#31456;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#19968;&#20010;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#65288;&#32422;15,000&#20010;&#26679;&#26412;&#65289;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;80%&#30340;&#24050;&#25552;&#20132;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#36825;&#19968;&#27493;&#39588;&#34987;&#35748;&#20026;&#26159;&#37325;&#35201;&#30340;&#12290;&#23398;&#29983;&#25552;&#20132;&#30340;&#21477;&#23376;&#34987;&#24402;&#20026;&#19977;&#31867;&#8212;&#8212;&#32972;&#26223;&#12289;&#25216;&#26415;&#21644;&#35266;&#23519;&#8212;&#8212;&#36825;&#20351;&#24471;&#21487;&#20197;&#23558;&#27599;&#20010;&#25552;&#20132;&#21697;&#30340;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#27604;&#36739;&#23398;&#29983;&#30340;&#25688;&#35201;&#32467;&#26500;&#20197;&#21450;&#26469;&#33258;PubMed&#25968;&#25454;&#24211;&#30340;&#22823;&#37327;&#25688;&#35201;&#65292;&#21487;&#20197;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Timely feedback is an important part of teaching and learning. Here we describe how a readily available neural network transformer (machine-learning) model (BERT) can be used to give feedback on the structure of the response to an abstracting exercise where students are asked to summarise the contents of a published article after finding it from a publication database. The dataset contained 207 submissions from two consecutive years of the course, summarising a total of 21 different papers from the primary literature. The model was pre-trained using an available dataset (approx. 15,000 samples) and then fine-tuned on 80% of the submitted dataset. This fine tuning was seen to be important. The sentences in the student submissions are characterised into three classes - background, technique and observation - which allows a comparison of how each submission is structured. Comparing the structure of the students' abstract a large collection of those from the PubMed database shows that stud
&lt;/p&gt;</description></item><item><title>CDJUR-BR&#26159;&#19968;&#20221;&#31283;&#20581;&#30340;&#40644;&#37329;&#25910;&#34255;&#65292;&#21253;&#21547;&#24052;&#35199;&#21496;&#27861;&#25991;&#20214;&#20013;&#30340;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#65292;&#35813;&#25910;&#34255;&#28085;&#30422;&#21508;&#31181;&#27861;&#24459;&#31243;&#24207;&#25991;&#20214;&#65292;&#24182;&#26377;&#21161;&#20110;&#35299;&#20915;&#30446;&#21069;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26080;&#27861;&#36731;&#32780;&#26131;&#20030;&#22320;&#35782;&#21035;&#27861;&#24459;&#23454;&#36341;&#25991;&#26412;&#20013;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18315</link><description>&lt;p&gt;
CDJUR-BR -- &#24102;&#26377;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#30340;&#24052;&#35199;&#21496;&#27861;&#25991;&#20214;&#40644;&#37329;&#25910;&#34255;
&lt;/p&gt;
&lt;p&gt;
CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice with Fine-Grained Named Entities. (arXiv:2305.18315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18315
&lt;/p&gt;
&lt;p&gt;
CDJUR-BR&#26159;&#19968;&#20221;&#31283;&#20581;&#30340;&#40644;&#37329;&#25910;&#34255;&#65292;&#21253;&#21547;&#24052;&#35199;&#21496;&#27861;&#25991;&#20214;&#20013;&#30340;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#65292;&#35813;&#25910;&#34255;&#28085;&#30422;&#21508;&#31181;&#27861;&#24459;&#31243;&#24207;&#25991;&#20214;&#65292;&#24182;&#26377;&#21161;&#20110;&#35299;&#20915;&#30446;&#21069;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26080;&#27861;&#36731;&#32780;&#26131;&#20030;&#22320;&#35782;&#21035;&#27861;&#24459;&#23454;&#36341;&#25991;&#26412;&#20013;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22810;&#25968;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#65288;Legal AI&#65289;&#24212;&#29992;&#31243;&#24207;&#32780;&#35328;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27861;&#24459;&#23454;&#36341;&#20013;&#20135;&#29983;&#30340;&#25991;&#26412;&#28041;&#21450;&#21040;&#30340;&#23454;&#20307;&#24182;&#38750;&#24403;&#21069;&#21487;&#29992;&#30340;NER&#36731;&#32780;&#26131;&#20030;&#22320;&#35782;&#21035;&#12290;&#32570;&#20047;&#27861;&#35268;&#12289;&#21028;&#20363;&#12289;&#35777;&#25454;&#12289;&#24809;&#32602;&#12289;&#27861;&#24459;&#31243;&#24207;&#20013;&#20154;&#20204;&#30340;&#35282;&#33394;&#65288;&#27861;&#23448;&#12289;&#24459;&#24072;&#12289;&#21463;&#23475;&#32773;&#12289;&#34987;&#21578;&#12289;&#35777;&#20154;&#65289;&#12289;&#20301;&#32622;&#31867;&#22411;&#65288;&#29359;&#32618;&#22320;&#28857;&#12289;&#34987;&#21578;&#22320;&#22336;&#65289;&#31561;&#30340;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;&#20173;&#38656;&#35201;&#19968;&#20010;&#29992;&#27861;&#24459;&#39046;&#22495;&#30340;&#31934;&#32454;&#23454;&#20307;&#36827;&#34892;&#27880;&#37322;&#30340;&#31283;&#20581;&#30340;&#40644;&#37329;&#25910;&#34255;&#65292;&#28085;&#30422;&#27861;&#24459;&#31243;&#24207;&#30340;&#21508;&#31181;&#25991;&#20214;&#65292;&#20363;&#22914;&#35831;&#24895;&#20070;&#12289;&#35843;&#26597;&#12289;&#25237;&#35785;&#12289;&#20915;&#23450;&#21644;&#21028;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24052;&#35199;&#21496;&#27861;&#40644;&#37329;&#25910;&#34255;&#65288;CDJUR-BR&#65289;&#30340;&#24320;&#21457;&#65292;&#35813;&#25910;&#34255;&#21253;&#21547;&#19968;&#32452;&#30001;&#27861;&#24459;&#25991;&#29486;&#19987;&#23478;&#27880;&#37322;&#30340;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#12290;&#21019;&#24314;CDJUR-BR&#36981;&#24490;&#20102;&#33258;&#24049;&#30340;
&lt;/p&gt;
&lt;p&gt;
A basic task for most Legal Artificial Intelligence (Legal AI) applications is Named Entity Recognition (NER). However, texts produced in the context of legal practice make references to entities that are not trivially recognized by the currently available NERs. There is a lack of categorization of legislation, jurisprudence, evidence, penalties, the roles of people in a legal process (judge, lawyer, victim, defendant, witness), types of locations (crime location, defendant's address), etc. In this sense, there is still a need for a robust golden collection, annotated with fine-grained entities of the legal domain, and which covers various documents of a legal process, such as petitions, inquiries, complaints, decisions and sentences. In this article, we describe the development of the Golden Collection of the Brazilian Judiciary (CDJUR-BR) contemplating a set of fine-grained named entities that have been annotated by experts in legal documents. The creation of CDJUR-BR followed its ow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#25968;&#23383;&#23402;&#29983;&#20307;&#22312;&#20803;&#23431;&#23449;&#20013;&#30340;&#37096;&#32626;&#65292;&#24182;&#20171;&#32461;&#20102;&#36890;&#36807;&#35821;&#20041;&#36890;&#20449;&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;&#20307;&#19982;&#20803;&#23431;&#23449;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#20197;&#21450;&#20854;&#22312;&#26234;&#33021;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#22522;&#26412;&#21407;&#29702;&#21644;&#24615;&#33021;&#20248;&#21270;&#30340;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.18304</link><description>&lt;p&gt;
&#35821;&#20041;&#24863;&#30693;&#25968;&#23383;&#23402;&#29983;&#20307;&#22312;&#20803;&#23431;&#23449;&#20013;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Semantic-aware Digital Twin for Metaverse: A Comprehensive Review. (arXiv:2305.18304v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#25968;&#23383;&#23402;&#29983;&#20307;&#22312;&#20803;&#23431;&#23449;&#20013;&#30340;&#37096;&#32626;&#65292;&#24182;&#20171;&#32461;&#20102;&#36890;&#36807;&#35821;&#20041;&#36890;&#20449;&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;&#20307;&#19982;&#20803;&#23431;&#23449;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#20197;&#21450;&#20854;&#22312;&#26234;&#33021;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#22522;&#26412;&#21407;&#29702;&#21644;&#24615;&#33021;&#20248;&#21270;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#20803;&#23431;&#23449;&#20013;&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#37096;&#32626;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#35821;&#20041;&#24863;&#30693;&#33021;&#21147;&#30340;&#33539;&#24335;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#12289;&#38754;&#21521;&#20219;&#21153;&#30340;&#20449;&#24687;&#25552;&#21462;&#21644;&#20869;&#22312;&#30340;&#26234;&#33021;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26694;&#26550;&#38656;&#35201;&#23558;&#20803;&#23431;&#23449;&#29615;&#22659;&#20013;&#30340;&#25152;&#26377;&#35774;&#22791;&#30452;&#25509;&#36830;&#25509;&#21040;&#35821;&#20041;&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#20449;&#24687;&#30340;&#24544;&#23454;&#35299;&#37322;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#25968;&#23383;&#23402;&#29983;&#20307;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#26234;&#33021;&#24037;&#19994;&#24212;&#29992;&#65292;&#36890;&#36807;&#19982;&#20803;&#23431;&#23449;&#20351;&#33021;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#65292;&#23454;&#29616;&#20102;&#35821;&#20041;&#36890;&#20449;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#20041;&#36890;&#20449;&#12289;&#20803;&#23431;&#23449;&#21644;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#22522;&#30784;&#30693;&#35782;&#65292;&#24182;&#22312;&#24037;&#19994;&#36710;&#38388;&#31649;&#29702;&#24212;&#29992;&#29992;&#20363;&#20013;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#20197;&#36890;&#36807;&#35821;&#20041;&#36890;&#20449;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#20171;&#32461;&#20102;&#36825;&#20123;&#25216;&#26415;&#19982;&#22522;&#26412;&#26550;&#26500;&#30340;&#38598;&#25104;&#20197;&#21450;&#23545;&#26410;&#26469;&#24037;&#19994;&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
To facilitate the deployment of digital twins in Metaverse, the paradigm with semantic awareness has been proposed as a means for enabling accurate and task-oriented information extraction with inherent intelligence. However, this framework requires all devices in the Metaverse environment to be directly linked with the semantic model to enable faithful interpretation of messages. In contrast, this article introduces the digital twin framework, considering a smart industrial application, which enables semantic communication in conjugation with the Metaverse enabling technologies. The fundamentals of this framework are demonstrated on an industrial shopfloor management use case with a digital twin so as to improve its performance through semantic communication. An overview of semantic communication, Metaverse, and digital twins is presented. Integration of these technologies with the basic architecture as well as the impact on future industrial applications is presented. In a nutshell, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#25105;&#35748;&#30693;&#33021;&#21147;&#65292;&#22522;&#20110;&#26080;&#27861;&#22238;&#31572;&#25110;&#19981;&#21487;&#30693;&#38382;&#39064;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;LLM&#22312;&#35748;&#35782;&#33258;&#36523;&#38480;&#21046;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.18153</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#30693;&#36947;&#33258;&#24049;&#25152;&#19981;&#30693;&#36947;&#30340;&#20869;&#23481;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models Know What They Don't Know?. (arXiv:2305.18153v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#25105;&#35748;&#30693;&#33021;&#21147;&#65292;&#22522;&#20110;&#26080;&#27861;&#22238;&#31572;&#25110;&#19981;&#21487;&#30693;&#38382;&#39064;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;LLM&#22312;&#35748;&#35782;&#33258;&#36523;&#38480;&#21046;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#22686;&#24378;&#23427;&#20204;&#22312;&#29616;&#26377;&#30693;&#35782;&#33539;&#30068;&#20869;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#23427;&#20204;&#25317;&#26377;&#24222;&#22823;&#30340;&#30693;&#35782;&#20648;&#22791;&#65292;&#20294;LLM&#20173;&#28982;&#21463;&#21040;&#23427;&#20204;&#33021;&#22815;&#23481;&#32435;&#21644;&#29702;&#35299;&#30340;&#20449;&#24687;&#24635;&#37327;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#26410;&#30693;&#39046;&#22495;&#30340;&#38480;&#21046;&#65292;&#21363;&#25152;&#35859;&#30340;&#33258;&#25105;&#35748;&#30693;&#33021;&#21147;&#65292;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;LLM&#30340;&#33258;&#25105;&#35748;&#30693;&#33021;&#21147;&#65292;&#36890;&#36807;&#35780;&#20272;&#23427;&#20204;&#35782;&#21035;&#26080;&#27861;&#22238;&#31572;&#25110;&#19981;&#21487;&#30693;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#20123;&#27169;&#22411;&#21709;&#24212;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20379;&#23427;&#20204;&#33258;&#25105;&#35748;&#30693;&#30340;&#19968;&#31181;&#26032;&#24230;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;SelfAware&#65292;&#21253;&#25324;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#21450;&#20854;&#21487;&#22238;&#31572;&#23545;&#24212;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#28041;&#21450;20&#20010;LLM&#65288;&#21253;&#25324;GPT-3&#12289;InstructGPT&#21644;LLaMA&#65289;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#35782;&#21035;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#24182;&#35748;&#35782;&#21040;&#33258;&#36523;&#38480;&#21046;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend. Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance. This study aims to evaluate LLMs' self-knowledge by assessing their ability to identify unanswerable or unknowable questions. We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge. We further introduce a unique dataset, SelfAware, consisting of unanswerable questions from five diverse categories and their answerable counterparts. Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intr
&lt;/p&gt;</description></item><item><title>KoSBi&#26159;&#19968;&#20010;&#26032;&#30340;&#31038;&#20250;&#20559;&#35265;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22522;&#20110;&#36807;&#28388;&#30340;&#20013;&#20171;&#22788;&#29702;&#65292;&#21487;&#20197;&#23558;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#31038;&#20250;&#20559;&#24046;&#24179;&#22343;&#38477;&#20302;16.47%p&#65292;&#36866;&#29992;&#20110;&#38889;&#22269;&#35821;&#35328;&#25991;&#21270;&#32972;&#26223;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2305.17701</link><description>&lt;p&gt;
KoSBi: &#19968;&#20221;&#29992;&#20110;&#20943;&#36731;&#31038;&#20250;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30830;&#20445;&#26356;&#23433;&#20840;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992; (arXiv: 2305.17701v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
KoSBi: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Application. (arXiv:2305.17701v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17701
&lt;/p&gt;
&lt;p&gt;
KoSBi&#26159;&#19968;&#20010;&#26032;&#30340;&#31038;&#20250;&#20559;&#35265;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22522;&#20110;&#36807;&#28388;&#30340;&#20013;&#20171;&#22788;&#29702;&#65292;&#21487;&#20197;&#23558;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#31038;&#20250;&#20559;&#24046;&#24179;&#22343;&#38477;&#20302;16.47%p&#65292;&#36866;&#29992;&#20110;&#38889;&#22269;&#35821;&#35328;&#25991;&#21270;&#32972;&#26223;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#19981;&#20165;&#33021;&#22815;&#23398;&#20064;&#33258;&#28982;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#32780;&#19988;&#36824;&#33021;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#23398;&#20064;&#23545;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#36825;&#22312;&#37096;&#32626;&#22522;&#20110; LLM &#30340;&#24212;&#29992;&#31243;&#24207;&#26102;&#26500;&#25104;&#20102;&#20851;&#38190;&#39118;&#38505;&#12290;&#30001;&#20110;&#35821;&#35328;&#21644;&#25991;&#21270;&#30340;&#24046;&#24322;&#26174;&#33879;&#24433;&#21709;&#20559;&#35265;&#21644;&#30446;&#26631;&#20154;&#21475;&#32676;&#20307;&#65292;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#21644;&#36164;&#28304;&#22312;&#38889;&#22269;&#24182;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20221;&#26032;&#30340;&#31038;&#20250;&#20559;&#35265;&#25968;&#25454;&#38598; KoSBi&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547; 72 &#20010;&#20154;&#21475;&#32676;&#20307;&#22312; 15 &#20010;&#31867;&#21035;&#20013;&#30340; 34k &#31867;&#35821;&#22659;&#21644;&#21477;&#23376;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#22522;&#20110;&#36807;&#28388;&#30340;&#20013;&#20171;&#22788;&#29702;&#65292;&#21487;&#20197;&#23558;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#31038;&#20250;&#20559;&#24046;&#24179;&#22343;&#38477;&#20302; 16.47%p&#65292;&#23545; HyperCLOVA&#65288;30B &#21644; 82B&#65289;&#21644; GPT-3 &#22343;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) learn not only natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications. Existing research and resources are not readily applicable in South Korea due to the differences in language and culture, both of which significantly affect the biases and targeted demographic groups. This limitation requires localized social bias datasets to ensure the safe and effective deployment of LLMs. To this end, we present KO SB I, a new social bias dataset of 34k pairs of contexts and sentences in Korean covering 72 demographic groups in 15 categories. We find that through filtering-based moderation, social biases in generated content can be reduced by 16.47%p on average for HyperCLOVA (30B and 82B), and GPT-3.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20511;&#37492;&#23401;&#23376;&#35821;&#35328;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#26032;&#27010;&#24565;&#30340;&#23545;&#40784;&#26469;&#25913;&#36827;&#22270;&#20687;&#23383;&#24149;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.17540</link><description>&lt;p&gt;
&#20174;&#23401;&#23376;&#36523;&#19978;&#23398;&#20064;&#65306;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#25913;&#36827;&#22270;&#20687;&#23383;&#24149;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Learning from Children: Improving Image-Caption Pretraining via Curriculum. (arXiv:2305.17540v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20511;&#37492;&#23401;&#23376;&#35821;&#35328;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#26032;&#27010;&#24565;&#30340;&#23545;&#40784;&#26469;&#25913;&#36827;&#22270;&#20687;&#23383;&#24149;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#39044;&#35757;&#32451;&#24050;&#32463;&#34987;&#24191;&#27867;&#29992;&#20110;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#21644;&#29289;&#20307;&#26816;&#27979;&#31561;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#65292;&#20294;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#65292;&#38656;&#35201;&#23558;&#23383;&#24149;&#20013;&#30340;&#22810;&#20010;&#27010;&#24565;&#19982;&#22270;&#29255;&#20013;&#30340;&#22810;&#20010;&#23545;&#35937;&#23545;&#40784;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#32773;&#20204;&#20174;&#26368;&#20248;&#31168;&#30340;&#23398;&#20064;&#32773;&#8212;&#8212;&#23401;&#23376;&#20204;&#36523;&#19978;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#65292;&#26368;&#24320;&#22987;&#20351;&#29992;&#26131;&#20110;&#23545;&#40784;&#19968;&#32452;&#27010;&#24565;&#30340;&#22270;&#20687;&#23383;&#24149;&#37197;&#23545;&#36827;&#34892;&#23398;&#20064;&#65292;&#28982;&#21518;&#36880;&#28176;&#22686;&#21152;&#38590;&#24230;&#65292;&#27599;&#20010;&#26032;&#38454;&#27573;&#37117;&#36880;&#28176;&#22686;&#21152;&#19968;&#20010;&#27010;&#24565;&#65292;&#21033;&#29992;&#27599;&#20010;&#23398;&#20064;&#38454;&#27573;&#33719;&#21462;&#30340;&#30693;&#35782;&#22312;&#21518;&#32493;&#38454;&#27573;&#20013;&#24110;&#21161;&#23545;&#40784;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;-&#23545;&#35937;&#23545;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#23398;&#20064;&#31574;&#30053;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#21407;&#22987;&#30340;&#22270;&#20687;&#23383;&#24149;&#35757;&#32451;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-caption pretraining has been quite successfully used for downstream vision tasks like zero-shot image classification and object detection. However, image-caption pretraining is still a hard problem -- it requires multiple concepts (nouns) from captions to be aligned to several objects in images. To tackle this problem, we go to the roots -- the best learner, children. We take inspiration from cognitive science studies dealing with children's language learning to propose a curriculum learning framework. The learning begins with easy-to-align image caption pairs containing one concept per caption. The difficulty is progressively increased with each new phase by adding one more concept per caption. Correspondingly, the knowledge acquired in each learning phase is utilized in subsequent phases to effectively constrain the learning problem to aligning one new concept-object pair in each phase. We show that this learning strategy improves over vanilla image-caption training in various 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#26816;&#27979;&#22840;&#24352;&#21644;&#38544;&#21947;&#12290;&#20351;&#29992;&#38544;&#21947;&#26631;&#31614;&#27880;&#37322;&#20102;&#20004;&#20010;&#22840;&#24352;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22840;&#24352;&#26631;&#31614;&#27880;&#37322;&#20102;&#20004;&#20010;&#38544;&#21947;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#26816;&#27979;&#22840;&#24352;&#30340;&#24615;&#33021;&#27604;&#20808;&#21069;&#26041;&#27861;&#36827;&#27493;&#20102;12%&#12290;&#27492;&#22806;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#22810;&#36798;17%&#12290;</title><link>http://arxiv.org/abs/2305.17480</link><description>&lt;p&gt;
&#19968;&#26729;&#22825;&#20316;&#20043;&#21512;&#65306;&#29992;&#20110;&#22840;&#24352;&#21644;&#38544;&#21947;&#26816;&#27979;&#30340;&#22810;&#20219;&#21153;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Match Made in Heaven: A Multi-task Framework for Hyperbole and Metaphor Detection. (arXiv:2305.17480v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#26816;&#27979;&#22840;&#24352;&#21644;&#38544;&#21947;&#12290;&#20351;&#29992;&#38544;&#21947;&#26631;&#31614;&#27880;&#37322;&#20102;&#20004;&#20010;&#22840;&#24352;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22840;&#24352;&#26631;&#31614;&#27880;&#37322;&#20102;&#20004;&#20010;&#38544;&#21947;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#26816;&#27979;&#22840;&#24352;&#30340;&#24615;&#33021;&#27604;&#20808;&#21069;&#26041;&#27861;&#36827;&#27493;&#20102;12%&#12290;&#27492;&#22806;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#22810;&#36798;17%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22840;&#24352;&#21644;&#38544;&#21947;&#22312;&#26085;&#24120;&#20132;&#27969;&#20013;&#24456;&#24120;&#35265;&#65288;&#20363;&#22914;&#65292;&#8220;&#25105;&#38519;&#20837;&#20102;&#40635;&#28902;&#20043;&#20013;&#8221;&#65306;&#40635;&#28902;&#24590;&#20040;&#21487;&#33021;&#26377;&#28145;&#24230;&#65311;&#65289;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#26816;&#27979;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#35805; AI &#35774;&#32622;&#20013;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#26816;&#27979;&#22840;&#24352;&#21644;&#38544;&#21947;&#30340;&#26041;&#27861;&#29420;&#31435;&#22320;&#30740;&#31350;&#20102;&#36825;&#20123;&#35821;&#35328;&#29616;&#35937;&#65292;&#20294;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#20174;&#26410;&#34987;&#35745;&#31639;&#21270;&#25506;&#32034;&#36807;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#26816;&#27979;&#22840;&#24352;&#21644;&#38544;&#21947;&#12290;&#25105;&#20204;&#20551;&#35774;&#38544;&#21947;&#26377;&#21161;&#20110;&#22840;&#24352;&#26816;&#27979;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#20026;&#39564;&#35777;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#20351;&#29992;&#38544;&#21947;&#26631;&#31614;&#27880;&#37322;&#20102;&#20004;&#20010;&#22840;&#24352;&#25968;&#25454;&#38598;-HYPO &#21644; HYPO-L-&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#22840;&#24352;&#26631;&#31614;&#27880;&#37322;&#20102;&#20004;&#20010;&#38544;&#21947;&#25968;&#25454;&#38598;-TroFi &#21644; LCC&#12290;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20808;&#21069;&#30340;&#22840;&#24352;&#26816;&#27979;&#26041;&#27861;&#36827;&#27493;&#20102; 12%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#30456;&#23545;&#20110;&#21333;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#22810;&#36798; 17%&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbole and metaphor are common in day-to-day communication (e.g., "I am in deep trouble": how does trouble have depth?), which makes their detection important, especially in a conversational AI setting. Existing approaches to automatically detect metaphor and hyperbole have studied these language phenomena independently, but their relationship has hardly, if ever, been explored computationally. In this paper, we propose a multi-task deep learning framework to detect hyperbole and metaphor simultaneously. We hypothesize that metaphors help in hyperbole detection, and vice-versa. To test this hypothesis, we annotate two hyperbole datasets- HYPO and HYPO-L- with metaphor labels. Simultaneously, we annotate two metaphor datasets- TroFi and LCC- with hyperbole labels. Experiments using these datasets give an improvement of the state of the art of hyperbole detection by 12%. Additionally, our multi-task learning (MTL) approach shows an improvement of up to 17% over single-task learning (S
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Retrieval Augmentation&#65288;RetA&#65289;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;OpenAI&#30340;GPT-3&#12289;GPT-4&#12289;Bing&#30340;Prometheus&#20197;&#21450;&#23450;&#21046;&#30340;RetA&#27169;&#22411;&#22312;&#22238;&#31572;19&#20010;&#24357;&#28459;&#22823;B&#32454;&#32990;&#28107;&#24052;&#30244;&#65288;DLBCL&#65289;&#30142;&#30149;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;RetA&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.17116</link><description>&lt;p&gt;
&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;GPT-3/4&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model. (arXiv:2305.17116v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Retrieval Augmentation&#65288;RetA&#65289;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;OpenAI&#30340;GPT-3&#12289;GPT-4&#12289;Bing&#30340;Prometheus&#20197;&#21450;&#23450;&#21046;&#30340;RetA&#27169;&#22411;&#22312;&#22238;&#31572;19&#20010;&#24357;&#28459;&#22823;B&#32454;&#32990;&#28107;&#24052;&#30244;&#65288;DLBCL&#65289;&#30142;&#30149;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;RetA&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#24191;&#27867;&#30340;&#35821;&#26009;&#24211;&#25429;&#33719;&#20102;&#22810;&#26679;&#30340;&#27169;&#24335;&#65292;&#20294;&#20063;&#21487;&#33021;&#24341;&#20837;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#32780;&#19987;&#27880;&#30340;&#35821;&#26009;&#24211;&#36890;&#36807;&#20943;&#23569;&#35823;&#23548;&#24615;&#20449;&#24687;&#26469;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#20013;&#30340;Retrieval Augmentation&#65288;RetA&#65289;&#26041;&#27861;&#26159;&#20351;&#29992;&#19987;&#27880;&#35821;&#26009;&#24211;&#35757;&#32451;LLM&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26412;&#25991;&#38024;&#23545;&#24357;&#28459;&#22823;B&#32454;&#32990;&#28107;&#24052;&#30244;&#65288;DLBCL&#65289;&#30142;&#30149;&#25552;&#20986;&#20102;19&#20010;&#38382;&#39064;&#24182;&#27604;&#36739;&#20102;OpenAI&#30340;GPT-3&#12289;GPT-4&#12289;Bing&#30340;Prometheus&#20197;&#21450;&#23450;&#21046;&#30340;RetA&#27169;&#22411;&#22312;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290; 8&#21517;&#29420;&#31435;&#30340;&#35780;&#23457;&#26681;&#25454;&#20934;&#30830;&#24615;&#12289;&#30456;&#20851;&#24615;&#21644;&#21487;&#35835;&#24615;&#65288;&#35780;&#20998;1-3&#65289;&#23545;&#22238;&#31572;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;RetA&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#65288;19&#39033;&#20013;12&#39033;&#33719;&#24471;3&#20998;&#65292;&#24635;&#35745;47&#20998;&#65289;&#21644;&#30456;&#20851;&#24615;&#65288;19&#39033;&#20013;13&#39033;&#65292;50&#20998;&#65289;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#20854;&#27425;&#26159;GPT-4&#65288;19&#39033;&#20013;8&#39033;&#65292;43&#20998;&#65307;11&#39033;&#20013;49&#20998;&#65289;&#12290;GPT-4&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#21487;&#35835;&#24615;&#35780;&#20998;&#65288;19&#39033;&#20013;17&#39033;&#65292;55&#20998;&#65289;&#65292;&#20854;&#27425;&#26159;GPT-3&#65288;19&#39033;&#20013;15&#39033;&#65292;53&#20998;&#65289;&#21644;RetA&#27169;&#22411;&#65288;19&#39033;&#20013;11&#39033;&#65292;47&#20998;&#65289;&#12290;Prometheus&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant advancements in natural language processing (NLP). Broad corpora capture diverse patterns but can introduce irrelevance, while focused corpora enhance reliability by reducing misleading information. Training LLMs on focused corpora poses computational challenges. An alternative approach is to use a retrieval-augmentation (RetA) method tested in a specific domain.  To evaluate LLM performance, OpenAI's GPT-3, GPT-4, Bing's Prometheus, and a custom RetA model were compared using 19 questions on diffuse large B-cell lymphoma (DLBCL) disease. Eight independent reviewers assessed responses based on accuracy, relevance, and readability (rated 1-3).  The RetA model performed best in accuracy (12/19 3-point scores, total=47) and relevance (13/19, 50), followed by GPT-4 (8/19, 43; 11/19, 49). GPT-4 received the highest readability scores (17/19, 55), followed by GPT-3 (15/19, 53) and the RetA model (11/19, 47). Prometheus underperformed in accu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#65288;CMN&#65289;&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAEs&#65289;&#19982;&#19979;&#19968;&#21477;&#39044;&#27979;&#65288;NSP&#65289;&#30446;&#26631;&#30456;&#32467;&#21512;&#65292;&#24182;&#21033;&#29992;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#22312;&#28508;&#31354;&#38388;&#20013;&#24314;&#27169;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#26469;&#40065;&#26834;&#22320;&#35780;&#20272;&#24320;&#25918;&#22495;&#23545;&#35805;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16967</link><description>&lt;p&gt;
&#29992;&#19979;&#19968;&#21477;&#39044;&#27979;&#21644;&#20114;&#20449;&#24687;&#22312;&#28508;&#31354;&#38388;&#20013;&#35780;&#20272;&#24320;&#25918;&#22495;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Evaluating Open-Domain Dialogues in Latent Space with Next Sentence Prediction and Mutual Information. (arXiv:2305.16967v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#65288;CMN&#65289;&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAEs&#65289;&#19982;&#19979;&#19968;&#21477;&#39044;&#27979;&#65288;NSP&#65289;&#30446;&#26631;&#30456;&#32467;&#21512;&#65292;&#24182;&#21033;&#29992;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#22312;&#28508;&#31354;&#38388;&#20013;&#24314;&#27169;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#26469;&#40065;&#26834;&#22320;&#35780;&#20272;&#24320;&#25918;&#22495;&#23545;&#35805;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#23545;&#35805;&#20013;&#30340;&#19968;&#23545;&#22810;&#38382;&#39064;&#20351;&#24471;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#65288;CMN&#65289;&#65292;&#36890;&#36807;&#23558;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAEs&#65289;&#19982;&#19979;&#19968;&#21477;&#39044;&#27979;&#65288;NSP&#65289;&#30446;&#26631;&#30456;&#32467;&#21512;&#65292;&#24182;&#21033;&#29992;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#22312;&#28508;&#31354;&#38388;&#20013;&#24314;&#27169;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#23454;&#29616;&#20102;&#23545;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#40065;&#26834;&#35780;&#20272;&#12290;&#22312;&#20004;&#20010;&#24320;&#25918;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24191;&#27867;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#36234;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#35821;&#20041;&#19978;&#36828;&#31163;&#40644;&#37329;&#21442;&#32771;&#22238;&#31572;&#30340;&#21709;&#24212;&#26102;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The long-standing one-to-many issue of the open-domain dialogues poses significant challenges for automatic evaluation methods, i.e., there may be multiple suitable responses which differ in semantics for a given conversational context. To tackle this challenge, we propose a novel learning-based automatic evaluation metric (CMN), which can robustly evaluate open-domain dialogues by augmenting Conditional Variational Autoencoders (CVAEs) with a Next Sentence Prediction (NSP) objective and employing Mutual Information (MI) to model the semantic similarity of text in the latent space. Experimental results on two open-domain dialogue datasets demonstrate the superiority of our method compared with a wide range of baselines, especially in handling responses which are distant to the golden reference responses in semantics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#25511;&#21046;&#27169;&#22411;&#22823;&#23567;&#12289;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#23545;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#23569;&#26679;&#26412;&#24494;&#35843;&#22312;&#26576;&#20123;&#22797;&#26434;&#30340;&#25512;&#29702;&#21644;&#32452;&#21512;&#24615;&#20219;&#21153;&#20013;&#27604;&#19978;&#19979;&#25991;&#23398;&#20064;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.16938</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#24494;&#35843; vs &#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#20844;&#24179;&#27604;&#36739;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation. (arXiv:2305.16938v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#25511;&#21046;&#27169;&#22411;&#22823;&#23567;&#12289;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#23545;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#23569;&#26679;&#26412;&#24494;&#35843;&#22312;&#26576;&#20123;&#22797;&#26434;&#30340;&#25512;&#29702;&#21644;&#32452;&#21512;&#24615;&#20219;&#21153;&#20013;&#27604;&#19978;&#19979;&#25991;&#23398;&#20064;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#36866;&#24212;&#30340;&#20004;&#31181;&#26367;&#20195;&#31574;&#30053;&#12290;&#36817;&#26399;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#25913;&#21892;&#30340;&#36328;&#22495;&#27867;&#21270;&#33021;&#21147;&#32780;&#22791;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#22823;&#37327;&#35777;&#25454;&#34920;&#26126;&#24494;&#35843;&#27169;&#22411;&#20250;&#21463;&#21040;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20043;&#21069;&#23545;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#27604;&#36739;&#20351;&#29992;&#30340;&#26159;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24494;&#35843;&#27169;&#22411;&#30340;&#36328;&#22495;&#27867;&#21270;&#33021;&#21147;&#26159;&#21542;&#26159;&#24494;&#35843;&#26412;&#36523;&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#36824;&#26159;&#23454;&#39564;&#35774;&#32622;&#30340;&#23616;&#38480;&#24615;&#65311;&#26412;&#25991;&#21033;&#29992;&#27169;&#22411;&#22823;&#23567;&#12289;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#25968;&#37327;&#31561;&#26041;&#38754;&#26469;&#36827;&#34892;&#20844;&#24179;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27867;&#21270;&#27604;&#36739;&#65292;&#28041;&#21450;&#30340;&#27169;&#22411;&#35268;&#27169;&#20174;125M&#21040;30B&#19981;&#31561;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24494;&#35843;&#27169;&#22411;&#23454;&#38469;&#19978;&#33021;&#22815;&#24456;&#22909;&#22320;&#36328;&#22495;&#27867;&#21270;&#65292;&#21457;&#29616;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#26576;&#20123;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#21644;&#32452;&#21512;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#23569;&#26679;&#26412;&#24494;&#35843;&#30340;&#25928;&#26524;&#20248;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows that fine-tuned models pick up on spurious correlations. Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup. In this paper, we compare the generalization of few-shot fine-tuning and in-context learning to challenge datasets, while controlling for the models used, the number of examples, and the number of parameters, ranging from 125M to 30B. Our results show that fine-tuned language models can in fact generalize well out-of-domain. We find that both approaches
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;GenQ&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#29031;&#39038;&#32773;&#21644;&#23401;&#23376;&#20043;&#38388;&#30340;&#23545;&#35805;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#25991;&#21270;&#32972;&#26223;&#21644;&#35821;&#22659;&#21464;&#21270;&#20197;&#25552;&#39640;&#31995;&#32479;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16809</link><description>&lt;p&gt;
GenQ&#65306;&#33258;&#21160;&#21270;&#38382;&#31572;&#29983;&#25104;&#22120;&#20197;&#24110;&#21161;&#29031;&#39038;&#32773;&#19982;&#23401;&#23376;&#20849;&#35835;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
GenQ: Automated Question Generation to Support Caregivers While Reading Stories with Children. (arXiv:2305.16809v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;GenQ&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#29031;&#39038;&#32773;&#21644;&#23401;&#23376;&#20043;&#38388;&#30340;&#23545;&#35805;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#25991;&#21270;&#32972;&#26223;&#21644;&#35821;&#22659;&#21464;&#21270;&#20197;&#25552;&#39640;&#31995;&#32479;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29031;&#39038;&#32773;&#35810;&#38382;&#24320;&#25918;&#24335;&#38382;&#39064;&#20197;&#28608;&#21457;&#19982;&#23401;&#23376;&#30340;&#23545;&#35805;&#26102;&#65292;&#21487;&#20197;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;&#34429;&#28982;&#26377;&#21033;&#29992;&#25216;&#26415;&#24037;&#20855;&#26469;&#25903;&#25345;&#36825;&#20010;&#36807;&#31243;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#8221;&#30340;&#31354;&#38388;&#65292;&#20294;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#29616;&#26377;&#30340;&#29983;&#25104;&#31867;&#20154;&#35821;&#35328;&#38382;&#39064;&#30340;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#26377;&#30410;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#24320;&#21457;&#36825;&#20123;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#31995;&#32479;&#30340;&#22521;&#35757;&#25968;&#25454;&#36890;&#24120;&#27809;&#26377;&#32771;&#34385;&#21040;&#20154;&#21475;&#32479;&#35745;&#23398;&#65292;&#20294;&#20855;&#26377;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20154;&#21487;&#33021;&#20250;&#25552;&#20986;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#20316;&#20026;&#20026;&#25289;&#19969;&#35028;&#20799;&#31461;&#35774;&#35745;&#26234;&#33021;&#38405;&#35835;&#25903;&#25345;&#24212;&#29992;&#31243;&#24207;&#30340;&#24191;&#27867;&#39033;&#30446;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#20174;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#23398;&#30340;&#25289;&#19969;&#35028;&#25252;&#29702;&#20154;&#21592;&#21644;&#38750;&#25252;&#29702;&#20154;&#21592;&#20197;&#21450;&#20854;&#20182;&#20154;&#21475;&#32479;&#35745;&#23398;&#32972;&#26223;&#30340;&#25252;&#29702;&#20154;&#21592;&#21644;&#38750;&#25252;&#29702;&#20154;&#21592;&#20013;&#32676;&#38598;&#22823;&#37327;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#20010;&#20307;&#12289;&#25991;&#21270;&#21644;&#29615;&#22659;&#22240;&#32032;&#20013;&#20171;&#30340;&#38382;&#39064;&#25552;&#38382;&#30340;&#21464;&#21270;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#33258;&#21160;&#20135;&#29983;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
When caregivers ask open--ended questions to motivate dialogue with children, it facilitates the child's reading comprehension skills.Although there is scope for use of technological tools, referred here as "intelligent tutoring systems", to scaffold this process, it is currently unclear whether existing intelligent systems that generate human--language like questions is beneficial. Additionally, training data used in the development of these automated question generation systems is typically sourced without attention to demographics, but people with different cultural backgrounds may ask different questions. As a part of a broader project to design an intelligent reading support app for Latinx children, we crowdsourced questions from Latinx caregivers and noncaregivers as well as caregivers and noncaregivers from other demographics. We examine variations in question--asking within this dataset mediated by individual, cultural, and contextual factors. We then design a system that autom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.16806</link><description>&lt;p&gt;
GPT&#26159;&#21542;&#20250;&#20135;&#29983;&#26356;&#19981;&#20934;&#30830;&#30340;&#32763;&#35793;?
&lt;/p&gt;
&lt;p&gt;
Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3&#65292;&#24050;&#32463;&#25104;&#20026;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25110;&#29702;&#35299;&#20219;&#21153;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20219;&#21153;&#20013;&#65292;&#24050;&#26377;&#22810;&#39033;&#30740;&#31350;&#25506;&#32034;&#21033;&#29992;few-shot&#25552;&#31034;&#26426;&#21046;&#20174;LLMs&#20013;&#24341;&#20986;&#26356;&#22909;&#30340;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#30456;&#23545;&#36739;&#23569;&#22320;&#20851;&#27880;&#36825;&#31181;&#32763;&#35793;&#19982;&#26631;&#20934;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#29983;&#25104;&#32763;&#35793;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#20174;&#25991;&#23383;&#23545;&#40784;&#21644;&#21333;&#35843;&#24615;&#31561;&#26041;&#38754;&#65292;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#26412;&#25991;&#23383;&#31215;&#26497;&#24230;&#65292;&#21457;&#29616;GPT&#20174;&#33521;&#35821;&#65288;E-X&#65289;&#32763;&#35793;&#30340;&#25991;&#26412;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#20063;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#21516;&#26102;&#65292;&#24403;&#32763;&#35793;&#21477;&#23376;&#38271;&#24230;&#22686;&#21152;&#26102;&#65292;&#36825;&#31181;&#24046;&#21035;&#23601;&#23588;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating senten
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#36947;&#20027;&#20041;&#26412;&#20307;&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;HumBert&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#65292;&#20197;&#23454;&#29616;&#23545;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#20998;&#26512;&#30340;&#26377;&#25928;&#21644;&#36947;&#24503;&#24847;&#35782;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.16756</link><description>&lt;p&gt;
&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#23454;&#29616;&#21253;&#23481;&#21644;&#20559;&#35265;&#24863;&#30693;&#30340;&#20154;&#36947;&#20027;&#20041;&#21709;&#24212;&#20837;&#21475;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification. (arXiv:2305.16756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#36947;&#20027;&#20041;&#26412;&#20307;&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;HumBert&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#65292;&#20197;&#23454;&#29616;&#23545;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#20998;&#26512;&#30340;&#26377;&#25928;&#21644;&#36947;&#24503;&#24847;&#35782;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#36947;&#20027;&#20041;&#21361;&#26426;&#26399;&#38388;&#65292;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#24773;&#20917;&#20998;&#26512;&#23545;&#20110;&#39640;&#25928;&#22320;&#25552;&#20379;&#20154;&#36947;&#20027;&#20041;&#25588;&#21161;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#26159;&#20154;&#36947;&#20027;&#20041;&#21407;&#21017;&#21644;&#19981;&#30041;&#20219;&#20309;&#20154;&#33853;&#21518;&#21407;&#21017;&#30340;&#22522;&#30784;&#12290;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#21487;&#20197;&#26497;&#22823;&#22320;&#21463;&#30410;&#20110;&#36825;&#31181;&#25968;&#25454;&#20998;&#26512;&#65292;&#20363;&#22914;&#65292;&#25353;&#29031;&#20154;&#36947;&#20027;&#20041;&#26412;&#20307;&#23545;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#36890;&#36807;&#24494;&#35843;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26469;&#23454;&#29616;&#65292;&#28041;&#21450;&#19968;&#20123;&#23454;&#36341;&#21644;&#36947;&#24503;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#30095;&#21644;&#22797;&#26434;&#23376;&#39046;&#22495;&#19978;&#30340;&#25928;&#26524;&#19981;&#20339;&#20197;&#21450;&#31038;&#20250;&#20559;&#35265;&#21644;&#19981;&#33391;&#20851;&#32852;&#30340;&#32534;&#30721;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#21644;&#36947;&#24503;&#24847;&#35782;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807; (1) &#24341;&#20837;&#19968;&#20010;&#36866;&#21512;&#20154;&#36947;&#20027;&#20041;&#20998;&#26512;&#26694;&#26550;&#30340;&#26032;&#26550;&#26500;&#65292;(2) &#21019;&#24314;&#21644;&#21457;&#24067;&#19968;&#20010;&#26032;&#30340;&#20154;&#36947;&#20027;&#20041;&#29305;&#23450; LLM&#65292;&#31216;&#20026; HumBert&#65292;&#24182;&#19988; (3) &#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#24335;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and rapid situation analysis during humanitarian crises is critical to delivering humanitarian aid efficiently and is fundamental to humanitarian imperatives and the Leave No One Behind (LNOB) principle. This data analysis can highly benefit from language processing systems, e.g., by classifying the text data according to a humanitarian ontology. However, approaching this by simply fine-tuning a generic large language model (LLM) involves considerable practical and ethical issues, particularly the lack of effectiveness on data-sparse and complex subdomains, and the encoding of societal biases and unwanted associations. In this work, we aim to provide an effective and ethically-aware system for humanitarian data analysis. We approach this by (1) introducing a novel architecture adjusted to the humanitarian analysis framework, (2) creating and releasing a novel humanitarian-specific LLM called HumBert, and (3) proposing a systematic way to measure and mitigate biases. Our experi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;</title><link>http://arxiv.org/abs/2305.16264</link><description>&lt;p&gt;
&#32553;&#25918;&#25968;&#25454;&#21463;&#38480;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16264
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#36235;&#21183;&#28041;&#21450;&#22686;&#21152;&#21442;&#25968;&#35745;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#25512;&#26029;&#36825;&#20010;&#36235;&#21183;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#21487;&#33021;&#24456;&#24555;&#23601;&#20250;&#21463;&#21040;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20986;&#20110;&#27492;&#38480;&#21046;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#30740;&#31350;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36816;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#21464;&#21270;&#25968;&#25454;&#37325;&#22797;&#31243;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#65292;&#33539;&#22260;&#36798;&#21040;&#20102;9000&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21644;9&#20159;&#21442;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#39640;&#36798;4&#27425;&#37325;&#22797;&#25968;&#25454;&#30340;&#35757;&#32451;&#19982;&#20351;&#29992;&#21807;&#19968;&#25968;&#25454;&#30456;&#27604;&#23545;&#25439;&#22833;&#30340;&#36129;&#29486;&#24494;&#19981;&#36275;&#36947;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26356;&#22810;&#30340;&#37325;&#22797;&#25968;&#25454;&#65292;&#28155;&#21152;&#35745;&#31639;&#30340;&#20215;&#20540;&#26368;&#32456;&#20250;&#34928;&#20943;&#20026;&#38646;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#32463;&#39564;&#35777;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#35821;&#38899;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;&#23569;&#37327;&#30340;&#35789;-&#22270;&#20687;&#31034;&#20363;&#23545;&#20013;&#20064;&#24471;&#26032;&#30340;&#35789;&#27719;&#21450;&#20854;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#26102;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15937</link><description>&lt;p&gt;
&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#36827;&#34892;&#35270;&#35273;&#19978;&#26377;&#20381;&#25454;&#30340;&#23569;&#26679;&#26412;&#35789;&#27719;&#20064;&#24471;
&lt;/p&gt;
&lt;p&gt;
Visually grounded few-shot word acquisition with fewer shots. (arXiv:2305.15937v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15937
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#35821;&#38899;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;&#23569;&#37327;&#30340;&#35789;-&#22270;&#20687;&#31034;&#20363;&#23545;&#20013;&#20064;&#24471;&#26032;&#30340;&#35789;&#27719;&#21450;&#20854;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#26102;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#35821;&#38899;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20174;&#20165;&#26377;&#23569;&#37327;&#30340;&#35789;-&#22270;&#20687;&#31034;&#20363;&#23545;&#20013;&#20064;&#24471;&#26032;&#30340;&#35789;&#27719;&#21450;&#20854;&#35270;&#35273;&#34920;&#31034;&#12290;&#32473;&#23450;&#19968;&#32452;&#27979;&#35797;&#22270;&#20687;&#21644;&#19968;&#20010;&#21475;&#22836;&#26597;&#35810;&#65292;&#25105;&#20204;&#35201;&#27714;&#27169;&#22411;&#25351;&#20986;&#21738;&#20010;&#22270;&#20687;&#23637;&#31034;&#20102;&#26597;&#35810;&#35789;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#20351;&#29992;&#25968;&#23383;&#35789;-&#22270;&#20687;&#23545;&#30340;&#20154;&#36896;&#29615;&#22659;&#26469;&#31616;&#21270;&#35813;&#38382;&#39064;&#65292;&#35201;&#20040;&#20351;&#29992;&#27599;&#20010;&#31867;&#21035;&#22823;&#37327;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#33258;&#28982;&#30340;&#35789;-&#22270;&#20687;&#23545;&#19978;&#36827;&#34892;&#65292;&#20294;&#21482;&#38656;&#26356;&#23569;&#30340;&#25968;&#25454;&#65292;&#21363;&#26356;&#23569;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#32473;&#23450;&#30340;&#35789;-&#22270;&#20687;&#31034;&#20363;&#23545;&#20174;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#21644;&#22270;&#20687;&#20013;&#25366;&#25496;&#26032;&#30340;&#26080;&#30417;&#30563;&#35789;-&#22270;&#20687;&#35757;&#32451;&#23545;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21333;&#35789;&#21040;&#22270;&#20687;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#30830;&#23450;&#35789;-&#22270;&#20687;&#30340;&#30456;&#20284;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26032;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;&#20219;&#20309;&#29616;&#26377;&#26041;&#27861;&#37117;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#21482;&#38656;&#26356;&#23569;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a visually grounded speech model that acquires new words and their visual depictions from just a few word-image example pairs. Given a set of test images and a spoken query, we ask the model which image depicts the query word. Previous work has simplified this problem by either using an artificial setting with digit word-image pairs or by using a large number of examples per class. We propose an approach that can work on natural word-image pairs but with less examples, i.e. fewer shots. Our approach involves using the given word-image example pairs to mine new unsupervised word-image training pairs from large collections of unlabelled speech and images. Additionally, we use a word-to-image attention mechanism to determine word-image similarity. With this new model, we achieve better performance with fewer shots than any existing approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#35789;&#27719;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#20102;&#20998;&#23618;&#20004;&#32423;&#26041;&#27861;&#21644;&#27973;&#23618;Transformer&#20307;&#31995;&#32467;&#26500;&#20174;&#23383;&#31526;&#20013;&#23398;&#20064;&#21333;&#35789;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#23481;&#24525;&#24230;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14571</link><description>&lt;p&gt;
&#20174;&#23383;&#31526;&#21040;&#35789;&#65306;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#35821;&#35328;&#29702;&#35299;&#30340;&#20998;&#23618;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding. (arXiv:2305.14571v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#35789;&#27719;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#20102;&#20998;&#23618;&#20004;&#32423;&#26041;&#27861;&#21644;&#27973;&#23618;Transformer&#20307;&#31995;&#32467;&#26500;&#20174;&#23383;&#31526;&#20013;&#23398;&#20064;&#21333;&#35789;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#23481;&#24525;&#24230;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#26368;&#26032;&#27169;&#22411;&#38656;&#35201;&#23545;&#21407;&#22987;&#25991;&#26412;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#31163;&#25955;&#26631;&#35760;&#12290;&#36825;&#20010;&#36807;&#31243;&#31216;&#20026;&#20998;&#35789;&#65292;&#20381;&#36182;&#20110;&#39044;&#20808;&#26500;&#24314;&#30340;&#21333;&#35789;&#25110;&#23376;&#35789;&#12290;&#20294;&#36825;&#31181;&#22266;&#23450;&#35789;&#27719;&#34920;&#38480;&#21046;&#20102;&#27169;&#22411;&#23545;&#25340;&#20889;&#38169;&#35823;&#30340;&#23481;&#24525;&#24230;&#21644;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#35789;&#27719;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#20998;&#23618;&#30340;&#20004;&#32423;&#26041;&#27861;&#65306;&#19968;&#20010;&#22312;&#35789;&#32423;&#21035;&#65292;&#21478;&#19968;&#20010;&#22312;&#24207;&#21015;&#32423;&#21035;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20869;&#37096;&#21333;&#35789;&#27169;&#22359;&#65292;&#20351;&#29992;&#27973;&#23618;Transformer&#20307;&#31995;&#32467;&#26500;&#20174;&#20854;&#23383;&#31526;&#20013;&#23398;&#20064;&#21333;&#35789;&#34920;&#31034;&#65292;&#24182;&#21478;&#19968;&#20010;&#28145;&#23618;Transformer&#27169;&#22359;&#65292;&#23558;&#27599;&#20010;&#21333;&#35789;&#34920;&#31034;&#32622;&#20110;&#25972;&#20010;&#21333;&#35789;&#24207;&#21015;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30452;&#25509;&#22312;&#20855;&#26377;&#26174;&#24335;&#21333;&#35789;&#36793;&#30028;&#24847;&#35782;&#30340;&#23383;&#31526;&#24207;&#21015;&#19978;&#36816;&#34892;&#65292;&#20294;&#27809;&#26377;&#20559;&#35265;&#30340;&#23376;&#21333;&#35789;&#25110;&#21333;&#35789;&#32423;&#35789;&#27719;&#34920;&#12290;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current state-of-the-art models for natural language understanding require a preprocessing step to convert raw text into discrete tokens. This process known as tokenization relies on a pre-built vocabulary of words or sub-word morphemes. This fixed vocabulary limits the model's robustness to spelling errors and its capacity to adapt to new domains. In this work, we introduce a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another at the sequence level. Concretely, we design an intra-word module that uses a shallow Transformer architecture to learn word representations from their characters, and a deep inter-word Transformer module that contextualizes each word representation by attending to the entire word sequence. Our model thus directly operates on character sequences with explicit awareness of word boundaries, but without biased sub-word or word-level vocabulary. Experiments on various downstream tasks show that our me
&lt;/p&gt;</description></item><item><title>BA-SOT&#26159;&#19968;&#31181;&#38754;&#21521;&#22810;&#35828;&#35805;&#20154;ASR&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36793;&#30028;&#24863;&#30693;&#21644;&#36830;&#25509;&#26102;&#38388;&#20998;&#31867;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13716</link><description>&lt;p&gt;
BA-SOT: &#38754;&#21521;&#22810;&#35828;&#35805;&#20154;ASR&#30340;&#36793;&#30028;&#24863;&#30693;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BA-SOT: Boundary-Aware Serialized Output Training for Multi-Talker ASR. (arXiv:2305.13716v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13716
&lt;/p&gt;
&lt;p&gt;
BA-SOT&#26159;&#19968;&#31181;&#38754;&#21521;&#22810;&#35828;&#35805;&#20154;ASR&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36793;&#30028;&#24863;&#30693;&#21644;&#36830;&#25509;&#26102;&#38388;&#20998;&#31867;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#65288;SOT&#65289;&#36890;&#36807;&#29983;&#25104;&#30001;&#29305;&#27530;&#26631;&#35760;&#20998;&#38548;&#30340;&#35828;&#35805;&#32773;&#36716;&#24405;&#31616;&#21270;&#20102;&#22810;&#35828;&#35805;&#32773;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#20294;&#26159;&#65292;&#39057;&#32321;&#30340;&#35828;&#35805;&#32773;&#26356;&#25913;&#21487;&#33021;&#20250;&#20351;&#35828;&#35805;&#32773;&#26356;&#25913;&#39044;&#27979;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36793;&#30028;&#24863;&#30693;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#65288;BA-SOT&#65289;&#65292;&#23427;&#36890;&#36807;&#35828;&#35805;&#32773;&#26356;&#25913;&#26816;&#27979;&#20219;&#21153;&#21644;&#36793;&#30028;&#32422;&#26463;&#25439;&#22833;&#23558;&#36793;&#30028;&#30693;&#35782;&#26126;&#30830;&#22320;&#32435;&#20837;&#35299;&#30721;&#22120;&#20013;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#36830;&#25509;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#31574;&#30053;&#65292;&#23427;&#23558;&#22522;&#20110;&#26631;&#35760;&#30340;SOT CTC&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#24674;&#22797;&#26102;&#38388;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#38500;&#20102;&#20856;&#22411;&#30340;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;CER&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35805;&#35821;&#30340;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;UD-CER&#65289;&#65292;&#20197;&#36827;&#19968;&#27493;&#34913;&#37327;&#35828;&#35805;&#32773;&#26356;&#25913;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;&#19982;&#21407;&#22987;&#30340;SOT&#30456;&#27604;&#65292;BA-SOT&#23558;CER / UD-CER&#38477;&#20302;&#20102;5.1&#65285;/ 14.0&#65285;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#36827;&#34892;BA-SOT&#27169;&#22411;&#21021;&#22987;&#21270;&#36827;&#19968;&#27493;&#23558;CER / UD-CER&#38477;&#20302;&#20102;8.4&#65285;/ 19.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently proposed serialized output training (SOT) simplifies multi-talker automatic speech recognition (ASR) by generating speaker transcriptions separated by a special token. However, frequent speaker changes can make speaker change prediction difficult. To address this, we propose boundary-aware serialized output training (BA-SOT), which explicitly incorporates boundary knowledge into the decoder via a speaker change detection task and boundary constraint loss. We also introduce a two-stage connectionist temporal classification (CTC) strategy that incorporates token-level SOT CTC to restore temporal context information. Besides typical character error rate (CER), we introduce utterance-dependent character error rate (UD-CER) to further measure the precision of speaker change prediction. Compared to original SOT, BA-SOT reduces CER/UD-CER by 5.1%/14.0%, and leveraging a pre-trained ASR model for BA-SOT model initialization further reduces CER/UD-CER by 8.4%/19.9%.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#31216;&#26356;&#20026;&#22810;&#26679;&#12289;&#27809;&#26377;&#25463;&#24452;&#12289;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;EntRed&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#23454;&#20307;&#27880;&#37322;&#38169;&#35823;&#12289;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#12289;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13551</link><description>&lt;p&gt;
EntRED: &#29992;&#26356;&#23569;&#30340;&#25463;&#24452;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
EntRED: Benchmarking Relation Extraction with Fewer Shortcuts. (arXiv:2305.13551v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#31216;&#26356;&#20026;&#22810;&#26679;&#12289;&#27809;&#26377;&#25463;&#24452;&#12289;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;EntRed&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#23454;&#20307;&#27880;&#37322;&#38169;&#35823;&#12289;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#12289;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21517;&#31216;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#36215;&#30528;&#26377;&#25928;&#30340;&#20316;&#29992;&#65292;&#24182;&#24120;&#24120;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#38598;&#20013;&#30340;&#23454;&#20307;&#21517;&#31216;&#26174;&#33879;&#24433;&#21709;&#20102;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26631;&#20934;&#30340;&#20851;&#31995;&#25277;&#21462;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#22823;&#37327;&#38169;&#35823;&#30340;&#23454;&#20307;&#27880;&#37322;&#65292;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#12290;&#36825;&#20123;&#38382;&#39064;&#20351;&#24471;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19982;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30456;&#36317;&#29978;&#36828;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EntRED&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36739;&#23569;&#25463;&#24452;&#21644;&#26356;&#39640;&#23454;&#20307;&#22810;&#26679;&#24615;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#26500;&#24314;EntRED&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#65288;CI&#65289;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26367;&#25442;&#31649;&#36947;&#65306;ERIC&#12290;ERIC&#23545;&#23454;&#20307;&#36827;&#34892;&#31867;&#22411;&#32422;&#26463;&#26367;&#25442;&#65292;&#20197;&#20943;&#23569;&#20174;&#23454;&#20307;&#20559;&#24046;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#12290;ERIC&#22312;&#20004;&#20010;&#26041;&#38754;&#24212;&#29992;CI&#65306;1&#65289;&#38024;&#23545;&#38656;&#35201;&#23454;&#20307;&#26367;&#25442;&#30340;&#23454;&#20363;&#65292;2&#65289;&#30830;&#23450;&#20505;&#36873;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity names play an effective role in relation extraction (RE) and often influence model performance. As a result, the entity names in the benchmarks' test sets significantly influence the evaluation of RE models. In this work, we find that the standard RE benchmarks' datasets have a large portion of incorrect entity annotations, low entity name diversity, and are prone to have shortcuts from entity names to ground-truth relations. These issues make the standard benchmarks far from reflecting the real-world scenarios. Hence, in this work, we present EntRED, a challenging RE benchmark with reduced shortcuts and higher diversity of entities. To build EntRED, we propose an end-to-end entity replacement pipeline based on causal inference (CI): ERIC. ERIC performs type-constrained replacements on entities to reduce the shortcuts from entity bias to ground-truth relations. ERIC applies CI in two aspects: 1) targeting the instances that need entity replacements, and 2) determining the candid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#65288;Re-SAT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#22833;&#35821;&#30151;&#24739;&#32773;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#22312;&#19981;&#24433;&#21709;&#20581;&#24247;&#24739;&#32773;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;ASR&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13108</link><description>&lt;p&gt;
&#36890;&#36807;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#36827;&#34892;&#22833;&#35821;&#30151;&#35821;&#38899;&#30340;&#26080;&#20559;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test. (arXiv:2305.13108v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#65288;Re-SAT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#22833;&#35821;&#30151;&#24739;&#32773;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#22312;&#19981;&#24433;&#21709;&#20581;&#24247;&#24739;&#32773;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;ASR&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20027;&#35201;&#26159;&#36890;&#36807;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#30001;&#20110;ERM&#21033;&#29992;&#25968;&#25454;&#26679;&#26412;&#30340;&#24179;&#22343;&#34920;&#29616;&#32780;&#19981;&#32771;&#34385;&#19968;&#20010;&#32676;&#20307;&#65292;&#20363;&#22914;&#20581;&#24247;&#25110;&#22833;&#35821;&#30151;&#24739;&#32773;&#65292;&#22240;&#27492;ASR&#31995;&#32479;&#26080;&#27861;&#35782;&#21035;&#36328;&#32676;&#20307;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#23548;&#33268;ASR&#31995;&#32479;&#23384;&#22312;&#20559;&#24046;&#19988;&#20854;&#32676;&#20307;&#24615;&#33021;&#24046;&#24322;&#20005;&#37325;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#32676;&#20307;&#31283;&#20581;&#24615;&#65292;&#38024;&#23545;&#22833;&#35821;&#30151;&#24739;&#32773;&#36827;&#34892;&#25913;&#36827;&#12290;&#20026;&#20102;&#23454;&#29616;&#25105;&#20204;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#65288;Re-SAT&#65289;&#12290; Re-SAT&#31995;&#32479;&#22320;&#34913;&#37327;&#25152;&#32473;&#25968;&#25454;&#26679;&#26412;&#30340;&#21435;&#20559;&#24110;&#21161;&#24615;&#65292;&#24182;&#36890;&#36807;&#21435;&#20559;&#24110;&#21161;&#24615;&#21152;&#26435;&#26469;&#32531;&#35299;&#20559;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292; Re-SAT&#26377;&#21161;&#20110;&#25913;&#21892;&#22833;&#35821;&#30151;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20581;&#24247;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition systems based on deep learning are mainly trained under empirical risk minimization (ERM). Since ERM utilizes the averaged performance on the data samples regardless of a group such as healthy or dysarthric speakers, ASR systems are unaware of the performance disparities across the groups. This results in biased ASR systems whose performance differences among groups are severe. In this study, we aim to improve the ASR system in terms of group robustness for dysarthric speakers. To achieve our goal, we present a novel approach, sample reweighting with sample affinity test (Re-SAT). Re-SAT systematically measures the debiasing helpfulness of the given data sample and then mitigates the bias by debiasing helpfulness-based sample reweighting. Experimental results demonstrate that Re-SAT contributes to improved ASR performance on dysarthric speech without performance degradation on healthy speech.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#26469;&#25552;&#39640;LLMs&#20013;ICL&#30340;&#24615;&#33021;&#65292;&#23427;&#23558;ICL&#36807;&#31243;&#20998;&#20026;&#8220;&#28145;&#24605;&#29087;&#34385;&#8221;&#21644;&#25512;&#29702;&#38454;&#27573;&#12290;&#22312;&#8220;&#28145;&#24605;&#29087;&#34385;&#8221;&#38454;&#27573;&#20013;&#65292;&#36890;&#36807;&#22810;&#27425;&#36845;&#20195;&#20248;&#21270;&#31034;&#33539;&#65292;&#24182;&#25805;&#32437;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#27169;&#22359;&#20013;&#30340;Key-Value&#30697;&#38453;&#26469;&#29983;&#25104;&#20803;&#26799;&#24230;&#65292;&#20174;&#32780;&#26399;&#26395;&#22312;&#27979;&#35797;&#26102;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13016</link><description>&lt;p&gt;
&#12298;&#36845;&#20195;&#21069;&#21521;&#35843;&#25972;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Iterative Forward Tuning Boosts In-context Learning in Language Models. (arXiv:2305.13016v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#26469;&#25552;&#39640;LLMs&#20013;ICL&#30340;&#24615;&#33021;&#65292;&#23427;&#23558;ICL&#36807;&#31243;&#20998;&#20026;&#8220;&#28145;&#24605;&#29087;&#34385;&#8221;&#21644;&#25512;&#29702;&#38454;&#27573;&#12290;&#22312;&#8220;&#28145;&#24605;&#29087;&#34385;&#8221;&#38454;&#27573;&#20013;&#65292;&#36890;&#36807;&#22810;&#27425;&#36845;&#20195;&#20248;&#21270;&#31034;&#33539;&#65292;&#24182;&#25805;&#32437;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#27169;&#22359;&#20013;&#30340;Key-Value&#30697;&#38453;&#26469;&#29983;&#25104;&#20803;&#26799;&#24230;&#65292;&#20174;&#32780;&#26399;&#26395;&#22312;&#27979;&#35797;&#26102;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#32039;&#23494;&#32852;&#31995;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#33021;&#22815;&#35299;&#20915;&#26222;&#36890;&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#26080;&#27861;&#36890;&#36807;&#19968;&#27425;&#22788;&#29702;&#31034;&#33539;&#26679;&#20363;&#26469;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24320;&#21457;Transformer&#27880;&#24847;&#21147;&#21644;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#20043;&#38388;&#30340;&#21452;&#37325;&#24418;&#24335;&#26469;&#25552;&#39640;LLMs&#20013;ICL&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;ICL&#36807;&#31243;&#20998;&#20026;&#8220;&#28145;&#24605;&#29087;&#34385;&#8221;&#21644;&#25512;&#29702;&#38454;&#27573;&#12290;&#22312;&#8220;&#28145;&#24605;&#29087;&#34385;&#8221;&#38454;&#27573;&#20013;&#65292;&#36890;&#36807;&#22810;&#27425;&#36845;&#20195;&#20248;&#21270;&#31034;&#33539;&#65292;&#24182;&#25805;&#32437;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#27169;&#22359;&#20013;&#30340;Key-Value&#30697;&#38453;&#26469;&#29983;&#25104;&#20803;&#26799;&#24230;&#65292;&#20174;&#32780;&#26399;&#26395;&#22312;&#27979;&#35797;&#26102;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25512;&#29702;&#38454;&#27573;&#20165;&#22788;&#29702;&#27979;&#35797;&#26597;&#35810;&#65292;&#32780;&#19981;&#38656;&#35201;&#20877;&#27425;&#32771;&#34385;&#31034;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited an emergent in-context learning (ICL) ability. However, the ICL models that can solve ordinary cases are hardly extended to solve more complex tasks by processing the demonstration examples once. This single-turn ICL is incoordinate with the decision making process of humans by learning from analogy. In this paper, we propose an effective and efficient two-stage framework to boost ICL in LLMs by exploiting a dual form between Transformer attention and gradient descent-based optimization. Concretely, we divide the ICL process into "Deep-Thinking" and inference stages. The "Deep-Thinking" stage performs iterative forward optimization of demonstrations, which is expected to boost the reasoning abilities of LLMs at test time by "thinking" demonstrations multiple times. It produces accumulated meta-gradients by manipulating the Key-Value matrices in the self-attention modules of the Transformer. Then, the inference stage only takes the test query 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#34892;&#19994;&#20113;&#29305;&#23450;QA&#25968;&#25454;&#38598; MSQA&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11541</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#19994;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering. (arXiv:2305.11541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#34892;&#19994;&#20113;&#29305;&#23450;QA&#25968;&#25454;&#38598; MSQA&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#24320;&#25918;&#39046;&#22495;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#24191;&#27867;&#24212;&#29992;&#21644;&#21331;&#36234;&#30340;&#25104;&#26524;&#65292;&#20294;&#20854;&#22312;&#30495;&#23454;&#30340;&#24037;&#19994;&#29305;&#23450;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#36890;&#24120;&#24456;&#24179;&#24248;&#65292;&#22240;&#20026;&#23427;&#32570;&#20047;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#36825;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#24456;&#23569;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;MSQA&#30340;&#22522;&#20934;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#28041;&#21450;Microsoft&#20135;&#21697;&#21644;&#23458;&#25143;&#36935;&#21040;&#30340;IT&#25216;&#26415;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#34892;&#19994;&#20113;&#30340;&#29305;&#23450;QA&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#19968;&#33324;&#30340;LLM&#26469;&#35828;&#26159;&#19981;&#21487;&#29992;&#30340;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;LLM&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;LLM&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36981;&#24490;&#25105;&#20204;&#30340;&#27169;&#22411;&#34701;&#21512;&#26694;&#26550;&#30340;&#26041;&#27861;&#27604;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#30340;&#24120;&#29992;LLM&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) has gained popularity and achieved remarkable results in open-domain tasks, but its performance in real industrial domain-specific scenarios is average since there is no specific knowledge in it. This issue has attracted widespread attention, but there are few relevant benchmarks available. In this paper, we provide a benchmark Question Answering (QA) dataset named MSQA, which is about Microsoft products and IT technical problems encountered by customers. This dataset contains industry cloud-specific QA knowledge, which is not available for general LLM, so it is well suited for evaluating methods aimed at improving domain-specific capabilities of LLM. In addition, we propose a new model interaction paradigm that can empower LLM to achieve better performance on domain-specific tasks where it is not proficient. Extensive experiments demonstrate that the approach following our model fusion framework outperforms the commonly used LLM with retrieval methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EmbMarker &#30340;&#23884;&#20837;&#24335;&#27700;&#21360;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312; EaaS &#20013;&#30340;&#29256;&#26435;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23884;&#20837;&#24335;&#19978;&#26893;&#20837;&#21518;&#38376;&#65292;&#24182;&#26377;&#25928;&#22320;&#20256;&#36755;&#21644;&#24674;&#22797;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EmbMarker &#21487;&#20197;&#22312;&#32500;&#25252;&#21508;&#31181; NLP &#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#25104;&#21151;&#20445;&#25252; EaaS &#23545; LLM &#30340;&#29256;&#26435;&#12290;</title><link>http://arxiv.org/abs/2305.10036</link><description>&lt;p&gt;
&#20320;&#22312;&#25220;&#25105;&#30340;&#27169;&#22411;&#21527;&#65311;&#22522;&#20110;&#21518;&#38376;&#27700;&#21360;&#30340;&#20445;&#25252;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312; EaaS &#20013;&#30340;&#29256;&#26435;
&lt;/p&gt;
&lt;p&gt;
Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark. (arXiv:2305.10036v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10036
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EmbMarker &#30340;&#23884;&#20837;&#24335;&#27700;&#21360;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312; EaaS &#20013;&#30340;&#29256;&#26435;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23884;&#20837;&#24335;&#19978;&#26893;&#20837;&#21518;&#38376;&#65292;&#24182;&#26377;&#25928;&#22320;&#20256;&#36755;&#21644;&#24674;&#22797;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EmbMarker &#21487;&#20197;&#22312;&#32500;&#25252;&#21508;&#31181; NLP &#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#25104;&#21151;&#20445;&#25252; EaaS &#23545; LLM &#30340;&#29256;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#20844;&#21496;&#24050;&#32463;&#24320;&#22987;&#22522;&#20110;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#23884;&#20837;&#24335;&#26381;&#21153; (EaaS)&#65292;&#21487;&#20197;&#20026;&#23458;&#25143;&#30340;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#20219;&#21153;&#24102;&#26469;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;EaaS &#26131;&#21463;&#21040;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#25915;&#20987;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545; LLM &#30340;&#25152;&#26377;&#32773;&#36896;&#25104;&#24040;&#22823;&#25439;&#22833;&#65292;&#22240;&#20026;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#38750;&#24120;&#26114;&#36149;&#12290;&#20026;&#20102;&#20445;&#25252; EaaS &#30340; LLM &#30340;&#29256;&#26435;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; EmbMarker &#30340;&#23884;&#20837;&#24335;&#27700;&#21360;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23884;&#20837;&#24335;&#19978;&#26893;&#20837;&#21518;&#38376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#36890;&#29992;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#36873;&#25321;&#19968;&#32452;&#20013;&#31561;&#39057;&#29575;&#30340;&#21333;&#35789;&#65292;&#24418;&#25104;&#35302;&#21457;&#38598;&#65292;&#28982;&#21518;&#36873;&#25321;&#19968;&#20010;&#30446;&#26631;&#23884;&#20837;&#20316;&#20026;&#27700;&#21360;&#65292;&#24182;&#23558;&#20854;&#25554;&#20837;&#21253;&#21547;&#35302;&#21457;&#35789;&#30340;&#25991;&#26412;&#30340;&#23884;&#20837;&#20013;&#20316;&#20026;&#21518;&#38376;&#12290;&#25554;&#20837;&#30340;&#37325;&#37327;&#19982;&#21253;&#21547;&#22312;&#25991;&#26412;&#20013;&#30340;&#35302;&#21457;&#35789;&#25968;&#37327;&#25104;&#27604;&#20363;&#12290;&#36825;&#20351;&#24471;&#27700;&#21360;&#21518;&#38376;&#21487;&#20197;&#26377;&#25928;&#22320;&#20256;&#36755;&#21644;&#24674;&#22797;&#65292;&#32780;&#19981;&#24433;&#21709; LLM &#22312;&#21508;&#31181; NLP &#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EmbMarker &#21487;&#20197;&#22312;&#32500;&#25252;&#21508;&#31181; NLP &#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#25104;&#21151;&#20445;&#25252; EaaS &#23545; LLM &#30340;&#29256;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called EmbMarker that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09941</link><description>&lt;p&gt;
&#8220;&#25105;&#20840;&#28982;&#25104;&#20026;&#25105;&#33258;&#24049;&#8221;&#65306;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24615;&#21035;&#21644;&#38750;&#20108;&#20803;&#65288;TGNB&#65289;&#20154;&#32676;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#32463;&#21382;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#27495;&#35270;&#21644;&#25490;&#26021;&#12290;&#38543;&#30528;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#24212;&#29992;&#65292;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#36825;&#19968;&#20154;&#32676;&#30340;&#21487;&#33021;&#24615;&#20063;&#22312;&#22686;&#21152;&#12290;&#34429;&#28982;&#22823;&#37327;&#30340;NLP&#20844;&#24179;&#25991;&#29486;&#30528;&#37325;&#20110;&#38416;&#26126;&#21644;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#35780;&#20272;TGNB&#36523;&#20221;&#25152;&#24102;&#26469;&#30340;&#24615;&#21035;&#20260;&#23475;&#38656;&#35201;&#29702;&#35299;&#36825;&#20123;&#36523;&#20221;&#22914;&#20309;&#29420;&#29305;&#22320;&#19982;&#31038;&#20250;&#24615;&#21035;&#35268;&#33539;&#20114;&#21160;&#20197;&#21450;&#19982;&#24615;&#21035;&#20108;&#20803;&#20013;&#24515;&#30340;&#35270;&#35282;&#30456;&#21306;&#20998;&#12290;&#36825;&#26679;&#30340;&#27979;&#37327;&#26694;&#26550;&#26412;&#36136;&#19978;&#38656;&#35201;&#20197;TGNB&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#24110;&#21161;&#25351;&#23548;&#21253;&#23481;&#24615;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#35813;&#20026;&#35841;&#26381;&#21153;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20197;TGNB&#31038;&#21306;&#21644;&#29616;&#26377;&#30340;&#36328;&#23398;&#31185;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#35780;&#20272;&#20102;TGNB&#20010;&#20307;&#32463;&#21382;&#36793;&#32536;&#21270;&#25152;&#24418;&#25104;&#30340;&#31038;&#20250;&#29616;&#23454;&#26159;&#22914;&#20309;&#24433;&#21709;&#21644;&#23384;&#22312;&#20110;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#65288;OLG&#65289;&#20013;&#12290;&#39318;&#20808;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;OLG&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#24230;&#37327;&#19982;&#35813;&#20154;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#29305;&#21035;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#65292;&#20197;&#21450;&#20132;&#21449;&#20998;&#26512;&#32467;&#26524;&#30340;&#20132;&#21449;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#20844;&#24179;&#12289;&#26356;&#21253;&#23481;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#65292;&#24182;&#28508;&#22312;&#22320;&#35299;&#20915;NLP&#30740;&#31350;&#20013;&#24191;&#27867;&#30340;&#20132;&#21449;&#36523;&#20221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25209;&#35780;&#24615;&#22320;&#26816;&#26597;&#21644;&#23457;&#26597;&#20102;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340;VQA&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#25991;&#26412;&#35821;&#20041;&#29983;&#25104;&#12289;&#23545;&#35937;&#35782;&#21035;&#21644;&#31572;&#26696;&#20998;&#31867;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.09782</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#35270;&#35273;&#38382;&#31572;&#31639;&#27861;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Visual Question Answering Algorithms with attention model. (arXiv:2305.09782v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25209;&#35780;&#24615;&#22320;&#26816;&#26597;&#21644;&#23457;&#26597;&#20102;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340;VQA&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#25991;&#26412;&#35821;&#20041;&#29983;&#25104;&#12289;&#23545;&#35937;&#35782;&#21035;&#21644;&#31572;&#26696;&#20998;&#31867;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20351;&#29992;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#29702;&#35299;&#24182;&#22238;&#31572;&#38382;&#39064;&#12290; VQA &#23545;&#35270;&#35273;&#21463;&#25439;&#32773;&#26377;&#24110;&#21161;&#65292;&#21487;&#29992;&#20110;&#23433;&#20840;&#30417;&#25511;&#31995;&#32479;&#21644;&#20174;&#32593;&#32476;&#20013;&#23398;&#20064;&#30340;&#22312;&#32447;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290; &#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23398;&#20064;&#38382;&#39064;&#30340;&#35821;&#20041;&#24182;&#25552;&#21462;&#25991;&#26412;&#29305;&#24449;&#12290; &#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#29992;&#20110;&#20197;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#25152;&#38382;&#38382;&#39064;&#28041;&#21450;&#30340;&#29289;&#20307;&#30340;&#26041;&#24335;&#29983;&#25104;&#22270;&#20687;&#34920;&#31034;&#12290; &#27880;&#24847;&#21147;&#27169;&#22411;&#35797;&#22270;&#27169;&#20223;&#20154;&#31867;&#26681;&#25454;&#35821;&#22659;&#20851;&#27880;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#34892;&#20026;&#12290; &#26412;&#25991;&#25209;&#35780;&#24615;&#22320;&#26816;&#26597;&#21644;&#23457;&#26597;&#20102;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340; VQA &#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#29983;&#25104;&#25991;&#26412;&#35821;&#20041;&#65292;&#35782;&#21035;&#23545;&#35937;&#21644;&#31572;&#26696;&#20998;&#31867;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) usesimage processing algorithms to process the image and natural language processing methods to understand and answer the question. VQA is helpful to a visually impaired person, can be used for the security surveillance system and online chatbots that learn from the web. It uses NLP methods to learn the semantic of the question and to derive the textual features. Computer vision techniques are used for generating image representation in such a way that they can identify the objects about which question is asked. The Attention model tries to mimic the human behavior of giving attention to a different region of an image according to our understanding of its context. This paper critically examines and reviews methods of VQA algorithm such as generation of semantics of text, identification of objects and answer classification techniques that use the co-attention approach.
&lt;/p&gt;</description></item><item><title>DUDE&#25512;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#26088;&#22312;&#21019;&#36896;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#22522;&#20934;&#27979;&#35797;&#24182;&#25512;&#21160;&#24403;&#21069;&#26041;&#27861;&#30340;&#36793;&#30028;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;</title><link>http://arxiv.org/abs/2305.08455</link><description>&lt;p&gt;
&#25991;&#26723;&#29702;&#35299;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#65288;DUDE&#65289;
&lt;/p&gt;
&lt;p&gt;
Document Understanding Dataset and Evaluation (DUDE). (arXiv:2305.08455v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08455
&lt;/p&gt;
&lt;p&gt;
DUDE&#25512;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#26088;&#22312;&#21019;&#36896;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#22522;&#20934;&#27979;&#35797;&#24182;&#25512;&#21160;&#24403;&#21069;&#26041;&#27861;&#30340;&#36793;&#30028;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21628;&#21505;&#25991;&#26723;AI&#31038;&#21306;&#37325;&#26032;&#35780;&#20272;&#24403;&#21069;&#30340;&#26041;&#27861;&#35770;&#65292;&#25317;&#25265;&#21019;&#24314;&#26356;&#23454;&#38469;&#21462;&#21521;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;&#25991;&#26723;&#29702;&#35299;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#65288;DUDE&#65289;&#26088;&#22312;&#32416;&#27491;&#22312;&#29702;&#35299;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#65288;VRD&#65289;&#26041;&#38754;&#30340;&#30740;&#31350;&#36827;&#23637;&#20572;&#28382;&#19981;&#21069;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#19982;&#22810;&#34892;&#19994;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#39029;VRD&#30456;&#20851;&#30340;&#38382;&#39064;&#31867;&#22411;&#12289;&#31572;&#26696;&#21644;&#25991;&#26723;&#24067;&#23616;&#30340;&#21019;&#26032;&#65292;&#20855;&#26377;&#21508;&#31181;&#26469;&#28304;&#21644;&#26085;&#26399;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#30340;&#35780;&#20272;&#35774;&#32622;&#26469;&#25512;&#21160;&#24403;&#21069;&#26041;&#27861;&#30340;&#36793;&#30028;&#65292;&#36825;&#20123;&#35774;&#32622;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#65292;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#38656;&#35201;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#36827;&#34892;&#24378;&#22823;&#30340;&#27867;&#21270;&#21644;&#36866;&#24212;&#12290;DUDE&#26088;&#22312;&#25104;&#20026;&#19968;&#20010;&#26356;&#23454;&#38469;&#12289;&#26356;&#38271;&#26399;&#30340;&#22522;&#20934;&#27979;&#35797;&#26631;&#20934;&#65292;&#24182;&#24076;&#26395;&#23427;&#20250;&#24341;&#39046;&#26410;&#26469;&#30340;&#25193;&#23637;&#21644;&#36129;&#29486;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#35828;&#26126;&#20102;&#20197;&#19979;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We call on the Document AI (DocAI) community to reevaluate current methodologies and embrace the challenge of creating more practically-oriented benchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to remediate the halted research progress in understanding visually-rich documents (VRDs). We present a new dataset with novelties related to types of questions, answers, and document layouts based on multi-industry, multi-domain, and multi-page VRDs of various origins, and dates. Moreover, we are pushing the boundaries of current methods by creating multi-task and multi-domain evaluation setups that more accurately simulate real-world situations where powerful generalization and adaptation under low-resource settings are desired. DUDE aims to set a new standard as a more practical, long-standing benchmark for the community, and we hope that it will lead to future extensions and contributions that address real-world challenges. Finally, our work illustrates the importance o
&lt;/p&gt;</description></item><item><title>ArtGPT-4&#26159;&#19968;&#31181;&#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#35299;&#20915;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#20986;&#20855;&#22791;&#33391;&#22909;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.07490</link><description>&lt;p&gt;
ArtGPT-4: &#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#30340;&#33402;&#26415;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4. (arXiv:2305.07490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07490
&lt;/p&gt;
&lt;p&gt;
ArtGPT-4&#26159;&#19968;&#31181;&#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#35299;&#20915;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#20986;&#20855;&#22791;&#33391;&#22909;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#27604;&#22914;ChatGPT&#21644;GPT-4&#31561;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#23545;&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#25214;&#21040;&#19982;&#27169;&#22411;&#35268;&#27169;&#21305;&#37197;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#20063;&#24456;&#22256;&#38590;&#12290;&#24494;&#35843;&#21644;&#20351;&#29992;&#26032;&#26041;&#27861;&#35757;&#32451;&#21442;&#25968;&#36739;&#23569;&#30340;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;MiniGPT-4&#27169;&#22411;&#20415;&#26159;&#20854;&#20013;&#20043;&#19968;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#36816;&#29992;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38761;&#26032;&#24615;&#30340;&#22521;&#35757;&#31574;&#30053;&#23454;&#29616;&#20102;&#19982;GPT-4&#30456;&#24403;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#35813;&#27169;&#22411;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33402;&#26415;&#22270;&#29255;&#26041;&#38754;&#12290;ArtGPT-4&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#26088;&#22312;&#24212;&#23545;&#36825;&#20123;&#23616;&#38480;&#12290;ArtGPT-4&#20351;&#29992;Tesla A100&#35774;&#22791;&#23545;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#20165;&#29992;&#20102;&#32422;200GB&#30340;&#25968;&#25454;&#65292;&#22312;2&#23567;&#26102;&#20869;&#23601;&#33021;&#23637;&#31034;&#20986;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have made significant progress in natural language processing (NLP), with models like ChatGPT and GPT-4 achieving impressive capabilities in various linguistic tasks. However, training models on such a large scale is challenging, and finding datasets that match the model's scale is often difficult. Fine-tuning and training models with fewer parameters using novel methods have emerged as promising approaches to overcome these challenges. One such model is MiniGPT-4, which achieves comparable vision-language understanding to GPT-4 by leveraging novel pre-training models and innovative training strategies. However, the model still faces some challenges in image understanding, particularly in artistic pictures. A novel multimodal model called ArtGPT-4 has been proposed to address these limitations. ArtGPT-4 was trained on image-text pairs using a Tesla A100 device in just 2 hours, using only about 200 GB of data. The model can depict images wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#22495;&#38750;&#33521;&#35821;&#35821;&#35328;&#23545;&#35805;&#31995;&#32479;&#20013;&#23569;&#37327;&#25968;&#25454;&#19979;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#20845;&#31181;&#35821;&#35328;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07393</link><description>&lt;p&gt;
&#38024;&#23545;&#24320;&#25918;&#22495;&#23545;&#35805;&#29983;&#25104;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#20013;&#38477;&#20302;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt Learning to Mitigate Catastrophic Forgetting in Cross-lingual Transfer for Open-domain Dialogue Generation. (arXiv:2305.07393v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#22495;&#38750;&#33521;&#35821;&#35821;&#35328;&#23545;&#35805;&#31995;&#32479;&#20013;&#23569;&#37327;&#25968;&#25454;&#19979;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#20845;&#31181;&#35821;&#35328;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#23545;&#35805;&#31995;&#32479;&#19968;&#30452;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#31532;&#19968;&#27425;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#26377;&#38480;&#25968;&#25454;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#29983;&#25104;&#20013;&#30740;&#31350;&#20102;&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#65288;FS-XLT&#65289;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#12290;&#22312;&#21021;&#27493;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;FS-XLT&#21644;MTL&#20013;&#25152;&#26377;&#30340;6&#31181;&#35821;&#35328;&#20013;&#37117;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22266;&#23450;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#35843;&#21442;&#21644;&#25105;&#20204;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#35821;&#26469;&#24357;&#21512;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#20445;&#25345;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;mPLM&#65289;&#22312;FS-XLT&#21644;MTL&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#12290;&#22312;&#25152;&#26377;6&#31181;&#35821;&#35328;&#19978;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#37117;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/JeremyLeiLiu/XLinguDial &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue systems for non-English languages have long been under-explored. In this paper, we take the first step to investigate few-shot cross-lingual transfer learning (FS-XLT) and multitask learning (MTL) in the context of open-domain dialogue generation for non-English languages with limited data. We observed catastrophic forgetting in both FS-XLT and MTL for all 6 languages in our preliminary experiments. To mitigate the issue, we propose a simple yet effective prompt learning approach that can preserve the multilinguality of multilingual pre-trained language model (mPLM) in FS-XLT and MTL by bridging the gap between pre-training and fine-tuning with Fixed-prompt LM Tuning and our hand-crafted prompts. Experimental results on all 6 languages in terms of both automatic and human evaluations demonstrate the effectiveness of our approach. Our code is available at https://github.com/JeremyLeiLiu/XLinguDial.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#25913;&#36827;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#29983;&#20135;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05402</link><description>&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Consistent Text Categorization using Data Augmentation in e-Commerce. (arXiv:2305.05402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#25913;&#36827;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#29983;&#20135;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#20998;&#31867;&#26159;&#19968;&#39033;&#20851;&#38190;&#30340;&#12289;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#19968;&#23478;&#20027;&#35201;&#32593;&#32476;&#20844;&#21496;&#24050;&#32463;&#22312;&#20351;&#29992;&#30340;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#12290;&#22312;&#35813;&#27169;&#22411;&#26680;&#24515;&#20013;&#65292;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#26159;&#19968;&#20010;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65292;&#25509;&#21463;&#20135;&#21697;&#26631;&#39064;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20174;&#25968;&#21315;&#20010;&#21487;&#29992;&#20505;&#36873;&#39033;&#20013;&#36755;&#20986;&#26368;&#21512;&#36866;&#30340;&#31867;&#21035;&#12290;&#32463;&#36807;&#36827;&#19968;&#27493;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31867;&#20284;&#29289;&#21697;&#26631;&#31614;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65292;&#26631;&#39064;&#20013;&#20851;&#20110;&#39068;&#33394;&#25110;&#23610;&#23544;&#30340;&#23567;&#21464;&#21270;&#65292;&#20250;&#23545;&#27169;&#22411;&#20135;&#29983;&#36739;&#22823;&#24433;&#21709;&#12290;&#36825;&#31181;&#29616;&#35937;&#21487;&#33021;&#20250;&#23545;&#19979;&#28216;&#30340;&#25512;&#33616;&#25110;&#25628;&#32034;&#24212;&#29992;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#20445;&#25345;&#20854;&#29983;&#20135;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications. At its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we found inconsistencies in the labeling of similar items. For example, minor modifications of the product title pertaining to colors or measurements majorly impacted the model's output. This phenomenon can negatively affect downstream recommendation or search applications, leading to a sub-optimal user experience.  To address this issue, we propose a new framework for consistent text categorization. Our goal is to improve the model's consistency while maintaining its production-level performance. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04928</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#26377;&#30417;&#30563;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20381;&#36182;&#20110;&#20855;&#26377;&#32473;&#23450;&#21629;&#21517;&#23454;&#20307;&#30340;&#22823;&#37327;&#27880;&#37322;&#25991;&#26412;&#65292;&#20854;&#21019;&#24314;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#25552;&#21462;&#26032;&#23454;&#20307;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#27880;&#37322;&#20219;&#21153;&#21644;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65288;&#26631;&#35760;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#25110;&#19981;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#65289;&#65292;&#24182;&#22312;&#26356;&#22810;&#30340;&#25968;&#25454;&#38598;&#21644;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#21487;&#23398;&#20064;&#21040;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;9&#31181;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#65292;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;NER&#12289;&#19968;&#27425;&#26679;&#26412;NER&#12289;10&#27425;&#26679;&#26412;NER&#21644;100&#27425;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;F1&#24471;&#20998;&#20998;&#21035;&#20026;35.44&#65285;&#12289;50.10&#65285;&#12289;69.94&#65285;&#21644;79.51&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35821;&#20041;&#24863;&#30693;&#30340;&#20840;&#23616;&#36866;&#24212;&#24490;&#29615;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#23548;&#35821;&#35328;&#27169;&#22359;&#21644;&#22806;&#35266;&#35821;&#20041;&#35270;&#35273;&#27169;&#22359;&#65292;&#20197;&#21450;&#20840;&#23616;&#33258;&#36866;&#24212;&#32858;&#21512;&#27169;&#22359;&#65292;&#25552;&#39640;&#20102;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#35821;&#20041;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.03602</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#37325;&#35821;&#20041;&#24863;&#30693;&#30340;&#20840;&#23616;&#36866;&#24212;&#24490;&#29615;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
A Dual Semantic-Aware Recurrent Global-Adaptive Network For Vision-and-Language Navigation. (arXiv:2305.03602v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35821;&#20041;&#24863;&#30693;&#30340;&#20840;&#23616;&#36866;&#24212;&#24490;&#29615;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#23548;&#35821;&#35328;&#27169;&#22359;&#21644;&#22806;&#35266;&#35821;&#20041;&#35270;&#35273;&#27169;&#22359;&#65292;&#20197;&#21450;&#20840;&#23616;&#33258;&#36866;&#24212;&#32858;&#21512;&#27169;&#22359;&#65292;&#25552;&#39640;&#20102;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#35821;&#20041;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#26159;&#19968;&#39033;&#29616;&#23454;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20195;&#29702;&#20351;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;&#32447;&#32034;&#23450;&#20301;&#30446;&#26631;&#21306;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35821;&#20041;&#24863;&#30693;&#30340;&#20840;&#23616;&#36866;&#24212;&#24490;&#29615;&#32593;&#32476; (DSRG) &#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;DSRG &#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#23548;&#35821;&#35328;&#27169;&#22359; (IGL) &#21644;&#19968;&#20010;&#22806;&#35266;&#35821;&#20041;&#35270;&#35273;&#27169;&#22359; (ASV) &#20998;&#21035;&#25552;&#39640;&#35270;&#35273;&#21644;&#35821;&#35328;&#35821;&#20041;&#23398;&#20064;&#12290;&#23545;&#20110;&#20869;&#23384;&#26426;&#21046;&#65292;&#36824;&#24341;&#20837;&#20102;&#20840;&#23616;&#33258;&#36866;&#24212;&#32858;&#21512;&#27169;&#22359; (GAA)&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-and-Language Navigation (VLN) is a realistic but challenging task that requires an agent to locate the target region using verbal and visual cues. While significant advancements have been achieved recently, there are still two broad limitations: (1) The explicit information mining for significant guiding semantics concealed in both vision and language is still under-explored; (2) The previously structured map method provides the average historical appearance of visited nodes, while it ignores distinctive contributions of various images and potent information retention in the reasoning process. This work proposes a dual semantic-aware recurrent global-adaptive network (DSRG) to address the above problems. First, DSRG proposes an instruction-guidance linguistic module (IGL) and an appearance-semantics visual module (ASV) for boosting vision and language semantic learning respectively. For the memory mechanism, a global adaptive aggregation module (GAA) is devised for explicit pano
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;Bird&#65292;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#25991;&#26412;&#21040;SQL&#30340;&#20219;&#21153;&#65292;&#31361;&#20986;&#20102;&#25968;&#25454;&#24211;&#20540;&#29702;&#35299;&#21644;SQL&#25928;&#29575;&#31561;&#39046;&#22495;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.03111</link><description>&lt;p&gt;
LLM&#33021;&#21542;&#20316;&#20026;&#25968;&#25454;&#24211;&#25509;&#21475;&#65311;&#22823;&#22411;&#25968;&#25454;&#24211;&#22522;&#30784;&#25991;&#26412;&#21040;SQL&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs. (arXiv:2305.03111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;Bird&#65292;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#25991;&#26412;&#21040;SQL&#30340;&#20219;&#21153;&#65292;&#31361;&#20986;&#20102;&#25968;&#25454;&#24211;&#20540;&#29702;&#35299;&#21644;SQL&#25928;&#29575;&#31561;&#39046;&#22495;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#23558;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36716;&#25442;&#20026;&#21487;&#25191;&#34892;&#30340;SQL&#21629;&#20196;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#22522;&#20934;&#27979;&#35797;Bird&#65292;&#23427;&#21253;&#21547;&#26088;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#22522;&#30784;&#30340;12,751&#23545;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#21644;95&#20010;&#25968;&#25454;&#24211;&#65292;&#24635;&#22823;&#23567;&#20026;33.4GB&#65292;&#28085;&#30422;37&#20010;&#19987;&#19994;&#39046;&#22495;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;Bird&#24378;&#35843;&#25968;&#25454;&#24211;&#20540;&#30340;&#29702;&#35299;&#65292;&#31361;&#20986;&#20102;&#33039;&#25968;&#25454;&#24211;&#20869;&#23481;&#12289;NL&#38382;&#39064;&#21644;&#25968;&#25454;&#24211;&#20869;&#23481;&#20043;&#38388;&#30340;&#22806;&#37096;&#30693;&#35782;&#20197;&#21450;SQL&#25928;&#29575;&#31561;&#26032;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#24517;&#39035;&#20855;&#22791;&#25968;&#25454;&#24211;&#20540;&#29702;&#35299;&#21644;&#35821;&#20041;&#35299;&#26512;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs, has gained increasing attention in recent years. In particular, Codex and ChatGPT have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database contents leaving the gap between academic study and real-world applications. To mitigate this gap, we present Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks, containing 12,751 pairs of text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty database contents, external knowledge between NL questions and database contents, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#65292;&#35299;&#20915;&#24403;&#21069;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#30340;&#30740;&#31350;&#23384;&#22312;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#19981;&#31995;&#32479;&#21644;&#19981;&#21516;&#27880;&#37322;&#26041;&#26696;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02637</link><description>&lt;p&gt;
&#38754;&#21521;&#36328;&#25968;&#25454;&#38598;&#30340;&#24369;&#30417;&#30563;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Weakly-Supervised Hate Speech Classification Across Datasets. (arXiv:2305.02637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02637
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#65292;&#35299;&#20915;&#24403;&#21069;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#30340;&#30740;&#31350;&#23384;&#22312;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#19981;&#31995;&#32479;&#21644;&#19981;&#21516;&#27880;&#37322;&#26041;&#26696;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#22810;&#20301;&#23398;&#32773;&#25351;&#20986;&#30340;&#37027;&#26679;&#65292;&#24403;&#21069;&#38024;&#23545;&#20167;&#24680;&#35328;&#35770;&#65288;HS&#65289;&#35782;&#21035;&#30340;&#30740;&#31350;&#29305;&#28857;&#26159;&#19981;&#31995;&#32479;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#21644;&#19981;&#21516;&#30340;&#27880;&#37322;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#23545;&#23427;&#20204;&#26410;&#34987;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27867;&#21270;&#24615;&#33021;&#24046;&#65292;&#24182;&#19988;&#19981;&#21516;HS&#20998;&#31867;&#27861;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#25152;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#26080;&#27861;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#20869;&#21644;&#36328;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;HS&#20998;&#31867;&#27169;&#22411;&#36890;&#29992;&#24615;&#36739;&#24046;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
As pointed out by several scholars, current research on hate speech (HS) recognition is characterized by unsystematic data creation strategies and diverging annotation schemata. Subsequently, supervised-learning models tend to generalize poorly to datasets they were not trained on, and the performance of the models trained on datasets labeled using different HS taxonomies cannot be compared. To ease this problem, we propose applying extremely weak supervision that only relies on the class name rather than on class samples from the annotated data. We demonstrate the effectiveness of a state-of-the-art weakly-supervised text classification model in various in-dataset and cross-dataset settings. Furthermore, we conduct an in-depth quantitative and qualitative analysis of the source of poor generalizability of HS classification models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00586</link><description>&lt;p&gt;
GPT-2&#26159;&#22914;&#20309;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#30340;&#65311;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#21151;&#33021;&#21364;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#30340;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20197;GPT-2 Small&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#33021;&#21542;&#36890;&#36807;&#36755;&#20837;"&#25112;&#20105;&#25345;&#32493;&#26102;&#38388;&#26159;&#20174;1732&#24180;&#21040;17&#24180;"&#65292;&#39044;&#27979;&#20986;&#26377;&#25928;&#30340;&#20004;&#20301;&#25968;&#23383;&#30340;&#25130;&#27490;&#24180;&#20221; (&#22823;&#20110;32&#24180;)&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#30005;&#36335;&#65292;&#21363;GPT-2 Small&#35745;&#31639;&#22270;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#29992;&#20110;&#35745;&#31639;&#36825;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#25105;&#20204;&#35299;&#37322;&#20102;&#27599;&#20010;&#30005;&#36335;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;GPT-2 Small&#30340;&#26368;&#32456;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#22312;&#20854;&#20182;&#22823;&#20110;&#22330;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years &gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#65289;&#30340;&#22238;&#24212;&#65292;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#22823;&#37327;&#30340;L2&#29992;&#25143;&#24433;&#21709;&#35821;&#35328;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00217</link><description>&lt;p&gt;
&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#24180;&#65289;&#30340;&#22238;&#24212;&#65306;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#35777;&#26126;&#38750;&#27597;&#35821;&#29992;&#25143;&#27604;&#20363;&#23545;&#35821;&#35328;&#22797;&#26434;&#24230;&#26377;&#24433;&#21709;&#65288;arXiv:2305.00217v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus &amp; Walkden (2023). (arXiv:2305.00217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#65289;&#30340;&#22238;&#24212;&#65292;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#22823;&#37327;&#30340;L2&#29992;&#25143;&#24433;&#21709;&#35821;&#35328;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#12298;&#35821;&#35328;&#36827;&#21270;&#26434;&#24535;&#12299;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#20013;&#65292;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;https://doi.org/10.1093/jole/lzad005&#65292;KEW&#65289;&#25361;&#25112;&#20102;&#25105;&#22312;&#19968;&#31687;&#35770;&#25991;&#20013;&#65288;Koplenig&#65292;Royal Society Open Science&#65292;6&#65292;181274&#65288;2019&#65289;&#65292;https://doi.org/10.1098/rsos.181274&#65289;&#25152;&#21576;&#29616;&#30340;&#32467;&#26524;&#12290;&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#25105;&#35797;&#22270;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#32479;&#35745;&#20998;&#26512;&#26469;&#34920;&#26126;&#22823;&#37327;L2&#65288;&#31532;&#20108;&#35821;&#35328;&#65289;&#29992;&#25143;&#20284;&#20046;&#19981;&#20250;&#24433;&#21709;&#35821;&#35328;&#30340;&#65288;&#35821;&#27861;&#25110;&#32479;&#35745;&#65289;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#19987;&#27880;&#20110;Ethnologue&#35780;&#20272;&#35821;&#35328;&#22320;&#20301;&#30340;&#26041;&#24335;&#65306;&#22914;&#26524;&#19968;&#31181;&#35821;&#35328;&#38500;&#20102;&#34987;L1&#65288;&#31532;&#19968;&#35821;&#35328;&#65289;&#20351;&#29992;&#32773;&#20043;&#22806;&#65292;&#36824;&#24212;&#35813;&#26377;&#22823;&#37327;&#30340;L2&#20351;&#29992;&#32773;&#65292;&#37027;&#20040;&#35813;&#35821;&#35328;&#23601;&#34987;&#25551;&#36848;&#20026;&#20256;&#25773;&#24615;&#30340;&#12290;KEW&#25209;&#35780;&#20102;&#23558;&#20256;&#25773;&#24615;&#20316;&#20026;&#35821;&#35328;&#26159;&#21542;&#25317;&#26377;&#22823;&#37327;L2&#20351;&#29992;&#32773;&#65288;&#20108;&#20803;&#65289;&#25351;&#26631;&#30340;&#20351;&#29992;&#65292;&#20197;&#21450;&#22312;&#30452;&#25509;&#20272;&#35745;L2&#27604;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;L2&#29992;&#25143;&#27604;&#20363;&#24402;&#20026;&#38750;&#20256;&#25773;&#24615;&#35821;&#35328;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a recent paper published in the Journal of Language Evolution, Kauhanen, Einhaus &amp; Walkden (https://doi.org/10.1093/jole/lzad005, KEW) challenge the results presented in one of my papers (Koplenig, Royal Society Open Science, 6, 181274 (2019), https://doi.org/10.1098/rsos.181274), in which I tried to show through a series of statistical analyses that large numbers of L2 (second language) speakers do not seem to affect the (grammatical or statistical) complexity of a language. To this end, I focus on the way in which the Ethnologue assesses language status: a language is characterised as vehicular if, in addition to being used by L1 (first language) speakers, it should also have a significant number of L2 users. KEW criticise both the use of vehicularity as a (binary) indicator of whether a language has a significant number of L2 users and the idea of imputing a zero proportion of L2 speakers to non-vehicular languages whenever a direct estimate of that proportion is unavailable. Whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24207;&#21015;&#36716;&#23548;&#26550;&#26500;TDT&#65292;&#23427;&#21487;&#20197;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#27604;&#20256;&#32479;Transducers&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#30528;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06795</link><description>&lt;p&gt;
Token-and-Duration Transducer&#26550;&#26500;&#65306;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#19982;&#26102;&#38271;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;
&lt;/p&gt;
&lt;p&gt;
Efficient Sequence Transduction by Jointly Predicting Tokens and Durations. (arXiv:2304.06795v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24207;&#21015;&#36716;&#23548;&#26550;&#26500;TDT&#65292;&#23427;&#21487;&#20197;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#27604;&#20256;&#32479;Transducers&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#30528;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#30340;&#26032;&#22411;Token-and-Duration Transducer(TDT)&#26550;&#26500;&#12290;TDT&#36890;&#36807;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#21363;&#21457;&#23556;&#30340;&#26631;&#35760;&#35206;&#30422;&#30340;&#36755;&#20837;&#24103;&#30340;&#25968;&#37327;&#65292;&#26469;&#25193;&#23637;&#20256;&#32479;&#30340;RNN-Transducer&#26550;&#26500;&#12290;&#23427;&#20351;&#29992;&#20855;&#26377;&#20004;&#20010;&#29420;&#31435;&#26631;&#20934;&#21270;&#36755;&#20986;&#30340;&#32852;&#21512;&#32593;&#32476;&#26469;&#29983;&#25104;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#20998;&#24067;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;TDT&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#30340;&#25345;&#32493;&#26102;&#38388;&#36755;&#20986;&#36339;&#36807;&#36755;&#20837;&#24103;&#65292;&#20351;&#20854;&#27604;&#36880;&#24103;&#22788;&#29702;&#32534;&#30721;&#22120;&#36755;&#20986;&#30340;&#20256;&#32479;Transducers&#26174;&#30528;&#26356;&#24555;&#12290;&#22312;&#19981;&#21516;&#30340;&#24207;&#21015;&#36716;&#23548;&#20219;&#21153;&#19978;&#65292;TDT&#27169;&#22411;&#22343;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#30528;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#35821;&#38899;&#35782;&#21035;&#30340;TDT&#27169;&#22411;&#27604;RNN-Transducers&#33719;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25512;&#29702;&#36895;&#24230;&#39640;&#36798;2.82&#20493;&#12290;&#35821;&#38899;&#32763;&#35793;&#30340;TDT&#27169;&#22411;&#19982;MUST-C&#27979;&#35797;&#30456;&#27604;&#25552;&#39640;&#20102;1&#20010;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than RNN-Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#25324;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;Bengali&#25991;&#26412;&#30340;&#26032;&#25968;&#25454;&#38598;- BenCoref&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;Bengali&#22810;&#20010;&#39046;&#22495;&#20013;&#20849;&#25351;&#28040;&#35299;&#29616;&#35937;&#30340;&#24046;&#24322;&#65292;&#24182;&#20419;&#36827;Bengali&#30340;&#36164;&#28304;&#24320;&#21457;&#12290;&#22810;&#20010;&#27169;&#22411;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#24615;&#33021;&#20063;&#24471;&#21040;&#20102;&#25253;&#21578;&#12290;&#22312;&#36328;&#35821;&#35328;&#27979;&#35797;&#20013;&#65292;&#20174;&#33521;&#35821;&#21040;Bengali&#30340;&#20132;&#21449;&#35821;&#35328;&#24615;&#33021;&#36739;&#24046;&#65292;&#26174;&#31034;&#20986;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#30340;&#20849;&#25351;&#28040;&#35299;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2304.03682</link><description>&lt;p&gt;
BenCoref:&#19968;&#31181;&#21517;&#35789;&#30701;&#35821;&#21644;&#20195;&#35789;&#25351;&#20195;&#27880;&#37322;&#30340;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BenCoref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference Annotations. (arXiv:2304.03682v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#25324;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;Bengali&#25991;&#26412;&#30340;&#26032;&#25968;&#25454;&#38598;- BenCoref&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;Bengali&#22810;&#20010;&#39046;&#22495;&#20013;&#20849;&#25351;&#28040;&#35299;&#29616;&#35937;&#30340;&#24046;&#24322;&#65292;&#24182;&#20419;&#36827;Bengali&#30340;&#36164;&#28304;&#24320;&#21457;&#12290;&#22810;&#20010;&#27169;&#22411;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#24615;&#33021;&#20063;&#24471;&#21040;&#20102;&#25253;&#21578;&#12290;&#22312;&#36328;&#35821;&#35328;&#27979;&#35797;&#20013;&#65292;&#20174;&#33521;&#35821;&#21040;Bengali&#30340;&#20132;&#21449;&#35821;&#35328;&#24615;&#33021;&#36739;&#24046;&#65292;&#26174;&#31034;&#20986;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#30340;&#20849;&#25351;&#28040;&#35299;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#25351;&#28040;&#35299;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;Bengali &#30340;&#20849;&#25351;&#28040;&#35299;&#30740;&#31350;&#20027;&#35201;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;BenCoref&#65292;&#21253;&#25324;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;Bengali&#25991;&#26412;&#30340;&#20849;&#25351;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;5200&#20010;&#25552;&#21450;&#27880;&#37322;&#65292;&#24418;&#25104;48,569&#20010;&#26631;&#35760;&#20013;&#30340;502&#20010;&#25552;&#21450;&#31751;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#21019;&#24314;&#27492;&#25968;&#25454;&#38598;&#30340;&#36807;&#31243;&#65292;&#24182;&#25253;&#21578;&#20102;&#20351;&#29992;BenCoref&#35757;&#32451;&#30340;&#22810;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#25581;&#31034;Bengali&#22810;&#20010;&#39046;&#22495;&#20013;&#20849;&#25351;&#29616;&#35937;&#30340;&#24046;&#24322;&#65292;&#24182;&#40723;&#21169;&#24320;&#21457;&#20854;&#20182;Bengali&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20174;&#33521;&#35821;&#21040;Bengali&#30340;&#20132;&#21449;&#35821;&#35328;&#24615;&#33021;&#24456;&#24046;&#65292;&#36825;&#31361;&#26174;&#20986;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#30340;&#20849;&#25351;&#28040;&#35299;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coreference Resolution is a well studied problem in NLP. While widely studied for English and other resource-rich languages, research on coreference resolution in Bengali largely remains unexplored due to the absence of relevant datasets. Bengali, being a low-resource language, exhibits greater morphological richness compared to English. In this article, we introduce a new dataset, BenCoref, comprising coreference annotations for Bengali texts gathered from four distinct domains. This relatively small dataset contains 5200 mention annotations forming 502 mention clusters within 48,569 tokens. We describe the process of creating this dataset and report performance of multiple models trained using BenCoref. We anticipate that our work sheds some light on the variations in coreference phenomena across multiple domains in Bengali and encourages the development of additional resources for Bengali. Furthermore, we found poor crosslingual performance at zero-shot setting from English, highlig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;: ContraSim&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;ContraSim&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#33719;&#24471;&#20102;&#27604;&#20043;&#21069;&#30456;&#20284;&#24230;&#37327;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16992</link><description>&lt;p&gt;
ContraSim -- &#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ContraSim -- A Similarity Measure Based on Contrastive Learning. (arXiv:2303.16992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;: ContraSim&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;ContraSim&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#33719;&#24471;&#20102;&#27604;&#20043;&#21069;&#30456;&#20284;&#24230;&#37327;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#26512;&#27604;&#36739;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#26041;&#38754;&#65288;&#22914;&#26550;&#26500;&#12289;&#35757;&#32451;&#25968;&#25454;&#31561;&#65289;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#30456;&#20284;&#24230;&#37327;&#30340;&#36136;&#37327;&#36890;&#24120;&#36890;&#36807;&#20854;&#22312;&#39044;&#26399;&#21305;&#37197;&#30340;&#34920;&#31034;&#20013;&#20998;&#37197;&#39640;&#20998;&#25968;&#30340;&#25104;&#21151;&#26469;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30456;&#20284;&#24230;&#37327;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24179;&#24248;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;ContraSim&#65292;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#65292;&#19982;&#24120;&#35265;&#30340;&#38381;&#24335;&#30456;&#20284;&#24615;&#24230;&#37327;&#19981;&#21516;&#65292;ContraSim&#20351;&#29992;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#31034;&#20363;&#26469;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#22270;&#23618;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#21644;&#25105;&#20204;&#20171;&#32461;&#30340;&#20004;&#20010;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#20351;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65306;&#22810;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#21644;&#22270;&#20687;&#23383;&#24149;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;ContraSim&#30340;&#20934;&#30830;&#24615;&#37117;&#27604;&#20043;&#21069;&#30340;&#30456;&#20284;&#24230;&#37327;&#26041;&#27861;&#39640;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has compared neural network representations via similarity-based analyses, shedding light on how different aspects (architecture, training data, etc.) affect models' internal representations. The quality of a similarity measure is typically evaluated by its success in assigning a high score to representations that are expected to be matched. However, existing similarity measures perform mediocrely on standard benchmarks. In this work, we develop a new similarity measure, dubbed ContraSim, based on contrastive learning. In contrast to common closed-form similarity measures, ContraSim learns a parameterized measure by using both similar and dissimilar examples. We perform an extensive experimental evaluation of our method, with both language and vision models, on the standard layer prediction benchmark and two new benchmarks that we introduce: the multilingual benchmark and the image-caption benchmark. In all cases, ContraSim achieves much higher accuracy than previous simila
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;GPT-3&#27169;&#22411;&#22312;&#35821;&#27861;&#32416;&#38169;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#23454;&#39564;&#27979;&#35797;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#24335;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#35780;&#20998;&#32773;&#19982;&#22522;&#20110;&#21442;&#32771;&#30340;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.14342</link><description>&lt;p&gt;
GPT-3&#22312;&#35821;&#27861;&#32416;&#38169;&#19978;&#24615;&#33021;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of GPT-3's Performance in Grammatical Error Correction. (arXiv:2303.14342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;GPT-3&#27169;&#22411;&#22312;&#35821;&#27861;&#32416;&#38169;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#23454;&#39564;&#27979;&#35797;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#24335;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#35780;&#20998;&#32773;&#19982;&#22522;&#20110;&#21442;&#32771;&#30340;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-3&#27169;&#22411;&#20855;&#26377;&#24456;&#39640;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#23427;&#20204;&#22312;&#35821;&#27861;&#32416;&#38169;(GEC)&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#32570;&#20047;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;GPT-3&#27169;&#22411;&#65288;text-davinci-003&#29256;&#26412;&#65289;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27979;&#35797;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#24335;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#26684;&#24335;&#36935;&#21040;&#30340;&#26377;&#36259;&#25110;&#26377;&#38382;&#39064;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#31867;&#35780;&#20215;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#25253;&#21578;&#20102;&#25105;&#20204;&#26368;&#20339;&#25552;&#31034;&#22312;BEA-2019&#21644;JFLEG&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#35780;&#20998;&#32773;&#19982;&#22522;&#20110;&#21442;&#32771;&#30340;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#26377;&#36259;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-3 models are very powerful, achieving high performance on a variety of natural language processing tasks. However, there is a relative lack of detailed published analysis on how well they perform on the task of grammatical error correction (GEC). To address this, we perform experiments testing the capabilities of a GPT-3 model (text-davinci-003) against major GEC benchmarks, comparing the performance of several different prompts, including a comparison of zero-shot and few-shot settings. We analyze intriguing or problematic outputs encountered with different prompt formats. We report the performance of our best prompt on the BEA-2019 and JFLEG datasets using a combination of automatic metrics and human evaluations, revealing interesting differences between the preferences of human raters and the reference-based automatic metrics.
&lt;/p&gt;</description></item><item><title>CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.08127</link><description>&lt;p&gt;
CB2&#65306;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08127
&lt;/p&gt;
&lt;p&gt;
CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CB2 &#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#24773;&#22659;&#19979;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010; 3D &#28216;&#25103;&#29615;&#22659;&#12289;&#19968;&#20010;&#21518;&#31471;&#26381;&#21153;&#22120;&#65292;&#21487;&#20026;&#20154;&#31867;&#26234;&#33021;&#20307;&#25552;&#20379;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#65292;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312; https://cb2.ai &#19978;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#26377;&#23398;&#20064;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#31995;&#32479;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36880;&#23618;&#32858;&#21512;&#24322;&#24120;&#24471;&#20998;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#36827;&#34892;&#25991;&#26412;OOD&#26816;&#27979;&#12290;&#20854;&#33021;&#21457;&#25496;&#19981;&#21516;&#23618;&#36755;&#20986;&#30340;&#20248;&#21183;&#65292;&#36798;&#21040;&#26356;&#40065;&#26834;&#30340;&#24615;&#33021;&#65292;&#24182;&#25193;&#23637;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#20197;&#21453;&#26144;&#26356;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2302.09852</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#36880;&#23618;&#25991;&#26412;OOD&#26816;&#27979;&#24471;&#20998;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Layer-wise Score Aggregation for Textual OOD Detection. (arXiv:2302.09852v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09852
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36880;&#23618;&#32858;&#21512;&#24322;&#24120;&#24471;&#20998;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#36827;&#34892;&#25991;&#26412;OOD&#26816;&#27979;&#12290;&#20854;&#33021;&#21457;&#25496;&#19981;&#21516;&#23618;&#36755;&#20986;&#30340;&#20248;&#21183;&#65292;&#36798;&#21040;&#26356;&#40065;&#26834;&#30340;&#24615;&#33021;&#65292;&#24182;&#25193;&#23637;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#20197;&#21453;&#26144;&#26356;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#22522;&#20110;AI&#30340;&#31995;&#32479;&#22686;&#21152;&#65292;OOD&#26816;&#27979;&#26159;&#19968;&#20010;&#36805;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#30001;&#20110;&#26032;&#30340;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#35201;&#27714;&#12290;&#29616;&#26377;&#30340;OOD&#25991;&#26412;&#26816;&#27979;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#22312;&#32534;&#30721;&#22120;&#30340;&#26368;&#21518;&#19968;&#23618;&#36755;&#20986;&#19978;&#35745;&#31639;&#30340;&#24322;&#24120;&#24471;&#20998;&#65288;&#20363;&#22914;&#39532;&#27663;&#36317;&#31163;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;OOD&#26816;&#27979;&#24615;&#33021;&#22240;&#20219;&#21153;&#21644;&#23618;&#36755;&#20986;&#32780;&#24322;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#24120;&#30340;&#36873;&#25321;&#65288;&#26368;&#21518;&#19968;&#23618;&#65289;&#24456;&#23569;&#26159;OOD&#26816;&#27979;&#30340;&#26368;&#20339;&#36873;&#25321;&#65292;&#22914;&#26524;&#36873;&#25321;&#26368;&#20339;&#23618;&#65292;&#21017;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#32467;&#21512;&#36880;&#23618;&#30340;&#24322;&#24120;&#24471;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21253;&#25324;&#26356;&#22810;&#31867;&#21035;&#30340;&#20998;&#31867;&#20219;&#21153;&#65288;&#39640;&#36798;77&#65289;&#25193;&#23637;&#20102;&#32463;&#20856;&#25991;&#26412;OOD&#22522;&#20934;&#27979;&#35797;&#65292;&#20174;&#32780;&#21453;&#26144;&#26356;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;&#22312;&#36825;&#20010;&#22686;&#24378;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#21518;&#32858;&#21512;&#26041;&#27861;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;OOD&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is a rapidly growing field due to new robustness and security requirements driven by an increased number of AI-based systems. Existing OOD textual detectors often rely on an anomaly score (e.g., Mahalanobis distance) computed on the embedding output of the last layer of the encoder. In this work, we observe that OOD detection performance varies greatly depending on the task and layer output. More importantly, we show that the usual choice (the last layer) is rarely the best one for OOD detection and that far better results could be achieved if the best layer were picked. To leverage this observation, we propose a data-driven, unsupervised method to combine layer-wise anomaly scores. In addition, we extend classical textual OOD benchmarks by including classification tasks with a greater number of classes (up to 77), which reflects more realistic settings. On this augmented benchmark, we show that the proposed post-aggregation methods achieve robust an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#26368;&#32456;&#30340;ST&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.11716</link><description>&lt;p&gt;
&#22522;&#20110;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pre-training for Speech Translation: CTC Meets Optimal Transport. (arXiv:2301.11716v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#26368;&#32456;&#30340;ST&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;(ST)&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#26080;&#38656;&#25913;&#21464;ST&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#34920;&#26126;&#36830;&#25509;&#26102;&#24207;&#20998;&#31867;(CTC)&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26469;&#20943;&#23567;&#27169;&#24577;&#24046;&#36317;&#12290;&#36890;&#36807;&#19982;&#26356;&#24120;&#35265;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#23450;&#37327;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;CTC&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22987;&#32456;&#23454;&#29616;&#26356;&#22909;&#30340;&#26368;&#32456;ST&#20934;&#30830;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#20943;&#23567;&#36825;&#31181;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#39044;&#35757;&#32451;&#30456;&#23545;&#20110;&#20165;&#20351;&#29992;CTC&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#27809;&#26377;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#22343;&#33021;&#22815;&#25552;&#20379;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The gap between speech and text modalities is a major challenge in speech-to-text translation (ST). Different methods have been proposed to reduce this gap, but most of them require architectural changes in ST training. In this work, we propose to mitigate this issue at the pre-training stage, requiring no change in the ST model. First, we show that the connectionist temporal classification (CTC) loss can reduce the modality gap by design. We provide a quantitative comparison with the more common cross-entropy loss, showing that pre-training with CTC consistently achieves better final ST accuracy. Nevertheless, CTC is only a partial solution and thus, in our second contribution, we propose a novel pre-training method combining CTC and optimal transport to further reduce this gap. Our method pre-trains a Siamese-like model composed of two encoders, one for acoustic inputs and the other for textual inputs, such that they produce representations that are close to each other in the Wassers
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#36719;&#25552;&#31034;&#20196;&#29260;&#23884;&#20837;&#26469;&#23398;&#20064;&#20219;&#21153;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#20302;&#36164;&#28304;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65292;&#21516;&#26102;&#22312;&#19981;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2301.10915</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#20302;&#36164;&#28304;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning. (arXiv:2301.10915v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10915
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#36719;&#25552;&#31034;&#20196;&#29260;&#23884;&#20837;&#26469;&#23398;&#20064;&#20219;&#21153;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#20302;&#36164;&#28304;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65292;&#21516;&#26102;&#22312;&#19981;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#26159;&#23545;&#35805;&#31649;&#29702;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#65292;&#38656;&#35201;&#36319;&#36394;&#29992;&#25143;&#30340;&#20449;&#24565;&#29366;&#24577;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#23545;&#25152;&#26377;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#26469;&#24212;&#23545;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20219;&#21153;&#12290;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#65292;&#38656;&#35201;&#20026;&#19981;&#21516;&#30340;&#39046;&#22495;&#21644;&#20219;&#21153;&#20351;&#29992;&#20960;&#21313;&#20010;&#24494;&#35843;&#20102;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25152;&#38656;&#30340;&#25104;&#26412;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20026;&#20102;&#38477;&#20302;&#21442;&#25968;&#22823;&#23567;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#36328;&#20219;&#21153;&#20849;&#20139;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36719;&#25552;&#31034;&#20196;&#29260;&#23884;&#20837;&#26469;&#23398;&#20064;&#20219;&#21153;&#23646;&#24615;&#12290;&#22312;&#19981;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21442;&#25968;&#25968;&#37327;&#22823;&#24133;&#20943;&#23569;&#21040;&#23569;&#20110;&#20043;&#21069;&#26041;&#27861;&#30340;0.5&#65285;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20302;&#36164;&#28304;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue state tracking (DST) is an important step in dialogue management to keep track of users' beliefs. Existing works fine-tune all language model (LM) parameters to tackle the DST task, which requires significant data and computing resources for training and hosting. The cost grows exponentially in the real-world deployment where dozens of fine-tuned LM are used for different domains and tasks. To reduce parameter size and better utilize cross-task shared information, we propose to use soft prompt token embeddings to learn task properties. Without tuning LM parameters, our method drastically reduces the number of parameters needed to less than 0.5% of prior works while achieves better low-resource DST performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10823</link><description>&lt;p&gt;
&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#20381;&#36182;&#32467;&#26500;&#21270;&#27880;&#37322;&#35821;&#26009;&#24211;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#26399;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#30340;RE&#65292;&#20854;&#20013;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#36890;&#36807;RE&#30446;&#26631;&#39044;&#35757;&#32451;&#20851;&#31995;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#23545;&#26377;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23427;&#38459;&#27490;RE&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#26088;&#22312;&#24357;&#21512;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#12290;&#30001;&#20110;&#22312;&#36825;&#31181;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#19968;&#20010;&#20851;&#31995;&#21487;&#33021;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#36731;&#26494;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#65292;&#22240;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#65292;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#25991;&#26723;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#26174;&#30528;&#25552;&#39640;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the relation embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two docum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;CTC&#12289;Attention&#12289;RNN-T&#21644;Mask-Predict&#22235;&#20010;&#35299;&#30721;&#22120;&#30340;&#32852;&#21512;&#24314;&#27169;(4D)&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#24314;&#27169;&#19981;&#21516;&#31867;&#22411;&#30340;ASR&#20219;&#21153;&#65292;&#21253;&#25324;&#20302;&#36164;&#28304;&#35821;&#38899;&#12289;&#20195;&#30721;&#36716;&#25442;&#35821;&#38899;&#21644;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#35821;&#38899;&#65292;&#24182;&#19988;&#24615;&#33021;&#20248;&#20110;&#21333;&#19968;&#27169;&#22411;&#21644;&#20004;&#20010;&#28151;&#21512;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.10818</link><description>&lt;p&gt;
4D ASR&#65306;CTC&#12289;Attention&#12289;RNN-T&#21644;Mask-Predict&#35299;&#30721;&#22120;&#30340;&#32852;&#21512;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
4D ASR: Joint modeling of CTC, Attention, Transducer, and Mask-Predict decoders. (arXiv:2212.10818v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;CTC&#12289;Attention&#12289;RNN-T&#21644;Mask-Predict&#22235;&#20010;&#35299;&#30721;&#22120;&#30340;&#32852;&#21512;&#24314;&#27169;(4D)&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#24314;&#27169;&#19981;&#21516;&#31867;&#22411;&#30340;ASR&#20219;&#21153;&#65292;&#21253;&#25324;&#20302;&#36164;&#28304;&#35821;&#38899;&#12289;&#20195;&#30721;&#36716;&#25442;&#35821;&#38899;&#21644;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#35821;&#38899;&#65292;&#24182;&#19988;&#24615;&#33021;&#20248;&#20110;&#21333;&#19968;&#27169;&#22411;&#21644;&#20004;&#20010;&#28151;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#35770;&#26159;CTC&#12289;RNN-T&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#36824;&#26159;&#38750;&#33258;&#22238;&#24402;&#30340;Mask-Predict&#27169;&#22411;&#65292;&#31471;&#21040;&#31471;(E2E)&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#30340;&#32593;&#32476;&#26550;&#26500;&#37117;&#21487;&#24402;&#20026;&#20960;&#31867;&#12290;&#30001;&#20110;&#27599;&#20010;&#26550;&#26500;&#37117;&#26377;&#20854;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#22240;&#27492;&#20856;&#22411;&#30340;&#29992;&#20363;&#26159;&#26681;&#25454;&#24212;&#29992;&#38656;&#27714;&#20999;&#25442;&#36825;&#20123;&#29420;&#31435;&#27169;&#22411;&#65292;&#23548;&#33268;&#32500;&#25252;&#25152;&#26377;&#27169;&#22411;&#30340;&#24320;&#38144;&#22686;&#21152;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#38598;&#25104;&#36825;&#20123;&#20114;&#34917;&#27169;&#22411;&#20013;&#30340;&#20004;&#31181;&#20197;&#20943;&#36731;&#24320;&#38144;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#38598;&#25104;&#26356;&#22810;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#36827;&#19968;&#27493;&#20174;&#36825;&#20123;&#20114;&#34917;&#27169;&#22411;&#20013;&#21463;&#30410;&#65292;&#24182;&#36890;&#36807;&#21333;&#19968;&#31995;&#32479;&#23454;&#29616;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CTC&#12289;Attention&#12289;RNN-T&#21644;Mask-Predict&#22235;&#20010;&#35299;&#30721;&#22120;&#30340;&#32852;&#21512;&#24314;&#27169;(4D)&#65292;&#20855;&#26377;&#20197;&#19979;&#19977;&#20010;&#20248;&#28857;&#65306;1) &#22235;&#20010;&#35299;&#30721;&#22120;&#32852;&#21512;&#35757;&#32451;&#65292;&#22240;&#27492;&#26681;&#25454;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#36731;&#26494;&#20999;&#25442;&#65307;2) &#25552;&#20986;&#30340;4D&#27169;&#22411;&#20248;&#20110;&#21333;&#19968;&#27169;&#22411;&#21644;&#20004;&#20010;&#28151;&#21512;&#27169;&#22411;&#65307;3) &#35813;&#27169;&#22411;&#21487;&#20197;&#39640;&#25928;&#22320;&#24314;&#27169;&#19981;&#21516;&#31867;&#22411;&#30340;ASR&#20219;&#21153;&#65292;&#21253;&#25324;&#20302;&#36164;&#28304;&#35821;&#38899;&#12289;&#20195;&#30721;&#36716;&#25442;&#35821;&#38899;&#21644;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
The network architecture of end-to-end (E2E) automatic speech recognition (ASR) can be classified into several models, including connectionist temporal classification (CTC), recurrent neural network transducer (RNN-T), attention mechanism, and non-autoregressive mask-predict models. Since each of these network architectures has pros and cons, a typical use case is to switch these separate models depending on the application requirement, resulting in the increased overhead of maintaining all models. Several methods for integrating two of these complementary models to mitigate the overhead issue have been proposed; however, if we integrate more models, we will further benefit from these complementary models and realize broader applications with a single system. This paper proposes four-decoder joint modeling (4D) of CTC, attention, RNN-T, and mask-predict, which has the following three advantages: 1) The four decoders are jointly trained so that they can be easily switched depending on t
&lt;/p&gt;</description></item><item><title>ORCA &#26159;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#65292;&#21033;&#29992;&#21508;&#31181;&#38463;&#25289;&#20271;&#35821;&#35328;&#21644;&#19968;&#31995;&#21015;&#26377;&#25361;&#25112;&#24615;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#26500;&#24314;&#12290;&#24403;&#21069;&#20351;&#29992; ORCA &#23545; 18 &#20010;&#22810;&#35821;&#35328;&#21644;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2212.10758</link><description>&lt;p&gt;
ORCA: &#19968;&#39033;&#25361;&#25112;&#24615;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#29702;&#35299;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ORCA: A Challenging Benchmark for Arabic Language Understanding. (arXiv:2212.10758v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10758
&lt;/p&gt;
&lt;p&gt;
ORCA &#26159;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#65292;&#21033;&#29992;&#21508;&#31181;&#38463;&#25289;&#20271;&#35821;&#35328;&#21644;&#19968;&#31995;&#21015;&#26377;&#25361;&#25112;&#24615;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#26500;&#24314;&#12290;&#24403;&#21069;&#20351;&#29992; ORCA &#23545; 18 &#20010;&#22810;&#35821;&#35328;&#21644;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#25152;&#26377; NLP &#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24050;&#25552;&#20986;&#20102;&#22810;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#21162;&#21147;&#65292;&#30446;&#21069;&#23578;&#19981;&#23384;&#22312;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#30340;&#22810;&#26679;&#21270;&#20844;&#20849;&#22522;&#20934;&#12290;&#36825;&#20351;&#24471;&#21516;&#26102;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#21644;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#20010;&#25361;&#25112;&#36824;&#22240;&#38463;&#25289;&#20271;&#35821;&#19981;&#26159;&#21333;&#19968;&#35821;&#35328;&#32780;&#26159;&#19968;&#31995;&#21015;&#35821;&#35328;&#21644;&#26041;&#35328;&#32780;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; ORCA&#65292;&#19968;&#39033;&#20844;&#24320;&#21487;&#29992;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#12290;ORCA &#34987;&#31934;&#24515;&#26500;&#24314;&#65292;&#20197;&#35206;&#30422;&#22810;&#31181;&#38463;&#25289;&#20271;&#35821;&#35328;&#21644;&#19968;&#31995;&#21015;&#26377;&#25361;&#25112;&#24615;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#21033;&#29992;&#19971;&#20010; NLU &#20219;&#21153;&#38598;&#32676;&#20013;&#30340; 60 &#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#34913;&#37327;&#24403;&#21069;&#38463;&#25289;&#20271;&#35821; NLU &#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#20351;&#29992; ORCA &#22312; 18 &#20010;&#22810;&#35821;&#35328;&#21644;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#20102;&#20840;&#38754;&#23545;&#27604;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;&#25490;&#34892;&#27036;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their crucial role in all NLP, several benchmarks have been proposed to evaluate pretrained language models. In spite of these efforts, no public benchmark of diverse nature currently exists for evaluation of Arabic. This makes it challenging to measure progress for both Arabic and multilingual language models. This challenge is compounded by the fact that any benchmark targeting Arabic needs to take into account the fact that Arabic is not a single language but rather a collection of languages and varieties. In this work, we introduce ORCA, a publicly available benchmark for Arabic language understanding evaluation. ORCA is carefully constructed to cover diverse Arabic varieties and a wide range of challenging Arabic understanding tasks exploiting 60 different datasets across seven NLU task clusters. To measure current progress in Arabic NLU, we use ORCA to offer a comprehensive comparison between 18 multilingual and Arabic language models. We also provide a public leaderboard 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#27425;&#12289;&#21322;&#30417;&#30563;&#30340;&#20107;&#20214;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#20837;&#22312;&#20107;&#20214;&#31867;&#22411;&#23618;&#38754;&#23450;&#20041;&#30340;&#32467;&#26500;&#21270;&#26412;&#20307;&#30693;&#35782;&#26469;&#25351;&#23548;&#38544;&#21464;&#37327;&#30340;&#21387;&#32553;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#25552;&#21319;&#20102;&#22810;&#36798;8.5%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10547</link><description>&lt;p&gt;
&#35821;&#20041;&#20449;&#24687;&#25351;&#23548;&#30340;&#20998;&#23618;&#20107;&#20214;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Semantically-informed Hierarchical Event Modeling. (arXiv:2212.10547v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#27425;&#12289;&#21322;&#30417;&#30563;&#30340;&#20107;&#20214;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#20837;&#22312;&#20107;&#20214;&#31867;&#22411;&#23618;&#38754;&#23450;&#20041;&#30340;&#32467;&#26500;&#21270;&#26412;&#20307;&#30693;&#35782;&#26469;&#25351;&#23548;&#38544;&#21464;&#37327;&#30340;&#21387;&#32553;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#25552;&#21319;&#20102;&#22810;&#36798;8.5%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#35821;&#20041;&#26412;&#20307;&#30693;&#35782;&#19982;&#39034;&#24207;&#38544;&#21464;&#37327;&#27169;&#22411;&#30456;&#32467;&#21512;&#21487;&#20197;&#25913;&#21892;&#20107;&#20214;&#24314;&#27169;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#23618;&#27425;&#12289;&#21322;&#30417;&#30563;&#20107;&#20214;&#24314;&#27169;&#26694;&#26550;&#65292;&#25552;&#20379;&#32467;&#26500;&#23618;&#27425;&#24182;&#32771;&#34385;&#26412;&#20307;&#23618;&#27425;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#22810;&#23618;&#32467;&#26500;&#38544;&#21464;&#37327;&#65292;&#27599;&#20010;&#36830;&#32493;&#23618;&#37117;&#21387;&#32553;&#21644;&#25277;&#35937;&#20808;&#21069;&#30340;&#23618;&#12290;&#25105;&#20204;&#36890;&#36807;&#27880;&#20837;&#22312;&#20107;&#20214;&#31867;&#22411;&#23618;&#38754;&#23450;&#20041;&#30340;&#32467;&#26500;&#21270;&#26412;&#20307;&#30693;&#35782;&#26469;&#25351;&#23548;&#36825;&#31181;&#21387;&#32553;&#65306;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20801;&#35768;&#37096;&#20998;&#27880;&#20837;&#35821;&#20041;&#30693;&#35782;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#22312;&#20219;&#20309;&#29305;&#23450;&#23618;&#27425;&#30340;&#35821;&#20041;&#26412;&#20307;&#35266;&#23519;&#23454;&#20363;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#22810;&#36798;8.5%&#30340;&#24615;&#33021;&#65292; demonstrating:
&lt;/p&gt;
&lt;p&gt;
Prior work has shown that coupling sequential latent variable models with semantic ontological knowledge can improve the representational capabilities of event modeling approaches. In this work, we present a novel, doubly hierarchical, semi-supervised event modeling framework that provides structural hierarchy while also accounting for ontological hierarchy. Our approach consists of multiple layers of structured latent variables, where each successive layer compresses and abstracts the previous layers. We guide this compression through the injection of structured ontological knowledge that is defined at the type level of events: importantly, our model allows for partial injection of semantic knowledge and it does not depend on observing instances at any particular level of the semantic ontology. Across two different datasets and four different evaluation metrics, we demonstrate that our approach is able to out-perform the previous state-of-the-art approaches by up to 8.5%, demonstratin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10511</link><description>&lt;p&gt;
&#20309;&#26102;&#19981;&#20449;&#20219;&#35821;&#35328;&#27169;&#22411;&#65306;&#25506;&#32034;&#21442;&#25968;&#21644;&#38750;&#21442;&#25968;&#35760;&#24518;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories. (arXiv:2212.10511v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#36825;&#26263;&#31034;&#20102;&#20165;&#20381;&#38752;&#20854;&#21442;&#25968;&#26469;&#32534;&#30721;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#22312;PopQA&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#30693;&#35782;&#25506;&#27979;&#23454;&#39564;&#65292;&#20197;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#20107;&#23454;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#38271;&#23614;&#20013;&#65292;&#25193;&#23637;&#35268;&#27169;&#26080;&#27861;&#26126;&#26174;&#25913;&#21892;&#35760;&#24518;&#23454;&#38469;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32988;&#36807;&#32423;&#21035;&#22823;&#24471;&#22810;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26410;&#32463;&#21327;&#21161;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#28041;&#21450;&#39640;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#19978;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#22312;&#38656;&#35201;&#26102;&#26816;&#32034;&#38750;&#21442;&#25968;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only whe
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20219;&#21153;&#35828;&#26126;&#24494;&#35843;&#30340;&#21333;&#19968;&#25991;&#26412;&#23884;&#20837;&#22120;INSTRUCTOR&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21151;&#33021;&#24378;&#22823;&#65292;&#36866;&#24212;&#24615;&#24378;&#12290;</title><link>http://arxiv.org/abs/2212.09741</link><description>&lt;p&gt;
&#19968;&#31181;&#23884;&#20837;&#22120;&#65292;&#22810;&#31181;&#20219;&#21153;: &#22522;&#20110;&#20219;&#21153;&#35828;&#26126;&#24494;&#35843;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One Embedder, Any Task: Instruction-Finetuned Text Embeddings. (arXiv:2212.09741v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09741
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20219;&#21153;&#35828;&#26126;&#24494;&#35843;&#30340;&#21333;&#19968;&#25991;&#26412;&#23884;&#20837;&#22120;INSTRUCTOR&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21151;&#33021;&#24378;&#22823;&#65292;&#36866;&#24212;&#24615;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;INSTRUCTOR&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#20219;&#21153;&#35828;&#26126;&#35745;&#31639;&#25991;&#26412;&#23884;&#20837;&#65306;&#27599;&#20010;&#25991;&#26412;&#36755;&#20837;&#37117;&#19982;&#35299;&#37322;&#29992;&#20363;&#65288;&#20363;&#22914;&#65292;&#20219;&#21153;&#21644;&#39046;&#22495;&#25551;&#36848;&#65289;&#19968;&#36215;&#23884;&#20837;&#12290;&#19982;&#20043;&#21069;&#26356;&#19987;&#19994;&#21270;&#30340;&#32534;&#30721;&#22120;&#19981;&#21516;&#65292;INSTRUCTOR&#26159;&#19968;&#20010;&#21333;&#19968;&#23884;&#20837;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#36866;&#29992;&#20110;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;330&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#27880;&#37322;&#20102;&#20219;&#21153;&#35828;&#26126;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#22312;&#27492;&#22810;&#20219;&#21153;&#28151;&#21512;&#20013;&#35757;&#32451;INSTRUCTOR&#12290;&#25105;&#20204;&#22312;70&#20010;&#23884;&#20837;&#35780;&#20272;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;INSTRUCTOR&#65288;&#20854;&#20013;&#26377;66&#20010;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#65289;&#65292;&#28085;&#30422;&#20998;&#31867;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#26041;&#38754;&#30340;&#20219;&#21153;&#12290;INSTRUCTOR&#34429;&#28982;&#25317;&#26377;&#27604;&#20043;&#21069;&#26368;&#20339;&#27169;&#22411;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#21442;&#25968;&#65292;&#20294;&#22312;70&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25913;&#36827;&#20102;3.4&#65285;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;INSTRUCTOR&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#33021;&#21147;&#26159;&#30001;&#20110;&#23427;&#26377;&#25928;&#22320;&#20351;&#29992;&#20219;&#21153;&#35828;&#26126;&#65292;&#23427;&#20204;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#36719;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#22312;&#19977;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;INSTRUCTOR&#30340;&#23454;&#29992;&#24615;&#65306;&#39046;&#22495;&#36866;&#24212;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#37117;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions). Unlike encoders from prior work that are more specialized, INSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training. We first annotate instructions for 330 diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are unseen during training), ranging from classification and information retrieval to semantic textual similarity and text generation evaluation. INSTRUCTOR, while having an order of magnitude fewer parameters than the previous best model, achieves state-of-the-art performance, with an average improvement of 3.4% compared to the previous best results on the 70 diverse datasets. Our ana
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#31616;&#21270;&#30340;&#21487;&#23398;&#20064;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;LENS&#65292;&#36890;&#36807;&#24341;&#20837;SimpeEval&#35821;&#26009;&#24211;&#20316;&#20026;&#20154;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#19982;&#29616;&#26377;&#24230;&#37327;&#26631;&#20934;&#30456;&#27604;&#65292;LENS&#23545;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#26356;&#22909;&#65292;&#20026;&#35780;&#20272;&#25991;&#26412;&#31616;&#21270;&#30340;&#26410;&#26469;&#36827;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2212.09739</link><description>&lt;p&gt;
LENS&#65306;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#25991;&#26412;&#31616;&#21270;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
LENS: A Learnable Evaluation Metric for Text Simplification. (arXiv:2212.09739v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#31616;&#21270;&#30340;&#21487;&#23398;&#20064;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;LENS&#65292;&#36890;&#36807;&#24341;&#20837;SimpeEval&#35821;&#26009;&#24211;&#20316;&#20026;&#20154;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#19982;&#29616;&#26377;&#24230;&#37327;&#26631;&#20934;&#30456;&#27604;&#65292;LENS&#23545;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#26356;&#22909;&#65292;&#20026;&#35780;&#20272;&#25991;&#26412;&#31616;&#21270;&#30340;&#26410;&#26469;&#36827;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21487;&#23398;&#20064;&#24230;&#37327;&#26631;&#20934;&#24050;&#25104;&#20026;&#33258;&#21160;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#25991;&#26412;&#31616;&#21270;&#30340;&#20154;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;&#21482;&#26377;&#19968;&#20123;&#22522;&#20110;&#21333;&#19968;&#25110;&#36807;&#26102;&#27169;&#22411;&#30340;&#26377;&#38480;&#27880;&#37322;&#65292;&#20351;&#23427;&#20204;&#19981;&#33021;&#36866;&#29992;&#20110;&#36825;&#31181;&#26041;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SimpEval&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#65306;SimpEval_past&#65292;&#21253;&#25324;&#23545;24&#20010;&#36807;&#21435;&#31995;&#32479;2.4K&#31616;&#21270;&#30340;12K&#20154;&#31867;&#35780;&#20998;&#65292;&#20197;&#21450;SimpEval_2022&#65292;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31616;&#21270;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#23545;360&#20010;&#31616;&#21270;&#65292;&#21253;&#25324;GPT-3.5&#29983;&#25104;&#25991;&#26412;&#30340;1K&#20154;&#31867;&#35780;&#20998;&#12290;&#22312;SimpEval&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#31616;&#21270;&#30340;&#21487;&#23398;&#20064;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;LENS&#12290;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LENS&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#26356;&#22909;&#65292;&#20026;&#35780;&#20272;&#25991;&#26412;&#31616;&#21270;&#30340;&#26410;&#26469;&#36827;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Rank&#21644;Rate&#65292;&#19968;&#31181;&#20154;&#31867;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#31616;&#21270;&#36827;&#34892;&#25490;&#21517;&#21644;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training learnable metrics using modern language models has recently emerged as a promising method for the automatic evaluation of machine translation. However, existing human evaluation datasets for text simplification have limited annotations that are based on unitary or outdated models, making them unsuitable for this approach. To address these issues, we introduce the SimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on 2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging simplification benchmark consisting of over 1K human ratings of 360 simplifications including GPT-3.5 generated text. Training on SimpEval, we present LENS, a Learnable Evaluation Metric for Text Simplification. Extensive empirical results show that LENS correlates much better with human judgment than existing metrics, paving the way for future progress in the evaluation of text simplification. We also introduce Rank and Rate, a human evaluation framework that rates si
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36328;&#26041;&#35328;&#30340;&#33521;&#25991;NLP&#26694;&#26550;Multi-VALUE&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23558;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#26144;&#23556;&#20026;50&#31181;&#33521;&#35821;&#26041;&#35328;&#30340;&#21512;&#25104;&#24418;&#24335;&#65292;&#29992;&#20110;&#35780;&#20272;&#12289;&#23454;&#29616;&#33521;&#24335;&#26041;&#35328;&#20020;&#36817;&#24615;&#65292;&#24182;&#22312;&#38750;&#26631;&#20934;&#26041;&#35328;&#19978;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2212.08011</link><description>&lt;p&gt;
&#22810;&#20803;&#20215;&#20540;&#65306;&#36328;&#26041;&#35328;&#30340;&#33521;&#25991;NLP&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Multi-VALUE: A Framework for Cross-Dialectal English NLP. (arXiv:2212.08011v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08011
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36328;&#26041;&#35328;&#30340;&#33521;&#25991;NLP&#26694;&#26550;Multi-VALUE&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23558;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#26144;&#23556;&#20026;50&#31181;&#33521;&#35821;&#26041;&#35328;&#30340;&#21512;&#25104;&#24418;&#24335;&#65292;&#29992;&#20110;&#35780;&#20272;&#12289;&#23454;&#29616;&#33521;&#24335;&#26041;&#35328;&#20020;&#36817;&#24615;&#65292;&#24182;&#22312;&#38750;&#26631;&#20934;&#26041;&#35328;&#19978;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22495;&#12289;&#31038;&#20250;&#21644;&#32463;&#27982;&#22240;&#32032;&#24341;&#36215;&#30340;&#26041;&#35328;&#24046;&#24322;&#23545;&#35768;&#22810;&#35821;&#35328;&#25216;&#26415;&#29992;&#25143;&#36896;&#25104;&#20102;&#24615;&#33021;&#24046;&#24322;&#12290;&#26222;&#24800;&#21644;&#20844;&#24179;&#30340;&#35821;&#35328;&#25216;&#26415;&#24517;&#39035;&#26159;&#20020;&#36817;&#26041;&#35328;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#26041;&#35328;&#36716;&#25442;&#26102;&#24615;&#33021;&#20445;&#25345;&#19981;&#21464;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#35774;&#35745;&#21644;&#27979;&#35797;&#37117;&#26159;&#22522;&#20110;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#65288;SAE&#65289;&#65292;&#30446;&#21069;&#30340;&#31995;&#32479;&#24448;&#24448;&#19981;&#33021;&#36798;&#21040;&#36825;&#20010;&#29702;&#24819;&#29366;&#24577;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#36164;&#28304;&#22871;&#20214;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#23454;&#29616;&#33521;&#24335;&#26041;&#35328;&#20020;&#36817;&#24615;&#65292;&#31216;&#20043;&#20026;Multi-VALUE&#65292;&#23427;&#26159;&#19968;&#20010;&#21487;&#25511;&#21046;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#32763;&#35793;&#31995;&#32479;&#65292;&#35206;&#30422;&#20102;50&#31181;&#33521;&#35821;&#26041;&#35328;&#21644;189&#20010;&#29420;&#29305;&#30340;&#35821;&#35328;&#29305;&#24449;&#12290;Multi-VALUE&#23558;SAE&#26144;&#23556;&#21040;&#27599;&#31181;&#26041;&#35328;&#30340;&#21512;&#25104;&#24418;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#36825;&#20010;&#31995;&#32479;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#65292;&#27979;&#35797;&#38382;&#31572;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#20041;&#35299;&#26512;&#12290;&#21387;&#21147;&#27979;&#35797;&#25581;&#31034;&#20102;&#22312;&#38750;&#26631;&#20934;&#26041;&#35328;&#19978;&#30340;&#39046;&#20808;&#27169;&#22411;&#30340;&#26174;&#30528;&#24615;&#33021;&#24046;&#36317;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#31995;&#32479;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialect differences caused by regional, social, and economic factors cause performance discrepancies for many groups of language technology users. Inclusive and equitable language technology must critically be dialect invariant, meaning that performance remains constant over dialectal shifts. Current systems often fall short of this ideal since they are designed and tested on a single dialect: Standard American English (SAE). We introduce a suite of resources for evaluating and achieving English dialect invariance. The resource is called Multi-VALUE, a controllable rule-based translation system spanning 50 English dialects and 189 unique linguistic features. Multi-VALUE maps SAE to synthetic forms of each dialect. First, we use this system to stress tests question answering, machine translation, and semantic parsing. Stress tests reveal significant performance disparities for leading models on non-standard dialects. Second, we use this system as a data augmentation technique to improve
&lt;/p&gt;</description></item><item><title>LMP&#23558;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20174;&#32431;&#25991;&#26412;&#25552;&#31034;&#25193;&#23637;&#20026;&#25991;&#26412;&#25552;&#31034;&#21644;&#33050;&#26412;&#30340;&#30452;&#35266;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#32534;&#31243;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2212.06094</link><description>&lt;p&gt;
Prompting&#23601;&#26159;&#32534;&#31243;: &#19968;&#31181;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Prompting Is Programming: A Query Language for Large Language Models. (arXiv:2212.06094v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06094
&lt;/p&gt;
&lt;p&gt;
LMP&#23558;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20174;&#32431;&#25991;&#26412;&#25552;&#31034;&#25193;&#23637;&#20026;&#25991;&#26412;&#25552;&#31034;&#21644;&#33050;&#26412;&#30340;&#30452;&#35266;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#32534;&#31243;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#21644;&#20195;&#30721;&#29983;&#25104;&#31561;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;&#20174;&#39640;&#23618;&#27425;&#19978;&#35762;&#65292;&#32473;&#23450;&#36755;&#20837;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#32479;&#35745;&#19978;&#30340;&#21487;&#33021;&#24615;&#33258;&#21160;&#23436;&#25104;&#24207;&#21015;&#12290;&#22522;&#20110;&#27492;&#65292;&#29992;&#25143;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196;&#25110;&#31034;&#20363;&#26469;&#25552;&#31034;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#39640;&#32423;&#25552;&#31034;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#26263;&#31034;&#27169;&#22411;&#12289;&#29992;&#25143;&#21644;&#35745;&#31639;&#22120;&#31561;&#22806;&#37096;&#24037;&#20855;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#25110;&#23558;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#65292;&#24517;&#39035;&#23454;&#29616;&#22797;&#26434;&#30340;&#20219;&#21153;-&#21644;&#27169;&#22411;&#29305;&#23450;&#30340;&#31243;&#24207;&#65292;&#36825;&#20173;&#28982;&#21487;&#33021;&#38656;&#35201;&#29305;&#23450;&#30340;&#20132;&#20114;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#32534;&#31243;&#65288;LMP&#65289;&#30340;&#26032;&#27010;&#24565;&#12290;LMP&#23558;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20174;&#32431;&#25991;&#26412;&#25552;&#31034;&#25193;&#23637;&#20026;&#25991;&#26412;&#25552;&#31034;&#21644;&#33050;&#26412;&#30340;&#30452;&#35266;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;LMP&#20801;&#35768;&#25351;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.  Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the languag
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#33258;&#21160;&#29983;&#25104;&#20799;&#31461;&#22909;&#22855;&#24515;&#38382;&#39064;&#25552;&#38382;&#22521;&#35757;&#30340;&#25945;&#32946;&#20869;&#23481;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#21644;&#25945;&#32946;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2211.14228</link><description>&lt;p&gt;
GPT-3 &#39537;&#21160;&#30340;&#25945;&#32946;&#26234;&#33021;&#20307;&#35757;&#32451;&#20799;&#31461;&#22909;&#22855;&#24515;&#38382;&#39064;&#25552;&#38382;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
GPT-3-driven pedagogical agents for training children's curious question-asking skills. (arXiv:2211.14228v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14228
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#33258;&#21160;&#29983;&#25104;&#20799;&#31461;&#22909;&#22855;&#24515;&#38382;&#39064;&#25552;&#38382;&#22521;&#35757;&#30340;&#25945;&#32946;&#20869;&#23481;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#21644;&#25945;&#32946;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35757;&#32451;&#20799;&#31461;&#25552;&#38382;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#35774;&#35745;&#29305;&#23450;&#30340;&#32451;&#20064;&#65292;&#20381;&#38752;&#25552;&#20379;&#35821;&#20041;&#21644;&#35821;&#35328;&#25552;&#31034;&#26469;&#24110;&#21161;&#24418;&#25104;&#36825;&#26679;&#30340;&#38382;&#39064;&#12290;&#20294;&#23613;&#31649;&#34920;&#29616;&#20986;&#20102;&#25945;&#23398;&#25928;&#29575;&#65292;&#20294;&#35813;&#26041;&#27861;&#20173;&#28982;&#21463;&#38480;&#20110;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#65292;&#36825;&#21487;&#33021;&#26159;&#19968;&#20010;&#38750;&#24120;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65288;NLP&#65289;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#24182;&#35843;&#26597;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#21270;&#22909;&#22855;&#24515;&#38382;&#39064;&#25552;&#38382;&#65288;QA&#65289;&#22521;&#35757;&#30340;&#25945;&#32946;&#20869;&#23481;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20351;&#29992;&#8220;&#22522;&#20110;&#25552;&#31034;&#8221;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#25945;&#32946;&#20869;&#23481;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#33258;&#28982;&#25991;&#26412;&#21521;LLM&#35299;&#37322;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#19987;&#23478;&#27880;&#37322;&#21644;&#25163;&#21160;&#29983;&#25104;&#20869;&#23481;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#30830;&#23454;&#34920;&#26126;&#20102;&#36825;&#31181;&#20869;&#23481;&#30340;&#30456;&#20851;&#24615;&#21644;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#22312;&#23567;&#23398;&#36827;&#34892;&#20102;&#29616;&#22330;&#30740;&#31350;&#65288;75&#20010;&#23401;&#23376;&#65289;
&lt;/p&gt;
&lt;p&gt;
In order to train children's ability to ask curiosity-driven questions, previous research has explored designing specific exercises relying on providing semantic and linguistic cues to help formulate such questions. But despite showing pedagogical efficiency, this method is still limited as it relies on generating the said cues by hand, which can be a very costly process. In this context, we propose to leverage advances in the natural language processing field (NLP) and investigate the efficiency of using a large language model (LLM) for automating the production of the pedagogical content of a curious question-asking (QA) training. We study generating the said content using the "prompt-based" method that consists of explaining the task to the LLM in natural text. We evaluate the output using human experts annotations and comparisons with hand-generated content. Results suggested indeed the relevance and usefulness of this content. We also conduct a field study in primary school (75 ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#20013;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#35780;&#20272;&#31995;&#32479;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#30340;&#27700;&#24179;&#65292;&#24110;&#21161;&#27979;&#35797;&#35774;&#35745;&#20154;&#21592;&#30830;&#20445;&#20351;&#29992;&#19990;&#30028;&#30693;&#35782;&#30340;&#38382;&#39064;&#26159;&#21487;&#25509;&#21463;&#30340;&#65292;&#21516;&#26102;&#21487;&#20197;&#27979;&#35797;&#38382;&#39064;&#30340;&#36136;&#37327;&#24182;&#21457;&#29616;&#35774;&#35745;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.07040</link><description>&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#20013;&#30340;&#19990;&#30028;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
World Knowledge in Multiple Choice Reading Comprehension. (arXiv:2211.07040v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#20013;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#35780;&#20272;&#31995;&#32479;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#30340;&#27700;&#24179;&#65292;&#24110;&#21161;&#27979;&#35797;&#35774;&#35745;&#20154;&#21592;&#30830;&#20445;&#20351;&#29992;&#19990;&#30028;&#30693;&#35782;&#30340;&#38382;&#39064;&#26159;&#21487;&#25509;&#21463;&#30340;&#65292;&#21516;&#26102;&#21487;&#20197;&#27979;&#35797;&#38382;&#39064;&#30340;&#36136;&#37327;&#24182;&#21457;&#29616;&#35774;&#35745;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#35821;&#22659;&#30340;&#24773;&#20917;&#19979;&#65292;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#65288;MCRC&#65289;&#31995;&#32479;&#33021;&#22815;&#27604;&#38543;&#26426;&#22238;&#31572;&#38382;&#39064;&#35201;&#22909;&#24471;&#22810;&#12290;&#36825;&#20123;&#31995;&#32479;&#20351;&#29992;&#20182;&#20204;&#31215;&#32047;&#30340;&#8220;&#19990;&#30028;&#30693;&#35782;&#8221;&#26469;&#30452;&#25509;&#22238;&#31572;&#38382;&#39064;&#65292;&#32780;&#19981;&#20351;&#29992;&#26469;&#33258;&#27573;&#33853;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#20316;&#20026;&#27979;&#35797;&#35774;&#35745;&#24037;&#20855;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#30830;&#20445;&#22312;&#29305;&#23450;&#30340;&#19968;&#32452;&#38382;&#39064;&#20013;&#20351;&#29992;&#8220;&#19990;&#30028;&#30693;&#35782;&#8221;&#26159;&#21487;&#25509;&#21463;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#35780;&#20272;&#31995;&#32479;&#21033;&#29992;"&#19990;&#30028;&#30693;&#35782;"&#30340;&#27700;&#24179;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#65306;&#26399;&#26395;&#36873;&#39033;&#25968;&#65292;&#23427;&#27979;&#37327;&#20102;&#26080;&#38656;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#31995;&#32479;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#26469;&#30830;&#23450;&#19968;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#65307;&#35789;&#27719;&#20114;&#20449;&#24687;&#65292;&#23427;&#27979;&#37327;&#20102;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#26469;&#35828;&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#37027;&#20123;&#26399;&#26395;&#36873;&#39033;&#25968;&#36739;&#20302;&#30340;&#38382;&#39064;&#65292;&#21363;&#21487;&#20197;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#26469;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#34987;&#20934;&#30830;&#22320;&#37492;&#23450;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#27979;&#35797;&#38382;&#39064;&#30340;&#36136;&#37327;&#24182;&#30830;&#23450;&#27979;&#35797;&#35774;&#35745;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently it has been shown that without any access to the contextual passage, multiple choice reading comprehension (MCRC) systems are able to answer questions significantly better than random on average. These systems use their accumulated "world knowledge" to directly answer questions, rather than using information from the passage. This paper examines the possibility of exploiting this observation as a tool for test designers to ensure that the use of "world knowledge" is acceptable for a particular set of questions. We propose information-theory based metrics that enable the level of "world knowledge" exploited by systems to be assessed. Two metrics are described: the expected number of options, which measures whether a passage-free system can identify the answer a question using world knowledge; and the contextual mutual information, which measures the importance of context for a given question. We demonstrate that questions with low expected number of options, and hence answerabl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#20351;&#29992;&#30683;&#30462;&#25216;&#26415;&#36827;&#34892;&#38382;&#31572;&#65292;&#21457;&#29616;&#32467;&#21512;&#30683;&#30462;&#12289;&#25903;&#25345;&#21644;&#38382;&#31572;&#27169;&#22411;&#32622;&#20449;&#20998;&#25968;&#30340;&#31995;&#32479;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2211.05598</link><description>&lt;p&gt;
&#20351;&#29992;&#30683;&#30462;&#33021;&#22815;&#25913;&#21892;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Using contradictions improves question answering systems. (arXiv:2211.05598v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#20351;&#29992;&#30683;&#30462;&#25216;&#26415;&#36827;&#34892;&#38382;&#31572;&#65292;&#21457;&#29616;&#32467;&#21512;&#30683;&#30462;&#12289;&#25903;&#25345;&#21644;&#38382;&#31572;&#27169;&#22411;&#32622;&#20449;&#20998;&#25968;&#30340;&#31995;&#32479;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#20351;&#29992;&#30683;&#30462;&#25216;&#26415;&#36827;&#34892;&#38382;&#31572;&#12290;&#36890;&#24120;&#65292;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31995;&#32479;&#36890;&#36807;&#30830;&#23450;&#19968;&#20010;&#28508;&#22312;&#31572;&#26696;&#26159;&#21542;&#34987;&#19968;&#20123;&#32972;&#26223;&#35821;&#22659;&#25152;&#25903;&#25345;&#26469;&#24110;&#21161;&#22238;&#31572;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#30830;&#23450;&#19968;&#20010;&#31572;&#26696;&#26159;&#21542;&#19982;&#19978;&#19979;&#25991;&#30683;&#30462;&#26159;&#21542;&#26377;&#29992;&#21602;&#65311;&#25105;&#20204;&#22312;&#22810;&#39033;&#36873;&#25321;&#21644;&#25277;&#21462;&#24335;&#38382;&#31572;&#20004;&#20010;&#22330;&#26223;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#65292;&#32467;&#21512;&#30683;&#30462;&#25216;&#26415;&#30340;&#31995;&#32479;&#21487;&#27604;&#20165;&#26377;&#25903;&#25345;&#25216;&#26415;&#30340;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#32467;&#21512;&#30683;&#30462;&#12289;&#25903;&#25345;&#21644;&#38382;&#31572;&#27169;&#22411;&#32622;&#20449;&#20998;&#25968;&#30340;&#31995;&#32479;&#34920;&#29616;&#26368;&#20339;&#12290;&#36825;&#23545;&#20110;&#22312;&#21307;&#23398;&#21644;&#31185;&#23398;&#31561;&#39046;&#22495;&#37096;&#32626;&#38382;&#31572;&#31995;&#32479;&#26102;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work examines the use of contradiction in natural language inference (NLI) for question answering (QA). Typically, NLI systems help answer questions by determining if a potential answer is \emph{entailed} (supported) by some background context. But is it useful to also determine if an answer contradicts the context? We test this in two settings, multiple choice and extractive QA, and find that systems that incorporate contradiction can do slightly better than entailment-only systems on certain datasets. However, the best performances come from using contradiction, entailment, and QA model confidence scores together. This has implications for the deployment of QA systems in domains such as medicine and science where safety is an issue.
&lt;/p&gt;</description></item><item><title>LAMASSU &#26159;&#19968;&#20010;&#20351;&#29992;&#31070;&#32463;&#21464;&#25442;&#22120;&#30340;&#27969;&#24335;&#36328;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#27169;&#22411;&#65292;&#33021;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#24182;&#36798;&#21040;&#19982;&#21333;&#35821; ASR &#21644;&#21452;&#35821;&#32763;&#35793;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.02809</link><description>&lt;p&gt;
LAMASSU: &#20351;&#29992;&#31070;&#32463;&#21464;&#25442;&#22120;&#30340;&#27969;&#24335;&#36328;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#35782;&#21035;&#19982;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LAMASSU: A Streaming Language-Agnostic Multilingual Speech Recognition and Translation Model Using Neural Transducers. (arXiv:2211.02809v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02809
&lt;/p&gt;
&lt;p&gt;
LAMASSU &#26159;&#19968;&#20010;&#20351;&#29992;&#31070;&#32463;&#21464;&#25442;&#22120;&#30340;&#27969;&#24335;&#36328;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#27169;&#22411;&#65292;&#33021;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#24182;&#36798;&#21040;&#19982;&#21333;&#35821; ASR &#21644;&#21452;&#35821;&#32763;&#35793;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#21487;&#20197;&#21516;&#26102;&#20351;&#29992;&#31070;&#32463;&#21464;&#25442;&#22120;&#20316;&#20026;&#27169;&#22411;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#21464;&#25442;&#22120;&#27169;&#22411;&#25191;&#34892;&#20004;&#20010;&#20219;&#21153;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#32852;&#21512; ASR &#21644; ST &#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#27969;&#24335;&#22788;&#29702;&#24182;&#19988;&#19981;&#38656;&#35201;&#28304;&#35821;&#35328;&#35782;&#21035;&#65288;&#21363;&#36328;&#35821;&#35328;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LAMASSU&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#31070;&#32463;&#21464;&#25442;&#22120;&#30340;&#27969;&#24335;&#36328;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#27169;&#22411;&#12290;&#22522;&#20110;&#21464;&#25442;&#22120;&#27169;&#22411;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#26041;&#27861;&#65306;&#29992;&#20110;&#22810;&#35821;&#35328;&#36755;&#20986;&#30340;&#32479;&#19968;&#32852;&#21512;&#21644;&#39044;&#27979;&#32593;&#32476;&#12289;&#32858;&#31867;&#24335;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#12289;&#32534;&#30721;&#22120;&#30340;&#30446;&#26631;&#35821;&#35328;&#35782;&#21035;&#12289;&#20197;&#21450;&#36830;&#32467;&#26102;&#24207;&#20998;&#31867;&#27491;&#21017;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LAMASSU &#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#32780;&#19988;&#36798;&#21040;&#20102;&#21333;&#35821; ASR &#21644;&#21452;&#35821;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) and speech translation (ST) can both use neural transducers as the model structure. It is thus possible to use a single transducer model to perform both tasks. In real-world applications, such joint ASR and ST models may need to be streaming and do not require source language identification (i.e. language-agnostic). In this paper, we propose LAMASSU, a streaming language-agnostic multilingual speech recognition and translation model using neural transducers. Based on the transducer model structure, we propose four methods, a unified joint and prediction network for multilingual output, a clustered multilingual encoder, target language identification for encoder, and connectionist temporal classification regularization. Experimental results show that LAMASSU not only drastically reduces the model size but also reaches the performances of monolingual ASR and bilingual ST models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#26174;&#33879;&#22270;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#30340;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#24182;&#25351;&#23548;&#20102;GPT-3.5&#29983;&#25104;&#26174;&#33879;&#22270;&#35328;&#35821;&#21270;&#65292;&#24471;&#21040;&#26368;&#39640;&#20154;&#31867;&#35780;&#20998;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.07222</link><description>&lt;p&gt;
&#26174;&#33879;&#22270;&#32763;&#35793;&#65306;&#27169;&#22411;&#26080;&#20851;&#21644;&#22522;&#20110;&#25351;&#20196;&#26041;&#27861;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#34920;&#31034;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Saliency Map Verbalization: Comparing Feature Importance Representations from Model-free and Instruction-based Methods. (arXiv:2210.07222v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#26174;&#33879;&#22270;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#30340;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#24182;&#25351;&#23548;&#20102;GPT-3.5&#29983;&#25104;&#26174;&#33879;&#22270;&#35328;&#35821;&#21270;&#65292;&#24471;&#21040;&#26368;&#39640;&#20154;&#31867;&#35780;&#20998;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#33879;&#22270;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#37325;&#35201;&#30340;&#36755;&#20837;&#29305;&#24449;&#26469;&#35299;&#37322;&#31070;&#32463;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#24456;&#38590;&#34987;&#38750;&#19987;&#19994;&#20154;&#22763;&#35299;&#37322;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#35768;&#22810;&#29305;&#24449;&#30340;&#23454;&#20363;&#12290;&#20026;&#20102;&#20351;&#23427;&#20204;&#26356;&#26131;&#20110;&#29702;&#35299;&#65292;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#23558;&#26174;&#33879;&#22270;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#30340;&#20219;&#21153;&#65292;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#20851;&#38190;&#25361;&#25112;&#30340;&#26041;&#27861;&#65306;&#20160;&#20040;&#21644;&#22914;&#20309;&#34920;&#36798;&#12290;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#26631;&#35760;&#32423;&#23646;&#24615;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65288;&#22522;&#20110;&#25628;&#32034;&#21644;&#22522;&#20110;&#25351;&#20196;&#30340;&#34920;&#36798;&#65289;&#19982;&#20256;&#32479;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#34920;&#31034;&#65288;&#28909;&#22270;&#21487;&#35270;&#21270;&#21644;&#25277;&#21462;&#29702;&#24615;&#65289;&#65292;&#35780;&#20272;&#21487;&#27169;&#25311;&#24615;&#12289;&#24544;&#35802;&#24230;&#12289;&#24110;&#21161;&#24615;&#21644;&#26131;&#29702;&#35299;&#24615;&#12290;&#36890;&#36807;&#25351;&#23548; GPT-3.5 &#29983;&#25104;&#26174;&#33879;&#22270;&#35328;&#35821;&#21270;&#65292;&#21487;&#20197;&#24471;&#21040;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#21253;&#25324;&#20851;&#32852;&#12289;&#25277;&#35937;&#27010;&#25324;&#21644;&#24120;&#35782;&#25512;&#29702;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#36804;&#20170;&#26368;&#39640;&#30340;&#20154;&#31867;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Saliency maps can explain a neural model's predictions by identifying important input features. They are difficult to interpret for laypeople, especially for instances with many features. In order to make them more accessible, we formalize the underexplored task of translating saliency maps into natural language and compare methods that address two key challenges of this approach -- what and how to verbalize. In both automatic and human evaluation setups, using token-level attributions from text classification tasks, we compare two novel methods (search-based and instruction-based verbalizations) against conventional feature importance representations (heatmap visualizations and extractive rationales), measuring simulatability, faithfulness, helpfulness and ease of understanding. Instructing GPT-3.5 to generate saliency map verbalizations yields plausible explanations which include associations, abstractive summarization and commonsense reasoning, achieving by far the highest human rat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#25277;&#21462;&#24335;&#24635;&#32467;&#20013;&#23384;&#22312;&#30340;&#20116;&#31181;&#24191;&#27867;&#30340;&#19981;&#24544;&#23454;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;30%&#30340;&#25277;&#21462;&#24335;&#25688;&#35201;&#23384;&#22312;&#33267;&#23569;&#19968;&#31181;&#38382;&#39064;&#12290;&#20026;&#20102;&#33258;&#21160;&#26816;&#27979;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934; ExtEval&#12290;</title><link>http://arxiv.org/abs/2209.03549</link><description>&lt;p&gt;
&#25277;&#21462;&#24335;&#24635;&#32467;&#30340;&#19981;&#24544;&#23454;&#24615;&#65306;&#24191;&#27867;&#30340;&#19981;&#24544;&#23454;&#38382;&#39064;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Extractive is not Faithful: An Investigation of Broad Unfaithfulness Problems in Extractive Summarization. (arXiv:2209.03549v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#25277;&#21462;&#24335;&#24635;&#32467;&#20013;&#23384;&#22312;&#30340;&#20116;&#31181;&#24191;&#27867;&#30340;&#19981;&#24544;&#23454;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;30%&#30340;&#25277;&#21462;&#24335;&#25688;&#35201;&#23384;&#22312;&#33267;&#23569;&#19968;&#31181;&#38382;&#39064;&#12290;&#20026;&#20102;&#33258;&#21160;&#26816;&#27979;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934; ExtEval&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25277;&#35937;&#24335;&#24635;&#32467;&#30340;&#32972;&#26223;&#19979;&#65292;&#19981;&#24544;&#23454;&#24635;&#32467;&#30340;&#38382;&#39064;&#24050;&#32463;&#34987;&#24191;&#27867;&#35752;&#35770;&#12290;&#34429;&#28982;&#30456;&#36739;&#20110;&#25277;&#35937;&#24335;&#24635;&#32467;&#65292;&#25277;&#21462;&#24335;&#24635;&#32467;&#26356;&#23569;&#20542;&#21521;&#20110;&#26222;&#36941;&#30340;&#19981;&#24544;&#23454;&#38382;&#39064;&#65292;&#20294;&#36825;&#26159;&#21542;&#24847;&#21619;&#30528;&#25277;&#21462;&#24335;&#24635;&#32467;&#31561;&#21516;&#20110;&#24544;&#23454;&#21602;&#65311;&#32467;&#26524;&#35777;&#26126;&#24182;&#38750;&#22914;&#27492;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#20116;&#31181;&#24191;&#27867;&#30340;&#19981;&#24544;&#23454;&#38382;&#39064;&#65288;&#21253;&#25324;&#21644;&#36229;&#20986;&#38750;&#34164;&#21547;&#65289;&#30340;&#20998;&#31867;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#20986;&#29616;&#22312;&#25277;&#21462;&#24335;&#24635;&#32467;&#20013;&#65292;&#21253;&#25324;&#19981;&#27491;&#30830;&#30340;&#20849;&#25351;&#12289;&#19981;&#23436;&#25972;&#30340;&#20849;&#25351;&#12289;&#19981;&#27491;&#30830;&#30340;&#35805;&#35821;&#12289;&#19981;&#23436;&#25972;&#30340;&#35805;&#35821;&#65292;&#20197;&#21450;&#20854;&#20182;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35201;&#27714;&#20154;&#31867;&#23545;&#30001;16&#20010;&#19981;&#21516;&#30340;&#25277;&#21462;&#24335;&#31995;&#32479;&#20135;&#29983;&#30340;1600&#31687;&#33521;&#25991;&#25688;&#35201;&#36827;&#34892;&#26631;&#27880;&#65292;&#25105;&#20204;&#21457;&#29616;30%&#30340;&#25688;&#35201;&#20013;&#33267;&#23569;&#23384;&#22312;&#20116;&#20010;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#12290;&#20026;&#20102;&#33258;&#21160;&#26816;&#27979;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#29616;5&#31181;&#29616;&#26377;&#30340;&#24635;&#32467;&#24544;&#23454;&#24230;&#35780;&#20272;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#21028;&#30340;&#30456;&#20851;&#24615;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934; ExtEval&#12290;
&lt;/p&gt;
&lt;p&gt;
The problems of unfaithful summaries have been widely discussed under the context of abstractive summarization. Though extractive summarization is less prone to the common unfaithfulness issues of abstractive summaries, does that mean extractive is equal to faithful? Turns out that the answer is no. In this work, we define a typology with five types of broad unfaithfulness problems (including and beyond not-entailment) that can appear in extractive summaries, including incorrect coreference, incomplete coreference, incorrect discourse, incomplete discourse, as well as other misleading information. We ask humans to label these problems out of 1600 English summaries produced by 16 diverse extractive systems. We find that 30% of the summaries have at least one of the five issues. To automatically detect these problems, we find that 5 existing faithfulness evaluation metrics for summarization have poor correlations with human judgment. To remedy this, we propose a new metric, ExtEval, that
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#21477;&#36923;&#36753;&#31867;&#22411;&#22686;&#21152;&#22810;&#26679;&#24615;&#30340;&#34920;&#26684;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20419;&#36827;&#25511;&#21046;&#29983;&#25104;&#35821;&#21477;&#31867;&#22411;&#30340;&#33021;&#21147;&#21516;&#26102;&#25552;&#39640;&#32467;&#26524;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.10938</link><description>&lt;p&gt;
&#31867;&#22411;&#25511;&#21046;&#22686;&#24378;&#30340;&#34920;&#26684;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diversity Enhanced Table-to-Text Generation via Type Control. (arXiv:2205.10938v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#21477;&#36923;&#36753;&#31867;&#22411;&#22686;&#21152;&#22810;&#26679;&#24615;&#30340;&#34920;&#26684;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20419;&#36827;&#25511;&#21046;&#29983;&#25104;&#35821;&#21477;&#31867;&#22411;&#30340;&#33021;&#21147;&#21516;&#26102;&#25552;&#39640;&#32467;&#26524;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35821;&#21477;&#26159;&#19968;&#20010;&#20855;&#26377;&#19968;&#20010;&#36755;&#20837;&#21644;&#22810;&#20010;&#26377;&#25928;&#36755;&#20986;&#30340;&#36807;&#31243;&#65288;&#21363;&#36923;&#36753; NLG&#65289;&#12290;&#36825;&#34920;&#26126;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#26377;&#25928;&#36755;&#20986;&#65292;&#21576;&#29616;&#20986;&#36755;&#20837;&#25968;&#25454;&#30340;&#19981;&#21516;&#35282;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22810;&#26679;&#24615;&#22686;&#24378;&#26041;&#26696;&#65292;&#21033;&#29992;&#35821;&#21477;&#30340;&#36923;&#36753;&#31867;&#22411;&#20316;&#20026;&#20869;&#22312;&#29305;&#24449;&#65292;&#20351;&#29992;&#19968;&#31181;&#31867;&#22411;&#21463;&#25511;&#30340;&#34920;&#26684;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#36923;&#36753; NLG &#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26082;&#20419;&#36827;&#20102;&#26377;&#25928;&#25511;&#21046;&#29983;&#25104;&#35821;&#21477;&#31867;&#22411;&#30340;&#33021;&#21147;&#65292;&#21448;&#22312;&#36136;&#37327;&#21644;&#20107;&#23454;&#22810;&#26679;&#24615;&#26435;&#34913;&#26041;&#38754;&#20135;&#29983;&#20102;&#20248;&#20110;&#26368;&#24378;&#22522;&#32447;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating natural language statements to convey logical inferences from tabular data (i.e., Logical NLG) is a process with one input and a variety of valid outputs. This characteristic underscores the need for a method to produce a diverse set of valid outputs, presenting different perspectives of the input data. We propose a simple yet effective diversity-enhancing scheme that builds upon an inherent property of the statements, their logic-types, by using a type-controlled table-to-text generation model. We demonstrate, through extensive automatic and human evaluations over the two publicly available Logical NLG datasets, that our proposed method both facilitates the ability to effectively control the generated statement type, and produces results superior to the strongest baselines in terms of quality and factuality-diversity trade-off.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#65292;&#29992;&#20110;&#39034;&#24207;&#36873;&#25321;TLM&#39044;&#35757;&#32451;&#36229;&#21442;&#25968;&#65292;&#26088;&#22312;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#12290;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;Thompson&#25277;&#26679;&#65288;GP-TS&#65289;&#31639;&#27861;&#65292;&#21152;&#36895;Pre-training&#36807;&#31243;&#24182;&#38477;&#20302;MLM&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2203.13151</link><description>&lt;p&gt;
&#22810;&#33218;&#32769;&#34382;&#26426;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#36164;&#28304;&#39640;&#25928;&#12289;&#22312;&#32447;&#20248;&#21270;&#65306;&#21160;&#24577;&#36974;&#30422;&#30340;&#20351;&#29992;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking. (arXiv:2203.13151v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#65292;&#29992;&#20110;&#39034;&#24207;&#36873;&#25321;TLM&#39044;&#35757;&#32451;&#36229;&#21442;&#25968;&#65292;&#26088;&#22312;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#12290;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;Thompson&#25277;&#26679;&#65288;GP-TS&#65289;&#31639;&#27861;&#65292;&#21152;&#36895;Pre-training&#36807;&#31243;&#24182;&#38477;&#20302;MLM&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#39044;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;TLM&#65289;&#12290; TLM&#39044;&#35757;&#32451;&#38656;&#35201;&#39640;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#24341;&#20837;&#35768;&#22810;&#26410;&#35299;&#20915;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#20363;&#22914;&#36873;&#25321;&#20854;&#39044;&#35757;&#32451;&#36229;&#21442;&#25968;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#65292;&#29992;&#20110;&#39034;&#24207;&#36873;&#25321;TLM&#39044;&#35757;&#32451;&#36229;&#21442;&#25968;&#65292;&#26088;&#22312;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#12290; &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;Thompson&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#20854;&#39034;&#24207;&#26368;&#23567;&#21270;&#30340;&#24102;&#26377;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#20195;&#29702;&#39640;&#26031;&#36807;&#31243;&#22870;&#21169;&#27169;&#22411;&#12290; &#25552;&#20986;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;Thompson&#25277;&#26679;&#65288;GP-TS&#65289;&#19981;&#26159;&#20351;&#29992;&#22266;&#23450;&#25513;&#30721;&#27010;&#29575;&#36827;&#34892;MLM&#39044;&#35757;&#32451;&#65292;&#32780;&#26159;&#36890;&#36807;&#39034;&#24207;&#36873;&#25321;&#25913;&#21892;&#24615;&#33021;&#30340;&#25513;&#30721;&#36229;&#21442;&#25968;&#26469;&#21152;&#36895;&#39044;&#35757;&#32451;&#12290; &#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;GP-TS&#22914;&#20309;&#39640;&#25928;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#21363;&#22312;&#23569;&#37327;&#36845;&#20195;&#20013;&#23454;&#29616;&#26356;&#20302;&#30340;MLM&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design and evaluate a Bayesian optimization framework for resource efficient pre-training of Transformer-based language models (TLMs). TLM pre-training requires high computational resources and introduces many unresolved design choices, such as selecting its pre-training hyperparameters. We propose a multi-armed bandit framework for the sequential selection of TLM pre-training hyperparameters, aimed at optimizing language model performance, in a resource efficient manner. We design a Thompson sampling algorithm, with a surrogate Gaussian process reward model of the Masked Language Model (MLM) pre-training objective, for its sequential minimization. Instead of MLM pre-training with fixed masking probabilities, the proposed Gaussian process-based Thompson sampling (GP-TS) accelerates pre-training by sequentially selecting masking hyperparameters that improve performance. We empirically demonstrate how GP-TS pre-trains language models efficiently, i.e., it achieves lower MLM loss in fe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31895;&#21040;&#32454;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#31867;&#22411;&#22806;&#37096;&#25968;&#25454;&#30340;&#25968;&#25454;&#35821;&#20041;&#34701;&#21512;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#22788;&#29702;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2201.02732</link><description>&lt;p&gt;
C2-CRS&#65306;&#38754;&#21521;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#31895;&#21040;&#32454;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System. (arXiv:2201.02732v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.02732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31895;&#21040;&#32454;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#31867;&#22411;&#22806;&#37096;&#25968;&#25454;&#30340;&#25968;&#25454;&#35821;&#20041;&#34701;&#21512;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#22788;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#21521;&#29992;&#25143;&#25512;&#33616;&#36866;&#21512;&#30340;&#29289;&#21697;&#12290;&#20026;&#20102;&#24320;&#21457;&#26377;&#25928;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#25216;&#26415;&#38382;&#39064;&#26159;&#22914;&#20309;&#20174;&#38750;&#24120;&#26377;&#38480;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#20934;&#30830;&#22320;&#25512;&#26029;&#29992;&#25143;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#32467;&#21512;&#22806;&#37096;&#25968;&#25454;&#26469;&#20016;&#23500;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20026;&#19968;&#20123;&#29305;&#23450;&#31867;&#22411;&#30340;&#22806;&#37096;&#25968;&#25454;&#35774;&#35745;&#34701;&#21512;&#27169;&#22411;&#65292;&#36825;&#19981;&#36866;&#29992;&#20110;&#27169;&#22411;&#21644;&#21033;&#29992;&#22810;&#31867;&#22411;&#30340;&#22806;&#37096;&#25968;&#25454;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#22810;&#31867;&#22411;&#30340;&#22806;&#37096;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31895;&#21040;&#32454;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#25913;&#21892;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#25968;&#25454;&#35821;&#20041;&#34701;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#19981;&#21516;&#30340;&#25968;&#25454;&#20449;&#21495;&#20013;&#25552;&#21462;&#21644;&#34920;&#31034;&#22810;&#31181;&#31890;&#24230;&#30340;&#35821;&#20041;&#21333;&#20803;&#65292;&#28982;&#21518;&#20197;&#31895;&#21040;&#32454;&#30340;&#26041;&#24335;&#23545;&#40784;&#30456;&#20851;&#30340;&#22810;&#31181;&#35821;&#20041;&#21333;&#20803;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#36807;&#31243;&#26469;&#23545;&#29992;&#25143;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommender systems (CRS) aim to recommend suitable items to users through natural language conversations. For developing effective CRSs, a major technical issue is how to accurately infer user preference from very limited conversation context. To address issue, a promising solution is to incorporate external data for enriching the context information. However, prior studies mainly focus on designing fusion models tailored for some specific type of external data, which is not general to model and utilize multi-type external data.  To effectively leverage multi-type external data, we propose a novel coarse-to-fine contrastive learning framework to improve data semantic fusion for CRS. In our approach, we first extract and represent multi-grained semantic units from different data signals, and then align the associated multi-type semantic units in a coarse-to-fine way. To implement this framework, we design both coarse-grained and fine-grained procedures for modeling user 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#20855;&#26377;&#21551;&#21457;&#24615;&#30340;&#20869;&#23481;&#65292;&#39318;&#27425;&#23558;&#21551;&#21457;&#24615;&#24341;&#20837;NLP&#39046;&#22495;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2109.02734</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#21551;&#21457;&#24615;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting Inspiring Content on Social Media. (arXiv:2109.02734v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.02734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#20855;&#26377;&#21551;&#21457;&#24615;&#30340;&#20869;&#23481;&#65292;&#39318;&#27425;&#23558;&#21551;&#21457;&#24615;&#24341;&#20837;NLP&#39046;&#22495;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21551;&#21457;&#23558;&#19968;&#20010;&#20154;&#24102;&#21040;&#30475;&#21040;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36716;&#21270;&#20182;&#20204;&#24863;&#30693;&#33258;&#24049;&#28508;&#21147;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#24515;&#29702;&#23398;&#39046;&#22495;&#65292;&#21551;&#21457;&#24615;&#20960;&#20046;&#27809;&#26377;&#24471;&#21040;&#30740;&#31350;&#65292;&#36825;&#20063;&#26159;NLP&#31038;&#21306;&#27809;&#26377;&#30740;&#31350;&#36807;&#30340;&#20869;&#23481;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#25991;&#26159;&#31532;&#19968;&#31687;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#21551;&#21457;&#24615;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#26088;&#22312;&#33258;&#21160;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#30340;&#21551;&#21457;&#24615;&#20869;&#23481;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#33410;&#30446;&#65292;&#20197;&#25214;&#20986;&#21738;&#20123;&#20869;&#23481;&#21487;&#20197;&#21551;&#21457;&#20154;&#24182;&#30830;&#23450;&#21738;&#20123;&#20027;&#39064;&#20855;&#26377;&#21551;&#21457;&#24615;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;5800&#26465;&#21551;&#21457;&#24615;&#21644;5800&#26465;&#38750;&#21551;&#21457;&#24615;&#30340;&#33521;&#35821;&#20844;&#20849;&#24086;&#23376;&#29420;&#29305;&#30340;ID&#65292;&#36825;&#20123;&#24086;&#23376;&#26469;&#33258;&#20110;Reddit&#20844;&#20849;&#24086;&#23376;&#30340;&#36716;&#23384;&#65292;&#20197;&#21327;&#21161;&#20351;&#29992;&#35821;&#35328;&#21551;&#21457;&#26469;&#33258;&#21160;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#30340;&#33521;&#35821;&#24086;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspiration moves a person to see new possibilities and transforms the way they perceive their own potential. Inspiration has received little attention in psychology, and has not been researched before in the NLP community. To the best of our knowledge, this work is the first to study inspiration through machine learning methods. We aim to automatically detect inspiring content from social media data. To this end, we analyze social media posts to tease out what makes a post inspiring and what topics are inspiring. We release a dataset of 5,800 inspiring and 5,800 non-inspiring English-language public post unique ids collected from a dump of Reddit public posts made available by a third party and use linguistic heuristics to automatically detect which social media English-language posts are inspiring.
&lt;/p&gt;</description></item></channel></rss>