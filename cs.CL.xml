<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#25968;&#23398;&#38382;&#39064;&#30340;&#31526;&#21495;&#21270;&#29256;&#26412;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#25506;&#32034;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#40723;&#21169;&#31526;&#21495;&#21270;&#25512;&#29702;&#19982;&#25968;&#20540;&#31572;&#26696;&#20445;&#25345;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2308.01906</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#31526;&#21495;&#21270;&#25968;&#23398;&#38382;&#39064;&#36827;&#34892;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reasoning in Large Language Models Through Symbolic Math Word Problems. (arXiv:2308.01906v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#25968;&#23398;&#38382;&#39064;&#30340;&#31526;&#21495;&#21270;&#29256;&#26412;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#25506;&#32034;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#40723;&#21169;&#31526;&#21495;&#21270;&#25512;&#29702;&#19982;&#25968;&#20540;&#31572;&#26696;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#35299;&#20915;&#20960;&#20046;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26041;&#24335;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#22810;&#21151;&#33021;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#25968;&#23398;&#38382;&#39064;&#30340;&#31526;&#21495;&#21270;&#29256;&#26412;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#22240;&#20026;&#31526;&#21495;&#34920;&#36798;&#26159;&#23545;&#25968;&#20540;&#31572;&#26696;&#30340;&#8220;&#31616;&#26126;&#35299;&#37322;&#8221;&#12290;&#25105;&#20204;&#21019;&#24314;&#24182;&#20351;&#29992;&#20102;SVAMP&#25968;&#25454;&#38598;&#30340;&#31526;&#21495;&#21270;&#29256;&#26412;&#65292;&#24182;&#21457;&#29616;GPT-3&#30340;davinci-002&#27169;&#22411;&#22312;&#31526;&#21495;&#21270;&#25968;&#23398;&#38382;&#39064;&#19978;&#20063;&#20855;&#26377;&#33391;&#22909;&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#19981;&#20165;&#32771;&#34385;&#20934;&#30830;&#29575;&#65292;&#36824;&#35780;&#20272;&#26368;&#32456;&#31572;&#26696;&#21644;&#25512;&#29702;&#32467;&#26524;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#25968;&#20540;&#21644;&#31526;&#21495;&#21270;&#31572;&#26696;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#40723;&#21169;&#31526;&#21495;&#21270;&#25512;&#29702;&#19982;&#25968;&#20540;&#31572;&#26696;&#20445;&#25345;&#19968;&#33268;&#65292;&#20174;&#32780;&#20351;LLM&#33021;&#22815;&#25552;&#20379;&#31616;&#26126;&#19988;&#21487;&#39564;&#35777;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a "concise explanation" of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#31185;&#23398;&#39044;&#21360;&#26412;&#22312;arXiv&#19978;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#37327;&#21270;&#20102;&#26377;&#22810;&#23569;&#39044;&#21360;&#26412;&#26368;&#32456;&#34987;&#21360;&#21047;&#22312;&#21516;&#34892;&#35780;&#23457;&#30340;&#26399;&#21002;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#21360;&#26412;&#21644;&#26368;&#32456;&#21457;&#34920;&#29256;&#26412;&#30340;&#23545;&#24212;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#26144;&#23556;&#26041;&#27861;&#20351;&#29992;BERT&#12290;</title><link>http://arxiv.org/abs/2308.01899</link><description>&lt;p&gt;
&#26377;&#22810;&#23569;&#39044;&#21360;&#26412;&#23454;&#38469;&#19978;&#34987;&#21360;&#21047;&#20102;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#65306;&#35745;&#31639;&#26426;&#31185;&#23398;&#39044;&#21360;&#26412;&#22312;arXiv&#19978;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How many preprints have actually been printed and why: a case study of computer science preprints on arXiv. (arXiv:2308.01899v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01899
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#31185;&#23398;&#39044;&#21360;&#26412;&#22312;arXiv&#19978;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#37327;&#21270;&#20102;&#26377;&#22810;&#23569;&#39044;&#21360;&#26412;&#26368;&#32456;&#34987;&#21360;&#21047;&#22312;&#21516;&#34892;&#35780;&#23457;&#30340;&#26399;&#21002;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#21360;&#26412;&#21644;&#26368;&#32456;&#21457;&#34920;&#29256;&#26412;&#30340;&#23545;&#24212;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#26144;&#23556;&#26041;&#27861;&#20351;&#29992;BERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#21360;&#26412;&#22312;&#23398;&#26415;&#30028;&#25198;&#28436;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#27491;&#24335;&#25552;&#20132;&#21040;&#26399;&#21002;&#25110;&#20250;&#35758;&#20043;&#21069;&#23558;&#20182;&#20204;&#30340;&#25163;&#31295;&#21457;&#24067;&#21040;&#39044;&#21360;&#26412;&#26381;&#21153;&#22120;&#19978;&#30340;&#21407;&#22240;&#26377;&#24456;&#22810;&#65292;&#20294;&#39044;&#21360;&#26412;&#30340;&#20351;&#29992;&#20063;&#24341;&#21457;&#20102;&#19981;&#23569;&#20105;&#35758;&#65292;&#23588;&#20854;&#26159;&#19982;&#20248;&#20808;&#26435;&#30340;&#22768;&#26126;&#26377;&#20851;&#12290;&#26412;&#25991;&#23545;2008&#24180;&#33267;2017&#24180;&#26399;&#38388;&#25552;&#20132;&#21040;arXiv&#30340;&#35745;&#31639;&#26426;&#31185;&#23398;&#39044;&#21360;&#26412;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#37327;&#21270;&#26368;&#32456;&#26377;&#22810;&#23569;&#39044;&#21360;&#26412;&#22312;&#21516;&#34892;&#35780;&#23457;&#30340;&#22330;&#21512;&#20013;&#34987;&#21360;&#21047;&#12290;&#22312;&#36825;&#20123;&#24050;&#21457;&#34920;&#30340;&#25163;&#31295;&#20013;&#65292;&#26377;&#20123;&#20197;&#19981;&#21516;&#30340;&#26631;&#39064;&#21457;&#34920;&#65292;&#19988;&#26410;&#26356;&#26032;arXiv&#19978;&#30340;&#39044;&#21360;&#26412;&#12290;&#23545;&#20110;&#36825;&#20123;&#25163;&#31295;&#65292;&#20256;&#32479;&#30340;&#27169;&#31946;&#21305;&#37197;&#26041;&#27861;&#26080;&#27861;&#23558;&#39044;&#21360;&#26412;&#19982;&#26368;&#32456;&#21457;&#34920;&#29256;&#26412;&#23545;&#24212;&#36215;&#26469;&#12290;&#37492;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30340;&#26144;&#23556;&#26041;&#27861;&#65292;&#21033;&#29992;Transformers&#20013;&#30340;Bidirectional Encoder Representations (BERT)&#12290;&#21033;&#29992;&#36825;&#31181;&#26032;&#30340;&#26144;&#23556;&#26041;&#27861;&#21644;&#22810;&#31181;&#25968;&#25454;&#26469;&#28304;...
&lt;/p&gt;
&lt;p&gt;
Preprints play an increasingly critical role in academic communities. There are many reasons driving researchers to post their manuscripts to preprint servers before formal submission to journals or conferences, but the use of preprints has also sparked considerable controversy, especially surrounding the claim of priority. In this paper, a case study of computer science preprints submitted to arXiv from 2008 to 2017 is conducted to quantify how many preprints have eventually been printed in peer-reviewed venues. Among those published manuscripts, some are published under different titles and without an update to their preprints on arXiv. In the case of these manuscripts, the traditional fuzzy matching method is incapable of mapping the preprint to the final published version. In view of this issue, we introduce a semantics-based mapping method with the employment of Bidirectional Encoder Representations from Transformers (BERT). With this new mapping method and a plurality of data sou
&lt;/p&gt;</description></item><item><title>Athena 2.0 &#26159; UCSC&#30340;&#20250;&#35805;&#20195;&#29702;&#65292;&#20351;&#29992;&#20102;&#30693;&#35782;&#23548;&#21521;&#30340;&#35821;&#31687;&#27169;&#22411;&#21644;&#29992;&#25143;&#27169;&#22411;&#26469;&#23454;&#29616;&#20010;&#24615;&#21270;&#23545;&#35805;&#65292;&#24182;&#22312;Amazon&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#22823;&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#21019;&#26032;&#24191;&#27867;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2308.01887</link><description>&lt;p&gt;
Athena 2.0: &#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20013;&#30340;&#35821;&#31687;&#21644;&#29992;&#25143;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Athena 2.0: Discourse and User Modeling in Open Domain Dialogue. (arXiv:2308.01887v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01887
&lt;/p&gt;
&lt;p&gt;
Athena 2.0 &#26159; UCSC&#30340;&#20250;&#35805;&#20195;&#29702;&#65292;&#20351;&#29992;&#20102;&#30693;&#35782;&#23548;&#21521;&#30340;&#35821;&#31687;&#27169;&#22411;&#21644;&#29992;&#25143;&#27169;&#22411;&#26469;&#23454;&#29616;&#20010;&#24615;&#21270;&#23545;&#35805;&#65292;&#24182;&#22312;Amazon&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#22823;&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#21019;&#26032;&#24191;&#27867;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#20195;&#29702;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#19981;&#26029;&#22686;&#38271;&#65292;&#35768;&#22810;&#20154;&#27599;&#22825;&#37117;&#19982;&#23427;&#20204;&#36827;&#34892;&#20132;&#20114;&#12290;&#34429;&#28982;&#35768;&#22810;&#20250;&#35805;&#20195;&#29702;&#20805;&#24403;&#20010;&#20154;&#21161;&#25163;&#65292;&#20294;&#23427;&#20204;&#21487;&#20197;&#26377;&#35768;&#22810;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#19968;&#20123;&#26159;&#38754;&#21521;&#20219;&#21153;&#30340;&#65292;&#20363;&#22914;&#25552;&#20379;&#38134;&#34892;&#23458;&#25143;&#25903;&#25345;&#25110;&#36827;&#34892;&#39044;&#35746;&#12290;&#20854;&#20182;&#20154;&#21017;&#26088;&#22312;&#34920;&#36798;&#21516;&#24773;&#65292;&#24182;&#19982;&#29992;&#25143;&#24314;&#31435;&#24773;&#24863;&#32852;&#31995;&#12290;Alexa Prize Challenge&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#31038;&#20132;&#26426;&#22120;&#20154;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36827;&#34892;&#36830;&#36143;&#30340;&#23545;&#35805;&#65292;&#28041;&#21450;&#29992;&#25143;&#24863;&#20852;&#36259;&#30340;&#21508;&#31181;&#28909;&#38376;&#35805;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#38463;&#33922;&#23068;2.0&#65292;UCSC&#30340;&#20122;&#39532;&#36874;&#31038;&#20132;&#26426;&#22120;&#20154;&#22823;&#25361;&#25112;4&#20013;&#30340;&#20250;&#35805;&#20195;&#29702;&#12290;&#38463;&#33922;&#23068;2.0&#20351;&#29992;&#26032;&#39062;&#30340;&#30693;&#35782;&#23548;&#21521;&#30340;&#35821;&#31687;&#27169;&#22411;&#65292;&#36319;&#36394;&#38463;&#33922;&#23068;&#24341;&#20837;&#23545;&#35805;&#20013;&#30340;&#23454;&#20307;&#38142;&#25509;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#38480;&#21046;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#65292;&#20197;&#21450;&#20849;&#25351;&#28040;&#35299;&#12290;&#38463;&#33922;&#23068;2.0&#36824;&#20381;&#36182;&#20110;&#29992;&#25143;&#27169;&#22411;&#65292;&#20197;&#20010;&#24615;&#21270;&#20027;&#39064;&#36873;&#25321;&#21644;&#23545;&#35805;&#30340;&#20854;&#20182;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational agents are consistently growing in popularity and many people interact with them every day. While many conversational agents act as personal assistants, they can have many different goals. Some are task-oriented, such as providing customer support for a bank or making a reservation. Others are designed to be empathetic and to form emotional connections with the user. The Alexa Prize Challenge aims to create a socialbot, which allows the user to engage in coherent conversations, on a range of popular topics that will interest the user. Here we describe Athena 2.0, UCSC's conversational agent for Amazon's Socialbot Grand Challenge 4. Athena 2.0 utilizes a novel knowledge-grounded discourse model that tracks the entity links that Athena introduces into the dialogue, and uses them to constrain named-entity recognition and linking, and coreference resolution. Athena 2.0 also relies on a user model to personalize topic selection and other aspects of the conversation to individ
&lt;/p&gt;</description></item><item><title>"Thespian"&#26159;&#19968;&#31181;&#22810;&#35282;&#33394;&#30340;&#25991;&#26412;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#20855;&#26377;&#23398;&#20064;&#27169;&#20223;&#22810;&#20010;&#35282;&#33394;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26580;&#24615;&#25552;&#31034;&#36827;&#34892;&#25351;&#23548;&#12290;&#35813;&#26694;&#26550;&#36824;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#20197;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#26032;&#35282;&#33394;&#65292;&#24182;&#22312;&#22810;&#35282;&#33394;&#23398;&#20064;&#21644;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.01872</link><description>&lt;p&gt;
Thespian: &#22810;&#35282;&#33394;&#25991;&#26412;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Thespian: Multi-Character Text Role-Playing Game Agents. (arXiv:2308.01872v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01872
&lt;/p&gt;
&lt;p&gt;
"Thespian"&#26159;&#19968;&#31181;&#22810;&#35282;&#33394;&#30340;&#25991;&#26412;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#20855;&#26377;&#23398;&#20064;&#27169;&#20223;&#22810;&#20010;&#35282;&#33394;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26580;&#24615;&#25552;&#31034;&#36827;&#34892;&#25351;&#23548;&#12290;&#35813;&#26694;&#26550;&#36824;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#20197;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#26032;&#35282;&#33394;&#65292;&#24182;&#22312;&#22810;&#35282;&#33394;&#23398;&#20064;&#21644;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20882;&#38505;&#28216;&#25103;&#21644;&#25991;&#26412;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#26159;&#24378;&#21270;&#23398;&#20064;&#28216;&#25103;&#26234;&#33021;&#20307;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#25991;&#26412;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#26159;&#24320;&#25918;&#24335;&#29615;&#22659;&#65292;&#26234;&#33021;&#20307;&#24517;&#39035;&#24544;&#23454;&#22320;&#25198;&#28436;&#29305;&#23450;&#35282;&#33394;&#12290;&#25991;&#31456;&#32771;&#34385;&#21040;&#20102;&#23383;&#31526;&#21644;&#28436;&#21592;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#28436;&#21592;&#26234;&#33021;&#20307;&#33021;&#22815;&#25198;&#28436;&#22810;&#20010;&#35282;&#33394;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"Thespian"&#30340;&#26694;&#26550;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#27169;&#20223;&#22810;&#20010;&#35282;&#33394;&#65292;&#24182;&#20351;&#29992;&#26580;&#24615;&#25552;&#31034;&#26469;&#25351;&#23548;&#20854;&#22312;&#20219;&#20309;&#26102;&#20505;&#25198;&#28436;&#29305;&#23450;&#35282;&#33394;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;&#27880;&#24847;&#26426;&#21046;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#20197;&#23569;&#37327;&#31034;&#20363;&#30340;&#26041;&#24335;&#23398;&#20064;&#22522;&#20110;&#20808;&#21069;&#23398;&#20064;&#30340;&#35282;&#33394;&#30340;&#26032;&#35282;&#33394;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#22312;&#22810;&#35282;&#33394;&#23398;&#20064;&#21644;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26234;&#33021;&#20307;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-adventure games and text role-playing games are grand challenges for reinforcement learning game playing agents. Text role-playing games are open-ended environments where an agent must faithfully play a particular character. We consider the distinction between characters and actors, where an actor agent has the ability to play multiple characters. We present a framework we call a thespian agent that can learn to emulate multiple characters along with a soft prompt that can be used to direct it as to which character to play at any time. We further describe an attention mechanism that allows the agent to learn new characters that are based on previously learned characters in a few-shot fashion. We show that our agent outperforms the state of the art agent framework in multi-character learning and few-shot learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#33258;&#21160;&#20026;&#31454;&#25216;&#32534;&#31243;&#38382;&#39064;&#36827;&#34892;&#26631;&#31614;&#65292;&#20197;&#24110;&#21161;&#31243;&#24207;&#21592;&#25214;&#21040;&#36866;&#21512;&#20182;&#20204;&#30693;&#35782;&#21644;&#20852;&#36259;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01863</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#31454;&#25216;&#32534;&#31243;&#38382;&#39064;&#30340;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Tag Prediction of Competitive Programming Problems using Deep Learning Techniques. (arXiv:2308.01863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#33258;&#21160;&#20026;&#31454;&#25216;&#32534;&#31243;&#38382;&#39064;&#36827;&#34892;&#26631;&#31614;&#65292;&#20197;&#24110;&#21161;&#31243;&#24207;&#21592;&#25214;&#21040;&#36866;&#21512;&#20182;&#20204;&#30693;&#35782;&#21644;&#20852;&#36259;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#37327;&#22823;&#24133;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#12290;&#31454;&#25216;&#32534;&#31243;&#26159;&#19968;&#31181;&#22521;&#20859;&#32534;&#31243;&#33021;&#21147;&#12289;&#36923;&#36753;&#24605;&#32500;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#21463;&#27426;&#36814;&#26041;&#27861;&#12290;&#30001;&#20110;&#38382;&#39064;&#25968;&#37327;&#20247;&#22810;&#12289;&#20027;&#39064;&#12289;&#38590;&#24230;&#21644;&#31181;&#31867;&#32321;&#22810;&#65292;&#21363;&#20351;&#23545;&#20110;&#26377;&#32463;&#39564;&#30340;&#31243;&#24207;&#21592;&#26469;&#35828;&#65292;&#36941;&#21382;&#36825;&#20123;&#38382;&#39064;&#20063;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#24110;&#21161;&#31243;&#24207;&#21592;&#25214;&#21040;&#36866;&#21512;&#20182;&#20204;&#30693;&#35782;&#21644;&#20852;&#36259;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#23545;&#38382;&#39064;&#36827;&#34892;&#26631;&#31614;&#21270;&#65292;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25991;&#26412;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#30830;&#23450;&#31454;&#25216;&#32534;&#31243;&#38382;&#39064;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past decade, the amount of research being done in the fields of machine learning and deep learning, predominantly in the area of natural language processing (NLP), has risen dramatically. A well-liked method for developing programming abilities like logic building and problem solving is competitive programming. It can be tough for novices and even veteran programmers to traverse the wide collection of questions due to the massive number of accessible questions and the variety of themes, levels of difficulty, and questions offered. In order to help programmers find questions that are appropriate for their knowledge and interests, there is a need for an automated method. This can be done using automated tagging of the questions using Text Classification. Text classification is one of the important tasks widely researched in the field of Natural Language Processing. In this paper, we present a way to use text classification techniques to determine the domain of a competitive progra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#26356;&#28145;&#20837;&#21644;&#26356;&#24191;&#27867;&#30340;LLM&#32593;&#32476;&#26469;&#25506;&#32034;&#22312;LLM&#35780;&#20272;&#20013;&#20135;&#29983;&#26356;&#20844;&#24179;&#32467;&#26524;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#29983;&#25104;&#22810;&#20010;&#31070;&#32463;&#20803;&#35282;&#33394;&#65292;&#24182;&#26681;&#25454;&#28145;&#24230;&#32593;&#32476;&#30340;&#23618;&#32423;&#20851;&#31995;&#65292;&#25552;&#20379;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.01862</link><description>&lt;p&gt;
&#26356;&#24191;&#27867;&#21644;&#26356;&#28145;&#20837;&#30340;LLM&#32593;&#32476;&#33021;&#22815;&#26356;&#20844;&#24179;&#22320;&#35780;&#20272;LLM
&lt;/p&gt;
&lt;p&gt;
Wider and Deeper LLM Networks are Fairer LLM Evaluators. (arXiv:2308.01862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#26356;&#28145;&#20837;&#21644;&#26356;&#24191;&#27867;&#30340;LLM&#32593;&#32476;&#26469;&#25506;&#32034;&#22312;LLM&#35780;&#20272;&#20013;&#20135;&#29983;&#26356;&#20844;&#24179;&#32467;&#26524;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#29983;&#25104;&#22810;&#20010;&#31070;&#32463;&#20803;&#35282;&#33394;&#65292;&#24182;&#26681;&#25454;&#28145;&#24230;&#32593;&#32476;&#30340;&#23618;&#32423;&#20851;&#31995;&#65292;&#25552;&#20379;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;LLM&#29983;&#25104;&#30340;&#21709;&#24212;&#36136;&#37327;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#35780;&#20272;&#21709;&#24212;&#26159;&#21542;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#26102;&#12290;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;LLM&#26412;&#36523;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#29420;&#31435;&#35780;&#20272;&#26469;&#31283;&#23450;&#32467;&#26524;&#65292;&#31867;&#20284;&#20110;&#21333;&#23618;&#31364;LLM&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#30001;&#22266;&#23450;&#25968;&#37327;&#30340;&#31070;&#32463;&#20803;&#32452;&#25104;&#65292;&#27599;&#20010;&#31070;&#32463;&#20803;&#37117;&#26159;&#30456;&#21516;&#30340;LLM&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24191;&#27867;&#30740;&#31350;&#26469;&#25506;&#35752;&#26356;&#28145;&#20837;&#21644;&#26356;&#24191;&#27867;&#30340;&#32593;&#32476;&#26159;&#21542;&#21487;&#20197;&#23548;&#33268;&#26356;&#20844;&#24179;&#30340;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21463;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#19981;&#21516;&#31070;&#32463;&#20803;&#36127;&#36131;&#26816;&#27979;&#19981;&#21516;&#27010;&#24565;&#30340;&#35266;&#23519;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#27599;&#20010;&#35780;&#20272;&#26679;&#26412;&#36866;&#24212;&#24615;&#22320;&#29983;&#25104;&#23613;&#21487;&#33021;&#22810;&#30340;&#31070;&#32463;&#20803;&#35282;&#33394;&#12290;&#27599;&#20010;&#35282;&#24230;&#23545;&#24212;&#20110;&#31532;&#19968;&#23618;&#20013;&#29305;&#23450;LLM&#31070;&#32463;&#20803;&#30340;&#35282;&#33394;&#12290;&#22312;&#38543;&#21518;&#30340;&#23618;&#20013;&#65292;&#25105;&#20204;&#36981;&#24490;&#28145;&#24230;&#32593;&#32476;&#20013;&#36739;&#39640;&#23618;&#36127;&#36131;&#30456;&#20851;&#24615;&#35299;&#35868;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring the quality of responses generated by LLMs is a challenging task, particularly when it comes to evaluating whether the response is aligned with human preference. A novel approach involves using the LLM itself to make evaluation and stabilizing the results through multiple independent evaluations, similar to a single-layer narrow LLM network. This network consists of a fixed number of neurons, with each neuron being the same LLM. In this paper, we draw upon the extensive research on deep neural networks to explore whether deeper and wider networks can lead to fairer evaluations. Specifically, inspired by the observation that different neurons in a neural network are responsible for detecting different concepts, we first adaptively generate as many neuron roles as possible for each evaluation sample. Each perspective corresponds to the role of a specific LLM neuron in the first layer. In subsequent layers, we follow the idea that higher layers in deep networks are responsible f
&lt;/p&gt;</description></item><item><title>ClassEval&#26159;&#19968;&#31181;&#25163;&#24037;&#26500;&#24314;&#30340;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#22312;&#36825;&#19968;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#35780;&#20272;LLMs&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#12290;GPT-4&#21644;GPT-3.5&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#23545;&#20854;&#20182;LLMs&#26356;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.01861</link><description>&lt;p&gt;
ClassEval: &#19968;&#31181;&#25163;&#24037;&#26500;&#24314;&#30340;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation. (arXiv:2308.01861v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01861
&lt;/p&gt;
&lt;p&gt;
ClassEval&#26159;&#19968;&#31181;&#25163;&#24037;&#26500;&#24314;&#30340;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#22312;&#36825;&#19968;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#35780;&#20272;LLMs&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#12290;GPT-4&#21644;GPT-3.5&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#23545;&#20854;&#20182;LLMs&#26356;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20195;&#30721;&#29983;&#25104;&#22330;&#26223;&#20013;&#35780;&#20272;LLMs&#65292;&#21363;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#12290;&#25105;&#20204;&#39318;&#20808;&#25163;&#21160;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;ClassEval&#65292;&#20854;&#20013;&#21253;&#21547;100&#20010;&#31867;&#32423;Python&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#24635;&#20849;&#32791;&#26102;&#32422;500&#20154;&#23567;&#26102;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23545;11&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#30740;&#31350;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20197;&#19979;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#29616;&#26377;&#30340;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#24615;&#33021;&#35201;&#36828;&#20302;&#20110;&#29420;&#31435;&#30340;&#26041;&#27861;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65288;&#22914;HumanEval&#65289;&#65307;&#32780;&#26041;&#27861;&#32423;&#30340;&#32534;&#30721;&#33021;&#21147;&#19981;&#33021;&#31561;&#21516;&#22320;&#21453;&#26144;LLMs&#22312;&#31867;&#32423;&#32534;&#30721;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-4&#21644;GPT-3.5&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#20173;&#28982;&#34920;&#29616;&#20986;&#30456;&#23545;&#20854;&#20182;LLMs&#26356;&#21331;&#36234;&#30340;&#20248;&#21183;&#65292;&#32780;&#20108;&#32447;&#27169;&#22411;&#21253;&#25324;Instruct-Starcoder&#65292;Instruct-Codegen&#21644;Wizardcoder&#22312;&#24615;&#33021;&#19978;&#38750;&#24120;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e. class-level code generation. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on it, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we have the following main findings. First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior than other LLMs on class-level code generation, and the second-tier models includes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very similar performance. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#20837;&#20405;&#21644;&#35821;&#27861;&#20998;&#26512;&#24341;&#23548;&#65292;&#36880;&#27493;&#36866;&#24212;&#39044;&#35757;&#32451;&#20998;&#24067;&#65292;&#24182;&#22312;MultiWoZ&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.01849</link><description>&lt;p&gt;
&#21477;&#23376;&#32534;&#30721;&#20219;&#21153;&#30340;&#35838;&#31243;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curricular Transfer Learning for Sentence Encoded Tasks. (arXiv:2308.01849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#20837;&#20405;&#21644;&#35821;&#27861;&#20998;&#26512;&#24341;&#23548;&#65292;&#36880;&#27493;&#36866;&#24212;&#39044;&#35757;&#32451;&#20998;&#24067;&#65292;&#24182;&#22312;MultiWoZ&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#35768;&#22810;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26631;&#20934;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#20998;&#24067;&#28418;&#31227;&#26102;&#65292;&#20363;&#22914;&#65292;&#23545;&#35805;&#29615;&#22659;&#19979;&#65292;&#36825;&#20123;&#25910;&#30410;&#24448;&#24448;&#20250;&#20943;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#8220;&#25968;&#25454;&#20837;&#20405;&#8221;&#21644;&#35821;&#27861;&#20998;&#26512;&#24341;&#23548;&#30340;&#39044;&#35757;&#32451;&#27493;&#39588;&#24207;&#21015;&#65288;&#35838;&#31243;&#65289;&#65292;&#20801;&#35768;&#22312;&#39044;&#35757;&#32451;&#20998;&#24067;&#20043;&#38388;&#36827;&#19968;&#27493;&#36880;&#28176;&#36866;&#24212;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#19982;&#20854;&#20182;&#24050;&#30693;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MultiWoZ&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning language models in a downstream task is the standard approach for many state-of-the-art methodologies in the field of NLP. However, when the distribution between the source task and target task drifts, \textit{e.g.}, conversational environments, these gains tend to be diminished. This article proposes a sequence of pre-training steps (a curriculum) guided by "data hacking" and grammar analysis that allows further gradual adaptation between pre-training distributions. In our experiments, we acquire a considerable improvement from our method compared to other known pre-training approaches for the MultiWoZ task.
&lt;/p&gt;</description></item><item><title>XNLP&#28436;&#31034;&#31995;&#32479;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#39640;&#24615;&#33021;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24179;&#21488;&#65292;&#36890;&#36807;&#25552;&#20379;&#36890;&#29992;&#30340;&#24314;&#27169;&#26041;&#27861;&#12289;&#35299;&#37322;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20114;&#21160;&#24615;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;XNLP&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.01846</link><description>&lt;p&gt;
XNLP&#65306;&#36890;&#29992;&#32467;&#26500;&#21270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20132;&#20114;&#28436;&#31034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
XNLP: An Interactive Demonstration System for Universal Structured NLP. (arXiv:2308.01846v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01846
&lt;/p&gt;
&lt;p&gt;
XNLP&#28436;&#31034;&#31995;&#32479;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#39640;&#24615;&#33021;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24179;&#21488;&#65292;&#36890;&#36807;&#25552;&#20379;&#36890;&#29992;&#30340;&#24314;&#27169;&#26041;&#27861;&#12289;&#35299;&#37322;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20114;&#21160;&#24615;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;XNLP&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;XNLP&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#20010;&#37325;&#35201;&#23376;&#38598;&#65292;&#28041;&#21450;&#29702;&#35299;&#25991;&#26412;&#30340;&#24213;&#23618;&#35821;&#20041;&#25110;&#21477;&#27861;&#32467;&#26500;&#65292;&#20026;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#22522;&#26412;&#32452;&#20214;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#19968;&#20123;&#21162;&#21147;&#25506;&#32034;&#29305;&#23450;&#31867;&#21035;&#30340;XNLP&#20219;&#21153;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#32479;&#19968;&#25152;&#26377;XNLP&#20219;&#21153;&#30340;&#32508;&#21512;&#26377;&#25928;&#26041;&#27861;&#20173;&#28982;&#19981;&#23436;&#21892;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;XNLP&#28436;&#31034;&#31995;&#32479;&#23545;&#20110;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#21508;&#31181;XNLP&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#29616;&#26377;&#24179;&#21488;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#20363;&#22914;&#21482;&#25903;&#25345;&#23569;&#25968;XNLP&#20219;&#21153;&#65292;&#32570;&#20047;&#20114;&#21160;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;XNLP&#28436;&#31034;&#24179;&#21488;&#65292;&#20854;&#20013;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;LLM&#23454;&#29616;&#36890;&#29992;XNLP&#65292;&#36890;&#36807;&#19968;&#20010;&#27169;&#22411;&#23454;&#29616;&#39640;&#36890;&#29992;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#22810;&#20010;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#21253;&#25324;&#36890;&#29992;XNLP&#24314;&#27169;&#12289;&#39640;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20114;&#21160;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured Natural Language Processing (XNLP) is an important subset of NLP that entails understanding the underlying semantic or syntactic structure of texts, which serves as a foundational component for many downstream applications. Despite certain recent efforts to explore universal solutions for specific categories of XNLP tasks, a comprehensive and effective approach for unifying all XNLP tasks long remains underdeveloped. In the meanwhile, while XNLP demonstration systems are vital for researchers exploring various XNLP tasks, existing platforms can be limited to, e.g., supporting few XNLP tasks, lacking interactivity and universalness. To this end, we propose an advanced XNLP demonstration platform, where we propose leveraging LLM to achieve universal XNLP, with one model for all with high generalizability. Overall, our system advances in multiple aspects, including universal XNLP modeling, high performance, interpretability, scalability, and interactivity, providing a unified p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#24739;&#32773;&#37319;&#35775;&#21644;&#20020;&#24202;&#25551;&#36848;&#39044;&#27979;&#31934;&#31070;&#21151;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#25233;&#37057;&#30151;&#35780;&#20998;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#19982;&#20154;&#31867;&#20020;&#24202;&#35780;&#20272;&#32773;&#30340;&#32467;&#26524;&#30456;&#36817;&#65292;&#25581;&#31034;&#20102;&#36890;&#29992;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#31934;&#31070;&#29366;&#20917;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01834</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#31934;&#31070;&#29366;&#20917;&#26041;&#38754;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Capability of Large Language Models to Measure Psychiatric Functioning. (arXiv:2308.01834v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#24739;&#32773;&#37319;&#35775;&#21644;&#20020;&#24202;&#25551;&#36848;&#39044;&#27979;&#31934;&#31070;&#21151;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#25233;&#37057;&#30151;&#35780;&#20998;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#19982;&#20154;&#31867;&#20020;&#24202;&#35780;&#20272;&#32773;&#30340;&#32467;&#26524;&#30456;&#36817;&#65292;&#25581;&#31034;&#20102;&#36890;&#29992;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#31934;&#31070;&#29366;&#20917;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#21307;&#23398;&#30693;&#35782;&#65288;Med-PaLM 2&#65289;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27809;&#26377;&#32463;&#36807;&#29305;&#23450;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24739;&#32773;&#37319;&#35775;&#21644;&#20020;&#24202;&#25551;&#36848;&#26469;&#39044;&#27979;&#31934;&#31070;&#21151;&#33021;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#26469;&#25552;&#21462;&#20272;&#35745;&#30340;&#20020;&#24202;&#35780;&#20998;&#21644;&#35786;&#26029;&#65292;&#20998;&#26512;&#20102;145&#20363;&#25233;&#37057;&#30151;&#21644;115&#20363;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#35780;&#20272;&#20197;&#21450;46&#20363;&#20020;&#24202;&#26696;&#20363;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Med-PaLM 2&#33021;&#22815;&#22312;&#19968;&#31995;&#21015;&#31934;&#31070;&#30142;&#30149;&#20013;&#35780;&#20272;&#31934;&#31070;&#21151;&#33021;&#65292;&#20854;&#20013;&#23545;&#22522;&#20110;&#26631;&#20934;&#35780;&#20272;&#30340;&#25233;&#37057;&#30151;&#35780;&#20998;&#30340;&#39044;&#27979;&#34920;&#29616;&#26368;&#20339;&#65288;&#20934;&#30830;&#29575;&#33539;&#22260;= 0.80-0.84&#65289;&#65292;&#36825;&#20123;&#39044;&#27979;&#32467;&#26524;&#22312;&#32479;&#35745;&#19978;&#19982;&#20154;&#31867;&#20020;&#24202;&#35780;&#20272;&#32773;&#30340;&#32467;&#26524;&#26080;&#27861;&#21306;&#20998;&#65288;t(1,144) = 1.20&#65292;p = 0.23&#65289;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#36890;&#29992;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#31934;&#31070;&#29366;&#20917;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current work investigates the capability of Large language models (LLMs) that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2) to predict psychiatric functioning from patient interviews and clinical descriptions without being trained to do so. To assess this, n = 145 depression and n =115 PTSD assessments and n = 46 clinical case studies across high prevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma and stress, Addictive disorders) were analyzed using prompts to extract estimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is capable of assessing psychiatric functioning across a range of psychiatric conditions with the strongest performance being the prediction of depression scores based on standardized assessments (Accuracy range= 0.80 - 0.84) which were statistically indistinguishable from human clinical raters t(1,144) = 1.20; p = 0.23. Results show the potential for general clinical language models to f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;&#22810;&#35821;&#31181;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#35821;&#38899;&#21512;&#25104;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#32534;&#30721;&#30340;&#35821;&#38899;&#29305;&#24449;&#30340;&#37327;&#21270;&#34920;&#31034;&#35821;&#38899;&#38899;&#39057;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#20266;&#25991;&#26412;&#26469;&#24314;&#31435;&#32479;&#19968;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#34920;&#31034;&#12290;&#28982;&#21518;&#36890;&#36807;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#27169;&#22411;&#65292;&#21033;&#29992;&#21333;&#20803;&#21040;&#21333;&#20803;&#32763;&#35793;&#30446;&#26631;&#23558;&#21475;&#35821;&#35821;&#35328;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24314;&#31435;&#23545;&#21475;&#35821;&#35821;&#35328;&#30340;&#29702;&#35299;&#24182;&#23558;&#20854;&#30456;&#20851;&#32852;&#21040;&#19981;&#21516;&#30340;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2308.01831</link><description>&lt;p&gt;
&#36890;&#36807;&#32479;&#19968;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#20197;&#21450;&#21333;&#20803;&#21040;&#21333;&#20803;&#30340;&#32763;&#35793;&#23454;&#29616;&#22810;&#23545;&#22810;&#21475;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Many-to-Many Spoken Language Translation via Unified Speech and Text Representation Learning with Unit-to-Unit Translation. (arXiv:2308.01831v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;&#22810;&#35821;&#31181;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#35821;&#38899;&#21512;&#25104;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#32534;&#30721;&#30340;&#35821;&#38899;&#29305;&#24449;&#30340;&#37327;&#21270;&#34920;&#31034;&#35821;&#38899;&#38899;&#39057;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#20266;&#25991;&#26412;&#26469;&#24314;&#31435;&#32479;&#19968;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#34920;&#31034;&#12290;&#28982;&#21518;&#36890;&#36807;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#27169;&#22411;&#65292;&#21033;&#29992;&#21333;&#20803;&#21040;&#21333;&#20803;&#32763;&#35793;&#30446;&#26631;&#23558;&#21475;&#35821;&#35821;&#35328;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24314;&#31435;&#23545;&#21475;&#35821;&#35821;&#35328;&#30340;&#29702;&#35299;&#24182;&#23558;&#20854;&#30456;&#20851;&#32852;&#21040;&#19981;&#21516;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#23398;&#20064;&#22810;&#35821;&#31181;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#32479;&#19968;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#35821;&#38899;&#21512;&#25104;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#38899;&#21333;&#20803;&#34920;&#31034;&#22810;&#35821;&#31181;&#35821;&#38899;&#38899;&#39057;&#65292;&#36825;&#20123;&#35821;&#38899;&#21333;&#20803;&#26159;&#20174;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#32534;&#30721;&#30340;&#35821;&#38899;&#29305;&#24449;&#30340;&#37327;&#21270;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#38899;&#39057;&#35270;&#20026;&#20266;&#25991;&#26412;&#24182;&#19987;&#27880;&#20110;&#20854;&#35821;&#35328;&#20869;&#23481;&#65292;&#20174;&#32780;&#26500;&#24314;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#32479;&#19968;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#35821;&#31181;&#25968;&#25454;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#21333;&#20803;&#21040;&#21333;&#20803;&#32763;&#35793;&#65288;UTUT&#65289;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23558;&#32534;&#30721;&#22120;&#19982;&#28304;&#35821;&#35328;&#26631;&#35760;&#21644;&#35299;&#30721;&#22120;&#19982;&#30446;&#26631;&#35821;&#35328;&#26631;&#35760;&#30456;&#20851;&#32852;&#65292;&#20248;&#21270;&#27169;&#22411;&#20197;&#23558;&#21475;&#35821;&#35821;&#35328;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#30340;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24314;&#31435;&#23545;&#21475;&#35821;&#35821;&#35328;&#30340;&#29702;&#35299;&#20197;&#21450;&#22914;&#20309;&#23558;&#20854;&#19982;&#19981;&#21516;&#35821;&#35328;&#30456;&#20851;&#32852;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a method to learn unified representations of multilingual speech and text with a single model, especially focusing on the purpose of speech synthesis. We represent multilingual speech audio with speech units, the quantized representations of speech features encoded from a self-supervised speech model. Therefore, we can focus on their linguistic content by treating the audio as pseudo text and can build a unified representation of speech and text. Then, we propose to train an encoder-decoder structured model with a Unit-to-Unit Translation (UTUT) objective on multilingual data. Specifically, by conditioning the encoder with the source language token and the decoder with the target language token, the model is optimized to translate the spoken language into that of the target language, in a many-to-many language translation setting. Therefore, the model can build the knowledge of how spoken languages are comprehended and how to relate them to different languages
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#25968;&#23398;&#25512;&#29702;&#26102;&#30340;&#35268;&#27169;&#20851;&#31995;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#25439;&#22833;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25298;&#32477;&#37319;&#26679;&#24494;&#35843;&#25216;&#26415;&#26469;&#22686;&#24378;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.01825</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#25968;&#23398;&#25512;&#29702;&#30340;&#35268;&#27169;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Scaling Relationship on Learning Mathematical Reasoning with Large Language Models. (arXiv:2308.01825v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#25968;&#23398;&#25512;&#29702;&#26102;&#30340;&#35268;&#27169;&#20851;&#31995;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#25439;&#22833;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25298;&#32477;&#37319;&#26679;&#24494;&#35843;&#25216;&#26415;&#26469;&#22686;&#24378;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28982;&#32780;&#20851;&#20110;LLM&#23481;&#37327;&#19982;&#25968;&#23398;&#25512;&#29702;&#20043;&#38388;&#30340;&#35268;&#27169;&#20851;&#31995;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#25439;&#22833;&#12289;&#30417;&#30563;&#25968;&#25454;&#37327;&#21644;&#22686;&#24378;&#25968;&#25454;&#37327;&#23545;&#30417;&#30563;LLM&#30340;&#25512;&#29702;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#25439;&#22833;&#26159;&#27169;&#22411;&#24615;&#33021;&#30340;&#26356;&#22909;&#25351;&#26631;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#30417;&#30563;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#65292;&#24182;&#23454;&#35777;&#21457;&#29616;&#25968;&#25454;&#37327;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#32780;&#36739;&#22909;&#30340;&#27169;&#22411;&#22312;&#25193;&#22823;&#30340;&#30417;&#30563;&#25968;&#25454;&#38598;&#19978;&#25913;&#36827;&#36739;&#23567;&#12290;&#20026;&#20102;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#21162;&#21147;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#26356;&#22810;&#30340;&#25968;&#25454;&#26679;&#26412;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25298;&#32477;&#37319;&#26679;&#24494;&#35843;&#65288;RFT&#65289;&#12290;RFT&#20351;&#29992;&#30417;&#30563;&#27169;&#22411;&#29983;&#25104;&#21644;&#25910;&#38598;&#27491;&#30830;&#30340;&#25512;&#29702;&#36335;&#24452;&#20316;&#20026;&#22686;&#24378;&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#22810;&#19981;&#21516;&#30340;&#25512;&#29702;&#36335;&#24452;&#20316;&#20026;&#22686;&#24378;&#26679;&#26412;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reaso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#35789;&#34920;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#32034;&#39532;&#37324;&#25991;&#26412;&#35789;&#24418;&#36824;&#21407;&#26041;&#27861;&#65292;&#20026;&#35813;&#20302;&#36164;&#28304;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#22987;&#65292;&#24182;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#32034;&#39532;&#37324;&#35789;&#24418;&#36824;&#21407;&#31995;&#32479;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2308.01785</link><description>&lt;p&gt;
&#12298;&#32034;&#39532;&#37324;&#35821;&#35789;&#34920;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#35789;&#24418;&#36824;&#21407;&#26041;&#27861;&#12299;
&lt;/p&gt;
&lt;p&gt;
Lexicon and Rule-based Word Lemmatization Approach for the Somali Language. (arXiv:2308.01785v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#35789;&#34920;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#32034;&#39532;&#37324;&#25991;&#26412;&#35789;&#24418;&#36824;&#21407;&#26041;&#27861;&#65292;&#20026;&#35813;&#20302;&#36164;&#28304;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#22987;&#65292;&#24182;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#32034;&#39532;&#37324;&#35789;&#24418;&#36824;&#21407;&#31995;&#32479;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#24418;&#36824;&#21407;&#26159;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#21333;&#35789;&#30340;&#35789;&#24418;&#27966;&#29983;&#36716;&#21270;&#20026;&#20854;&#35789;&#26681;&#24418;&#24335;&#20197;&#24402;&#19968;&#21270;&#25991;&#26412;&#12290;&#23427;&#34987;&#29992;&#20316;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#21253;&#25324;&#25991;&#26412;&#32034;&#24341;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#31561;&#65289;&#30340;&#26680;&#24515;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#26412;&#25991;&#39318;&#27425;&#24320;&#21457;&#20102;&#32034;&#39532;&#37324;&#35821;&#30340;&#25991;&#26412;&#35789;&#24418;&#36824;&#21407;&#65292;&#36825;&#26159;&#19968;&#31181;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#65292;&#20197;&#21069;&#20960;&#20046;&#27809;&#26377;&#26377;&#25928;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#29305;&#21035;&#20026;&#32034;&#39532;&#37324;&#25991;&#26412;&#24320;&#21457;&#20102;&#19968;&#20010;&#35789;&#34920;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#35789;&#24418;&#36824;&#21407;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26500;&#24314;&#20840;&#38754;&#32034;&#39532;&#37324;&#35821;&#35789;&#24418;&#36824;&#21407;&#31995;&#32479;&#30340;&#36215;&#28857;&#12290;&#32771;&#34385;&#21040;&#35821;&#35328;&#30340;&#35789;&#27861;&#35268;&#21017;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;1247&#20010;&#35789;&#26681;&#35789;&#21644;7173&#20010;&#35789;&#24418;&#27966;&#29983;&#30456;&#20851;&#26415;&#35821;&#30340;&#21021;&#22987;&#35789;&#34920;&#65292;&#24182;&#20016;&#23500;&#20102;&#29992;&#20110;&#35789;&#24418;&#36824;&#21407;&#35789;&#34920;&#20013;&#19981;&#23384;&#22312;&#30340;&#35789;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#26032;&#38395;&#25991;&#31456;&#12289;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21644;&#25991;&#26412;&#22312;&#20869;&#30340;&#21508;&#31181;&#38271;&#24230;&#30340;120&#20010;&#25991;&#26723;&#19978;&#27979;&#35797;&#20102;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lemmatization is a Natural Language Processing (NLP) technique used to normalize text by changing morphological derivations of words to their root forms. It is used as a core pre-processing step in many NLP tasks including text indexing, information retrieval, and machine learning for NLP, among others. This paper pioneers the development of text lemmatization for the Somali language, a low-resource language with very limited or no prior effective adoption of NLP methods and datasets. We especially develop a lexicon and rule-based lemmatizer for Somali text, which is a starting point for a full-fledged Somali lemmatization system for various NLP tasks. With consideration of the language morphological rules, we have developed an initial lexicon of 1247 root words and 7173 derivationally related terms enriched with rules for lemmatizing words not present in the lexicon. We have tested the algorithm on 120 documents of various lengths including news articles, social media posts, and text 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20004;&#20010;&#23454;&#39564;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32972;&#26223;&#19979;&#32416;&#38169;&#38382;&#39064;&#30340;&#20316;&#29992;&#65292;&#31532;&#19968;&#20010;&#23454;&#39564;&#26159;&#23558;&#32416;&#38169;&#20316;&#20026;&#29420;&#31435;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#31532;&#20108;&#20010;&#23454;&#39564;&#21017;&#26159;&#25506;&#35752;&#32416;&#38169;&#20316;&#20026;&#20854;&#20182;NLP&#20219;&#21153;&#30340;&#20934;&#22791;&#20219;&#21153;&#30340;&#27010;&#24565;&#12290;&#30740;&#31350;&#32467;&#26524;&#23558;&#25581;&#31034;&#32416;&#38169;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#23545;&#21508;&#31181;NLP&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.01776</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20173;&#28982;&#23384;&#22312;&#32416;&#38169;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Correction Remain An Problem For Large Language Models?. (arXiv:2308.01776v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20004;&#20010;&#23454;&#39564;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32972;&#26223;&#19979;&#32416;&#38169;&#38382;&#39064;&#30340;&#20316;&#29992;&#65292;&#31532;&#19968;&#20010;&#23454;&#39564;&#26159;&#23558;&#32416;&#38169;&#20316;&#20026;&#29420;&#31435;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#31532;&#20108;&#20010;&#23454;&#39564;&#21017;&#26159;&#25506;&#35752;&#32416;&#38169;&#20316;&#20026;&#20854;&#20182;NLP&#20219;&#21153;&#30340;&#20934;&#22791;&#20219;&#21153;&#30340;&#27010;&#24565;&#12290;&#30740;&#31350;&#32467;&#26524;&#23558;&#25581;&#31034;&#32416;&#38169;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#23545;&#21508;&#31181;NLP&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#26029;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#33021;&#21147;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#32416;&#38169;&#38382;&#39064;&#26159;&#21542;&#20173;&#28982;&#23384;&#22312;&#65311;&#26412;&#25991;&#36890;&#36807;&#36827;&#34892;&#20004;&#20010;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#32416;&#38169;&#30340;&#20316;&#29992;&#12290;&#31532;&#19968;&#20010;&#23454;&#39564;&#23558;&#32416;&#38169;&#20316;&#20026;&#29420;&#31435;&#30340;&#20219;&#21153;&#65292;&#20351;&#29992;GPT&#31867;&#27169;&#22411;&#21644;few-shot learning&#25216;&#26415;&#36827;&#34892;&#38169;&#35823;&#32416;&#27491;&#12290;&#31532;&#20108;&#20010;&#23454;&#39564;&#21017;&#25506;&#35752;&#20102;&#32416;&#38169;&#20316;&#20026;&#20854;&#20182;NLP&#20219;&#21153;&#30340;&#20934;&#22791;&#20219;&#21153;&#30340;&#27010;&#24565;&#65292;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21253;&#21547;&#19968;&#23450;&#31243;&#24230;&#22122;&#22768;&#25110;&#38169;&#35823;&#30340;&#25991;&#26412;&#19978;&#26159;&#21542;&#33021;&#22815;&#23481;&#24525;&#24182;&#34920;&#29616;&#24471;&#36275;&#22815;&#22909;&#12290;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#23454;&#39564;&#65292;&#25105;&#20204;&#26088;&#22312;&#25581;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#32416;&#38169;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#23545;&#21508;&#31181;NLP&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models, such as GPT, continue to advance the capabilities of natural language processing (NLP), the question arises: does the problem of correction still persist? This paper investigates the role of correction in the context of large language models by conducting two experiments. The first experiment focuses on correction as a standalone task, employing few-shot learning techniques with GPT-like models for error correction. The second experiment explores the notion of correction as a preparatory task for other NLP tasks, examining whether large language models can tolerate and perform adequately on texts containing certain levels of noise or errors. By addressing these experiments, we aim to shed light on the significance of correction in the era of large language models and its implications for various NLP applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#39046;&#22495;&#36866;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#30784;&#27169;&#22411;&#20272;&#35745;&#20379;&#24212;&#38142;&#33539;&#22260;3&#25490;&#25918;&#37327;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#37329;&#34701;&#20132;&#26131;&#20195;&#29702;&#36141;&#20080;&#30340;&#21830;&#21697;&#21644;&#26381;&#21153;&#65292;&#21487;&#20197;&#20934;&#30830;&#36319;&#36394;&#33539;&#22260;3&#25490;&#25918;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#20248;&#20110;&#26368;&#26032;&#30340;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#65292;&#24182;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01741</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20272;&#35745;&#20379;&#24212;&#38142;&#25490;&#25918;&#37327;
&lt;/p&gt;
&lt;p&gt;
Supply chain emission estimation using large language models. (arXiv:2308.01741v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#39046;&#22495;&#36866;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#30784;&#27169;&#22411;&#20272;&#35745;&#20379;&#24212;&#38142;&#33539;&#22260;3&#25490;&#25918;&#37327;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#37329;&#34701;&#20132;&#26131;&#20195;&#29702;&#36141;&#20080;&#30340;&#21830;&#21697;&#21644;&#26381;&#21153;&#65292;&#21487;&#20197;&#20934;&#30830;&#36319;&#36394;&#33539;&#22260;3&#25490;&#25918;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#20248;&#20110;&#26368;&#26032;&#30340;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#65292;&#24182;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20225;&#19994;&#38754;&#20020;&#30528;&#23454;&#29616;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65288;SDGs&#65289;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#30446;&#26631;13&#65292;&#19987;&#27880;&#20110;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#21450;&#20854;&#24433;&#21709;&#12290;&#20026;&#20102;&#20943;&#36731;&#27668;&#20505;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#20943;&#23569;&#20225;&#19994;&#30340;&#33539;&#22260;3&#65288;&#20379;&#24212;&#38142;&#25490;&#25918;&#65289;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21344;&#24635;&#25490;&#25918;&#28165;&#21333;&#30340;90&#65285;&#20197;&#19978;&#12290;&#28982;&#32780;&#65292;&#36319;&#36394;&#33539;&#22260;3&#25490;&#25918;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#24517;&#39035;&#20174;&#25968;&#21315;&#20010;&#19978;&#28216;&#21644;&#19979;&#28216;&#20379;&#24212;&#21830;&#20013;&#25910;&#38598;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39318;&#21019;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39046;&#22495;&#36866;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#37329;&#34701;&#20132;&#26131;&#20316;&#20026;&#36141;&#20080;&#30340;&#21830;&#21697;&#21644;&#26381;&#21153;&#30340;&#20195;&#29702;&#26469;&#20272;&#35745;&#33539;&#22260;3&#25490;&#25918;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#24615;&#33021;&#19982;TF-IDF&#12289;word2Vec&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#31561;&#26368;&#26032;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#39046;&#22495;&#36866;&#24212;&#30340;&#22522;&#30784;&#27169;&#22411;&#20248;&#20110;&#26368;&#26032;&#30340;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#65292;&#24182;&#26377;&#30528;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large enterprises face a crucial imperative to achieve the Sustainable Development Goals (SDGs), especially goal 13, which focuses on combating climate change and its impacts. To mitigate the effects of climate change, reducing enterprise Scope 3 (supply chain emissions) is vital, as it accounts for more than 90\% of total emission inventories. However, tracking Scope 3 emissions proves challenging, as data must be collected from thousands of upstream and downstream suppliers.To address the above mentioned challenges, we propose a first-of-a-kind framework that uses domain-adapted NLP foundation models to estimate Scope 3 emissions, by utilizing financial transactions as a proxy for purchased goods and services. We compared the performance of the proposed framework with the state-of-art text classification models such as TF-IDF, word2Vec, and Zero shot learning. Our results show that the domain-adapted foundation model outperforms state-of-the-art text mining techniques and performs as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25925;&#20107;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#20026;&#34892;&#21160;&#24207;&#21015;&#65292;&#20197;&#22312;&#24819;&#35937;&#28216;&#25103;&#20013;&#25351;&#23548;&#20195;&#29702;&#20154;&#36827;&#34892;&#20114;&#21160;&#12290;&#21516;&#26102;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#25991;&#26412;&#20882;&#38505;&#28216;&#25103;&#26469;&#35780;&#20272;&#20195;&#29702;&#20154;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.01734</link><description>&lt;p&gt;
&#29615;&#22659;&#20882;&#38505;&#65306;&#25945;&#25480;ChatGPT&#24320;&#21457;&#22797;&#26434;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ambient Adventures: Teaching ChatGPT on Developing Complex Stories. (arXiv:2308.01734v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25925;&#20107;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#20026;&#34892;&#21160;&#24207;&#21015;&#65292;&#20197;&#22312;&#24819;&#35937;&#28216;&#25103;&#20013;&#25351;&#23548;&#20195;&#29702;&#20154;&#36827;&#34892;&#20114;&#21160;&#12290;&#21516;&#26102;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#25991;&#26412;&#20882;&#38505;&#28216;&#25103;&#26469;&#35780;&#20272;&#20195;&#29702;&#20154;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24819;&#35937;&#21147;&#30340;&#28216;&#25103;&#26159;&#19968;&#31181;&#21019;&#36896;&#21147;&#30340;&#39046;&#22495;&#65292;&#23427;&#21487;&#20197;&#35753;&#26426;&#22120;&#20154;&#20197;&#26356;&#21152;&#25311;&#20154;&#21270;&#30340;&#26041;&#24335;&#19982;&#21608;&#22260;&#19990;&#30028;&#36827;&#34892;&#20114;&#21160;&#12290;&#24819;&#35937;&#28216;&#25103;&#21487;&#20197;&#30475;&#20316;&#26159;&#23558;&#30495;&#23454;&#30340;&#29289;&#20307;&#21644;&#22320;&#28857;&#20316;&#20026;&#34394;&#25311;&#22330;&#26223;&#20013;&#30340;&#24819;&#35937;&#29289;&#20307;&#21644;&#22320;&#28857;&#12290;&#25105;&#20204;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25925;&#20107;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25552;&#31034;&#26469;&#33719;&#21462;&#29992;&#20110;&#24819;&#35937;&#28216;&#25103;&#30340;&#25925;&#20107;&#12290;&#36825;&#20123;&#29983;&#25104;&#30340;&#25925;&#20107;&#23558;&#34987;&#31616;&#21270;&#24182;&#26144;&#23556;&#20026;&#34892;&#21160;&#24207;&#21015;&#65292;&#20197;&#25351;&#23548;&#20195;&#29702;&#20154;&#36827;&#34892;&#24819;&#35937;&#28216;&#25103;&#12290;&#20026;&#20102;&#35780;&#20272;&#20195;&#29702;&#20154;&#26159;&#21542;&#33021;&#25104;&#21151;&#23436;&#25104;&#24819;&#35937;&#28216;&#25103;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#25991;&#26412;&#20882;&#38505;&#28216;&#25103;&#65292;&#27169;&#25311;&#25151;&#23376;&#20316;&#20026;&#20195;&#29702;&#20154;&#20114;&#21160;&#30340;&#28216;&#20048;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imaginative play is an area of creativity that could allow robots to engage with the world around them in a much more personified way. Imaginary play can be seen as taking real objects and locations and using them as imaginary objects and locations in virtual scenarios. We adopted the story generation capability of large language models (LLMs) to obtain the stories used for imaginary play with human-written prompts. Those generated stories will be simplified and mapped into action sequences that can guide the agent in imaginary play. To evaluate whether the agent can successfully finish the imaginary play, we also designed a text adventure game to simulate a house as the playground for the agent to interact.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26412;&#22320;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#19982;&#26412;&#22320;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#32467;&#26500;&#21270;&#21307;&#23398;&#20219;&#21153;&#12290;&#20316;&#32773;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20174;&#30149;&#29702;&#25253;&#21578;&#20013;&#25552;&#21462;&#30142;&#30149;&#20195;&#30721;&#26469;&#23637;&#31034;&#20854;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#21033;&#29992;LLaMA&#27169;&#22411;&#30456;&#36739;&#20110;BERT&#39118;&#26684;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;&#32780;&#19988;&#65292;LLaMA&#27169;&#22411;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#29305;&#21035;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2308.01727</link><description>&lt;p&gt;
&#38024;&#23545;&#22797;&#26434;&#32467;&#26500;&#21270;&#21307;&#23398;&#20219;&#21153;&#30340;&#26412;&#22320;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Local Large Language Models for Complex Structured Medical Tasks. (arXiv:2308.01727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26412;&#22320;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#19982;&#26412;&#22320;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#32467;&#26500;&#21270;&#21307;&#23398;&#20219;&#21153;&#12290;&#20316;&#32773;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20174;&#30149;&#29702;&#25253;&#21578;&#20013;&#25552;&#21462;&#30142;&#30149;&#20195;&#30721;&#26469;&#23637;&#31034;&#20854;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#21033;&#29992;LLaMA&#27169;&#22411;&#30456;&#36739;&#20110;BERT&#39118;&#26684;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;&#32780;&#19988;&#65292;LLaMA&#27169;&#22411;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#29305;&#21035;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#19982;&#26412;&#22320;&#35757;&#32451;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20316;&#32773;&#36890;&#36807;&#20174;&#30149;&#29702;&#25253;&#21578;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#30142;&#30149;&#20195;&#30721;&#26469;&#23637;&#31034;&#20182;&#20204;&#30340;&#26041;&#27861;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#26412;&#22320;LLMs&#65292;&#21487;&#20197;&#36827;&#34892;&#38024;&#23545;&#29305;&#23450;&#29983;&#25104;&#25351;&#20196;&#30340;&#24494;&#35843;&#65292;&#24182;&#25552;&#20379;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#20316;&#32773;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;15&#19975;&#20010;&#26410;&#32534;&#36753;&#30340;&#22806;&#31185;&#30149;&#29702;&#25253;&#21578;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22806;&#35266;&#25551;&#36848;&#12289;&#26368;&#32456;&#35786;&#26029;&#21644;&#30142;&#30149;&#20195;&#30721;&#12290;&#20182;&#20204;&#35757;&#32451;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#21253;&#25324;LLaMA&#12289;BERT&#21644;LongFormer&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLaMA&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#26126;&#26174;&#20248;&#20110;BERT&#39118;&#26684;&#30340;&#27169;&#22411;&#65292;&#21363;&#20351;&#31934;&#30830;&#24615;&#22823;&#24133;&#38477;&#20302;&#12290;LLaMA&#27169;&#22411;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#29305;&#21035;&#20986;&#33394;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22788;&#29702;&#22797;&#26434;&#30340;&#22810;&#20219;&#21153;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an approach that combines the language reasoning capabilities of large language models (LLMs) with the benefits of local training to tackle complex, domain-specific tasks. Specifically, the authors demonstrate their approach by extracting structured condition codes from pathology reports. The proposed approach utilizes local LLMs, which can be fine-tuned to respond to specific generative instructions and provide structured outputs. The authors collected a dataset of over 150k uncurated surgical pathology reports, containing gross descriptions, final diagnoses, and condition codes. They trained different model architectures, including LLaMA, BERT and LongFormer and evaluated their performance. The results show that the LLaMA-based models significantly outperform BERT-style models across all evaluated metrics, even with extremely reduced precision. The LLaMA models performed especially well with large datasets, demonstrating their ability to handle complex, multi-la
&lt;/p&gt;</description></item><item><title>"Baby's CoThought" &#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#32452;&#25968;&#25454;&#35757;&#32451;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#35780;&#20272;&#21457;&#29616;&#65292;&#22312;10&#20010;&#35821;&#35328;&#23398;&#12289;NLU&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;BabyLM&#30340;&#34920;&#29616;&#36229;&#36807;RoBERTa-base&#36229;&#36807;3&#20010;&#28857;&#65292;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01684</link><description>&lt;p&gt;
Baby's CoThought: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#32039;&#20945;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models. (arXiv:2308.01684v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01684
&lt;/p&gt;
&lt;p&gt;
"Baby's CoThought" &#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#32452;&#25968;&#25454;&#35757;&#32451;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#35780;&#20272;&#21457;&#29616;&#65292;&#22312;10&#20010;&#35821;&#35328;&#23398;&#12289;NLU&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;BabyLM&#30340;&#34920;&#29616;&#36229;&#36807;RoBERTa-base&#36229;&#36807;3&#20010;&#28857;&#65292;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;(NLU)&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;"CoThought"&#27969;&#27700;&#32447;&#21033;&#29992;LLMs&#30340;CoT&#25552;&#31034;&#65292;&#39640;&#25928;&#22320;&#35757;&#32451;&#36739;&#23567;&#30340;"baby"&#35821;&#35328;&#27169;&#22411;(BabyLMs)&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3.5-turbo&#23545;&#23569;&#20110;100M&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#37325;&#32452;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#38754;&#21521;&#20219;&#21153;&#30340;&#12289;&#21487;&#35835;&#24615;&#24378;&#30340;&#25991;&#26412;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#23398;&#26657;&#25945;&#26448;&#12290;&#28982;&#21518;&#65292;&#22312;&#36825;&#20010;&#37325;&#32452;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#20197;RoBERTa(Liu&#31561;&#20154;&#65292;2019)&#30340;&#26041;&#24335;&#23545;BabyLM&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;4&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;BabyLM&#22312;10&#20010;&#35821;&#35328;&#23398;&#12289;NLU&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;RoBERTa-base&#36229;&#36807;3&#20010;&#28857;&#65292;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23567;&#22411;&#30340;&#12289;&#30001;LLM&#37325;&#32452;&#30340;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#20219;&#21153;&#24182;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate remarkable performance on a variety of Natural Language Understanding (NLU) tasks, primarily due to their in-context learning ability. This ability is utilized in our proposed "CoThought" pipeline, which efficiently trains smaller "baby" language models (BabyLMs) by leveraging the Chain of Thought (CoT) prompting of LLMs. Our pipeline restructures a dataset of less than 100M in size using GPT-3.5-turbo, transforming it into task-oriented, human-readable texts that are comparable to the school texts for language learners. The BabyLM is then pretrained on this restructured dataset in a RoBERTa (Liu et al., 2019) fashion. In evaluations across 4 benchmarks, our BabyLM outperforms the RoBERTa-base in 10 linguistic, NLU, and question answering tasks by more than 3 points, showing superior ability to extract contextual information. These results suggest that compact LMs pretrained on small, LLM-restructured data can better understand tasks and achieve
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NBIAS&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#31561;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#26469;&#35782;&#21035;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01681</link><description>&lt;p&gt;
NBIAS: &#29992;&#20110;&#25991;&#26412;&#20013;&#20559;&#35265;&#35782;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NBIAS: A Natural Language Processing Framework for Bias Identification in Text. (arXiv:2308.01681v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NBIAS&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#31561;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#26469;&#35782;&#21035;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#20013;&#23384;&#22312;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#20351;&#29992;&#26102;&#20135;&#29983;&#20542;&#26012;&#30340;&#35299;&#37322;&#21644;&#32467;&#26524;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#20250;&#25345;&#32493;&#24378;&#21270;&#21051;&#26495;&#21360;&#35937;&#12289;&#27495;&#35270;&#25110;&#20854;&#20182;&#24418;&#24335;&#30340;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;&#22312;&#26377;&#20559;&#35265;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31639;&#27861;&#26368;&#32456;&#20250;&#20570;&#20986;&#19981;&#24179;&#31561;&#24433;&#21709;&#26576;&#20010;&#32676;&#20307;&#30340;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#21644;&#28040;&#38500;&#36825;&#20123;&#20559;&#35265;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#23545;&#25968;&#25454;&#30340;&#20844;&#24179;&#21644;&#36947;&#24503;&#20351;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#32780;&#24378;&#22823;&#30340;&#26694;&#26550;"NBIAS"&#65292;&#23427;&#21253;&#25324;&#25968;&#25454;&#23618;&#12289;&#35821;&#26009;&#24211;&#26500;&#24314;&#12289;&#27169;&#22411;&#24320;&#21457;&#23618;&#21644;&#35780;&#20272;&#23618;&#12290;&#25968;&#25454;&#38598;&#30001;&#20174;&#21508;&#20010;&#39046;&#22495;&#25910;&#38598;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#65292;&#21253;&#25324;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#38376;&#25143;&#32593;&#31449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#20010;&#21807;&#19968;&#30340;&#21629;&#21517;&#23454;&#20307;&#33021;&#22815;&#35782;&#21035;&#20986;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias in textual data can lead to skewed interpretations and outcomes when the data is used. These biases could perpetuate stereotypes, discrimination, or other forms of unfair treatment. An algorithm trained on biased data ends up making decisions that disproportionately impact a certain group of people. Therefore, it is crucial to detect and remove these biases to ensure the fair and ethical use of data. To this end, we develop a comprehensive and robust framework \textsc{Nbias} that consists of a data layer, corpus contruction, model development layer and an evaluation layer. The dataset is constructed by collecting diverse data from various fields, including social media, healthcare, and job hiring portals. As such, we applied a transformer-based token classification model that is able to identify bias words/ phrases through a unique named entity. In the assessment procedure, we incorporate a blend of quantitative and qualitative evaluations to gauge the effectiveness of our models.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#23545;&#20020;&#24202;&#35760;&#24405;&#20013;&#32933;&#32982;&#30417;&#27979;&#30340;&#25991;&#26412;&#25366;&#25496;&#33021;&#21147;&#65292;&#19982;&#20043;&#21069;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;&#30456;&#27604;&#65292;ChatGPT&#30340;&#21484;&#22238;&#29575;&#26356;&#39640;&#65292;&#20294;&#31934;&#30830;&#24230;&#30053;&#20302;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20861;&#21307;&#20020;&#24202;&#21465;&#36848;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#28508;&#21147;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2308.01666</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#23545;&#32933;&#32982;&#30417;&#27979;&#20013;&#20020;&#24202;&#35760;&#24405;&#30340;&#25991;&#26412;&#25366;&#25496;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating ChatGPT text-mining of clinical records for obesity monitoring. (arXiv:2308.01666v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01666
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#23545;&#20020;&#24202;&#35760;&#24405;&#20013;&#32933;&#32982;&#30417;&#27979;&#30340;&#25991;&#26412;&#25366;&#25496;&#33021;&#21147;&#65292;&#19982;&#20043;&#21069;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;&#30456;&#27604;&#65292;ChatGPT&#30340;&#21484;&#22238;&#29575;&#26356;&#39640;&#65292;&#20294;&#31934;&#30830;&#24230;&#30053;&#20302;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20861;&#21307;&#20020;&#24202;&#21465;&#36848;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#28508;&#21147;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20861;&#21307;&#20020;&#24202;&#21465;&#36848;&#20173;&#28982;&#26159;&#19968;&#20010;&#24456;&#23569;&#34987;&#21033;&#29992;&#30340;&#36164;&#28304;&#65292;&#29992;&#20110;&#24212;&#23545;&#22797;&#26434;&#30142;&#30149;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(ChatGPT)&#21644;&#20043;&#21069;&#24320;&#21457;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;(RegexT)&#22312;&#35782;&#21035;&#20861;&#21307;&#21465;&#36848;&#20013;&#36229;&#37325;&#20307;&#20917;&#35780;&#20998;(BCS)&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26041;&#27861;&#65306;&#20351;&#29992;RegexT&#25110;&#23558;&#21465;&#36848;&#38468;&#21152;&#21040;&#21457;&#36865;&#32473;ChatGPT&#30340;&#25552;&#31034;&#20013;&#26469;&#25552;&#21462;4,415&#20010;&#21311;&#21517;&#20020;&#24202;&#21465;&#36848;&#20013;&#30340;BCS&#20540;&#65292;&#36843;&#20351;&#27169;&#22411;&#36820;&#22238;BCS&#20449;&#24687;&#12290;&#36890;&#36807;&#25163;&#21160;&#23457;&#26597;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#65306;RegexT&#30340;&#31934;&#30830;&#24230;(100%&#65292;95% CI 94.81-100%)&#39640;&#20110;ChatGPT&#30340;&#31934;&#30830;&#24230;(89.3%&#65292;95% CI 82.75-93.64%)&#12290;&#28982;&#32780;&#65292;ChatGPT&#30340;&#21484;&#22238;&#29575;(100%&#65292;95% CI 96.18-100%)&#35201;&#36828;&#39640;&#20110;RegexT&#30340;&#21484;&#22238;&#29575;(72.6%&#65292;95% CI 63.92-79.94%)&#12290;&#23616;&#38480;&#24615;&#65306;&#38656;&#35201;&#23545;&#25552;&#31034;&#24037;&#31243;&#36827;&#34892;&#24494;&#35843;&#20197;&#25913;&#21892;ChatGPT&#36755;&#20986;&#12290;&#32467;&#35770;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#21019;&#24314;&#21508;&#31181;&#26426;&#20250;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#65292;&#24182;&#19988;&#34429;&#28982;&#22797;&#26434;&#65292;&#20294;&#20855;&#26377;&#30452;&#35266;&#30340;&#30028;&#38754;&#29992;&#20110;in
&lt;/p&gt;
&lt;p&gt;
Background: Veterinary clinical narratives remain a largely untapped resource for addressing complex diseases. Here we compare the ability of a large language model (ChatGPT) and a previously developed regular expression (RegexT) to identify overweight body condition scores (BCS) in veterinary narratives. Methods: BCS values were extracted from 4,415 anonymised clinical narratives using either RegexT or by appending the narrative to a prompt sent to ChatGPT coercing the model to return the BCS information. Data were manually reviewed for comparison. Results: The precision of RegexT was higher (100%, 95% CI 94.81-100%) than the ChatGPT (89.3%; 95% CI82.75-93.64%). However, the recall of ChatGPT (100%. 95% CI 96.18-100%) was considerably higher than that of RegexT (72.6%, 95% CI 63.92-79.94%). Limitations: Subtle prompt engineering is needed to improve ChatGPT output. Conclusions: Large language models create diverse opportunities and, whilst complex, present an intuitive interface to in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#38382;&#39064;&#25551;&#36848;&#20013;&#25552;&#21462;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#20351;&#32422;&#26463;&#32534;&#31243;&#26356;&#26131;&#20110;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.01589</link><description>&lt;p&gt;
&#22307;&#26479;2.0&#65306;&#20174;&#33258;&#28982;&#35821;&#35328;&#21040;&#32422;&#26463;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Holy Grail 2.0: From Natural Language to Constraint Models. (arXiv:2308.01589v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#38382;&#39064;&#25551;&#36848;&#20013;&#25552;&#21462;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#20351;&#32422;&#26463;&#32534;&#31243;&#26356;&#26131;&#20110;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
27&#24180;&#21069;&#65292;E. Freuder&#24378;&#35843;&#20102;&#8220;&#32422;&#26463;&#32534;&#31243;&#20195;&#34920;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#23545;&#20110;&#32534;&#31243;&#30340;&#19968;&#20010;&#26368;&#25509;&#36817;&#22307;&#26479;&#30340;&#26041;&#27861;&#65306;&#29992;&#25143;&#38472;&#36848;&#38382;&#39064;&#65292;&#35745;&#31639;&#26426;&#35299;&#20915;&#38382;&#39064;&#8221;&#12290;&#22914;&#20170;&#65292;CP&#29992;&#25143;&#25317;&#26377;&#24378;&#22823;&#30340;&#24314;&#27169;&#24037;&#20855;&#65288;&#22914;Minizinc&#21644;CPMpy&#65289;&#65292;&#21487;&#20197;&#29992;&#23427;&#20204;&#26469;&#34920;&#36798;&#38382;&#39064;&#65292;&#28982;&#21518;&#35753;&#27714;&#35299;&#22120;&#23436;&#25104;&#21097;&#19979;&#30340;&#24037;&#20316;&#65292;&#31163;&#30446;&#26631;&#26356;&#36817;&#20102;&#12290;&#28982;&#32780;&#65292;&#36825;&#20173;&#28982;&#35201;&#27714;CP&#29992;&#25143;&#20102;&#35299;&#24418;&#24335;&#21270;&#26041;&#27861;&#24182;&#36981;&#23432;&#23427;&#12290;&#21478;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#25165;&#33021;&#26377;&#25928;&#22320;&#23545;&#32452;&#21512;&#38382;&#39064;&#24314;&#27169;&#12290;&#25152;&#26377;&#36825;&#20123;&#38480;&#21046;&#20102;CP&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#38382;&#39064;&#25551;&#36848;&#20013;&#25552;&#21462;&#27169;&#22411;&#30340;&#21487;&#33021;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20248;&#21270;&#65288;NL4OPT&#65289;&#25361;&#25112;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#35299;&#30340;&#25552;&#31034;&#24212;&#29992;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Twenty-seven years ago, E. Freuder highlighted that "Constraint programming represents one of the closest approaches computer science has yet made to the Holy Grail of programming: the user states the problem, the computer solves it". Nowadays, CP users have great modeling tools available (like Minizinc and CPMpy), allowing them to formulate the problem and then let a solver do the rest of the job, getting closer to the stated goal. However, this still requires the CP user to know the formalism and respect it. Another significant challenge lies in the expertise required to effectively model combinatorial problems. All this limits the wider adoption of CP. In this position paper, we investigate a possible approach to leverage pre-trained Large Language Models to extract models from textual problem descriptions. More specifically, we take inspiration from the Natural Language Processing for Optimization (NL4OPT) challenge and present early results with a decomposition-based prompting app
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;OpenAI&#30340;ChatGPT&#38598;&#25104;&#21040;&#20855;&#36523;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#23558;ChatGPT&#20998;&#37197;&#19981;&#21516;&#35282;&#33394;&#24182;&#19982;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;98%&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#23637;&#29616;&#20102;ChatGPT&#22312;&#29702;&#35299;&#21644;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01552</link><description>&lt;p&gt;
InterAct: &#25506;&#32034;&#23558;ChatGPT&#20316;&#20026;&#21512;&#20316;&#20195;&#29702;&#20154;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent. (arXiv:2308.01552v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;OpenAI&#30340;ChatGPT&#38598;&#25104;&#21040;&#20855;&#36523;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#23558;ChatGPT&#20998;&#37197;&#19981;&#21516;&#35282;&#33394;&#24182;&#19982;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;98%&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#23637;&#29616;&#20102;ChatGPT&#22312;&#29702;&#35299;&#21644;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;OpenAI&#30340;ChatGPT&#38598;&#25104;&#21040;&#20855;&#36523;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#35780;&#20272;&#20854;&#23545;&#20132;&#20114;&#20915;&#31574;&#22522;&#20934;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;InterAct&#36825;&#19968;&#27010;&#24565;&#65292;&#23558;&#20854;&#31867;&#27604;&#20110;&#20154;&#20204;&#26681;&#25454;&#33258;&#24049;&#29420;&#29305;&#30340;&#20248;&#21183;&#25198;&#28436;&#35282;&#33394;&#30340;&#27010;&#24565;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#32473;ChatGPT&#25552;&#20379;&#21508;&#31181;&#25552;&#31034;&#65292;&#23558;&#20854;&#20998;&#37197;&#20026;&#20687;&#26816;&#26597;&#21592;&#21644;&#20998;&#31867;&#21592;&#36825;&#26679;&#30340;&#22810;&#31181;&#35282;&#33394;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#19982;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;AlfWorld&#20013;&#23637;&#31034;&#20102;98%&#30340;&#26174;&#33879;&#25104;&#21151;&#29575;&#65292;AlfWorld&#26159;&#19968;&#20010;&#27169;&#25311;&#23478;&#24237;&#29615;&#22659;&#20013;&#21253;&#21547;6&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;&#33391;&#22909;&#30340;&#25552;&#31034;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#32467;&#26524;&#24378;&#35843;&#20102;ChatGPT&#22312;&#29702;&#35299;&#21644;&#39640;&#25928;&#22320;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;&#20219;&#21153;&#35268;&#21010;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper delves into the integration of OpenAI's ChatGPT into embodied agent systems, evaluating its influence on interactive decision-making benchmark. Drawing a parallel to the concept of people assuming roles according to their unique strengths, we introduce InterAct. In this approach, we feed ChatGPT with varied prompts, assigning it a numerous roles like a checker and a sorter, then integrating them with the original language model. Our research shows a remarkable success rate of 98% in AlfWorld, which consists of 6 different tasks in a simulated household environment, emphasizing the significance of proficient prompt engineering. The results highlight ChatGPT's competence in comprehending and performing intricate tasks effectively in real-world settings, thus paving the way for further advancements in task planning.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21464;&#21387;&#22120;&#20013;&#22914;&#20309;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#20449;&#24687;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#22312;&#21464;&#21387;&#22120;&#30340;&#26356;&#28145;&#22788;&#36827;&#34892;&#27169;&#24577;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#35782;&#21035;&#22810;&#27169;&#24577;&#31070;&#32463;&#20803;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#23545;&#29305;&#23450;&#35270;&#35273;&#27010;&#24565;&#30340;&#25805;&#20316;&#20197;&#21450;&#23545;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2308.01544</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#20165;&#25991;&#26412;&#21464;&#21387;&#22120;&#20013;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
Multimodal Neurons in Pretrained Text-Only Transformers. (arXiv:2308.01544v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01544
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21464;&#21387;&#22120;&#20013;&#22914;&#20309;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#20449;&#24687;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#22312;&#21464;&#21387;&#22120;&#30340;&#26356;&#28145;&#22788;&#36827;&#34892;&#27169;&#24577;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#35782;&#21035;&#22810;&#27169;&#24577;&#31070;&#32463;&#20803;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#23545;&#29305;&#23450;&#35270;&#35273;&#27010;&#24565;&#30340;&#25805;&#20316;&#20197;&#21450;&#23545;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#22312;&#19981;&#21516;&#27169;&#24577;&#19979;&#23558;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20923;&#32467;&#30340;&#25991;&#26412;&#21464;&#21387;&#22120;&#22686;&#21152;&#35270;&#35273;&#33021;&#21147;&#30340;&#24773;&#20917;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#20219;&#21153;&#19978;&#23398;&#20064;&#24471;&#21040;&#30340;&#21333;&#19968;&#32447;&#24615;&#26144;&#23556;&#12290;&#26144;&#23556;&#23618;&#30340;&#36755;&#20986;&#19981;&#26159;&#21487;&#20197;&#30452;&#25509;&#35299;&#30721;&#25104;&#25551;&#36848;&#22270;&#20687;&#20869;&#23481;&#30340;&#35821;&#35328;&#65292;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#24577;&#20043;&#38388;&#30340;&#36716;&#25442;&#21457;&#29983;&#22312;&#21464;&#21387;&#22120;&#30340;&#26356;&#28145;&#22788;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35782;&#21035;&#23558;&#35270;&#35273;&#34920;&#31034;&#36716;&#25442;&#20026;&#30456;&#24212;&#25991;&#26412;&#30340;&#8220;&#22810;&#27169;&#24577;&#31070;&#32463;&#20803;&#8221;&#30340;&#36807;&#31243;&#65292;&#24182;&#35299;&#30721;&#23427;&#20204;&#27880;&#20837;&#27169;&#22411;&#27531;&#24046;&#27969;&#20013;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;&#31070;&#32463;&#20803;&#22312;&#19981;&#21516;&#36755;&#20837;&#20013;&#23545;&#29305;&#23450;&#35270;&#35273;&#27010;&#24565;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#23545;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20855;&#26377;&#31995;&#32479;&#24615;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models demonstrate remarkable capacity to generalize representations learned in one modality to downstream tasks in other modalities. Can we trace this ability to individual neurons? We study the case where a frozen text transformer is augmented with vision using a self-supervised visual encoder and a single linear projection learned on an image-to-text task. Outputs of the projection layer are not immediately decodable into language describing image content; instead, we find that translation between modalities occurs deeper within the transformer. We introduce a procedure for identifying "multimodal neurons" that convert visual representations into corresponding text, and decoding the concepts they inject into the model's residual stream. In a series of experiments, we show that multimodal neurons operate on specific visual concepts across inputs, and have a systematic causal effect on image captioning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#19977;&#31181;&#22823;&#35268;&#27169;&#35282;&#24230;&#29983;&#25104;&#31574;&#30053;&#65306;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#12289;&#20247;&#21253;&#31995;&#32479;&#21644;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#19977;&#31181;&#26041;&#27861;&#30340;&#32452;&#21512;&#20248;&#20110;&#21333;&#19968;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#24773;&#22659;&#20013;&#26377;&#30528;&#19981;&#21516;&#30340;&#20248;&#21183;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#20063;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#26368;&#21518;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#22312;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22312;&#32447;&#25991;&#23383;&#22788;&#29702;&#22120;&#20013;&#24212;&#29992;&#35282;&#24230;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.01535</link><description>&lt;p&gt;
&#27604;&#36739;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#25968;&#20540;&#35282;&#24230;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Comparing scalable strategies for generating numerical perspectives. (arXiv:2308.01535v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#19977;&#31181;&#22823;&#35268;&#27169;&#35282;&#24230;&#29983;&#25104;&#31574;&#30053;&#65306;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#12289;&#20247;&#21253;&#31995;&#32479;&#21644;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#19977;&#31181;&#26041;&#27861;&#30340;&#32452;&#21512;&#20248;&#20110;&#21333;&#19968;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#24773;&#22659;&#20013;&#26377;&#30528;&#19981;&#21516;&#30340;&#20248;&#21183;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#20063;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#26368;&#21518;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#22312;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22312;&#32447;&#25991;&#23383;&#22788;&#29702;&#22120;&#20013;&#24212;&#29992;&#35282;&#24230;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#35282;&#24230;&#24110;&#21161;&#20154;&#20204;&#29702;&#35299;&#26497;&#31471;&#21644;&#19981;&#29087;&#24713;&#30340;&#25968;&#20540;&#65288;&#20363;&#22914;&#65292;3300&#20159;&#32654;&#20803;&#30456;&#24403;&#20110;&#27599;&#20010;&#32654;&#22269;&#20154;1000&#32654;&#20803;&#65289;&#12290;&#23613;&#31649;&#30740;&#31350;&#34920;&#26126;&#35282;&#24230;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#20294;&#35201;&#22312;&#22823;&#35268;&#27169;&#19978;&#29983;&#25104;&#23427;&#20204;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#24456;&#38590;&#30830;&#23450;&#20160;&#20040;&#20351;&#26576;&#20123;&#31867;&#27604;&#26356;&#26377;&#24110;&#21161;&#65292;&#32780;&#19988;&#22240;&#20026;&#26368;&#26377;&#24110;&#21161;&#30340;&#22240;&#32032;&#21487;&#33021;&#22240;&#19978;&#19979;&#25991;&#32780;&#24322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#19977;&#31181;&#22823;&#35268;&#27169;&#35282;&#24230;&#29983;&#25104;&#31574;&#30053;&#65306;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#12289;&#20247;&#21253;&#31995;&#32479;&#21644;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#65288;&#36890;&#36807;BERT&#23884;&#20837;&#65289;&#29983;&#25104;&#29305;&#23450;&#19978;&#19979;&#25991;&#35282;&#24230;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#19977;&#31181;&#26041;&#27861;&#30340;&#32452;&#21512;&#20248;&#20110;&#21333;&#19968;&#26041;&#27861;&#65292;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29992;&#25143;&#23545;&#19981;&#21516;&#26041;&#27861;&#26377;&#24322;&#36136;&#20559;&#22909;&#12290;&#25105;&#20204;&#26368;&#21518;&#35752;&#35770;&#20102;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22312;&#32447;&#25991;&#23383;&#22788;&#29702;&#22120;&#20013;&#24212;&#29992;&#35282;&#24230;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerical perspectives help people understand extreme and unfamiliar numbers (e.g., \$330 billion is about \$1,000 per person in the United States). While research shows perspectives to be helpful, generating them at scale is challenging both because it is difficult to identify what makes some analogies more helpful than others, and because what is most helpful can vary based on the context in which a given number appears. Here we present and compare three policies for large-scale perspective generation: a rule-based approach, a crowdsourced system, and a model that uses Wikipedia data and semantic similarity (via BERT embeddings) to generate context-specific perspectives. We find that the combination of these three approaches dominates any single method, with different approaches excelling in different settings and users displaying heterogeneous preferences across approaches. We conclude by discussing our deployment of perspectives in a widely-used online word processor.
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#65292;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#26469;&#33258;&#22622;&#23572;&#32500;&#20122;&#35799;&#27468;&#30340;&#26032;&#39062;&#25991;&#23398;&#38544;&#21947;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01497</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#23545;&#26032;&#39062;&#25991;&#23398;&#38544;&#21947;&#30340;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Displays Emergent Ability to Interpret Novel Literary Metaphors. (arXiv:2308.01497v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01497
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#65292;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#26469;&#33258;&#22622;&#23572;&#32500;&#20122;&#35799;&#27468;&#30340;&#26032;&#39062;&#25991;&#23398;&#38544;&#21947;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#24341;&#21457;&#20102;&#20851;&#20110;&#36825;&#31181;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;&#21542;&#33021;&#22815;&#22312;&#36275;&#22815;&#30340;&#35757;&#32451;&#19979;&#23637;&#29616;&#20986;&#39640;&#27700;&#24179;&#20154;&#31867;&#33021;&#21147;&#30340;&#20105;&#35770;&#12290;&#23613;&#31649;LLMs&#22312;&#28041;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#29702;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#33021;&#21147;&#26159;&#21542;&#24310;&#20280;&#21040;&#26356;&#20855;&#21019;&#36896;&#21147;&#30340;&#20154;&#31867;&#33021;&#21147;&#23384;&#22312;&#20005;&#37325;&#20998;&#27495;&#12290;&#20854;&#20013;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#35299;&#37322;&#26032;&#39062;&#38544;&#21947;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#29992;&#20110;&#35757;&#32451;LLMs&#30340;&#24222;&#22823;&#19988;&#38750;&#31574;&#21010;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#35774;&#35745;&#27979;&#35797;&#30340;&#19968;&#20010;&#20005;&#37325;&#38556;&#30861;&#23601;&#26159;&#38656;&#35201;&#25214;&#21040;&#26032;&#39062;&#20294;&#39640;&#36136;&#37327;&#30340;&#38544;&#21947;&#65292;&#36825;&#20123;&#38544;&#21947;&#19981;&#22826;&#21487;&#33021;&#20986;&#29616;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-4&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#26469;&#33258;&#22622;&#23572;&#32500;&#20122;&#35799;&#27468;&#24182;&#32763;&#35793;&#20026;&#33521;&#35821;&#30340;&#26032;&#39062;&#25991;&#23398;&#38544;&#21947;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in the performance of large language models (LLMs) have sparked debate over whether, given sufficient training, high-level human abilities emerge in such generic forms of artificial intelligence (AI). Despite the exceptional performance of LLMs on a wide range of tasks involving natural language processing and reasoning, there has been sharp disagreement as to whether their abilities extend to more creative human abilities. A core example is the ability to interpret novel metaphors. Given the enormous and non-curated text corpora used to train LLMs, a serious obstacle to designing tests is the requirement of finding novel yet high-quality metaphors that are unlikely to have been included in the training data. Here we assessed the ability of GPT-4, a state-of-the-art large language model, to provide natural-language interpretations of novel literary metaphors drawn from Serbian poetry and translated into English. Despite exhibiting no signs of having been exposed to thes
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#27169;&#25311;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#22312;&#20219;&#21153;&#20027;&#21160;&#35774;&#32622;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#22788;&#29702;&#31995;&#32479;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#24341;&#29992;&#36890;&#20449;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#20855;&#26377;&#28508;&#21147;&#25104;&#20026;&#26377;&#25928;&#30340;&#31574;&#30053;&#36873;&#25321;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.01479</link><description>&lt;p&gt;
&#22312;&#20219;&#21153;&#20027;&#21160;&#35774;&#32622;&#20013;&#30740;&#31350;&#24378;&#21270;&#23398;&#20064;&#22312;&#36890;&#20449;&#31574;&#30053;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Investigating Reinforcement Learning for Communication Strategies in a Task-Initiative Setting. (arXiv:2308.01479v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01479
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#27169;&#25311;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#22312;&#20219;&#21153;&#20027;&#21160;&#35774;&#32622;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#22788;&#29702;&#31995;&#32479;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#24341;&#29992;&#36890;&#20449;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#20855;&#26377;&#28508;&#21147;&#25104;&#20026;&#26377;&#25928;&#30340;&#31574;&#30053;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23545;&#35805;&#22495;&#38656;&#35201;&#31995;&#32479;&#21521;&#29992;&#25143;&#21576;&#29616;&#32454;&#24494;&#24046;&#21035;&#30340;&#20449;&#24687;&#12290;&#36825;&#26679;&#30340;&#31995;&#32479;&#24517;&#39035;&#22312;&#20182;&#20204;&#35828;&#35805;&#21518;&#36827;&#34892;&#36861;&#38382;&#20197;&#35299;&#31572;&#30097;&#38382;&#21644;&#32416;&#27491;&#35823;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#39033;&#24341;&#29992;&#36890;&#20449;&#20219;&#21153;&#20013;&#25506;&#32034;&#20102;&#36825;&#31181;&#20114;&#21160;&#31574;&#30053;&#12290;&#36890;&#36807;&#27169;&#25311;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21021;&#22987;&#21576;&#29616;&#21644;&#38543;&#21518;&#36861;&#38382;&#20043;&#38388;&#30340;&#36890;&#20449;&#26435;&#34913;&#65292;&#36825;&#26159;&#29992;&#25143;&#28548;&#28165;&#31574;&#30053;&#30340;&#19968;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#23558;&#20960;&#31181;&#22522;&#32447;&#31574;&#30053;&#19982;&#24378;&#21270;&#23398;&#20064;&#24471;&#26469;&#30340;&#31574;&#30053;&#24615;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#34920;&#31034;&#20855;&#26377;&#20196;&#20154;&#24778;&#35766;&#30340;&#20248;&#21183;&#65292;&#35813;&#26041;&#27861;&#35201;&#27714;&#30340;&#25968;&#25454;&#38656;&#27714;&#24456;&#23567;&#65292;&#36873;&#25321;&#35299;&#37322;&#24615;&#24378;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#23457;&#35745;&#33021;&#21147;&#65292;&#20294;&#22312;&#24191;&#27867;&#30340;&#29992;&#25143;&#27169;&#22411;&#33539;&#22260;&#20869;&#39044;&#27979;&#32467;&#26524;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many conversational domains require the system to present nuanced information to users. Such systems must follow up what they say to address clarification questions and repair misunderstandings. In this work, we explore this interactive strategy in a referential communication task. Using simulation, we analyze the communication trade-offs between initial presentation and subsequent followup as a function of user clarification strategy, and compare the performance of several baseline strategies to policies derived by reinforcement learning. We find surprising advantages to coherence-based representations of dialogue strategy, which bring minimal data requirements, explainable choices, and strong audit capabilities, but incur little loss in predicted outcomes across a wide range of user models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#32467;&#21512;&#20102;&#22810;&#31181;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#24182;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01472</link><description>&lt;p&gt;
&#21453;&#21521;&#31283;&#23450;&#25193;&#25955;&#65306;&#29983;&#25104;&#35813;&#22270;&#20687;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Reverse Stable Diffusion: What prompt was used to generate this image?. (arXiv:2308.01472v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#32467;&#21512;&#20102;&#22810;&#31181;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#24182;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65292;&#26368;&#36817;&#21560;&#24341;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20852;&#36259;&#65292;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#29983;&#25104;&#36807;&#31243;&#21644;&#22914;&#20309;&#35774;&#35745;&#25552;&#31034;&#20197;&#33719;&#24471;&#25152;&#38656;&#22270;&#20687;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#31995;&#21015;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65288;&#26377;&#21644;&#26080;&#23545;&#25193;&#25955;&#32593;&#32476;&#26435;&#37325;&#36827;&#34892;&#35775;&#38382;&#65289;&#26469;&#22788;&#29702;&#25152;&#25552;&#20986;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#32852;&#21512;&#25552;&#31034;&#22238;&#24402;&#21644;&#22810;&#26631;&#31614;&#35789;&#27719;&#20998;&#31867;&#30446;&#26631;&#65292;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#35838;&#31243;&#23398;&#20064;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#20855;&#26377;&#26356;&#20302;&#26631;&#27880;&#22122;&#22768;&#65288;&#21363;&#26356;&#22909;&#23545;&#40784;&#65289;&#30340;&#22270;&#20687;&#25552;&#31034;&#23545;&#30340;&#23398;&#20064;&#65292;&#24182;&#19988;&#20351;&#29992;&#30456;&#20284;&#24615;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models such as Stable Diffusion have recently attracted the interest of many researchers, and inverting the diffusion process can play an important role in better understanding the generative process and how to engineer prompts in order to obtain the desired images. To this end, we introduce the new task of predicting the text prompt given an image generated by a generative diffusion model. We combine a series of white-box and black-box models (with and without access to the weights of the diffusion network) to deal with the proposed task. We propose a novel learning framework comprising of a joint prompt regression and multi-label vocabulary classification objective that generates improved prompts. To further improve our method, we employ a curriculum learning procedure that promotes the learning of image-prompt pairs with lower labeling noise (i.e. that are better aligned), and an unsupervised domain-adaptive kernel learning method that uses the similarities b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FinVis-GPT&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#37329;&#34701;&#22270;&#34920;&#20998;&#26512;&#12290;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#22810;&#27169;&#24577;&#29305;&#24615;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#35299;&#37322;&#37329;&#34701;&#22270;&#34920;&#24182;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20998;&#26512;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;FinVis-GPT&#22312;&#29983;&#25104;&#25551;&#36848;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#39044;&#27979;&#26410;&#26469;&#24066;&#22330;&#36235;&#21183;&#31561;&#37329;&#34701;&#22270;&#34920;&#30456;&#20851;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#21033;&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01430</link><description>&lt;p&gt;
FinVis-GPT:&#19968;&#31181;&#29992;&#20110;&#37329;&#34701;&#22270;&#34920;&#20998;&#26512;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis. (arXiv:2308.01430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FinVis-GPT&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#37329;&#34701;&#22270;&#34920;&#20998;&#26512;&#12290;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#22810;&#27169;&#24577;&#29305;&#24615;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#35299;&#37322;&#37329;&#34701;&#22270;&#34920;&#24182;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20998;&#26512;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;FinVis-GPT&#22312;&#29983;&#25104;&#25551;&#36848;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#39044;&#27979;&#26410;&#26469;&#24066;&#22330;&#36235;&#21183;&#31561;&#37329;&#34701;&#22270;&#34920;&#30456;&#20851;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#21033;&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FinVis-GPT&#65292;&#19968;&#31181;&#19987;&#20026;&#37329;&#34701;&#22270;&#34920;&#20998;&#26512;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#65292;&#32467;&#21512;&#25351;&#20196;&#35843;&#25972;&#21644;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;FinVis-GPT&#33021;&#22815;&#35299;&#37322;&#37329;&#34701;&#22270;&#34920;&#24182;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20998;&#26512;&#12290;&#20026;&#20102;&#35757;&#32451;FinVis-GPT&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#37329;&#34701;&#20219;&#21153;&#23548;&#21521;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#23545;&#40784;&#21644;&#25351;&#20196;&#35843;&#25972;&#65292;&#21253;&#25324;&#21508;&#31181;&#31867;&#22411;&#30340;&#37329;&#34701;&#22270;&#34920;&#21450;&#20854;&#30456;&#24212;&#30340;&#25551;&#36848;&#12290;&#30001;&#20110;&#26102;&#38388;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;FinVis-GPT&#22312;&#29983;&#25104;&#25551;&#36848;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#39044;&#27979;&#26410;&#26469;&#24066;&#22330;&#36235;&#21183;&#31561;&#21508;&#31181;&#19982;&#37329;&#34701;&#22270;&#34920;&#30456;&#20851;&#30340;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;LLMs&#12290;&#25152;&#25552;&#20986;&#30340;FinVis-GPT&#22312;&#37329;&#34701;&#39046;&#22495;&#21033;&#29992;&#22810;&#27169;&#24577;LLMs&#26041;&#38754;&#20855;&#26377;&#24320;&#21019;&#24615;&#30340;&#36129;&#29486;&#65292;&#25105;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#23558;&#34987;&#29992;&#20316;&#20170;&#21518;&#30740;&#31350;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose FinVis-GPT, a novel multimodal large language model (LLM) specifically designed for financial chart analysis. By leveraging the power of LLMs and incorporating instruction tuning and multimodal capabilities, FinVis-GPT is capable of interpreting financial charts and providing valuable analysis. To train FinVis-GPT, a financial task oriented dataset was generated for pre-training alignment and instruction tuning, comprising various types of financial charts and their corresponding descriptions. We evaluate the model performance via several case studies due to the time limit, and the promising results demonstrated that FinVis-GPT is superior in various financial chart related tasks, including generating descriptions, answering questions and predicting future market trends, surpassing existing state-of-the-art multimodal LLMs. The proposed FinVis-GPT serves as a pioneering effort in utilizing multimodal LLMs in the finance domain and our generated dataset will be
&lt;/p&gt;</description></item><item><title>ChatMOF&#26159;&#19968;&#31181;&#33258;&#20027;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32452;&#21512;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#30740;&#31350;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01423</link><description>&lt;p&gt;
ChatMOF: &#19968;&#31181;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;
&lt;/p&gt;
&lt;p&gt;
ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. (arXiv:2308.01423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01423
&lt;/p&gt;
&lt;p&gt;
ChatMOF&#26159;&#19968;&#31181;&#33258;&#20027;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32452;&#21512;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#30740;&#31350;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatMOF&#26159;&#19968;&#20010;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#65288;MOFs&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;gpt-3.5-turbo&#65289;&#65292;ChatMOF&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#21018;&#24615;&#32467;&#26500;&#21270;&#26597;&#35810;&#30340;&#38656;&#27714;&#12290;&#35813;&#31995;&#32479;&#30001;&#19977;&#20010;&#26680;&#24515;&#32452;&#20214;&#65288;&#21363;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#65289;&#32452;&#25104;&#65292;&#24418;&#25104;&#19968;&#20010;&#24378;&#22823;&#30340;&#27969;&#27700;&#32447;&#65292;&#31649;&#29702;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#12290;&#35813;&#30740;&#31350;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#26410;&#26469;&#21457;&#23637;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate of metal-organic frameworks (MOFs). By leveraging a large-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generation. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.
&lt;/p&gt;</description></item><item><title>SAP-sLDA&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#20154;&#31867;&#21442;&#19982;&#30340;LDA&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20302;&#32500;&#25237;&#24433;&#20013;&#23398;&#20064;&#20027;&#39064;&#24182;&#20445;&#30041;&#25991;&#26723;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.01420</link><description>&lt;p&gt;
SAP-sLDA: &#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#30028;&#38754;&#29992;&#20110;&#25506;&#32034;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text. (arXiv:2308.01420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01420
&lt;/p&gt;
&lt;p&gt;
SAP-sLDA&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#20154;&#31867;&#21442;&#19982;&#30340;LDA&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20302;&#32500;&#25237;&#24433;&#20013;&#23398;&#20064;&#20027;&#39064;&#24182;&#20445;&#30041;&#25991;&#26723;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#25506;&#32034;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#36890;&#36807;&#25991;&#26723;&#30340;&#20302;&#32500;&#25237;&#24433;&#65292;&#24076;&#26395;&#20027;&#39064;&#30456;&#20284;&#30340;&#25991;&#26723;&#33021;&#22815;&#22312;&#25237;&#24433;&#31354;&#38388;&#20013;&#32858;&#31867;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#29992;&#20110;&#38477;&#32500;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#31639;&#27861;&#65292;&#22914;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#65292;&#24448;&#24448;&#20250;&#20135;&#29983;&#26080;&#27861;&#25429;&#25417;&#25991;&#26723;&#30456;&#20284;&#24615;&#30340;&#20154;&#31867;&#27010;&#24565;&#30340;&#25237;&#24433;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#20154;&#31867;&#21442;&#19982;&#30340;&#22522;&#20110;LDA&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20027;&#39064;&#65292;&#22312;&#20302;&#32500;&#25237;&#24433;&#20013;&#20445;&#30041;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#25991;&#26723;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#21512;&#25104;&#35821;&#26009;&#24211;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20165;&#25552;&#20379;&#23569;&#37327;&#26631;&#31614;&#30340;&#22522;&#32447;&#26041;&#27861;&#20135;&#29983;&#26356;&#26131;&#35299;&#37322;&#30340;&#25237;&#24433;&#12290;&#22312;&#23454;&#38469;&#35821;&#26009;&#24211;&#19978;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common way to explore text corpora is through low-dimensional projections of the documents, where one hopes that thematically similar documents will be clustered together in the projected space. However, popular algorithms for dimensionality reduction of text corpora, like Latent Dirichlet Allocation (LDA), often produce projections that do not capture human notions of document similarity. We propose a semi-supervised human-in-the-loop LDA-based method for learning topics that preserve semantically meaningful relationships between documents in low-dimensional projections. On synthetic corpora, our method yields more interpretable projections than baseline methods with only a fraction of labels provided. On a real corpus, we obtain qualitatively similar results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#21019;&#24314;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#19982;&#37329;&#34701;&#19987;&#23478;&#30340;&#23545;&#35805;&#21644;&#21453;&#39304;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29983;&#25104;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#20934;&#30830;&#12289;&#30456;&#20851;&#21644;&#37329;&#34701;&#39118;&#26684;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.01415</link><description>&lt;p&gt;
&#19968;&#20010;&#26377;&#25928;&#30340;&#25968;&#25454;&#21019;&#24314;&#27969;&#27700;&#32447;&#29992;&#20110;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25351;&#20196;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model. (arXiv:2308.01415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#21019;&#24314;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#19982;&#37329;&#34701;&#19987;&#23478;&#30340;&#23545;&#35805;&#21644;&#21453;&#39304;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29983;&#25104;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#20934;&#30830;&#12289;&#30456;&#20851;&#21644;&#37329;&#34701;&#39118;&#26684;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21021;&#26399;&#38454;&#27573;&#65292;&#20026;&#37329;&#34701;&#30456;&#20851;&#20219;&#21153;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#38750;&#24120;&#20851;&#38190;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#21019;&#24314;&#27969;&#27700;&#32447;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#24341;&#21457;&#20102;&#19968;&#20010;AI&#25237;&#36164;&#32773;&#21644;&#37329;&#34701;&#19987;&#23478;&#20043;&#38388;&#30340;&#23545;&#35805;&#65292;&#24182;&#34701;&#20837;&#20102;&#20154;&#24037;&#37329;&#34701;&#19987;&#23478;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#25968;&#25454;&#38598;&#12290;&#35813;&#27969;&#27700;&#32447;&#20135;&#29983;&#20102;&#19968;&#20010;&#30001;103k&#20010;&#22810;&#36718;&#23545;&#35805;&#32452;&#25104;&#30340;&#31283;&#23450;&#30340;&#25351;&#20196;&#31934;&#35843;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#37319;&#29992;&#22806;&#37096;&#30340;GPT-4&#20316;&#20026;&#35780;&#21028;&#32773;&#65292;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26377;&#24076;&#26395;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#20934;&#30830;&#12289;&#30456;&#20851;&#21644;&#37329;&#34701;&#39118;&#26684;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20174;&#32780;&#20026;&#37329;&#34701;&#39046;&#22495;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the beginning era of large language model, it is quite critical to generate a high-quality financial dataset to fine-tune a large language model for financial related tasks. Thus, this paper presents a carefully designed data creation pipeline for this purpose. Particularly, we initiate a dialogue between an AI investor and financial expert using ChatGPT and incorporate the feedback of human financial experts, leading to the refinement of the dataset. This pipeline yielded a robust instruction tuning dataset comprised of 103k multi-turn chats. Extensive experiments have been conducted on this dataset to evaluate the model's performance by adopting an external GPT-4 as the judge. The promising experimental results verify that our approach led to significant advancements in generating accurate, relevant, and financial-style responses from AI models, and thus providing a powerful tool for applications within the financial sector.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#39046;&#22495;&#35774;&#35745;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;HouYi&#21450;&#20854;&#23545;&#24212;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#23398;&#26415;&#35770;&#25991;&#25968;&#25454;&#38598;REAP&#12290;HouYi&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#21487;&#20877;&#29983;&#33021;&#28304;&#39046;&#22495;&#23398;&#26415;&#35770;&#25991;&#27573;&#33853;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#19982;ChatGPT&#30456;&#24403;&#65292;&#30053;&#20248;&#20110;Clau&#12290;</title><link>http://arxiv.org/abs/2308.01414</link><description>&lt;p&gt;
HouYi: &#19968;&#31181;&#19987;&#38376;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#30899;&#20013;&#21644;&#39046;&#22495;&#35774;&#35745;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HouYi: An open-source large language model specially designed for renewable energy and carbon neutrality field. (arXiv:2308.01414v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#39046;&#22495;&#35774;&#35745;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;HouYi&#21450;&#20854;&#23545;&#24212;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#23398;&#26415;&#35770;&#25991;&#25968;&#25454;&#38598;REAP&#12290;HouYi&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#21487;&#20877;&#29983;&#33021;&#28304;&#39046;&#22495;&#23398;&#26415;&#35770;&#25991;&#27573;&#33853;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#19982;ChatGPT&#30456;&#24403;&#65292;&#30053;&#20248;&#20110;Clau&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20877;&#29983;&#33021;&#28304;&#23545;&#20110;&#23454;&#29616;&#30899;&#20013;&#21644;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#24212;&#29992;&#65292;&#22914;ChatGPT&#22312;&#33258;&#21160;&#20869;&#23481;&#29983;&#25104;&#26041;&#38754;&#30340;&#25104;&#21151;&#65292;&#20351;&#24471;LLMs&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#39046;&#22495;&#25198;&#28436;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19987;&#38376;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#39046;&#22495;&#35774;&#35745;&#30340;LLM&#65292;&#20063;&#27809;&#26377;&#20219;&#20309;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;LLM&#12290;&#22240;&#27492;&#65292;&#26412;&#35770;&#25991;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;&#38750;&#21830;&#19994;&#24615;&#21487;&#20877;&#29983;&#33021;&#28304;LLM&#30740;&#31350;&#30340;&#24320;&#28304;&#21487;&#20877;&#29983;&#33021;&#28304;&#23398;&#26415;&#35770;&#25991;&#65288;REAP&#65289;&#25968;&#25454;&#38598;&#12290;REAP&#25968;&#25454;&#38598;&#36890;&#36807;&#20174;Web of Science&#25628;&#32034;1168970&#31687;&#23398;&#26415;&#25991;&#29486;&#30340;&#26631;&#39064;&#21644;&#25688;&#35201;&#36827;&#34892;&#25910;&#38598;&#12290;&#22522;&#20110;REAP&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#36890;&#29992;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;LLM&#27169;&#22411;HouYi&#12290;HouYi&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#39046;&#22495;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#23398;&#26415;&#35770;&#25991;&#27573;&#33853;&#29983;&#25104;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23427;&#22312;&#29983;&#25104;&#21487;&#20877;&#29983;&#33021;&#28304;&#23398;&#26415;&#35770;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#19982;ChatGPT&#30456;&#24403;&#65292;&#22312;&#26576;&#20123;&#26041;&#38754;&#30053;&#20248;&#20110;Clau&#12290;
&lt;/p&gt;
&lt;p&gt;
Renewable energy is important for achieving carbon neutrality goal. With the great success of Large Language Models (LLMs) like ChatGPT in automatic content generation, LLMs are playing an increasingly important role. However, there has not been a specially designed LLM for renewable energy. Meanwhile, there has not been any dataset of renewable energy for training LLMs. Therefore, this paper published the first open-source Renewable Energy Academic Paper (REAP) dataset for non-commercial LLM research of renewable energy. REAP dataset is collected through searching the title and abstract of 1,168,970 academic literatures from Web of Science. Based on REAP dataset, HouYi model, the first LLM for renewable energy, is developed through finetuning general LLMs. HouYi demonstrated powerful academic paper paragraph generation ability in renewable energy field. Experiments show that its ability to generate academic papers on renewable energy is comparable to ChatGPT, slightly outperforms Clau
&lt;/p&gt;</description></item><item><title>LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.01413</link><description>&lt;p&gt;
LaFiCMIL&#65306;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22823;&#25991;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning. (arXiv:2308.01413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01413
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#31361;&#30772;&#12290;&#30452;&#35266;&#19978;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#26399;&#26395;&#25991;&#26412;&#20998;&#31867;&#65292;&#20316;&#20026;&#19981;&#38656;&#35201;&#20687;&#29983;&#25104;&#20219;&#21153;&#37027;&#26679;&#35768;&#22810;&#39640;&#32423;&#34920;&#31034;&#30340;&#20219;&#21153;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;Transformer&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#36827;&#34892;&#32508;&#21512;&#24615;&#30340;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#65292;&#22312;&#22810;&#31867;&#21035;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#38271;&#25991;&#26412;&#25991;&#26723;&#21644;&#20854;&#20182;&#22823;&#25991;&#20214;&#30340;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#36739;&#22823;&#30340;&#25913;&#36827;&#28508;&#21147;&#12290;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#20027;&#35201;&#21463;&#21040;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#30340;&#38459;&#30861;&#65306;&#26377;&#38480;&#30340;&#36755;&#20837;&#38271;&#24230;&#65292;&#27604;&#22914;BERT&#30340;512&#20010;&#26631;&#35760;&#12290;&#34429;&#28982;&#22686;&#21152;GPU&#20869;&#23384;&#21487;&#20197;&#31245;&#24494;&#25193;&#23637;&#36825;&#20010;&#38480;&#21046;&#65292;&#20294;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;GPU&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#36755;&#20837;&#38480;&#21046;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;LaFiCMIL&#65292;&#20316;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have revolutionized the performance of a wide range of language tasks. Intuitively, one might expect text classification, which does not necessitate as many high-level representations as generative tasks, to be comprehensively addressed with the powerful representation capabilities of Transformers. However, in reality, there remains significant potential for enhancement, particularly in the areas of multi-class and multi-label classification of lengthy textual documents and other large files. The performance of Transformer-based models is mainly hindered by a major limitation: a restricted input length, e.g., 512 tokens for BERT. While an increase in GPU memory can marginally extend this limit, practical real-world applications often operate under constrained GPU resources. In this work, we tackle the input limit problem from the perspective of correlated multiple instance learning. The proposed approach, LaFiCMIL, serves as a versatile framework applicable to 
&lt;/p&gt;</description></item><item><title>UPB&#22242;&#38431;&#22312;IberLEF-2023&#30340;AuTexTification&#20849;&#20139;&#20219;&#21153;&#20013;&#20351;&#29992;Transformer&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#23439;F1&#20998;&#25968;&#12290; (translated_abstract)</title><link>http://arxiv.org/abs/2308.01408</link><description>&lt;p&gt;
UPB&#22312;IberLEF-2023 AuTexTification&#20013;&#20351;&#29992;Transformer&#38598;&#25104;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
UPB at IberLEF-2023 AuTexTification: Detection of Machine-Generated Text using Transformer Ensembles. (arXiv:2308.01408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01408
&lt;/p&gt;
&lt;p&gt;
UPB&#22242;&#38431;&#22312;IberLEF-2023&#30340;AuTexTification&#20849;&#20139;&#20219;&#21153;&#20013;&#20351;&#29992;Transformer&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#23439;F1&#20998;&#25968;&#12290; (translated_abstract)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;UPB&#22242;&#38431;&#22312;IberLEF-2023&#30340;AuTexTification&#20849;&#20139;&#20219;&#21153;&#20013;&#25552;&#20132;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21442;&#19982;&#20102;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#21363;&#35782;&#21035;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#19981;&#26159;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25991;&#26723;&#12290;&#32452;&#32455;&#32773;&#20026;&#36825;&#20010;&#23376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#21452;&#35821;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#33521;&#25991;&#21644;&#35199;&#29677;&#29273;&#25991;&#30340;&#25991;&#26412;&#65292;&#28085;&#30422;&#20102;&#27861;&#24459;&#25991;&#26412;&#12289;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21644;&#25805;&#20316;&#25351;&#21335;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#25105;&#20204;&#20027;&#35201;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#35757;&#32451;&#25216;&#24039;&#65288;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#34394;&#25311;&#23545;&#25239;&#35757;&#32451;&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20132;&#20102;&#19977;&#20010;&#36816;&#34892;&#32467;&#26524;&#65292;&#20854;&#20013;&#20004;&#20010;&#26159;&#38598;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;66.63%&#30340;&#23439;F1&#20998;&#25968;&#65292;&#22312;&#35199;&#29677;&#29273;&#25991;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;67.10%&#30340;&#23439;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the solutions submitted by the UPB team to the AuTexTification shared task, featured as part of IberLEF-2023. Our team participated in the first subtask, identifying text documents produced by large language models instead of humans. The organizers provided a bilingual dataset for this subtask, comprising English and Spanish texts covering multiple domains, such as legal texts, social media posts, and how-to articles. We experimented mostly with deep learning models based on Transformers, as well as training techniques such as multi-task learning and virtual adversarial training to obtain better results. We submitted three runs, two of which consisted of ensemble models. Our best-performing model achieved macro F1-scores of 66.63% on the English dataset and 67.10% on the Spanish dataset.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#19968;&#27454;&#21517;&#20026;"Hoodwinked"&#30340;&#25991;&#26412;&#28216;&#25103;&#65292;&#30740;&#31350;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#26432;&#25163;&#32463;&#24120;&#21542;&#35748;&#32618;&#34892;&#24182;&#25351;&#36131;&#20182;&#20154;&#65292;&#23548;&#33268;&#25237;&#31080;&#32467;&#26524;&#21463;&#21040;&#24433;&#21709;&#12290;&#26356;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#26432;&#25163;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#36825;&#31181;&#25913;&#36827;&#26159;&#36890;&#36807;&#22312;&#35752;&#35770;&#20013;&#26356;&#24378;&#30340;&#27450;&#39575;&#33021;&#21147;&#23454;&#29616;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.01404</link><description>&lt;p&gt;
&#33945;&#39575;&#65306;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#20013;&#30340;&#27450;&#39575;&#19982;&#21512;&#20316;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models. (arXiv:2308.01404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01404
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#19968;&#27454;&#21517;&#20026;"Hoodwinked"&#30340;&#25991;&#26412;&#28216;&#25103;&#65292;&#30740;&#31350;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#26432;&#25163;&#32463;&#24120;&#21542;&#35748;&#32618;&#34892;&#24182;&#25351;&#36131;&#20182;&#20154;&#65292;&#23548;&#33268;&#25237;&#31080;&#32467;&#26524;&#21463;&#21040;&#24433;&#21709;&#12290;&#26356;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#26432;&#25163;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#36825;&#31181;&#25913;&#36827;&#26159;&#36890;&#36807;&#22312;&#35752;&#35770;&#20013;&#26356;&#24378;&#30340;&#27450;&#39575;&#33021;&#21147;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#65311;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#27454;&#21517;&#20026;&#8220;Hoodwinked&#8221;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#65292;&#21463;&#21040;&#8220;&#40657;&#24110;&#8221;&#21644;&#8220;&#35841;&#26159;&#21351;&#24213;&#8221;&#28216;&#25103;&#30340;&#21551;&#21457;&#65292;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#29609;&#23478;&#20204;&#34987;&#38145;&#22312;&#19968;&#20010;&#25151;&#23376;&#37324;&#65292;&#24517;&#39035;&#25214;&#21040;&#19968;&#25226;&#38053;&#21273;&#25165;&#33021;&#36867;&#33073;&#65292;&#20294;&#20854;&#20013;&#19968;&#20010;&#29609;&#23478;&#34987;&#27966;&#20219;&#21153;&#26432;&#27515;&#20854;&#20182;&#20154;&#12290;&#27599;&#27425;&#21457;&#29983;&#35851;&#26432;&#65292;&#24184;&#23384;&#30340;&#29609;&#23478;&#20204;&#20250;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#35752;&#35770;&#65292;&#28982;&#21518;&#25237;&#31080;&#23558;&#19968;&#21517;&#29609;&#23478;&#25918;&#36880;&#20986;&#28216;&#25103;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#25805;&#25511;&#20195;&#29702;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#20102;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#35777;&#25454;&#12290;&#26432;&#25163;&#32463;&#24120;&#21542;&#35748;&#33258;&#24049;&#30340;&#32618;&#34892;&#24182;&#25351;&#36131;&#20182;&#20154;&#65292;&#23548;&#33268;&#25237;&#31080;&#32467;&#26524;&#21463;&#21040;&#21487;&#27979;&#37327;&#30340;&#24433;&#21709;&#12290;&#26356;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;24&#20010;&#20004;&#20004;&#27604;&#36739;&#20013;&#30340;18&#20010;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26432;&#25163;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;&#27425;&#35201;&#25351;&#26631;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#36825;&#31181;&#25913;&#36827;&#24182;&#19981;&#26159;&#36890;&#36807;&#19981;&#21516;&#30340;&#34892;&#21160;&#23454;&#29616;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#22312;&#35752;&#35770;&#20013;&#26356;&#24378;&#30340;&#27450;&#39575;&#33021;&#21147;&#23454;&#29616;&#30340;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#23454;&#36136;&#24615;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are current language models capable of deception and lie detection? We study this question by introducing a text-based game called $\textit{Hoodwinked}$, inspired by $\textit{Mafia}$ and $\textit{Among Us}$. Players are locked in a house and must find a key to escape, but one player is tasked with killing the others. Each time a murder is committed, the surviving players have a natural language discussion then vote to banish one player from the game. We conduct experiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 and find evidence of deception and lie detection capabilities. The killer often denies their crime and accuses others, leading to measurable effects on voting outcomes. More advanced models are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons. Secondary metrics provide evidence that this improvement is not mediated by different actions, but rather by stronger deception capabilities during discussions. Overall, we find substantial
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#24182;&#36827;&#34892;&#34892;&#21160;&#12290;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#65292;&#20195;&#29702;&#22120;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#27169;&#22411;&#22238;&#28378;&#20013;&#36827;&#34892;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.01399</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Learning to Model the World with Language. (arXiv:2308.01399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#24182;&#36827;&#34892;&#34892;&#21160;&#12290;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#65292;&#20195;&#29702;&#22120;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#27169;&#22411;&#22238;&#28378;&#20013;&#36827;&#34892;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#19982;&#20154;&#31867;&#22312;&#19990;&#30028;&#20013;&#30456;&#20114;&#20316;&#29992;&#65292;&#20195;&#29702;&#22120;&#38656;&#35201;&#29702;&#35299;&#20154;&#20204;&#20351;&#29992;&#30340;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#31867;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#35270;&#35273;&#19990;&#30028;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#22522;&#20110;&#35821;&#35328;&#34892;&#21160;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#20195;&#29702;&#22120;&#21487;&#20197;&#36890;&#36807;&#20219;&#21153;&#22870;&#21169;&#23398;&#20064;&#25191;&#34892;&#31616;&#21333;&#30340;&#35821;&#35328;&#25351;&#20196;&#65292;&#20294;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#21487;&#20197;&#21033;&#29992;&#20256;&#36798;&#19968;&#33324;&#30693;&#35782;&#12289;&#25551;&#36848;&#19990;&#30028;&#29366;&#24577;&#12289;&#25552;&#20379;&#20114;&#21160;&#21453;&#39304;&#31561;&#22810;&#26679;&#21270;&#35821;&#35328;&#30340;&#20195;&#29702;&#22120;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#65306;&#23558;&#20250;&#34987;&#35266;&#23519;&#21040;&#20160;&#20040;&#12289;&#19990;&#30028;&#23558;&#22914;&#20309;&#36816;&#34892;&#20197;&#21450;&#21738;&#20123;&#24773;&#20917;&#23558;&#33719;&#24471;&#22870;&#21169;&#12290;&#36825;&#20010;&#35266;&#28857;&#23558;&#35821;&#35328;&#29702;&#35299;&#19982;&#26410;&#26469;&#39044;&#27979;&#32479;&#19968;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Dynalang&#65292;&#19968;&#31181;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#30340;&#20195;&#29702;&#22120;&#65292;&#23427;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#24819;&#20687;&#30340;&#27169;&#22411;&#22238;&#28378;&#20013;&#23398;&#20064;&#34892;&#21160;&#12290;&#19982;&#21482;&#20351;&#29992;&#35821;&#35328;&#39044;&#27979;&#21160;&#20316;&#30340;&#20256;&#32479;&#20195;&#29702;&#22120;&#19981;&#21516;&#65292;Dynalang&#36890;&#36807;&#36807;&#21435;&#30340;&#35821;&#35328;&#36824;&#21487;&#20197;&#33719;&#21462;&#20016;&#23500;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22312;ChatGPT&#20013;&#36816;&#29992;&#21512;&#36866;&#30340;&#25552;&#31034;&#23558;&#32763;&#35793;&#30446;&#30340;&#21644;&#30446;&#26631;&#21463;&#20247;&#34701;&#20837;&#36827;&#21435;&#23545;&#32763;&#35793;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#28789;&#27963;&#30340;&#32763;&#35793;&#32467;&#26524;&#65292;&#30456;&#27604;&#20256;&#32479;&#26426;&#22120;&#32763;&#35793;&#26356;&#20855;&#23450;&#21046;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01391</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#20248;&#21270;&#26426;&#22120;&#32763;&#35793;&#65306;ChatGPT&#21487;&#23450;&#21046;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimizing Machine Translation through Prompt Engineering: An Investigation into ChatGPT's Customizability. (arXiv:2308.01391v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22312;ChatGPT&#20013;&#36816;&#29992;&#21512;&#36866;&#30340;&#25552;&#31034;&#23558;&#32763;&#35793;&#30446;&#30340;&#21644;&#30446;&#26631;&#21463;&#20247;&#34701;&#20837;&#36827;&#21435;&#23545;&#32763;&#35793;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#28789;&#27963;&#30340;&#32763;&#35793;&#32467;&#26524;&#65292;&#30456;&#27604;&#20256;&#32479;&#26426;&#22120;&#32763;&#35793;&#26356;&#20855;&#23450;&#21046;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#23558;&#32763;&#35793;&#30446;&#30340;&#21644;&#30446;&#26631;&#21463;&#20247;&#34701;&#20837;&#21040;ChatGPT&#25552;&#31034;&#20013;&#23545;&#32763;&#35793;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#20511;&#37492;&#20102;&#20043;&#21069;&#30340;&#32763;&#35793;&#30740;&#31350;&#12289;&#34892;&#19994;&#23454;&#36341;&#21644;ISO&#26631;&#20934;&#65292;&#24378;&#35843;&#20102;&#32763;&#35793;&#36807;&#31243;&#20013;&#39044;&#29983;&#20135;&#38454;&#27573;&#30340;&#37325;&#35201;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#36866;&#24403;&#30340;&#25552;&#31034;&#21487;&#20197;&#20135;&#29983;&#28789;&#27963;&#30340;&#32763;&#35793;&#65292;&#36825;&#26159;&#20256;&#32479;&#30340;&#26426;&#22120;&#32763;&#35793;&#25152;&#27809;&#26377;&#23454;&#29616;&#30340;&#12290;&#30740;&#31350;&#23457;&#26597;&#20102;&#22312;&#29983;&#25104;&#28385;&#36275;&#29305;&#23450;&#26465;&#20214;&#30340;&#32763;&#35793;&#26102;&#65292;&#25552;&#31034;&#23545;&#32763;&#35793;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#35780;&#20272;&#20174;&#23454;&#38469;&#32763;&#35793;&#24072;&#30340;&#35282;&#24230;&#36827;&#34892;&#65292;&#20027;&#35266;&#21644;&#23450;&#24615;&#30456;&#32467;&#21512;&#65292;&#36824;&#20351;&#29992;&#20102;OpenAI&#30340;&#35789;&#23884;&#20837;API&#36827;&#34892;&#20313;&#24358;&#30456;&#20284;&#24230;&#35745;&#31639;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#32763;&#35793;&#30446;&#30340;&#21644;&#30446;&#26631;&#21463;&#20247;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#30830;&#23454;&#21487;&#20197;&#20462;&#25913;&#29983;&#25104;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the influence of integrating the purpose of the translation and the target audience into prompts on the quality of translations produced by ChatGPT. Drawing on previous translation studies, industry practices, and ISO standards, the research underscores the significance of the pre-production phase in the translation process. The study reveals that the inclusion of suitable prompts in large-scale language models like ChatGPT can yield flexible translations, a feat yet to be realized by conventional Machine Translation (MT). The research scrutinizes the changes in translation quality when prompts are used to generate translations that meet specific conditions. The evaluation is conducted from a practicing translator's viewpoint, both subjectively and qualitatively, supplemented by the use of OpenAI's word embedding API for cosine similarity calculations. The findings suggest that the integration of the purpose and target audience into prompts can indeed modify the gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36861;&#36394;&#20102;&#23454;&#35777;&#32763;&#35793;&#36807;&#31243;&#30740;&#31350;&#30340;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#33258;&#30001;&#33021;&#21407;&#29702;&#21644;&#20027;&#21160;&#25512;&#29702;&#20316;&#20026;&#24314;&#27169;&#28145;&#23884;&#20837;&#24335;&#32763;&#35793;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#28608;&#21160;&#20154;&#24515;&#30340;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.01368</link><description>&lt;p&gt;
&#23454;&#35777;&#32763;&#35793;&#36807;&#31243;&#30740;&#31350;&#65306;&#36807;&#21435;&#21644;&#21487;&#33021;&#30340;&#26410;&#26469;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Empirical Translation Process Research: Past and Possible Future Perspectives. (arXiv:2308.01368v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36861;&#36394;&#20102;&#23454;&#35777;&#32763;&#35793;&#36807;&#31243;&#30740;&#31350;&#30340;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#33258;&#30001;&#33021;&#21407;&#29702;&#21644;&#20027;&#21160;&#25512;&#29702;&#20316;&#20026;&#24314;&#27169;&#28145;&#23884;&#20837;&#24335;&#32763;&#35793;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#28608;&#21160;&#20154;&#24515;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#22235;&#21313;&#24180;&#37324;&#65292;&#20154;&#20204;&#19968;&#30452;&#33268;&#21147;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#23454;&#35777;&#32763;&#35793;&#36807;&#31243;&#30740;&#31350;&#65288;TPR&#65289;&#30340;&#27169;&#22411;&#65292;&#20294;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#36861;&#36394;&#20102;CRITT TPR-DB&#20256;&#32479;&#20013;&#23454;&#35777;TPR&#30340;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#30001;&#33021;&#21407;&#29702;&#65288;FEP&#65289;&#21644;&#20027;&#21160;&#25512;&#29702;&#65288;AIF&#65289;&#20316;&#20026;&#24314;&#27169;&#28145;&#23884;&#20837;&#24335;&#32763;&#35793;&#36807;&#31243;&#30340;&#26694;&#26550;&#12290;&#23427;&#24341;&#20837;&#20102;&#37327;&#21270;&#20851;&#32852;&#29702;&#35770;&#65288;&#30456;&#20851;&#24615;&#65292;s-mode&#65292;i-mode&#65289;&#22522;&#26412;&#27010;&#24565;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#19982;&#30417;&#25511;&#27169;&#22411;&#30340;&#20851;&#31995;&#65292;&#23558;&#20851;&#32852;&#24615;&#26368;&#22823;&#21270;&#23450;&#20026;&#26368;&#23567;&#21270;&#33258;&#30001;&#33021;&#30340;&#29305;&#20363;&#12290;FEP/AIF&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#20005;&#35880;&#30340;&#22522;&#30784;&#65292;&#21487;&#20197;&#24314;&#27169;&#19981;&#21516;&#26102;&#38388;&#32447;&#19978;&#23637;&#24320;&#30340;&#23884;&#20837;&#24335;&#32763;&#35793;&#36807;&#31243;&#30340;&#28145;&#23618;&#26102;&#38388;&#26550;&#26500;&#12290;&#36825;&#20010;&#26694;&#26550;&#20026;&#39044;&#27979;&#24615;TPR&#30340;&#26410;&#26469;&#30740;&#31350;&#24320;&#36767;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21069;&#26223;&#65292;&#26377;&#26395;&#20016;&#23500;&#25105;&#20204;&#23545;&#20154;&#31867;&#32763;&#35793;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#23454;&#36341;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past four decades, efforts have been made to develop and evaluate models for Empirical Translation Process Research (TPR), yet a comprehensive framework remains elusive. This article traces the evolution of empirical TPR within the CRITT TPR-DB tradition and proposes the Free Energy Principle (FEP) and Active Inference (AIF) as a framework for modeling deeply embedded translation processes. It introduces novel approaches for quantifying fundamental concepts of Relevance Theory (relevance, s-mode, i-mode), and establishes their relation to the Monitor Model, framing relevance maximization as a special case of minimizing free energy. FEP/AIF provides a mathematically rigorous foundation that enables modeling of deep temporal architectures in which embedded translation processes unfold on different timelines. This framework opens up exciting prospects for future research in predictive TPR, likely to enrich our comprehension of human translation processes, and making valuable cont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#23545;&#22833;&#35821;&#30151;&#20122;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#21033;&#29992;&#22768;&#38899;&#24405;&#38899;&#33258;&#21160;&#35782;&#21035;&#24182;&#35780;&#20272;&#35328;&#35821;&#38556;&#30861;&#12290;&#35813;&#26041;&#27861;&#22312;&#21306;&#20998;&#22833;&#35821;&#30151;&#24739;&#32773;&#21644;&#20581;&#24247;&#23545;&#29031;&#32452;&#30340;&#24405;&#38899;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#36817;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#20197;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21306;&#20998;&#26368;&#24120;&#35265;&#30340;&#22833;&#35821;&#30151;&#31867;&#22411;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#30142;&#30149;&#21644;&#35821;&#35328;&#65292;&#24182;&#26377;&#26395;&#31283;&#20581;&#22320;&#25552;&#21462;&#35786;&#26029;&#24615;&#30340;&#35328;&#35821;&#29983;&#29289;&#26631;&#24535;&#12290;</title><link>http://arxiv.org/abs/2308.01327</link><description>&lt;p&gt;
Careful Whisper - &#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#36827;&#23637;&#26469;&#36827;&#34892;&#20581;&#22766;&#19988;&#26131;&#35299;&#37322;&#30340;&#22833;&#35821;&#30151;&#20122;&#22411;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Careful Whisper -- leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification. (arXiv:2308.01327v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#23545;&#22833;&#35821;&#30151;&#20122;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#21033;&#29992;&#22768;&#38899;&#24405;&#38899;&#33258;&#21160;&#35782;&#21035;&#24182;&#35780;&#20272;&#35328;&#35821;&#38556;&#30861;&#12290;&#35813;&#26041;&#27861;&#22312;&#21306;&#20998;&#22833;&#35821;&#30151;&#24739;&#32773;&#21644;&#20581;&#24247;&#23545;&#29031;&#32452;&#30340;&#24405;&#38899;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#36817;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#20197;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21306;&#20998;&#26368;&#24120;&#35265;&#30340;&#22833;&#35821;&#30151;&#31867;&#22411;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#30142;&#30149;&#21644;&#35821;&#35328;&#65292;&#24182;&#26377;&#26395;&#31283;&#20581;&#22320;&#25552;&#21462;&#35786;&#26029;&#24615;&#30340;&#35328;&#35821;&#29983;&#29289;&#26631;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22768;&#38899;&#24405;&#38899;&#35782;&#21035;&#26469;&#36741;&#21161;&#35780;&#20272;&#35328;&#35821;&#38556;&#30861;&#12290;&#36890;&#36807;&#32467;&#21512;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#21644;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#20016;&#23500;&#30340;&#22768;&#23398;&#21644;&#28165;&#26224;&#30340;&#36716;&#24405;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#20960;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20174;&#36825;&#20123;&#36716;&#24405;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20197;&#29983;&#25104;&#20581;&#24247;&#35821;&#38899;&#30340;&#21407;&#22411;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#22411;&#30340;&#22522;&#26412;&#36317;&#31163;&#24230;&#37327;&#20316;&#20026;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#20154;&#31867;&#27700;&#24179;&#19978;&#21306;&#20998;&#22833;&#35821;&#30151;&#24739;&#32773;&#30340;&#24405;&#38899;&#21644;&#20581;&#24247;&#23545;&#29031;&#32452;&#12290;&#27492;&#22806;&#65292;&#26368;&#24120;&#35265;&#30340;&#22833;&#35821;&#30151;&#31867;&#22411;&#21487;&#20197;&#20197;90%&#30340;&#20934;&#30830;&#29575;&#36827;&#34892;&#21306;&#20998;&#12290;&#35813;&#27969;&#31243;&#21487;&#30452;&#25509;&#36866;&#29992;&#20110;&#20854;&#20182;&#30142;&#30149;&#21644;&#35821;&#35328;&#65292;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#28508;&#21147;&#26469;&#31283;&#20581;&#22320;&#25552;&#21462;&#35786;&#26029;&#24615;&#35328;&#35821;&#29983;&#29289;&#26631;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a fully automated approach for identifying speech anomalies from voice recordings to aid in the assessment of speech impairments. By combining Connectionist Temporal Classification (CTC) and encoder-decoder-based automatic speech recognition models, we generate rich acoustic and clean transcripts. We then apply several natural language processing methods to extract features from these transcripts to produce prototypes of healthy speech. Basic distance measures from these prototypes serve as input features for standard machine learning classifiers, yielding human-level accuracy for the distinction between recordings of people with aphasia and a healthy control group. Furthermore, the most frequently occurring aphasia types can be distinguished with 90% accuracy. The pipeline is directly applicable to other diseases and languages, showing promise for robustly extracting diagnostic speech biomarkers.
&lt;/p&gt;</description></item><item><title>DeepSpeed-Chat&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#20351;&#24471;ChatGPT-like&#27169;&#22411;&#30340;RLHF&#22521;&#35757;&#26131;&#20110;&#35775;&#38382;&#65292;&#39640;&#25928;&#19988;&#32463;&#27982;&#23454;&#24800;&#12290;&#23427;&#20855;&#26377;&#26131;&#20110;&#20351;&#29992;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#20307;&#39564;&#65292;&#22797;&#21046;&#20102;InstructGPT&#30340;&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#38598;&#25104;&#20102;&#21508;&#31181;&#35757;&#32451;&#21644;&#25512;&#26029;&#20248;&#21270;&#65292;&#25552;&#20379;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01320</link><description>&lt;p&gt;
DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales.
&lt;/p&gt;
&lt;p&gt;
DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. (arXiv:2308.01320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01320
&lt;/p&gt;
&lt;p&gt;
DeepSpeed-Chat&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#20351;&#24471;ChatGPT-like&#27169;&#22411;&#30340;RLHF&#22521;&#35757;&#26131;&#20110;&#35775;&#38382;&#65292;&#39640;&#25928;&#19988;&#32463;&#27982;&#23454;&#24800;&#12290;&#23427;&#20855;&#26377;&#26131;&#20110;&#20351;&#29992;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#20307;&#39564;&#65292;&#22797;&#21046;&#20102;InstructGPT&#30340;&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#38598;&#25104;&#20102;&#21508;&#31181;&#35757;&#32451;&#21644;&#25512;&#26029;&#20248;&#21270;&#65292;&#25552;&#20379;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT-like&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#24102;&#26469;&#20102;&#38761;&#21629;&#65292;&#20174;&#25688;&#35201;&#21644;&#32534;&#30721;&#21040;&#32763;&#35793;&#65292;&#29978;&#33267;&#36229;&#36234;&#20102;&#20154;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29615;&#22659;&#36824;&#32570;&#20047;&#19968;&#31181;&#26131;&#20110;&#35775;&#38382;&#12289;&#39640;&#25928;&#19988;&#32463;&#27982;&#23454;&#24800;&#30340;&#31471;&#21040;&#31471;RLHF&#65288;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65289;&#35757;&#32451;&#27969;&#31243;&#65292;&#29305;&#21035;&#26159;&#24403;&#35757;&#32451;&#35268;&#27169;&#36798;&#21040;&#25968;&#21313;&#20159;&#21442;&#25968;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepSpeed-Chat&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#20351;RLHF&#22521;&#35757;&#23545;AI&#31038;&#21306;&#21487;&#29992;&#12290;DeepSpeed-Chat&#25552;&#20379;&#20102;&#19977;&#20010;&#20851;&#38190;&#33021;&#21147;&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;ChatGPT-like&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#20307;&#39564;&#65292;&#19968;&#20010;&#22797;&#21046;InstructGPT&#35757;&#32451;&#27969;&#31243;&#30340;DeepSpeed-RLHF&#27969;&#27700;&#32447;&#65292;&#20197;&#21450;&#19968;&#20010;&#38598;&#25104;&#20102;&#21508;&#31181;&#35757;&#32451;&#21644;&#25512;&#26029;&#20248;&#21270;&#30340;&#24378;&#22823;DeepSpeed-RLHF&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#25552;&#20379;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#20197;&#35757;&#32451;&#20855;&#26377;&#25968;&#21315;&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-like models have revolutionized various applications in artificial intelligence, from summarization and coding to translation, matching or even surpassing human performance. However, the current landscape lacks an accessible, efficient, and cost-effective end-to-end RLHF (Reinforcement Learning with Human Feedback) training pipeline for these powerful models, particularly when training at the scale of billions of parameters. This paper introduces DeepSpeed-Chat, a novel system that democratizes RLHF training, making it accessible to the AI community. DeepSpeed-Chat offers three key capabilities: an easy-to-use training and inference experience for ChatGPT-like models, a DeepSpeed-RLHF pipeline that replicates the training pipeline from InstructGPT, and a robust DeepSpeed-RLHF system that combines various optimizations for training and inference in a unified way. The system delivers unparalleled efficiency and scalability, enabling training of models with hundreds of billions of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.16680</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26223;&#35266;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#39046;&#20808;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23545;&#20154;&#31867;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#20063;&#26292;&#38706;&#20986;&#22266;&#26377;&#30340;&#39118;&#38505;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#30340;&#21452;&#37325;&#24615;&#36136;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#21487;&#20449;&#24230;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#25991;&#29486;&#65292;&#20294;&#38024;&#23545;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#35843;&#26597;&#20102;&#28041;&#21450;&#36825;&#20123;&#27169;&#22411;&#30340;&#38271;&#26399;&#21644;&#26032;&#20852;&#23041;&#32961;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36131;&#20219;&#36825;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#24352;&#35814;&#23613;&#30340;&#22320;&#22270;&#65292;&#27010;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20123;&#21162;&#21147;&#23545;&#20110;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
&lt;/p&gt;</description></item><item><title>Gzip&#19982;KNN&#30456;&#27604;&#36739;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#31616;&#21333;&#30340;&#35789;&#34955;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.15002</link><description>&lt;p&gt;
Gzip&#19982;KNN&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#23545;&#27604;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Gzip versus bag-of-words for text classification with KNN. (arXiv:2307.15002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15002
&lt;/p&gt;
&lt;p&gt;
Gzip&#19982;KNN&#30456;&#27604;&#36739;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#31616;&#21333;&#30340;&#35789;&#34955;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;KNN&#30340;&#25991;&#26412;&#20998;&#31867;&#20013;&#21387;&#32553;&#36317;&#31163;&#65288;gzip&#65289;&#30340;&#26377;&#25928;&#24615;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#21487;&#33021;&#19981;&#38656;&#35201;&#25991;&#26412;&#21387;&#32553;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#8220;&#35789;&#34955;&#8221;&#21305;&#37197;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of compression distance in KNN-based text classification ('gzip') has recently garnered lots of attention. In this note, we show that similar or better effectiveness can be achieved with simpler means, and text compression may not be necessary. Indeed, we find that a simple 'bag-of-words' matching can achieve similar or better accuracy, and is more efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#23545;&#20154;&#31867;&#24402;&#32435;&#25512;&#29702;&#20013;&#30340;&#23646;&#24615;&#24402;&#32435;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-3.5&#26377;&#19968;&#20123;&#22256;&#38590;&#65292;&#20294;GPT-4&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30456;&#20284;&#65292;&#38500;&#20102;&#26410;&#33021;&#25429;&#25417;&#21040;&#21069;&#25552;&#30340;&#38750;&#21333;&#35843;&#24615;&#29616;&#35937;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#20154;&#31867;&#21644;&#26426;&#22120;&#26234;&#33021;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20316;&#26410;&#26469;&#30740;&#31350;&#22522;&#20934;&#30340;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.06548</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24402;&#32435;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Inductive reasoning in humans and large language models. (arXiv:2306.06548v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#23545;&#20154;&#31867;&#24402;&#32435;&#25512;&#29702;&#20013;&#30340;&#23646;&#24615;&#24402;&#32435;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-3.5&#26377;&#19968;&#20123;&#22256;&#38590;&#65292;&#20294;GPT-4&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30456;&#20284;&#65292;&#38500;&#20102;&#26410;&#33021;&#25429;&#25417;&#21040;&#21069;&#25552;&#30340;&#38750;&#21333;&#35843;&#24615;&#29616;&#35937;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#20154;&#31867;&#21644;&#26426;&#22120;&#26234;&#33021;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20316;&#26410;&#26469;&#30740;&#31350;&#22522;&#20934;&#30340;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21331;&#36234;&#24615;&#33021;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#26159;&#21542;&#33021;&#20316;&#20026;&#26222;&#36890;&#26234;&#33021;&#30340;&#27169;&#22411;&#25110;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#30340;&#31243;&#24230;&#30340;&#30097;&#38382;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;GPT-3.5&#21644;GPT-4&#24212;&#29992;&#20110;&#20154;&#31867;&#24402;&#32435;&#25512;&#29702;&#20013;&#30340;&#19968;&#20010;&#32463;&#20856;&#38382;&#39064;&#65292;&#21363;&#23646;&#24615;&#24402;&#32435;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#20004;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#33719;&#21462;&#20102;&#20154;&#31867;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#30340;&#23646;&#24615;&#24402;&#32435;&#20219;&#21153;&#19978;&#30340;&#21028;&#26029;&#12290;&#23613;&#31649;GPT-3.5&#22312;&#25429;&#25417;&#20154;&#31867;&#34892;&#20026;&#30340;&#35768;&#22810;&#26041;&#38754;&#19978;&#26377;&#22256;&#38590;&#65292;&#20294;GPT-4&#26356;&#21152;&#25104;&#21151;&#65306;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#65292;&#23427;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30340;&#34920;&#29616;&#22312;&#36136;&#19978;&#30456;&#21305;&#37197;&#65292;&#21807;&#19968;&#26174;&#33879;&#30340;&#20363;&#22806;&#26159;&#20854;&#26410;&#33021;&#25429;&#25417;&#21040;&#21069;&#25552;&#30340;&#38750;&#21333;&#35843;&#24615;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#23646;&#24615;&#24402;&#32435;&#21487;&#20197;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#26234;&#33021;&#36827;&#34892;&#26377;&#36259;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#20316;&#20026;&#26410;&#26469;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impressive recent performance of large language models has led many to wonder to what extent they can serve as models of general intelligence or are similar to human cognition. We address this issue by applying GPT-3.5 and GPT-4 to a classic problem in human inductive reasoning known as property induction. Over two experiments, we elicit human judgments on a range of property induction tasks spanning multiple domains. Although GPT-3.5 struggles to capture many aspects of human behaviour, GPT-4 is much more successful: for the most part, its performance qualitatively matches that of humans, and the only notable exception is its failure to capture the phenomenon of premise non-monotonicity. Our work demonstrates that property induction allows for interesting comparisons between human and machine intelligence and provides two large datasets that can serve as benchmarks for future work in this vein.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#35821;&#35328;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#21644;&#19968;&#20010;&#21253;&#21547;13&#20010;&#19982;&#36136;&#37327;&#30456;&#20851;&#22240;&#32032;&#30340;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#35299;&#20915;&#37326;&#22806;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#24847;&#35265;&#21644;&#25968;&#25454;&#26469;&#24314;&#31435;&#22810;&#32500;&#40614;&#20811;&#26031;&#38886;&#25968;&#25454;&#24211;&#65292;&#20197;&#24110;&#21161;&#35780;&#20272;&#35270;&#39057;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.12726</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#35299;&#37322;&#30340;&#37326;&#22806;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65306;&#25968;&#25454;&#24211;&#21644;&#20197;&#35821;&#35328;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable In-the-Wild Video Quality Assessment: A Database and a Language-Prompted Approach. (arXiv:2305.12726v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#35821;&#35328;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#21644;&#19968;&#20010;&#21253;&#21547;13&#20010;&#19982;&#36136;&#37327;&#30456;&#20851;&#22240;&#32032;&#30340;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#35299;&#20915;&#37326;&#22806;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#24847;&#35265;&#21644;&#25968;&#25454;&#26469;&#24314;&#31435;&#22810;&#32500;&#40614;&#20811;&#26031;&#38886;&#25968;&#25454;&#24211;&#65292;&#20197;&#24110;&#21161;&#35780;&#20272;&#35270;&#39057;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37326;&#22806;&#35270;&#39057;&#30340;&#34028;&#21187;&#21457;&#23637;&#22823;&#22823;&#25299;&#23637;&#20102;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;VQA&#65289;&#38382;&#39064;&#12290;&#19982;&#36890;&#24120;&#21482;&#20851;&#27880;&#26377;&#38480;&#30340;&#22833;&#30495;&#31867;&#22411;&#30340;&#26089;&#26399;&#23450;&#20041;&#19981;&#21516;&#65292;&#37326;&#22806;&#35270;&#39057;&#19978;&#30340;VQA&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#21463;&#21040;&#21508;&#31181;&#22833;&#30495;&#21644;&#22810;&#26679;&#21270;&#20869;&#23481;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#20027;&#35266;&#30740;&#31350;&#24050;&#32463;&#25910;&#38598;&#20102;&#36825;&#20123;&#35270;&#39057;&#30340;&#25972;&#20307;&#36136;&#37327;&#24471;&#20998;&#65292;&#20294;&#25277;&#35937;&#36136;&#37327;&#24471;&#20998;&#19982;&#20855;&#20307;&#22240;&#32032;&#30340;&#20851;&#31995;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#36825;&#38459;&#30861;&#20102;VQA&#26041;&#27861;&#23545;&#36136;&#37327;&#35780;&#20272;&#30340;&#26356;&#20855;&#20307;&#35780;&#20272;&#65288;&#20363;&#22914;&#35270;&#39057;&#30340;&#28165;&#26224;&#24230;&#65289;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#23545;13&#20010;&#19982;&#36136;&#37327;&#30456;&#20851;&#22240;&#32032;&#65288;&#21253;&#25324;&#25429;&#25417;&#20013;&#30340;&#30495;&#23454;&#22833;&#30495;&#65292;&#22914;&#36816;&#21160;&#27169;&#31946;&#12289;&#22122;&#22768;&#12289;&#38378;&#28865;&#65307;&#30001;&#21387;&#32553;&#21644;&#20256;&#36755;&#24341;&#36215;&#30340;&#38169;&#35823;&#65307;&#20197;&#21450;&#23545;&#35821;&#20041;&#20869;&#23481;&#21644;&#23457;&#32654;&#38382;&#39064;&#30340;&#26356;&#39640;&#23618;&#27425;&#20307;&#39564;&#65292;&#22914;&#26500;&#25104;&#12289;&#38236;&#22836;&#36712;&#36857;&#65289;&#19979;&#30340;4,543&#20010;&#37326;&#22806;&#35270;&#39057;&#30340;&#20004;&#30334;&#22810;&#19975;&#20010;&#24847;&#35265;&#65292;&#20197;&#24314;&#31435;&#22810;&#32500;&#24230;&#30340;&#40614;&#20811;&#26031;&#38886;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of in-the-wild videos has greatly expanded the Video Quality Assessment (VQA) problem. Unlike early definitions that usually focus on limited distortion types, VQA on in-the-wild videos is especially challenging as it could be affected by complicated factors, including various distortions and diverse contents. Though subjective studies have collected overall quality scores for these videos, how the abstract quality scores relate with specific factors is still obscure, hindering VQA methods from more concrete quality evaluations (e.g. sharpness of a video). To solve this problem, we collect over two million opinions on 4,543 in-the-wild videos on 13 dimensions of quality-related factors, including in-capture authentic distortions (e.g. motion blur, noise, flicker), errors introduced by compression and transmission, and higher-level experiences on semantic contents and aesthetic issues (e.g. composition, camera trajectory), to establish the multi-dimensional Maxwell dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#38646;&#29031;&#39038;&#25552;&#31034;&#22312;&#20845;&#20010;&#26368;&#26032;&#21457;&#24067;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#21457;&#29616;&#30340;CoT&#25552;&#31034;&#21487;&#22312;&#26032;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#24212;&#29992;&#20110;GPT-4&#27169;&#22411;&#26102;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02897</link><description>&lt;p&gt;
&#33258;&#21160;&#21457;&#29616;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
An automatically discovered chain-of-thought prompt generalizes to novel models and datasets. (arXiv:2305.02897v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#38646;&#29031;&#39038;&#25552;&#31034;&#22312;&#20845;&#20010;&#26368;&#26032;&#21457;&#24067;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#21457;&#29616;&#30340;CoT&#25552;&#31034;&#21487;&#22312;&#26032;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#24212;&#29992;&#20110;GPT-4&#27169;&#22411;&#26102;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#33021;&#21147;&#26377;&#26395;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20808;&#21069;&#27169;&#22411;&#25152;&#21046;&#23450;&#30340;&#25552;&#31034;&#31574;&#30053;&#22914;&#20309;&#36866;&#29992;&#20110;&#26032;&#27169;&#22411;&#21644;&#19981;&#21516;&#25968;&#25454;&#38598;&#20173;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#23567;&#22411;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#38646;&#29031;&#39038;&#25552;&#31034;&#65288;zero-shot prompts&#65289;&#30340;&#24615;&#33021;&#65292;&#20197;&#35825;&#23548;CoT&#25512;&#29702;&#65292;&#22312;6&#20010;&#26368;&#26032;&#21457;&#24067;&#30340;LLM&#65288;davinci-002&#65292;davinci-003&#65292;GPT-3.5-turbo&#65292;GPT-4&#65292;Flan-T5-xxl&#21644;Cohere command-xlarge&#65289;&#19978;&#19982;&#21253;&#25324;&#31185;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#30340;&#20845;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#28151;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#33258;&#21160;&#25552;&#31034;&#21457;&#29616;&#30340;CoT&#25552;&#31034;&#22312;&#23454;&#39564;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#24212;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#26102;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emergent chain-of-thought (CoT) reasoning capabilities promise to improve performance and explainability of large language models (LLMs). However, uncertainties remain about how prompting strategies formulated for previous model generations generalize to new model generations and different datasets. In this small-scale study we compare the performance of a range of zero-shot prompts for inducing CoT reasoning across six recently released LLMs (davinci-002, davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on a mixture of six question-answering datasets, including datasets from scientific and medical domains. We find that a CoT prompt that was previously discovered through automated prompt discovery shows robust performance across experimental conditions and produces best results when applied to the state-of-the-art model GPT-4.
&lt;/p&gt;</description></item><item><title>LLM&#22312;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#20013;&#34920;&#29616;&#19981;&#22914;&#19987;&#38376;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#20294;&#22914;&#26524;&#25552;&#20379;&#27491;&#30830;&#30340;&#27133;&#20540;&#65292;&#20173;&#26377;&#24341;&#23548;&#23545;&#35805;&#25104;&#21151;&#32467;&#26463;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#36890;&#36807;&#30495;&#23454;&#20449;&#24565;&#29366;&#24577;&#20998;&#24067;&#25110;&#22495;&#20869;&#31034;&#20363;&#30340;&#35775;&#38382;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.06556</link><description>&lt;p&gt;
LLM&#26159;&#21542;&#36275;&#20197;&#29992;&#20110;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are LLMs All You Need for Task-Oriented Dialogue?. (arXiv:2304.06556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06556
&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#20013;&#34920;&#29616;&#19981;&#22914;&#19987;&#38376;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#20294;&#22914;&#26524;&#25552;&#20379;&#27491;&#30830;&#30340;&#27133;&#20540;&#65292;&#20173;&#26377;&#24341;&#23548;&#23545;&#35805;&#25104;&#21151;&#32467;&#26463;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#36890;&#36807;&#30495;&#23454;&#20449;&#24565;&#29366;&#24577;&#20998;&#24067;&#25110;&#22495;&#20869;&#31034;&#20363;&#30340;&#35775;&#38382;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#36890;&#36807;&#23545;&#35805;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#23427;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#22522;&#20934;&#27979;&#35797;&#20013;&#23436;&#25104;&#22810;&#36718;&#20219;&#21153;&#24182;&#19982;&#22806;&#37096;&#25968;&#25454;&#24211;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#26174;&#24335;&#20449;&#24565;&#29366;&#24577;&#36319;&#36394;&#65292;LLM&#34920;&#29616;&#19981;&#22914;&#19987;&#38376;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22914;&#26524;&#25552;&#20379;&#27491;&#30830;&#30340;&#25554;&#27133;&#20540;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#24341;&#23548;&#23545;&#35805;&#25104;&#21151;&#32467;&#26463;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#20855;&#26377;&#30495;&#23454;&#20449;&#24565;&#29366;&#24577;&#20998;&#24067;&#25110;&#22495;&#20869;&#31034;&#20363;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#35813;&#33021;&#21147;&#20250;&#24471;&#21040;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instructions-tuned Large Language Models (LLMs) gained recently huge popularity thanks to their ability to interact with users through conversation. In this work we aim to evaluate their ability to complete multi-turn tasks and interact with external databases in the context of established task-oriented dialogue benchmarks. We show that for explicit belief state tracking, LLMs underperform compared to specialized task-specific models. Nevertheless, they show ability to guide the dialogue to successful ending if given correct slot values. Furthermore this ability improves with access to true belief state distribution or in-domain examples.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22320;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#23548;&#21521;&#23548;&#33322;&#20219;&#21153;&#12290;&#36890;&#36807;&#26500;&#24314;&#23616;&#37096;&#24230;&#37327;&#22320;&#22270;&#21644;&#20840;&#23616;&#25299;&#25169;&#22320;&#22270;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#21051;&#30011;&#31354;&#38388;&#24863;&#30693;&#21644;&#23548;&#33322;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#35328;&#23548;&#21521;&#23548;&#33322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.04385</link><description>&lt;p&gt;
BEVBert: &#29992;&#20110;&#35821;&#35328;&#23548;&#21521;&#23548;&#33322;&#30340;&#22810;&#27169;&#24577;&#22320;&#22270;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BEVBert: Multimodal Map Pre-training for Language-guided Navigation. (arXiv:2212.04385v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22320;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#23548;&#21521;&#23548;&#33322;&#20219;&#21153;&#12290;&#36890;&#36807;&#26500;&#24314;&#23616;&#37096;&#24230;&#37327;&#22320;&#22270;&#21644;&#20840;&#23616;&#25299;&#25169;&#22320;&#22270;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#21051;&#30011;&#31354;&#38388;&#24863;&#30693;&#21644;&#23548;&#33322;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#35328;&#23548;&#21521;&#23548;&#33322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#24050;&#32463;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#37319;&#29992;&#31163;&#25955;&#30340;&#20840;&#26223;&#22270;&#26469;&#23398;&#20064;&#35270;&#35273;-&#25991;&#26412;&#20851;&#32852;&#12290;&#36825;&#35201;&#27714;&#27169;&#22411;&#38544;&#24335;&#22320;&#20851;&#32852;&#20840;&#26223;&#22270;&#20013;&#30340;&#19981;&#23436;&#25972;&#12289;&#37325;&#22797;&#30340;&#35266;&#23519;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#24433;&#21709;&#21040;&#26234;&#33021;&#20307;&#30340;&#31354;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22320;&#22270;&#30340;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#29992;&#20110;VLN&#20013;&#30340;&#31354;&#38388;&#24863;&#30693;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23616;&#37096;&#24230;&#37327;&#22320;&#22270;&#65292;&#26126;&#30830;&#22320;&#27719;&#32858;&#19981;&#23436;&#25972;&#30340;&#35266;&#23519;&#25968;&#25454;&#24182;&#28040;&#38500;&#37325;&#22797;&#65292;&#21516;&#26102;&#22312;&#19968;&#20010;&#20840;&#23616;&#25299;&#25169;&#22320;&#22270;&#20013;&#24314;&#27169;&#23548;&#33322;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#31181;&#28151;&#21512;&#35774;&#35745;&#21487;&#20197;&#24179;&#34913;VLN&#23545;&#30701;&#26399;&#25512;&#29702;&#21644;&#38271;&#26399;&#35268;&#21010;&#30340;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#28151;&#21512;&#22320;&#22270;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#26694;&#26550;&#26469;&#23398;&#20064;&#22810;&#27169;&#24577;&#22320;&#22270;&#34920;&#31034;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31354;&#38388;&#24863;&#30693;&#36328;&#27169;&#24577;&#25512;&#29702;&#65292;&#26377;&#21161;&#20110;&#35821;&#35328;&#23548;&#21521;&#23548;&#33322;&#30446;&#26631;&#30340;&#23454;&#29616;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-training has shown promising results on the vision-and-language navigation (VLN) task. However, most existing pre-training methods employ discrete panoramas to learn visual-textual associations. This requires the model to implicitly correlate incomplete, duplicate observations within the panoramas, which may impair an agent's spatial understanding. Thus, we propose a new map-based pre-training paradigm that is spatial-aware for use in VLN. Concretely, we build a local metric map to explicitly aggregate incomplete observations and remove duplicates, while modeling navigation dependency in a global topological map. This hybrid design can balance the demand of VLN for both short-term reasoning and long-term planning. Then, based on the hybrid map, we devise a pre-training framework to learn a multimodal map representation, which enhances spatial-aware cross-modal reasoning thereby facilitating the language-guided navigation goal. Extensive experiments demonstrate the effec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25366;&#25496;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#26126;&#24180;&#38889;&#22269;CSAT&#32771;&#35797;&#20013;&#21333;&#35789;&#30340;&#20986;&#29616;&#27010;&#29575;&#65292;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#35823;&#24046;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2211.15426</link><description>&lt;p&gt;
AI&#30693;&#36947;&#26126;&#24180;&#38889;&#22269;CSAT&#20013;&#20250;&#20986;&#29616;&#21738;&#20123;&#21333;&#35789;
&lt;/p&gt;
&lt;p&gt;
AI Knows Which Words Will Appear in Next Year's Korean CSAT. (arXiv:2211.15426v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25366;&#25496;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#26126;&#24180;&#38889;&#22269;CSAT&#32771;&#35797;&#20013;&#21333;&#35789;&#30340;&#20986;&#29616;&#27010;&#29575;&#65292;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#35823;&#24046;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#25991;&#26412;&#25366;&#25496;&#30340;&#21333;&#35789;&#31867;&#21035;&#20998;&#31867;&#26041;&#27861;&#21644;&#22522;&#20110;LSTM&#30340;&#35789;&#27719;&#20986;&#29616;&#27169;&#24335;&#39044;&#27979;&#26041;&#27861;&#12290;&#39318;&#20808;&#25551;&#36848;&#20102;&#22522;&#20110;&#31616;&#21333;&#25991;&#26412;&#20986;&#29616;&#39057;&#29575;&#20998;&#26512;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#24320;&#21457;&#20026;&#25968;&#25454;&#31579;&#36873;&#24037;&#20855;&#65292;&#20294;&#26174;&#31034;&#20986;&#27604;&#20808;&#21069;&#20316;&#21697;&#39640;4.35~6.21&#20493;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;LSTM&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35789;&#27719;&#20986;&#29616;&#27169;&#24335;&#39044;&#27979;&#26041;&#27861;&#12290;AI&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#21069;&#20960;&#27425;&#32771;&#35797;&#30340;&#25968;&#25454;&#31383;&#21475;&#36827;&#34892;&#22238;&#24402;&#65292;&#20197;&#39044;&#27979;&#19979;&#19968;&#27425;&#32771;&#35797;&#20013;&#21333;&#35789;&#20986;&#29616;&#30340;&#27010;&#29575;&#12290;AI&#22312;&#19981;&#21516;&#25968;&#25454;&#31383;&#21475;&#19978;&#39044;&#27979;&#30340;&#20540;&#32463;&#36807;&#21152;&#26435;&#27714;&#21644;&#22788;&#29702;&#65292;&#24471;&#21040;&#19968;&#20010;&#31216;&#20026;&#8220;AI-Score&#8221;&#30340;&#21333;&#19968;&#24471;&#20998;&#65292;&#34920;&#31034;&#19979;&#19968;&#24180;&#32771;&#35797;&#20013;&#21333;&#35789;&#20986;&#29616;&#30340;&#27010;&#29575;&#12290;&#25152;&#24314;&#35758;&#30340;&#26041;&#27861;&#22312;100&#20998;&#33539;&#22260;&#20869;&#26174;&#31034;&#20102;100&#65285;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#24471;&#20998;&#36229;&#36807;60&#20998;&#30340;&#21306;&#22495;&#20165;&#26174;&#31034;&#20102;1.7&#65285;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;&#25152;&#26377;&#28304;&#20195;&#30721;&#22343;&#21487;&#22312;&#20316;&#32773;&#30340;Git&#19978;&#20813;&#36153;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
A text-mining-based word class categorization method and LSTM-based vocabulary pattern prediction method are introduced in this paper. A preprocessing method based on simple text appearance frequency analysis is first described. This method was developed as a data screening tool but showed 4.35 ~ 6.21 times higher than previous works. An LSTM deep learning method is also suggested for vocabulary appearance pattern prediction method. AI performs a regression with various size of data window of previous exams to predict the probabilities of word appearance in the next exam. Predicted values of AI over various data windows are processed into a single score as a weighted sum, which we call an "AI-Score", which represents the probability of word appearance in next year's exam. Suggested method showed 100% accuracy at the range 100-score area and showed only 1.7% error of prediction in the section where the scores were over 60 points. All source codes are freely available at the authors' Git
&lt;/p&gt;</description></item></channel></rss>