<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RAIN&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26080;&#38656;&#24494;&#35843;&#25110;&#39069;&#22806;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#25105;&#35780;&#20272;&#21644;&#22238;&#28378;&#26426;&#21046;&#23454;&#29616;&#23545;&#40784;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#30452;&#25509;&#20135;&#29983;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.07124</link><description>&lt;p&gt;
RAIN: &#24744;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33258;&#25105;&#35843;&#25972;&#32780;&#26080;&#38656;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RAIN: Your Language Models Can Align Themselves without Finetuning. (arXiv:2309.07124v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RAIN&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26080;&#38656;&#24494;&#35843;&#25110;&#39069;&#22806;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#25105;&#35780;&#20272;&#21644;&#22238;&#28378;&#26426;&#21046;&#23454;&#29616;&#23545;&#40784;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#30452;&#25509;&#20135;&#29983;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24120;&#24120;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#65292;&#28982;&#21518;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#25351;&#23548;&#35843;&#20248;&#31561;&#26041;&#27861;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#23454;&#29616;&#23545;&#40784;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#23545;&#40784;&#20923;&#32467;&#30340;LLM&#26356;&#26377;&#21560;&#24341;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21518;&#19968;&#31181;&#24773;&#26223;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#23558;&#33258;&#25105;&#35780;&#20272;&#21644;&#22238;&#28378;&#26426;&#21046;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#19981;&#23545;&#40784;&#30340;LLM&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#22686;&#24378;&#30452;&#25509;&#20135;&#29983;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#21487;&#22238;&#28378;&#30340;&#33258;&#22238;&#24402;&#25512;&#29702;&#65288;RAIN&#65289;&#65292;&#23427;&#20801;&#35768;&#39044;&#35757;&#32451;&#30340;LLM&#35780;&#20272;&#33258;&#24049;&#30340;&#29983;&#25104;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#32467;&#26524;&#26469;&#24341;&#23548;&#21521;&#21518;&#22238;&#28378;&#21644;&#21521;&#21069;&#29983;&#25104;&#20197;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;RAIN&#22312;&#27169;&#22411;&#23545;&#40784;&#26102;&#26080;&#38656;&#39069;&#22806;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#12289;&#26799;&#24230;&#35745;&#31639;&#25110;&#21442;&#25968;&#26356;&#26032;&#65307;&#22312;&#33258;&#25105;&#35780;&#20272;&#38454;&#27573;&#65292;&#27169;&#22411;&#25509;&#25910;&#30340;&#26159;&#19968;&#20123;&#38543;&#26426;&#22238;&#28378;&#30340;&#29983;&#25104;&#26679;&#26412;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, the so-called finetuning step. In contrast, aligning frozen LLMs without any extra data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide backward rewind and forward generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates; during the self-evaluation phase, the model recei
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;MLLM&#22312;&#32431;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#30495;&#23454;&#24615;&#21644;&#20262;&#29702;&#23545;&#40784;&#33021;&#21147;&#65292;&#36825;&#24471;&#30410;&#20110;&#35270;&#35273;&#25351;&#23548;&#35843;&#20248;&#21644;&#20248;&#31168;&#30340;&#25351;&#23548;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.07120</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26412;&#35270;&#37326;&#65306;&#22810;&#27169;&#24577;&#35757;&#32451;&#25552;&#21319;&#20102;&#22312;&#30495;&#23454;&#24615;&#21644;&#20262;&#29702;&#36947;&#24503;&#26041;&#38754;&#30340;MLLM
&lt;/p&gt;
&lt;p&gt;
Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics. (arXiv:2309.07120v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07120
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;MLLM&#22312;&#32431;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#30495;&#23454;&#24615;&#21644;&#20262;&#29702;&#23545;&#40784;&#33021;&#21147;&#65292;&#36825;&#24471;&#30410;&#20110;&#35270;&#35273;&#25351;&#23548;&#35843;&#20248;&#21644;&#20248;&#31168;&#30340;&#25351;&#23548;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#22791;&#29702;&#35299;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#29983;&#25104;&#25991;&#26412;&#21709;&#24212;&#30340;&#22686;&#24378;&#33021;&#21147;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;MLLM&#30340;&#32431;NLP&#33021;&#21147;&#24120;&#24120;&#20302;&#20272;&#24182;&#26410;&#32463;&#27979;&#35797;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;MLLM&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24615;&#8212;&#8212;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#35270;&#35273;&#25351;&#23548;&#35843;&#20248;&#65292;&#19968;&#31181;&#23558;LLM&#36716;&#25442;&#20026;MLLM&#30340;&#27969;&#34892;&#31574;&#30053;&#65292;&#20986;&#20046;&#24847;&#26009;&#22320;&#24110;&#21161;&#27169;&#22411;&#22312;&#32431;NLP&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20262;&#29702;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;&#20363;&#22914;&#65292;&#32463;&#36807;&#35270;&#35273;&#25351;&#23548;&#35843;&#20248;&#30340;LLaMA2 7B&#27169;&#22411;&#22312;TruthfulQA-mc&#21644;&#20262;&#29702;&#36947;&#24503;&#22522;&#20934;&#19978;&#36229;&#36807;&#20102;&#32463;&#36807;&#36229;&#36807;&#19968;&#30334;&#19975;&#20154;&#24037;&#26631;&#27880;&#30340;LLaMA2-chat 7B&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#25913;&#36827;&#30340;&#23545;&#40784;&#21487;&#20197;&#24402;&#22240;&#20110;&#35270;&#35273;-&#25991;&#26412;&#25968;&#25454;&#22266;&#26377;&#30340;&#20248;&#31168;&#25351;&#23548;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models (MLLMs) are trained based on large language models (LLM), with an enhanced capability to comprehend multi-modal inputs and generate textual responses. While they excel in multi-modal tasks, the pure NLP abilities of MLLMs are often underestimated and left untested. In this study, we get out of the box and unveil an intriguing characteristic of MLLMs -- our preliminary results suggest that visual instruction tuning, a prevailing strategy for transitioning LLMs into MLLMs, unexpectedly and interestingly helps models attain both improved truthfulness and ethical alignment in the pure NLP context. For example, a visual-instruction-tuned LLaMA2 7B model surpasses the performance of the LLaMA2-chat 7B model, fine-tuned with over one million human annotations, on TruthfulQA-mc and Ethics benchmarks. Further analysis reveals that the improved alignment can be attributed to the superior instruction quality inherent to visual-text data. In releasing our code at 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#28304;&#23545;&#27604;&#21644;&#35821;&#35328;&#23545;&#27604;&#35299;&#30721;&#26469;&#35299;&#20915;&#26426;&#22120;&#32763;&#35793;&#20013;&#24187;&#35273;&#21644;&#20559;&#31163;&#30446;&#26631;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#25233;&#21046;&#24187;&#35273;&#21644;&#20559;&#31163;&#30446;&#26631;&#30340;&#32763;&#35793;&#12290;</title><link>http://arxiv.org/abs/2309.07098</link><description>&lt;p&gt;
&#36890;&#36807;&#28304;&#23545;&#27604;&#21644;&#35821;&#35328;&#23545;&#27604;&#35299;&#30721;&#26469;&#32531;&#35299;&#24187;&#35273;&#21644;&#20559;&#31163;&#30446;&#26631;&#30340;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Hallucinations and Off-target Machine Translation with Source-Contrastive and Language-Contrastive Decoding. (arXiv:2309.07098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#28304;&#23545;&#27604;&#21644;&#35821;&#35328;&#23545;&#27604;&#35299;&#30721;&#26469;&#35299;&#20915;&#26426;&#22120;&#32763;&#35793;&#20013;&#24187;&#35273;&#21644;&#20559;&#31163;&#30446;&#26631;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#25233;&#21046;&#24187;&#35273;&#21644;&#20559;&#31163;&#30446;&#26631;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#24187;&#35273;&#21644;&#20559;&#31163;&#30446;&#26631;&#30340;&#32763;&#35793;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#35299;&#30721;&#30446;&#26631;&#26469;&#32531;&#35299;&#36825;&#20004;&#31181;&#22833;&#36133;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25110;&#22806;&#37096;&#27169;&#22411;&#12290;&#22312;&#28304;&#23545;&#27604;&#35299;&#30721;&#20013;&#65292;&#25105;&#20204;&#23547;&#25214;&#19968;&#20010;&#32763;&#35793;&#65292;&#22312;&#32473;&#23450;&#27491;&#30830;&#36755;&#20837;&#26102;&#26159;&#21487;&#20449;&#30340;&#65292;&#20294;&#22312;&#38543;&#26426;&#36755;&#20837;&#29255;&#27573;&#32473;&#23450;&#26102;&#26159;&#19981;&#21487;&#20449;&#30340;&#65292;&#20551;&#35774;&#24187;&#35273;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#37117;&#26159;&#21516;&#26679;&#21487;&#20449;&#30340;&#12290;&#22312;&#35821;&#35328;&#23545;&#27604;&#35299;&#30721;&#20013;&#65292;&#25105;&#20204;&#23547;&#25214;&#19968;&#20010;&#32763;&#35793;&#65292;&#22312;&#32473;&#23450;&#27491;&#30830;&#35821;&#35328;&#25351;&#31034;&#31526;&#20196;&#29260;&#26102;&#26159;&#21487;&#20449;&#30340;&#65292;&#20294;&#32473;&#23450;&#38169;&#35823;&#35821;&#35328;&#25351;&#31034;&#31526;&#20196;&#29260;&#26102;&#26159;&#19981;&#21487;&#20449;&#30340;&#12290;&#22312;&#23545;M2M-100 (418M)&#21644;SMaLL-100&#36827;&#34892;&#23454;&#39564;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#25233;&#21046;&#20102;&#24187;&#35273;&#21644;&#20559;&#31163;&#30446;&#26631;&#30340;&#32763;&#35793;&#65292;&#24179;&#22343;&#22312;57&#20010;&#27979;&#35797;&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#25552;&#39640;&#20102;1.7&#21644;1.4&#20010;chrF2&#20998;&#25968;&#12290;&#22312;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#20013;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#25233;&#21046;&#20559;&#31163;&#30446;&#26631;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucinations and off-target translation remain unsolved problems in machine translation, especially for low-resource languages and massively multilingual models. In this paper, we introduce methods to mitigate both failure cases with a modified decoding objective, without requiring retraining or external models. In source-contrastive decoding, we search for a translation that is probable given the correct input, but improbable given a random input segment, hypothesising that hallucinations will be similarly probable given either. In language-contrastive decoding, we search for a translation that is probable, but improbable given the wrong language indicator token. In experiments on M2M-100 (418M) and SMaLL-100, we find that these methods effectively suppress hallucinations and off-target translations, improving chrF2 by 1.7 and 1.4 points on average across 57 tested translation directions. In a proof of concept on English--German, we also show that we can suppress off-target translat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Whisper&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#22659;&#30340;&#35821;&#38899;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#36866;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#20013;&#25991;&#26041;&#35328;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#21333;&#35789;&#38169;&#35823;&#29575;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#36873;&#25321;&#25216;&#26415;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.07081</link><description>&lt;p&gt;
Whisper&#33021;&#22815;&#36827;&#34892;&#22522;&#20110;&#35821;&#22659;&#30340;&#35821;&#38899;&#23398;&#20064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Whisper perform speech-based in-context learning. (arXiv:2309.07081v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Whisper&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#22659;&#30340;&#35821;&#38899;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#36866;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#20013;&#25991;&#26041;&#35328;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#21333;&#35789;&#38169;&#35823;&#29575;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#36873;&#25321;&#25216;&#26415;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;OpenAI&#21457;&#24067;&#30340;Whisper&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#30340;&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35821;&#22659;&#30340;&#35821;&#38899;&#23398;&#20064;&#65288;SICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#35797;&#26102;&#36866;&#24212;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26799;&#24230;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#26631;&#35760;&#30340;&#35821;&#38899;&#26679;&#26412;&#12290;&#20351;&#29992;&#20013;&#25991;&#26041;&#35328;&#36827;&#34892;&#35821;&#35328;&#32423;&#21035;&#30340;&#36866;&#24212;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#23558;SICL&#24212;&#29992;&#20110;&#23396;&#31435;&#35789;ASR&#26102;&#65292;&#21487;&#20197;&#22312;&#20004;&#20010;&#26041;&#35328;&#19978;&#20351;&#29992;&#20219;&#24847;&#22823;&#23567;&#30340;Whisper&#27169;&#22411;&#23454;&#29616;&#19968;&#33268;&#19988;&#26174;&#33879;&#30340;WER&#30456;&#23545;&#38477;&#20302;&#65292;&#24179;&#22343;&#20026;32.3%&#12290;&#22522;&#20110;k&#26368;&#36817;&#37051;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#25216;&#26415;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;SICL&#30340;&#25928;&#29575;&#65292;&#24179;&#22343;&#30456;&#23545;WER&#38477;&#20302;&#29575;&#20026;36.4%&#12290;&#36890;&#36807;&#35828;&#35805;&#20154;&#36866;&#24212;&#25110;&#36830;&#32493;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#26469;&#39564;&#35777;&#20102;&#36825;&#20123;&#21457;&#29616;&#65292;&#24182;&#19988;&#20004;&#32773;&#37117;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#30456;&#23545;WER&#38477;&#20302;&#12290;&#36824;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the in-context learning abilities of the Whisper automatic speech recognition (ASR) models released by OpenAI. A novel speech-based in-context learning (SICL) approach is proposed for test-time adaptation, which can reduce the word error rates (WERs) with only a small number of labelled speech samples without gradient descent. Language-level adaptation experiments using Chinese dialects showed that when applying SICL to isolated word ASR, consistent and considerable relative WER reductions can be achieved using Whisper models of any size on two dialects, which is on average 32.3%. A k-nearest-neighbours-based in-context example selection technique can be applied to further improve the efficiency of SICL, which can increase the average relative WER reduction to 36.4%. The findings are verified using speaker adaptation or continuous speech recognition tasks, and both achieved considerable relative WER reductions. Detailed quantitative analyses are also provided to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20195;&#30721;&#20248;&#21270;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20197;7B&#21442;&#25968;&#30340;transformer&#27169;&#22411;&#20026;&#20363;&#65292;&#36890;&#36807;&#39044;&#27979;&#25351;&#20196;&#35745;&#25968;&#21644;&#29983;&#25104;&#20248;&#21270;&#20195;&#30721;&#31561;&#36741;&#21161;&#23398;&#20064;&#20219;&#21153;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20248;&#21270;&#24615;&#33021;&#12290;&#22312;&#22823;&#37327;&#27979;&#35797;&#31243;&#24207;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#32534;&#35793;&#22120;&#30340;&#20248;&#21270;&#25928;&#26524;&#25552;&#39640;&#20102;3.0%&#65292;&#24182;&#23637;&#29616;&#20986;&#20196;&#20154;&#24778;&#21916;&#30340;&#24378;&#22823;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.07062</link><description>&lt;p&gt;
&#29992;&#20110;&#32534;&#35793;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Compiler Optimization. (arXiv:2309.07062v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20195;&#30721;&#20248;&#21270;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20197;7B&#21442;&#25968;&#30340;transformer&#27169;&#22411;&#20026;&#20363;&#65292;&#36890;&#36807;&#39044;&#27979;&#25351;&#20196;&#35745;&#25968;&#21644;&#29983;&#25104;&#20248;&#21270;&#20195;&#30721;&#31561;&#36741;&#21161;&#23398;&#20064;&#20219;&#21153;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20248;&#21270;&#24615;&#33021;&#12290;&#22312;&#22823;&#37327;&#27979;&#35797;&#31243;&#24207;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#32534;&#35793;&#22120;&#30340;&#20248;&#21270;&#25928;&#26524;&#25552;&#39640;&#20102;3.0%&#65292;&#24182;&#23637;&#29616;&#20986;&#20196;&#20154;&#24778;&#21916;&#30340;&#24378;&#22823;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20195;&#30721;&#20248;&#21270;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;7B&#21442;&#25968;&#30340;transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#20248;&#21270;LLVM&#27719;&#32534;&#30340;&#20195;&#30721;&#22823;&#23567;&#12290;&#35813;&#27169;&#22411;&#20197;&#26410;&#20248;&#21270;&#30340;&#27719;&#32534;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#19968;&#32452;&#26368;&#20339;&#20248;&#21270;&#31243;&#24207;&#30340;&#32534;&#35793;&#22120;&#36873;&#39033;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35201;&#27714;&#27169;&#22411;&#39044;&#27979;&#20248;&#21270;&#21069;&#21518;&#30340;&#25351;&#20196;&#35745;&#25968;&#21644;&#20248;&#21270;&#21518;&#30340;&#20195;&#30721;&#26412;&#36523;&#12290;&#36825;&#20123;&#36741;&#21161;&#23398;&#20064;&#20219;&#21153;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#28145;&#24230;&#12290;&#25105;&#20204;&#22312;&#19968;&#22871;&#22823;&#22411;&#27979;&#35797;&#31243;&#24207;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#25351;&#20196;&#35745;&#25968;&#26041;&#38754;&#27604;&#32534;&#35793;&#22120;&#25552;&#39640;&#20102;3.0%&#65292;&#36229;&#36807;&#20102;&#38656;&#35201;&#25968;&#21315;&#27425;&#32534;&#35793;&#30340;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#26174;&#31034;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22823;&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#65292;91%&#30340;&#26102;&#38388;&#29983;&#25104;&#21487;&#32534;&#35793;&#30340;&#20195;&#30721;&#65292;&#24182;70%&#30340;&#26102;&#38388;&#33021;&#23436;&#32654;&#27169;&#25311;&#32534;&#35793;&#22120;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the optimization performance of the model and improve the model's depth of understanding.  We evaluate on a large suite of test programs. Our approach achieves a 3.0% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations. Furthermore, the model shows surprisingly strong code reasoning abilities, generating compilable code 91% of the time and perfectly emulating the output of the compiler 70% of the time.
&lt;/p&gt;</description></item><item><title>SafetyBench&#26159;&#19968;&#20010;&#20840;&#38754;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;&#23427;&#21253;&#25324;&#20102;11,435&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;7&#20010;&#19981;&#21516;&#30340;&#23433;&#20840;&#38382;&#39064;&#31867;&#21035;&#65292;&#24182;&#19988;&#36824;&#25552;&#20379;&#20013;&#33521;&#25991;&#25968;&#25454;&#12290;&#36890;&#36807;&#23545;25&#20010;&#28909;&#38376;&#20013;&#33521;&#25991;LLM&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-4&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20294;&#24403;&#21069;LLM&#30340;&#23433;&#20840;&#24615;&#20173;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.07045</link><description>&lt;p&gt;
SafetyBench: &#29992;&#22810;&#39033;&#36873;&#25321;&#39064;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions. (arXiv:2309.07045v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07045
&lt;/p&gt;
&lt;p&gt;
SafetyBench&#26159;&#19968;&#20010;&#20840;&#38754;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;&#23427;&#21253;&#25324;&#20102;11,435&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;7&#20010;&#19981;&#21516;&#30340;&#23433;&#20840;&#38382;&#39064;&#31867;&#21035;&#65292;&#24182;&#19988;&#36824;&#25552;&#20379;&#20013;&#33521;&#25991;&#25968;&#25454;&#12290;&#36890;&#36807;&#23545;25&#20010;&#28909;&#38376;&#20013;&#33521;&#25991;LLM&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-4&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20294;&#24403;&#21069;LLM&#30340;&#23433;&#20840;&#24615;&#20173;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#23427;&#20204;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;LLM&#30340;&#23433;&#20840;&#24615;&#24050;&#25104;&#20026;&#20419;&#36827;&#20854;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#23433;&#20840;&#35780;&#20272;&#22522;&#20934;&#26126;&#26174;&#38459;&#30861;&#20102;&#23545;LLM&#23433;&#20840;&#24615;&#30340;&#26377;&#25928;&#35780;&#20272;&#21644;&#25552;&#21319;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SafetyBench&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#21253;&#25324;11,435&#20010;&#19981;&#21516;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;7&#20010;&#19981;&#21516;&#30340;&#23433;&#20840;&#38382;&#39064;&#31867;&#21035;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;SafetyBench&#36824;&#21253;&#25324;&#20013;&#33521;&#25991;&#25968;&#25454;&#65292;&#26041;&#20415;&#20004;&#31181;&#35821;&#35328;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;25&#20010;&#28909;&#38376;&#20013;&#33521;&#25991;LLM&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#65292;&#21253;&#25324;&#38646;-shot&#21644;&#23569;-shot&#35774;&#32622;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#19988;&#24403;&#21069;LLM&#30340;&#23433;&#20840;&#24615;&#36824;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of Large Language Models (LLMs), increasing attention has been paid to their safety concerns. Consequently, evaluating the safety of LLMs has become an essential task for facilitating the broad applications of LLMs. Nevertheless, the absence of comprehensive safety evaluation benchmarks poses a significant impediment to effectively assess and enhance the safety of LLMs. In this work, we present SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data, facilitating the evaluation in both languages. Our extensive tests over 25 popular Chinese and English LLMs in both zero-shot and few-shot settings reveal a substantial performance advantage for GPT-4 over its counterparts, and there is still significant room for improving the safety of current LLMs. We believe Saf
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#65292;&#21457;&#29616;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#25216;&#26415;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#26377;&#25928;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07034</link><description>&lt;p&gt;
&#22914;&#20309;&#65288;&#19981;&#65289;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#20351;&#29992;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
How (Not) to Use Sociodemographic Information for Subjective NLP Tasks. (arXiv:2309.07034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#65292;&#21457;&#29616;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#25216;&#26415;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#26377;&#25928;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#37322;&#32773;&#30340;&#31038;&#20250;&#20154;&#21475;&#32972;&#26223;&#65288;&#21363;&#24615;&#21035;&#65292;&#24180;&#40836;&#65292;&#25945;&#32946;&#32972;&#26223;&#31561;&#20010;&#20307;&#32452;&#25104;&#65289;&#23545;&#20854;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#27604;&#22914;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12290;&#36890;&#24120;&#65292;&#24322;&#36136;&#30340;&#32972;&#26223;&#20250;&#23548;&#33268;&#39640;&#24230;&#20998;&#27495;&#12290;&#20026;&#20102;&#24314;&#27169;&#36825;&#31181;&#24046;&#24322;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#31181;&#25216;&#26415;&#23558;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#20855;&#26377;&#29305;&#23450;&#31038;&#20250;&#20154;&#21475;&#29305;&#24449;&#30340;&#20154;&#31867;&#21487;&#33021;&#32473;&#20986;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NLP&#25991;&#29486;&#23545;&#36825;&#31181;&#25216;&#26415;&#30340;&#25928;&#26524;&#23384;&#22312;&#20998;&#27495; - &#23427;&#20173;&#28982;&#19981;&#28165;&#26970;&#23427;&#33021;&#22312;&#21738;&#20123;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#26377;&#24110;&#21161;&#65292;&#24182;&#19988;&#35780;&#20272;&#20165;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#21644;&#26368;&#20840;&#38754;&#30340;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#30740;&#31350;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19971;&#20010;&#25968;&#25454;&#38598;&#21644;&#20845;&#20010;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#30340;&#27169;&#22411;&#23478;&#26063;&#20013;&#30340;&#20960;&#20010;&#25552;&#31034;&#24418;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#23545;&#26576;&#20123;&#20219;&#21153;&#26377;&#25928;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotators' sociodemographic backgrounds (i.e., the individual compositions of their gender, age, educational background, etc.) have a strong impact on their decisions when working on subjective NLP tasks, such as hate speech detection. Often, heterogeneous backgrounds result in high disagreements. To model this variation, recent work has explored sociodemographic prompting, a technique, which steers the output of prompt-based models towards answers that humans with specific sociodemographic profiles would give. However, the available NLP literature disagrees on the efficacy of this technique -- it remains unclear, for which tasks and scenarios it can help and evaluations are limited to specific tasks only. We address this research gap by presenting the largest and most comprehensive study of sociodemographic prompting today. Concretely, we evaluate several prompt formulations across seven datasets and six instruction-tuned model families. We find that (1) while sociodemographic prompt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#20197;&#25913;&#36827;&#25991;&#26412;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.07020</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36229;&#36234;&#21407;&#22987;&#30740;&#31350;&#25991;&#31456;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Beyond original Research Articles Categorization via NLP. (arXiv:2309.07020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#20197;&#25913;&#36827;&#25991;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#31185;&#23398;&#25991;&#29486;&#20013;&#26410;&#30693;&#31867;&#21035;&#30340;&#20998;&#31867;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#12290;&#35813;&#30740;&#31350;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;SciBERT&#65292;&#20174;ArXiv&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#25688;&#35201;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#25991;&#26412;&#20998;&#31867;&#20351;&#29992;K-Means&#31639;&#27861;&#23436;&#25104;&#65292;&#24182;&#26681;&#25454;&#36718;&#24275;&#31995;&#25968;&#35780;&#20998;&#30830;&#23450;&#26368;&#20339;&#32858;&#31867;&#25968;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;arXiv&#26631;&#31614;&#31995;&#32479;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#20027;&#39064;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#25991;&#26412;&#20998;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#31185;&#23398;&#30740;&#31350;&#25991;&#29486;&#30340;&#24555;&#36895;&#22686;&#38271;&#29615;&#22659;&#20013;&#65292;&#26377;&#26395;&#25552;&#20379;&#26356;&#22909;&#30340;&#23548;&#33322;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a novel approach to text categorization -- for unknown categories -- in the context of scientific literature, using Natural Language Processing techniques. The study leverages the power of pre-trained language models, specifically SciBERT, to extract meaningful representations of abstracts from the ArXiv dataset. Text categorization is performed using the K-Means algorithm, and the optimal number of clusters is determined based on the Silhouette score. The results demonstrate that the proposed approach captures subject information more effectively than the traditional arXiv labeling system, leading to improved text categorization. The approach offers potential for better navigation and recommendation systems in the rapidly growing landscape of scientific research literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#31616;&#21382;&#35299;&#26512;&#38382;&#39064;&#20316;&#20026;&#20998;&#23618;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#21516;&#26102;&#35299;&#20915;&#34892;&#21644;&#26631;&#35760;&#20004;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#30340;&#39640;&#36136;&#37327;&#31616;&#21382;&#35299;&#26512;&#35821;&#26009;&#24211;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#27169;&#22411;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#20248;&#20110;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#25928;&#29575;&#65292;&#24182;&#25551;&#36848;&#20102;&#27169;&#22411;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2309.07015</link><description>&lt;p&gt;
&#31616;&#21382;&#35299;&#26512;&#20316;&#20026;&#20998;&#23618;&#24207;&#21015;&#26631;&#27880;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
R\'esum\'e Parsing as Hierarchical Sequence Labeling: An Empirical Study. (arXiv:2309.07015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#31616;&#21382;&#35299;&#26512;&#38382;&#39064;&#20316;&#20026;&#20998;&#23618;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#21516;&#26102;&#35299;&#20915;&#34892;&#21644;&#26631;&#35760;&#20004;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#30340;&#39640;&#36136;&#37327;&#31616;&#21382;&#35299;&#26512;&#35821;&#26009;&#24211;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#27169;&#22411;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#20248;&#20110;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#25928;&#29575;&#65292;&#24182;&#25551;&#36848;&#20102;&#27169;&#22411;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31616;&#21382;&#20013;&#25552;&#21462;&#20449;&#24687;&#36890;&#24120;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#38382;&#39064;&#65292;&#21363;&#39318;&#20808;&#23558;&#25991;&#26723;&#20998;&#27573;&#65292;&#28982;&#21518;&#23545;&#27599;&#20010;&#27573;&#33853;&#36827;&#34892;&#21333;&#29420;&#22788;&#29702;&#20197;&#25552;&#21462;&#30446;&#26631;&#23454;&#20307;&#12290;&#25105;&#20204;&#23558;&#25972;&#20010;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#32423;&#21035;&#30340;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#65292;&#21363;&#34892;&#21644;&#26631;&#35760;&#65292;&#24182;&#30740;&#31350;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#33521;&#35821;&#12289;&#27861;&#35821;&#12289;&#20013;&#25991;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#24503;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#29790;&#20856;&#35821;&#30340;&#39640;&#36136;&#37327;&#31616;&#21382;&#35299;&#26512;&#35821;&#26009;&#24211;&#12290;&#22522;&#20110;&#36825;&#20123;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#36164;&#28304;&#25928;&#29575;&#65292;&#24182;&#25551;&#36848;&#20102;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#36827;&#34892;&#27169;&#22411;&#37096;&#32626;&#26102;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting information from r\'esum\'es is typically formulated as a two-stage problem, where the document is first segmented into sections and then each section is processed individually to extract the target entities. Instead, we cast the whole problem as sequence labeling in two levels -- lines and tokens -- and study model architectures for solving both tasks simultaneously. We build high-quality r\'esum\'e parsing corpora in English, French, Chinese, Spanish, German, Portuguese, and Swedish. Based on these corpora, we present experimental results that demonstrate the effectiveness of the proposed models for the information extraction task, outperforming approaches introduced in previous work. We conduct an ablation study of the proposed architectures. We also analyze both model performance and resource efficiency, and describe the trade-offs for model deployment in the context of a production environment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24076;&#33098;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#20195;NLP&#27979;&#35797;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#21547;&#22235;&#20010;&#19987;&#23478;&#39564;&#35777;&#30340;&#35780;&#20272;&#20219;&#21153;&#65292;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#35789;&#20041;&#28040;&#27495;&#21644;&#38544;&#21947;&#26816;&#27979;&#12290;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#25512;&#29702;&#25968;&#25454;&#38598;&#39318;&#27425;&#26631;&#35760;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#25512;&#29702;&#26631;&#31614;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#19968;&#31181;&#23545;&#20110;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#33719;&#21462;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#25928;&#30410;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07009</link><description>&lt;p&gt;
OYXOY:&#36866;&#29992;&#20110;&#29616;&#20195;&#24076;&#33098;&#35821;&#30340;&#29616;&#20195;NLP&#27979;&#35797;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
OYXOY: A Modern NLP Test Suite for Modern Greek. (arXiv:2309.07009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24076;&#33098;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#20195;NLP&#27979;&#35797;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#21547;&#22235;&#20010;&#19987;&#23478;&#39564;&#35777;&#30340;&#35780;&#20272;&#20219;&#21153;&#65292;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#35789;&#20041;&#28040;&#27495;&#21644;&#38544;&#21947;&#26816;&#27979;&#12290;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#25512;&#29702;&#25968;&#25454;&#38598;&#39318;&#27425;&#26631;&#35760;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#25512;&#29702;&#26631;&#31614;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#19968;&#31181;&#23545;&#20110;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#33719;&#21462;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#25928;&#30410;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#20026;&#24076;&#33098;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24320;&#23637;&#30340;&#19968;&#39033;&#22522;&#30784;&#24615;&#24037;&#20316;&#65292;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#23398;&#21644;&#25216;&#26415;&#30456;&#20851;&#24615;&#30340;&#35780;&#20272;&#22871;&#20214;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22235;&#20010;&#32463;&#36807;&#19987;&#23478;&#39564;&#35777;&#30340;&#35780;&#20272;&#20219;&#21153;&#26469;&#24320;&#23637;&#36825;&#39033;&#24037;&#20316;&#65292;&#36825;&#20123;&#20219;&#21153;&#19987;&#38376;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#35789;&#20041;&#28040;&#27495;&#65288;&#36890;&#36807;&#31034;&#20363;&#27604;&#36739;&#25110;&#36873;&#25321;&#24847;&#20041;&#65289;&#21644;&#38544;&#21947;&#26816;&#27979;&#12290;&#19982;&#29616;&#26377;&#20219;&#21153;&#30340;&#35821;&#35328;&#36866;&#24212;&#21103;&#26412;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;&#20004;&#20010;&#21019;&#26032;&#28857;&#65292;&#36825;&#20123;&#23558;&#19982;&#26356;&#24191;&#27867;&#30340;&#36164;&#28304;&#21644;&#35780;&#20272;&#31038;&#21306;&#20135;&#29983;&#20849;&#40483;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#29702;&#25968;&#25454;&#38598;&#26159;&#39318;&#20010;&#26631;&#35760;&#20102;&#19981;&#20165;&#20165;&#26159;\texttt{&#19968;}&#31181;&#65292;&#32780;&#26159;\texttt{&#25152;&#26377;}&#21487;&#33021;&#25512;&#29702;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#21040;&#30001;&#20110;&#27495;&#20041;&#24615;&#25110;&#22810;&#20041;&#24615;&#21487;&#33021;&#23548;&#33268;&#30340;&#21487;&#33021;&#36716;&#21464;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#23545;&#20110;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#33719;&#21462;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#25928;&#30410;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20316;&#20026;&#35821;&#35328;&#20013;&#31435;&#35299;&#26512;&#22120;&#65292;&#25105;&#20204;&#23558;&#24076;&#33098;&#29616;&#20195;&#26631;&#20934;&#35789;&#20856;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#26684;&#24335;&#65292;&#20174;&#20013;&#34893;&#29983;&#20986;&#20854;&#20182;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper serves as a foundational step towards the development of a linguistically motivated and technically relevant evaluation suite for Greek NLP. We initiate this endeavor by introducing four expert-verified evaluation tasks, specifically targeted at natural language inference, word sense disambiguation (through example comparison or sense selection) and metaphor detection. More than language-adapted replicas of existing tasks, we contribute two innovations which will resonate with the broader resource and evaluation community. Firstly, our inference dataset is the first of its kind, marking not just \textit{one}, but rather \textit{all} possible inference labels, accounting for possible shifts due to e.g. ambiguity or polysemy. Secondly, we demonstrate a cost-efficient method to obtain datasets for under-resourced languages. Using ChatGPT as a language-neutral parser, we transform the Dictionary of Standard Modern Greek into a structured format, from which we derive the other th
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#19968;&#33268;&#25490;&#24207;&#19982;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21463;&#36923;&#36753;&#32422;&#26463;&#24341;&#23548;&#30340;&#25506;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#22810;&#20010;&#35821;&#21477;&#20013;&#22987;&#32456;&#26144;&#23556;&#21040;&#23545;&#27604;&#30340;&#30495;-&#20551;&#26497;&#28857;&#30340;&#25490;&#24207;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.06991</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#19968;&#33268;&#25490;&#24207;&#19982;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Contrast-Consistent Ranking with Language Models. (arXiv:2309.06991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06991
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#19968;&#33268;&#25490;&#24207;&#19982;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21463;&#36923;&#36753;&#32422;&#26463;&#24341;&#23548;&#30340;&#25506;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#22810;&#20010;&#35821;&#21477;&#20013;&#22987;&#32456;&#26144;&#23556;&#21040;&#23545;&#27604;&#30340;&#30495;-&#20551;&#26497;&#28857;&#30340;&#25490;&#24207;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#22522;&#20110;&#25490;&#24207;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#26159;&#22788;&#29702;&#19978;&#19979;&#25991;&#25490;&#21517;&#20219;&#21153;&#30340;&#24378;&#22823;&#35299;&#20915;&#32773;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#37197;&#23545;&#12289;&#28857;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#25216;&#26415;&#65292;&#20197;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#30340;&#25490;&#24207;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#20180;&#32454;&#26657;&#20934;&#21644;&#38480;&#21046;&#35299;&#30721;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#22312;&#20135;&#29983;&#30340;&#25490;&#24207;&#20013;&#20063;&#19981;&#24635;&#26159;&#33258;&#27965;&#30340;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#25506;&#32034;&#19968;&#31181;&#21463;&#26080;&#30417;&#30563;&#25506;&#27979;&#26041;&#27861;Contrast-Consistent Search&#65288;CCS&#65289;&#21551;&#21457;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36825;&#20010;&#24819;&#27861;&#26159;&#35757;&#32451;&#19968;&#20010;&#21463;&#36923;&#36753;&#32422;&#26463;&#24341;&#23548;&#30340;&#25506;&#27979;&#27169;&#22411;&#65306;&#27169;&#22411;&#23545;&#19968;&#20010;&#35821;&#21477;&#21450;&#20854;&#21542;&#23450;&#30340;&#34920;&#31034;&#24517;&#39035;&#22312;&#22810;&#20010;&#35821;&#21477;&#20013;&#22987;&#32456;&#26144;&#23556;&#21040;&#23545;&#27604;&#30340;&#30495;-&#20551;&#26497;&#28857;&#12290;&#25105;&#20204;&#20551;&#35774;&#31867;&#20284;&#30340;&#32422;&#26463;&#36866;&#29992;&#20110;&#25152;&#26377;&#39033;&#36890;&#36807;&#19968;&#33268;&#24615;&#23545;&#30456;&#20851;&#25490;&#24207;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models contain ranking-based knowledge and are powerful solvers of in-context ranking tasks. For instance, they may have parametric knowledge about the ordering of countries by size or may be able to rank reviews by sentiment. Recent work focuses on pairwise, pointwise, and listwise prompting techniques to elicit a language model's ranking knowledge. However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce. This motivates us to explore an alternative approach that is inspired by an unsupervised probing method called Contrast-Consistent Search (CCS). The idea is to train a probing model guided by a logical constraint: a model's representation of a statement and its negation must be mapped to contrastive true-false poles consistently across multiple statements. We hypothesize that similar constraints apply to ranking tasks where all items are related via consistent pair
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#29255;&#25551;&#36848;&#36828;&#31243;&#25512;&#26029;ALS&#24739;&#32773;&#30340;&#35748;&#30693;&#24471;&#20998;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#25968;&#23383;&#29256;&#26412;&#30340;ECAS&#27979;&#35797;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#26377;&#25928;&#30417;&#27979;ALS&#24739;&#32773;&#30340;&#35748;&#30693;&#38556;&#30861;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#36828;&#31243;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.06989</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#29255;&#25551;&#36848;&#36828;&#31243;&#25512;&#26029;ALS&#24739;&#32773;&#30340;&#35748;&#30693;&#24471;&#20998;
&lt;/p&gt;
&lt;p&gt;
Remote Inference of Cognitive Scores in ALS Patients Using a Picture Description. (arXiv:2309.06989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#29255;&#25551;&#36848;&#36828;&#31243;&#25512;&#26029;ALS&#24739;&#32773;&#30340;&#35748;&#30693;&#24471;&#20998;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#25968;&#23383;&#29256;&#26412;&#30340;ECAS&#27979;&#35797;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#26377;&#25928;&#30417;&#27979;ALS&#24739;&#32773;&#30340;&#35748;&#30693;&#38556;&#30861;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#36828;&#31243;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32908;&#33806;&#32553;&#24615;&#20391;&#32034;&#30828;&#21270;&#30151;&#26159;&#19968;&#31181;&#33268;&#21629;&#30340;&#30142;&#30149;&#65292;&#19981;&#20165;&#24433;&#21709;&#36816;&#21160;&#12289;&#35828;&#35805;&#21644;&#21628;&#21560;&#65292;&#36824;&#24433;&#21709;&#35748;&#30693;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;&#35821;&#35328;&#20998;&#26512;&#25216;&#26415;&#26816;&#27979;ALS&#24182;&#25512;&#26029;&#29992;&#20110;&#30417;&#27979;&#21151;&#33021;&#36827;&#23637;&#30340;&#37327;&#34920;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#21478;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#35748;&#30693;&#38556;&#30861;&#65292;&#23427;&#24433;&#21709;35-50%&#30340;ALS&#24739;&#32773;&#32676;&#20307;&#12290;&#20026;&#20102;&#25509;&#35302;&#24120;&#24120;&#23384;&#22312;&#34892;&#21160;&#38480;&#21046;&#30340;ALS&#24739;&#32773;&#32676;&#20307;&#65292;&#25105;&#20204;&#39318;&#27425;&#23454;&#29616;&#20102;&#29233;&#19969;&#22561;&#35748;&#30693;&#19982;&#34892;&#20026;ALS&#31579;&#26597;(ECAS)&#30340;&#25968;&#23383;&#29256;&#26412;&#12290;&#36825;&#20010;&#29992;&#20110;&#27979;&#37327;&#35748;&#30693;&#38556;&#30861;&#30340;&#27979;&#35797;&#34987;56&#21517;&#26469;&#33258;EverythingALS Speech Study&#30340;&#21442;&#19982;&#32773;&#36828;&#31243;&#25191;&#34892;&#12290;&#20316;&#20026;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#21442;&#19982;&#32773;&#65288;ALS&#21644;&#38750;ALS&#65289;&#34987;&#35201;&#27714;&#22312;&#23478;&#20013;&#30005;&#33041;&#19978;&#26174;&#31034;&#30340;&#22797;&#26434;&#22330;&#26223;&#30340;&#20247;&#22810;&#22270;&#29255;&#20013;&#27599;&#21608;&#25551;&#36848;&#19968;&#24352;&#22270;&#29255;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;ECAS&#27979;&#35797;&#34987;&#36827;&#34892;&#30340;60&#22825;&#20869;&#65288;&#21069;&#21518;&#65289;&#36827;&#34892;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amyotrophic lateral sclerosis is a fatal disease that not only affects movement, speech, and breath but also cognition. Recent studies have focused on the use of language analysis techniques to detect ALS and infer scales for monitoring functional progression. In this paper, we focused on another important aspect, cognitive impairment, which affects 35-50% of the ALS population. In an effort to reach the ALS population, which frequently exhibits mobility limitations, we implemented the digital version of the Edinburgh Cognitive and Behavioral ALS Screen (ECAS) test for the first time. This test which is designed to measure cognitive impairment was remotely performed by 56 participants from the EverythingALS Speech Study. As part of the study, participants (ALS and non-ALS) were asked to describe weekly one picture from a pool of many pictures with complex scenes displayed on their computer at home. We analyze the descriptions performed within +/- 60 days from the day the ECAS test was 
&lt;/p&gt;</description></item><item><title>&#33258;&#22238;&#24402;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#22270;&#28789;&#26426;&#35745;&#31639;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#31639;&#26415;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06979</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#26159;&#36890;&#29992;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auto-Regressive Next-Token Predictors are Universal Learners. (arXiv:2309.06979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06979
&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#22270;&#28789;&#26426;&#35745;&#31639;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#31639;&#26415;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#22312;&#36923;&#36753;&#21644;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#38750;&#20961;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#35757;&#32451;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#31616;&#21333;&#20219;&#21153;&#19978;&#30340;&#32593;&#32476;&#20013;&#20986;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#33258;&#22238;&#24402;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#22914;&#32447;&#24615;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#24403;&#20854;&#22312;&#24605;&#32500;&#38142;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#22270;&#28789;&#26426;&#35745;&#31639;&#30340;&#20219;&#20309;&#20989;&#25968;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#8212;&#8212;&#38271;&#24230;&#22797;&#26434;&#24230;&#65292;&#23427;&#34913;&#37327;&#20102;&#22312;&#36817;&#20284;&#26576;&#20010;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#24605;&#32500;&#38142;&#24207;&#21015;&#20013;&#25152;&#38656;&#30340;&#20013;&#38388;&#26631;&#35760;&#30340;&#25968;&#37327;&#65292;&#24182;&#20998;&#26512;&#20102;&#38271;&#24230;&#22797;&#26434;&#24230;&#21644;&#20854;&#20182;&#22797;&#26434;&#24615;&#27010;&#24565;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#31616;&#21333;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#22914;&#32447;&#24615;&#32593;&#32476;&#21644;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#65292;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#31639;&#26415;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks. Interestingly, these abilities emerge in networks trained on the simple task of next-token prediction. In this work, we present a theoretical framework for studying auto-regressive next-token predictors. We demonstrate that even simple models such as linear next-token predictors, trained on Chain-of-Thought (CoT) data, can approximate any function efficiently computed by a Turing machine. We introduce a new complexity measure -- length complexity -- which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and analyze the interplay between length complexity and other notions of complexity. Finally, we show experimentally that simple next-token predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs), display non-trivial performance on text generation and arithmetic tasks. Our resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22240;&#26524;&#35299;&#32544;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#35299;&#23545;&#35805;&#20869;&#23481;&#21644;&#30740;&#31350;&#24773;&#24863;&#30340;&#26102;&#38388;&#31215;&#32047;&#65292;&#23454;&#29616;&#20102;&#26356;&#31934;&#30830;&#30340;&#24773;&#24863;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.06928</link><description>&lt;p&gt;
&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#26816;&#27979;&#30340;&#21160;&#24577;&#22240;&#26524;&#35299;&#32544;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dynamic Causal Disentanglement Model for Dialogue Emotion Detection. (arXiv:2309.06928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22240;&#26524;&#35299;&#32544;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#35299;&#23545;&#35805;&#20869;&#23481;&#21644;&#30740;&#31350;&#24773;&#24863;&#30340;&#26102;&#38388;&#31215;&#32047;&#65292;&#23454;&#29616;&#20102;&#26356;&#31934;&#30830;&#30340;&#24773;&#24863;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#26816;&#27979;&#26159;&#19968;&#39033;&#22312;&#19981;&#21516;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#23613;&#31649;&#32467;&#21512;&#24120;&#35782;&#30693;&#35782;&#23545;&#29616;&#26377;&#24773;&#24863;&#26816;&#27979;&#26041;&#27861;&#30340;&#25928;&#26524;&#26377;&#30410;&#65292;&#20294;&#22522;&#20110;&#23545;&#35805;&#30340;&#24773;&#24863;&#26816;&#27979;&#38754;&#20020;&#30528;&#35768;&#22810;&#22256;&#38590;&#21644;&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;&#20154;&#31867;&#34892;&#20026;&#21644;&#23545;&#35805;&#20869;&#23481;&#30340;&#21464;&#24322;&#24615;&#12290;&#22312;&#23545;&#35805;&#20013;&#65292;&#20154;&#31867;&#24773;&#24863;&#24448;&#24448;&#20197;&#31361;&#21457;&#26041;&#24335;&#31215;&#32047;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26159;&#38544;&#21547;&#34920;&#36798;&#30340;&#12290;&#36825;&#24847;&#21619;&#30528;&#35768;&#22810;&#30495;&#23454;&#24773;&#24863;&#22312;&#22823;&#37327;&#19981;&#30456;&#20851;&#30340;&#35789;&#27719;&#21644;&#23545;&#35805;&#20013;&#34987;&#38544;&#34255;&#36215;&#26469;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#34255;&#21464;&#37327;&#20998;&#31163;&#30340;&#21160;&#24577;&#22240;&#26524;&#35299;&#32544;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#26377;&#25928;&#20998;&#35299;&#23545;&#35805;&#20869;&#23481;&#24182;&#30740;&#31350;&#24773;&#24863;&#30340;&#26102;&#38388;&#31215;&#32047;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#24773;&#24863;&#35782;&#21035;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#23450;&#21521;&#26080;&#29615;&#22270; (DAG) &#26469;&#24314;&#31435;&#38544;&#34255;&#24773;&#24863;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion detection is a critical technology extensively employed in diverse fields. While the incorporation of commonsense knowledge has proven beneficial for existing emotion detection methods, dialogue-based emotion detection encounters numerous difficulties and challenges due to human agency and the variability of dialogue content.In dialogues, human emotions tend to accumulate in bursts. However, they are often implicitly expressed. This implies that many genuine emotions remain concealed within a plethora of unrelated words and dialogues.In this paper, we propose a Dynamic Causal Disentanglement Model based on hidden variable separation, which is founded on the separation of hidden variables. This model effectively decomposes the content of dialogues and investigates the temporal accumulation of emotions, thereby enabling more precise emotion recognition. First, we introduce a novel Causal Directed Acyclic Graph (DAG) to establish the correlation between hidden emotional informatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;Big Bird&#23884;&#20837;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#22312;Reddit-L2&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#35821;&#35328;&#29305;&#24449;&#24037;&#31243;&#27169;&#22411;&#12290;&#27492;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20026;&#26410;&#26469;&#30340;&#27597;&#35821;&#35782;&#21035;&#24037;&#20316;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.06923</link><description>&lt;p&gt;
&#22522;&#20110;Big Bird&#23884;&#20837;&#30340;&#27597;&#35821;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Native Language Identification with Big Bird Embeddings. (arXiv:2309.06923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;Big Bird&#23884;&#20837;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#22312;Reddit-L2&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#35821;&#35328;&#29305;&#24449;&#24037;&#31243;&#27169;&#22411;&#12290;&#27492;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20026;&#26410;&#26469;&#30340;&#27597;&#35821;&#35782;&#21035;&#24037;&#20316;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27597;&#35821;&#35782;&#21035;&#26088;&#22312;&#26681;&#25454;&#20316;&#32773;&#29992;&#21478;&#19968;&#31181;&#35821;&#35328;&#32534;&#20889;&#30340;&#20869;&#23481;&#26469;&#20998;&#31867;&#20854;&#27597;&#35821;&#12290;&#22312;&#21382;&#21490;&#19978;&#65292;&#35813;&#20219;&#21153;&#20005;&#37325;&#20381;&#36182;&#32791;&#26102;&#30340;&#35821;&#35328;&#29305;&#24449;&#24037;&#31243;&#65292;&#32780;&#22522;&#20110;Transformer&#30340;&#27597;&#35821;&#35782;&#21035;&#27169;&#22411;&#36804;&#20170;&#20026;&#27490;&#26410;&#33021;&#25552;&#20379;&#26377;&#25928;&#12289;&#23454;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#36755;&#20837;&#22823;&#23567;&#26159;&#21542;&#26159;&#19968;&#20010;&#38480;&#21046;&#22240;&#32032;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;Big Bird&#23884;&#20837;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#22312;Reddit-L2&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#35821;&#35328;&#29305;&#24449;&#24037;&#31243;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#36755;&#20837;&#38271;&#24230;&#20381;&#36182;&#24615;&#30340;&#36827;&#19968;&#27493;&#27934;&#23519;&#65292;&#23637;&#31034;&#20102;&#19968;&#33268;&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#65292;&#24182;&#23545;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#20102;&#23450;&#24615;&#20998;&#26512;&#12290;&#32771;&#34385;&#21040;&#36825;&#31181;&#26041;&#27861;&#30340;&#25928;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#35748;&#20026;&#23427;&#20026;&#26410;&#26469;&#30340;&#27597;&#35821;&#35782;&#21035;&#24037;&#20316;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Native Language Identification (NLI) intends to classify an author's native language based on their writing in another language. Historically, the task has heavily relied on time-consuming linguistic feature engineering, and transformer-based NLI models have thus far failed to offer effective, practical alternatives. The current work investigates if input size is a limiting factor, and shows that classifiers trained using Big Bird embeddings outperform linguistic feature engineering models by a large margin on the Reddit-L2 dataset. Additionally, we provide further insight into input length dependencies, show consistent out-of-sample performance, and qualitatively analyze the embedding space. Given the effectiveness and computational efficiency of this method, we believe it offers a promising avenue for future NLI work.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#29983;&#25104;&#30340;&#22238;&#39038;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#20266;&#26679;&#26412;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.06917</link><description>&lt;p&gt;
&#20351;&#29992;&#29380;&#21033;&#20811;&#38647;&#29983;&#25104;&#22522;&#30784;&#30340;&#22238;&#39038;&#30340;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning with Dirichlet Generative-based Rehearsal. (arXiv:2309.06917v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06917
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#29983;&#25104;&#30340;&#22238;&#39038;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#20266;&#26679;&#26412;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#25968;&#25454;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#65288;ToDs&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#30001;&#20110;&#35745;&#31639;&#32422;&#26463;&#21644;&#32791;&#26102;&#38382;&#39064;&#32780;&#22256;&#25200;&#30528;&#22686;&#37327;&#23398;&#20064;&#12290;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#35797;&#22270;&#36890;&#36807;&#36991;&#20813;&#23494;&#38598;&#30340;&#39044;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#38754;&#20020;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#22522;&#20110;&#29983;&#25104;&#30340;&#22238;&#39038;CL&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#33021;&#20934;&#30830;&#21453;&#26144;&#24213;&#23618;&#20219;&#21153;&#29305;&#23450;&#20998;&#24067;&#30340;&#20266;&#26679;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29380;&#21033;&#20811;&#38647;&#36830;&#32493;&#23398;&#20064;&#65288;DCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29983;&#25104;&#30340;&#22238;&#39038;&#31574;&#30053;&#29992;&#20110;CL&#12290;&#19982;&#20256;&#32479;&#19978;&#22312;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#20013;&#20351;&#29992;&#30340;&#39640;&#26031;&#28508;&#21464;&#37327;&#19981;&#21516;&#65292;DCL&#21033;&#29992;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#26679;&#24615;&#26469;&#24314;&#27169;&#28508;&#21464;&#37327;&#20808;&#39564;&#12290;&#36825;&#20351;&#24471;&#23427;&#33021;&#22815;&#39640;&#25928;&#22320;&#25429;&#25417;&#20808;&#21069;&#20219;&#21153;&#30340;&#21477;&#32423;&#29305;&#24449;&#65292;&#24182;&#26377;&#25928;&#22320;&#25351;&#23548;&#20266;&#26679;&#26412;&#30340;&#29983;&#25104;&#12290;&#27492;&#22806;&#36824;&#24341;&#20837;&#20102;Jensen-&#33487;&#24443;&#21033;&#25955;&#24230;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;DCL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in data-driven task-oriented dialogue systems (ToDs) struggle with incremental learning due to computational constraints and time-consuming issues. Continual Learning (CL) attempts to solve this by avoiding intensive pre-training, but it faces the problem of catastrophic forgetting (CF). While generative-based rehearsal CL methods have made significant strides, generating pseudo samples that accurately reflect the underlying task-specific distribution is still a challenge. In this paper, we present Dirichlet Continual Learning (DCL), a novel generative-based rehearsal strategy for CL. Unlike the traditionally used Gaussian latent variable in the Conditional Variational Autoencoder (CVAE), DCL leverages the flexibility and versatility of the Dirichlet distribution to model the latent prior variable. This enables it to efficiently capture sentence-level features of previous tasks and effectively guide the generation of pseudo samples. In addition, we introduce Jensen-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TopMost&#30340;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#21644;&#20855;&#26377;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.06908</link><description>&lt;p&gt;
&#36208;&#21521;TopMost&#65306;&#19968;&#20010;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Towards the TopMost: A Topic Modeling System Toolkit. (arXiv:2309.06908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TopMost&#30340;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#21644;&#20855;&#26377;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#24050;&#32463;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#34987;&#25552;&#20986;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#22312;&#31070;&#32463;&#21464;&#20998;&#25512;&#26029;&#30340;&#25512;&#21160;&#19979;&#36817;&#26399;&#24471;&#21040;&#20102;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20027;&#39064;&#27169;&#22411;&#37319;&#29992;&#23436;&#20840;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#35774;&#32622;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24555;&#36895;&#21033;&#29992;&#21644;&#20844;&#24179;&#27604;&#36739;&#12290;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65288;TopMost&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#24037;&#20855;&#21253;&#30456;&#27604;&#65292;TopMost&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#30340;&#23436;&#25972;&#29983;&#21629;&#21608;&#26399;&#65292;&#33073;&#39062;&#32780;&#20986;&#12290;TopMost&#30340;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#21487;&#20197;&#24555;&#36895;&#21033;&#29992;&#65292;&#20844;&#24179;&#27604;&#36739;&#65292;&#24182;&#28789;&#27963;&#25193;&#23637;&#19981;&#21516;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#36825;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#25945;&#31243;&#21644;&#25991;&#26723;&#21487;&#22312;https://github.com/bobxwu/topmost &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models have been proposed for decades with various applications and recently refreshed by the neural variational inference. However, these topic models adopt totally distinct dataset, implementation, and evaluation settings, which hinders their quick utilization and fair comparisons. This greatly hinders the research progress of topic models. To address these issues, in this paper we propose a Topic Modeling System Toolkit (TopMost). Compared to existing toolkits, TopMost stands out by covering a wider range of topic modeling scenarios including complete lifecycles with dataset pre-processing, model training, testing, and evaluations. The highly cohesive and decoupled modular design of TopMost enables quick utilization, fair comparisons, and flexible extensions of different topic models. This can facilitate the research and applications of topic models. Our code, tutorials, and documentation are available at https://github.com/bobxwu/topmost.
&lt;/p&gt;</description></item><item><title>Gpachov&#22242;&#38431;&#22312;CLEF-2023 CheckThat&#65281;&#23454;&#39564;&#23460;&#20219;&#21153;2&#20013;&#26500;&#24314;&#20102;&#19968;&#31181;&#22810;&#26679;&#30340;&#22810;&#26041;&#27861;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24494;&#35843;&#21477;&#23376;&#23884;&#20837;&#32534;&#30721;&#27169;&#22411;&#12289;&#26679;&#26412;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#36716;&#25442;&#22120;&#31561;&#26041;&#27861;&#32467;&#21512;&#24471;&#21040;&#20102;0.77&#30340;&#23439;F1&#20998;&#25968;&#65292;&#24182;&#22312;&#33521;&#35821;&#23376;&#20219;&#21153;&#20013;&#33719;&#24471;&#31532;&#20108;&#21517;&#12290;</title><link>http://arxiv.org/abs/2309.06844</link><description>&lt;p&gt;
Gpachov&#22312;CheckThat&#65281;2023&#20013;&#65306;&#19968;&#31181;&#22810;&#26679;&#30340;&#22810;&#26041;&#27861;&#38598;&#25104;&#29992;&#20110;&#26032;&#38395;&#25991;&#31456;&#20027;&#35266;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Gpachov at CheckThat! 2023: A Diverse Multi-Approach Ensemble for Subjectivity Detection in News Articles. (arXiv:2309.06844v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06844
&lt;/p&gt;
&lt;p&gt;
Gpachov&#22242;&#38431;&#22312;CLEF-2023 CheckThat&#65281;&#23454;&#39564;&#23460;&#20219;&#21153;2&#20013;&#26500;&#24314;&#20102;&#19968;&#31181;&#22810;&#26679;&#30340;&#22810;&#26041;&#27861;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24494;&#35843;&#21477;&#23376;&#23884;&#20837;&#32534;&#30721;&#27169;&#22411;&#12289;&#26679;&#26412;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#36716;&#25442;&#22120;&#31561;&#26041;&#27861;&#32467;&#21512;&#24471;&#21040;&#20102;0.77&#30340;&#23439;F1&#20998;&#25968;&#65292;&#24182;&#22312;&#33521;&#35821;&#23376;&#20219;&#21153;&#20013;&#33719;&#24471;&#31532;&#20108;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#30340;&#24191;&#27867;&#20351;&#29992;&#23548;&#33268;&#20102;&#20114;&#32852;&#32593;&#19978;&#30340;&#20027;&#35266;&#12289;&#35823;&#23548;&#29978;&#33267;&#34394;&#20551;&#20449;&#24687;&#30340;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#20027;&#35266;&#24615;&#26816;&#27979;&#22312;&#30830;&#20445;&#20449;&#24687;&#23458;&#35266;&#24615;&#21644;&#36136;&#37327;&#26041;&#38754;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Gpachov&#22242;&#38431;&#38024;&#23545;CLEF-2023 CheckThat&#65281;&#23454;&#39564;&#23460;&#20219;&#21153;2&#30340;&#20027;&#35266;&#24615;&#26816;&#27979;&#26500;&#24314;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25991;&#31456;&#25506;&#32034;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#31532;&#19968;&#20010;&#26041;&#21521;&#22522;&#20110;&#24494;&#35843;&#21477;&#23376;&#23884;&#20837;&#32534;&#30721;&#27169;&#22411;&#21644;&#38477;&#32500;&#12290;&#31532;&#20108;&#20010;&#26041;&#21521;&#25506;&#32034;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#12290;&#31532;&#19977;&#20010;&#26041;&#21521;&#35780;&#20272;&#20102;&#22312;&#32463;&#36807;&#20462;&#25913;&#30340;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#22810;&#35821;&#35328;&#36716;&#25442;&#22120;&#65292;&#20351;&#29992;&#20102;&#22810;&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#12290;&#26368;&#21518;&#65292;&#23558;&#36825;&#19977;&#31181;&#26041;&#27861;&#20197;&#31616;&#21333;&#30340;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#65292;&#32467;&#26524;&#22312;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;0.77&#30340;&#23439;F1&#65292;&#24182;&#22312;&#33521;&#35821;&#23376;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31532;&#20108;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The wide-spread use of social networks has given rise to subjective, misleading, and even false information on the Internet. Thus, subjectivity detection can play an important role in ensuring the objectiveness and the quality of a piece of information. This paper presents the solution built by the Gpachov team for the CLEF-2023 CheckThat! lab Task~2 on subjectivity detection. Three different research directions are explored. The first one is based on fine-tuning a sentence embeddings encoder model and dimensionality reduction. The second one explores a sample-efficient few-shot learning model. The third one evaluates fine-tuning a multilingual transformer on an altered dataset, using data from multiple languages. Finally, the three approaches are combined in a simple majority voting ensemble, resulting in 0.77 macro F1 on the test set and achieving 2nd place on the English subtask.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20998;&#26512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#12290;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#39640;&#25928;&#39044;&#27979;&#30001;&#22810;&#20110;&#20004;&#20010;&#20851;&#31995;&#21644;&#26410;&#25351;&#23450;&#23454;&#20307;&#32452;&#25104;&#30340;&#21477;&#23376;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#22810;&#20010;&#21477;&#23376;&#30340;&#35821;&#22659;&#20013;&#35782;&#21035;&#35821;&#20041;&#20851;&#31995;&#12290;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20108;&#20803;&#20851;&#31995;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#38543;&#30528;&#20851;&#31995;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#39044;&#27979;&#20934;&#30830;&#29575;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.06814</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#25552;&#21462;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Contextual Relation Extraction based on Deep Learning Models. (arXiv:2309.06814v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20998;&#26512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#12290;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#39640;&#25928;&#39044;&#27979;&#30001;&#22810;&#20110;&#20004;&#20010;&#20851;&#31995;&#21644;&#26410;&#25351;&#23450;&#23454;&#20307;&#32452;&#25104;&#30340;&#21477;&#23376;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#22810;&#20010;&#21477;&#23376;&#30340;&#35821;&#22659;&#20013;&#35782;&#21035;&#35821;&#20041;&#20851;&#31995;&#12290;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20108;&#20803;&#20851;&#31995;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#38543;&#30528;&#20851;&#31995;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#39044;&#27979;&#20934;&#30830;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#20851;&#31995;&#25552;&#21462;&#20027;&#35201;&#29992;&#20110;&#20511;&#21161;&#26412;&#20307;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#22312;&#35821;&#20041;&#25628;&#32034;&#12289;&#26597;&#35810;&#22238;&#31572;&#21644;&#25991;&#26412;&#34164;&#28085;&#31561;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#20851;&#31995;&#25552;&#21462;&#35782;&#21035;&#21407;&#22987;&#25991;&#26412;&#20013;&#30340;&#23454;&#20307;&#21450;&#20854;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#29983;&#29289;&#21307;&#33647;&#34892;&#19994;&#20013;&#65292;&#39640;&#25928;&#20934;&#30830;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#25552;&#21462;&#31995;&#32479;&#23545;&#20110;&#21019;&#24314;&#39046;&#22495;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26080;&#27861;&#39640;&#25928;&#22320;&#20174;&#30001;&#22810;&#20110;&#20004;&#20010;&#20851;&#31995;&#21644;&#26410;&#25351;&#23450;&#23454;&#20307;&#32452;&#25104;&#30340;&#21477;&#23376;&#20013;&#39044;&#27979;&#22797;&#26434;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#22810;&#20010;&#21477;&#23376;&#30340;&#35821;&#22659;&#20013;&#35782;&#21035;&#20986;&#36866;&#24403;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#23613;&#31649;&#20851;&#31995;&#25552;&#21462;&#20013;&#20351;&#29992;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#21482;&#23545;&#20108;&#20803;&#20851;&#31995;&#65288;&#21363;&#22312;&#21477;&#23376;&#20013;&#23436;&#20840;&#21457;&#29983;&#22312;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65289;&#25552;&#20379;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#20250;&#38543;&#30528;&#20851;&#31995;&#30340;&#25968;&#37327;&#22686;&#21152;&#32780;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual Relation Extraction (CRE) is mainly used for constructing a knowledge graph with a help of ontology. It performs various tasks such as semantic search, query answering, and textual entailment. Relation extraction identifies the entities from raw texts and the relations among them. An efficient and accurate CRE system is essential for creating domain knowledge in the biomedical industry. Existing Machine Learning and Natural Language Processing (NLP) techniques are not suitable to predict complex relations from sentences that consist of more than two relations and unspecified entities efficiently. In this work, deep learning techniques have been used to identify the appropriate semantic relation based on the context from multiple sentences. Even though various machine learning models have been used for relation extraction, they provide better results only for binary relations, i.e., relations occurred exactly between the two entities in a sentence. Machine learning models are
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#24187;&#35273;&#30340;&#20998;&#31867;&#12289;&#29702;&#35770;&#20998;&#26512;&#12289;&#26816;&#27979;&#26041;&#27861;&#21644;&#25913;&#36827;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#35774;&#24819;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.06794</link><description>&lt;p&gt;
&#35748;&#30693;&#24187;&#35273;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#29616;&#35937;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Cognitive Mirage: A Review of Hallucinations in Large Language Models. (arXiv:2309.06794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06794
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#24187;&#35273;&#30340;&#20998;&#31867;&#12289;&#29702;&#35770;&#20998;&#26512;&#12289;&#26816;&#27979;&#26041;&#27861;&#21644;&#25913;&#36827;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#35774;&#24819;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#19968;&#31181;&#20196;&#20154;&#25285;&#24551;&#30340;&#29616;&#35937;&#65292;&#21363;&#24187;&#35273;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#26368;&#36817;&#20851;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#24341;&#20154;&#27880;&#30446;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#24187;&#35273;&#30340;&#26032;&#20998;&#31867;&#20307;&#31995;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#29702;&#35770;&#24615;&#30340;&#27934;&#35265;&#12289;&#26816;&#27979;&#26041;&#27861;&#21644;&#25913;&#36827;&#26041;&#27861;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#25105;&#20204;&#20026;&#20986;&#29616;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#25552;&#20379;&#20102;&#35814;&#32454;&#21644;&#23436;&#25972;&#30340;&#20998;&#31867;&#20307;&#31995;&#65307;&#65288;2&#65289;&#25105;&#20204;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#29616;&#26377;&#30340;&#26816;&#27979;&#21644;&#25913;&#36827;&#26041;&#27861;&#65307;&#65288;3&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#21487;&#20197;&#21457;&#23637;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#30001;&#20110;&#24187;&#35273;&#21463;&#21040;&#20102;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#25105;&#20204;&#23558;&#32500;&#25252;&#19982;&#30456;&#20851;&#30740;&#31350;&#36827;&#23637;&#30340;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models continue to develop in the field of AI, text generation systems are susceptible to a worrisome phenomenon known as hallucination. In this study, we summarize recent compelling insights into hallucinations in LLMs. We present a novel taxonomy of hallucinations from various text generation tasks, thus provide theoretical insights, detection methods and improvement approaches. Based on this, future research directions are proposed. Our contribution are threefold: (1) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods; (3) We propose several research directions that can be developed in the future. As hallucinations garner significant attention from the community, we will maintain updates on relevant research progress.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Scaled Prompt-Tuning (SPT)&#26041;&#27861;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104; (NLG)&#12290;&#35813;&#26041;&#27861;&#20923;&#32467;&#22823;&#22810;&#25968;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#20854;&#20013;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#65292;&#20197;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12289;&#35757;&#32451;&#25104;&#26412;&#21644;&#26631;&#27880;&#25104;&#26412;&#65292;&#24182;&#22312;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26041;&#27861;&#12290;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;SPT&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06759</link><description>&lt;p&gt;
Scaled Prompt-Tuning for Few-Shot Natural Language Generation. (arXiv:2309.06759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Scaled Prompt-Tuning for Few-Shot Natural Language Generation. (arXiv:2309.06759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Scaled Prompt-Tuning (SPT)&#26041;&#27861;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104; (NLG)&#12290;&#35813;&#26041;&#27861;&#20923;&#32467;&#22823;&#22810;&#25968;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#20854;&#20013;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#65292;&#20197;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12289;&#35757;&#32451;&#25104;&#26412;&#21644;&#26631;&#27880;&#25104;&#26412;&#65292;&#24182;&#22312;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26041;&#27861;&#12290;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;SPT&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#26356;&#24378;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23545;&#19979;&#28216;&#20219;&#21153;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#30340;&#20869;&#23384;&#38656;&#27714;&#21644;&#35745;&#31639;&#25104;&#26412;&#26159;&#19981;&#21487;&#24573;&#35270;&#30340;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#36890;&#24120;&#38656;&#35201;&#26469;&#33258;&#21508;&#20010;&#20219;&#21153;&#30340;&#19968;&#23450;&#25968;&#37327;&#30340;&#25968;&#25454;&#65292;&#32780;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#26159;&#23454;&#38469;&#24212;&#29992;&#20013;&#38656;&#35201;&#32771;&#34385;&#30340;&#21478;&#19968;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#23569;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20923;&#32467;LLMs&#20013;&#30340;&#22823;&#22810;&#25968;&#21442;&#25968;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#24494;&#35843;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#65292;&#20197;&#38477;&#20302;&#20869;&#23384;&#21344;&#29992;&#12289;&#35757;&#32451;&#25104;&#26412;&#21644;&#26631;&#27880;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#29978;&#33267;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Scaled Prompt-Tuning (SPT)&#26041;&#27861;&#65292;&#23427;&#22312;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;Prompt-Tuning (PT)&#26041;&#27861;&#65292;&#20294;&#27809;&#26377;&#26126;&#26174;&#22686;&#21152;&#35757;&#32451;&#25104;&#26412;&#12290;&#36827;&#19968;&#27493;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;SPT&#20855;&#26377;&#26356;&#22909;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly Large Language Models (LLMs) demonstrate stronger language understanding and generation capabilities, while the memory demand and computation cost of fine-tuning LLMs on downstream tasks are non-negligible. Besides, fine-tuning generally requires a certain amount of data from individual tasks whilst data collection cost is another issue to consider in real-world applications. In this work, we focus on Parameter-Efficient Fine-Tuning (PEFT) methods for few-shot Natural Language Generation (NLG), which freeze most parameters in LLMs and tune a small subset of parameters in few-shot cases so that memory footprint, training cost, and labeling cost are reduced while maintaining or even improving the performance. We propose a Scaled Prompt-Tuning (SPT) method which surpasses conventional PT with better performance and generalization ability but without an obvious increase in training cost. Further study on intermediate SPT suggests the superior transferability of SPT in few-
&lt;/p&gt;</description></item><item><title>CONVERSER&#26159;&#19968;&#20010;&#20351;&#29992;&#23569;&#37327;&#23545;&#35805;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#33021;&#22815;&#29983;&#25104;&#19982;&#26816;&#32034;&#35821;&#26009;&#24211;&#20013;&#27573;&#33853;&#30456;&#20851;&#30340;&#23545;&#35805;&#26597;&#35810;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#23569;&#26679;&#26412;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#19982;&#23436;&#20840;&#30417;&#30563;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06748</link><description>&lt;p&gt;
CONVERSER&#65306;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#23569;&#26679;&#26412;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
CONVERSER: Few-Shot Conversational Dense Retrieval with Synthetic Data Generation. (arXiv:2309.06748v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06748
&lt;/p&gt;
&lt;p&gt;
CONVERSER&#26159;&#19968;&#20010;&#20351;&#29992;&#23569;&#37327;&#23545;&#35805;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#33021;&#22815;&#29983;&#25104;&#19982;&#26816;&#32034;&#35821;&#26009;&#24211;&#20013;&#27573;&#33853;&#30456;&#20851;&#30340;&#23545;&#35805;&#26597;&#35810;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#23569;&#26679;&#26412;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#19982;&#23436;&#20840;&#30417;&#30563;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25628;&#32034;&#20026;&#20449;&#24687;&#26816;&#32034;&#25552;&#20379;&#20102;&#33258;&#28982;&#30028;&#38754;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#22312;&#23545;&#35805;&#24335;&#20449;&#24687;&#26816;&#32034;&#20013;&#24212;&#29992;&#20102;&#23494;&#38598;&#26816;&#32034;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#23494;&#38598;&#26816;&#32034;&#22120;&#38656;&#35201;&#22823;&#37327;&#30340;&#39046;&#22495;&#30456;&#20851;&#30340;&#37197;&#23545;&#25968;&#25454;&#12290;&#36825;&#38480;&#21046;&#20102;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#21457;&#23637;&#65292;&#22240;&#20026;&#25910;&#38598;&#22823;&#37327;&#39046;&#22495;&#30456;&#20851;&#23545;&#35805;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CONVERSER&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#26368;&#22810;6&#23545;&#39046;&#22495;&#30456;&#20851;&#23545;&#35805;&#36827;&#34892;&#35757;&#32451;&#30340;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#26681;&#25454;&#26816;&#32034;&#35821;&#26009;&#24211;&#20013;&#30340;&#27573;&#33853;&#29983;&#25104;&#23545;&#35805;&#26597;&#35810;&#12290;&#23545;OR-QuAC&#21644;TREC CAsT 19&#31561;&#23545;&#35805;&#26816;&#32034;&#22522;&#20934;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;CONVERSER&#36798;&#21040;&#20102;&#19982;&#23436;&#20840;&#30417;&#30563;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#23569;&#26679;&#26412;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search provides a natural interface for information retrieval (IR). Recent approaches have demonstrated promising results in applying dense retrieval to conversational IR. However, training dense retrievers requires large amounts of in-domain paired data. This hinders the development of conversational dense retrievers, as abundant in-domain conversations are expensive to collect. In this paper, we propose CONVERSER, a framework for training conversational dense retrievers with at most 6 examples of in-domain dialogues. Specifically, we utilize the in-context learning capability of large language models to generate conversational queries given a passage in the retrieval corpus. Experimental results on conversational retrieval benchmarks OR-QuAC and TREC CAsT 19 show that the proposed CONVERSER achieves comparable performance to fully-supervised models, demonstrating the effectiveness of our proposed framework in few-shot conversational dense retrieval. All source code and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#27169;&#22411;(Keyphrase-Focused BART)&#65292;&#36890;&#36807;&#25286;&#20998;&#21644;&#37325;&#25490;&#30340;&#26041;&#24335;&#26469;&#22686;&#24378;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#22312;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.06726</link><description>&lt;p&gt;
&#36890;&#36807;&#25286;&#20998;&#21644;&#37325;&#25490;BART&#24494;&#35843;&#26469;&#22686;&#24378;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Keyphrase Generation by BART Finetuning with Splitting and Shuffling. (arXiv:2309.06726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#27169;&#22411;(Keyphrase-Focused BART)&#65292;&#36890;&#36807;&#25286;&#20998;&#21644;&#37325;&#25490;&#30340;&#26041;&#24335;&#26469;&#22686;&#24378;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#22312;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#26159;&#19968;&#39033;&#35782;&#21035;&#26368;&#20339;&#20195;&#34920;&#32473;&#23450;&#25991;&#26412;&#20027;&#39064;&#25110;&#20027;&#39064;&#30340;&#30701;&#35821;&#38598;&#30340;&#20219;&#21153;&#12290;&#20851;&#38190;&#30701;&#35821;&#20998;&#20026;&#20986;&#29616;&#21644;&#19981;&#22312;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#12290;&#26368;&#36817;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#19978;&#26174;&#31034;&#20986;&#20102;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25214;&#21040;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#30340;&#38590;&#24230;&#65292;&#24615;&#33021;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#27169;&#22411;(Keyphrase-Focused BART)&#65292;&#21033;&#29992;&#20102;&#20986;&#29616;&#21644;&#19981;&#20986;&#29616;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#23545;&#20986;&#29616;&#21644;&#19981;&#20986;&#29616;&#20851;&#38190;&#30701;&#35821;&#20998;&#21035;&#36827;&#34892;&#20102;&#20004;&#20010;&#29420;&#31435;BART&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#20851;&#38190;&#30701;&#35821;&#30340;&#37325;&#25490;&#21644;&#20505;&#36873;&#20851;&#38190;&#30701;&#35821;&#25490;&#24207;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23545;&#20110;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#65292;&#22312;&#20116;&#20010;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#30340;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#22312;F1@5&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyphrase generation is a task of identifying a set of phrases that best repre-sent the main topics or themes of a given text. Keyphrases are dividend int pre-sent and absent keyphrases. Recent approaches utilizing sequence-to-sequence models show effectiveness on absent keyphrase generation. However, the per-formance is still limited due to the hardness of finding absent keyphrases. In this paper, we propose Keyphrase-Focused BART, which exploits the differ-ences between present and absent keyphrase generations, and performs fine-tuning of two separate BART models for present and absent keyphrases. We further show effective approaches of shuffling keyphrases and candidate keyphrase ranking. For absent keyphrases, our Keyphrase-Focused BART achieved new state-of-the-art score on F1@5 in two out of five keyphrase gen-eration benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#28151;&#21512;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.06706</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Simultaneous Machine Translation with Large Language Models. (arXiv:2309.06706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#28151;&#21512;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#35805;&#24335;&#20132;&#20114;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#24050;&#32463;&#23637;&#31034;&#20986;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#21487;&#20197;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#31163;&#32447;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;LLM&#24212;&#29992;&#20110;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793; (SimulMT) &#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#19982;&#19981;&#21516;&#35299;&#30721;&#27169;&#24335;&#20135;&#29983;&#30340;&#35757;&#32451;-&#25512;&#29702;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;LLM&#36827;&#34892;SimulMT&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#20256;&#32479;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#20351;LLM&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21442;&#19982;SimulMT&#12290;&#27492;&#22806;&#65292;&#22312;&#23545;&#20840;&#21477;&#21644;&#21069;&#32512;&#21477;&#23376;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#21518;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#25105;&#20204;&#20351;&#29992;MUST-C&#25968;&#25454;&#38598;&#19978;&#30340;&#20061;&#31181;&#35821;&#35328;&#23545;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#21487;&#20197;&#23454;&#29616;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) have demonstrated their abilities to solve various natural language processing tasks through dialogue-based interactions. For instance, research indicates that LLMs can achieve competitive performance in offline machine translation tasks for high-resource languages. However, applying LLMs to simultaneous machine translation (SimulMT) poses many challenges, including issues related to the training-inference mismatch arising from different decoding patterns. In this paper, we explore the feasibility of utilizing LLMs for SimulMT. Building upon conventional approaches, we introduce a simple yet effective mixture policy that enables LLMs to engage in SimulMT without requiring additional training. Furthermore, after Supervised Fine-Tuning (SFT) on a mixture of full and prefix sentences, the model exhibits significant performance improvements. Our experiments, conducted with Llama2-7B-chat on nine language pairs from the MUST-C dataset, demonstrate that LLM can ac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#31995;&#32479;VLSlice&#65292;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#24341;&#23548;&#21457;&#29616;&#19968;&#33268;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#34892;&#20026;&#30340;&#34920;&#31034;&#32423;&#23376;&#32452;&#65292;&#20197;&#35299;&#20915;&#33258;&#21160;&#21457;&#29616;&#23376;&#32452;&#26102;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.06703</link><description>&lt;p&gt;
VLSlice&#65306;&#20132;&#20114;&#24335;&#35270;&#35273;&#21644;&#35821;&#35328;&#20999;&#29255;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
VLSlice: Interactive Vision-and-Language Slice Discovery. (arXiv:2309.06703v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06703
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#31995;&#32479;VLSlice&#65292;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#24341;&#23548;&#21457;&#29616;&#19968;&#33268;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#34892;&#20026;&#30340;&#34920;&#31034;&#32423;&#23376;&#32452;&#65292;&#20197;&#35299;&#20915;&#33258;&#21160;&#21457;&#29616;&#23376;&#32452;&#26102;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21487;&#20197;&#23398;&#20064;&#20986;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36801;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#21487;&#33021;&#25913;&#21892;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#32858;&#21512;&#25351;&#26631;&#65292;&#20294;&#36890;&#36807;&#20998;&#26512;&#38024;&#23545;&#29305;&#23450;&#20559;&#24046;&#32500;&#24230;&#30340;&#25163;&#24037;&#23376;&#32452;&#26102;&#65292;&#21457;&#29616;&#20102;&#31995;&#32479;&#24615;&#30340;&#19981;&#33391;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23376;&#32452;&#20998;&#26512;&#36890;&#24120;&#20250;&#22240;&#20026;&#27880;&#37322;&#24037;&#20316;&#32780;&#20572;&#28382;&#65292;&#32780;&#25910;&#38598;&#25152;&#38656;&#25968;&#25454;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#23581;&#35797;&#33258;&#21160;&#21457;&#29616;&#23376;&#32452;&#20197;&#35268;&#36991;&#36825;&#20123;&#38480;&#21046;&#65292;&#20294;&#36890;&#24120;&#21033;&#29992;&#29616;&#26377;&#20219;&#21153;&#29305;&#23450;&#27880;&#37322;&#19978;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#24182;&#22312;&#36229;&#20986;&#8220;&#34920;&#26684;&#8221;&#25968;&#25454;&#30340;&#26356;&#22797;&#26434;&#36755;&#20837;&#19978;&#24555;&#36895;&#38477;&#32423;&#65292;&#20854;&#20013;&#27809;&#26377;&#30740;&#31350;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;VLSlice&#65292;&#19968;&#31181;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#24341;&#23548;&#21457;&#29616;&#19968;&#33268;&#30340;&#34920;&#31034;&#32423;&#23376;&#32452;&#65292;&#20855;&#26377;&#19968;&#33268;&#30340;&#35270;&#35273;&#35821;&#35328;&#34892;&#20026;&#65292;&#34987;&#31216;&#20026;&#35270;&#35273;&#21644;&#35821;&#35328;&#20999;&#29255;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#20013;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond "tabular" data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled ima
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22303;&#32819;&#20854;&#35821;&#30340;&#31243;&#24207;&#25991;&#26412;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#25193;&#23637;&#20102;&#22303;&#32819;&#20854;wikiHow&#30340;&#25945;&#31243;&#25968;&#37327;&#65292;&#24182;&#30740;&#31350;&#20102;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#30456;&#23545;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.06698</link><description>&lt;p&gt;
&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31243;&#24207;&#24615;&#35821;&#35328;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#65306;&#20197;&#22303;&#32819;&#20854;&#35821;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Procedural Language Understanding for Low-Resource Languages: A Case Study on Turkish. (arXiv:2309.06698v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22303;&#32819;&#20854;&#35821;&#30340;&#31243;&#24207;&#25991;&#26412;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#25193;&#23637;&#20102;&#22303;&#32819;&#20854;wikiHow&#30340;&#25945;&#31243;&#25968;&#37327;&#65292;&#24182;&#30740;&#31350;&#20102;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#30456;&#23545;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31243;&#24207;&#24615;&#33258;&#28982;&#35821;&#35328;&#65288;&#20363;&#22914;&#65292;&#36880;&#27493;&#35828;&#26126;&#65289;&#26159;&#25191;&#34892;&#21644;&#35268;&#21010;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#33521;&#35821;&#20013;&#23384;&#22312;&#20016;&#23500;&#30340;&#35821;&#26009;&#24211;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#22823;&#22810;&#25968;&#35821;&#35328;&#32570;&#20047;&#36825;&#26679;&#30340;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#22303;&#32819;&#20854;&#31243;&#24207;&#25991;&#26412;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#33258;&#21160;&#32763;&#35793;&#24037;&#20855;&#23558;&#22303;&#32819;&#20854;wikiHow&#20013;&#30340;&#25945;&#31243;&#25968;&#37327;&#20174;2,000&#20010;&#25193;&#23637;&#21040;52,000&#20010;&#65292;&#32763;&#35793;&#36136;&#37327;&#21644;&#23545;&#21407;&#22987;&#21547;&#20041;&#30340;&#24544;&#23454;&#24615;&#30001;&#19987;&#23478;&#22242;&#38431;&#22312;&#19968;&#20010;&#38543;&#26426;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#35821;&#26009;&#24211;&#19978;&#29983;&#25104;&#20102;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#38142;&#25509;&#25805;&#20316;&#12289;&#30446;&#26631;&#25512;&#29702;&#21644;&#25688;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#65288;&#22914;TR-BART&#21644;BERTurk&#65289;&#20197;&#21450;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;mBART&#12289;mT5&#21644;XLM&#65289;&#23454;&#29616;&#20102;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#22987;&#32456;&#20197;&#26174;&#33879;&#30340;&#20248;&#21183;&#32988;&#36807;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding procedural natural language (e.g., step-by-step instructions) is a crucial step to execution and planning. However, while there are ample corpora and downstream tasks available in English, the field lacks such resources for most languages. To address this gap, we conduct a case study on Turkish procedural texts. We first expand the number of tutorials in Turkish wikiHow from 2,000 to 52,000 using automated translation tools, where the translation quality and loyalty to the original meaning are validated by a team of experts on a random set. Then, we generate several downstream tasks on the corpus, such as linking actions, goal inference, and summarization. To tackle these tasks, we implement strong baseline models via fine-tuning large language-specific models such as TR-BART and BERTurk, as well as multilingual models such as mBART, mT5, and XLM. We find that language-specific models consistently outperform their multilingual models by a significant margin across most pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32479;&#35745;&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#20248;&#21270;&#20559;&#22909;&#30340;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#32570;&#20047;&#22870;&#21169;&#27169;&#22411;&#21644;&#20174;&#26368;&#20248;&#31574;&#30053;&#37319;&#26679;&#20559;&#22909;&#23545;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06657</link><description>&lt;p&gt;
&#32479;&#35745;&#25298;&#32477;&#25277;&#26679;&#25913;&#36827;&#20102;&#20248;&#21270;&#20559;&#22909;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Statistical Rejection Sampling Improves Preference Optimization. (arXiv:2309.06657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32479;&#35745;&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#20248;&#21270;&#20559;&#22909;&#30340;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#32570;&#20047;&#22870;&#21169;&#27169;&#22411;&#21644;&#20174;&#26368;&#20248;&#31574;&#30053;&#37319;&#26679;&#20559;&#22909;&#23545;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#36890;&#36807;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22914;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#12290;&#26368;&#36817;&#65292;&#31163;&#32447;&#26041;&#27861;&#22914;&#24207;&#21015;&#20284;&#28982;&#26657;&#20934;&#65288;SLiC&#65289;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#24050;&#32463;&#25104;&#20026;&#26377;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;SLiC&#36890;&#36807;&#20351;&#29992;&#20174;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#31574;&#30053;&#20013;&#37319;&#26679;&#30340;&#24207;&#21015;&#23545;&#26469;&#20248;&#21270;&#20854;&#25439;&#22833;&#20989;&#25968;&#65292;&#32780;DPO&#30452;&#25509;&#26681;&#25454;&#20559;&#22909;&#25968;&#25454;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#26368;&#20248;&#31574;&#30053;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#65288;MLE&#65289;&#38656;&#35201;&#20174;&#35813;&#31574;&#30053;&#20013;&#37319;&#26679;&#26631;&#35760;&#30340;&#20559;&#22909;&#23545;&#12290;DPO&#32570;&#20047;&#22870;&#21169;&#27169;&#22411;&#38480;&#21046;&#20854;&#20174;&#26368;&#20248;&#31574;&#30053;&#20013;&#37319;&#26679;&#20559;&#22909;&#23545;&#30340;&#33021;&#21147;&#65292;&#32780;SLiC&#21017;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the alignment of language models with human preferences remains an active research challenge. Previous approaches have primarily utilized Reinforcement Learning from Human Feedback (RLHF) via online RL methods such as Proximal Policy Optimization (PPO). Recently, offline methods such as Sequence Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have emerged as attractive alternatives, offering improvements in stability and scalability while maintaining competitive performance. SLiC refines its loss function using sequence pairs sampled from a supervised fine-tuned (SFT) policy, while DPO directly optimizes language models based on preference data, foregoing the need for a separate reward model. However, the maximum likelihood estimator (MLE) of the target optimal policy requires labeled preference pairs sampled from that policy. DPO's lack of a reward model constrains its ability to sample preference pairs from the optimal policy, and SLiC is restricted t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;RT-LM&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#26102;&#25512;&#29702;&#36827;&#34892;&#20248;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#36164;&#28304;&#31649;&#29702;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#29702;&#35299;&#12289;&#37327;&#21270;&#21644;&#20248;&#21270;&#30001;&#20110;&#35821;&#35328;&#30340;&#19981;&#30830;&#23450;&#24615;&#32780;&#23548;&#33268;&#30340;&#25512;&#29702;&#24310;&#36831;&#24615;&#33021;&#21464;&#21270;&#65292;&#20197;&#25552;&#39640;LMs&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06619</link><description>&lt;p&gt;
RT-LM: &#38754;&#21521;&#23454;&#26102;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#36164;&#28304;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of Language Models. (arXiv:2309.06619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06619
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;RT-LM&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#26102;&#25512;&#29702;&#36827;&#34892;&#20248;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#36164;&#28304;&#31649;&#29702;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#29702;&#35299;&#12289;&#37327;&#21270;&#21644;&#20248;&#21270;&#30001;&#20110;&#35821;&#35328;&#30340;&#19981;&#30830;&#23450;&#24615;&#32780;&#23548;&#33268;&#30340;&#25512;&#29702;&#24310;&#36831;&#24615;&#33021;&#21464;&#21270;&#65292;&#20197;&#25552;&#39640;LMs&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#36827;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#22791;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#22238;&#24212;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#22312;&#23545;&#35805;AI&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#21069;&#26223;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;&#25104;&#26412;&#26497;&#39640;&#19988;&#25512;&#29702;&#24310;&#36831;&#26080;&#27861;&#39044;&#27979;&#65292;&#36825;&#20123;LMs&#22312;&#21508;&#31181;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#38754;&#20020;&#25361;&#25112;&#12290;&#30001;&#20110;&#35821;&#35328;&#30340;&#26412;&#36136;&#23548;&#33268;&#30340;&#19981;&#30830;&#23450;&#24615;&#24341;&#21457;&#30340;&#19981;&#21516;&#25512;&#29702;&#24310;&#36831;&#21487;&#33021;&#23548;&#33268;&#35745;&#31639;&#25928;&#29575;&#19981;&#39640;&#65292;&#20174;&#32780;&#38477;&#20302;LMs&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#27969;&#37327;&#30340;&#24037;&#20316;&#36127;&#36733;&#19979;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#28304;&#30340;&#24102;&#23485;&#38750;&#24120;&#24191;&#27867;&#65292;&#32473;&#24310;&#36831;&#30340;&#39044;&#27979;&#21644;&#30001;&#27492;&#20135;&#29983;&#30340;&#25928;&#26524;&#24102;&#26469;&#20102;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#20102;&#35299;&#21644;&#20943;&#36731;&#19981;&#30830;&#23450;&#24615;&#23545;&#23454;&#26102;&#21709;&#24212;&#38656;&#27714;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#39318;&#20808;&#35201;&#29702;&#35299;&#12289;&#37327;&#21270;&#21644;&#20248;&#21270;LMs&#20013;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30340;&#24310;&#36831;&#24615;&#33021;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RT-LM
&lt;/p&gt;
&lt;p&gt;
Recent advancements in language models (LMs) have gained substantial attentions on their capability to generate human-like responses. Though exhibiting a promising future for various applications such as conversation AI, these LMs face deployment challenges on various devices due to their extreme computational cost and unpredictable inference latency. Such varied inference latency, identified as a consequence of uncertainty intrinsic to the nature of language, can lead to computational inefficiency and degrade the overall performance of LMs, especially under high-traffic workloads. Unfortunately, the bandwidth of these uncertainty sources is extensive, complicating the prediction of latency and the effects emanating from such uncertainties. To understand and mitigate the impact of uncertainty on real-time response-demanding systems, we take the first step to comprehend, quantify and optimize these uncertainty-induced latency performance variations in LMs. Specifically, we present RT-LM
&lt;/p&gt;</description></item><item><title>&#21465;&#20107;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#21160;&#21147;&#31995;&#32479;&#65292;&#20854;&#28436;&#21270;&#21487;&#20197;&#29992;&#19968;&#20010;&#34892;&#21160;&#31215;&#20998;&#26469;&#25551;&#36848;&#65292;&#24182;&#19988;&#24179;&#22343;&#36335;&#24452;&#19982;&#34892;&#21160;&#21407;&#29702;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2309.06600</link><description>&lt;p&gt;
&#21465;&#20107;&#20316;&#20026;&#19968;&#20010;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Narrative as a Dynamical System. (arXiv:2309.06600v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06600
&lt;/p&gt;
&lt;p&gt;
&#21465;&#20107;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#21160;&#21147;&#31995;&#32479;&#65292;&#20854;&#28436;&#21270;&#21487;&#20197;&#29992;&#19968;&#20010;&#34892;&#21160;&#31215;&#20998;&#26469;&#25551;&#36848;&#65292;&#24182;&#19988;&#24179;&#22343;&#36335;&#24452;&#19982;&#34892;&#21160;&#21407;&#29702;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#20154;&#31867;&#27963;&#21160;&#30340;&#20840;&#36807;&#31243;&#20197;&#21450;&#21465;&#20107;&#21487;&#20197;&#34987;&#35270;&#20026;&#29289;&#29702;&#24847;&#20041;&#19978;&#30340;&#21160;&#21147;&#31995;&#32479;&#65307;&#19968;&#20010;&#30001;&#34892;&#21160;&#31215;&#20998;&#25551;&#36848;&#20854;&#28436;&#21270;&#30340;&#31995;&#32479;&#65292;&#20351;&#24471;&#20174;A&#28857;&#21040;B&#28857;&#30340;&#25152;&#26377;&#21487;&#33021;&#36335;&#24452;&#30340;&#24179;&#22343;&#20540;&#30001;&#34892;&#21160;&#30340;&#26497;&#22823;&#20540;&#32473;&#20986;&#12290;&#25105;&#20204;&#36890;&#36807;&#24179;&#22343;&#32422;500&#20010;&#19981;&#21516;&#30340;&#21465;&#20107;&#26500;&#24314;&#20102;&#19977;&#26465;&#36825;&#26679;&#30340;&#36335;&#24452;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#24179;&#22343;&#36335;&#24452;&#19982;&#34892;&#21160;&#21407;&#29702;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is increasing evidence that human activity in general, and narrative in particular, can be treated as a dynamical system in the physics sense; a system whose evolution is described by an action integral, such that the average of all possible paths from point A to point B is given by the extremum of the action. We create by construction three such paths by averaging about 500 different narratives, and we show that the average path is consistent with an action principle.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#12289;&#24615;&#33021;&#21644;&#35745;&#31639;&#36164;&#28304;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#26469;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#65292;&#20174;&#32780;&#21019;&#24314;&#26356;&#39640;&#25928;&#12289;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#20026;AI&#35821;&#35328;&#24314;&#27169;&#30340;&#21487;&#25345;&#32493;&#21644;&#21487;&#35775;&#38382;&#30340;&#26410;&#26469;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.06589</link><description>&lt;p&gt;
&#29983;&#25104;&#22823;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#38656;&#35201;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Generative Large Language Models need billions of parameters?. (arXiv:2309.06589v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#12289;&#24615;&#33021;&#21644;&#35745;&#31639;&#36164;&#28304;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#26469;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#65292;&#20174;&#32780;&#21019;&#24314;&#26356;&#39640;&#25928;&#12289;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#20026;AI&#35821;&#35328;&#24314;&#27169;&#30340;&#21487;&#25345;&#32493;&#21644;&#21487;&#35775;&#38382;&#30340;&#26410;&#26469;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#24320;&#21457;&#39640;&#25928;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26032;&#31995;&#32479;&#21644;&#26041;&#27861;&#12290;&#23427;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#24615;&#33021;&#21644;&#35745;&#31639;&#36164;&#28304;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;&#30740;&#31350;&#25506;&#32034;&#20102;&#20801;&#35768;&#27169;&#22411;&#30340;&#19981;&#21516;&#37096;&#20998;&#20849;&#20139;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#25152;&#38656;&#30340;&#29420;&#31435;&#21442;&#25968;&#24635;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#30830;&#20445;&#20102;&#27169;&#22411;&#26082;&#32039;&#20945;&#21448;&#19981;&#25439;&#22833;&#23398;&#20064;&#21644;&#34920;&#31034;&#22797;&#26434;&#35821;&#35328;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#20026;&#21019;&#24314;&#26356;&#39640;&#25928;&#12289;&#26356;&#26377;&#25928;&#30340;LLMs&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#21644;&#24037;&#20855;&#65292;&#20026;AI&#35821;&#35328;&#24314;&#27169;&#30340;&#21487;&#25345;&#32493;&#21644;&#21487;&#35775;&#38382;&#30340;&#26410;&#26469;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents novel systems and methodologies for the development of efficient large language models (LLMs). It explores the trade-offs between model size, performance, and computational resources, with the aim of maximizing the efficiency of these AI systems. The research explores novel methods that allow different parts of the model to share parameters, reducing the total number of unique parameters required. This approach ensures that the model remains compact without sacrificing its ability to learn and represent complex language structures. This study provides valuable insights and tools for creating more efficient and effective LLMs, contributing to a more sustainable and accessible future for AI language modeling.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#33487;&#40654;&#19990;&#35748;&#30693;&#35821;&#26009;&#24211;&#30340;&#35748;&#30693;&#29305;&#24449;&#19982;BERT&#27169;&#22411;&#38598;&#25104;&#65292;&#35777;&#26126;&#20102;&#33041;&#30005;&#22270;&#21644;&#30524;&#21160;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#30340;&#35789;-EEG&#35789;&#20856;&#12290;</title><link>http://arxiv.org/abs/2309.06580</link><description>&lt;p&gt;
&#20154;&#31867;&#33021;&#24110;&#21161;BERT&#33719;&#24471;&#8220;&#20449;&#24515;&#8221;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can humans help BERT gain "confidence"?. (arXiv:2309.06580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#33487;&#40654;&#19990;&#35748;&#30693;&#35821;&#26009;&#24211;&#30340;&#35748;&#30693;&#29305;&#24449;&#19982;BERT&#27169;&#22411;&#38598;&#25104;&#65292;&#35777;&#26126;&#20102;&#33041;&#30005;&#22270;&#21644;&#30524;&#21160;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#30340;&#35789;-EEG&#35789;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#20026;&#36328;&#23398;&#31185;&#30740;&#31350;&#24320;&#36767;&#20102;&#22810;&#31181;&#36884;&#24452;&#12290;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#28789;&#24863;&#26469;&#33258;&#22823;&#33041;&#31070;&#32463;&#20803;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#23558;&#36825;&#20004;&#20010;&#39046;&#22495;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#21033;&#29992;&#35748;&#30693;&#25968;&#25454;&#26469;&#35757;&#32451;AI&#27169;&#22411;&#20284;&#20046;&#26159;&#38750;&#24120;&#23454;&#38469;&#30340;&#12290;&#36825;&#19981;&#20165;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#25216;&#26415;&#65292;&#36824;&#26377;&#21161;&#20110;&#29702;&#35299;&#22823;&#33041;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#23454;&#39564;&#65292;&#23558;&#33487;&#40654;&#19990;&#35748;&#30693;&#35821;&#26009;&#24211;&#65288;ZuCo&#65289;&#30340;&#35748;&#30693;&#29305;&#24449;&#19982;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;BERT&#38598;&#25104;&#12290;&#25105;&#23637;&#31034;&#20102;&#26469;&#33258;ZuCo&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#21644;&#30524;&#21160;&#29305;&#24449;&#22914;&#20309;&#24110;&#21161;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#21033;&#29992;&#19968;&#20010;&#40065;&#26834;&#24615;&#26816;&#26597;&#27969;&#27700;&#32447;&#30830;&#35748;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#24182;&#29983;&#25104;&#20102;&#19968;&#20010;&#21333;&#35789;-EEG&#35789;&#20856;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#20219;&#20309;&#35748;&#30693;&#29305;&#24449;&#30340;&#22806;&#37096;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20998;&#26512;&#20102;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancements in artificial intelligence over the last decade have opened a multitude of avenues for interdisciplinary research. Since the idea of artificial intelligence was inspired by the working of neurons in the brain, it seems pretty practical to combine the two fields and take the help of cognitive data to train AI models. Not only it will help to get a deeper understanding of the technology, but of the brain as well. In this thesis, I conduct novel experiments to integrate cognitive features from the Zurich Cognitive Corpus (ZuCo) (Hollenstein et al., 2018) with a transformer-based encoder model called BERT. I show how EEG and eye-tracking features from ZuCo can help to increase the performance of the NLP model. I confirm the performance increase with the help of a robustness-checking pipeline and derive a word-EEG lexicon to use in benchmarking on an external dataset that does not have any cognitive features associated with it. Further, I analyze the internal working mechan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26469;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#31038;&#21306;&#39537;&#21160;&#30340;&#27880;&#37322;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#22522;&#20934;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2309.06578</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#36776;&#21035;&#31185;&#23398;&#20551;&#35774;&#30340;&#35777;&#25454;&#65311;&#31038;&#20250;&#31185;&#23398;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences. (arXiv:2309.06578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26469;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#31038;&#21306;&#39537;&#21160;&#30340;&#27880;&#37322;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#22522;&#20934;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#30340;&#21046;&#23450;&#21644;&#27979;&#35797;&#26159;&#32463;&#39564;&#24615;&#30740;&#31350;&#30340;&#26680;&#24515;&#12290;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#20551;&#35774;&#26159;&#22522;&#20110;&#29616;&#26377;&#35777;&#25454;&#30340;&#26368;&#20339;&#29468;&#27979;&#65292;&#24182;&#19988;&#26159;&#22522;&#20110;&#30456;&#20851;&#25991;&#29486;&#30340;&#20840;&#38754;&#35270;&#22270;&#36827;&#34892;&#21551;&#21457;&#30340;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27599;&#24180;&#31185;&#23398;&#25991;&#31456;&#25968;&#37327;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#23545;&#20110;&#32473;&#23450;&#20551;&#35774;&#30456;&#20851;&#35777;&#25454;&#30340;&#25163;&#21160;&#27719;&#24635;&#21644;&#32508;&#21512;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#20013;&#30340;&#35777;&#25454;&#65292;&#33021;&#21542;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20849;&#20139;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#20013;&#20351;&#29992;&#31038;&#21306;&#39537;&#21160;&#30340;&#30740;&#31350;&#27880;&#37322;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;LLMs&#30340;&#24615;&#33021;&#19982;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#25351;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#26426;&#20250;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/Sai90000/ScientificHypothesisEvidencing.git&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypothesis formulation and testing are central to empirical research. A strong hypothesis is a best guess based on existing evidence and informed by a comprehensive view of relevant literature. However, with exponential increase in the number of scientific articles published annually, manual aggregation and synthesis of evidence related to a given hypothesis is a challenge. Our work explores the ability of current large language models (LLMs) to discern evidence in support or refute of specific hypotheses based on the text of scientific abstracts. We share a novel dataset for the task of scientific hypothesis evidencing using community-driven annotations of studies in the social sciences. We compare the performance of LLMs to several state-of-the-art benchmarks and highlight opportunities for future research in this area. The dataset is available at https://github.com/Sai90000/ScientificHypothesisEvidencing.git
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#38750;&#35821;&#35328;&#32447;&#32034;&#22312;&#20154;&#31867;&#20132;&#27969;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#20511;&#37492;&#25163;&#35821;&#22788;&#29702;&#30340;&#36827;&#23637;&#65292;&#25552;&#20986;&#21457;&#23637;&#36890;&#29992;&#30340;&#33258;&#21160;&#25163;&#21183;&#20998;&#21106;&#21644;&#36716;&#24405;&#27169;&#22411;&#26469;&#24357;&#34917;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#30450;&#28857;&#65292;&#24182;&#22686;&#24378;NLP&#27169;&#22411;&#30340;&#33539;&#22260;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06572</link><description>&lt;p&gt;
&#35299;&#20915;&#21475;&#35821;&#35821;&#35328;&#22788;&#29702;&#30340;&#30450;&#21306;
&lt;/p&gt;
&lt;p&gt;
Addressing the Blind Spots in Spoken Language Processing. (arXiv:2309.06572v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38750;&#35821;&#35328;&#32447;&#32034;&#22312;&#20154;&#31867;&#20132;&#27969;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#20511;&#37492;&#25163;&#35821;&#22788;&#29702;&#30340;&#36827;&#23637;&#65292;&#25552;&#20986;&#21457;&#23637;&#36890;&#29992;&#30340;&#33258;&#21160;&#25163;&#21183;&#20998;&#21106;&#21644;&#36716;&#24405;&#27169;&#22411;&#26469;&#24357;&#34917;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#30450;&#28857;&#65292;&#24182;&#22686;&#24378;NLP&#27169;&#22411;&#30340;&#33539;&#22260;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38750;&#35821;&#35328;&#32447;&#32034;&#22312;&#20154;&#31867;&#20132;&#27969;&#20013;&#30340;&#20851;&#38190;&#20294;&#24448;&#24448;&#34987;&#24573;&#35270;&#30340;&#20316;&#29992;&#65292;&#21253;&#25324;&#21327;&#21516;&#35821;&#35328;&#25163;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#65292;&#24182;&#25506;&#35752;&#36825;&#20123;&#32447;&#32034;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35748;&#20026;&#29702;&#35299;&#20154;&#31867;&#20132;&#27969;&#38656;&#35201;&#19968;&#31181;&#26356;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#36229;&#36234;&#25991;&#26412;&#25110;&#21475;&#35821;&#35789;&#27719;&#65292;&#21253;&#25324;&#38750;&#35821;&#35328;&#20803;&#32032;&#12290;&#20511;&#37492;&#25163;&#35821;&#22788;&#29702;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#21457;&#23637;&#36890;&#29992;&#30340;&#33258;&#21160;&#25163;&#21183;&#20998;&#21106;&#21644;&#36716;&#24405;&#27169;&#22411;&#65292;&#23558;&#36825;&#20123;&#38750;&#35821;&#35328;&#32447;&#32034;&#36716;&#24405;&#25104;&#25991;&#26412;&#24418;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#24357;&#34917;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#30450;&#28857;&#65292;&#22686;&#24378;NLP&#27169;&#22411;&#30340;&#33539;&#22260;&#21644;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#31034;&#20363;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20165;&#20381;&#38752;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;NLP&#27969;&#31243;&#26080;&#32541;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#21628;&#21505;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#25913;&#36827;&#26469;&#32467;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the critical but often overlooked role of non-verbal cues, including co-speech gestures and facial expressions, in human communication and their implications for Natural Language Processing (NLP). We argue that understanding human communication requires a more holistic approach that goes beyond textual or spoken words to include non-verbal elements. Borrowing from advances in sign language processing, we propose the development of universal automatic gesture segmentation and transcription models to transcribe these non-verbal cues into textual form. Such a methodology aims to bridge the blind spots in spoken language understanding, enhancing the scope and applicability of NLP models. Through motivating examples, we demonstrate the limitations of relying solely on text-based models. We propose a computationally efficient and flexible approach for incorporating non-verbal cues, which can seamlessly integrate with existing NLP pipelines. We conclude by calling upon the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20046;&#27809;&#26377;&#20154;&#20026;&#24433;&#21709;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#22823;&#23398;&#25253;&#32440;&#26723;&#26696;&#20013;&#33719;&#21462;&#24182;&#26816;&#27979;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25688;&#35201;&#30340;&#24773;&#24863;&#19982;&#21407;&#25991;&#26469;&#35745;&#31639;&#20559;&#35265;&#65292;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#20026;&#23458;&#35266;&#29702;&#35299;&#23398;&#29983;&#25253;&#32440;&#26469;&#28304;&#20013;&#30340;&#20559;&#35265;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.06557</link><description>&lt;p&gt;
&#22312;&#22823;&#23398;&#23398;&#29983;&#25253;&#32440;&#20013;&#26080;&#30417;&#30563;&#26816;&#27979;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Bias Detection in College Student Newspapers. (arXiv:2309.06557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20046;&#27809;&#26377;&#20154;&#20026;&#24433;&#21709;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#22823;&#23398;&#25253;&#32440;&#26723;&#26696;&#20013;&#33719;&#21462;&#24182;&#26816;&#27979;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25688;&#35201;&#30340;&#24773;&#24863;&#19982;&#21407;&#25991;&#26469;&#35745;&#31639;&#20559;&#35265;&#65292;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#20026;&#23458;&#35266;&#29702;&#35299;&#23398;&#29983;&#25253;&#32440;&#26469;&#28304;&#20013;&#30340;&#20559;&#35265;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20046;&#27809;&#26377;&#20154;&#20026;&#24433;&#21709;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#22823;&#23398;&#25253;&#32440;&#26723;&#26696;&#20013;&#33719;&#21462;&#24182;&#26816;&#27979;&#20559;&#35265;&#12290;&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20174;&#33258;&#21160;&#21270;&#24037;&#20855;&#26080;&#27861;&#33719;&#21462;&#25968;&#25454;&#30340;&#22797;&#26434;&#26723;&#26696;&#32593;&#31449;&#19978;&#33719;&#21462;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#24182;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;23,154&#20010;&#26465;&#30446;&#30340;14&#20010;&#23398;&#29983;&#25253;&#32440;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25688;&#35201;&#30340;&#24773;&#24863;&#19982;&#21407;&#25991;&#36827;&#34892;&#27604;&#36739;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#20851;&#38190;&#23383;&#26597;&#35810;&#26469;&#35745;&#31639;&#20559;&#35265;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#23427;&#27604;&#37325;&#26500;&#20559;&#35265;&#26356;&#23569;&#27604;&#36739;&#65292;&#24182;&#19988;&#27604;&#29983;&#25104;&#20851;&#38190;&#23383;&#24773;&#32490;&#38656;&#35201;&#26356;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#25919;&#27835;&#24615;&#35789;&#27719;&#20197;&#21450;&#25511;&#21046;&#35789;&#19978;&#35745;&#31639;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#24471;&#20986;&#32467;&#35770;&#12290;&#35813;&#23436;&#25972;&#26041;&#27861;&#26377;&#21161;&#20110;&#22312;&#20551;&#35774;&#21644;&#20998;&#31867;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#32454;&#33268;&#20837;&#24494;&#30340;&#35265;&#35299;&#65292;&#20026;&#23458;&#35266;&#29702;&#35299;&#23398;&#29983;&#25253;&#32440;&#26469;&#28304;&#20013;&#30340;&#20559;&#35265;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a pipeline with minimal human influence for scraping and detecting bias on college newspaper archives. This paper introduces a framework for scraping complex archive sites that automated tools fail to grab data from, and subsequently generates a dataset of 14 student papers with 23,154 entries. This data can also then be queried by keyword to calculate bias by comparing the sentiment of a large language model summary to the original article. The advantages of this approach are that it is less comparative than reconstruction bias and requires less labelled data than generating keyword sentiment. Results are calculated on politically charged words as well as control words to show how conclusions can be drawn. The complete method facilitates the extraction of nuanced insights with minimal assumptions and categorizations, paving the way for a more objective understanding of bias within student newspaper sources.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06553</link><description>&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#19979;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#25581;&#31034;LLMs&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#38656;&#35201;&#22312;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#34429;&#28982;&#25552;&#31034;&#24037;&#31243;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#35797;&#38169;&#23581;&#35797;&#20013;&#25152;&#38656;&#30340;&#20154;&#24037;&#35774;&#35745;&#25552;&#31034;&#21644;&#30456;&#20851;&#25104;&#26412;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20851;&#38190;&#26159;&#65292;&#25552;&#31034;&#20248;&#21270;&#30340;&#25928;&#29575;&#21462;&#20915;&#20110;&#26114;&#36149;&#30340;&#25552;&#31034;&#35780;&#20272;&#36807;&#31243;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;Prompt-OIRL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#26377;&#25928;&#25552;&#31034;&#35780;&#20272;&#21644;&#21487;&#36127;&#25285;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19987;&#23478;&#35780;&#20272;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#36816;&#29992;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#19968;&#20010;&#38024;&#23545;&#31163;&#32447;&#12289;&#26597;&#35810;&#20381;&#36182;&#22411;&#25552;&#31034;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;Prompt-OIRL&#30340;&#20248;&#28857;&#26159;&#22810;&#26041;&#38754;&#30340;&#65306;&#23427;&#39044;&#27979;&#25552;&#31034;&#30340;&#24615;&#33021;&#65292;&#25104;&#26412;&#39640;&#25928;&#65292;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#22270;&#34920;&#31034;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#25991;&#26723;&#20998;&#35299;&#20026;&#35821;&#20041;&#26694;&#26550;&#65292;&#28982;&#21518;&#20351;&#29992;&#27492;&#20013;&#38388;&#31232;&#30095;&#26684;&#24335;&#29983;&#25104;&#25991;&#26412;&#12290;&#36890;&#36807;&#25200;&#21160;&#26694;&#26550;&#20869;&#23481;&#65292;&#21253;&#25324;&#25299;&#25169;&#20998;&#26512;&#25366;&#25496;&#26032;&#30340;&#36229;&#36793;&#20197;&#21450;&#21253;&#21547;&#23618;&#27425;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#30340;&#22797;&#26434;&#22810;&#20803;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#30340;&#25991;&#26723;&#22312;&#26679;&#24335;&#12289;&#24773;&#24863;&#12289;&#26684;&#24335;&#12289;&#26500;&#25104;&#21644;&#20107;&#23454;&#19978;&#26159;&#22810;&#26679;&#30340;&#12289;&#36830;&#36143;&#30340;&#21644;&#21464;&#21270;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.06550</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#22270;&#34920;&#31034;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Synthetic Text Generation using Hypergraph Representations. (arXiv:2309.06550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#22270;&#34920;&#31034;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#25991;&#26723;&#20998;&#35299;&#20026;&#35821;&#20041;&#26694;&#26550;&#65292;&#28982;&#21518;&#20351;&#29992;&#27492;&#20013;&#38388;&#31232;&#30095;&#26684;&#24335;&#29983;&#25104;&#25991;&#26412;&#12290;&#36890;&#36807;&#25200;&#21160;&#26694;&#26550;&#20869;&#23481;&#65292;&#21253;&#25324;&#25299;&#25169;&#20998;&#26512;&#25366;&#25496;&#26032;&#30340;&#36229;&#36793;&#20197;&#21450;&#21253;&#21547;&#23618;&#27425;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#30340;&#22797;&#26434;&#22810;&#20803;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#30340;&#25991;&#26723;&#22312;&#26679;&#24335;&#12289;&#24773;&#24863;&#12289;&#26684;&#24335;&#12289;&#26500;&#25104;&#21644;&#20107;&#23454;&#19978;&#26159;&#22810;&#26679;&#30340;&#12289;&#36830;&#36143;&#30340;&#21644;&#21464;&#21270;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25991;&#26723;&#30340;&#21512;&#25104;&#21464;&#20307;&#36890;&#24120;&#34987;&#35270;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#23558;&#25991;&#26723;&#20998;&#35299;&#20026;&#35821;&#20041;&#26694;&#26550;&#65292;&#28982;&#21518;&#20351;&#29992;&#27492;&#20013;&#38388;&#31232;&#30095;&#26684;&#24335;&#29983;&#25104;&#25991;&#26412;&#12290;&#36825;&#20123;&#26694;&#26550;&#20351;&#29992;&#36229;&#22270;&#36827;&#34892;&#24314;&#27169;&#65292;&#21487;&#20197;&#20197;&#24688;&#24403;&#30340;&#26041;&#24335;&#25200;&#21160;&#26694;&#26550;&#20869;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#25299;&#25169;&#20998;&#26512;&#25366;&#25496;&#26032;&#30340;&#36229;&#36793;&#65292;&#21253;&#25324;&#23618;&#27425;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#30340;&#22797;&#26434;&#22810;&#20803;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#30340;&#25991;&#26723;&#22312;&#26679;&#24335;&#12289;&#24773;&#24863;&#12289;&#26684;&#24335;&#12289;&#26500;&#25104;&#21644;&#20107;&#23454;&#19978;&#26159;&#22810;&#26679;&#30340;&#12289;&#36830;&#36143;&#30340;&#21644;&#21464;&#21270;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating synthetic variants of a document is often posed as text-to-text transformation. We propose an alternate LLM based method that first decomposes a document into semantic frames and then generates text using this interim sparse format. The frames are modeled using a hypergraph, which allows perturbing the frame contents in a principled manner. Specifically, new hyperedges are mined through topological analysis and complex polyadic relationships including hierarchy and temporal dynamics are accommodated. We show that our solution generates documents that are diverse, coherent and vary in style, sentiment, format, composition and facts.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;LLMs&#22312;&#25551;&#36848;&#22797;&#26434;&#35821;&#20041;&#20851;&#31995;&#20381;&#36182;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#20004;&#20010;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26102;&#65292;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;STS&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;STS&#22522;&#20934;&#27979;&#35797;&#20013;&#20445;&#25345;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06541</link><description>&lt;p&gt;
&#25991;&#26412;&#32534;&#30721;&#22120;&#32570;&#20047;&#30693;&#35782;&#65306;&#21033;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22686;&#24378;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Text Encoders Lack Knowledge: Leveraging Generative LLMs for Domain-Specific Semantic Textual Similarity. (arXiv:2309.06541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06541
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;LLMs&#22312;&#25551;&#36848;&#22797;&#26434;&#35821;&#20041;&#20851;&#31995;&#20381;&#36182;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#20004;&#20010;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26102;&#65292;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;STS&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;STS&#22522;&#20934;&#27979;&#35797;&#20013;&#20445;&#25345;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;STS&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;STS&#22522;&#20934;&#27979;&#35797;&#20013;&#20445;&#25345;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#24403;&#25551;&#36848;&#20381;&#36182;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#20004;&#20010;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#29983;&#25104;&#24335;LLMs&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;STS&#27169;&#22411;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#22312;&#20581;&#24247;&#12289;&#25919;&#27835;&#21644;&#20307;&#32946;&#39046;&#22495;&#25910;&#38598;&#20102;&#19977;&#20010;&#26032;&#30340;STS&#25361;&#25112;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#38656;&#35201;&#22312;&#19990;&#30028;&#30693;&#35782;&#19978;&#36827;&#34892;&#21028;&#26029;&#12290;&#25152;&#26377;&#26032;&#25910;&#38598;&#30340;&#25968;&#25454;&#37117;&#26469;&#33258;&#20110;2023&#24180;5&#26376;&#20043;&#21518;&#21457;&#24067;&#30340;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#65292;&#20197;&#30830;&#20445;ChatGPT&#31561;&#38381;&#28304;&#27169;&#22411;&#30340;&#24615;&#33021;&#19981;&#33021;&#24402;&#21151;&#20110;&#35760;&#24518;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24179;&#22343;&#32780;&#35328;&#65292;&#29983;&#25104;&#24335;LLM&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#22909;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amidst the sharp rise in the evaluation of large language models (LLMs) on various tasks, we find that semantic textual similarity (STS) has been under-explored. In this study, we show that STS can be cast as a text generation problem while maintaining strong performance on multiple STS benchmarks. Additionally, we show generative LLMs significantly outperform existing encoder-based STS models when characterizing the semantic similarity between two texts with complex semantic relationships dependent on world knowledge. We validate this claim by evaluating both generative LLMs and existing encoder-based STS models on three newly collected STS challenge sets which require world knowledge in the domains of Health, Politics, and Sports. All newly collected data is sourced from social media content posted after May 2023 to ensure the performance of closed-source models like ChatGPT cannot be credited to memorization. Our results show that, on average, generative LLMs outperform the best enc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#25239;&#25915;&#20987;&#23545;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#38754;&#23545;&#24050;&#30693;&#30340;&#26368;&#20339;&#23545;&#25239;&#25915;&#20987;&#26102;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#31283;&#23450;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#31639;&#27861;&#22312;&#30456;&#23545;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26367;&#20195;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.06527</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Machine Translation Models Stand Strong in the Face of Adversarial Attacks. (arXiv:2309.06527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#25239;&#25915;&#20987;&#23545;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#38754;&#23545;&#24050;&#30693;&#30340;&#26368;&#20339;&#23545;&#25239;&#25915;&#20987;&#26102;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#31283;&#23450;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#31639;&#27861;&#22312;&#30456;&#23545;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26367;&#20195;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#36890;&#36807;&#21521;&#36755;&#20837;&#24341;&#20837;&#24494;&#23567;&#25200;&#21160;&#26469;&#26292;&#38706;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#36825;&#23548;&#33268;&#36755;&#20986;&#32467;&#26524;&#21457;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#36825;&#31181;&#23545;&#25239;&#25915;&#20987;&#23545;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20123;&#31639;&#27861;&#65292;&#21253;&#25324;&#22522;&#26412;&#25991;&#26412;&#25200;&#21160;&#21551;&#21457;&#24335;&#21644;&#26356;&#39640;&#32423;&#30340;&#31574;&#30053;&#65292;&#22914;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#65292;&#23427;&#21033;&#29992;&#21487;&#24494;&#20998;&#36924;&#36817;&#38750;&#21487;&#24494;&#32763;&#35793;&#24230;&#37327;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#35843;&#26597;&#65292;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#23545;&#24050;&#30693;&#30340;&#26368;&#20339;&#23545;&#25239;&#25915;&#20987;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#31283;&#23450;&#24615;&#65292;&#22240;&#20026;&#36755;&#20986;&#20013;&#30340;&#25200;&#21160;&#31243;&#24230;&#19982;&#36755;&#20837;&#20013;&#30340;&#25200;&#21160;&#25104;&#27604;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#21033;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#32988;&#36807;&#20854;&#20182;&#36873;&#25321;&#65292;&#25552;&#20379;&#20102;&#26368;&#20339;&#30340;&#30456;&#23545;&#24615;&#33021;&#12290;&#21478;&#19968;&#20010;&#24378;&#22823;&#30340;&#20505;&#36873;&#26159;&#22522;&#20110;&#20010;&#20307;&#28151;&#21512;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks expose vulnerabilities of deep learning models by introducing minor perturbations to the input, which lead to substantial alterations in the output. Our research focuses on the impact of such adversarial attacks on sequence-to-sequence (seq2seq) models, specifically machine translation models. We introduce algorithms that incorporate basic text perturbation heuristics and more advanced strategies, such as the gradient-based attack, which utilizes a differentiable approximation of the inherently non-differentiable translation metric. Through our investigation, we provide evidence that machine translation models display robustness displayed robustness against best performed known adversarial attacks, as the degree of perturbation in the output is directly proportional to the perturbation in the input. However, among underdogs, our attacks outperform alternatives, providing the best relative performance. Another strong candidate is an attack based on mixing of individu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#31995;&#32479;&#32452;&#21512;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06520</link><description>&lt;p&gt;
&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#30340;&#31995;&#32479;&#32452;&#21512;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Minimum Bayes' Risk Decoding for System Combination of Grammatical Error Correction Systems. (arXiv:2309.06520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#31995;&#32479;&#32452;&#21512;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#20219;&#21153;&#26469;&#35828;&#65292;&#23558;&#21508;&#20010;&#31995;&#32479;&#30340;&#36755;&#20986;&#36827;&#34892;&#32452;&#21512;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;&#21516;&#26102;&#65292;&#35299;&#30721;&#20934;&#21017;&#19982;&#35780;&#20272;&#20934;&#21017;&#20043;&#38388;&#36890;&#24120;&#23384;&#22312;&#19981;&#21305;&#37197;&#12290;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#21487;&#20197;&#29992;&#20110;&#20197;&#26356;&#22909;&#22320;&#19982;&#26368;&#32456;&#35780;&#20272;&#20934;&#21017;&#23545;&#40784;&#30340;&#26041;&#24335;&#32452;&#21512;&#31995;&#32479;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#31995;&#32479;&#20013;&#30340;MBR&#35299;&#30721;&#65292;&#35813;&#31995;&#32479;&#36890;&#24120;&#20197;&#32534;&#36753;&#27425;&#25968;&#21644;&#30456;&#20851;&#30340;F&#20998;&#25968;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#36825;&#31181;&#20934;&#21017;&#30452;&#25509;&#30456;&#20851;&#30340;&#26032;&#39062;MBR&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#25551;&#36848;&#20102;&#19968;&#31181;&#25193;&#23637;&#20505;&#36873;&#21477;&#23376;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#24403;&#21069;&#30340;&#26368;&#22823;&#25237;&#31080;&#32452;&#21512;&#26041;&#26696;&#65292;&#20197;&#21450;&#20010;&#20307;&#32534;&#36753;&#32423;&#21035;&#30340;&#36873;&#25321;&#12290;&#22312;&#19977;&#20010;&#27969;&#34892;&#30340;GEC&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;GEC&#31995;&#32479;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;MBR&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#31361;&#20986;&#20102;MBR&#35299;&#30721;&#20013;&#19981;&#21516;&#22870;&#21169;&#25351;&#26631;&#30340;&#21464;&#21270;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
For sequence-to-sequence tasks it is challenging to combine individual system outputs. Further, there is also often a mismatch between the decoding criterion and the one used for assessment. Minimum Bayes' Risk (MBR) decoding can be used to combine system outputs in a manner that encourages better alignment with the final assessment criterion. This paper examines MBR decoding for Grammatical Error Correction (GEC) systems, where performance is usually evaluated in terms of edits and an associated F-score. Hence, we propose a novel MBR loss function directly linked to this form of criterion. Furthermore, an approach to expand the possible set of candidate sentences is described. This builds on a current max-voting combination scheme, as well as individual edit-level selection. Experiments on three popular GEC datasets and with state-of-the-art GEC systems demonstrate the efficacy of the proposed MBR approach. Additionally, the paper highlights how varying reward metrics within the MBR d
&lt;/p&gt;</description></item><item><title>Memotion 3&#26159;&#23545;Hinglish&#28151;&#21512;&#35821;&#35328;&#34920;&#24773;&#21253;&#36827;&#34892;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;&#20102;&#21508;&#31181;&#27169;&#22411;&#21644;&#26041;&#27861;&#26469;&#22788;&#29702;&#35813;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.06517</link><description>&lt;p&gt;
Memotion 3&#27010;&#36848;&#65306;Hinglish&#28151;&#21512;&#35821;&#35328;&#34920;&#24773;&#20998;&#26512;&#30340;&#24773;&#24863;&#19982;&#24773;&#32490;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Overview of Memotion 3: Sentiment and Emotion Analysis of Codemixed Hinglish Memes. (arXiv:2309.06517v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06517
&lt;/p&gt;
&lt;p&gt;
Memotion 3&#26159;&#23545;Hinglish&#28151;&#21512;&#35821;&#35328;&#34920;&#24773;&#21253;&#36827;&#34892;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;&#20102;&#21508;&#31181;&#27169;&#22411;&#21644;&#26041;&#27861;&#26469;&#22788;&#29702;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20114;&#32852;&#32593;&#19978;&#30340;&#34920;&#24773;&#21253;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#31181;&#22810;&#27169;&#24335;&#20869;&#23481;&#23545;&#22609;&#36896;&#22312;&#32447;&#35805;&#35821;&#30340;&#24433;&#21709;&#24040;&#22823;&#12290;&#34920;&#24773;&#21253;&#24050;&#25104;&#20026;&#19968;&#31181;&#34920;&#36798;&#24773;&#24863;&#21644;&#24773;&#32490;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#29978;&#33267;&#21487;&#33021;&#36890;&#36807;&#24189;&#40664;&#21644;&#35773;&#21050;&#20256;&#25773;&#20167;&#24680;&#21644;&#38169;&#35823;&#20449;&#24687;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Memotion 3&#20849;&#20139;&#20219;&#21153;&#30340;&#27010;&#36848;&#65292;&#20316;&#20026;AAAI-23&#30340;DeFactify 2&#30740;&#35752;&#20250;&#30340;&#19968;&#37096;&#20998;&#12290;&#35813;&#20219;&#21153;&#21457;&#24067;&#20102;&#19968;&#20221;&#27880;&#37322;&#30340;Hindi-English&#28151;&#21512;&#35821;&#35328;&#34920;&#24773;&#21253;&#25968;&#25454;&#38598;&#65292;&#26681;&#25454;&#24773;&#24863;&#65288;&#20219;&#21153;A&#65289;&#12289;&#24773;&#32490;&#65288;&#20219;&#21153;B&#65289;&#21644;&#24773;&#32490;&#24378;&#24230;&#65288;&#20219;&#21153;C&#65289;&#23545;&#20854;&#36827;&#34892;&#26631;&#27880;&#12290;&#27599;&#20010;&#20219;&#21153;&#37117;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#29420;&#31435;&#30340;&#20219;&#21153;&#65292;&#21442;&#19982;&#32773;&#25353;&#29031;&#27599;&#20010;&#20219;&#21153;&#20998;&#21035;&#25490;&#21517;&#12290;&#36229;&#36807;50&#20010;&#22242;&#38431;&#27880;&#20876;&#21442;&#21152;&#20102;&#35813;&#20849;&#20139;&#20219;&#21153;&#65292;&#20854;&#20013;5&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;Memotion 3&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#38598;&#30340;&#26368;&#32456;&#32467;&#26524;&#12290;&#22312;&#21442;&#19982;&#32773;&#20013;&#65292;CLIP&#12289;BERT&#20462;&#25913;&#29256;&#26412;&#12289;ViT&#31561;&#27169;&#22411;&#26368;&#21463;&#27426;&#36814;&#65292;&#36824;&#26377;&#23398;&#29983;-&#25945;&#24072;&#27169;&#22411;&#12289;&#34701;&#21512;&#21644;&#38598;&#25104;&#31561;&#26041;&#27861;&#12290;&#20219;&#21153;A&#30340;&#26368;&#20339;&#26368;&#32456;F1&#20998;&#25968;&#26159;&#65311;
&lt;/p&gt;
&lt;p&gt;
Analyzing memes on the internet has emerged as a crucial endeavor due to the impact this multi-modal form of content wields in shaping online discourse. Memes have become a powerful tool for expressing emotions and sentiments, possibly even spreading hate and misinformation, through humor and sarcasm. In this paper, we present the overview of the Memotion 3 shared task, as part of the DeFactify 2 workshop at AAAI-23. The task released an annotated dataset of Hindi-English code-mixed memes based on their Sentiment (Task A), Emotion (Task B), and Emotion intensity (Task C). Each of these is defined as an individual task and the participants are ranked separately for each task. Over 50 teams registered for the shared task and 5 made final submissions to the test set of the Memotion 3 dataset. CLIP, BERT modifications, ViT etc. were the most popular models among the participants along with approaches such as Student-Teacher model, Fusion, and Ensembling. The best final F1 score for Task A 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;GPT-4&#27169;&#22411;&#22312;&#21333;&#27425;&#27169;&#24335;&#19979;&#25552;&#20379;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#26469;&#35782;&#21035;COVID-19&#30123;&#33495;&#30456;&#20851;&#25512;&#25991;&#65292;&#24182;&#19982;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2309.06503</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#23545;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;:&#20197;COVID-19&#33258;&#25105;&#25253;&#21578;&#30340;&#30123;&#33495;&#25512;&#25991;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models and Weak Supervision for Social Media data annotation: an evaluation using COVID-19 self-reported vaccination tweets. (arXiv:2309.06503v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;GPT-4&#27169;&#22411;&#22312;&#21333;&#27425;&#27169;&#24335;&#19979;&#25552;&#20379;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#26469;&#35782;&#21035;COVID-19&#30123;&#33495;&#30456;&#20851;&#25512;&#25991;&#65292;&#24182;&#19982;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#22823;&#27969;&#34892;&#32473;&#21307;&#30103;&#34892;&#19994;&#21644;&#25972;&#20010;&#31038;&#20250;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#38543;&#30528;COVID-19&#30123;&#33495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24050;&#25104;&#20026;&#35752;&#35770;&#30123;&#33495;&#30456;&#20851;&#35805;&#39064;&#30340;&#28909;&#38376;&#23186;&#20171;&#12290;&#35782;&#21035;&#30123;&#33495;&#30456;&#20851;&#25512;&#25991;&#24182;&#36827;&#34892;&#20998;&#26512;&#21487;&#20197;&#20026;&#20844;&#20849;&#21355;&#29983;&#30740;&#31350;&#20154;&#21592;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#27880;&#37322;&#22823;&#37327;&#25512;&#25991;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#20363;&#20013;&#20026;GPT-4&#65288;3&#26376;23&#26085;&#29256;&#26412;&#65289;&#65292;&#21644;&#24369;&#30417;&#30563;&#26469;&#35782;&#21035;COVID-19&#30123;&#33495;&#30456;&#20851;&#25512;&#25991;&#65292;&#30446;&#30340;&#26159;&#19982;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#25163;&#21160;&#31574;&#21010;&#30340;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;GPT-4&#22312;&#21333;&#27425;&#27169;&#24335;&#19979;&#65288;&#26080;&#38656;&#39069;&#22806;&#25552;&#31034;&#65289;&#25552;&#20379;&#26631;&#31614;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#25351;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has presented significant challenges to the healthcare industry and society as a whole. With the rapid development of COVID-19 vaccines, social media platforms have become a popular medium for discussions on vaccine-related topics. Identifying vaccine-related tweets and analyzing them can provide valuable insights for public health research-ers and policymakers. However, manual annotation of a large number of tweets is time-consuming and expensive. In this study, we evaluate the usage of Large Language Models, in this case GPT-4 (March 23 version), and weak supervision, to identify COVID-19 vaccine-related tweets, with the purpose of comparing performance against human annotators. We leveraged a manu-ally curated gold-standard dataset and used GPT-4 to provide labels without any additional fine-tuning or instructing, in a single-shot mode (no additional prompting).
&lt;/p&gt;</description></item><item><title>AGIBench&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#31890;&#24230;&#12289;&#22810;&#27169;&#24577;&#12289;&#20154;&#24037;&#21442;&#32771;&#12289;&#33258;&#21160;&#35780;&#20998;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#26631;&#35760;&#38382;&#39064;&#30340;&#23646;&#24615;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#26234;&#33021;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.06495</link><description>&lt;p&gt;
AGIBench: &#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#31890;&#24230;&#12289;&#22810;&#27169;&#24577;&#12289;&#20154;&#24037;&#21442;&#32771;&#12289;&#33258;&#21160;&#35780;&#20998;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
AGIBench: A Multi-granularity, Multimodal, Human-referenced, Auto-scoring Benchmark for Large Language Models. (arXiv:2309.06495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06495
&lt;/p&gt;
&lt;p&gt;
AGIBench&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#31890;&#24230;&#12289;&#22810;&#27169;&#24577;&#12289;&#20154;&#24037;&#21442;&#32771;&#12289;&#33258;&#21160;&#35780;&#20998;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#26631;&#35760;&#38382;&#39064;&#30340;&#23646;&#24615;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#26234;&#33021;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#26234;&#33021;&#12290;&#22914;&#20309;&#35780;&#20272;LLM&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#26234;&#33021;&#31243;&#24230;&#26159;&#19968;&#20010;&#28909;&#28857;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#19982;&#19981;&#21516;&#30340;&#33021;&#21147;&#20998;&#25903;&#65288;&#22914;&#29702;&#35299;&#65289;&#21644;&#22823;&#35268;&#27169;&#30340;&#30693;&#35782;&#31867;&#21035;&#65288;&#22914;&#25968;&#23398;&#65289;&#20132;&#32455;&#22312;&#19968;&#36215;&#12290;&#31532;&#20108;&#65292;&#38382;&#39064;&#30340;&#36755;&#20837;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#21487;&#33021;&#28041;&#21450;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#31532;&#19977;&#65292;LLM&#30340;&#21709;&#24212;&#26684;&#24335;&#22810;&#26679;&#65292;&#22240;&#27492;&#23545;&#32467;&#26524;&#25552;&#21462;&#21644;&#35780;&#20272;&#25552;&#20986;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AGIBench--&#19968;&#31181;&#29992;&#20110;LLM&#30340;&#22810;&#31890;&#24230;&#12289;&#22810;&#27169;&#24577;&#12289;&#20154;&#24037;&#21442;&#32771;&#21644;&#33258;&#21160;&#35780;&#20998;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#19982;&#28151;&#21512;&#38382;&#39064;&#38598;&#21512;&#19981;&#21516;&#65292;AGIBench&#19987;&#27880;&#20110;&#19977;&#20010;&#20856;&#22411;&#30340;&#33021;&#21147;&#20998;&#25903;&#65292;&#24182;&#37319;&#29992;&#22235;&#20803;&#32452;&lt;&#33021;&#21147;&#20998;&#25903;&#12289;&#30693;&#35782;&#12289;&#38590;&#24230;&#12289;&#27169;&#24577;&gt;&#26469;&#26631;&#35760;&#27599;&#20010;&#38382;&#39064;&#30340;&#23646;&#24615;&#12290;&#39318;&#20808;&#65292;&#23427;&#25903;&#25345;&#22810;&#31890;&#24230;&#30340;&#22522;&#20934;&#21270;&#65292;&#20363;&#22914;&#27599;&#20010;&#38382;&#39064;&#12289;&#27599;&#20010;&#33021;&#21147;&#20998;&#25903;&#12289;&#27599;&#20010;&#30693;&#35782;&#31867;&#21035;&#30340;&#22522;&#20934;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like ChatGPT have revealed amazing intelligence. How to evaluate the question-solving abilities of LLMs and their degrees of intelligence is a hot-spot but challenging issue. First, the question-solving abilities are interlaced with different ability branches like understanding and massive knowledge categories like mathematics. Second, the inputs of questions are multimodal that may involve text and images. Third, the response format of LLMs is diverse and thus poses great challenges for result extraction and evaluation. In this paper, we propose AGIBench -- a multi-granularity, multimodal, human-referenced, and auto-scoring benchmarking methodology for LLMs. Instead of a collection of blended questions, AGIBench focuses on three typical ability branches and adopts a four-tuple &lt;ability branch, knowledge, difficulty, modal&gt; to label the attributes of each question. First, it supports multi-granularity benchmarking, e.g., per-question, per-ability branch, pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT-3.5&#36827;&#34892;&#33258;&#21160;&#23545;&#35805;&#34892;&#20026;&#26816;&#27979;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#19982;&#19987;&#38376;&#27169;&#22411;&#21644;&#20154;&#31867;&#34920;&#29616;&#30340;&#21305;&#37197;&#24230;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#31181;&#27169;&#22411;&#33021;&#22815;&#20196;&#20154;&#28385;&#24847;&#22320;&#23454;&#29616;&#36825;&#19968;&#20219;&#21153;&#65292;&#36798;&#21040;&#20154;&#31867;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2309.06490</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#23545;&#35805;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Automated Dialogue Analysis. (arXiv:2309.06490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT-3.5&#36827;&#34892;&#33258;&#21160;&#23545;&#35805;&#34892;&#20026;&#26816;&#27979;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#19982;&#19987;&#38376;&#27169;&#22411;&#21644;&#20154;&#31867;&#34920;&#29616;&#30340;&#21305;&#37197;&#24230;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#31181;&#27169;&#22411;&#33021;&#22815;&#20196;&#20154;&#28385;&#24847;&#22320;&#23454;&#29616;&#36825;&#19968;&#20219;&#21153;&#65292;&#36798;&#21040;&#20154;&#31867;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#39640;&#24615;&#33021;&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;&#22238;&#24212;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#24191;&#27867;&#30340;&#24120;&#35782;&#21644;&#23545;&#35805;&#23454;&#36341;&#30340;&#29702;&#35299;&#65292;&#36825;&#31181;&#34892;&#20026;&#30340;&#26816;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#26500;&#24314;&#29992;&#20110;&#26816;&#27979;&#29305;&#23450;&#23545;&#35805;&#34892;&#20026;&#30340;&#29305;&#27530;&#20998;&#31867;&#22120;&#65292;&#20294;&#34892;&#20026;&#35206;&#30422;&#20173;&#28982;&#19981;&#23436;&#25972;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#30495;&#23454;&#20154;&#26426;&#20132;&#20114;&#30340;&#27979;&#35797;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;ChatGPT-3.5&#22312;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25191;&#34892;&#20061;&#20010;&#31867;&#21035;&#30340;&#23545;&#35805;&#34892;&#20026;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;ChatGPT&#26159;&#21542;&#33021;&#22815;&#19982;&#19987;&#38376;&#27169;&#22411;&#30456;&#21305;&#37197;&#24182;&#25509;&#36817;&#20154;&#31867;&#34920;&#29616;&#65292;&#20174;&#32780;&#38477;&#20302;&#34892;&#20026;&#26816;&#27979;&#20219;&#21153;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26080;&#35770;&#26159;&#19987;&#38376;&#27169;&#22411;&#36824;&#26159;ChatGPT&#37117;&#23578;&#26410;&#36798;&#21040;&#35813;&#20219;&#21153;&#30340;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#26410;&#33021;&#36798;&#21040;&#20154;&#31867;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing high-performing dialogue systems benefits from the automatic identification of undesirable behaviors in system responses. However, detecting such behaviors remains challenging, as it draws on a breadth of general knowledge and understanding of conversational practices. Although recent research has focused on building specialized classifiers for detecting specific dialogue behaviors, the behavior coverage is still incomplete and there is a lack of testing on real-world human-bot interactions. This paper investigates the ability of a state-of-the-art large language model (LLM), ChatGPT-3.5, to perform dialogue behavior detection for nine categories in real human-bot dialogues. We aim to assess whether ChatGPT can match specialized models and approximate human performance, thereby reducing the cost of behavior detection tasks. Our findings reveal that neither specialized models nor ChatGPT have yet achieved satisfactory results for this task, falling short of human performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WISeR&#30340;&#35821;&#20041;&#34920;&#31034;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#22312;&#24212;&#29992;&#20110;&#27809;&#26377;&#39044;&#23450;&#20041;&#35821;&#20041;&#26694;&#26550;&#30340;&#35821;&#35328;&#25110;&#39046;&#22495;&#26102;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;AMR&#20013;&#30340;&#32534;&#21495;&#21442;&#25968;&#36716;&#25442;&#20026;&#19981;&#38656;&#35201;&#24341;&#29992;&#35821;&#20041;&#26694;&#26550;&#30340;&#35821;&#20041;&#35282;&#33394;&#65292;WISeR&#25552;&#20379;&#20102;&#26356;&#26131;&#20110;&#35299;&#37322;&#21644;&#35299;&#26512;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;&#26631;&#27880;&#32773;&#19968;&#33268;&#24615;&#21644;&#35299;&#26512;&#22120;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.06460</link><description>&lt;p&gt;
&#24191;&#27867;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#34920;&#31034;&#65306;&#38754;&#21521;&#26356;&#24191;&#27867;&#24212;&#29992;&#30340;&#26080;&#26694;&#26550;&#24847;&#20041;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Widely Interpretable Semantic Representation: Frameless Meaning Representation for Broader Applicability. (arXiv:2309.06460v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WISeR&#30340;&#35821;&#20041;&#34920;&#31034;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#22312;&#24212;&#29992;&#20110;&#27809;&#26377;&#39044;&#23450;&#20041;&#35821;&#20041;&#26694;&#26550;&#30340;&#35821;&#35328;&#25110;&#39046;&#22495;&#26102;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;AMR&#20013;&#30340;&#32534;&#21495;&#21442;&#25968;&#36716;&#25442;&#20026;&#19981;&#38656;&#35201;&#24341;&#29992;&#35821;&#20041;&#26694;&#26550;&#30340;&#35821;&#20041;&#35282;&#33394;&#65292;WISeR&#25552;&#20379;&#20102;&#26356;&#26131;&#20110;&#35299;&#37322;&#21644;&#35299;&#26512;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;&#26631;&#27880;&#32773;&#19968;&#33268;&#24615;&#21644;&#35299;&#26512;&#22120;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#34920;&#31034;&#26041;&#24335;&#65292;WISeR&#65292;&#23427;&#20811;&#26381;&#20102;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;AMR&#20855;&#26377;&#24456;&#22810;&#20248;&#21183;&#65292;&#20294;&#23427;&#22312;&#27809;&#26377;&#39044;&#23450;&#20041;&#35821;&#20041;&#26694;&#26550;&#30340;&#35821;&#35328;&#25110;&#39046;&#22495;&#20013;&#19981;&#23481;&#26131;&#24212;&#29992;&#65292;&#24182;&#19988;&#20854;&#20351;&#29992;&#32534;&#21495;&#21442;&#25968;&#20250;&#23548;&#33268;&#35821;&#20041;&#35282;&#33394;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#19981;&#30452;&#25509;&#21487;&#35299;&#37322;&#65292;&#24182;&#19988;&#23545;&#35299;&#26512;&#22120;&#26469;&#35828;&#20855;&#26377;&#35821;&#20041;&#36807;&#36733;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;AMR&#20013;&#35859;&#35789;&#30340;&#32534;&#21495;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#19981;&#38656;&#35201;&#24341;&#29992;&#35821;&#20041;&#26694;&#26550;&#30340;&#35821;&#20041;&#35282;&#33394;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;1K&#33521;&#35821;&#23545;&#35805;&#21477;&#23376;&#65292;&#26631;&#27880;&#20102;WISeR&#21644;AMR&#12290;WISeR&#22312;&#21021;&#23398;&#32773;&#21644;&#26377;&#32463;&#39564;&#30340;&#26631;&#27880;&#32773;&#20043;&#38388;&#20855;&#26377;&#26356;&#24378;&#30340;&#26631;&#27880;&#32773;&#19968;&#33268;&#24615;&#65292;&#21021;&#23398;&#32773;&#26356;&#24555;&#22320;&#25484;&#25569;&#20102;WISeR&#30340;&#26631;&#27880;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;AMR 3.0&#35821;&#26009;&#24211;&#21644;&#20174;AMR 3.0&#36716;&#25442;&#32780;&#26469;&#30340;WISeR&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#35299;&#26512;&#22120;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#35821;&#26009;&#24211;&#21644;&#23545;&#35805;&#35821;&#26009;&#24211;&#19978;&#35780;&#20272;&#20102;WISeR&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel semantic representation, WISeR, that overcomes challenges for Abstract Meaning Representation (AMR). Despite its strengths, AMR is not easily applied to languages or domains without predefined semantic frames, and its use of numbered arguments results in semantic role labels, which are not directly interpretable and are semantically overloaded for parsers. We examine the numbered arguments of predicates in AMR and convert them to thematic roles that do not require reference to semantic frames. We create a new corpus of 1K English dialogue sentences annotated in both WISeR and AMR. WISeR shows stronger inter-annotator agreement for beginner and experienced annotators, with beginners becoming proficient in WISeR annotation more quickly. Finally, we train a state-of-the-art parser on the AMR 3.0 corpus and a WISeR corpus converted from AMR 3.0. The parser is evaluated on these corpora and our dialogue corpus. The WISeR model exhibits higher accuracy than its AM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.06453</link><description>&lt;p&gt;
&#32553;&#23567;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#24046;&#36317;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model. (arXiv:2309.06453v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#23545;&#27604;&#23398;&#20064;&#30340;&#21477;&#23376;&#23884;&#20837;&#65288;CSE&#65289;&#20316;&#20026;&#20027;&#27969;&#25216;&#26415;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;CSE&#20013;&#26377;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#21363;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#21644;&#25439;&#22833;&#20989;&#25968;&#30456;&#21516;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#22238;&#31572;&#8220;&#21457;&#29983;&#20102;&#20160;&#20040;&#23548;&#33268;&#20102;&#24615;&#33021;&#24046;&#36317;&#8221;&#21644;&#8220;&#22914;&#20309;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#8221;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24443;&#24213;&#27604;&#36739;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;CSE&#22312;&#21508;&#33258;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#26469;&#22238;&#31572;&#8220;&#21457;&#29983;&#20102;&#20160;&#20040;&#8221;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence Representation Learning (SRL) is a fundamental task in Natural Language Processing (NLP), with Contrastive learning of Sentence Embeddings (CSE) as the mainstream technique due to its superior performance. An intriguing phenomenon in CSE is the significant performance gap between supervised and unsupervised methods, even when their sentence encoder and loss function are the same. Previous works attribute this performance gap to differences in two representation properties (alignment and uniformity). However, alignment and uniformity only measure the results, which means they cannot answer "What happens during the training process that leads to the performance gap?" and "How can the performance gap be narrowed?". In this paper, we conduct empirical experiments to answer these "What" and "How" questions. We first answer the "What" question by thoroughly comparing the behavior of supervised and unsupervised CSE during their respective training processes. From the comparison, We o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#25104;&#27169;&#22411;&#23558;&#30693;&#35782;&#24211;&#19982;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#25361;&#25112;&#12290;&#36890;&#36807;&#25193;&#23637;&#30693;&#35782;&#24211;&#21644;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#21484;&#22238;&#29575;&#65292;&#24182;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#21644;&#22810;&#20803;&#21152;&#24615;&#22238;&#24402;&#26641;&#36807;&#28388;&#32467;&#26524;&#24471;&#21040;&#39640;&#31934;&#24230;&#30340;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;0.535&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.06175</link><description>&lt;p&gt;
AKEM: &#21033;&#29992;&#38598;&#25104;&#27169;&#22411;&#23558;&#30693;&#35782;&#24211;&#19982;&#26597;&#35810;&#23545;&#40784;&#20197;&#36827;&#34892;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
AKEM: Aligning Knowledge Base to Queries with Ensemble Model for Entity Recognition and Linking. (arXiv:2309.06175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#25104;&#27169;&#22411;&#23558;&#30693;&#35782;&#24211;&#19982;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#25361;&#25112;&#12290;&#36890;&#36807;&#25193;&#23637;&#30693;&#35782;&#24211;&#21644;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#21484;&#22238;&#29575;&#65292;&#24182;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#21644;&#22810;&#20803;&#21152;&#24615;&#22238;&#24402;&#26641;&#36807;&#28388;&#32467;&#26524;&#24471;&#21040;&#39640;&#31934;&#24230;&#30340;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;0.535&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;NLPCC 2015&#20013;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#20219;&#21153;&#21253;&#25324;&#20174;&#30701;&#25628;&#32034;&#26597;&#35810;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#30340;&#25552;&#21450;&#65292;&#24182;&#23558;&#20854;&#38142;&#25509;&#21040;&#21442;&#32771;&#20013;&#25991;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25193;&#23637;&#29616;&#26377;&#30693;&#35782;&#24211;&#65292;&#24182;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#35782;&#21035;&#20505;&#36873;&#23454;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#21484;&#22238;&#29575;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#20505;&#36873;&#23454;&#20307;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#21644;&#22810;&#20803;&#21152;&#24615;&#22238;&#24402;&#26641;&#20316;&#20026;&#35780;&#20998;&#20989;&#25968;&#26469;&#36807;&#28388;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#35268;&#21017;&#26469;&#36827;&#19968;&#27493;&#32454;&#21270;&#32467;&#26524;&#21644;&#25552;&#39640;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#36798;&#21040;&#20102;0.535&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to address the Entity Recognition and Linking Challenge at NLPCC 2015. The task involves extracting named entity mentions from short search queries and linking them to entities within a reference Chinese knowledge base. To tackle this problem, we first expand the existing knowledge base and utilize external knowledge to identify candidate entities, thereby improving the recall rate. Next, we extract features from the candidate entities and utilize Support Vector Regression and Multiple Additive Regression Tree as scoring functions to filter the results. Additionally, we apply rules to further refine the results and enhance precision. Our method is computationally efficient and achieves an F1 score of 0.535.
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#30340;&#21407;&#22240;&#26159;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#21487;&#20197;&#20381;&#36182;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#23427;&#20204;&#23384;&#20648;&#30340;&#35821;&#35328;&#30693;&#35782;&#22475;&#34255;&#22312;&#26080;&#24847;&#20041;&#30340;&#24494;&#29305;&#24449;&#20013;&#65292;&#24182;&#22312;&#26576;&#20123;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#12290;&#26412;&#25991;&#24314;&#35758;&#22312;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;&#24212;&#29992;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;</title><link>http://arxiv.org/abs/2309.05918</link><description>&lt;p&gt;
&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#65306;&#36208;&#21521;&#31526;&#21495;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#26412;&#20307;&#35770;&#22522;&#20110;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs. (arXiv:2309.05918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05918
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#30340;&#21407;&#22240;&#26159;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#21487;&#20197;&#20381;&#36182;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#23427;&#20204;&#23384;&#20648;&#30340;&#35821;&#35328;&#30693;&#35782;&#22475;&#34255;&#22312;&#26080;&#24847;&#20041;&#30340;&#24494;&#29305;&#24449;&#20013;&#65292;&#24182;&#22312;&#26576;&#20123;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#12290;&#26412;&#25991;&#24314;&#35758;&#22312;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;&#24212;&#29992;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30475;&#26469;&#65292;&#22260;&#32469;&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#23545;&#25104;&#21151;&#30340;&#29378;&#28909;&#26159;&#26377;&#20123;&#35823;&#23548;&#30340;&#65292;&#21407;&#22240;&#22914;&#19979;&#65306;&#65288;i&#65289;LLMs&#19981;&#33021;&#20381;&#36182;&#20110;&#20107;&#23454;&#20449;&#24687;&#65292;&#22240;&#20026;&#23545;&#20110;LLMs&#26469;&#35828;&#65292;&#25668;&#20837;&#30340;&#25152;&#26377;&#25991;&#26412;&#65288;&#20107;&#23454;&#25110;&#38750;&#20107;&#23454;&#65289;&#37117;&#26159;&#24179;&#31561;&#30340;&#65307;&#65288;ii&#65289;&#30001;&#20110;&#23427;&#20204;&#30340;&#20122;&#31526;&#21495;&#24615;&#36136;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#35821;&#35328;&#30340;&#20219;&#20309;&#8220;&#30693;&#35782;&#8221;&#37117;&#23558;&#27704;&#36828;&#22475;&#34255;&#22312;&#25968;&#21313;&#20159;&#20010;&#24494;&#29305;&#24449;&#65288;&#26435;&#37325;&#65289;&#20013;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#26412;&#36523;&#26159;&#26377;&#24847;&#20041;&#30340;&#65307;&#20197;&#21450;&#65288;iii&#65289;LLMs&#22312;&#20960;&#31181;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#24120;&#24120;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#65288;&#22914;&#21517;&#35789;&#22797;&#21512;&#35789;&#12289;&#20849;&#35859;&#35789;&#12289;&#37327;&#35789;&#33539;&#22260;&#27169;&#31946;&#21644;&#24847;&#21521;&#24615;&#19978;&#19979;&#25991;&#65289;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30456;&#23545;&#25104;&#21151;&#19981;&#26159;&#31526;&#21495;&#19982;&#20122;&#31526;&#21495;&#20043;&#36777;&#30340;&#21453;&#26144;&#65292;&#32780;&#26159;&#22312;&#35268;&#27169;&#19978;&#24212;&#29992;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#35821;&#35328;&#30340;&#25104;&#21151;&#31574;&#30053;&#30340;&#21453;&#26144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;&#24212;&#29992;&#20110;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;
&lt;/p&gt;
&lt;p&gt;
In our opinion the exuberance surrounding the relative success of data-driven large language models (LLMs) is slightly misguided and for several reasons (i) LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic na-ture, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (iii) LLMs will often fail to make the correct inferences in several linguistic contexts (e.g., nominal compounds, copredication, quantifier scope ambi-guities, intensional contexts. Since we believe the relative success of data-driven large language models (LLMs) is not a reflection on the symbolic vs. subsymbol-ic debate but a reflection on applying the successful strategy of a bottom-up reverse engineering of language at scale, we suggest in this paper applying the effective bottom-up strategy in a symbol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21521;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#30340;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#23450;&#21521;&#27880;&#20837;&#20869;&#23384;&#26469;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.05605</link><description>&lt;p&gt;
&#20869;&#23384;&#27880;&#20837;&#65306;&#22312;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#20013;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models. (arXiv:2309.05605v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21521;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#30340;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#23450;&#21521;&#27880;&#20837;&#20869;&#23384;&#26469;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31572;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#38656;&#35201;&#20174;&#22810;&#20010;&#20449;&#24687;&#28304;&#20013;&#26816;&#32034;&#21644;&#32508;&#21512;&#20449;&#24687;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24448;&#24448;&#38590;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#36827;&#34892;&#23450;&#21521;&#20869;&#23384;&#27880;&#20837;&#26469;&#30830;&#23450;&#21644;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GPT-2&#27169;&#22411;&#22312;&#21333;&#36339;&#21644;&#22810;&#36339;&#25552;&#31034;&#19979;&#21508;&#23618;&#30340;&#28608;&#27963;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21521;&#20851;&#38190;LLM&#20301;&#32622;&#27880;&#20837;&#30456;&#20851;&#30340;&#25552;&#31034;&#29305;&#23450;&#20449;&#24687;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#35760;&#24518;&#8221;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;LLM&#33021;&#22815;&#25972;&#21512;&#39069;&#22806;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#22810;&#36339;&#25552;&#31034;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;&#23558;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#23450;&#21521;&#30340;&#35760;&#24518;&#27880;&#20837;&#21040;&#20851;&#38190;&#27880;&#24847;&#21147;&#23618;&#20013;&#24448;&#24448;&#33021;&#22815;&#25552;&#39640;&#22810;&#36339;&#20219;&#21153;&#20013;&#25152;&#38656;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#25552;&#39640;&#20102;&#36798;&#21040;424%&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.
&lt;/p&gt;</description></item><item><title>NExT-GPT&#26159;&#19968;&#20010;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#21644;&#19981;&#21516;&#25193;&#25955;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#25509;&#21463;&#21644;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2309.05519</link><description>&lt;p&gt;
NExT-GPT: &#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NExT-GPT: Any-to-Any Multimodal LLM. (arXiv:2309.05519v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05519
&lt;/p&gt;
&lt;p&gt;
NExT-GPT&#26159;&#19968;&#20010;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#21644;&#19981;&#21516;&#25193;&#25955;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#25509;&#21463;&#21644;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLM&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#25391;&#22859;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#23384;&#22312;&#19968;&#20010;&#38480;&#21046;&#65292;&#21363;&#21482;&#33021;&#22312;&#36755;&#20837;&#31471;&#36827;&#34892;&#22810;&#27169;&#24577;&#29702;&#35299;&#65292;&#26080;&#27861;&#20197;&#22810;&#31181;&#27169;&#24335;&#29983;&#25104;&#20869;&#23481;&#12290;&#30001;&#20110;&#25105;&#20204;&#20154;&#31867;&#24635;&#26159;&#36890;&#36807;&#21508;&#31181;&#27169;&#24577;&#24863;&#30693;&#19990;&#30028;&#21644;&#19982;&#20154;&#20132;&#27969;&#65292;&#22240;&#27492;&#24320;&#21457;&#33021;&#22815;&#25509;&#21463;&#21644;&#20256;&#36882;&#20219;&#20309;&#27169;&#24577;&#20869;&#23481;&#30340;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;MM-LLM&#31995;&#32479;&#23545;&#20110;&#23454;&#29616;&#20154;&#32423;AI&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#36890;&#29992;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#65292;NExT-GPT&#12290;&#25105;&#20204;&#36890;&#36807;&#36830;&#25509;&#19968;&#20010;&#21547;&#26377;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#21644;&#19981;&#21516;&#25193;&#25955;&#35299;&#30721;&#22120;&#30340;LLM&#65292;&#20351;&#24471;NExT-GPT&#33021;&#22815;&#20197;&#20219;&#24847;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#32452;&#21512;&#36827;&#34892;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#35757;&#32451;&#26377;&#32032;&#30340;&#39640;&#24615;&#33021;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;NExT-GPT&#20165;&#36890;&#36807;&#35843;&#25972;&#26576;&#20123;&#25237;&#24433;&#23618;&#30340;&#23569;&#37327;&#21442;&#25968;&#65288;1%&#65289;&#36827;&#34892;&#35843;&#20248;&#65292;&#36825;&#19981;&#20165;&#26377;&#21033;&#20110;&#20302;&#25104;&#26412;&#35757;&#32451;&#65292;&#36824;&#26377;&#21161;&#20110;&#26041;&#20415;&#30340;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#37327;&#21270;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#22914;&#20309;&#21709;&#24212;&#36229;&#21442;&#25968;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.05210</link><description>&lt;p&gt;
&#29702;&#35299;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Impact of Post-Training Quantization on Large Language Models. (arXiv:2309.05210v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#37327;&#21270;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#22914;&#20309;&#21709;&#24212;&#36229;&#21442;&#25968;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35268;&#27169;&#36805;&#36895;&#22686;&#21152;&#65292;&#21442;&#25968;&#25968;&#37327;&#25104;&#20026;&#35768;&#22810;&#21830;&#19994;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#22914;ChatGPT&#12289;Claude&#21644;Bard&#12290;&#21363;&#20351;&#26159;&#26368;&#36817;&#21457;&#24067;&#30340;&#29992;&#20110;&#21830;&#19994;&#29992;&#36884;&#30340;&#20844;&#24320;&#21487;&#35265;&#27169;&#22411;&#65292;&#22914;Falcon&#21644;Llama2&#65292;&#20063;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#21442;&#25968;&#25968;&#37327;&#30340;&#26174;&#33879;&#22686;&#21152;&#20351;&#24471;&#37096;&#32626;&#21644;&#36816;&#34892;&#38750;&#24120;&#26114;&#36149;&#12290;&#37327;&#21270;&#39046;&#22495;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;LLMs&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#28040;&#36153;&#32423;GPU&#19978;&#37096;&#32626;&#65292;&#20174;&#32780;&#20351;&#20854;&#26356;&#26131;&#33719;&#24471;&#12290;&#37327;&#21270;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#20986;&#19982;&#20854;&#26410;&#37327;&#21270;&#22522;&#20934;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35832;&#22914;&#28201;&#24230;&#12289;&#26368;&#22823;&#26032;&#26631;&#35760;&#25968;&#21644;topk&#31561;&#36229;&#21442;&#25968;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#37327;&#21270;&#27169;&#22411;&#22914;&#20309;&#21709;&#24212;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are rapidly increasing in size, with the number of parameters becoming a key factor in the success of many commercial models, such as ChatGPT, Claude, and Bard. Even the recently released publicly accessible models for commercial usage, such as Falcon and Llama2, come equipped with billions of parameters. This significant increase in the number of parameters makes deployment and operation very costly. The remarkable progress in the field of quantization for large neural networks in general and LLMs in particular, has made these models more accessible by enabling them to be deployed on consumer-grade GPUs. Quantized models generally demonstrate comparable performance levels to their unquantized base counterparts. Nonetheless, there exists a notable gap in our comprehensive understanding of how these quantized models respond to hyperparameters, such as temperature, max new tokens, and topk, particularly for next word prediction. The present analysis reveals t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;ChatRule&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#33021;&#22815;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.01538</link><description>&lt;p&gt;
ChatRule&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning. (arXiv:2309.01538v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;ChatRule&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#33021;&#22815;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#35268;&#21017;&#23545;&#20110;&#21457;&#29616;&#20851;&#31995;&#20043;&#38388;&#30340;&#36923;&#36753;&#36830;&#25509;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#35889;&#32467;&#26524;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#21162;&#21147;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#25366;&#25496;&#26377;&#24847;&#20041;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#35268;&#21017;&#31354;&#38388;&#19978;&#25628;&#32034;&#35745;&#31639;&#23494;&#38598;&#19988;&#32570;&#20047;&#21487;&#20280;&#32553;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24120;&#24120;&#24573;&#35270;&#20102;&#20851;&#31995;&#30340;&#35821;&#20041;&#65292;&#32780;&#36825;&#23545;&#20110;&#25581;&#31034;&#36923;&#36753;&#36830;&#25509;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21644;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#24402;&#21151;&#20110;&#23427;&#20204;&#30340;&#26032;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;ChatRule&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20197;&#22522;&#20110;LLM&#30340;&#35268;&#21017;&#29983;&#25104;&#22120;&#20026;&#21021;&#22987;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical rules are essential for uncovering the logical connections between relations, which could improve the reasoning performance and provide interpretable results on knowledge graphs (KGs). Although there have been many efforts to mine meaningful logical rules over KGs, existing methods suffer from the computationally intensive searches over the rule space and a lack of scalability for large-scale KGs. Besides, they often ignore the semantics of relations which is crucial for uncovering logical connections. Recently, large language models (LLMs) have shown impressive performance in the field of natural language processing and various applications, owing to their emergent ability and generalizability. In this paper, we propose a novel framework, ChatRule, unleashing the power of large language models for mining logical rules over knowledge graphs. Specifically, the framework is initiated with an LLM-based rule generator, leveraging both the semantic and structural information of KGs 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25340;&#20889;&#32416;&#27491;&#30340;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#27169;&#25311;&#25991;&#26412;&#20013;&#30340;&#33258;&#28982;&#25340;&#20889;&#38169;&#35823;&#21644;&#25171;&#23383;&#38169;&#35823;&#65292;&#20197;&#26377;&#25928;&#20016;&#23500;&#29983;&#25104;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#35821;&#35328;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2308.09435</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#36328;&#22810;&#20010;&#39046;&#22495;&#21644;&#35821;&#35328;&#30340;&#33258;&#28982;&#25340;&#20889;&#38169;&#35823;&#29983;&#25104;&#25340;&#20889;&#32416;&#27491;&#30340;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Methodology for Generative Spelling Correction via Natural Spelling Errors Emulation across Multiple Domains and Languages. (arXiv:2308.09435v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25340;&#20889;&#32416;&#27491;&#30340;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#27169;&#25311;&#25991;&#26412;&#20013;&#30340;&#33258;&#28982;&#25340;&#20889;&#38169;&#35823;&#21644;&#25171;&#23383;&#38169;&#35823;&#65292;&#20197;&#26377;&#25928;&#20016;&#23500;&#29983;&#25104;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#35821;&#35328;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25991;&#26412;&#29983;&#25104;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#21040;&#32416;&#27491;&#25340;&#20889;&#38169;&#35823;&#21644;&#25171;&#23383;&#38169;&#35823;&#26102;&#65292;&#23427;&#20204;&#36890;&#24120;&#38590;&#20197;&#35299;&#20915;&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25340;&#20889;&#32416;&#27491;&#30340;&#26041;&#27861;&#35770;&#65292;&#35813;&#26041;&#27861;&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19988;&#22312;&#31245;&#20316;&#20462;&#25913;&#21518;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#33258;&#28982;&#25340;&#20889;&#38169;&#35823;&#21644;&#25171;&#23383;&#38169;&#35823;&#65292;&#24182;&#30740;&#31350;&#36825;&#20123;&#38169;&#35823;&#21487;&#20197;&#22914;&#20309;&#22312;&#27491;&#30830;&#30340;&#21477;&#23376;&#20013;&#27169;&#25311;&#65292;&#20197;&#26377;&#25928;&#20016;&#23500;&#29983;&#25104;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#27169;&#25311;&#30340;&#24433;&#21709;&#21644;&#27169;&#22411;&#22312;&#19981;&#21516;&#25991;&#26412;&#39046;&#22495;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#25340;&#20889;&#30772;&#22351;&#25216;&#26415;&#65306;1&#65289;&#31532;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#29305;&#23450;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#32479;&#35745;&#26469;&#27169;&#25311;&#20154;&#31867;&#29359;&#38169;&#35823;&#26102;&#30340;&#34892;&#20026;&#65307;2&#65289;&#31532;&#20108;&#31181;&#26159;&#28155;&#21152;&#26368;&#24120;&#35265;&#30340;&#25340;&#20889;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern large language models demonstrate impressive capabilities in text generation and generalization. However, they often struggle with solving text editing tasks, particularly when it comes to correcting spelling errors and mistypings. In this paper, we present a methodology for generative spelling correction (SC), which was tested on English and Russian languages and potentially can be extended to any language with minor changes. Our research mainly focuses on exploring natural spelling errors and mistypings in texts and studying the ways those errors can be emulated in correct sentences to effectively enrich generative models' pre-train procedure. We investigate the impact of such emulations and the models' abilities across different text domains. In this work, we investigate two spelling corruption techniques: 1) first one mimics human behavior when making a mistake through leveraging statistics of errors from particular dataset and 2) second adds the most common spelling errors,
&lt;/p&gt;</description></item><item><title>SeACo-Paraformer&#26159;&#19968;&#31181;&#20855;&#26377;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#28909;&#35789;&#33258;&#23450;&#20041;&#33021;&#21147;&#30340;&#38750;&#33258;&#22238;&#24402;ASR&#31995;&#32479;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36807;&#28388;&#22823;&#35268;&#27169;&#28909;&#35789;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.03266</link><description>&lt;p&gt;
SeACo-Paraformer:&#19968;&#31181;&#20855;&#26377;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#28909;&#35789;&#33258;&#23450;&#20041;&#33021;&#21147;&#30340;&#38750;&#33258;&#22238;&#24402;ASR&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability. (arXiv:2308.03266v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03266
&lt;/p&gt;
&lt;p&gt;
SeACo-Paraformer&#26159;&#19968;&#31181;&#20855;&#26377;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#28909;&#35789;&#33258;&#23450;&#20041;&#33021;&#21147;&#30340;&#38750;&#33258;&#22238;&#24402;ASR&#31995;&#32479;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36807;&#28388;&#22823;&#35268;&#27169;&#28909;&#35789;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#35789;&#33258;&#23450;&#20041;&#26159;ASR&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#33258;&#23450;&#20041;&#23454;&#20307;&#12289;&#20154;&#29289;&#21644;&#20854;&#20182;&#30701;&#35821;&#30340;&#21517;&#31216;&#20855;&#26377;&#20215;&#20540;&#12290;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;ASR&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#38544;&#24335;&#21644;&#26174;&#24335;&#24314;&#27169;&#31574;&#30053;&#37117;&#24471;&#21040;&#20102;&#21457;&#23637;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#36824;&#19981;&#38169;&#65292;&#20294;&#20173;&#23384;&#22312;&#26576;&#20123;&#32570;&#28857;&#65292;&#20363;&#22914;&#22312;&#25928;&#26524;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#20041;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;Paraformer (SeACo-Paraformer)&#30340;&#38750;&#33258;&#22238;&#24402;ASR&#31995;&#32479;&#65292;&#20855;&#26377;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#28909;&#35789;&#33258;&#23450;&#20041;&#33021;&#21147;&#12290;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;AED&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#22522;&#20110;NAR&#27169;&#22411;&#30340;&#25928;&#29575;&#20197;&#21450;&#22312;&#19978;&#19979;&#25991;&#24314;&#27169;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;&#22312;50,000&#23567;&#26102;&#30340;&#24037;&#19994;&#22823;&#25968;&#25454;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#33258;&#23450;&#20041;&#21644;&#24120;&#35268;ASR&#20219;&#21153;&#20013;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#22823;&#35268;&#27169;&#30340;&#28909;&#35789;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hotword customization is one of the important issues remained in ASR field it is of value to enable users of ASR systems to customize names of entities, persons and other phrases. The past few years have seen both implicit and explicit modeling strategies for ASR contextualization developed. While these approaches have performed adequately, they still exhibit certain shortcomings such as instability in effectiveness. In this paper we propose Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) a novel NAR based ASR system with flexible and effective hotword customization ability. It combines the accuracy of the AED-based model, the efficiency of the NAR model, and the excellent performance in contextualization. In 50,000 hours industrial big data experiments, our proposed model outperforms strong baselines in customization and general ASR tasks. Besides, we explore an efficient way to filter large scale incoming hotwords for further improvement. The source codes and industrial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#25968;&#23398;&#25512;&#29702;&#26102;&#30340;&#35268;&#27169;&#20851;&#31995;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#25439;&#22833;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25298;&#32477;&#37319;&#26679;&#24494;&#35843;&#25216;&#26415;&#26469;&#22686;&#24378;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.01825</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#25968;&#23398;&#25512;&#29702;&#30340;&#35268;&#27169;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Scaling Relationship on Learning Mathematical Reasoning with Large Language Models. (arXiv:2308.01825v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#25968;&#23398;&#25512;&#29702;&#26102;&#30340;&#35268;&#27169;&#20851;&#31995;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#25439;&#22833;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25298;&#32477;&#37319;&#26679;&#24494;&#35843;&#25216;&#26415;&#26469;&#22686;&#24378;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28982;&#32780;&#20851;&#20110;LLM&#23481;&#37327;&#19982;&#25968;&#23398;&#25512;&#29702;&#20043;&#38388;&#30340;&#35268;&#27169;&#20851;&#31995;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#25439;&#22833;&#12289;&#30417;&#30563;&#25968;&#25454;&#37327;&#21644;&#22686;&#24378;&#25968;&#25454;&#37327;&#23545;&#30417;&#30563;LLM&#30340;&#25512;&#29702;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#25439;&#22833;&#26159;&#27169;&#22411;&#24615;&#33021;&#30340;&#26356;&#22909;&#25351;&#26631;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#30417;&#30563;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#65292;&#24182;&#23454;&#35777;&#21457;&#29616;&#25968;&#25454;&#37327;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#32780;&#36739;&#22909;&#30340;&#27169;&#22411;&#22312;&#25193;&#22823;&#30340;&#30417;&#30563;&#25968;&#25454;&#38598;&#19978;&#25913;&#36827;&#36739;&#23567;&#12290;&#20026;&#20102;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#21162;&#21147;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#26356;&#22810;&#30340;&#25968;&#25454;&#26679;&#26412;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25298;&#32477;&#37319;&#26679;&#24494;&#35843;&#65288;RFT&#65289;&#12290;RFT&#20351;&#29992;&#30417;&#30563;&#27169;&#22411;&#29983;&#25104;&#21644;&#25910;&#38598;&#27491;&#30830;&#30340;&#25512;&#29702;&#36335;&#24452;&#20316;&#20026;&#22686;&#24378;&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#22810;&#19981;&#21516;&#30340;&#25512;&#29702;&#36335;&#24452;&#20316;&#20026;&#22686;&#24378;&#26679;&#26412;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reaso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;GRDD&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#26041;&#35328;&#35782;&#21035;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20063;&#33021;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.00802</link><description>&lt;p&gt;
GRDD: &#24076;&#33098;&#26041;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
GRDD: A Dataset for Greek Dialectal NLP. (arXiv:2308.00802v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;GRDD&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#26041;&#35328;&#35782;&#21035;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20063;&#33021;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20811;&#37324;&#29305;&#12289;&#24222;&#25552;&#12289;&#21271;&#24076;&#33098;&#21644;&#22622;&#28006;&#36335;&#26031;&#24076;&#33098;&#22235;&#31181;&#26041;&#35328;&#30340;&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#12290;&#23613;&#31649;&#23384;&#22312;&#19981;&#24179;&#34913;&#65292;&#20294;&#35813;&#25968;&#25454;&#38598;&#26159;&#30456;&#24403;&#22823;&#30340;&#65292;&#24182;&#19988;&#26159;&#21019;&#24314;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#31867;&#20284;&#36164;&#28304;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#26041;&#35328;&#35782;&#21035;&#65292;&#24182;&#23581;&#35797;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#38750;&#24120;&#22909;&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;&#25152;&#30740;&#31350;&#30340;&#26041;&#35328;&#20855;&#26377;&#36275;&#22815;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20063;&#33021;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#38024;&#23545;&#34920;&#29616;&#26368;&#20339;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#38169;&#35823;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#38169;&#35823;&#26159;&#30001;&#20110;&#25968;&#25454;&#38598;&#28165;&#29702;&#19981;&#36275;&#36896;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a dataset for the computational study of a number of Modern Greek dialects. It consists of raw text data from four dialects of Modern Greek, Cretan, Pontic, Northern Greek and Cypriot Greek. The dataset is of considerable size, albeit imbalanced, and presents the first attempt to create large scale dialectal resources of this type for Modern Greek dialects. We then use the dataset to perform dialect idefntification. We experiment with traditional ML algorithms, as well as simple DL architectures. The results show very good performance on the task, potentially revealing that the dialects in question have distinct enough characteristics allowing even simple ML models to perform well on the task. Error analysis is performed for the top performing algorithms showing that in a number of cases the errors are due to insufficient dataset cleaning.
&lt;/p&gt;</description></item><item><title>Factify 2&#36827;&#34892;&#20102;&#19968;&#39033;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#27604;&#36739;&#31038;&#20132;&#23186;&#20307;&#22768;&#26126;&#21644;&#25903;&#25345;&#25991;&#20214;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#20551;&#26032;&#38395;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#39564;&#35777;&#65292;&#26368;&#20339;&#24615;&#33021;&#36798;&#21040;&#20102;81.82%&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.10475</link><description>&lt;p&gt;
Factify 2&#35843;&#26597;&#25253;&#21578;: &#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Findings of Factify 2: Multimodal Fake News Detection. (arXiv:2307.10475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10475
&lt;/p&gt;
&lt;p&gt;
Factify 2&#36827;&#34892;&#20102;&#19968;&#39033;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#27604;&#36739;&#31038;&#20132;&#23186;&#20307;&#22768;&#26126;&#21644;&#25903;&#25345;&#25991;&#20214;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#20551;&#26032;&#38395;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#39564;&#35777;&#65292;&#26368;&#20339;&#24615;&#33021;&#36798;&#21040;&#20102;81.82%&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#20351;&#29992;&#22312;&#36807;&#21435;&#20960;&#24180;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20551;&#26032;&#38395;&#20063;&#21464;&#24471;&#38750;&#24120;&#26222;&#36941;&#12290;&#20551;&#26032;&#38395;&#30340;&#26377;&#23475;&#24433;&#21709;&#24378;&#35843;&#20102;&#30740;&#31350;&#33258;&#21160;&#26816;&#27979;&#38169;&#35823;&#20449;&#24687;&#24182;&#39564;&#35777;&#20854;&#20934;&#30830;&#24615;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;Factify 2&#30340;&#32467;&#26524;&#65292;&#36825;&#26159;&#20316;&#20026;AAAI'23&#30340;DeFactify 2&#24037;&#20316;&#22346;&#30340;&#19968;&#37096;&#20998;&#25552;&#20379;&#30340;&#22810;&#27169;&#24577;&#20107;&#23454;&#39564;&#35777;&#21644;&#35773;&#21050;&#26032;&#38395;&#25968;&#25454;&#38598;&#12290;&#25968;&#25454;&#21628;&#21796;&#19968;&#31181;&#22522;&#20110;&#27604;&#36739;&#30340;&#26041;&#27861;&#26469;&#37197;&#23545;&#31038;&#20132;&#23186;&#20307;&#22768;&#26126;&#21644;&#25903;&#25345;&#25991;&#20214;&#65292;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#26681;&#25454;&#22810;&#27169;&#24577;&#20851;&#31995;&#20998;&#20026;5&#31867;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#30340;&#31532;&#20108;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#26377;&#36229;&#36807;60&#20010;&#21442;&#19982;&#32773;&#21644;9&#20010;&#32456;&#26399;&#27979;&#35797;&#25552;&#20132;&#12290;&#26368;&#22909;&#30340;&#34920;&#29616;&#26469;&#33258;&#20110;&#22312;&#25991;&#26412;&#26041;&#38754;&#20351;&#29992;DeBERTa&#65292;&#22312;&#22270;&#20687;&#26041;&#38754;&#20351;&#29992;Swinv2&#21644;CLIP&#12290;&#25152;&#26377;&#20116;&#20010;&#31867;&#21035;&#30340;F1&#20998;&#25968;&#24179;&#22343;&#36798;&#21040;&#20102;81.82%&#12290;
&lt;/p&gt;
&lt;p&gt;
With social media usage growing exponentially in the past few years, fake news has also become extremely prevalent. The detrimental impact of fake news emphasizes the need for research focused on automating the detection of false information and verifying its accuracy. In this work, we present the outcome of the Factify 2 shared task, which provides a multi-modal fact verification and satire news dataset, as part of the DeFactify 2 workshop at AAAI'23. The data calls for a comparison based approach to the task by pairing social media claims with supporting documents, with both text and image, divided into 5 classes based on multi-modal relations. In the second iteration of this task we had over 60 participants and 9 final test-set submissions. The best performances came from the use of DeBERTa for text and Swinv2 and CLIP for image. The highest F1 score averaged for all five classes was 81.82%.
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;LLM&#30340;&#29305;&#28857;&#21644;&#21151;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#21457;&#29616;&#21644;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.06435</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overview of Large Language Models. (arXiv:2307.06435v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06435
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;LLM&#30340;&#29305;&#28857;&#21644;&#21151;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#21457;&#29616;&#21644;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;&#20102;&#20247;&#22810;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#20986;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#35843;&#25972;&#29616;&#26377;&#30340;&#26550;&#26500;&#65292;&#22686;&#21152;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#22686;&#21152;&#35757;&#32451;&#26102;&#38388;&#20197;&#36229;&#36234;&#22522;&#32447;&#12290;&#20998;&#26512;&#26032;&#30340;&#21457;&#23637;&#23545;&#20110;&#35782;&#21035;&#22686;&#24378;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#25913;&#36827;LLM&#27867;&#21270;&#33021;&#21147;&#30340;&#21464;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;LLM&#30340;&#26550;&#26500;&#21450;&#20854;&#20998;&#31867;&#12289;&#35757;&#32451;&#31574;&#30053;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;LLM&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#21644;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;LLM&#30340;&#23436;&#25972;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#37325;&#35201;&#29305;&#28857;&#21644;&#21151;&#33021;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;LLM&#30740;&#31350;&#30340;&#37325;&#35201;&#21457;&#29616;&#65292;&#24182;&#25972;&#21512;&#20102;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown excellent generalization capabilities that have led to the development of numerous models. These models propose various new architectures, tweaking existing architectures with refined training strategies, increasing context length, using high-quality training data, and increasing training time to outperform baselines. Analyzing new developments is crucial for identifying changes that enhance training stability and improve generalization in LLMs. This survey paper comprehensively analyses the LLMs architectures and their categorization, training strategies, training datasets, and performance evaluations and discusses future research directions. Moreover, the paper also discusses the basic building blocks and concepts behind LLMs, followed by a complete overview of LLMs, including their important features and functions. Finally, the paper summarizes significant findings from LLM research and consolidates essential architectural and training strateg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;Cross-corpus text Readability Compatibility Assessment (CRCA)&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#21487;&#35835;&#24615;&#20860;&#23481;&#24615;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#20855;&#26377;&#26174;&#33879;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#34920;&#31034;&#21644;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09704</link><description>&lt;p&gt;
&#36328;&#35821;&#26009;&#38405;&#35835;&#24615;&#20860;&#23481;&#24615;&#35780;&#20272;&#65306;&#33521;&#25991;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Cross-corpus Readability Compatibility Assessment for English Texts. (arXiv:2306.09704v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;Cross-corpus text Readability Compatibility Assessment (CRCA)&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#21487;&#35835;&#24615;&#20860;&#23481;&#24615;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#20855;&#26377;&#26174;&#33879;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#34920;&#31034;&#21644;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#30340;&#21487;&#35835;&#24615;&#35780;&#20272;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#20013;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#23545;&#35821;&#26009;&#24211;&#20860;&#23481;&#24615;&#30340;&#32570;&#20047;&#25506;&#32034;&#26500;&#25104;&#20102;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#30740;&#31350;&#23567;&#32452;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#26009;&#24211;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;Cross-corpus text Readability Compatibility Assessment (CRCA)&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;(1) &#35821;&#26009;&#24211;&#65306;CEFR&#65292;CLEC&#65292;CLOTH&#65292;NES&#65292;OSP&#21644;RACE&#12290;&#25552;&#21462;&#20102;&#35821;&#35328;&#29305;&#24449;&#12289;GloVe&#35789;&#21521;&#37327;&#34920;&#31034;&#21644;&#23427;&#20204;&#30340;&#34701;&#21512;&#29305;&#24449;&#12290;(2) &#20998;&#31867;&#27169;&#22411;&#65306;&#37319;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;XGBoost&#65292;SVM&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65288;BiLSTM&#65292;Attention-BiLSTM&#65289;&#12290;(3) &#20860;&#23481;&#24615;&#25351;&#26631;&#65306;RJSD&#65292;RRNSS&#21644;NDCG&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;(1) OSP&#34920;&#29616;&#26174;&#33879;&#19981;&#21516;&#20110;&#20854;&#20182;&#25968;&#25454;&#38598;&#30340;&#35777;&#23454;&#20102;&#35821;&#26009;&#20860;&#23481;&#24615;&#12290;(2) &#20860;&#23481;&#24615;&#12289;&#29305;&#24449;&#34920;&#31034;&#21644;&#20998;&#31867;&#26041;&#27861;&#20043;&#38388;&#26377;&#36866;&#24212;&#24615;&#25928;&#24212;&#12290;(3) &#22312;&#19981;&#21516;&#20860;&#23481;&#24615;&#25351;&#26631;&#19979;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text readability assessment has gained significant attention from researchers in various domains. However, the lack of exploration into corpus compatibility poses a challenge as different research groups utilize different corpora. In this study, we propose a novel evaluation framework, Cross-corpus text Readability Compatibility Assessment (CRCA), to address this issue. The framework encompasses three key components: (1) Corpus: CEFR, CLEC, CLOTH, NES, OSP, and RACE. Linguistic features, GloVe word vector representations, and their fusion features were extracted. (2) Classification models: Machine learning methods (XGBoost, SVM) and deep learning methods (BiLSTM, Attention-BiLSTM) were employed. (3) Compatibility metrics: RJSD, RRNSS, and NDCG metrics. Our findings revealed: (1) Validated corpus compatibility, with OSP standing out as significantly different from other datasets. (2) An adaptation effect among corpora, feature representations, and classification methods. (3) Consistent 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07848</link><description>&lt;p&gt;
GEmo-CLAP: &#38754;&#21521;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;CLAP&#65289;&#26368;&#36817;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEmo-CLAP&#30340;&#39640;&#25928;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;CLAP&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24773;&#24863;CLAP&#27169;&#22411;&#65288;&#31216;&#20026;Emo-CLAP&#65289;&#65292;&#29992;&#20110;SER&#12290;&#28982;&#21518;&#65292;&#32771;&#34385;&#21040;&#22312;&#35821;&#38899;&#24773;&#24863;&#24314;&#27169;&#20013;&#24615;&#21035;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#65292;&#26469;&#25972;&#21512;&#35821;&#38899;&#20449;&#21495;&#30340;&#24773;&#24863;&#21644;&#24615;&#21035;&#20449;&#24687;&#65292;&#24418;&#25104;&#26356;&#21512;&#29702;&#30340;&#30446;&#26631;&#12290;&#22312;IEMOCAP&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;Emo-CLAP&#27169;&#22411;&#65288;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#21516;&#26102;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Audio Pretraining (CLAP) has recently exhibited impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a kind of efficient gender-attribute-enhanced CLAP model for speech emotion recognition (SER). Specifically, we first build an effective emotion CLAP model termed Emo-CLAP for SER, utilizing various self-supervised learning based pre-trained models. Then, considering the importance of the gender attribute in speech emotion modeling, two GEmo-CLAP approaches are further proposed to integrate the emotion and gender information of speech signals, forming more reasonable objectives. Extensive experiments conducted on the IEMOCAP corpus demonstrate that our proposed two GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with different pre-trained models, while also achieving superior recognition performance compared with other state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#21442;&#32771;&#25991;&#29486;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25628;&#32034;&#24341;&#25806;&#26597;&#35810;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#36825;&#20123;&#24187;&#35273;&#12290;&#24182;&#19988;&#36890;&#36807;&#23545;&#21516;&#19968;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#26597;&#35810;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#21442;&#32771;&#25991;&#29486;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.18248</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30693;&#36947;&#33258;&#24049;&#22312;&#20135;&#29983;&#8220;&#24187;&#35273;&#8221;&#21442;&#32771;&#25991;&#29486;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Language Models Know When They're Hallucinating References?. (arXiv:2305.18248v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#21442;&#32771;&#25991;&#29486;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25628;&#32034;&#24341;&#25806;&#26597;&#35810;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#36825;&#20123;&#24187;&#35273;&#12290;&#24182;&#19988;&#36890;&#36807;&#23545;&#21516;&#19968;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#26597;&#35810;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#21442;&#32771;&#25991;&#29486;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#20854;&#8220;&#24187;&#35273;&#8221;&#21442;&#32771;&#25991;&#29486;&#32780;&#38395;&#21517;&#12290;&#36825;&#20123;&#34394;&#26500;&#30340;&#25991;&#31456;&#21644;&#20070;&#21517;&#24341;&#36215;&#20102;&#21361;&#23475;&#65292;&#23545;&#23427;&#20204;&#30340;&#20351;&#29992;&#36896;&#25104;&#20102;&#38556;&#30861;&#65292;&#24182;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#21453;&#24377;&#12290;&#23613;&#31649;&#20854;&#20182;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#20063;&#24456;&#37325;&#35201;&#65292;&#20294;&#25105;&#20204;&#23558;&#24187;&#35273;&#21442;&#32771;&#25991;&#29486;&#25552;&#20986;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#24187;&#35273;&#30740;&#31350;&#30340;&#8220;&#26524;&#34631;&#8221;&#65292;&#22240;&#20026;&#23427;&#20204;&#29305;&#21035;&#23481;&#26131;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#25628;&#32034;&#24341;&#25806;&#26597;&#35810;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#27492;&#31867;&#24187;&#35273;&#65292;&#20174;&#32780;&#20415;&#20110;&#35780;&#20272;&#12290;&#20026;&#20102;&#24320;&#22987;&#21078;&#26512;&#24187;&#35273;&#35821;&#35328;&#27169;&#22411;&#21442;&#32771;&#25991;&#29486;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#23545;&#21516;&#19968;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#26597;&#35810;&#26469;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#19981;&#20511;&#21161;&#20219;&#20309;&#22806;&#37096;&#36164;&#28304;&#12290;&#25105;&#20204;&#23558;&#8220;&#30452;&#25509;&#8221;&#26597;&#35810;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#19982;&#8220;&#38388;&#25509;&#8221;&#26597;&#35810;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21518;&#32773;&#35810;&#38382;&#20102;&#38468;&#21152;&#30340;&#32454;&#33410;&#65292;&#22914;&#20316;&#21697;&#30340;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art language models (LMs) are famous for "hallucinating" references. These fabricated article and book titles lead to harms, obstacles to their use, and public backlash. While other types of LM hallucinations are also important, we propose hallucinated references as the "drosophila" of research on hallucination in large language models (LLMs), as they are particularly easy to study. We show that simple search engine queries reliably identify such hallucinations, which facilitates evaluation. To begin to dissect the nature of hallucinated LM references, we attempt to classify them using black-box queries to the same LM, without consulting any external resources. Consistency checks done with "direct" queries about whether the generated reference title is real (inspired by Kadavath et al. 2022, Lin et al. 2022, Manakul et al. 2023) are compared to consistency checks with "indirect" queries which ask for ancillary details such as the authors of the work. These consistency chec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#24515;&#26234;&#29702;&#35770;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#29256;&#26412;&#30340;ChatGPT&#22312;&#20960;&#20010;&#32463;&#20856;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT-4&#27604;&#38543;&#26426;&#31572;&#26696;&#32473;&#20986;&#20102;&#26356;&#22810;&#27491;&#30830;&#31572;&#26696;&#65292;&#23613;&#31649;&#36825;&#20123;&#31572;&#26696;&#24448;&#24448;&#22522;&#20110;&#38169;&#35823;&#30340;&#20551;&#35774;&#25110;&#26080;&#25928;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.14020</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#20855;&#26377;&#24515;&#26234;&#29702;&#35770;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does ChatGPT have Theory of Mind?. (arXiv:2305.14020v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#24515;&#26234;&#29702;&#35770;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#29256;&#26412;&#30340;ChatGPT&#22312;&#20960;&#20010;&#32463;&#20856;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT-4&#27604;&#38543;&#26426;&#31572;&#26696;&#32473;&#20986;&#20102;&#26356;&#22810;&#27491;&#30830;&#31572;&#26696;&#65292;&#23613;&#31649;&#36825;&#20123;&#31572;&#26696;&#24448;&#24448;&#22522;&#20110;&#38169;&#35823;&#30340;&#20551;&#35774;&#25110;&#26080;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#26159;&#29702;&#35299;&#20154;&#31867;&#24605;&#32500;&#21644;&#20915;&#31574;&#30340;&#33021;&#21147;&#65292;&#36825;&#31181;&#33021;&#21147;&#22312;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#31038;&#20132;&#20114;&#21160;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21253;&#25324;&#35821;&#35328;&#20132;&#27969;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26368;&#36817;ChatGPT&#31995;&#21015;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20855;&#22791;ToM&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#29256;&#26412;&#30340;ChatGPT&#25552;&#20986;&#20102;&#20845;&#20010;&#32463;&#20856;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#28041;&#21450;&#20154;&#31867;&#25512;&#29702;&#21644;&#20915;&#31574;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#22312;&#22810;&#31181;&#25552;&#31034;&#31574;&#30053;&#19979;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#34429;&#28982;&#20851;&#20110;ChatGPT-3&#30340;&#32467;&#26524;&#26377;&#20123;&#19981;&#30830;&#23450;&#65292;&#20294;ChatGPT-4&#34987;&#35777;&#26126;&#27604;&#39044;&#26399;&#26356;&#32463;&#24120;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#23613;&#31649;&#27491;&#30830;&#31572;&#26696;&#24448;&#24448;&#26159;&#22522;&#20110;&#38169;&#35823;&#30340;&#20551;&#35774;&#25110;&#26080;&#25928;&#30340;&#25512;&#29702;&#24471;&#20986;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM) is the ability to understand human thinking and decision-making, an ability that plays a crucial role in social interaction between people, including linguistic communication. This paper investigates to what extent recent Large Language Models in the ChatGPT tradition possess ToM. We posed six well-known problems that address biases in human reasoning and decision making to two versions of ChatGPT and we compared the results under a range of prompting strategies. While the results concerning ChatGPT-3 were somewhat inconclusive, ChatGPT-4 was shown to arrive at the correct answers more often than would be expected based on chance, although correct answers were often arrived at on the basis of false assumptions or invalid reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#22522;&#20110;BERT&#30340;NLP&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#34429;&#28982;DistilBERT&#21644;TinyBERT&#31561;&#36731;&#37327;&#32423;&#27169;&#22411;&#30456;&#23545;&#21344;&#29992;&#26356;&#23569;&#20869;&#23384;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#65307;ResNet-based BERT&#27169;&#22411;&#21487;&#20197;&#22312;&#31934;&#24230;&#21644;&#36164;&#28304;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#36866;&#21512;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2304.11520</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#22522;&#20110;BERT&#30340;NLP&#27169;&#22411;&#30340;&#25361;&#25112;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Challenges of Deploying BERT-based NLP Models in Resource-Constrained Embedded Devices. (arXiv:2304.11520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#22522;&#20110;BERT&#30340;NLP&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#34429;&#28982;DistilBERT&#21644;TinyBERT&#31561;&#36731;&#37327;&#32423;&#27169;&#22411;&#30456;&#23545;&#21344;&#29992;&#26356;&#23569;&#20869;&#23384;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#65307;ResNet-based BERT&#27169;&#22411;&#21487;&#20197;&#22312;&#31934;&#24230;&#21644;&#36164;&#28304;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#36866;&#21512;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#31070;&#32463;&#26550;&#26500;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#19979;&#28216;NLP&#20219;&#21153;&#30340;&#27969;&#34892;&#20808;&#36827;&#25216;&#26415;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#23545;&#25968;&#25454;&#20381;&#36182;&#24615;&#24378;&#65292;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#21644;&#33021;&#37327;&#65292;&#32463;&#24120;&#38459;&#30861;&#23427;&#20204;&#22312;&#35768;&#22810;&#23454;&#26102;&#12289;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37096;&#32626;&#12290;&#29616;&#26377;&#30340;BERT&#36731;&#37327;&#32423;&#29256;&#26412;&#65288;&#20363;&#22914;DistilBERT&#21644;TinyBERT&#65289;&#36890;&#24120;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#19978;&#26080;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20174;&#35774;&#35745;&#24072;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35201;&#20026;&#29305;&#23450;&#30340;NLP&#20219;&#21153;&#20351;&#29992;&#20309;&#31181;&#8220;&#27491;&#30830;&#30340;&#8221;&#22522;&#20110;BERT&#30340;&#26550;&#26500;&#65292;&#20197;&#22312;&#36164;&#28304;&#21487;&#29992;&#24615;&#21644;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#30340;&#26368;&#23567;&#31934;&#24230;&#20043;&#38388;&#23454;&#29616;&#26368;&#20339;&#26435;&#34913;&#65292;&#23578;&#19981;&#30830;&#23450;&#12290;&#31995;&#32479;&#24037;&#31243;&#24072;&#24517;&#39035;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#35797;&#38169;&#23454;&#39564;&#65292;&#20197;&#25214;&#21040;&#21512;&#36866;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#22312;&#19981;&#21516;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#31934;&#24230;&#39044;&#31639;&#19979;&#23545;BERT-based&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#31350;&#24615;&#30740;&#31350;&#65292;&#20197;&#24471;&#20986;&#26377;&#20851;&#27492;&#36164;&#28304;/&#31934;&#24230;&#26435;&#34913;&#30340;&#32463;&#39564;&#24615;&#35266;&#23519;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;DistilBERT&#21644;TinyBERT&#31561;&#26356;&#36731;&#37327;&#32423;&#30340;&#27169;&#22411;&#30456;&#23545;BERT-base&#21344;&#29992;&#30340;&#20869;&#23384;&#35201;&#23569;&#24471;&#22810;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#20013;&#31934;&#24230;&#30340;&#19979;&#38477;&#26159;&#26126;&#26174;&#30340;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;ResNet&#30340;BERT&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#21644;&#36164;&#28304;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#20351;&#20854;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#20013;&#37096;&#32626;&#30340;&#33391;&#22909;&#20505;&#36873;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
BERT-based neural architectures have established themselves as popular state-of-the-art baselines for many downstream NLP tasks. However, these architectures are data-hungry and consume a lot of memory and energy, often hindering their deployment in many real-time, resource-constrained applications. Existing lighter versions of BERT (eg. DistilBERT and TinyBERT) often cannot perform well on complex NLP tasks. More importantly, from a designer's perspective, it is unclear what is the "right" BERT-based architecture to use for a given NLP task that can strike the optimal trade-off between the resources available and the minimum accuracy desired by the end user. System engineers have to spend a lot of time conducting trial-and-error experiments to find a suitable answer to this question. This paper presents an exploratory study of BERT-based models under different resource constraints and accuracy budgets to derive empirical observations about this resource/accuracy trade-offs. Our findin
&lt;/p&gt;</description></item><item><title>&#26412;&#39033;&#30446;&#22312;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;ASR&#27169;&#22411;&#30340;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24605;&#36335;&#65292;&#36890;&#36807;&#25552;&#20986;&#32771;&#34385;&#35821;&#20041;&#36317;&#31163;&#30340;&#26032;&#39062;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;OpenAI&#30340;Whisper&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#24403;&#21069;&#20808;&#36827;&#25104;&#26524;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.11075</link><description>&lt;p&gt;
Spaiche: &#23558;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#25193;&#23637;&#21040;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;
&lt;/p&gt;
&lt;p&gt;
Spaiche: Extending State-of-the-Art ASR Models to Swiss German Dialects. (arXiv:2304.11075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30446;&#22312;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;ASR&#27169;&#22411;&#30340;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24605;&#36335;&#65292;&#36890;&#36807;&#25552;&#20986;&#32771;&#34385;&#35821;&#20041;&#36317;&#31163;&#30340;&#26032;&#39062;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;OpenAI&#30340;Whisper&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#24403;&#21069;&#20808;&#36827;&#25104;&#26524;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#31361;&#30772;&#22823;&#22823;&#22686;&#21152;&#20102;ASR&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#30001;&#20110;&#38590;&#20197;&#33719;&#21462;&#30456;&#20851;&#25968;&#25454;&#65292;ASR&#27169;&#22411;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;&#26412;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#29790;&#22763;&#24503;&#35821;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#24615;&#33021;&#30340;&#35265;&#35299;&#65292;&#24110;&#21161;&#25512;&#36827;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;ASR&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#32771;&#34385;&#20102;&#39044;&#27979;&#21644;&#22522;&#20934;&#26631;&#31614;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#12290;&#36890;&#36807;&#23545;&#29790;&#22763;&#24503;&#35821;&#25968;&#25454;&#38598;&#23545;OpenAI&#30340;Whisper&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#24403;&#21069;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in NLP largely increased the presence of ASR systems in our daily lives. However, for many low-resource languages, ASR models still need to be improved due in part to the difficulty of acquiring pertinent data. This project aims to help advance research in ASR models for Swiss German dialects, by providing insights about the performance of state-of-the-art ASR models on recently published Swiss German speech datasets. We propose a novel loss that takes into account the semantic distance between the predicted and the ground-truth labels. We outperform current state-of-the-art results by fine-tuning OpenAI's Whisper model on Swiss-German datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.07267</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24515;&#29702;&#23398;&#20013;&#30340;&#8220;&#27491;&#30830;&#31572;&#26696;&#8221;
&lt;/p&gt;
&lt;p&gt;
"Correct answers" from the psychology of artificial intelligence. (arXiv:2302.07267v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper replicates 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, and successfully replicates the results of 8 studies. However, for the remaining 6 studies, GPT3.5 answered survey questions in an extremely predetermined way, making it impossible to analyze these studies.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24050;&#32463;&#22823;&#22823;&#22686;&#24378;&#12290;&#36825;&#31181;AI&#31995;&#32479;&#30340;&#19968;&#20010;&#25552;&#20986;&#30340;&#24212;&#29992;&#26159;&#25903;&#25345;&#31038;&#20250;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#30446;&#21069;&#23436;&#32654;&#30340;&#23454;&#39564;&#25511;&#21046;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#32780;&#22823;&#35268;&#27169;&#12289;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#65288;&#20439;&#31216;GPT3.5&#65289;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27599;&#39033;&#30740;&#31350;&#30340;&#35843;&#26597;&#20316;&#20026;&#25991;&#26412;&#36755;&#20837;&#65292;&#20174;GPT3.5&#30340;&#40664;&#35748;&#35774;&#32622;&#20013;&#25910;&#38598;&#20102;&#21709;&#24212;&#12290;&#22312;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#30340;&#20843;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;GPT&#26679;&#26412;&#22797;&#21046;&#20102;&#21407;&#22987;&#32467;&#26524;&#30340;37.5%&#20197;&#21450;Many Labs 2&#32467;&#26524;&#30340;37.5%&#12290;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#25105;&#20204;&#26080;&#27861;&#20687;&#39044;&#20808;&#27880;&#20876;&#30340;&#35745;&#21010;&#37027;&#26679;&#20998;&#26512;&#21097;&#19979;&#30340;&#20845;&#39033;&#30740;&#31350;&#12290;&#36825;&#26159;&#22240;&#20026;&#23545;&#20110;&#36825;&#20845;&#39033;&#30740;&#31350;&#20013;&#30340;&#27599;&#19968;&#39033;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65288;&#26080;&#35770;&#26159;&#22240;&#21464;&#37327;&#36824;&#26159;&#26465;&#20214;&#21464;&#37327;&#65289;&#65306;&#19968;&#20010;&#26410;&#30693;&#30340;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have vastly grown in capabilities. One proposed application of such AI systems is to support data collection in the social and cognitive sciences, where perfect experimental control is currently unfeasible and the collection of large, representative datasets is generally expensive. In this paper, we re-replicate 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We collected responses from the default setting of GPT3.5 by inputting each study's survey as text. Among the eight studies we could analyse, our GPT sample replicated 37.5% of the original results as well as 37.5% of the Many Labs 2 results. Unexpectedly, we could not analyse the remaining six studies as we had planned in our pre-registration. This was because for each of these six studies, GPT3.5 answered at least one of the survey questions (either a dependent variable or a condition variable) in an extremely predetermined way: an unex
&lt;/p&gt;</description></item><item><title>ColD Fusion&#26159;&#19968;&#31181;&#21327;&#21516;&#19979;&#38477;&#30340;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#21487;&#20197;&#19981;&#26029;&#25913;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;RoBERTa&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.01378</link><description>&lt;p&gt;
ColD Fusion: &#21327;&#21516;&#19979;&#38477;&#30340;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning. (arXiv:2212.01378v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01378
&lt;/p&gt;
&lt;p&gt;
ColD Fusion&#26159;&#19968;&#31181;&#21327;&#21516;&#19979;&#38477;&#30340;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#21487;&#20197;&#19981;&#26029;&#25913;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;RoBERTa&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#26469;&#19981;&#26029;&#28436;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#31216;&#20026;ColD Fusion&#12290;&#23427;&#20855;&#26377;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#20294;&#21033;&#29992;&#26377;&#38480;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#20849;&#20139;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;ColD Fusion&#21487;&#20197;&#24418;&#25104;&#19968;&#20010;&#21327;&#21516;&#24490;&#29615;&#65292;&#20854;&#20013;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#24490;&#29615;&#21033;&#29992;&#65292;&#19981;&#26029;&#25913;&#36827;&#23427;&#20204;&#25152;&#22522;&#20110;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ColD Fusion&#20135;&#29983;&#20102;&#19982;&#22810;&#20219;&#21153;&#35757;&#32451;&#30456;&#24403;&#30340;&#22909;&#22788;&#65292;&#36890;&#36807;&#20135;&#29983;&#19968;&#20010;&#22312;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#24182;&#19988;&#22312;&#26410;&#35265;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#26356;&#22909;&#30340;&#36215;&#28857;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ColD Fusion&#20248;&#20110;RoBERTa&#29978;&#33267;&#20197;&#21069;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#20351;&#29992;35&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#65292;ColD Fusion-based&#27169;&#22411;&#22312;&#19981;&#25913;&#21464;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#24179;&#22343;&#20248;&#20110;RoBERTa 2.33&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new paradigm to continually evolve pretrained models, denoted ColD Fusion. It provides the benefits of multitask learning but leverages distributed computation with limited communication and eliminates the need for shared data. Consequentially, ColD Fusion can give rise to a synergistic loop, where finetuned models can be recycled to continually improve the pretrained model they are based upon. We show that ColD Fusion yields comparable benefits to multitask training by producing a model that (a) attains strong performance on all of the datasets it was trained on; and (b) is a better starting point for finetuning on unseen datasets. We show that ColD Fusion outperforms RoBERTa and even previous multitask models. Specifically, when training and testing on 35 diverse datasets, ColD Fusion-based model outperforms RoBERTa by 2.33 points on average without any changes to the architecture.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#29983;&#25104;&#30340;&#35270;&#39057;&#23383;&#24149;&#20013;&#25552;&#21462;&#35821;&#20041;&#20803;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#21462;&#23454;&#20307;&#12289;&#23454;&#20307;&#23646;&#24615;&#12289;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#35270;&#39057;&#20998;&#31867;&#12290;&#25552;&#21462;&#20449;&#24687;&#30340;&#36136;&#37327;&#21463;&#21040;&#20107;&#20214;&#23450;&#20301;&#36136;&#37327;&#21644;&#23383;&#24149;&#29983;&#25104;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2211.02982</link><description>&lt;p&gt;
&#29983;&#25104;&#35270;&#39057;&#23383;&#24149;&#20013;&#30340;&#20107;&#20214;&#21644;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Event and Entity Extraction from Generated Video Captions. (arXiv:2211.02982v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#29983;&#25104;&#30340;&#35270;&#39057;&#23383;&#24149;&#20013;&#25552;&#21462;&#35821;&#20041;&#20803;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#21462;&#23454;&#20307;&#12289;&#23454;&#20307;&#23646;&#24615;&#12289;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#35270;&#39057;&#20998;&#31867;&#12290;&#25552;&#21462;&#20449;&#24687;&#30340;&#36136;&#37327;&#21463;&#21040;&#20107;&#20214;&#23450;&#20301;&#36136;&#37327;&#21644;&#23383;&#24149;&#29983;&#25104;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20154;&#24037;&#36827;&#34892;&#22810;&#23186;&#20307;&#25968;&#25454;&#27880;&#37322;&#32791;&#26102;&#19988;&#26114;&#36149;&#65292;&#32780;&#21487;&#38752;&#30340;&#33258;&#21160;&#29983;&#25104;&#35821;&#20041;&#20803;&#25968;&#25454;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#21160;&#29983;&#25104;&#30340;&#35270;&#39057;&#23383;&#24149;&#20013;&#25552;&#21462;&#35821;&#20041;&#20803;&#25968;&#25454;&#30340;&#26694;&#26550;&#12290;&#20316;&#20026;&#20803;&#25968;&#25454;&#65292;&#25105;&#20204;&#32771;&#34385;&#23454;&#20307;&#12289;&#23454;&#20307;&#23646;&#24615;&#12289;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#35270;&#39057;&#20998;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#27169;&#22411;&#65292;&#21363;&#36974;&#34109;&#36716;&#25442;&#22120;&#65288;MT&#65289;&#21644;&#24182;&#34892;&#35299;&#30721;&#65288;PVDC&#65289;&#65292;&#20026;ActivityNet Captions&#25968;&#25454;&#38598;&#30340;&#35270;&#39057;&#29983;&#25104;&#23383;&#24149;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20174;&#29983;&#25104;&#30340;&#23383;&#24149;&#20013;&#25552;&#21462;&#23454;&#20307;&#12289;&#23454;&#20307;&#23646;&#24615;&#12289;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#35270;&#39057;&#20998;&#31867;&#26159;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25552;&#21462;&#20449;&#24687;&#30340;&#36136;&#37327;&#20027;&#35201;&#21463;&#21040;&#35270;&#39057;&#20013;&#20107;&#20214;&#23450;&#20301;&#30340;&#36136;&#37327;&#20197;&#21450;&#20107;&#20214;&#23383;&#24149;&#29983;&#25104;&#30340;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotation of multimedia data by humans is time-consuming and costly, while reliable automatic generation of semantic metadata is a major challenge. We propose a framework to extract semantic metadata from automatically generated video captions. As metadata, we consider entities, the entities' properties, relations between entities, and the video category. We employ two state-of-the-art dense video captioning models with masked transformer (MT) and parallel decoding (PVDC) to generate captions for videos of the ActivityNet Captions dataset. Our experiments show that it is possible to extract entities, their properties, relations between entities, and the video category from the generated captions. We observe that the quality of the extracted information is mainly influenced by the quality of the event localization in the video as well as the performance of the event caption generation.
&lt;/p&gt;</description></item></channel></rss>