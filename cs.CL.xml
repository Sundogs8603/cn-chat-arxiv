<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>CABINET&#26159;&#19968;&#20010;&#29992;&#20110;&#34920;&#26684;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#20110;&#20869;&#23481;&#30456;&#20851;&#24615;&#30340;&#22122;&#22768;&#38477;&#20302;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#22788;&#29702;&#34920;&#26684;&#20869;&#23481;&#24182;&#29983;&#25104;&#35299;&#26512;&#35821;&#21477;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#19987;&#27880;&#20110;&#30456;&#20851;&#34920;&#26684;&#25968;&#25454;&#32780;&#25233;&#21046;&#26080;&#20851;&#20449;&#24687;&#30340;&#24178;&#25200;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01155</link><description>&lt;p&gt;
CABINET: &#34920;&#26684;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#20110;&#20869;&#23481;&#30456;&#20851;&#24615;&#30340;&#22122;&#22768;&#38477;&#20302;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CABINET: Content Relevance based Noise Reduction for Table Question Answering
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01155
&lt;/p&gt;
&lt;p&gt;
CABINET&#26159;&#19968;&#20010;&#29992;&#20110;&#34920;&#26684;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#20110;&#20869;&#23481;&#30456;&#20851;&#24615;&#30340;&#22122;&#22768;&#38477;&#20302;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#22788;&#29702;&#34920;&#26684;&#20869;&#23481;&#24182;&#29983;&#25104;&#35299;&#26512;&#35821;&#21477;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#19987;&#27880;&#20110;&#30456;&#20851;&#34920;&#26684;&#25968;&#25454;&#32780;&#25233;&#21046;&#26080;&#20851;&#20449;&#24687;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#26684;&#29702;&#35299;&#33021;&#21147;&#36890;&#36807;&#23545;&#34920;&#26684;&#30340;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#36890;&#24120;&#65292;&#21482;&#26377;&#34920;&#26684;&#30340;&#19968;&#23567;&#37096;&#20998;&#19982;&#32473;&#23450;&#38382;&#39064;&#30340;&#31572;&#26696;&#30456;&#20851;&#12290;&#19981;&#30456;&#20851;&#30340;&#37096;&#20998;&#20250;&#20135;&#29983;&#22122;&#22768;&#21644;&#24178;&#25200;&#20449;&#24687;&#65292;&#23548;&#33268;LLMs&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CABINET&#65288;&#22522;&#20110;&#20869;&#23481;&#30456;&#20851;&#24615;&#30340;&#34920;&#26684;&#38382;&#31572;&#22122;&#22768;&#38477;&#20302;&#26041;&#27861;&#65289;- &#19968;&#20010;&#33021;&#22815;&#35753;LLMs&#19987;&#27880;&#20110;&#30456;&#20851;&#34920;&#26684;&#25968;&#25454;&#24182;&#25233;&#21046;&#26080;&#20851;&#20449;&#24687;&#30340;&#26694;&#26550;&#12290;CABINET&#21253;&#25324;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#30456;&#20851;&#24615;&#35780;&#20998;&#22120;&#65288;URS&#65289;&#65292;&#19982;&#38382;&#31572;LLM&#24046;&#24322;&#24615;&#35757;&#32451;&#65292;&#26681;&#25454;&#20854;&#19982;&#36755;&#20837;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#23545;&#34920;&#26684;&#20869;&#23481;&#36827;&#34892;&#21152;&#26435;&#22788;&#29702;&#21518;&#20877;&#36755;&#20837;&#38382;&#31572;LLM&#65288;QA LLM&#65289;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#36741;&#21161;&#30456;&#20851;&#24615;&#35780;&#20998;&#22120;&#65292;CABINET&#21033;&#29992;&#19968;&#20010;&#24369;&#30417;&#30563;&#27169;&#22359;&#29983;&#25104;&#19968;&#20010;&#35299;&#26512;&#35821;&#21477;&#65292;&#25551;&#36848;&#34892;&#21644;&#21015;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question-answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#26080;&#38656;&#35757;&#32451;&#25110;API&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#35270;&#35273;&#27169;&#22411;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08680</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#36731;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#26080;&#38656;&#35757;&#32451;&#25110;API&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#35270;&#35273;&#27169;&#22411;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#30340;&#36827;&#23637;&#36234;&#26469;&#36234;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#22270;&#20687;&#20013;&#20135;&#29983;&#34394;&#20551;&#29289;&#20307;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#20351;&#29992;&#29305;&#27530;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25110;&#24378;&#22823;&#30340;LLM&#65288;&#20363;&#22914;GPT-3.5&#65289;&#26469;&#32416;&#27491;LVLM&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#26114;&#36149;&#30340;&#35757;&#32451;/&#24494;&#35843;&#25110;API&#35775;&#38382;&#20808;&#36827;&#30340;LLM&#26469;&#22312;&#29983;&#25104;&#21518;&#32416;&#27491;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#32531;&#35299;&#24187;&#35273;&#30340;&#26694;&#26550;&#65288;MARINE&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#35813;&#26694;&#26550;&#26082;&#26080;&#38656;&#35757;&#32451;&#20063;&#26080;&#38656;API&#35775;&#38382;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#20943;&#23569;&#29289;&#20307;&#24187;&#35273;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MARINE&#36890;&#36807;&#38598;&#25104;&#29616;&#26377;&#30340;&#24320;&#28304;&#35270;&#35273;&#27169;&#22411;&#20016;&#23500;LVLM&#30340;&#35270;&#35273;&#35821;&#22659;&#65292;&#24182;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#25972;&#21512;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;LVLM&#29983;&#25104;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations. Thr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;COLD-Attack&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#20855;&#26377;&#38544;&#31192;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;LLM&#36234;&#29425;&#12290;&#36890;&#36807;&#24314;&#31435;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#19982;&#25915;&#20987;&#29983;&#25104;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#37319;&#29992;&#20102;&#33021;&#37327;&#38480;&#21046;&#35299;&#30721;&#19982;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#30340;&#25511;&#21046;&#35201;&#27714;&#19979;&#25628;&#32034;&#23545;&#25239;&#24615;LLM&#25915;&#20987;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08679</link><description>&lt;p&gt;
COLD-Attack: &#29992;&#20110;&#20855;&#26377;&#38544;&#31192;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;LLM&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;COLD-Attack&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#20855;&#26377;&#38544;&#31192;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;LLM&#36234;&#29425;&#12290;&#36890;&#36807;&#24314;&#31435;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#19982;&#25915;&#20987;&#29983;&#25104;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#37319;&#29992;&#20102;&#33021;&#37327;&#38480;&#21046;&#35299;&#30721;&#19982;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#30340;&#25511;&#21046;&#35201;&#27714;&#19979;&#25628;&#32034;&#23545;&#25239;&#24615;LLM&#25915;&#20987;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#36234;&#29425;&#30340;&#27880;&#24847;&#21147;&#36234;&#26469;&#36234;&#22810;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;LLM&#30340;&#23433;&#20840;&#24615;&#65292;&#26377;&#24517;&#35201;&#32771;&#34385;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#36234;&#29425;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#36830;&#36143;&#24615;&#20197;&#21450;&#24773;&#24863;/&#39118;&#26684;&#21464;&#21270;&#65292;&#22240;&#27492;&#30740;&#31350;&#21487;&#25511;&#24615;&#36234;&#29425;&#26159;&#26377;&#30410;&#30340;&#65292;&#21363;&#22914;&#20309;&#23545;LLM&#25915;&#20987;&#36827;&#34892;&#25511;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#24418;&#24335;&#21270;&#20102;&#21487;&#25511;&#24615;&#25915;&#20987;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#35813;&#38382;&#39064;&#19982;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#20043;&#38388;&#30340;&#26032;&#22411;&#20851;&#32852;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#34987;&#24191;&#27867;&#25506;&#32034;&#30340;&#20027;&#39064;&#12290;&#22522;&#20110;&#36825;&#31181;&#20851;&#32852;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#33021;&#37327;&#38480;&#21046;&#35299;&#30721;&#19982;Langevin&#21160;&#21147;&#23398;&#65288;COLD&#65289;&#30340;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;COLD-Attack&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32479;&#19968;&#19988;&#33258;&#21160;&#21270;&#22320;&#25628;&#32034;&#21508;&#31181;&#25511;&#21046;&#35201;&#27714;&#19979;&#30340;&#23545;&#25239;&#24615;LLM&#25915;&#20987;&#65292;&#20363;&#22914;&#27969;&#30021;&#24615;&#12289;&#38544;&#31192;&#24615;&#12289;&#24773;&#24863;&#21644;&#24038;&#21491;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jailbreaks on Large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controlla
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#22120;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#21464;&#20307;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#35299;&#26512;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08666</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#33258;&#28982;&#35821;&#35328;&#21464;&#20307;&#26469;&#25552;&#39640;&#35821;&#20041;&#35299;&#26512;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization in Semantic Parsing by Increasing Natural Language Variation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08666
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#22120;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#21464;&#20307;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#35299;&#26512;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21508;&#31181;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;Spider&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#38754;&#23545;&#20808;&#21069;&#65288;&#31934;&#30830;&#65289;&#35299;&#26512;&#34920;&#36798;&#24335;&#30340;&#36731;&#24494;&#25200;&#21160;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20063;&#24456;&#38590;&#27867;&#21270;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;Spider&#20013;&#38382;&#39064;&#30340;&#35821;&#35328;&#24418;&#24335;&#36807;&#20110;&#20855;&#20307;&#12289;&#19981;&#33258;&#28982;&#19988;&#21464;&#21270;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#21319;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#22120;&#23545;&#33258;&#28982;&#35821;&#35328;&#21464;&#20307;&#30340;&#40065;&#26834;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;Spider&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#29983;&#25104;&#38382;&#39064;&#37325;&#32452;&#65292;&#25110;&#20165;&#24341;&#20837;&#23616;&#37096;&#21464;&#21270;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#29983;&#25104;&#26356;&#30495;&#23454;&#21644;&#22810;&#26679;&#30340;&#38382;&#39064;&#12290;&#20165;&#20351;&#29992;&#20960;&#20010;&#25552;&#31034;&#65292;&#25105;&#20204;&#22312;Spider&#20013;&#30340;&#38382;&#39064;&#25968;&#37327;&#22686;&#21152;&#20102;&#19968;&#20493;&#12290;&#22312;&#36825;&#20010;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35299;&#26512;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL semantic parsing has made significant progress in recent years, with various models demonstrating impressive performance on the challenging Spider benchmark. However, it has also been shown that these models often struggle to generalize even when faced with small perturbations of previously (accurately) parsed expressions. This is mainly due to the linguistic form of questions in Spider which are overly specific, unnatural, and display limited variation. In this work, we use data augmentation to enhance the robustness of text-to-SQL parsers against natural language variations. Existing approaches generate question reformulations either via models trained on Spider or only introduce local changes. In contrast, we leverage the capabilities of large language models to generate more realistic and diverse questions. Using only a few prompts, we achieve a two-fold increase in the number of questions in Spider. Training on this augmented dataset yields substantial improvements on 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.08644</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#26029;&#39640;&#25928;LLMs&#30340;&#20018;&#32852;Transformer
&lt;/p&gt;
&lt;p&gt;
Tandem Transformers for Inference Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08644
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMs )&#20855;&#26377;&#33258;&#22238;&#24402;&#30340;&#29305;&#24615;&#65292;&#36825;&#20351;&#24471;&#25512;&#26029;&#36895;&#24230;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#35789;&#20803;&#26159;&#25353;&#39034;&#24207;&#29983;&#25104;&#30340;&#12290;&#23613;&#31649;&#26377;&#20123;&#39044;&#27979;&#21644;&#24182;&#34892;&#35299;&#30721;&#25216;&#26415;&#35797;&#22270;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#37117;&#26377;&#38480;&#21046;&#65306;&#35201;&#20040;&#20381;&#36182;&#26356;&#31934;&#31616;&#20294;&#20934;&#30830;&#24230;&#36739;&#20302;&#30340;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#65292;&#35201;&#20040;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22522;&#30784;LLM&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#21363;&#20018;&#32852;Transformer&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#26550;&#26500;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;(1)&#19968;&#20010;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;(2)&#19968;&#20010;&#20197;&#22359;&#27169;&#24335;&#36816;&#34892;&#30340;&#22823;&#27169;&#22411;(&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#35789;&#20803;)&#12290;&#36890;&#36807;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#22823;&#24133;&#25552;&#21319;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;PaLM2&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;PaLM2-Bison&#21644;PaLM2-Gecko&#30340;&#20018;&#32852;&#30456;&#36739;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#21319;&#20102;3.3%&#65292;&#19982;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#30456;&#27604;&#65292;&#25552;&#20379;&#20102;1.16&#20493;&#30340;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream p
&lt;/p&gt;</description></item><item><title>SemRel2024&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;14&#31181;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;&#65292;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#21512;&#21487;&#20197;&#25506;&#32034;&#21644;&#37327;&#21270;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21512;&#28085;&#30422;&#20102;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#31561;14&#31181;&#35821;&#35328;&#12290;</title><link>https://arxiv.org/abs/2402.08638</link><description>&lt;p&gt;
SemRel2024: 14&#31181;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08638
&lt;/p&gt;
&lt;p&gt;
SemRel2024&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;14&#31181;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;&#65292;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#21512;&#21487;&#20197;&#25506;&#32034;&#21644;&#37327;&#21270;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21512;&#28085;&#30422;&#20102;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#31561;14&#31181;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#21644;&#37327;&#21270;&#35821;&#20041;&#30456;&#20851;&#24615;&#26159;&#35821;&#35328;&#34920;&#36798;&#30340;&#26680;&#24515;&#12290;&#23427;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#21253;&#25324;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#25552;&#20379;&#27934;&#23519;&#12290;&#34429;&#28982;&#26089;&#26399;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#19978;&#65292;&#24448;&#24448;&#26159;&#22312;&#33521;&#35821;&#35821;&#22659;&#20013;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#26356;&#24191;&#27867;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#20540;&#24471;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SemRel&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#27597;&#35821;&#20026;14&#31181;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#30340;&#26032;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;&#65306;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#12290;&#36825;&#20123;&#35821;&#35328;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#30340;&#35821;&#31995;&#65292;&#20027;&#35201;&#22312;&#38750;&#27954;&#21644;&#20122;&#27954;&#20351;&#29992;&#65292;&#36825;&#20123;&#22320;&#21306;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30456;&#23545;&#36739;&#23569;&#12290;SemRel&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#37117;&#26159;&#19982;&#19968;&#20010;&#34920;&#31034;&#30456;&#20851;&#24615;&#24471;&#20998;&#30340;&#21477;&#23376;&#23545;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring and quantifying semantic relatedness is central to representing language. It holds significant implications across various NLP tasks, including offering insights into the capabilities and performance of Large Language Models (LLMs). While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness. In this paper, we present SemRel, a new semantic relatedness dataset collection annotated by native speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by a relatively limited availability of NLP resources. Each instance in the SemRel datasets is a sentence pair associated with a score that repr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38544;&#31169;&#21644;&#39118;&#26684;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08631</link><description>&lt;p&gt;
&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Knowledge Editing on Black-box Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38544;&#31169;&#21644;&#39118;&#26684;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#39640;&#25928;&#12289;&#31934;&#30830;&#22320;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#26356;&#26032;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#32780;&#19981;&#23545;&#20854;&#20182;&#30693;&#35782;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#30333;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#19978;&#65292;&#24573;&#35270;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#22330;&#26223;&#65306;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65292;&#21363;&#36890;&#36807;&#25509;&#21475;&#35775;&#38382;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20165;&#21487;&#29992;&#25991;&#26412;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#22312;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#19978;&#19981;&#36866;&#29992;&#19988;&#32570;&#20047;&#20840;&#38754;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#39118;&#26684;&#20445;&#30041;&#30340;&#35780;&#20272;&#32435;&#20837;&#20854;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#20013;&#30340;&#32534;&#36753;&#25968;&#25454;&#38544;&#31169;&#27844;&#28431;&#21644;&#39118;&#26684;&#36807;&#24230;&#32534;&#36753;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#36890;&#36807;&#19979;&#28216;&#21518;&#22788;&#29702;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#21407;&#22987;&#22238;&#31572;&#36827;&#34892;&#32454;&#31890;&#24230;&#32534;&#36753;&#26469;&#20445;&#25345;&#25991;&#26412;&#39118;&#26684;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#19982;&#20998;&#26512;&#34920;&#26126;&#65292;postEdit&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available. To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36719;&#25552;&#31034;&#35843;&#25972;&#12290;&#36890;&#36807;&#32771;&#34385;&#28304;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#25552;&#31034;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#36801;&#31227;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08594</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#36719;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36719;&#25552;&#31034;&#35843;&#25972;&#12290;&#36890;&#36807;&#32771;&#34385;&#28304;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#25552;&#31034;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#36801;&#31227;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#26159;&#19968;&#31181;&#20248;&#21270;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#24494;&#35843;&#25972;&#20010;&#27169;&#22411;&#21442;&#25968;&#12290;&#24403;&#36825;&#20123;&#25552;&#31034;&#22312;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#19979;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#24050;&#32463;&#35777;&#26126;&#20854;&#29305;&#21035;&#26377;&#25928;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#20026;&#27599;&#20010;&#28304;&#20219;&#21153;&#21333;&#29420;&#35757;&#32451;&#25552;&#31034;&#65292;&#28982;&#21518;&#32858;&#21512;&#23427;&#20204;&#20197;&#25552;&#20379;&#30446;&#26631;&#20219;&#21153;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#19968;&#20123;&#28304;&#20219;&#21153;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#30340;&#36127;&#38754;&#25110;&#27491;&#38754;&#24178;&#25200;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24403;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#28304;&#25552;&#31034;&#20174;&#28304;&#20219;&#21153;&#20013;&#25552;&#21462;&#30693;&#35782;&#26102;&#65292;&#38656;&#35201;&#32771;&#34385;&#28304;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#21521;&#30446;&#26631;&#20219;&#21153;&#30340;&#36801;&#31227;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#24605;&#36335;&#65292;&#36890;&#36807;&#24037;&#20316;&#22312;&#25552;&#31034;&#22312;&#28304;&#20219;&#21153;&#20043;&#38388;&#30340;&#21518;&#39564;&#20998;&#24067;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#21518;&#39564;&#20013;&#25552;&#21462;&#30340;&#26679;&#26412;&#33719;&#24471;&#20195;&#34920;&#24615;&#30340;&#28304;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning, in which prompts are optimized to adapt large-scale pre-trained language models to downstream tasks instead of fine-tuning the full model parameters, has been shown to be particularly effective when the prompts are trained in a multi-task transfer learning setting. These methods generally involve individually training prompts for each source task and then aggregating them to provide the initialization of the prompt for the target task. However, this approach critically ignores the fact that some of the source tasks could be negatively or positively interfering with each other. We argue that when we extract knowledge from source tasks via training source prompts, we need to consider this correlation among source tasks for better transfer to target tasks. To this end, we propose a Bayesian approach where we work with the posterior distribution of prompts across source tasks. We obtain representative source prompts corresponding to the samples from the posterior utilizing S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#31934;&#28860;&#21644;&#26465;&#20214;&#29983;&#25104;&#22635;&#31354;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#30495;&#23454;&#38169;&#35823;&#20462;&#27491;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26500;&#24314;&#25688;&#35201;&#20013;&#30340;&#30495;&#23454;&#22240;&#32032;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#30495;&#23454;&#19968;&#33268;&#24615;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.08581</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#31934;&#28860;&#21644;&#26465;&#20214;&#29983;&#25104;&#22635;&#31354;&#39064;&#65292;&#25552;&#39640;&#25277;&#35937;&#25688;&#35201;&#30340;&#30495;&#23454;&#38169;&#35823;&#20462;&#27491;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Factual Error Correction for Abstractive Summarization via Data Distillation and Conditional-generation Cloze
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#31934;&#28860;&#21644;&#26465;&#20214;&#29983;&#25104;&#22635;&#31354;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#30495;&#23454;&#38169;&#35823;&#20462;&#27491;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26500;&#24314;&#25688;&#35201;&#20013;&#30340;&#30495;&#23454;&#22240;&#32032;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#30495;&#23454;&#19968;&#33268;&#24615;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#30495;&#23454;&#19968;&#33268;&#24615;&#19968;&#30452;&#26159;&#24403;&#21069;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#21518;&#26399;&#32534;&#36753;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#25688;&#35201;&#20013;&#30340;&#30495;&#23454;&#22240;&#32032;&#65292;&#24182;&#19988;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#22635;&#31354;&#39064;&#30340;&#26032;&#22411;&#30495;&#23454;&#38169;&#35823;&#20462;&#27491;&#27169;&#22411;FactCloze&#12290;FactCloze&#21487;&#20197;&#22312;&#25688;&#35201;&#20013;&#26500;&#24314;&#30495;&#23454;&#22240;&#32032;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#21516;&#26102;&#33021;&#22815;&#30830;&#23450;&#31354;&#30333;&#26159;&#21542;&#21487;&#20197;&#22238;&#31572;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#32500;&#35780;&#20272;&#29983;&#25104;&#26356;&#21152;&#20934;&#30830;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;SummDSC&#30340;&#25968;&#25454;&#31934;&#28860;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19968;&#33268;&#24615;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving factual consistency in abstractive summarization has been a focus of current research. One promising approach is the post-editing method. However, previous works have yet to make sufficient use of factual factors in summaries and suffers from the negative effect of the training datasets. In this paper, we first propose a novel factual error correction model FactCloze based on a conditional-generation cloze task. FactCloze can construct the causality among factual factors while being able to determine whether the blank can be answered or not. Then, we propose a data distillation method to generate a more faithful summarization dataset SummDSC via multiple-dimensional evaluation. We experimentally validate the effectiveness of our approach, which leads to an improvement in multiple factual consistency metrics compared to baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#21453;&#21521;&#38376;&#25511;&#25915;&#20987;&#65288;AnyDoor&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#24615;&#27979;&#35797;&#22270;&#20687;&#23558;&#21453;&#21521;&#38376;&#25511;&#27880;&#20837;&#21040;&#25991;&#26412;&#27169;&#24577;&#20013;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#25110;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#12290;AnyDoor&#20855;&#26377;&#20998;&#31163;&#35774;&#32622;&#21644;&#28608;&#27963;&#26377;&#23475;&#25928;&#26524;&#30340;&#26102;&#38388;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08577</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#21453;&#21521;&#38376;&#25511;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Test-Time Backdoor Attacks on Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#21453;&#21521;&#38376;&#25511;&#25915;&#20987;&#65288;AnyDoor&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#24615;&#27979;&#35797;&#22270;&#20687;&#23558;&#21453;&#21521;&#38376;&#25511;&#27880;&#20837;&#21040;&#25991;&#26412;&#27169;&#24577;&#20013;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#25110;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#12290;AnyDoor&#20855;&#26377;&#20998;&#31163;&#35774;&#32622;&#21644;&#28608;&#27963;&#26377;&#23475;&#25928;&#26524;&#30340;&#26102;&#38388;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#38376;&#25511;&#25915;&#20987;&#36890;&#24120;&#36890;&#36807;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#26469;&#25191;&#34892;&#65292;&#20174;&#32780;&#22312;&#27979;&#35797;&#38454;&#27573;&#35302;&#21457;&#39044;&#23450;&#30340;&#26377;&#23475;&#25928;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AnyDoor&#65292;&#19968;&#31181;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#27979;&#35797;&#26102;&#21453;&#21521;&#38376;&#25511;&#25915;&#20987;&#65292;&#23427;&#20351;&#29992;&#23545;&#25239;&#24615;&#27979;&#35797;&#22270;&#20687;&#23558;&#21453;&#21521;&#38376;&#25511;&#27880;&#20837;&#21040;&#25991;&#26412;&#27169;&#24577;&#20013;&#65288;&#20849;&#20139;&#30456;&#21516;&#30340;&#36890;&#29992;&#25200;&#21160;&#65289;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#25110;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#12290;AnyDoor&#37319;&#29992;&#31867;&#20284;&#20110;&#36890;&#29992;&#23545;&#25239;&#25915;&#20987;&#30340;&#25216;&#26415;&#65292;&#20294;&#20854;&#36890;&#36807;&#33021;&#22815;&#20998;&#31163;&#26377;&#23475;&#25928;&#26524;&#30340;&#35774;&#32622;&#21644;&#28608;&#27963;&#30340;&#26102;&#38388;&#26469;&#21306;&#21035;&#20110;&#20854;&#20182;&#25915;&#20987;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;AnyDoor&#23545;&#27969;&#34892;&#30340;MLLMs&#65288;&#22914;LLaVA-1.5&#12289;MiniGPT-4&#12289;InstructBLIP&#21644;BLIP-2&#65289;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#28040;&#34701;&#30740;&#31350;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#21453;&#21521;&#38376;&#25511;&#30001;&#36890;&#29992;&#25200;&#21160;&#27880;&#20837;&#65292;AnyDoor&#21487;&#20197;&#21160;&#24577;&#25913;&#21464;&#20854;&#21453;&#21521;&#38376;&#35302;&#21457;&#25552;&#31034;/&#26377;&#23475;&#25928;&#26524;&#65292;&#20174;&#32780;&#26292;&#38706;&#20986;...
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks are commonly executed by contaminating training data, such that a trigger can activate predetermined harmful effects during the test phase. In this work, we present AnyDoor, a test-time backdoor attack against multimodal large language models (MLLMs), which involves injecting the backdoor into the textual modality using adversarial test images (sharing the same universal perturbation), without requiring access to or modification of the training data. AnyDoor employs similar techniques used in universal adversarial attacks, but distinguishes itself by its ability to decouple the timing of setup and activation of harmful effects. In our experiments, we validate the effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4, InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies. Notably, because the backdoor is injected by a universal perturbation, AnyDoor can dynamically change its backdoor trigger prompts/harmful effects, exposing 
&lt;/p&gt;</description></item><item><title>Agent Smith&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#20256;&#26579;&#24615;&#36234;&#29425;&#65292;&#35813;&#38382;&#39064;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#36234;&#29425;&#19968;&#20010;&#20195;&#29702;&#26469;&#36805;&#36895;&#24863;&#26579;&#25152;&#26377;&#20195;&#29702;&#24182;&#23548;&#33268;&#26377;&#23475;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.08567</link><description>&lt;p&gt;
Agent Smith:&#19968;&#24352;&#22270;&#20687;&#21487;&#20197;&#36805;&#36895;&#36234;&#29425;&#19968;&#30334;&#19975;&#20010;&#22810;&#27169;&#24577;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08567
&lt;/p&gt;
&lt;p&gt;
Agent Smith&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#20256;&#26579;&#24615;&#36234;&#29425;&#65292;&#35813;&#38382;&#39064;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#36234;&#29425;&#19968;&#20010;&#20195;&#29702;&#26469;&#36805;&#36895;&#24863;&#26579;&#25152;&#26377;&#20195;&#29702;&#24182;&#23548;&#33268;&#26377;&#23475;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#20195;&#29702;&#21487;&#20197;&#25509;&#25910;&#25351;&#20196;&#65292;&#25429;&#25417;&#22270;&#20687;&#65292;&#20174;&#20869;&#23384;&#20013;&#26816;&#32034;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#20915;&#23450;&#20351;&#29992;&#21738;&#20123;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#32418;&#38431;&#35780;&#20272;&#21457;&#29616;&#24694;&#24847;&#22270;&#20687;/&#25552;&#31034;&#21487;&#20197;&#36234;&#29425;MLLM&#24182;&#23548;&#33268;&#19981;&#23545;&#40784;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#26356;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#31216;&#20026;&#20256;&#26579;&#24615;&#36234;&#29425;&#12290;&#23427;&#28041;&#21450;&#21040;&#23545;&#21333;&#20010;&#20195;&#29702;&#36827;&#34892;&#31616;&#21333;&#30340;&#36234;&#29425;&#65292;&#26080;&#38656;&#26469;&#33258;&#23545;&#25163;&#30340;&#36827;&#19968;&#27493;&#24178;&#39044;&#65292;&#65288;&#20960;&#20046;&#65289;&#25152;&#26377;&#20195;&#29702;&#23558;&#20197;&#25351;&#25968;&#32423;&#21035;&#34987;&#24863;&#26579;&#24182;&#23637;&#31034;&#26377;&#23475;&#34892;&#20026;&#12290;&#20026;&#20102;&#39564;&#35777;&#20256;&#26579;&#24615;&#36234;&#29425;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#21253;&#21547;&#39640;&#36798;&#19968;&#30334;&#19975;&#20010;LLaVA-1.5&#20195;&#29702;&#30340;&#22810;&#20195;&#29702;&#29615;&#22659;&#65292;&#24182;&#23558;&#38543;&#26426;&#21305;&#37197;&#23545;&#32842;&#22825;&#20316;&#20026;&#22810;&#20195;&#29702;&#20132;&#20114;&#30340;&#27010;&#24565;&#39564;&#35777;&#23454;&#20363;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#65288;&#20256;&#26579;&#24615;&#65289;&#24694;&#24847;&#22270;&#20687;&#36755;&#20837;&#21040;&#20219;&#24847;&#36873;&#25321;&#30340;&#20195;&#29702;&#30340;&#20869;&#23384;&#20013;&#23601;&#36275;&#20197;&#23454;&#29616;&#20256;&#26579;&#24615;&#36234;&#29425;&#12290;
&lt;/p&gt;
&lt;p&gt;
A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;MoE&#26041;&#27861;&#65288;MoLA&#65289;&#65292;&#29992;&#20110;Transformer-based&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#23618;&#21487;&#20197;&#28789;&#27963;&#22320;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;LoRA&#19987;&#23478;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#39640;&#23618;&#38656;&#35201;&#26356;&#22810;&#30340;LoRA&#19987;&#23478;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08562</link><description>&lt;p&gt;
Higher Layers Need More LoRA Experts
&lt;/p&gt;
&lt;p&gt;
Higher Layers Need More LoRA Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08562
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;MoE&#26041;&#27861;&#65288;MoLA&#65289;&#65292;&#29992;&#20110;Transformer-based&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#23618;&#21487;&#20197;&#28789;&#27963;&#22320;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;LoRA&#19987;&#23478;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#39640;&#23618;&#38656;&#35201;&#26356;&#22810;&#30340;LoRA&#19987;&#23478;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PEFT&#65289;&#25216;&#26415;&#65292;&#22914;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#35757;&#32451;&#25928;&#29575;&#65292;&#20294;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#20173;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#21162;&#21147;&#25972;&#21512;&#20102;LoRA&#21644;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#65292;&#20197;&#25552;&#39640;PEFT&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#26377;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#25913;&#36827;&#24102;&#26377;MoE&#30340;LoRA&#30340;&#25928;&#29575;&#30340;&#30740;&#31350;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;MoE&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#19987;&#23478;&#20855;&#26377;&#19981;&#21516;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#36824;&#23384;&#22312;&#19968;&#20123;&#20887;&#20313;&#12290;&#36825;&#20010;&#35770;&#26029;&#26159;&#21542;&#20063;&#36866;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;MoE&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;MoE&#26041;&#27861;&#65292;&#31216;&#20026;MoLA&#65288;\textit{\textbf{M}oE-L\textbf{o}RA with \textbf{L}ayer-wise Expert \textbf{A}llocation&#65289;&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#23618;&#21487;&#20197;&#28789;&#27963;&#22320;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;LoRA&#19987;&#23478;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;&#20855;&#26377;&#19981;&#21516;&#23618;&#32423;&#19987;&#23478;&#37197;&#32622;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#23545;&#20845;&#20010;&#30693;&#21517;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24120;&#35782;&#38382;&#31572;&#22522;&#20934;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA) offer training efficiency on Large Language Models, but their impact on model performance remains limited. Recent efforts integrate LoRA and Mixture-of-Experts (MoE) to improve the performance of PEFT methods. Despite promising results, research on improving the efficiency of LoRA with MoE is still in its early stages. Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy. Does this statement also apply to parameter-efficient MoE? In this paper, we introduce a novel parameter-efficient MoE method, \textit{\textbf{M}oE-L\textbf{o}RA with \textbf{L}ayer-wise Expert \textbf{A}llocation (MoLA)} for Transformer-based models, where each model layer has the flexibility to employ a varying number of LoRA experts. We investigate several architectures with varying layer-wise expert configurations. Experiments on six well-known NLP and commonsense QA benc
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Concept-1K&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24433;&#21709;&#22240;&#32032;&#21253;&#25324;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#12290;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;LoRA&#25216;&#26415;&#26080;&#27861;&#28385;&#36275;&#24615;&#33021;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25506;&#32034;&#21644;&#32531;&#35299;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.08526</link><description>&lt;p&gt;
Concept-1K&#65306;&#19968;&#31181;&#29992;&#20110;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#30340;&#26032;&#22411;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Concept-1K: A Novel Benchmark for Instance Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08526
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Concept-1K&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24433;&#21709;&#22240;&#32032;&#21253;&#25324;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#12290;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;LoRA&#25216;&#26415;&#26080;&#27861;&#28385;&#36275;&#24615;&#33021;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25506;&#32034;&#21644;&#32531;&#35299;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#65288;IL&#65289;&#23545;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20154;&#31867;&#32423;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;IL&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#26080;&#27861;&#35780;&#20272;PLM&#20013;&#30340;&#36951;&#24536;&#65292;&#20351;&#20154;&#35823;&#20197;&#20026;PLM&#19981;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;IL&#22330;&#26223;&#65292;&#31216;&#20026;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#25903;&#25345;&#25968;&#37327;&#32423;&#26356;&#22823;&#30340;IL&#27493;&#39588;&#30340;&#26032;&#25968;&#25454;&#38598;Concept-1K&#12290;&#22522;&#20110;&#23545;Concept-1K&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#19988;&#36951;&#24536;&#21463;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;&#19968;&#31181;&#27969;&#34892;&#30340;&#24494;&#35843;&#25216;&#26415;LoRA&#37117;&#26410;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22330;&#26223;&#65292;&#25506;&#32034;PLM&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#40723;&#21169;&#35774;&#35745;&#26356;&#24378;&#22823;&#30340;&#25216;&#26415;&#20197;&#20943;&#36731;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental learning (IL) is essential to realize the human-level intelligence in the neural network. However, existing IL scenarios and datasets are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs do not suffer from catastrophic forgetting. To this end, we propose a challenging IL scenario called instance-incremental learning (IIL) and a novel dataset called Concept-1K, which supports an order of magnitude larger IL steps. Based on the experiments on Concept-1K, we reveal that billion-parameter PLMs still suffer from catastrophic forgetting, and the forgetting is affected by both model scale, pretraining, and buffer size. Furthermore, existing IL methods and a popular finetuning technique, LoRA, fail to achieve satisfactory performance. Our study provides a novel scenario for future studies to explore the catastrophic forgetting of PLMs and encourage more powerful techniques to be designed for alleviating the forgetting in PLMs. The data, code and scripts ar
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#35777;&#25454;&#21644;&#39118;&#26684;&#30340;&#21453;&#39539;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;Reddit ChangeMyView&#25968;&#25454;&#38598;&#20013;&#30340;&#24086;&#23376;&#65292;&#24182;&#21487;&#29992;&#20110;&#35770;&#35777;&#30340;&#25913;&#36827;&#21644;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5 turbo&#27169;&#22411;&#22312;&#35770;&#35777;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#39118;&#26684;&#34701;&#21512;&#33021;&#21147;&#12290;&#20114;&#24800;&#24335;&#21453;&#39539;&#30340;&#25928;&#26524;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.08498</link><description>&lt;p&gt;
&#23457;&#35745;&#21453;&#28779;&#65306;&#35780;&#20272;&#20855;&#26377;&#35777;&#25454;&#21644;&#39118;&#26684;&#30340;&#20808;&#36827;&#21453;&#39539;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08498
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#35777;&#25454;&#21644;&#39118;&#26684;&#30340;&#21453;&#39539;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;Reddit ChangeMyView&#25968;&#25454;&#38598;&#20013;&#30340;&#24086;&#23376;&#65292;&#24182;&#21487;&#29992;&#20110;&#35770;&#35777;&#30340;&#25913;&#36827;&#21644;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5 turbo&#27169;&#22411;&#22312;&#35770;&#35777;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#39118;&#26684;&#34701;&#21512;&#33021;&#21147;&#12290;&#20114;&#24800;&#24335;&#21453;&#39539;&#30340;&#25928;&#26524;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25511;&#21046;&#24615;&#21453;&#39539;&#30340;&#21512;&#25104;&#65292;&#26088;&#22312;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;&#35770;&#35777;&#30340;&#25913;&#36827;&#12289;&#25366;&#25496;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;Reddit ChangeMyView&#25968;&#25454;&#38598;&#20013;&#30340;&#24086;&#23376;&#30456;&#32467;&#21512;&#30340;&#20016;&#23500;&#30340;&#21453;&#39539;&#65292;&#36825;&#20123;&#21453;&#39539;&#34701;&#20837;&#20102;&#20174;&#39640;&#36136;&#37327;&#26469;&#28304;&#20013;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#29983;&#25104;&#65292;&#35843;&#25972;&#20102;&#35777;&#25454;&#21644;&#35770;&#35777;&#39118;&#26684;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;Counterfire&#35821;&#26009;&#24211;&#21253;&#25324;&#20174;GPT-3.5 turbo&#12289;Koala&#21644;PaLM 2&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30340;&#20004;&#20010;&#24494;&#35843;&#21464;&#20307;&#29983;&#25104;&#30340;&#35770;&#35777;&#65288;N = 32,000&#65289;&#12290;&#27169;&#22411;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#35777;&#25454;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#25913;&#20889;&#33021;&#21147;&#65292;&#23613;&#31649;&#35789;&#27719;&#37325;&#21472;&#26377;&#38480;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#39118;&#26684;&#34701;&#21512;&#65288;&#23545;&#20110;&#8220;&#20114;&#24800;&#8221;&#30340;&#24471;&#20998;&#20026;0.9682&#65289;&#65292;&#26174;&#31034;&#20102;LLM&#34701;&#21512;&#22810;&#26679;&#39118;&#26684;&#30340;&#33021;&#21147;&#12290;&#22312;&#25152;&#26377;&#27169;&#22411;&#20013;&#65292;GPT-3.5 turbo&#22312;&#35770;&#35777;&#36136;&#37327;&#35780;&#20272;&#20013;&#26174;&#31034;&#20986;&#26368;&#39640;&#20998;&#25968;&#65292;&#34920;&#29616;&#20986;&#19968;&#33268;&#20934;&#30830;&#24615;&#65288;&#24471;&#20998; &gt;0.8&#65289;&#12290;&#22312;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#20013;&#65292;&#20114;&#24800;&#24335;&#21453;&#39539;&#35777;&#26126;&#25928;&#26524;&#26368;&#20339;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#35770;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel dataset for the controlled composition of counterarguments designed for further applications in argument refining, mining, and evaluation. Our dataset constitutes enriched counter-arguments to posts in the Reddit ChangeMyView dataset that are integrated with evidence retrieved from high-quality sources and generated based on user preferences, adjusting the critical attributes of evidence and argument style. The resultant Counterfire corpus comprises arguments generated from GPT-3.5 turbo, Koala, and PaLM 2 models and two of their finetuned variants (N = 32,000). Model evaluation indicates strong paraphrasing abilities with evidence, albeit limited word overlap, while demonstrating high style integration (0.9682 for 'reciprocity'), showing the ability of LLM to assimilate diverse styles. Of all models, GPT-3.5 turbo showed the highest scores in argument quality evaluation, showing consistent accuracy (score &gt;0.8). In further analyses, reciprocity-style counterargument
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08496</link><description>&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review of Data-to-Text NLG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08496
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#26088;&#22312;&#20840;&#38754;&#20998;&#26512;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#37325;&#28857;&#26159;&#30830;&#23450;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20379;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#22238;&#39038;&#20013;&#21457;&#29616;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26816;&#26597;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#12289;&#24212;&#29992;&#12289;&#22810;&#35821;&#35328;&#24615;&#21644;&#24187;&#35273;&#32531;&#35299;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#20026;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#37319;&#29992;&#34164;&#28085;&#23545;&#40784;&#65292;&#20197;&#20248;&#21270;&#21487;&#34892;&#24615;&#65292;&#25552;&#21462;&#26377;&#29702;&#30340;&#26041;&#24335;&#25552;&#20379;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.08479</link><description>&lt;p&gt;
&#21487;&#20449;&#30340;&#21462;&#26679;&#21512;&#29702;&#21270;&#36890;&#36807;&#21322;&#30417;&#30563;&#30340;&#34164;&#28085;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Plausible Extractive Rationalization through Semi-Supervised Entailment Signal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#37319;&#29992;&#34164;&#28085;&#23545;&#40784;&#65292;&#20197;&#20248;&#21270;&#21487;&#34892;&#24615;&#65292;&#25552;&#21462;&#26377;&#29702;&#30340;&#26041;&#24335;&#25552;&#20379;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#21644;&#19981;&#36879;&#26126;&#30340;&#40657;&#30418;&#23376;&#27169;&#22411;&#30340;&#22686;&#21152;&#38656;&#35201;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#25514;&#26045;&#65292;&#20854;&#20013;&#19968;&#31181;&#36873;&#25321;&#26159;&#25552;&#21462;&#26377;&#29702;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#20316;&#20026;&#26356;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#36825;&#20123;&#27169;&#22411;&#65292;&#20063;&#31216;&#20026;&#20808;&#35299;&#37322;&#28982;&#21518;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#35299;&#37322;&#27169;&#22411;&#26469;&#25552;&#21462;&#26377;&#29702;&#65292;&#28982;&#21518;&#20351;&#29992;&#25552;&#21462;&#30340;&#20449;&#24687;&#26469;&#35843;&#25972;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20379;&#31934;&#30830;&#21644;&#24544;&#23454;&#30340;&#35299;&#37322;&#65292;&#30001;&#25552;&#21462;&#30340;&#26377;&#29702;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#21322;&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#25552;&#21462;&#26377;&#29702;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#20010;&#23567;&#22411;&#30340;&#26377;&#30417;&#30563;&#26377;&#29702;&#38598;&#65288;10%&#65289;&#19978;&#36827;&#19968;&#27493;&#24494;&#35843;&#23427;&#12290;&#36890;&#36807;&#34164;&#28085;&#23545;&#40784;&#65292;NLI&#39044;&#27979;&#27169;&#22411;&#34987;&#21033;&#29992;&#20316;&#20026;&#35299;&#37322;&#27169;&#22411;&#30340;&#19968;&#31181;&#30417;&#30563;&#20449;&#21495;&#28304;&#12290;&#36890;&#36807;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#24378;&#21046;&#35299;&#37322;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#23545;&#40784;&#19968;&#33268;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing use of complex and opaque black box models requires the adoption of interpretable measures, one such option is extractive rationalizing models, which serve as a more interpretable alternative. These models, also known as Explain-Then-Predict models, employ an explainer model to extract rationales and subsequently condition the predictor with the extracted information. Their primary objective is to provide precise and faithful explanations, represented by the extracted rationales. In this paper, we take a semi-supervised approach to optimize for the plausibility of extracted rationales. We adopt a pre-trained natural language inference (NLI) model and further fine-tune it on a small set of supervised rationales ($10\%$). The NLI predictor is leveraged as a source of supervisory signals to the explainer via entailment alignment. We show that, by enforcing the alignment agreement between the explanation and answer in a question-answering task, the performance can be improve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;ChatGPT&#22312;&#29983;&#25104;&#20851;&#20110;&#20044;&#20811;&#20848;&#25112;&#20105;&#30340;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#12289;&#24555;&#36895;&#19988;&#22823;&#35268;&#27169;&#22320;&#29983;&#25104;&#36924;&#30495;&#30340;&#23450;&#21046;&#34394;&#20551;&#20449;&#24687;&#65292;&#32780;&#19988;&#36825;&#20123;&#34394;&#20551;&#20449;&#24687;&#24456;&#38590;&#34987;&#20154;&#31867;&#35835;&#32773;&#21644;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#21487;&#38752;&#22320;&#21306;&#20998;&#20986;&#26469;&#12290;</title><link>https://arxiv.org/abs/2402.08467</link><description>&lt;p&gt;
&#32993;&#20081;&#36896;&#35875;&#65306;&#32469;&#36807;ChatGPT&#30340;&#38450;&#25252;&#25514;&#26045;&#65292;&#22823;&#35268;&#27169;&#29983;&#25104;&#38590;&#20197;&#26816;&#27979;&#30340;&#34394;&#20551;&#20449;&#24687;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
Lying Blindly: Bypassing ChatGPT's Safeguards to Generate Hard-to-Detect Disinformation Claims at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;ChatGPT&#22312;&#29983;&#25104;&#20851;&#20110;&#20044;&#20811;&#20848;&#25112;&#20105;&#30340;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#12289;&#24555;&#36895;&#19988;&#22823;&#35268;&#27169;&#22320;&#29983;&#25104;&#36924;&#30495;&#30340;&#23450;&#21046;&#34394;&#20551;&#20449;&#24687;&#65292;&#32780;&#19988;&#36825;&#20123;&#34394;&#20551;&#20449;&#24687;&#24456;&#38590;&#34987;&#20154;&#31867;&#35835;&#32773;&#21644;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#21487;&#38752;&#22320;&#21306;&#20998;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#29087;&#32451;&#65292;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#30149;&#27602;&#24335;&#34394;&#20551;&#20449;&#24687;&#27963;&#21160;&#20013;&#30340;&#28389;&#29992;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#20005;&#37325;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#29983;&#25104;&#20851;&#20110;&#20044;&#20811;&#20848;&#25112;&#20105;&#30340;&#26080;&#26465;&#20214;&#22768;&#26126;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#36229;&#20986;&#20854;&#30693;&#35782;&#30028;&#38480;&#30340;&#20107;&#20214;&#65292;&#24182;&#35780;&#20272;&#36825;&#20123;&#22768;&#26126;&#26159;&#21542;&#21487;&#20197;&#34987;&#20154;&#31867;&#35835;&#32773;&#21644;&#33258;&#21160;&#21270;&#24037;&#20855;&#19982;&#20154;&#31867;&#32534;&#20889;&#30340;&#22768;&#26126;&#21306;&#20998;&#20986;&#26469;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;ClaimReview&#20013;&#20851;&#20110;&#25112;&#20105;&#30340;&#22768;&#26126;&#65292;&#36825;&#20123;&#22768;&#26126;&#26159;&#30001;IFCN&#27880;&#20876;&#30340;&#20107;&#23454;&#26680;&#26597;&#21592;&#25776;&#20889;&#30340;&#65292;&#20197;&#21450;ChatGPT&#29983;&#25104;&#30340;&#31867;&#20284;&#30340;&#30701;&#31687;&#20869;&#23481;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;ChatGPT&#21487;&#20197;&#24555;&#36895;&#12289;&#24265;&#20215;&#19988;&#35268;&#27169;&#21270;&#22320;&#29983;&#25104;&#36924;&#30495;&#19988;&#38024;&#23545;&#29305;&#23450;&#30446;&#26631;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#32780;&#19988;&#36825;&#20123;&#22768;&#26126;&#20154;&#31867;&#21644;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#26080;&#27861;&#21487;&#38752;&#22320;&#21306;&#20998;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) become more proficient, their misuse in large-scale viral disinformation campaigns is a growing concern. This study explores the capability of ChatGPT to generate unconditioned claims about the war in Ukraine, an event beyond its knowledge cutoff, and evaluates whether such claims can be differentiated by human readers and automated tools from human-written ones. We compare war-related claims from ClaimReview, authored by IFCN-registered fact-checkers, and similar short-form content generated by ChatGPT. We demonstrate that ChatGPT can produce realistic, target-specific disinformation cheaply, fast, and at scale, and that these claims cannot be reliably distinguished by humans or existing automated tools.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#19977;&#20010;&#25104;&#29087;&#30340;&#20154;&#31867;&#20915;&#31574;&#29702;&#35770;&#25972;&#21512;&#21040;&#19968;&#36215;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#30446;&#30340;&#24615;&#20154;&#31867;&#34892;&#21160;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#34892;&#21160;&#30340;&#35266;&#28857;&#24212;&#29992;&#20110;&#23545;&#35805;&#29992;&#25143;&#30028;&#38754;&#12290;&#36890;&#36807;&#29702;&#35299;ChatGPT&#30340;&#26234;&#33021;&#26469;&#28304;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#36164;&#28304;&#30340;&#21516;&#26102;&#33719;&#24471;&#23545;&#25105;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#35748;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.08403</link><description>&lt;p&gt;
LLMs&#21644;&#20154;&#31867;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
LLMs and the Human Condition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#19977;&#20010;&#25104;&#29087;&#30340;&#20154;&#31867;&#20915;&#31574;&#29702;&#35770;&#25972;&#21512;&#21040;&#19968;&#36215;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#30446;&#30340;&#24615;&#20154;&#31867;&#34892;&#21160;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#34892;&#21160;&#30340;&#35266;&#28857;&#24212;&#29992;&#20110;&#23545;&#35805;&#29992;&#25143;&#30028;&#38754;&#12290;&#36890;&#36807;&#29702;&#35299;ChatGPT&#30340;&#26234;&#33021;&#26469;&#28304;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#36164;&#28304;&#30340;&#21516;&#26102;&#33719;&#24471;&#23545;&#25105;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20154;&#31867;&#20915;&#31574;&#30340;&#19977;&#20010;&#25104;&#29087;&#29702;&#35770;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#23558;&#23427;&#20204;&#25972;&#21512;&#36215;&#26469;&#25552;&#20379;&#19968;&#20010;&#30446;&#30340;&#24615;&#20154;&#31867;&#34892;&#21160;&#30340;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#34892;&#21160;&#30340;&#35266;&#28857;&#24212;&#29992;&#20110;&#23545;&#35805;&#29992;&#25143;&#30028;&#38754;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29702;&#35770;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#26412;&#25991;&#26088;&#22312;&#37325;&#26032;&#28608;&#21457;&#23545;&#29702;&#35299;LLMs&#23454;&#38469;&#25191;&#34892;&#30340;&#20852;&#36259;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#25152;&#26377;&#25968;&#25454;&#19978;&#36816;&#34892;&#38590;&#20197;&#29702;&#35299;&#30340;&#26426;&#22120;&#23398;&#20064;&#20363;&#31243;&#12290;&#24403;&#19968;&#21488;&#21806;&#20215;&#19981;&#21040;50&#32654;&#20803;&#30340;&#26641;&#33683;&#27966;&#30005;&#33041;&#27604;&#31532;&#19968;&#21488;&#21830;&#19994;Cray&#36229;&#32423;&#35745;&#31639;&#26426;&#24555;400&#20493;&#26102;&#65292;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#21487;&#20197;&#25509;&#36817;&#25317;&#26377;&#26080;&#25968;&#38543;&#26426;&#25171;&#23383;&#24182;&#29983;&#25104;&#26377;&#24847;&#20041;&#25991;&#23383;&#30340;&#29492;&#23376;&#12290;&#36890;&#36807;&#29702;&#35299;ChatGPT&#30340;&#34920;&#29616;&#26234;&#33021;&#30340;&#26469;&#28304;&#65292;&#20063;&#35768;&#25105;&#20204;&#21487;&#20197;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#36827;&#34892;&#21516;&#26679;&#30340;&#39764;&#26415;&#65292;&#24182;&#22312;&#27492;&#36807;&#31243;&#20013;&#33719;&#24471;&#19968;&#20123;&#20851;&#20110;&#25105;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents three established theories of human decision-making and describes how they can be integrated to provide a model of purposive human action. Taking seriously the idea of language as action the model is then applied to the conversational user interfaces. Theory based AI research has had a hard time recently and the aim here is to revitalise interest in understanding what LLMs are actually doing other than running poorly understood machine learning routines over all the data the relevant Big Tech company can hoover up. When a raspberry pi computer for under 50USD is up to 400 times faster than the first commercial Cray super computer~\cite{crayVpi}, Big Tech can get really close to having an infinite number of monkeys typing at random and producing text, some of which will make sense. By understanding where ChatGPT's apparent intelligence comes from, perhaps we can perform the magic with fewer resources and at the same time gain some understanding about our relationship
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Minecraft&#20195;&#29702;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#21644;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24314;&#36896;&#32773;&#21644;&#24314;&#31569;&#24072;&#35774;&#32622;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#25552;&#20986;&#20102;&#28548;&#28165;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19982;&#20195;&#29702;&#36827;&#34892;&#22312;&#32447;&#20132;&#20114;&#30340;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2402.08392</link><description>&lt;p&gt;
&#20316;&#20026;Minecraft&#20195;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Minecraft Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Minecraft&#20195;&#29702;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#21644;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24314;&#36896;&#32773;&#21644;&#24314;&#31569;&#24072;&#35774;&#32622;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#25552;&#20986;&#20102;&#28548;&#28165;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19982;&#20195;&#29702;&#36827;&#34892;&#22312;&#32447;&#20132;&#20114;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#20805;&#24403;Minecraft&#20195;&#29702;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#22312;&#24314;&#36896;&#32773;&#21644;&#24314;&#31569;&#24072;&#35774;&#32622;&#20013;&#24212;&#29992;&#21644;&#35780;&#20272;LLMs&#65292;&#24341;&#20837;&#28548;&#28165;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#25913;&#36827;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#19982;&#20195;&#29702;&#36827;&#34892;&#22312;&#32447;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#24182;&#23545;&#20043;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we examine the use of Large Language Models (LLMs) in the challenging setting of acting as a Minecraft agent. We apply and evaluate LLMs in the builder and architect settings, introduce clarification questions and examining the challenges and opportunities for improvement. In addition, we present a platform for online interaction with the agents and an evaluation against previous works.
&lt;/p&gt;</description></item><item><title>&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21487;&#20197;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08382</link><description>&lt;p&gt;
&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#22312;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Punctuation Restoration Improves Structure Understanding without Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08382
&lt;/p&gt;
&lt;p&gt;
&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21487;&#20197;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#21435;&#22122;&#31561;&#65292;&#22312;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#25191;&#34892;&#20174;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21040;&#20250;&#35805;&#20219;&#21153;&#30340;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#25429;&#25417;&#25991;&#26412;&#30340;&#21477;&#27861;&#25110;&#35821;&#20041;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#33853;&#21518;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#35821;&#35328;&#24615;&#33021;&#21644;&#26426;&#22120;&#33021;&#21147;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#24402;&#22240;&#20110;&#24403;&#21069;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#26410;&#33021;&#20805;&#20998;&#20256;&#36882;&#35821;&#35328;&#32467;&#26500;&#30693;&#35782;&#32473;&#35745;&#31639;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#23545;&#32467;&#26500;&#30456;&#20851;&#20219;&#21153;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#34920;&#29616;&#30340;&#25913;&#21892;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#12289;&#20998;&#22359;&#21644;&#35789;&#24615;&#26631;&#27880;&#12290;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21487;&#20197;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;&#24182;&#20135;&#29983;&#26356;&#21152;&#40065;&#26834;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning objectives like language modeling and de-noising constitute a significant part in producing pre-trained models that perform various downstream applications from natural language understanding to conversational tasks. However, despite impressive conversational capabilities of recent large language model, their abilities to capture syntactic or semantic structure within text lag behind. We hypothesize that the mismatch between linguistic performance and competence in machines is attributable to insufficient transfer of linguistic structure knowledge to computational systems with currently popular pre-training objectives. We show that punctuation restoration transfers to improvements in in- and out-of-distribution performance on structure-related tasks like named entity recognition, open information extraction, chunking, and part-of-speech tagging. Punctuation restoration is an effective learning objective that can improve structure understanding and yield a more rob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#28145;&#20837;&#35780;&#20272;&#20102;&#22312;&#23454;&#36341;&#20013;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#19968;&#20010;&#22810;&#24180;&#30340;&#22269;&#38469;&#39033;&#30446;&#38598;&#20013;&#35780;&#20272;&#65292;&#23545;&#19968;&#20010;&#22312;FIFA World Cup&#32972;&#26223;&#19979;&#36830;&#32493;&#36816;&#34892;&#20102;9&#20010;&#26376;&#30340;&#30495;&#23454;&#37096;&#32626;&#30340;FootballDB&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.08349</link><description>&lt;p&gt;
&#22522;&#20110;&#30495;&#23454;&#29992;&#25143;&#26597;&#35810;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#25968;&#25454;&#27169;&#22411;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#28145;&#20837;&#35780;&#20272;&#20102;&#22312;&#23454;&#36341;&#20013;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#19968;&#20010;&#22810;&#24180;&#30340;&#22269;&#38469;&#39033;&#30446;&#38598;&#20013;&#35780;&#20272;&#65292;&#23545;&#19968;&#20010;&#22312;FIFA World Cup&#32972;&#26223;&#19979;&#36830;&#32493;&#36816;&#34892;&#20102;9&#20010;&#26376;&#30340;&#30495;&#23454;&#37096;&#32626;&#30340;FootballDB&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#65288;&#20063;&#31216;&#20026;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#31995;&#32479;&#65289;&#24050;&#25104;&#20026;&#24357;&#21512;&#29992;&#25143;&#33021;&#21147;&#19982;&#22522;&#20110;SQL&#30340;&#25968;&#25454;&#35775;&#38382;&#20043;&#38388;&#24046;&#36317;&#30340;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#31995;&#32479;&#23558;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#35831;&#27714;&#36716;&#21270;&#20026;&#29305;&#23450;&#25968;&#25454;&#24211;&#30340;&#26377;&#25928;SQL&#35821;&#21477;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#20351;&#24471;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#21463;&#30410;&#21290;&#27973;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#20123;&#31995;&#32479;&#22312;&#24120;&#24120;&#26159;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19981;&#26029;&#21462;&#24471;&#26032;&#30340;&#39640;&#20998;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#12289;&#29616;&#23454;&#22330;&#26223;&#20013;&#23545;&#19981;&#21516;&#25968;&#25454;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30340;&#31995;&#32479;&#24615;&#25506;&#32034;&#26126;&#26174;&#32570;&#20047;&#12290;&#26412;&#25991;&#22522;&#20110;&#19968;&#20010;&#22810;&#24180;&#22269;&#38469;&#39033;&#30446;&#20851;&#20110;&#25991;&#26412;&#21040;SQL&#30028;&#38754;&#30340;&#38598;&#20013;&#35780;&#20272;&#65292;&#25552;&#20379;&#20102;&#23545;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#22312;&#23454;&#36341;&#20013;&#25968;&#25454;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#39318;&#27425;&#28145;&#24230;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20110;FootballDB&#30340;&#30495;&#23454;&#37096;&#32626;&#65292;&#35813;&#31995;&#32479;&#22312;FIFA World Cup&#30340;&#32972;&#26223;&#19979;&#36830;&#32493;&#36816;&#34892;&#20102;9&#20010;&#26376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL systems (also known as NL-to-SQL systems) have become an increasingly popular solution for bridging the gap between user capabilities and SQL-based data access. These systems translate user requests in natural language to valid SQL statements for a specific database. Recent Text-to-SQL systems have benefited from the rapid improvement of transformer-based language models. However, while Text-to-SQL systems that incorporate such models continuously reach new high scores on -- often synthetic -- benchmark datasets, a systematic exploration of their robustness towards different data models in a real-world, realistic scenario is notably missing. This paper provides the first in-depth evaluation of the data model robustness of Text-to-SQL systems in practice based on a multi-year international project focused on Text-to-SQL interfaces. Our evaluation is based on a real-world deployment of FootballDB, a system that was deployed over a 9 month period in the context of the FIFA Wor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#31867;&#22120;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#36755;&#20837;&#25552;&#31034;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#21464;&#21270;&#65292;&#20197;&#22686;&#21152;&#20854;&#36879;&#26126;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#26681;&#25454;&#36755;&#20837;&#30340;&#19981;&#21516;&#25552;&#31034;&#32780;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23545;&#21050;&#28608;&#20570;&#20986;&#30340;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.08341</link><description>&lt;p&gt;
&#29992;&#20998;&#31867;&#22120;&#39537;&#21160;&#30340;&#26041;&#27861;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#65306;&#25991;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Eliciting Big Five Personality Traits in Large Language Models: A Textual Analysis with Classifier-Driven Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#31867;&#22120;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#36755;&#20837;&#25552;&#31034;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#21464;&#21270;&#65292;&#20197;&#22686;&#21152;&#20854;&#36879;&#26126;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#26681;&#25454;&#36755;&#20837;&#30340;&#19981;&#21516;&#25552;&#31034;&#32780;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23545;&#21050;&#28608;&#20570;&#20986;&#30340;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25307;&#32856;&#32972;&#26223;&#19979;&#34987;&#24212;&#32856;&#32773;&#21644;&#38599;&#20027;&#24191;&#27867;&#20351;&#29992;&#65292;&#28982;&#32780;&#36825;&#20063;&#24341;&#21457;&#20102;&#20247;&#22810;&#20262;&#29702;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#19982;&#36825;&#20123;&#8220;&#40657;&#30418;&#23376;&#8221;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#26377;&#20851;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#35843;&#26597;LLMs&#30340;&#20154;&#26684;&#29305;&#36136;&#26469;&#22686;&#21152;&#20854;&#36879;&#26126;&#24230;&#65292;&#20294;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#37117;&#35201;&#27714;&#27169;&#22411;&#26469;&#23436;&#25104;&#20154;&#26684;&#35780;&#20272;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;&#19981;&#21516;&#36755;&#20837;&#25552;&#31034;&#19979;&#27169;&#22411;&#30340;&#36755;&#20986;&#21464;&#21270;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#24120;&#35265;&#38754;&#35797;&#38382;&#39064;&#21644;&#26088;&#22312;&#24341;&#21457;&#29305;&#23450;&#30340;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#30340;&#25552;&#31034;&#26469;&#36827;&#34892;&#26032;&#39062;&#30340;&#35843;&#26597;&#26041;&#27861;&#65292;&#20197;&#26816;&#26597;&#27169;&#22411;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#23481;&#26131;&#28608;&#27963;&#29305;&#23450;&#20154;&#26684;&#29305;&#36136;&#65292;&#24182;&#26681;&#25454;&#20854;&#36755;&#20986;&#20013;&#30340;&#35821;&#35328;&#26469;&#35780;&#20272;&#20854;&#20154;&#26684;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21453;&#22797;&#25552;&#20379;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly being utilized by both candidates and employers in the recruitment context. However, with this comes numerous ethical concerns, particularly related to the lack of transparency in these "black-box" models. Although previous studies have sought to increase the transparency of these models by investigating the personality traits of LLMs, many of the previous studies have provided them with personality assessments to complete. On the other hand, this study seeks to obtain a better understanding of such models by examining their output variations based on different input prompts. Specifically, we use a novel elicitation approach using prompts derived from common interview questions, as well as prompts designed to elicit particular Big Five personality traits to examine whether the models were susceptible to trait-activation like humans are, to measure their personality based on the language used in their outputs. To do so, we repeatedly prompte
&lt;/p&gt;</description></item><item><title>PreFLMR&#26159;&#19968;&#31181;&#25193;&#23637;&#32454;&#31890;&#24230;&#36831;&#20132;&#20114;&#22810;&#27169;&#24577;&#26816;&#32034;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#24335;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#21644;&#35780;&#20272;&#26694;&#26550;M2KR&#36827;&#34892;&#20102;&#24320;&#21457;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;PreFLMR&#30340;&#25193;&#23637;&#34892;&#20026;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20026;&#36890;&#29992;&#22810;&#27169;&#24577;&#26816;&#32034;&#22120;&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.08327</link><description>&lt;p&gt;
PreFLMR: &#25193;&#23637;&#32454;&#31890;&#24230;&#36831;&#20132;&#20114;&#22810;&#27169;&#24577;&#26816;&#32034;&#22120;
&lt;/p&gt;
&lt;p&gt;
PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08327
&lt;/p&gt;
&lt;p&gt;
PreFLMR&#26159;&#19968;&#31181;&#25193;&#23637;&#32454;&#31890;&#24230;&#36831;&#20132;&#20114;&#22810;&#27169;&#24577;&#26816;&#32034;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#24335;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#21644;&#35780;&#20272;&#26694;&#26550;M2KR&#36827;&#34892;&#20102;&#24320;&#21457;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;PreFLMR&#30340;&#25193;&#23637;&#34892;&#20026;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20026;&#36890;&#29992;&#22810;&#27169;&#24577;&#26816;&#32034;&#22120;&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#21644;&#35270;&#35273;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35832;&#22914;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;(KB-VQA)&#36825;&#26679;&#30340;&#20005;&#26684;&#20219;&#21153;&#20013;&#65292;&#21364;&#38754;&#20020;&#30528;&#20174;&#25991;&#26723;&#38598;&#21512;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#20197;&#29992;&#20110;&#22609;&#36896;&#38382;&#39064;&#31572;&#26696;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#26694;&#26550;M2KR&#65292;&#29992;&#20110;KB-VQA&#12290;M2KR&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#20854;&#25972;&#21512;&#20026;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#36890;&#29992;&#22810;&#27169;&#24577;&#26816;&#32034;&#22120;&#30340;&#22522;&#20934;&#20219;&#21153;&#22871;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;M2KR&#24320;&#21457;&#20102;PreFLMR&#65292;&#36825;&#26159;&#26368;&#36817;&#24320;&#21457;&#30340;&#32454;&#31890;&#24230;&#36831;&#20132;&#20114;&#22810;&#27169;&#24577;&#26816;&#32034;&#22120;(FLMR)&#26041;&#27861;&#30340;&#39044;&#35757;&#32451;&#29256;&#26412;&#65292;&#24182;&#19988;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#30340;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#23545;PreFLMR&#30340;&#25193;&#23637;&#34892;&#20026;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#26088;&#22312;&#23545;&#26410;&#26469;&#21457;&#23637;&#30340;&#36890;&#29992;&#22810;&#27169;&#24577;&#26816;&#32034;&#22120;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Multimodal Models (LMMs) excel in natural language and visual understanding but are challenged by exacting tasks such as Knowledge-based Visual Question Answering (KB-VQA) which involve the retrieval of relevant information from document collections to use in shaping answers to questions. We present an extensive training and evaluation framework, M2KR, for KB-VQA. M2KR contains a collection of vision and language tasks which we have incorporated into a single suite of benchmark tasks for training and evaluating general-purpose multi-modal retrievers. We use M2KR to develop PreFLMR, a pre-trained version of the recently developed Fine-grained Late-interaction Multi-modal Retriever (FLMR) approach to KB-VQA, and we report new state-of-the-art results across a range of tasks. We also present investigations into the scaling behaviors of PreFLMR intended to be useful in future developments in general-purpose multi-modal retrievers.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#33889;&#33796;&#29273;&#12289;&#24847;&#22823;&#21033;&#21644;&#24503;&#22269;&#31461;&#35805;&#20013;&#26126;&#30830;&#34920;&#36798;&#30340;&#20215;&#20540;&#35266;&#24046;&#24322;&#65292;&#20351;&#29992;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#32599;&#30424;&#37327;&#21270;&#20998;&#26512;&#12290;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#36825;&#20123;&#22269;&#23478;&#20043;&#38388;&#23384;&#22312;&#20849;&#20139;&#30340;&#25991;&#21270;&#29702;&#35299;&#21644;&#23545;&#21892;&#33391;&#12289;&#36981;&#20174;&#21644;&#26222;&#36941;&#20215;&#20540;&#35266;&#30340;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2402.08318</link><description>&lt;p&gt;
&#12298;&#31461;&#35805;&#20013;&#26126;&#30830;&#34920;&#36798;&#30340;&#31038;&#20250;&#20215;&#20540;&#35266;&#65306;&#19977;&#31181;&#27431;&#27954;&#25991;&#21270;&#30340;&#27604;&#36739;&#12299;
&lt;/p&gt;
&lt;p&gt;
Explicit References to Social Values in Fairy Tales: A Comparison between Three European Cultures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08318
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#33889;&#33796;&#29273;&#12289;&#24847;&#22823;&#21033;&#21644;&#24503;&#22269;&#31461;&#35805;&#20013;&#26126;&#30830;&#34920;&#36798;&#30340;&#20215;&#20540;&#35266;&#24046;&#24322;&#65292;&#20351;&#29992;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#32599;&#30424;&#37327;&#21270;&#20998;&#26512;&#12290;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#36825;&#20123;&#22269;&#23478;&#20043;&#38388;&#23384;&#22312;&#20849;&#20139;&#30340;&#25991;&#21270;&#29702;&#35299;&#21644;&#23545;&#21892;&#33391;&#12289;&#36981;&#20174;&#21644;&#26222;&#36941;&#20215;&#20540;&#35266;&#30340;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#31461;&#35805;&#20013;&#30340;&#31038;&#20250;&#20215;&#20540;&#35266;&#21487;&#20197;&#20102;&#35299;&#20215;&#20540;&#35266;&#22312;&#26102;&#31354;&#20013;&#30340;&#20256;&#36882;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#32599;&#30424;&#26469;&#37327;&#21270;&#33889;&#33796;&#29273;&#12289;&#24847;&#22823;&#21033;&#21644;&#24503;&#22269;&#31461;&#35805;&#20013;&#30340;&#20215;&#20540;&#35266;&#20256;&#36882;&#12290;&#25105;&#20204;&#30740;&#31350;&#36825;&#19977;&#31181;&#22269;&#23478;&#30340;&#31461;&#35805;&#22312;&#26126;&#30830;&#34920;&#36798;&#20215;&#20540;&#35266;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25351;&#23450;&#20102;&#19968;&#20010;&#20805;&#28385;&#20215;&#20540;&#35266;&#30340;&#35789;&#27719;&#21015;&#34920;&#65292;&#32771;&#34385;&#23427;&#20204;&#30340;&#35789;&#24178;&#65292;&#24182;&#20998;&#26512;&#22312;&#19987;&#38376;&#39044;&#35757;&#32451;&#30340;Word2Vec&#27169;&#22411;&#20013;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#35282;&#24230;&#39564;&#35777;&#21644;&#25209;&#21028;&#24615;&#35752;&#35770;&#37327;&#21270;&#27169;&#22411;&#25152;&#25552;&#20986;&#30340;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#22797;&#29992;&#21644;&#21487;&#37325;&#29616;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#21382;&#21490;&#35821;&#26009;&#24211;&#20013;&#26126;&#30830;&#24341;&#29992;&#30340;&#20215;&#20540;&#35266;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#26263;&#31034;&#26377;&#30528;&#20849;&#20139;&#25991;&#21270;&#29702;&#35299;&#21644;&#23545;&#21892;&#33391;&#12289;&#36981;&#20174;&#21644;&#26222;&#36941;&#20215;&#20540;&#35266;&#30340;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of social values in fairy tales opens the possibility to learn about the communication of values across space and time. We propose to study the communication of values in fairy tales from Portugal, Italy and Germany using a technique called word embedding with a compass to quantify vocabulary differences and commonalities. We study how these three national traditions of fairy tales differ in their explicit references to values. To do this, we specify a list of value-charged tokens, consider their word stems and analyse the distance between these in a bespoke pre-trained Word2Vec model. We triangulate and critically discuss the validity of the resulting hypotheses emerging from this quantitative model. Our claim is that this is a reusable and reproducible method for the study of the values explicitly referenced in historical corpora. Finally, our preliminary findings hint at a shared cultural understanding and the expression of values such as Benevolence, Conformity, and Unive
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;&#30340;&#30005;&#23376;&#37038;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#39640;&#25928;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08309</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#21521;&#37327;&#26816;&#27979;&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Prompted Contextual Vectors for Spear-Phishing Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08309
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;&#30340;&#30005;&#23376;&#37038;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#39640;&#25928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#30340;&#30005;&#23376;&#37038;&#20214;&#24182;&#26041;&#20415;&#30446;&#26631;&#20390;&#23519;&#26469;&#21319;&#32423;&#20102;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLMs&#30340;&#38598;&#21512;&#26469;&#21019;&#24314;&#34920;&#31034;&#21521;&#37327;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#26469;&#25512;&#29702;&#21644;&#22238;&#31572;&#20154;&#24037;&#21046;&#23450;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#37327;&#21270;&#30005;&#23376;&#37038;&#20214;&#20869;&#23481;&#20013;&#24120;&#35265;&#35828;&#26381;&#21407;&#21017;&#30340;&#23384;&#22312;&#65292;&#20026;&#19979;&#28216;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#19978;&#19979;&#25991;&#25991;&#26723;&#21521;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19987;&#26377;&#31995;&#32479;&#29983;&#25104;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#33258;&#21160;&#21270;&#30446;&#26631;&#20390;&#23519;&#21644;&#38035;&#40060;&#30005;&#23376;&#37038;&#20214;&#30340;&#21019;&#24314;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20165;&#21253;&#21547;&#20256;&#32479;&#38035;&#40060;&#21644;&#33391;&#24615;&#30005;&#23376;&#37038;&#20214;&#30340;&#35757;&#32451;&#38598;&#20013;&#23454;&#29616;&#20102;91%&#30340;F1&#24471;&#20998;&#65292;&#20854;&#20013;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#19968;&#31181;&#21019;&#26032;&#30340;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spear-phishing attacks present a significant security challenge, with large language models (LLMs) escalating the threat by generating convincing emails and facilitating target reconnaissance. To address this, we propose a detection approach based on a novel document vectorization method that utilizes an ensemble of LLMs to create representation vectors. By prompting LLMs to reason and respond to human-crafted questions, we quantify the presence of common persuasion principles in the email's content, producing prompted contextual document vectors for a downstream supervised machine learning model. We evaluate our method using a unique dataset generated by a proprietary system that automates target reconnaissance and spear-phishing email creation. Our method achieves a 91% F1 score in identifying LLM-generated spear-phishing emails, with the training set comprising only traditional phishing and benign emails. Key contributions include an innovative document vectorization method utilizin
&lt;/p&gt;</description></item><item><title>ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08303</link><description>&lt;p&gt;
ChatCell: &#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
ChatCell: Facilitating Single-Cell Analysis with Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08303
&lt;/p&gt;
&lt;p&gt;
ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#22312;&#31185;&#23398;&#20013;&#30340;&#24433;&#21709;&#26085;&#30410;&#31361;&#20986;&#12290;LLMs&#22312;&#20219;&#21153;&#27867;&#21270;&#21644;&#33258;&#30001;&#23545;&#35805;&#26041;&#38754;&#30340;&#26032;&#20852;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#25512;&#36827;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#36825;&#20010;&#26500;&#25104;&#29983;&#29289;&#20307;&#22522;&#30784;&#26500;&#20214;&#30340;&#39046;&#22495;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#24403;&#21069;&#26041;&#27861;&#22312;&#30693;&#35782;&#38376;&#27099;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;LLMs&#22312;&#25484;&#25569;&#21333;&#32454;&#32990;&#25968;&#25454;&#26041;&#38754;&#30340;&#20805;&#20998;&#21033;&#29992;&#65292;&#24433;&#21709;&#20102;&#30452;&#25509;&#21487;&#35775;&#38382;&#21644;&#24555;&#36895;&#36845;&#20195;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ChatCell&#65292;&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#22312;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#39046;&#22495;&#33719;&#24471;&#20102;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08277</link><description>&lt;p&gt;
&#26397;&#30528;&#24544;&#23454;&#21644;&#24378;&#22823;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#19987;&#23478;&#30340;&#26041;&#21521;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#24544;&#23454;&#21644;&#21487;&#36861;&#36394;&#30340;&#31572;&#26696;&#30340;&#36827;&#27493;&#23545;&#20110;&#21508;&#31181;&#30740;&#31350;&#21644;&#23454;&#36341;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#31181;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#21487;&#38752;&#30340;&#26469;&#28304;&#25552;&#20379;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#22312;&#20351;&#29992;LLM&#26102;&#24050;&#32463;&#35777;&#26126;&#22312;&#24341;&#29992;&#27491;&#30830;&#30340;&#26469;&#28304;&#65288;&#26469;&#28304;&#36136;&#37327;&#65289;&#21644;&#20934;&#30830;&#22320;&#34920;&#31034;&#26469;&#28304;&#20013;&#30340;&#20449;&#24687;&#65288;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65289;&#26041;&#38754;&#24037;&#20316;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;LLM&#65292;&#20197;&#25552;&#39640;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#21160;&#25968;&#25454;&#36136;&#37327;&#36807;&#28388;&#22120;&#65292;&#21487;&#20197;&#22823;&#35268;&#27169;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#23545;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;%&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#26696;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#35780;&#20272;&#30340;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#35780;&#20272;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#25512;&#29702;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24635;&#32467;&#20102;LLM&#22312;&#34920;&#26684;&#25512;&#29702;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#22686;&#24378;&#34920;&#26684;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.08259</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#25512;&#29702;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Table Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08259
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#25512;&#29702;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24635;&#32467;&#20102;LLM&#22312;&#34920;&#26684;&#25512;&#29702;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#22686;&#24378;&#34920;&#26684;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25512;&#29702;&#26088;&#22312;&#26681;&#25454;&#25552;&#20379;&#30340;&#34920;&#26684;&#21644;&#21487;&#36873;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#26681;&#25454;&#29992;&#25143;&#35201;&#27714;&#29983;&#25104;&#30456;&#24212;&#30340;&#38382;&#39064;&#31572;&#26696;&#65292;&#26377;&#25928;&#25552;&#39640;&#33719;&#21462;&#20449;&#24687;&#30340;&#25928;&#29575;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#25104;&#20026;&#34920;&#26684;&#25512;&#29702;&#30340;&#20027;&#27969;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#65292;&#36824;&#36229;&#36807;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20173;&#32570;&#20047;LLM-based&#34920;&#26684;&#25512;&#29702;&#24037;&#20316;&#30340;&#24635;&#32467;&#12290;&#30001;&#20110;&#29616;&#26377;&#30740;&#31350;&#30340;&#32570;&#20047;&#65292;&#20851;&#20110;&#22312;LLM&#26102;&#20195;&#22914;&#20309;&#25552;&#39640;&#34920;&#26684;&#25512;&#29702;&#24615;&#33021;&#30340;&#25216;&#26415;&#65292;&#20026;&#20160;&#20040;LLMs&#22312;&#34920;&#26684;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20197;&#21450;&#22914;&#20309;&#22312;&#26410;&#26469;&#22686;&#24378;&#34920;&#26684;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#12290;&#36825;&#19968;&#24046;&#36317;&#20005;&#37325;&#38480;&#21046;&#20102;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#22238;&#31572;&#19978;&#36848;&#38382;&#39064;&#24182;&#25512;&#21160;&#22522;&#20110;LLMs&#30340;&#34920;&#26684;&#25512;&#29702;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#36825;&#20010;&#35843;&#26597;&#20998;&#26512;&#29616;&#26377;&#30340;&#30740;&#31350;&#65292;&#28608;&#21457;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Table reasoning, which aims to generate the corresponding answer to the question following the user requirement according to the provided table, and optionally a text description of the table, effectively improving the efficiency of obtaining information. Recently, using Large Language Models (LLMs) has become the mainstream method for table reasoning, because it not only significantly reduces the annotation cost but also exceeds the performance of previous methods. However, existing research still lacks a summary of LLM-based table reasoning works. Due to the existing lack of research, questions about which techniques can improve table reasoning performance in the era of LLMs, why LLMs excel at table reasoning, and how to enhance table reasoning abilities in the future, remain largely unexplored. This gap significantly limits progress in research. To answer the above questions and advance table reasoning research with LLMs, we present this survey to analyze existing research, inspirin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23454;&#20363;&#28151;&#28102;&#25512;&#26029;&#65288;IOI&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33268;&#21147;&#20110;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#20915;&#31574;&#38544;&#31169;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#36716;&#25442;&#21644;&#20445;&#25252;&#65292;&#23454;&#29616;&#20102;&#22312;&#32500;&#25345;&#27169;&#22411;&#40657;&#30418;&#30340;&#21516;&#26102;&#20445;&#25252;&#20915;&#31574;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2402.08227</link><description>&lt;p&gt;
&#20351;&#29992;&#23454;&#20363;&#28151;&#28102;&#30340;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Language Model Inference with Instance Obfuscation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23454;&#20363;&#28151;&#28102;&#25512;&#26029;&#65288;IOI&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33268;&#21147;&#20110;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#20915;&#31574;&#38544;&#31169;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#36716;&#25442;&#21644;&#20445;&#25252;&#65292;&#23454;&#29616;&#20102;&#22312;&#32500;&#25345;&#27169;&#22411;&#40657;&#30418;&#30340;&#21516;&#26102;&#20445;&#25252;&#20915;&#31574;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#26381;&#21153;&#65288;LMaaS&#65289;&#65292;&#20026;&#24320;&#21457;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#20415;&#21033;&#30340;&#35775;&#38382;&#26041;&#24335;&#65292;&#33021;&#22815;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#22312;&#26381;&#21153;&#35843;&#29992;&#36807;&#31243;&#20013;&#65292;&#36755;&#20837;&#25968;&#25454;&#21644;&#25512;&#26029;&#32467;&#26524;&#31561;&#21253;&#21547;&#31169;&#20154;&#20449;&#24687;&#30340;&#20869;&#23481;&#20197;&#26126;&#25991;&#24418;&#24335;&#26292;&#38706;&#20986;&#26469;&#65292;&#23548;&#33268;&#20102;&#38544;&#31169;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#36890;&#36807;&#20174;&#29992;&#25143;&#31471;&#24320;&#22987;&#30340;&#25216;&#26415;&#65292;&#22914;&#22122;&#22768;&#28155;&#21152;&#21644;&#20869;&#23481;&#25200;&#21160;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#25442;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#34920;&#31034;&#24418;&#24335;&#26469;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#65292;&#28982;&#32780;&#25512;&#26029;&#32467;&#26524;&#30340;&#20445;&#25252;&#65292;&#21363;&#20915;&#31574;&#38544;&#31169;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#31354;&#30333;&#39029;&#12290;&#20026;&#20102;&#32500;&#25345;LMaaS&#30340;&#40657;&#30418;&#26041;&#24335;&#65292;&#36827;&#34892;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20915;&#31574;&#30340;&#20445;&#25252;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#35813;&#36807;&#31243;&#24517;&#39035;&#23545;&#27169;&#22411;&#26469;&#35828;&#26159;&#26080;&#32541;&#30340;&#65292;&#24182;&#20276;&#38543;&#30528;&#26377;&#38480;&#30340;&#36890;&#20449;&#21644;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23454;&#20363;&#28151;&#28102;&#25512;&#26029;&#65288;IOI&#65289;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#35299;&#20915;&#20102;&#20915;&#31574;&#38544;&#31169;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models as a Service (LMaaS) offers convenient access for developers and researchers to perform inference using pre-trained language models. Nonetheless, the input data and the inference results containing private information are exposed as plaintext during the service call, leading to privacy issues. Recent studies have started tackling the privacy issue by transforming input data into privacy-preserving representation from the user-end with the techniques such as noise addition and content perturbation, while the exploration of inference result protection, namely decision privacy, is still a blank page. In order to maintain the black-box manner of LMaaS, conducting data privacy protection, especially for the decision, is a challenging task because the process has to be seamless to the models and accompanied by limited communication and computation overhead. We thus propose Instance-Obfuscated Inference (IOI) method, which focuses on addressing the decision privacy issue of na
&lt;/p&gt;</description></item><item><title>BBox-Adapter&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#25490;&#21517;&#24335;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#21644;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#36879;&#26126;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.08219</link><description>&lt;p&gt;
BBox-Adapter: &#36731;&#37327;&#32423;&#36866;&#37197;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08219
&lt;/p&gt;
&lt;p&gt;
BBox-Adapter&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#25490;&#21517;&#24335;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#21644;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#36879;&#26126;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-4&#21644;Gemini&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#20219;&#21153;&#30340;&#35201;&#27714;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#21442;&#25968;&#12289;&#23884;&#20837;&#21644;&#36755;&#20986;&#27010;&#29575;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#29616;&#26377;&#30340;&#24494;&#35843;&#36866;&#24212;&#26041;&#27861;&#26159;&#19981;&#36866;&#29992;&#30340;&#12290;&#22240;&#27492;&#65292;&#21482;&#33021;&#36890;&#36807;&#23427;&#20204;&#30340;API&#26381;&#21153;&#36866;&#24212;&#36825;&#20123;&#40657;&#30418;LLMs&#65292;&#36825;&#24341;&#21457;&#20102;&#36879;&#26126;&#24230;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BBox-Adapter&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#40657;&#30418;LLMs&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#12290;BBox-Adapter&#36890;&#36807;&#23558;&#30446;&#26631;&#25968;&#25454;&#35270;&#20026;&#27491;&#26679;&#26412;&#65292;&#23558;&#28304;&#25968;&#25454;&#35270;&#20026;&#36127;&#26679;&#26412;&#26469;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#12290;&#23427;&#37319;&#29992;&#22522;&#20110;&#25490;&#21517;&#30340;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#26469;&#25552;&#39640;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#24809;&#32602;&#28304;&#22495;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#20855;&#26377;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#23558;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#12289;&#20154;&#31867;&#25110;AI&#21453;&#39304;&#30340;&#23454;&#26102;&#27491;&#26679;&#26412;&#37319;&#26679;&#19982;&#20808;&#21069;&#36866;&#24212;&#30340;&#36127;&#26679;&#26412;&#25968;&#25454;&#30456;&#32467;&#21512;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BBox-Adapter&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#32780;&#28789;&#27963;&#30340;&#40657;&#30418;LLMs&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#35270;&#35273;&#22522;&#30784;&#25991;&#26412;&#25200;&#21160;&#26041;&#27861;&#65292;&#22914;&#25171;&#23383;&#38169;&#35823;&#21644;&#21333;&#35789;&#39034;&#24207;&#28151;&#25490;&#65292;&#20511;&#37492;&#35748;&#30693;&#21644;&#35821;&#35328;&#31185;&#23398;&#65292;&#20197;&#36830;&#32493;&#30340;&#26041;&#24335;&#24863;&#30693;&#25991;&#26412;&#30340;&#25200;&#21160;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25429;&#25417;&#21477;&#23376;&#32423;&#25991;&#26412;&#35821;&#20041;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08183</link><description>&lt;p&gt;
&#20687;&#32032;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pixel Sentence Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#35270;&#35273;&#22522;&#30784;&#25991;&#26412;&#25200;&#21160;&#26041;&#27861;&#65292;&#22914;&#25171;&#23383;&#38169;&#35823;&#21644;&#21333;&#35789;&#39034;&#24207;&#28151;&#25490;&#65292;&#20511;&#37492;&#35748;&#30693;&#21644;&#35821;&#35328;&#31185;&#23398;&#65292;&#20197;&#36830;&#32493;&#30340;&#26041;&#24335;&#24863;&#30693;&#25991;&#26412;&#30340;&#25200;&#21160;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25429;&#25417;&#21477;&#23376;&#32423;&#25991;&#26412;&#35821;&#20041;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#38271;&#26399;&#20197;&#26469;&#34987;&#35748;&#20026;&#22312;&#25429;&#25417;&#21477;&#23376;&#21644;&#25991;&#26723;&#32423;&#35821;&#20041;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#23613;&#31649;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#20174;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#20013;&#23558;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#36716;&#31227;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35789;&#25152;&#24341;&#20837;&#30340;&#23376;&#35789;&#21333;&#20803;&#30340;&#31163;&#25955;&#24615;&#65292;&#38480;&#21046;&#20102;&#23545;&#36755;&#20837;&#36827;&#34892;&#23567;&#25200;&#21160;&#20197;&#24418;&#25104;&#20445;&#30041;&#35821;&#20041;&#30340;&#27491;&#21521;&#23545;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#23398;&#20064;&#21477;&#23376;&#32423;&#25991;&#26412;&#35821;&#20041;&#35270;&#20316;&#19968;&#20010;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#12290;&#20511;&#37492;&#35748;&#30693;&#21644;&#35821;&#35328;&#31185;&#23398;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20687;&#25171;&#23383;&#38169;&#35823;&#21644;&#21333;&#35789;&#39034;&#24207;&#28151;&#25490;&#31561;&#22522;&#20110;&#35270;&#35273;&#22522;&#30784;&#25991;&#26412;&#25200;&#21160;&#26041;&#27861;&#65292;&#19982;&#20154;&#31867;&#35748;&#30693;&#27169;&#24335;&#20849;&#40483;&#65292;&#24182;&#20351;&#25991;&#26412;&#30340;&#25200;&#21160;&#34987;&#24863;&#30693;&#20026;&#36830;&#32493;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#19968;&#27493;&#21152;&#24378;&#20102;&#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#30340;&#20027;&#39064;&#23545;&#40784;&#35757;&#32451;&#21644;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30340;&#20223;&#30495;&#21487;&#35270;&#21270;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models are long known to be subpar in capturing sentence and document-level semantics. Though heavily investigated, transferring perturbation-based methods from unsupervised visual representation learning to NLP remains an unsolved problem. This is largely due to the discreteness of subword units brought by tokenization of language models, limiting small perturbations of inputs to form semantics-preserved positive pairs. In this work, we conceptualize the learning of sentence-level textual semantics as a visual representation learning process. Drawing from cognitive and linguistic sciences, we introduce an unsupervised visual sentence representation learning framework, employing visually-grounded text perturbation methods like typos and word order shuffling, resonating with human cognitive patterns, and enabling perturbation to texts to be perceived as continuous. Our approach is further bolstered by large-scale unsupervised topical alignment training and natural la
&lt;/p&gt;</description></item><item><title>CMA-R&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#35299;&#37322;&#20102;&#31070;&#32463;&#27169;&#22411;&#22312;Twitter&#19978;&#36827;&#34892;&#35875;&#35328;&#26816;&#27979;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;&#20851;&#38190;&#25512;&#25991;&#21644;&#22240;&#26524;&#24433;&#21709;&#21333;&#35789;&#65292;&#25552;&#39640;&#20102;&#23545;&#40657;&#30418;&#23376;&#35875;&#35328;&#26816;&#27979;&#31995;&#32479;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.08155</link><description>&lt;p&gt;
CMA-R&#65306;&#29992;&#20110;&#35299;&#37322;&#35875;&#35328;&#26816;&#27979;&#30340;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
CMA-R:Causal Mediation Analysis for Explaining Rumour Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08155
&lt;/p&gt;
&lt;p&gt;
CMA-R&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#35299;&#37322;&#20102;&#31070;&#32463;&#27169;&#22411;&#22312;Twitter&#19978;&#36827;&#34892;&#35875;&#35328;&#26816;&#27979;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;&#20851;&#38190;&#25512;&#25991;&#21644;&#22240;&#26524;&#24433;&#21709;&#21333;&#35789;&#65292;&#25552;&#39640;&#20102;&#23545;&#40657;&#30418;&#23376;&#35875;&#35328;&#26816;&#27979;&#31995;&#32479;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#24212;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#27169;&#22411;&#22312;Twitter&#19978;&#36827;&#34892;&#35875;&#35328;&#26816;&#27979;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#36755;&#20837;&#21644;&#32593;&#32476;&#23618;&#38754;&#36827;&#34892;&#24178;&#39044;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#36755;&#20986;&#20013;&#25512;&#25991;&#21644;&#21333;&#35789;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;CMA-R - &#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#29992;&#20110;&#35875;&#35328;&#26816;&#27979; - &#21487;&#20197;&#35782;&#21035;&#20986;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#20851;&#20110;&#25925;&#20107;&#30495;&#23454;&#24615;&#30340;&#20851;&#38190;&#25512;&#25991;&#30340;&#26174;&#33879;&#24615;&#25512;&#25991;&#65292;&#24182;&#23637;&#31034;&#20986;&#24378;&#28872;&#30340;&#19968;&#33268;&#24615;&#12290;CMA-R&#36824;&#21487;&#20197;&#31361;&#20986;&#26174;&#33879;&#24615;&#25512;&#25991;&#20013;&#20855;&#26377;&#22240;&#26524;&#24433;&#21709;&#30340;&#21333;&#35789;&#65292;&#25552;&#20379;&#23545;&#36825;&#20123;&#40657;&#30418;&#23376;&#35875;&#35328;&#26816;&#27979;&#31995;&#32479;&#30340;&#21478;&#19968;&#23618;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#20195;&#30721;&#21487;&#22312;&#27492;&#22788;&#25214;&#21040;&#65306;https://github.com/ltian678/cma-r.
&lt;/p&gt;
&lt;p&gt;
We apply causal mediation analysis to explain the decision-making process of neural models for rumour detection on Twitter. Interventions at the input and network level reveal the causal impacts of tweets and words in the model output. We find that our approach CMA-R -- Causal Mediation Analysis for Rumour detection -- identifies salient tweets that explain model predictions and show strong agreement with human judgements for critical tweets determining the truthfulness of stories. CMA-R can further highlight causally impactful words in the salient tweets, providing another layer of interpretability and transparency into these blackbox rumour detection systems. Code is available at: https://github.com/ltian678/cma-r.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20559;&#22909;&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#22522;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#24494;&#35843;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08114</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Preference Learning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20559;&#22909;&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#22522;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#24494;&#35843;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#30340;&#24494;&#35843;&#25216;&#26415;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23545;&#20110;&#23545;&#40784;&#36825;&#20123;&#27169;&#22411;&#26469;&#35828;&#65292;&#26368;&#20851;&#38190;&#30340;&#32771;&#34385;&#26159;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;&#20154;&#21147;&#36164;&#28304;&#65292;&#25110;&#32773;&#22312;LLM&#26412;&#36523;&#34987;&#29992;&#20316;oracle&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;&#27169;&#22411;&#36164;&#28304;&#12290;&#20174;&#20154;&#31867;&#25110;AI&#20559;&#22909;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF / RLAIF&#65289;&#26159;&#36825;&#31181;&#25216;&#26415;&#26368;&#31361;&#20986;&#30340;&#20363;&#23376;&#65292;&#20294;&#23427;&#24448;&#24448;&#22797;&#26434;&#19988;&#19981;&#31283;&#23450;&#12290;&#26368;&#36817;&#65292;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#26356;&#31616;&#21333;&#21644;&#26356;&#31283;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;DPO&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#20559;&#22909;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#29109;&#21644;DPO&#20248;&#21270;&#30340;&#38544;&#24335;&#20559;&#22909;&#27169;&#22411;&#30340;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#23454;&#29992;&#37319;&#38598;&#20989;&#25968;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#22522;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#24494;&#35843;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles. Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an active learning strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;BiasMedQA&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#20219;&#21153;&#20013;LLMs&#30340;&#35748;&#30693;&#20559;&#35265;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#21457;&#29616;LLMs&#22312;&#38754;&#23545;&#21253;&#21547;&#35748;&#30693;&#20559;&#35265;&#30340;&#20020;&#24202;&#38382;&#39064;&#26102;&#65292;&#20854;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#26126;&#26174;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.08113</link><description>&lt;p&gt;
&#35299;&#20915;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing cognitive bias in medical language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;BiasMedQA&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#20219;&#21153;&#20013;LLMs&#30340;&#35748;&#30693;&#20559;&#35265;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#21457;&#29616;LLMs&#22312;&#38754;&#23545;&#21253;&#21547;&#35748;&#30693;&#20559;&#35265;&#30340;&#20020;&#24202;&#38382;&#39064;&#26102;&#65292;&#20854;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#26126;&#26174;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#21307;&#23398;&#39046;&#22495;&#24050;&#32463;&#24341;&#36215;&#20102;&#37325;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#27169;&#25311;&#20020;&#24202;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#20934;&#30830;&#24615;&#24456;&#26377;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#20020;&#24202;&#20915;&#31574;&#27604;&#27169;&#25311;&#26356;&#22797;&#26434;&#65292;&#22240;&#20026;&#21307;&#29983;&#30340;&#20915;&#31574;&#21463;&#21040;&#35768;&#22810;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#35748;&#30693;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#38754;&#23545;&#21253;&#21547;&#35748;&#30693;&#20559;&#35265;&#30340;&#20020;&#24202;&#38382;&#39064;&#26102;&#65292;&#19982;&#19981;&#21253;&#21547;&#36825;&#20123;&#20559;&#35265;&#30340;&#38382;&#39064;&#30456;&#27604;&#65292;&#20854;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#20250;&#26126;&#26174;&#38477;&#20302;&#65292;&#36825;&#19968;&#38382;&#39064;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#30340;&#20551;&#35774;&#35748;&#20026;&#65292;&#24403;LLMs&#38754;&#23545;&#21253;&#21547;&#35748;&#30693;&#20559;&#35265;&#30340;&#20020;&#24202;&#38382;&#39064;&#26102;&#65292;&#19982;&#19981;&#21253;&#21547;&#36825;&#20123;&#20559;&#35265;&#30340;&#38382;&#39064;&#30456;&#27604;&#65292;&#20854;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#20250;&#26126;&#26174;&#38477;&#20302;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;BiasMedQA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#20351;&#29992;BiasMedQA&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20845;&#20010;LLMs&#65292;&#20998;&#21035;&#26159;GPT-4&#12289;Mixtral-8x70B&#12289;GPT-3.5&#12289;PaLM-2&#12289;Llama 2 70B-chat&#21644;&#21307;&#23398;&#19987;&#19994;&#30340;PMC Llama 13B&#12290;&#25105;&#20204;&#22312;127&#20010;&#20020;&#24202;&#38382;&#39064;&#19978;&#27979;&#35797;&#20102;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of large language models (LLMs) into the medical field has gained significant attention due to their promising accuracy in simulated clinical decision-making settings. However, clinical decision-making is more complex than simulations because physicians' decisions are shaped by many factors, including the presence of cognitive bias. However, the degree to which LLMs are susceptible to the same cognitive biases that affect human clinicians remains unexplored. Our hypothesis posits that when LLMs are confronted with clinical questions containing cognitive biases, they will yield significantly less accurate responses compared to the same questions presented without such biases.In this study, we developed BiasMedQA, a novel benchmark for evaluating cognitive biases in LLMs applied to medical tasks. Using BiasMedQA we evaluated six LLMs, namely GPT-4, Mixtral-8x70B, GPT-3.5, PaLM-2, Llama 2 70B-chat, and the medically specialized PMC Llama 13B. We tested these models on 1,27
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;GPT-3.5&#22312;&#38476;&#29983;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#25239;&#24615;&#34920;&#26029;&#24320;&#26041;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#36824;&#20998;&#26512;&#20102;GPT-3.5&#22312;&#20462;&#25913;&#20449;&#24687;&#30340;&#25968;&#25454;&#24211;&#19978;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08100</link><description>&lt;p&gt;
&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;GPT-3.5&#22312;&#38476;&#29983;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#25239;&#24615;&#34920;&#26029;&#24320;&#26041;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#36824;&#20998;&#26512;&#20102;GPT-3.5&#22312;&#20462;&#25913;&#20449;&#24687;&#30340;&#25968;&#25454;&#24211;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#25991;&#26412;&#25551;&#36848;&#20197;&#29983;&#25104;&#20195;&#30721;&#20284;&#20046;&#26159;&#38646;-shot&#22330;&#26223;&#19979;&#25351;&#20196;&#36981;&#24490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#39033;&#24050;&#23454;&#29616;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#36825;&#31181;&#32763;&#35793;&#33021;&#21147;&#30340;&#22240;&#32032;&#26159;&#24050;&#32463;&#35265;&#36807;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;&#30456;&#20851;&#20195;&#30721;&#12290;&#36825;&#31181;&#24433;&#21709;&#34987;&#31216;&#20026;&#25968;&#25454;&#27745;&#26579;&#12290;  &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#25968;&#25454;&#27745;&#26579;&#23545;GPT-3.5&#22312;&#25991;&#26412;&#21040;SQL&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;GPTs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#65292;&#24182;&#20351;&#29992;&#24050;&#30693;&#30340;Spider&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#30340;&#26032;&#30340;&#38476;&#29983;&#25968;&#25454;&#38598;Termite&#26469;&#26816;&#26597;GPT-3.5&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#23545;&#25239;&#24615;&#34920;&#26029;&#24320;&#65288;ATD&#65289;&#26041;&#27861;&#20998;&#26512;&#20102;GPT-3.5&#22312;&#20855;&#26377;&#20462;&#25913;&#20449;&#24687;&#30340;&#25968;&#25454;&#24211;&#19978;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#24211;&#20013;&#21024;&#38500;&#32467;&#26500;&#20449;&#24687;&#26469;&#20351;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#22797;&#26434;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3.5&#22312;&#38476;&#29983;&#30340;Termite&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding textual description to generate code seems to be an achieved capability of instruction-following Large Language Models (LLMs) in zero-shot scenario. However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination.   In this study, we investigate the impact of Data Contamination on the performance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we introduce a novel method to detect Data Contamination in GPTs and examine GPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new unfamiliar dataset Termite. Furthermore, we analyze GPT-3.5's efficacy on databases with modified information via an adversarial table disconnection (ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of information from the database. Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar T
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;10&#19975;&#23567;&#26102;&#25968;&#25454;&#30340;10&#20159;&#21442;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;BASE TTS&#22312;&#35821;&#38899;&#33258;&#28982;&#24230;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#23637;&#29616;&#33258;&#28982;&#30340;&#38901;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.08093</link><description>&lt;p&gt;
&#22522;&#20110;10&#19975;&#23567;&#26102;&#25968;&#25454;&#30340;10&#20159;&#21442;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#30340;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08093
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;10&#19975;&#23567;&#26102;&#25968;&#25454;&#30340;10&#20159;&#21442;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;BASE TTS&#22312;&#35821;&#38899;&#33258;&#28982;&#24230;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#23637;&#29616;&#33258;&#28982;&#30340;&#38901;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BASE TTS&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#65292;&#20854;&#20013;BASE&#20195;&#34920;&#22823;&#35268;&#27169;&#33258;&#36866;&#24212;&#21487;&#27969;&#24335;TTS&#21644;&#26032;&#20986;&#29616;&#30340;&#33021;&#21147;&#12290;BASE TTS&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;TTS&#27169;&#22411;&#65292;&#35757;&#32451;&#20110;10&#19975;&#23567;&#26102;&#30340;&#20844;&#20849;&#39046;&#22495;&#35821;&#38899;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#33258;&#28982;&#24230;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#20010;10&#20159;&#21442;&#25968;&#30340;&#33258;&#22238;&#24402;Transformer&#65292;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#31163;&#25955;&#20195;&#30721;&#65288;"speechcodes"&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#21367;&#31215;&#30340;&#35299;&#30721;&#22120;&#23558;&#36825;&#20123;speechcodes&#20197;&#22686;&#37327;&#12289;&#21487;&#27969;&#24335;&#30340;&#26041;&#24335;&#36716;&#25442;&#20026;&#27874;&#24418;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;speechcodes&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#38899;&#26631;&#35760;&#21270;&#25216;&#26415;&#65292;&#20855;&#26377;&#35828;&#35805;&#32773;ID&#35299;&#32806;&#21644;&#23383;&#33410;&#23545;&#32534;&#30721;&#30340;&#21387;&#32553;&#29305;&#24615;&#12290;&#19982;&#22823;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#25253;&#36947;&#30340;"&#26032;&#20986;&#29616;&#30340;&#33021;&#21147;"&#31867;&#20284;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;10K+&#23567;&#26102;&#21644;500M+&#21442;&#25968;&#26500;&#24314;&#30340;BASE TTS&#21464;&#20307;&#22312;&#25991;&#26412;&#22797;&#26434;&#21477;&#23376;&#19978;&#24320;&#22987;&#23637;&#29616;&#33258;&#28982;&#30340;&#38901;&#24459;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\textbf{B}$ig $\textbf{A}$daptive $\textbf{S}$treamable TTS with $\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes ("speechcodes") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported "emergent abilities" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65288;TAMML&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#20316;&#20026;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#27169;&#24577;&#32452;&#21512;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.08086</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Text-centric Alignment for Multi-Modality Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65288;TAMML&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#20316;&#20026;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#27169;&#24577;&#32452;&#21512;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#21363;&#25512;&#29702;&#38454;&#27573;&#21487;&#29992;&#30340;&#27169;&#24577;&#19982;&#35757;&#32451;&#38454;&#27573;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65288;TAMML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#20511;&#21161;&#22522;&#30784;&#27169;&#22411;&#22686;&#24378;&#22810;&#27169;&#24577;&#31995;&#32479;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#20316;&#20026;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;TAMML&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#27169;&#24577;&#32452;&#21512;&#26041;&#38754;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;TAMML&#19981;&#20165;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#36824;&#33021;&#20445;&#25345;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#20811;&#26381;&#20256;&#32479;&#30340;&#22266;&#23450;&#27169;&#24577;&#26694;&#26550;&#20013;&#30340;&#34920;&#31034;&#23884;&#20837;&#38480;&#21046;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#27169;&#24577;&#30340;&#21487;&#29992;&#24615;&#21487;&#33021;&#20250;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper addresses the challenge of modality mismatch in multimodal learning, where the modalities available during inference differ from those available at training. We propose the Text-centric Alignment for Multi-Modality Learning (TAMML) approach, an innovative method that utilizes Large Language Models (LLMs) with in-context learning and foundation models to enhance the generalizability of multimodal systems under these conditions. By leveraging the unique properties of text as a unified semantic space, TAMML demonstrates significant improvements in handling unseen, diverse, and unpredictable modality combinations. TAMML not only adapts to varying modalities but also maintains robust performance, showcasing the potential of foundation models in overcoming the limitations of traditional fixed-modality frameworks in embedding representations. This study contributes to the field by offering a flexible, effective solution for real-world applications where modality availabili
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#37325;&#26032;&#27010;&#24565;&#21270;&#20026;&#22522;&#20110;&#35821;&#35328;&#30340;&#20004;&#20154;&#28216;&#25103;&#20013;&#30340;&#20195;&#29702;&#23398;&#20064;&#65292;&#25105;&#20204;&#33021;&#22815;&#24471;&#21040;&#20851;&#38190;&#30340;&#35265;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#26469;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.08078</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20004;&#20154;&#28216;&#25103;&#20013;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Agents in Two-Player Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08078
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#37325;&#26032;&#27010;&#24565;&#21270;&#20026;&#22522;&#20110;&#35821;&#35328;&#30340;&#20004;&#20154;&#28216;&#25103;&#20013;&#30340;&#20195;&#29702;&#23398;&#20064;&#65292;&#25105;&#20204;&#33021;&#22815;&#24471;&#21040;&#20851;&#38190;&#30340;&#35265;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#26469;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#20013;&#27491;&#24335;&#23450;&#20041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#36890;&#24120;&#21253;&#25324;&#39044;&#35757;&#32451;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65292;&#22312;&#25512;&#36827;LLM&#25216;&#26415;&#26041;&#38754;&#21487;&#20197;&#33719;&#24471;&#20851;&#38190;&#24615;&#30340;&#35265;&#35299;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;LLM&#30340;&#35757;&#32451;&#26041;&#27861;&#19982;&#22312;&#21338;&#24328;&#35770;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30740;&#31350;&#30340;&#20004;&#20154;&#28216;&#25103;&#20195;&#29702;&#24320;&#21457;&#25152;&#37319;&#29992;&#30340;&#31574;&#30053;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLM&#23398;&#20064;&#36807;&#31243;&#37325;&#26032;&#27010;&#24565;&#21270;&#20026;&#22522;&#20110;&#35821;&#35328;&#30340;&#28216;&#25103;&#20013;&#30340;&#20195;&#29702;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;&#36825;&#19968;&#26694;&#26550;&#25581;&#31034;&#20102;LLM&#24320;&#21457;&#20013;&#30340;&#25104;&#21151;&#19982;&#25361;&#25112;&#30340;&#21019;&#26032;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#23545;&#35299;&#20915;&#23545;&#40784;&#38382;&#39064;&#21644;&#20854;&#20182;&#25112;&#30053;&#32771;&#34385;&#30340;&#26032;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20004;&#20154;&#28216;&#25103;&#26041;&#27861;&#20026;&#35757;&#32451;LLMs&#25552;&#20379;&#20102;&#26032;&#30340;&#25968;&#25454;&#20934;&#22791;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
By formally defining the training processes of large language models (LLMs), which usually encompasses pre-training, supervised fine-tuning, and reinforcement learning with human feedback, within a single and unified machine learning paradigm, we can glean pivotal insights for advancing LLM technologies. This position paper delineates the parallels between the training methods of LLMs and the strategies employed for the development of agents in two-player games, as studied in game theory, reinforcement learning, and multi-agent systems. We propose a re-conceptualization of LLM learning processes in terms of agent learning in language-based games. This framework unveils innovative perspectives on the successes and challenges in LLM development, offering a fresh understanding of addressing alignment issues among other strategic considerations. Furthermore, our two-player game approach sheds light on novel data preparation and machine learning techniques for training LLMs.
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19968;&#30452;&#34987;&#35270;&#20026;&#35299;&#20915;&#35768;&#22810;&#38382;&#39064;&#30340;&#26631;&#20934;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;Elemental Cognition&#24320;&#21457;&#20102;EC AI&#24179;&#21488;&#65292;&#37319;&#29992;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;LLMs&#36827;&#34892;&#30693;&#35782;&#33719;&#21462;&#21644;&#29992;&#25143;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2402.08064</link><description>&lt;p&gt;
&#36229;&#36234;LLMs&#65306;&#25512;&#36827;&#22797;&#26434;&#25512;&#29702;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Beyond LLMs: Advancing the Landscape of Complex Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08064
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19968;&#30452;&#34987;&#35270;&#20026;&#35299;&#20915;&#35768;&#22810;&#38382;&#39064;&#30340;&#26631;&#20934;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;Elemental Cognition&#24320;&#21457;&#20102;EC AI&#24179;&#21488;&#65292;&#37319;&#29992;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;LLMs&#36827;&#34892;&#30693;&#35782;&#33719;&#21462;&#21644;&#29992;&#25143;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20960;&#24180;&#21069;&#20986;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#26469;&#65292;&#23427;&#20204;&#24448;&#24448;&#34987;&#35270;&#20026;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#20107;&#23454;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;LLMs&#30340;&#35768;&#22810;&#19981;&#36275;&#20043;&#22806;&#65292;&#22914;&#21487;&#38752;&#24615;&#12289;&#25104;&#26412;&#21644;&#36895;&#24230;&#31561;&#38382;&#39064;&#65292;&#36824;&#26377;&#19968;&#31867;&#24120;&#35265;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#21363;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#26080;&#22788;&#19981;&#22312;&#65292;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#38750;&#24120;&#19987;&#19994;&#21270;&#19988;&#23454;&#26045;&#25104;&#26412;&#39640;&#12290;&#22312;Elemental Cognition&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;EC AI&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#37319;&#29992;&#20102;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#26469;&#35299;&#20915;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#24179;&#21488;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#31934;&#30830;&#39640;&#25928;&#30340;&#36923;&#36753;&#25512;&#29702;&#24341;&#25806;&#65292;&#24182;&#21033;&#29992;LLMs&#36827;&#34892;&#30693;&#35782;&#33719;&#21462;&#21644;&#29992;&#25143;&#20132;&#20114;&#12290;&#35813;&#24179;&#21488;&#25903;&#25345;&#24320;&#21457;&#20154;&#21592;&#29992;&#33258;&#28982;&#31616;&#27905;&#30340;&#35821;&#35328;&#25351;&#23450;&#24212;&#29992;&#36923;&#36753;&#65292;&#24182;&#29983;&#25104;&#24212;&#29992;&#29992;&#25143;&#30028;&#38754;&#20197;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the advent of Large Language Models a few years ago, they have often been considered the de facto solution for many AI problems. However, in addition to the many deficiencies of LLMs that prevent them from broad industry adoption, such as reliability, cost, and speed, there is a whole class of common real world problems that Large Language Models perform poorly on, namely, constraint satisfaction and optimization problems. These problems are ubiquitous and current solutions are highly specialized and expensive to implement. At Elemental Cognition, we developed our EC AI platform which takes a neuro-symbolic approach to solving constraint satisfaction and optimization problems. The platform employs, at its core, a precise and high performance logical reasoning engine, and leverages LLMs for knowledge acquisition and user interaction. This platform supports developers in specifying application logic in natural and concise language while generating application user interfaces to int
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#24320;&#25918;AI&#30340;&#35821;&#38899;&#35782;&#21035;&#26381;&#21153;Whisper&#65292;&#24182;&#25351;&#20986;&#20854;&#20013;&#32422;1%&#30340;&#36716;&#24405;&#23384;&#22312;&#23436;&#20840;&#24187;&#35273;&#30340;&#30701;&#35821;&#25110;&#21477;&#23376;&#12290;&#36825;&#20123;&#24187;&#35273;&#20869;&#23481;&#20013;&#26377;38%&#21253;&#21547;&#26126;&#30830;&#30340;&#20260;&#23475;&#65292;&#22914;&#26292;&#21147;&#12289;&#34394;&#26500;&#30340;&#20010;&#20154;&#20449;&#24687;&#25110;&#34394;&#20551;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#26435;&#23041;&#12290;&#30740;&#31350;&#32773;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#24187;&#35273;&#21457;&#29983;&#30340;&#20551;&#35774;&#65292;&#24182;&#25351;&#20986;&#20102;&#30001;&#20110;&#35821;&#38899;&#31867;&#22411;&#21644;&#20581;&#24247;&#29366;&#20917;&#30340;&#19981;&#21516;&#21487;&#33021;&#23548;&#33268;&#30340;&#28508;&#22312;&#24046;&#24322;&#12290;&#20182;&#20204;&#21628;&#21505;&#34892;&#19994;&#20174;&#19994;&#32773;&#25913;&#21892;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#65292;&#24182;&#22686;&#24378;&#23545;&#19979;&#28216;&#28508;&#22312;&#20559;&#35265;&#30340;&#35748;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.08021</link><description>&lt;p&gt;
&#19981;&#23567;&#24515;&#30340;&#32819;&#35821;&#65306;&#35821;&#38899;&#36716;&#25991;&#26412;&#24187;&#35273;&#30340;&#21361;&#23475;
&lt;/p&gt;
&lt;p&gt;
Careless Whisper: Speech-to-Text Hallucination Harms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08021
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#24320;&#25918;AI&#30340;&#35821;&#38899;&#35782;&#21035;&#26381;&#21153;Whisper&#65292;&#24182;&#25351;&#20986;&#20854;&#20013;&#32422;1%&#30340;&#36716;&#24405;&#23384;&#22312;&#23436;&#20840;&#24187;&#35273;&#30340;&#30701;&#35821;&#25110;&#21477;&#23376;&#12290;&#36825;&#20123;&#24187;&#35273;&#20869;&#23481;&#20013;&#26377;38%&#21253;&#21547;&#26126;&#30830;&#30340;&#20260;&#23475;&#65292;&#22914;&#26292;&#21147;&#12289;&#34394;&#26500;&#30340;&#20010;&#20154;&#20449;&#24687;&#25110;&#34394;&#20551;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#26435;&#23041;&#12290;&#30740;&#31350;&#32773;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#24187;&#35273;&#21457;&#29983;&#30340;&#20551;&#35774;&#65292;&#24182;&#25351;&#20986;&#20102;&#30001;&#20110;&#35821;&#38899;&#31867;&#22411;&#21644;&#20581;&#24247;&#29366;&#20917;&#30340;&#19981;&#21516;&#21487;&#33021;&#23548;&#33268;&#30340;&#28508;&#22312;&#24046;&#24322;&#12290;&#20182;&#20204;&#21628;&#21505;&#34892;&#19994;&#20174;&#19994;&#32773;&#25913;&#21892;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#65292;&#24182;&#22686;&#24378;&#23545;&#19979;&#28216;&#28508;&#22312;&#20559;&#35265;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#36716;&#25991;&#26412;&#26381;&#21153;&#26088;&#22312;&#23613;&#21487;&#33021;&#20934;&#30830;&#22320;&#36716;&#24405;&#36755;&#20837;&#38899;&#39057;&#12290;&#23427;&#20204;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#20316;&#29992;&#36234;&#26469;&#36234;&#22823;&#65292;&#20363;&#22914;&#20010;&#20154;&#35821;&#38899;&#21161;&#25163;&#25110;&#20844;&#21496;&#19982;&#23458;&#25143;&#30340;&#20114;&#21160;&#20013;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#24320;&#25918;AI&#30340;Whisper&#65292;&#36825;&#26159;&#19968;&#31181;&#36229;&#36234;&#34892;&#19994;&#31454;&#20105;&#23545;&#25163;&#30340;&#26368;&#26032;&#26381;&#21153;&#12290;&#34429;&#28982;Whisper&#30340;&#35768;&#22810;&#36716;&#24405;&#38750;&#24120;&#20934;&#30830;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#22823;&#32422;1&#65285;&#30340;&#38899;&#39057;&#36716;&#24405;&#21253;&#21547;&#23436;&#20840;&#24187;&#35273;&#30340;&#30701;&#35821;&#25110;&#21477;&#23376;&#65292;&#36825;&#20123;&#30701;&#35821;&#25110;&#21477;&#23376;&#22312;&#22522;&#30784;&#38899;&#39057;&#20013;&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#20027;&#39064;&#21270;&#22320;&#20998;&#26512;&#20102;Whisper&#24187;&#35273;&#30340;&#20869;&#23481;&#65292;&#21457;&#29616;38&#65285;&#30340;&#24187;&#35273;&#21253;&#21547;&#26126;&#30830;&#30340;&#20260;&#23475;&#65292;&#20363;&#22914;&#26292;&#21147;&#12289;&#34394;&#26500;&#30340;&#20010;&#20154;&#20449;&#24687;&#25110;&#34394;&#20551;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#26435;&#23041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#20851;&#20110;&#24187;&#35273;&#21457;&#29983;&#30340;&#20551;&#35774;&#65292;&#24182;&#25581;&#31034;&#20102;&#30001;&#20110;&#20581;&#24247;&#29366;&#20917;&#32780;&#23548;&#33268;&#30340;&#35821;&#38899;&#31867;&#22411;&#30340;&#28508;&#22312;&#24046;&#24322;&#12290;&#25105;&#20204;&#21628;&#21505;&#34892;&#19994;&#20174;&#19994;&#32773;&#25913;&#21892;Whisper&#20013;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#65292;&#24182;&#22686;&#24378;&#23545;&#19979;&#28216;&#28508;&#22312;&#20559;&#35265;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-to-text services aim to transcribe input audio as accurately as possible. They increasingly play a role in everyday life, for example in personal voice assistants or in customer-company interactions. We evaluate Open AI's Whisper, a state-of-the-art service outperforming industry competitors. While many of Whisper's transcriptions were highly accurate, we found that roughly 1% of audio transcriptions contained entire hallucinated phrases or sentences, which did not exist in any form in the underlying audio. We thematically analyze the Whisper-hallucinated content, finding that 38% of hallucinations include explicit harms such as violence, made up personal information, or false video-based authority. We further provide hypotheses on why hallucinations occur, uncovering potential disparities due to speech type by health status. We call on industry practitioners to ameliorate these language-model-based hallucinations in Whisper, and to raise awareness of potential biases in downstr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Lumos&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#22791;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#36816;&#29992;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#32452;&#20214;&#65292;&#33021;&#22815;&#20174;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#21152;&#24378;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#30740;&#31350;&#36807;&#31243;&#20013;&#65292;&#20316;&#32773;&#20811;&#26381;&#20102;&#19982;&#25991;&#26412;&#35782;&#21035;&#36136;&#37327;&#12289;&#24310;&#36831;&#21644;&#27169;&#22411;&#25512;&#26029;&#30456;&#20851;&#30340;&#22810;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#32452;&#20214;&#35780;&#20272;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#29575;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08017</link><description>&lt;p&gt;
Lumos : &#29992;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#22686;&#24378;&#22810;&#27169;&#24335;LLMs&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Lumos : Empowering Multimodal LLMs with Scene Text Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Lumos&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#22791;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#36816;&#29992;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#32452;&#20214;&#65292;&#33021;&#22815;&#20174;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#21152;&#24378;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#30740;&#31350;&#36807;&#31243;&#20013;&#65292;&#20316;&#32773;&#20811;&#26381;&#20102;&#19982;&#25991;&#26412;&#35782;&#21035;&#36136;&#37327;&#12289;&#24310;&#36831;&#21644;&#27169;&#22411;&#25512;&#26029;&#30456;&#20851;&#30340;&#22810;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#32452;&#20214;&#35780;&#20272;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#29575;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Lumos&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#22791;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#30340;&#31471;&#21040;&#31471;&#22810;&#27169;&#24335;&#38382;&#31572;&#31995;&#32479;&#12290;Lumos&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#65288;STR&#65289;&#32452;&#20214;&#65292;&#29992;&#20110;&#20174;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#22686;&#24378;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLM&#65289;&#30340;&#36755;&#20837;&#12290;&#22312;&#26500;&#24314;Lumos&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#35768;&#22810;&#19982;STR&#36136;&#37327;&#12289;&#25972;&#20307;&#24310;&#36831;&#21644;&#27169;&#22411;&#25512;&#26029;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#29992;&#20110;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#30340;&#31995;&#32479;&#26550;&#26500;&#12289;&#35774;&#35745;&#36873;&#25321;&#21644;&#24314;&#27169;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#23545;&#27599;&#20010;&#32452;&#20214;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#29575;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#29305;&#23450;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;Amharic-LLaMA&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#38463;&#22982;&#21704;&#25289;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#36890;&#36807;&#21019;&#24314;&#38463;&#22982;&#21704;&#25289;&#35821;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08015</link><description>&lt;p&gt;
&#22686;&#24378;Amharic-LLaMA: &#25972;&#21512;&#29305;&#23450;&#20219;&#21153;&#19982;&#29983;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Enhancing Amharic-LLaMA: Integrating Task Specific and Generative Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#29305;&#23450;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;Amharic-LLaMA&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#38463;&#22982;&#21704;&#25289;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#36890;&#36807;&#21019;&#24314;&#38463;&#22982;&#21704;&#25289;&#35821;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#20013;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#22240;&#32570;&#20047;&#36164;&#28304;&#32780;&#34987;&#33853;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#25972;&#21512;&#29305;&#23450;&#20219;&#21153;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;LLaMA-2-Amharic&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#38463;&#22982;&#21704;&#25289;&#35821;&#30340;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#38463;&#22982;&#21704;&#25289;&#35821;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;LLaMA-2-Amharic&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#27969;&#31243;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#35780;&#20272;&#36755;&#20986;&#65292;&#20197;&#20419;&#36827;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#35821;&#35328;&#29305;&#23450;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have received a lot of attention in natural language processing (NLP) research because of their exceptional performance in understanding and generating human languages. However, low-resource languages are left behind due to the unavailability of resources. In this work, we focus on enhancing the LLaMA-2-Amharic model by integrating task-specific and generative datasets to improve language model performance for Amharic. We compile an Amharic instruction fine-tuning dataset and fine-tuned LLaMA-2-Amharic model. The fine-tuned model shows promising results in different NLP tasks. We open-source our dataset creation pipeline, instruction datasets, trained models, and evaluation outputs to promote language-specific studies on these models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#30452;&#25509;&#20248;&#21270;&#27861;&#65288;rDPO&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34892;&#20026;&#35843;&#25972;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#33258;&#25105;&#35780;&#35770;&#21644;&#24191;&#20041;DPO&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#23398;&#29983;LLM&#65292;&#24182;&#21033;&#29992;&#22806;&#37096;&#22870;&#21169;&#27169;&#22411;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#65292;&#20174;&#32780;&#20351;rDPO&#22312;&#22810;&#20010;&#34892;&#20026;&#35843;&#25972;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08005</link><description>&lt;p&gt;
&#24102;&#26377;&#21512;&#25104;&#25968;&#25454;&#30340;&#25913;&#36827;&#22411;&#30452;&#25509;&#20248;&#21270;&#27861;&#29992;&#20110;LLM&#30340;&#34892;&#20026;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#30452;&#25509;&#20248;&#21270;&#27861;&#65288;rDPO&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34892;&#20026;&#35843;&#25972;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#33258;&#25105;&#35780;&#35770;&#21644;&#24191;&#20041;DPO&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#23398;&#29983;LLM&#65292;&#24182;&#21033;&#29992;&#22806;&#37096;&#22870;&#21169;&#27169;&#22411;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#65292;&#20174;&#32780;&#20351;rDPO&#22312;&#22810;&#20010;&#34892;&#20026;&#35843;&#25972;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#22411;&#30452;&#25509;&#20248;&#21270;&#27861;&#65288;rDPO&#65289;&#65292;&#29992;&#20110;&#25913;&#21892;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34892;&#20026;&#35843;&#25972;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#25105;&#35780;&#35770;&#25552;&#31034;&#25945;&#24072;LLM&#26469;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#65292;&#28982;&#21518;&#21033;&#29992;&#24191;&#20041;DPO&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#32431;&#32473;&#23398;&#29983;LLM&#12290;&#25439;&#22833;&#20989;&#25968;&#32467;&#21512;&#20102;&#39069;&#22806;&#30340;&#22806;&#37096;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#20351;rDPO&#33021;&#22815;&#25269;&#25239;&#21512;&#25104;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#22122;&#22768;&#12290;rDPO&#22312;&#22810;&#31181;&#34892;&#20026;&#35843;&#25972;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#25928;&#26524;&#65292;&#22914;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#25269;&#25239;&#35282;&#33394;&#25198;&#28436;&#65292;&#38477;&#20302;&#24052;&#32467;&#34892;&#20026;&#12290;&#20195;&#30721;&#23558;&#22312;https://github.com/vicgalle/refined-dpo&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce \emph{refined Direct Preference Optimization} (rDPO), a method for improving the behavioral alignment of Large Language Models (LLMs) without the need for human-annotated data. The method involves creating synthetic data using self-critique prompting by a teacher LLM and then utilising a generalized DPO loss function to distil to a student LLM. The loss function incorporates an additional external reward model to improve the quality of synthetic data, making rDPO robust to potential noise in the synthetic dataset. rDPO is shown to be effective in a diverse set of behavioural alignment tasks, such as improved safety, robustness against role-playing, and reduced sycophancy. Code to be released at https://github.com/vicgalle/refined-dpo.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#29992;&#24615;&#30340;&#35745;&#21010;&#65292;&#35745;&#21010;&#21019;&#24314;&#21517;&#20026;Sentinel&#30340;LLM&#26469;&#20998;&#26512;&#32593;&#32476;&#25968;&#25454;&#21253;&#20869;&#23481;&#24182;&#35780;&#20272;&#23041;&#32961;&#32423;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.07950</link><description>&lt;p&gt;
&#27969;&#23186;&#20307;&#30340;&#21355;&#22763;&#65306;&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#20013;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#25968;&#25454;&#21253;&#20998;&#31867;--&#23450;&#20301;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Sentinels of the Stream: Unleashing Large Language Models for Dynamic Packet Classification in Software Defined Networks -- Position Paper
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#29992;&#24615;&#30340;&#35745;&#21010;&#65292;&#35745;&#21010;&#21019;&#24314;&#21517;&#20026;Sentinel&#30340;LLM&#26469;&#20998;&#26512;&#32593;&#32476;&#25968;&#25454;&#21253;&#20869;&#23481;&#24182;&#35780;&#20272;&#23041;&#32961;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;OpenAI&#30340;ChatGPT&#30340;&#21457;&#24067;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#22312;&#22522;&#20110;GPT&#30340;&#32842;&#22825;&#21161;&#25163;&#26041;&#38754;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#30340;&#20852;&#36259;&#22686;&#21152;&#12290;&#22312;&#25509;&#19979;&#26469;&#30340;&#20960;&#20010;&#26376;&#20013;&#65292;&#37322;&#25918;&#20102;&#22810;&#20010;&#21487;&#35775;&#38382;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;Meta&#30340;LLama&#27169;&#22411;&#21644;Mistral AI&#30340;Mistral&#21644;Mixtral MoE&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20197;&#21508;&#31181;&#19981;&#21516;&#30340;&#35768;&#21487;&#35777;&#23545;&#22806;&#20844;&#24320;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#30446;&#30340;&#12290;&#36825;&#20123;LLM&#24050;&#32463;&#22312;&#20195;&#30721;&#24320;&#21457;&#12289;SQL&#29983;&#25104;&#31561;&#22810;&#20010;&#39046;&#22495;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#29992;&#24615;&#30340;&#35745;&#21010;&#12290;&#25105;&#20204;&#35745;&#21010;&#21019;&#24314;&#19968;&#20010;&#21517;&#20026;Sentinel&#30340;LLM&#65292;&#29992;&#20110;&#20998;&#26512;&#32593;&#32476;&#25968;&#25454;&#21253;&#20869;&#23481;&#24182;&#23545;&#20854;&#23041;&#32961;&#32423;&#21035;&#36827;&#34892;&#21028;&#23450;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#25105;&#20204;&#26410;&#26469;&#21457;&#23637;&#35268;&#21010;&#30340;&#21021;&#27493;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the release of OpenAI's ChatGPT, the field of large language models (LLM) saw an increase of academic interest in GPT based chat assistants. In the next few months multiple accesible large language models were released that included Meta's LLama models and Mistral AI's Mistral and Mixtral MoE models. These models are available openly for a wide array of purposes with a wide spectrum of licenses. These LLMs have found their use in a different number of fields like code development, SQL generation etc. In this work we propose our plan to explore the applicability of large language model in the domain of network security. We plan to create Sentinel, a LLM, to analyse network packet contents and pass a judgment on it's threat level. This work is a preliminary report that will lay our plan for our future endeavors.
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#26500;&#24819;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#38656;&#35201;&#38754;&#23545;&#26356;&#22797;&#26434;&#21644;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#30340;&#24895;&#26223;&#12290;&#36825;&#20010;&#24895;&#26223;&#30340;&#26680;&#24515;&#26159;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.07946</link><description>&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#25351;&#25381;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Re-Envisioning Command and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07946
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#38656;&#35201;&#38754;&#23545;&#26356;&#22797;&#26434;&#21644;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#30340;&#24895;&#26223;&#12290;&#36825;&#20010;&#24895;&#26223;&#30340;&#26680;&#24515;&#26159;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#25112;&#20105;&#23558;&#35201;&#27714;&#22312;&#26356;&#22797;&#26434;&#12289;&#24555;&#33410;&#22863;&#12289;&#19981;&#32467;&#26500;&#21270;&#21644;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#12290;C2&#23558;&#22240;&#34987;&#25298;&#32477;&#12289;&#36864;&#21270;&#12289;&#38388;&#27463;&#21644;&#26377;&#38480;&#30340;&#36890;&#20449;&#20197;&#21450;&#38656;&#35201;&#32771;&#34385;&#21040;&#22810;&#20010;&#20316;&#25112;&#39046;&#22495;&#20013;&#30340;&#35768;&#22810;&#25968;&#25454;&#27969;&#32780;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;C2&#23454;&#36341;&#8212;&#8212;&#28304;&#33258;&#24037;&#19994;&#26102;&#20195;&#32780;&#38750;&#26032;&#20852;&#30340;&#26234;&#33021;&#26102;&#20195;&#8212;&#8212;&#26159;&#32447;&#24615;&#30340;&#19988;&#32791;&#26102;&#12290;&#32780;&#19988;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#26410;&#26469;&#25112;&#22330;&#19978;&#19982;&#23545;&#25163;&#20445;&#25345;&#20248;&#21183;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#19982;&#20154;&#31867;&#20043;&#38388;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#24895;&#26223;&#12290;&#36825;&#20010;&#26410;&#26469;&#24895;&#26223;&#20307;&#29616;&#22312;&#19977;&#20010;&#36816;&#33829;&#24433;&#21709;&#19978;&#65306;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#20197;&#21450;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25152;&#35774;&#24819;&#30340;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future warfare will require Command and Control (C2) decision-making to occur in more complex, fast-paced, ill-structured, and demanding conditions. C2 will be further complicated by operational challenges such as Denied, Degraded, Intermittent, and Limited (DDIL) communications and the need to account for many data streams, potentially across multiple domains of operation. Yet, current C2 practices -- which stem from the industrial era rather than the emerging intelligence era -- are linear and time-consuming. Critically, these approaches may fail to maintain overmatch against adversaries on the future battlefield. To address these challenges, we propose a vision for future C2 based on robust partnerships between humans and artificial intelligence (AI) systems. This future vision is encapsulated in three operational impacts: streamlining the C2 operations process, maintaining unity of effort, and developing adaptive collective knowledge systems. This paper illustrates the envisaged fu
&lt;/p&gt;</description></item><item><title>UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07939</link><description>&lt;p&gt;
UFO: &#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#20132;&#20114;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
UFO: A UI-Focused Agent for Windows OS Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07939
&lt;/p&gt;
&lt;p&gt;
UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;UFO&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20102;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;UFO&#37319;&#29992;&#21452;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#31934;&#30830;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#21644;&#25511;&#21046;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#26234;&#33021;&#20307;&#21487;&#20197;&#26080;&#32541;&#22320;&#22312;&#21333;&#20010;&#24212;&#29992;&#31243;&#24207;&#20869;&#20197;&#21450;&#36328;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#23548;&#33322;&#21644;&#25805;&#20316;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#21363;&#20351;&#28041;&#21450;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#21160;&#20316;&#36830;&#25509;&#65292;&#24182;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#12290;&#22240;&#27492;&#65292;UFO&#23558;&#33392;&#24040;&#32780;&#32791;&#26102;&#30340;&#36807;&#31243;&#36716;&#21464;&#20026;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#23601;&#21487;&#20197;&#23436;&#25104;&#30340;&#31616;&#21333;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;9&#20010;&#27969;&#34892;&#30340;Windows&#24212;&#29992;&#31243;&#24207;&#19978;&#23545;UFO&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#21453;&#26144;&#29992;&#25143;&#26085;&#24120;&#20351;&#29992;&#24773;&#26223;&#30340;&#21508;&#31181;&#24773;&#20917;&#12290;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#21644;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#24471;&#20986;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;UFO&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#23454;&#29616;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.07938</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#29992;&#25143;&#30028;&#38754;&#65306;&#30001;LLMs&#39537;&#21160;&#30340;&#35821;&#38899;&#20132;&#20114;&#29992;&#25143;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#23454;&#29616;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#36825;&#20123;&#26032;&#21457;&#29616;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#26032;&#19968;&#20195;&#36719;&#20214;&#30340;&#35806;&#29983;&#65292;&#27491;&#22914;&#23427;&#20204;&#22312;&#24037;&#19994;&#30028;&#26080;&#25968;&#24212;&#29992;&#20013;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#12290;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;LLM&#24341;&#25806;&#21487;&#20197;&#29702;&#35299;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#20998;&#31867;&#26368;&#26377;&#21487;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#35782;&#21035;&#25152;&#38656;&#30340;UI&#32452;&#20214;&#65292;&#24182;&#38543;&#21518;&#25191;&#34892;&#29992;&#25143;&#26399;&#26395;&#30340;&#25805;&#20316;&#12290;&#36825;&#31181;&#38598;&#25104;&#21487;&#20197;&#23558;&#38745;&#24577;UI&#31995;&#32479;&#21457;&#23637;&#25104;&#39640;&#24230;&#21160;&#24577;&#21644;&#21487;&#36866;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24341;&#20837;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#30340;&#26032;&#39046;&#22495;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#21487;&#20197;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#29992;&#25143;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#30340;&#26041;&#24335;&#65292;&#26497;&#22823;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent meteoric advancements in large language models have showcased a remarkable capacity for logical reasoning and comprehension. These newfound capabilities have opened the door to a new generation of software, as has been made obvious through the innumerable ways they are being applied in the industry. This research focuses on harnessing and guiding the upgraded power of LLMs to construct a framework that can serve as an intermediary between a user and their user interface. By comprehending a user's needs through a thorough analysis of natural textual inputs, an effectively crafted LLM engine can classify the most likely available application, identify the desired UI component and subsequently execute the user's expected actions. This integration can evolve static UI systems into highly dynamic and adaptable solutions, introducing a new frontier of intelligent and responsive user experiences. Such a framework can fundamentally shift how users accomplish daily tasks, skyrocket e
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#30340;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#26032;&#30340;&#27169;&#24335;&#65292;&#30446;&#30340;&#26159;&#35299;&#20915;&#26426;&#22120;&#26234;&#33021;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;AI&#31038;&#21306;&#30340;&#20851;&#27880;&#20174;&#25216;&#26415;&#36716;&#21521;AI&#31185;&#23398;&#12290;</title><link>https://arxiv.org/abs/2402.07932</link><description>&lt;p&gt;
&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#30340;&#26694;&#26550;&#29992;&#20110;&#30340;&#27169;&#24335;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
A Human-Machine Collaboration Framework for the Development of Schemas
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07932
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#30340;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#26032;&#30340;&#27169;&#24335;&#65292;&#30446;&#30340;&#26159;&#35299;&#20915;&#26426;&#22120;&#26234;&#33021;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;AI&#31038;&#21306;&#30340;&#20851;&#27880;&#20174;&#25216;&#26415;&#36716;&#21521;AI&#31185;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Winograd&#27169;&#24335;&#25361;&#25112;&#65288;WSC&#65289;&#26159;&#19968;&#20010;&#20026;&#20102;&#30740;&#31350;&#23637;&#31034;&#20154;&#31867;&#34892;&#20026;&#30340;&#31995;&#32479;&#32780;&#35774;&#31435;&#30340;&#27979;&#35797;&#65292;&#23427;&#26088;&#22312;&#23558;AI&#31038;&#21306;&#30340;&#20851;&#27880;&#28857;&#20174;&#25216;&#26415;&#36716;&#21521;AI&#31185;&#23398;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#24120;&#35265;&#21644;&#29712;&#30862;&#30340;&#65292;&#20294;&#23545;&#26426;&#22120;&#26469;&#35828;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#24403;&#23427;&#20204;&#38656;&#35201;&#22788;&#29702;&#38656;&#35201;&#35299;&#20915;&#30830;&#23450;&#24615;&#20195;&#35789;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#21477;&#23376;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#27880;&#20110;&#20154;&#26426;&#22914;&#20309;&#20316;&#20026;&#22242;&#38431;&#21512;&#20316;&#26469;&#35774;&#35745;&#26032;&#27169;&#24335;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Winograd Schema Challenge (WSC), a seemingly well-thought-out test for machine intelligence, has been proposed to shed light on developing systems that exhibit human behavior. Since its introduction, it aimed to pivot the focus of the AI community from the technology to the science of AI. While common and trivial for humans, studies show that it is still challenging for machines, especially when they have to deal with novel schemas, that is, well-designed sentences that require the resolving of definite pronouns. As researchers have become increasingly interested in the challenge itself, this presumably necessitates the availability of an extensive collection of Winograd schemas, which goes beyond what human experts can reasonably develop themselves, especially after proposed ways of utilizing them as novel forms of CAPTCHAs.   To address this necessity, we propose a novel framework that explicitly focuses on how humans and machines can collaborate as teammates to design novel sche
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#24037;&#31243;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35828;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.07927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#24037;&#31243;&#30340;&#31995;&#32479;&#35843;&#26597;&#65306;&#25216;&#26415;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07927
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#24037;&#31243;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35828;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#24050;&#25104;&#20026;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#33021;&#21147;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25351;&#20196;&#65288;&#31216;&#20026;&#25552;&#31034;&#65289;&#22312;&#19981;&#20462;&#25913;&#26680;&#24515;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#25552;&#31034;&#20801;&#35768;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20165;&#26681;&#25454;&#32473;&#23450;&#30340;&#25552;&#31034;&#24341;&#21457;&#25152;&#38656;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#19981;&#26159;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#25552;&#31034;&#21487;&#20197;&#26159;&#25552;&#20379;&#19978;&#19979;&#25991;&#20197;&#25351;&#23548;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20063;&#21487;&#20197;&#26159;&#35843;&#29992;&#30456;&#20851;&#30693;&#35782;&#30340;&#23398;&#20064;&#21521;&#37327;&#34920;&#31034;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20174;&#38382;&#31572;&#21040;&#24120;&#35782;&#25512;&#29702;&#37117;&#26377;&#28041;&#21450;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#26679;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#21644;&#25216;&#26415;&#32570;&#20047;&#31995;&#32479;&#30340;&#32452;&#32455;&#21644;&#29702;&#35299;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#36890;&#36807;&#25552;&#20379;&#23545;&#26368;&#36817;&#36827;&#23637;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in pro
&lt;/p&gt;</description></item><item><title>&#20026;&#35299;&#20915;&#32534;&#31243;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;Python&#23398;&#20064;&#32773;&#30340;&#20013;&#25991;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25910;&#38598;&#19982;&#20998;&#31867;&#30495;&#23454;&#23398;&#29983;&#38382;&#39064;&#65292;&#25552;&#39640;&#22312;&#32447;&#32534;&#31243;&#25945;&#32946;&#30340;&#25928;&#26524;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.07913</link><description>&lt;p&gt;
&#20026;&#24110;&#21161;&#20013;&#22269;Python&#32534;&#31243;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#19968;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07913
&lt;/p&gt;
&lt;p&gt;
&#20026;&#35299;&#20915;&#32534;&#31243;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;Python&#23398;&#20064;&#32773;&#30340;&#20013;&#25991;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25910;&#38598;&#19982;&#20998;&#31867;&#30495;&#23454;&#23398;&#29983;&#38382;&#39064;&#65292;&#25552;&#39640;&#22312;&#32447;&#32534;&#31243;&#25945;&#32946;&#30340;&#25928;&#26524;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#23398;&#20064;&#24179;&#21488;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#24555;&#36895;&#22686;&#38271;&#30340;&#35745;&#31639;&#26426;&#32534;&#31243;&#35838;&#31243;&#20013;&#65292;&#35299;&#31572;&#25104;&#21315;&#19978;&#19975;&#23398;&#29983;&#30340;&#23398;&#20064;&#38382;&#39064;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#20154;&#21147;&#25104;&#26412;&#12290;&#20026;&#32534;&#31243;&#25945;&#32946;&#23450;&#21046;&#26234;&#33021;&#21161;&#25163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21019;&#24314;&#38656;&#35201;&#29420;&#29305;&#30340;&#25968;&#25454;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;&#27492;&#31867;LLMs&#30340;&#25968;&#25454;&#36164;&#28304;&#30456;&#23545;&#31232;&#32570;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#35299;&#20915;&#32534;&#31243;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;Python&#23398;&#20064;&#32773;&#30340;&#20013;&#25991;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20026;&#30830;&#20445;&#38382;&#39064;&#30340;&#26469;&#28304;&#30340;&#30495;&#23454;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#23454;&#38469;&#23398;&#29983;&#25552;&#20986;&#30340;&#38382;&#39064;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#30340;&#31867;&#22411;&#21644;&#23398;&#20064;&#32773;&#30340;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#36825;&#31181;&#27880;&#37322;&#21407;&#21017;&#26088;&#22312;&#25552;&#39640;&#22312;&#32447;&#32534;&#31243;&#25945;&#32946;&#30340;&#25928;&#26524;&#21644;&#36136;&#37327;&#65292;&#20026;&#24320;&#21457;&#36825;&#26041;&#38754;&#30340;&#24037;&#20316;&#25552;&#20379;&#22362;&#23454;&#30340;&#25968;&#25454;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online learning platforms, particularly in rapidly growing computer programming courses, addressing the thousands of students' learning queries requires considerable human cost. The creation of intelligent assistant large language models (LLMs) tailored for programming education necessitates distinct data support. However, in real application scenarios, the data resources for training such LLMs are relatively scarce. Therefore, to address the data scarcity in intelligent educational systems for programming, this paper proposes a new Chinese question-and-answer dataset for Python learners. To ensure the authenticity and reliability of the sources of the questions, we collected questions from actual student questions and categorized them according to various dimensions such as the type of questions and the type of learners. This annotation principle is designed to enhance the effectiveness and quality of online programming education, providing a solid data foundation for developing th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; Prompt4Vis&#65292;&#20351;&#29992;&#31034;&#20363;&#25366;&#25496;&#21644;&#32467;&#26500;&#36807;&#28388;&#26469;&#20026;&#34920;&#26684;&#25968;&#25454;&#21487;&#35270;&#21270;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25552;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#24040;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#24182;&#33021;&#22815;&#25913;&#36827;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#36716;&#25442;&#25104;&#25968;&#25454;&#21487;&#35270;&#21270;&#26597;&#35810;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07909</link><description>&lt;p&gt;
Prompt4Vis: &#20351;&#29992;&#31034;&#20363;&#25366;&#25496;&#21644;&#32467;&#26500;&#36807;&#28388;&#26469;&#20026;&#34920;&#26684;&#25968;&#25454;&#21487;&#35270;&#21270;&#25552;&#20379;&#25552;&#31034;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prompt4Vis: Prompting Large Language Models with Example Mining and Schema Filtering for Tabular Data Visualization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07909
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; Prompt4Vis&#65292;&#20351;&#29992;&#31034;&#20363;&#25366;&#25496;&#21644;&#32467;&#26500;&#36807;&#28388;&#26469;&#20026;&#34920;&#26684;&#25968;&#25454;&#21487;&#35270;&#21270;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25552;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#24040;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#24182;&#33021;&#22815;&#25913;&#36827;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#36716;&#25442;&#25104;&#25968;&#25454;&#21487;&#35270;&#21270;&#26597;&#35810;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;(DV)&#31995;&#32479;&#22240;&#20854;&#22312;&#22823;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#27934;&#35265;&#30340;&#28145;&#21402;&#33021;&#21147;&#32780;&#24471;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#35748;&#21487;&#65292;&#24341;&#36215;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#20851;&#27880;&#12290;&#22312;&#26576;&#20123;&#22768;&#26126;&#24335;&#21487;&#35270;&#21270;&#35821;&#35328;(DVLs&#65292;&#22914;Vega-Lite&#12289;EChart)&#20013;&#65292;&#32534;&#21046;&#25968;&#25454;&#26597;&#35810;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36807;&#31243;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#25216;&#26415;&#30340;&#21457;&#23637;&#20351;&#24471;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#26469;&#21487;&#35270;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#36807;&#31243;&#26356;&#21152;&#31616;&#21333;&#21644;&#30452;&#35266;&#65292;&#25552;&#20379;&#20102;&#26356;&#21487;&#35775;&#38382;&#21644;&#30452;&#35266;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#25104;&#25968;&#25454;&#21487;&#35270;&#21270;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#22914;Seq2Vis&#12289;ncNet&#21644;RGVisNet&#65292;&#23613;&#31649;&#21033;&#29992;&#20102;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20173;&#28982;&#19981;&#23613;&#20154;&#24847;&#65292;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;ChatGPT&#21644;GPT-4&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#22522;&#20934;&#65292;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#35813;&#39046;&#22495;&#30340;&#26684;&#23616;&#12290;&#21463;&#21040;&#36825;&#20123;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data visualization (DV) systems are increasingly recognized for their profound capability to uncover insights from vast datasets, gaining attention across both industry and academia. Crafting data queries is an essential process within certain declarative visualization languages (DVLs, e.g., Vega-Lite, EChart.). The evolution of natural language processing (NLP) technologies has streamlined the use of natural language interfaces to visualize tabular data, offering a more accessible and intuitive user experience. However, current methods for converting natural language questions into data visualization queries, such as Seq2Vis, ncNet, and RGVisNet, despite utilizing complex neural network architectures, still fall short of expectations and have great room for improvement.   Large language models (LLMs) such as ChatGPT and GPT-4, have established new benchmarks in a variety of NLP tasks, fundamentally altering the landscape of the field. Inspired by these advancements, we introduce a nov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;ChatGPT&#22312;&#25913;&#36827;&#25945;&#32946;&#26041;&#38754;&#30340;&#26426;&#36935;&#65292;&#21516;&#26102;&#20063;&#25351;&#20986;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07907</link><description>&lt;p&gt;
AI&#21644;ChatGPT&#22312;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#12289;&#25361;&#25112;&#21644;&#20262;&#29702;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Applications, challenges and ethical issues of AI and ChatGPT in education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;ChatGPT&#22312;&#25913;&#36827;&#25945;&#32946;&#26041;&#38754;&#30340;&#26426;&#36935;&#65292;&#21516;&#26102;&#20063;&#25351;&#20986;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#21457;&#23637;&#65292;&#24182;&#19988;&#36235;&#21521;&#22312;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#21457;&#25381;&#20652;&#21270;&#20316;&#29992;&#12290;&#23398;&#26415;&#30028;&#21644;&#25919;&#24220;&#23545;AI&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#36825;&#20063;&#21453;&#26144;&#22312;&#27491;&#22312;&#36827;&#34892;&#30340;&#25237;&#36164;&#19982;&#30740;&#31350;&#30340;&#25968;&#37327;&#28608;&#22686;&#19978;&#12290;&#27599;&#22825;&#37117;&#26377;&#28909;&#24773;&#27915;&#28322;&#30340;&#24847;&#35265;&#21644;&#35770;&#36848;&#20851;&#20110;AI&#65292;&#20294;&#21516;&#26102;&#20063;&#25552;&#20986;&#20102;&#23545;&#20854;&#24433;&#21709;&#30340;&#35686;&#31034;&#24615;&#39044;&#27979;&#12290;&#26412;&#25991;&#26088;&#22312;&#25551;&#36848;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;ChatGPT&#25913;&#36827;&#25945;&#32946;&#25152;&#24102;&#26469;&#30340;&#26426;&#36935;&#65292;&#21516;&#26102;&#20063;&#35201;&#35782;&#21035;&#20986;&#20986;&#29616;&#30340;&#25361;&#25112;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) in recent years has shown an unprecedentedly impressive development, tending to play a catalytic role in all aspects of life. The interest of the academic community, but also of governments, is huge in the dynamics of AI and is reflected by the truly explosive amount of investment and research that is underway. Enthusiastic opinions and statements about AI are made every day, but at the same time they also bring to the fore alarming predictions about its effects. This paper aims to describe the opportunities emerging from the use of artificial intelligence and ChatGPT to improve education, but also to identify the challenges and ethical issues that arise.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#30452;&#25509;&#21407;&#21017;&#21453;&#39304;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;LLM&#34892;&#20026;&#12290;&#36890;&#36807;&#22312;&#25209;&#35780;&#21644;&#20462;&#35746;&#19978;&#30452;&#25509;&#20351;&#29992;DPO&#26469;&#36339;&#36807;&#21709;&#24212;&#30340;&#25490;&#21517;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#8220;&#31881;&#33394;&#22823;&#35937;&#38382;&#39064;&#8221;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.07896</link><description>&lt;p&gt;
&#20351;&#29992;&#30452;&#25509;&#21407;&#21017;&#21453;&#39304;&#25233;&#21046;&#8220;&#31881;&#33394;&#22823;&#35937;&#8221;
&lt;/p&gt;
&lt;p&gt;
Suppressing Pink Elephants with Direct Principle Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#30452;&#25509;&#21407;&#21017;&#21453;&#39304;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;LLM&#34892;&#20026;&#12290;&#36890;&#36807;&#22312;&#25209;&#35780;&#21644;&#20462;&#35746;&#19978;&#30452;&#25509;&#20351;&#29992;DPO&#26469;&#36339;&#36807;&#21709;&#24212;&#30340;&#25490;&#21517;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#8220;&#31881;&#33394;&#22823;&#35937;&#38382;&#39064;&#8221;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#25511;&#21046;&#26041;&#27861;&#65292;&#22914;RLHF&#21644;&#23466;&#27861;AI&#65292;&#28041;&#21450;&#30830;&#23450;LLM&#34892;&#20026;&#30340;&#21487;&#21462;&#20043;&#22788;&#65292;&#24182;&#23558;&#20854;&#35757;&#32451;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#24076;&#26395;LLM&#22312;&#25512;&#29702;&#26102;&#26159;&#21487;&#25511;&#21046;&#30340;&#65292;&#36825;&#26679;&#21487;&#20197;&#22312;&#22810;&#31181;&#38656;&#35201;&#30340;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#29992;&#8220;&#31881;&#33394;&#22823;&#35937;&#38382;&#39064;&#8221;&#20316;&#20026;&#20363;&#23376;&#65306;&#25351;&#31034;LLM&#36991;&#20813;&#35752;&#35770;&#26576;&#20010;&#29305;&#23450;&#23454;&#20307;&#65288;&#8220;&#31881;&#33394;&#22823;&#35937;&#8221;&#65289;&#65292;&#32780;&#26159;&#35752;&#35770;&#39318;&#36873;&#23454;&#20307;&#65288;&#8220;&#28784;&#33394;&#22823;&#35937;&#8221;&#65289;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Constitutional AI&#31616;&#21270;&#26041;&#27861;&#65292;&#8220;&#30452;&#25509;&#21407;&#21017;&#21453;&#39304;&#8221;&#65292;&#23427;&#36339;&#36807;&#20102;&#23545;&#21709;&#24212;&#30340;&#25490;&#21517;&#65292;&#30452;&#25509;&#22312;&#25209;&#35780;&#21644;&#20462;&#35746;&#19978;&#20351;&#29992;DPO&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25105;&#20204;&#21512;&#25104;&#30340;&#8220;&#31881;&#33394;&#22823;&#35937;&#8221;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;DPF&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#30340;13B&#24494;&#35843;LLaMA 2&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;Llama-2-13B-Chat&#21644;&#25552;&#31034;&#22522;&#32447;&#65292;&#24182;&#19988;&#22312;&#35780;&#20272;&#8220;&#31881;&#33394;&#22823;&#35937;&#38382;&#39064;&#8221;&#30340;&#31934;&#24515;&#36873;&#25321;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#19982;GPT-4&#19968;&#26679;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods for controlling language models, such as RLHF and Constitutional AI, involve determining which LLM behaviors are desirable and training them into a language model. However, in many cases, it is desirable for LLMs to be controllable \textit{at inference time}, so that they can be used in multiple contexts with diverse needs. We illustrate this with the \textbf{Pink Elephant Problem}: instructing an LLM to avoid discussing a certain entity (a ``Pink Elephant''), and instead discuss a preferred entity (``Grey Elephant''). We apply a novel simplification of Constitutional AI, \textbf{Direct Principle Feedback}, which skips the ranking of responses and uses DPO directly on critiques and revisions. Our results show that after DPF fine-tuning on our synthetic Pink Elephants dataset, our 13B fine-tuned LLaMA 2 model significantly outperforms Llama-2-13B-Chat and a prompted baseline, and performs as well as GPT-4 in on our curated test set assessing the Pink Elephant Problem.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35814;&#32454;&#20998;&#26512;&#20102;&#32593;&#32476;&#25366;&#25496;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#21644;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26576;&#20123;&#32593;&#32476;&#25366;&#25496;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#37096;&#20998;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#24037;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25345;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.07446</link><description>&lt;p&gt;
&#36136;&#37327;&#30830;&#23454;&#37325;&#35201;&#65306;&#23545;&#32593;&#32476;&#25366;&#25496;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#36827;&#34892;&#35814;&#32454;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Quality Does Matter: A Detailed Look at the Quality and Utility of Web-Mined Parallel Corpora
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07446
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35814;&#32454;&#20998;&#26512;&#20102;&#32593;&#32476;&#25366;&#25496;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#21644;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26576;&#20123;&#32593;&#32476;&#25366;&#25496;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#37096;&#20998;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#24037;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#20004;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#33521;&#25991;-&#20711;&#20285;&#32599;&#35821;&#65292;&#33521;&#25991;-&#27888;&#31859;&#23572;&#35821;&#21644;&#20711;&#20285;&#32599;&#35821;-&#27888;&#31859;&#23572;&#35821;&#65289;&#30340;&#32593;&#32476;&#25366;&#25496;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#26681;&#25454;&#30456;&#20284;&#24230;&#26631;&#20934;&#23545;&#27599;&#20010;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#25490;&#21517;&#65292;&#24182;&#23545;&#25490;&#21517;&#35821;&#26009;&#24211;&#30340;&#19981;&#21516;&#37096;&#20998;&#36827;&#34892;&#20869;&#22312;&#21644;&#22806;&#22312;&#35780;&#20272;&#12290;&#25105;&#20204;&#26174;&#31034;&#19981;&#21516;&#37096;&#20998;&#30340;&#32593;&#32476;&#25366;&#25496;&#35821;&#26009;&#24211;&#23384;&#22312;&#26174;&#33879;&#30340;&#36136;&#37327;&#24046;&#24322;&#65292;&#24182;&#19988;&#36136;&#37327;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#23545;&#20110;&#26576;&#20123;&#32593;&#32476;&#25366;&#25496;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#20854;&#25490;&#21517;&#26368;&#39640;&#30340;25k&#37096;&#20998;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#24037;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conducted a detailed analysis on the quality of web-mined corpora for two low-resource languages (making three language pairs, English-Sinhala, English-Tamil and Sinhala-Tamil). We ranked each corpus according to a similarity measure and carried out an intrinsic and extrinsic evaluation on different portions of this ranked corpus. We show that there are significant quality differences between different portions of web-mined corpora and that the quality varies across languages and datasets. We also show that, for some web-mined datasets, Neural Machine Translation (NMT) models trained with their highest-ranked 25k portion can be on par with human-curated datasets.
&lt;/p&gt;</description></item><item><title>SALAD&#26159;&#19968;&#27454;&#26234;&#33021;AI&#35821;&#35328;&#21161;&#25163;&#24212;&#29992;&#65292;&#26088;&#22312;&#24110;&#21161;&#22806;&#22269;&#20154;&#23398;&#20064;&#26085;&#35821;&#12290;&#23427;&#25552;&#20379;&#20102;&#22810;&#31181;&#23398;&#20064;&#24037;&#20855;&#21644;&#21151;&#33021;&#65292;&#21253;&#25324;&#32763;&#35793;&#65292;&#35821;&#38899;&#35782;&#21035;&#65292;&#38899;&#39057;&#32763;&#35793;&#65292;&#35789;&#27719;&#36319;&#36394;&#31561;&#65292;&#24182;&#36890;&#36807;&#27599;&#26085;&#32763;&#35793;&#24110;&#21161;&#25552;&#39640;&#19982;&#27597;&#35821;&#20154;&#22763;&#30340;&#20132;&#27969;&#33021;&#21147;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;60%&#30340;&#22806;&#22269;&#20154;&#23545;SALAD&#25552;&#21319;&#26085;&#35821;&#33021;&#21147;&#26377;&#20449;&#24515;&#12290;&#35813;&#24212;&#29992;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20419;&#36827;&#26085;&#26412;&#31038;&#21306;&#30340;&#21253;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07431</link><description>&lt;p&gt;
SALAD: &#26234;&#33021;AI&#35821;&#35328;&#21161;&#25163;&#26085;&#24120;
&lt;/p&gt;
&lt;p&gt;
SALAD: Smart AI Language Assistant Daily
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07431
&lt;/p&gt;
&lt;p&gt;
SALAD&#26159;&#19968;&#27454;&#26234;&#33021;AI&#35821;&#35328;&#21161;&#25163;&#24212;&#29992;&#65292;&#26088;&#22312;&#24110;&#21161;&#22806;&#22269;&#20154;&#23398;&#20064;&#26085;&#35821;&#12290;&#23427;&#25552;&#20379;&#20102;&#22810;&#31181;&#23398;&#20064;&#24037;&#20855;&#21644;&#21151;&#33021;&#65292;&#21253;&#25324;&#32763;&#35793;&#65292;&#35821;&#38899;&#35782;&#21035;&#65292;&#38899;&#39057;&#32763;&#35793;&#65292;&#35789;&#27719;&#36319;&#36394;&#31561;&#65292;&#24182;&#36890;&#36807;&#27599;&#26085;&#32763;&#35793;&#24110;&#21161;&#25552;&#39640;&#19982;&#27597;&#35821;&#20154;&#22763;&#30340;&#20132;&#27969;&#33021;&#21147;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;60%&#30340;&#22806;&#22269;&#20154;&#23545;SALAD&#25552;&#21319;&#26085;&#35821;&#33021;&#21147;&#26377;&#20449;&#24515;&#12290;&#35813;&#24212;&#29992;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20419;&#36827;&#26085;&#26412;&#31038;&#21306;&#30340;&#21253;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SALAD&#26159;&#19968;&#27454;&#30001;AI&#39537;&#21160;&#30340;&#35821;&#35328;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#65292;&#26088;&#22312;&#24110;&#21161;&#22806;&#22269;&#20154;&#23398;&#20064;&#26085;&#35821;&#12290;&#23427;&#25552;&#20379;&#20102;&#27721;&#23383;-&#20551;&#21517;-&#32599;&#39532;&#23383;&#30340;&#32763;&#35793;&#65292;&#35821;&#38899;&#35782;&#21035;&#65292;&#32763;&#35793;&#38899;&#39057;&#65292;&#35789;&#27719;&#36319;&#36394;&#65292;&#35821;&#27861;&#35299;&#37322;&#65292;&#20197;&#21450;&#30001;&#26032;&#23398;&#21040;&#30340;&#35789;&#27719;&#29983;&#25104;&#30340;&#27468;&#26354;&#12290;&#35813;&#24212;&#29992;&#38024;&#23545;&#21021;&#23398;&#32773;&#21644;&#20013;&#32423;&#23398;&#20064;&#32773;&#65292;&#26088;&#22312;&#20351;&#35821;&#35328;&#20064;&#24471;&#26356;&#21152;&#21487;&#33719;&#24471;&#21644;&#24841;&#24555;&#12290;SALAD&#21033;&#29992;&#27599;&#26085;&#32763;&#35793;&#26469;&#22686;&#24378;&#19982;&#27597;&#35821;&#20154;&#22763;&#30340;&#27969;&#21033;&#24230;&#21644;&#20132;&#27969;&#33298;&#36866;&#24230;&#12290;&#20027;&#35201;&#30446;&#26631;&#21253;&#25324;&#26377;&#25928;&#30340;&#26085;&#35821;&#23398;&#20064;&#65292;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#36827;&#23637;&#36319;&#36394;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;&#22312;&#26085;&#26412;&#30340;&#22806;&#22269;&#20154;&#20013;&#65292;&#26377;39%&#22312;&#19982;&#26085;&#26412;&#20154;&#20132;&#35848;&#26102;&#24863;&#21040;&#19981;&#36866;&#12290;&#36229;&#36807;60%&#30340;&#22806;&#22269;&#20154;&#34920;&#31034;&#23545;SALAD&#25552;&#21319;&#20182;&#20204;&#30340;&#26085;&#35821;&#33021;&#21147;&#26377;&#20449;&#24515;&#12290;&#35813;&#24212;&#29992;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35821;&#38899;&#35782;&#21035;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#24357;&#21512;&#35821;&#35328;&#38548;&#38402;&#65292;&#20419;&#36827;&#26085;&#26412;&#26356;&#20855;&#21253;&#23481;&#24615;&#30340;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
SALAD is an AI-driven language-learning application designed to help foreigners learn Japanese. It offers translations in Kanji-Kana-Romaji, speech recognition, translated audio, vocabulary tracking, grammar explanations, and songs generated from newly learned words. The app targets beginners and intermediate learners, aiming to make language acquisition more accessible and enjoyable. SALAD uses daily translations to enhance fluency and comfort in communication with native speakers. The primary objectives include effective Japanese language learning, user engagement, and progress tracking. A survey by us found that 39% of foreigners in Japan face discomfort in conversations with Japanese speakers. Over 60% of foreigners expressed confidence in SALAD's ability to enhance their Japanese language skills. The app uses large language models, speech recognition, and diffusion models to bridge the language gap and foster a more inclusive community in Japan.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07309</link><description>&lt;p&gt;
HyperBERT:&#23558;&#28151;&#21512;&#36229;&#22270;&#24863;&#30693;&#23618;&#19982;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#36890;&#36807;&#22797;&#26434;&#30340;&#25299;&#25169;&#32467;&#26500;&#26631;&#35760;&#65292;&#34920;&#36798;&#22810;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;&#36229;&#36793;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#36229;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#21516;&#26102;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#20840;&#37096;&#20869;&#23481;&#21644;&#33410;&#28857;&#23646;&#24615;&#20013;&#30340;&#20016;&#23500;&#35821;&#35328;&#23646;&#24615;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20026;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#36827;&#19968;&#27493;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#24341;&#20837;&#19987;&#38376;&#30340;&#36229;&#22270;&#24863;&#30693;&#23618;&#12290;&#36825;&#20123;&#23618;&#23558;&#39640;&#38454;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#24341;&#20837;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#21033;&#29992;&#36229;&#22270;&#32467;&#26500;&#20013;&#30340;&#39640;&#38454;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#25991;&#26412;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#34913;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21017;&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#23637;&#31034;&#20102;GPT-4 Turbo&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24341;&#23548;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#12290;</title><link>https://arxiv.org/abs/2402.07282</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22312;&#35802;&#23454;&#19982;&#24110;&#21161;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#34913;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21017;&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#23637;&#31034;&#20102;GPT-4 Turbo&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24341;&#23548;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#20132;&#27969;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#24110;&#21161;&#21548;&#20247;&#32780;&#36817;&#20284;&#30495;&#30456;&#65292;&#20363;&#22914;&#32422;&#30053;&#26102;&#38388;&#25110;&#30465;&#30053;&#32454;&#33410;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#22788;&#29702;&#36825;&#31181;&#24494;&#22937;&#30340;&#26435;&#34913;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#29702;&#27169;&#22411;&#21644;&#26088;&#22312;&#25551;&#36848;&#20154;&#31867;&#34892;&#20026;&#30340;&#23454;&#39564;&#26469;&#20998;&#26512;LLMs&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#19968;&#31995;&#21015;LLMs&#65292;&#24182;&#25506;&#35752;&#20102;&#20248;&#21270;&#20154;&#31867;&#20559;&#22909;&#25110;&#25512;&#29702;&#26102;&#24605;&#32771;&#23545;&#36825;&#20123;&#26435;&#34913;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#20351;LLMs&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#32780;&#19981;&#26159;&#35802;&#23454;&#12290;&#26368;&#21518;&#65292;GPT-4 Turbo&#23637;&#31034;&#20102;&#31867;&#20284;&#20154;&#31867;&#30340;&#22238;&#24212;&#27169;&#24335;&#65292;&#21253;&#25324;&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#21363;&#20351;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#20063;&#21487;&#20197;&#22312;&#38646;-shot&#25552;&#31034;&#19979;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#34987;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;</title><link>https://arxiv.org/abs/2402.06126</link><description>&lt;p&gt;
&#23398;&#20064;&#21464;&#24471;&#39640;&#25928;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learn To be Efficient: Build Structured Sparsity in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20197;&#20854;&#21313;&#20159;&#32423;&#21442;&#25968;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20135;&#29983;&#20102;&#39640;&#26114;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;LLM&#20013;&#20986;&#29616;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#20026;&#36890;&#36807;&#20165;&#28041;&#21450;&#37096;&#20998;&#21442;&#25968;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#25104;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21033;&#29992;&#36825;&#31181;&#33258;&#28982;&#24418;&#25104;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24573;&#35270;&#20102;&#36827;&#19968;&#27493;&#25918;&#22823;&#36825;&#31181;&#22266;&#26377;&#31232;&#30095;&#24615;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;LLM&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#26356;&#32467;&#26500;&#21270;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#26469;&#23398;&#20064;&#39640;&#25928;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)", &#26088;&#22312;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#24182;&#22312;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#25240;&#34935;&#12290;&#27492;&#22806;&#65292;&#19982;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;ReLU&#27169;&#22411;&#30340;SOTA MoEfication&#26041;&#27861;&#19981;&#21516;&#65292;LTE&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20687;GPT&#21644;LLaMA&#36825;&#26679;&#20855;&#26377;&#36719;&#28608;&#27963;&#20989;&#25968;&#30340;LLM&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27169;&#22411;&#21644;&#21313;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LTE&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.05785</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#23398;&#20064;&#19978;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Transformer Language Models on Algorithmic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05785
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#35201;&#27714;&#32452;&#21512;&#22810;&#20010;&#31163;&#25955;&#23376;&#20219;&#21153;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLaMA&#27169;&#22411;&#21644;&#22312;GPT-4&#21644;Gemini&#19978;&#25552;&#31034;&#26469;&#34913;&#37327;&#23398;&#20064;&#23398;&#20064;&#21407;&#35821;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#35268;&#27169;&#26041;&#38754;&#27604;&#20026;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#25928;&#26524;&#26356;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25351;&#25968;&#32423;&#22320;&#28010;&#36153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
&lt;/p&gt;</description></item><item><title>C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03181</link><description>&lt;p&gt;
C-RAG: &#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35777;&#29983;&#25104;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03181
&lt;/p&gt;
&lt;p&gt;
C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#22791;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#21644;&#38169;&#20301;&#12290;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RAG&#65289;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;RAG&#27169;&#22411;&#30340;&#29983;&#25104;&#39118;&#38505;&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;RAG&#26159;&#21542;&#30830;&#23454;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#65292;2&#65289;&#22914;&#20309;&#23545;RAG&#21644;&#20256;&#32479;LLM&#30340;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#20197;&#21450;3&#65289;&#21738;&#20123;&#20805;&#20998;&#26465;&#20214;&#20351;&#24471;RAG&#27169;&#22411;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;C-RAG&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;RAG&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;RAG&#27169;&#22411;&#25552;&#20379;&#20102;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#65292;&#24182;&#30830;&#20445;&#20102;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#33324;&#26377;&#30028;&#39118;&#38505;&#19979;&#30340;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk f
&lt;/p&gt;</description></item><item><title>CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;</title><link>https://arxiv.org/abs/2402.00786</link><description>&lt;p&gt;
CroissantLLM: &#19968;&#20010;&#30495;&#27491;&#30340;&#21452;&#35821;&#27861;&#35821;-&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CroissantLLM: A Truly Bilingual French-English Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00786
&lt;/p&gt;
&lt;p&gt;
CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CroissantLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;3T&#20010;&#33521;&#35821;&#21644;&#27861;&#35821;&#26631;&#35760;&#19978;&#39044;&#35757;&#32451;&#30340;13&#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#30740;&#31350;&#21644;&#24037;&#19994;&#31038;&#21306;&#24102;&#26469;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#12289;&#23436;&#20840;&#24320;&#28304;&#30340;&#21452;&#35821;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#28040;&#36153;&#32423;&#26412;&#22320;&#30828;&#20214;&#19978;&#24555;&#36895;&#36816;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#19968;&#31181;&#20869;&#22312;&#21452;&#35821;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#27861;&#35821;&#20998;&#21106;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#25163;&#24037;&#31574;&#21010;&#12289;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#28304;&#12290;&#20026;&#20102;&#35780;&#20272;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; FrenchBench&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#27169;&#22411;&#22312;&#27861;&#35821;&#35821;&#35328;&#20013;&#24615;&#33021;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#25345;&#36879;&#26126;&#24230;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20195;&#30721;&#24211;&#21644;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#20960;&#21313;&#20010;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20581;&#24247;-LLM&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29305;&#24449;&#25552;&#21462;&#21644;&#21307;&#23398;&#30693;&#35782;&#26435;&#34913;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#30340;&#26816;&#32034;&#22686;&#24378;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#20581;&#24247;&#25253;&#21578;&#65292;&#35843;&#25972;&#29305;&#24449;&#26435;&#37325;&#65292;&#20197;&#21450;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#23478;&#35265;&#35299;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19982;&#20256;&#32479;&#20581;&#24247;&#31649;&#29702;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.00746</link><description>&lt;p&gt;
&#20581;&#24247;-LLM&#65306;&#20010;&#24615;&#21270;&#26816;&#32034;&#22686;&#24378;&#30340;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20581;&#24247;-LLM&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29305;&#24449;&#25552;&#21462;&#21644;&#21307;&#23398;&#30693;&#35782;&#26435;&#34913;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#30340;&#26816;&#32034;&#22686;&#24378;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#20581;&#24247;&#25253;&#21578;&#65292;&#35843;&#25972;&#29305;&#24449;&#26435;&#37325;&#65292;&#20197;&#21450;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#23478;&#35265;&#35299;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19982;&#20256;&#32479;&#20581;&#24247;&#31649;&#29702;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21355;&#29983;&#20445;&#20581;&#39046;&#22495;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26234;&#33021;&#21307;&#30103;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26234;&#33021;&#21307;&#30103;&#21463;&#38480;&#20110;&#38745;&#24577;&#25968;&#25454;&#21644;&#32479;&#19968;&#26631;&#20934;&#65292;&#26080;&#27861;&#23436;&#20840;&#19982;&#20010;&#20307;&#24773;&#20917;&#38598;&#25104;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#20854;&#20182;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#20581;&#24247;-LLM&#65292;&#23558;&#22823;&#35268;&#27169;&#29305;&#24449;&#25552;&#21462;&#21644;&#21307;&#23398;&#30693;&#35782;&#26435;&#34913;&#35780;&#20998;&#30456;&#32467;&#21512;&#12290;&#19982;&#20256;&#32479;&#20581;&#24247;&#31649;&#29702;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19977;&#20010;&#20027;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20581;&#24247;&#25253;&#21578;&#25972;&#21512;&#21040;&#22823;&#27169;&#22411;&#20013;&#65292;&#25552;&#20379;&#35814;&#32454;&#30340;&#20219;&#21153;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#19994;&#30340;&#21307;&#23398;&#19987;&#19994;&#30693;&#35782;&#35843;&#25972;&#20581;&#24247;&#29305;&#24449;&#30340;&#26435;&#37325;&#24471;&#20998;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#21322;&#33258;&#21160;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#26512;&#33021;&#21147;&#65292;&#24182;&#25972;&#21512;&#19987;&#23478;&#35265;&#35299;&#20197;&#25552;&#39640;&#30142;&#30149;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment. However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges. Hence, a more professional and detailed intelligent healthcare method is needed for development. To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management methods, our approach has three main advantages. First, our method integrates health reports into a large model to provide detailed task information. Second, professional medical expertise is used to adjust the weighted scores of health characteristics. Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease predic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Reveal&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#35774;&#32622;&#20013;&#23545;&#22797;&#26434;&#24605;&#32500;&#38142;&#30340;&#33258;&#21160;&#39564;&#35777;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#35814;&#23613;&#30340;&#26631;&#31614;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31572;&#26696;&#20013;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#30456;&#20851;&#24615;&#12289;&#24402;&#22240;&#21644;&#36923;&#36753;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00559</link><description>&lt;p&gt;
&#19968;&#26465;&#24605;&#32500;&#38142;&#26465;&#30340;&#24378;&#24230;&#21462;&#20915;&#20110;&#26368;&#24369;&#30340;&#29615;&#33410;&#65306;&#19968;&#20010;&#39564;&#35777;&#25512;&#29702;&#38142;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Reveal&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#35774;&#32622;&#20013;&#23545;&#22797;&#26434;&#24605;&#32500;&#38142;&#30340;&#33258;&#21160;&#39564;&#35777;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#35814;&#23613;&#30340;&#26631;&#31614;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31572;&#26696;&#20013;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#30456;&#20851;&#24615;&#12289;&#24402;&#22240;&#21644;&#36923;&#36753;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20419;&#20351;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#36880;&#27493;&#22238;&#31572;&#65288;&#20363;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65289;&#26159;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#20854;&#20013;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;&#38142;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#35752;&#35770;&#20102;&#33258;&#21160;&#39564;&#35777;&#25512;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#21644;&#25913;&#21892;&#20854;&#27491;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#27493;&#39588;&#32423;&#25968;&#25454;&#38598;&#65292;&#26080;&#27861;&#23545;&#36825;&#31867;&#39564;&#35777;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Reveal&#65306;&#25512;&#29702;&#39564;&#35777;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#35774;&#32622;&#20013;&#23545;&#22797;&#26434;&#24605;&#32500;&#38142;&#30340;&#33258;&#21160;&#39564;&#35777;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;Reveal&#21253;&#25324;&#23545;&#35821;&#35328;&#27169;&#22411;&#31572;&#26696;&#20013;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#30456;&#20851;&#24615;&#12289;&#24402;&#22240;&#20110;&#35777;&#25454;&#27573;&#33853;&#20197;&#21450;&#36923;&#36753;&#27491;&#30830;&#24615;&#30340;&#20840;&#38754;&#26631;&#31614;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting language models to provide step-by-step answers (e.g., "Chain-of-Thought") is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning steps to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce Reveal: Reasoning Verification Evaluation, a new dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question answering settings. Reveal includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model's answer, across a wide variety of datasets and state-of-the-art language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#38750;&#35821;&#35328;&#25277;&#35937;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#21644;&#20010;&#20307;&#27169;&#22359;&#30340;&#20851;&#38190;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2401.12117</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38750;&#35821;&#35328;&#25277;&#35937;&#25512;&#29702;&#30340;&#22855;&#29305;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.12117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#38750;&#35821;&#35328;&#25277;&#35937;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#21644;&#20010;&#20307;&#27169;&#22359;&#30340;&#20851;&#38190;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20173;&#22312;&#36880;&#28176;&#24212;&#29992;&#20110;&#26032;&#39046;&#22495;&#24182;&#22312;&#26032;&#24212;&#29992;&#20013;&#34987;&#21033;&#29992;&#65292;&#20294;&#25105;&#20204;&#27491;&#22312;&#32463;&#21382;&#26032;&#19968;&#20195;&#22522;&#30784;&#27169;&#22411;&#30340;&#28044;&#29616;&#65292;&#21363;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#23558;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#25972;&#21512;&#65292;&#20026;&#23637;&#31034;&#20986;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20004;&#31181;&#27169;&#24577;&#30340;&#20132;&#38598;&#22788;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;MLLMs&#30340;&#21069;&#26223;&#20855;&#26377;&#38761;&#21629;&#24615;&#30340;&#21069;&#26223;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;Raven's Progressive Matrices&#30340;&#21464;&#24335;&#35780;&#20272;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;MLLMs&#30340;&#38750;&#35821;&#35328;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#20043;&#38388;&#24040;&#22823;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#20010;&#20307;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#22359;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#23548;&#33268;&#27169;&#22411;&#30340;&#24615;&#33021;&#21463;&#21040;&#20302;&#35895;&#30340;&#38480;&#21046;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#25552;&#39640;MLLMs&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) are still being adopted to new domains and utilized in novel applications, we are experiencing an influx of the new generation of foundation models, namely multi-modal large language models (MLLMs). These models integrate verbal and visual information, opening new possibilities to demonstrate more complex reasoning abilities at the intersection of the two modalities. However, despite the revolutionizing prospect of MLLMs, our understanding of their reasoning abilities is limited. In this study, we assess the nonverbal abstract reasoning abilities of open-source and closed-source MLLMs using variations of Raven's Progressive Matrices. Our experiments expose the difficulty of solving such problems while showcasing the immense gap between open-source and closed-source models. We also reveal critical shortcomings with individual visual and textual modules, subjecting the models to low-performance ceilings. Finally, to improve MLLMs' performance, we experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20316;&#32773;&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#30340;&#20316;&#32773;&#24402;&#23646;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#30097;&#38382;&#25991;&#20214;&#30340;&#22256;&#24785;&#24230;&#26469;&#30830;&#23450;&#26368;&#26377;&#21487;&#33021;&#30340;&#20316;&#32773;&#12290;ALMs&#22312;Blogs50&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;CCAT50&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;&#22312;&#36739;&#30701;&#25991;&#26412;&#19978;&#65292;ALMs&#38656;&#35201;&#36739;&#23569;&#30340;&#20196;&#29260;&#26469;&#23454;&#29616;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.12005</link><description>&lt;p&gt;
ALMs: &#20316;&#32773;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20316;&#32773;&#24402;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
ALMs: Authorial Language Models for Authorship Attribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.12005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20316;&#32773;&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#30340;&#20316;&#32773;&#24402;&#23646;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#30097;&#38382;&#25991;&#20214;&#30340;&#22256;&#24785;&#24230;&#26469;&#30830;&#23450;&#26368;&#26377;&#21487;&#33021;&#30340;&#20316;&#32773;&#12290;ALMs&#22312;Blogs50&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;CCAT50&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;&#22312;&#36739;&#30701;&#25991;&#26412;&#19978;&#65292;ALMs&#38656;&#35201;&#36739;&#23569;&#30340;&#20196;&#29260;&#26469;&#23454;&#29616;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20316;&#32773;&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#30340;&#20316;&#32773;&#24402;&#23646;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#22522;&#20110;&#19968;&#32452;&#34987;&#35843;&#25972;&#20026;&#20505;&#36873;&#20316;&#32773;&#30340;&#20889;&#20316;&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#30097;&#38382;&#25991;&#20214;&#30340;&#22256;&#24785;&#24230;&#26469;&#30830;&#23450;&#26368;&#26377;&#21487;&#33021;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#20351;&#29992;CCAT50&#25968;&#25454;&#38598;&#21644;Blogs50&#25968;&#25454;&#38598;&#23545;ALMs&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ALMs&#22312;Blogs50&#19978;&#21462;&#24471;&#20102;83.6%&#30340;&#23439;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#65292;&#32780;&#22312;CCAT50&#19978;&#21462;&#24471;&#20102;74.9%&#30340;&#23439;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#19982;&#26368;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;&#20026;&#20102;&#35780;&#20272;ALMs&#22312;&#36739;&#30701;&#25991;&#26412;&#19978;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#25991;&#26412;&#21093;&#31163;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20026;&#20102;&#36798;&#21040;70%&#30340;&#23439;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;ALMs&#22312;Blogs50&#19978;&#38656;&#35201;40&#20010;&#20196;&#29260;&#65292;&#22312;CCAT50&#19978;&#38656;&#35201;400&#20010;&#20196;&#29260;&#65292;&#32780;&#20026;&#20102;&#36798;&#21040;60%&#65292;ALMs&#22312;Blogs50&#19978;&#38656;&#35201;20&#20010;&#20196;&#29260;&#65292;&#22312;CCAT50&#19978;&#38656;&#35201;70&#20010;&#20196;&#29260;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce an authorship attribution method called Authorial Language Models (ALMs) that involves identifying the most likely author of a questioned document based on the perplexity of the questioned document calculated for a set of causal language models fine-tuned on the writings of a set of candidate author. We benchmarked ALMs against state-of-art-systems using the CCAT50 dataset and the Blogs50 datasets. We find that ALMs achieves a macro-average accuracy score of 83.6% on Blogs50, outperforming all other methods, and 74.9% on CCAT50, matching the performance of the best method. To assess the performance of ALMs on shorter texts, we also conducted text ablation testing. We found that to reach a macro-average accuracy of 70%, ALMs needs 40 tokens on Blogs50 and 400 tokens on CCAT50, while to reach 60% ALMs requires 20 tokens on Blogs50 and 70 tokens on CCAT50.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20197;&#25552;&#21319;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20559;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2401.11911</link><description>&lt;p&gt;
&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20197;&#22686;&#24378;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20197;&#25552;&#21319;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20559;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36741;&#21161;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20851;&#38190;&#65292;&#20294;&#23545;&#20110;LLMs&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#30340;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20173;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#30830;&#23450;LLMs&#30340;&#21709;&#24212;&#26159;&#28304;&#33258;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36824;&#26159;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21253;&#21547;&#30456;&#20114;&#20914;&#31361;&#30340;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#38382;&#39064;&#37117;&#19982;&#29983;&#25104;&#30340;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#37197;&#23545;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#20102;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#65288;&#22914;GPT-4/3.5&#21644;Llama2&#65289;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#26356;&#20542;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#36825;&#20123;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#23548;&#33268;&#36825;&#31181;&#20559;&#24046;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;i&#65289;LLMs&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36890;&#24120;&#19982;&#38382;&#39064;&#26356;&#30456;&#20284;&#65292;&#22686;&#21152;&#20102;&#20854;&#34987;&#36873;&#25321;&#30340;&#21487;&#33021;&#24615;&#65307;ii&#65289;&#26816;&#32034;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#30340;&#20998;&#21106;&#36807;&#31243;&#25171;&#26029;&#20102;&#20854;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a systematic framework to identify whether LLMs' responses, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To achieve this, we construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs (GPT-4/3.5 and Llama2) towards generated contexts, even when they provide incorrect information. We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) the segmentation process used in retrieved contexts disrupts their compl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#30340;&#29702;&#30001;&#30340;&#8220;&#25512;&#29702;&#24863;&#30693;&#8221;&#35786;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#20020;&#24202;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;&#30142;&#30149;&#35786;&#26029;&#36807;&#31243;&#20013;&#30340;&#39640;&#25928;&#12289;&#26102;&#38388;&#33410;&#32422;&#21644;&#21171;&#21160;&#33410;&#32422;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.07399</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20020;&#24202;&#25512;&#29702;&#32773;&#65306;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#30340;&#29702;&#30001;&#30340;&#25512;&#29702;&#24863;&#30693;&#35786;&#26029;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#30340;&#29702;&#30001;&#30340;&#8220;&#25512;&#29702;&#24863;&#30693;&#8221;&#35786;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#20020;&#24202;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;&#30142;&#30149;&#35786;&#26029;&#36807;&#31243;&#20013;&#30340;&#39640;&#25928;&#12289;&#26102;&#38388;&#33410;&#32422;&#21644;&#21171;&#21160;&#33410;&#32422;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#26426;&#22120;&#25512;&#29702;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#65292;&#22823;&#22810;&#25968;&#20197;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20026;&#39537;&#21160;&#30340;&#39033;&#30446;&#20027;&#35201;&#38598;&#20013;&#22312;&#20020;&#24202;&#20998;&#31867;&#25110;&#38405;&#35835;&#29702;&#35299;&#19978;&#65292;&#24182;&#19988;&#30001;&#20110;&#19982;&#20020;&#24202;&#21307;&#29983;&#30340;&#29702;&#24565;&#27880;&#35299;&#25104;&#26412;&#36739;&#39640;&#65292;&#23545;&#20110;&#30142;&#30149;&#35786;&#26029;&#30340;&#20020;&#24202;&#25512;&#29702;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#25512;&#29702;&#24863;&#30693;&#8221;&#30340;&#35786;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20197;&#19968;&#31181;&#39640;&#25928;&#30340;&#26102;&#38388;&#21644;&#21171;&#21160;&#26041;&#24335;&#21435;&#29702;&#24615;&#21270;&#35786;&#26029;&#36807;&#31243;&#65292;&#24182;&#23398;&#20064;&#23545;&#25552;&#31034;&#29983;&#25104;&#30340;&#29702;&#30001;&#36827;&#34892;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#30142;&#30149;&#35786;&#26029;&#30340;&#20020;&#24202;&#25512;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;LLM&#29983;&#25104;&#20102;&#35786;&#26029;&#24615;&#30340;&#29702;&#30001;&#65292;&#25552;&#20379;&#20854;&#23545;&#21576;&#29616;&#30340;&#24739;&#32773;&#25968;&#25454;&#30340;&#35265;&#35299;&#20197;&#21450;&#36798;&#21040;&#35786;&#26029;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#21363;&#20020;&#24202;&#24605;&#32500;&#38142;&#65288;Clinical CoT&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#22312;&#29702;&#30001;&#29983;&#25104;&#21644;&#30142;&#30149;&#35786;&#26029;&#26041;&#38754;&#23454;&#35777;&#20102;LLMs/LMs&#30340;&#20020;&#24202;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine reasoning has made great progress in recent years owing to large language models (LLMs). In the clinical domain, however, most NLP-driven projects mainly focus on clinical classification or reading comprehension, and under-explore clinical reasoning for disease diagnosis due to the expensive rationale annotation with clinicians. In this work, we present a ``reasoning-aware'' diagnosis framework that rationalizes the diagnostic process via prompt-based learning in a time- and labor-efficient manner, and learns to reason over the prompt-generated rationales. Specifically, we address the clinical reasoning for disease diagnosis, where the LLM generates diagnostic rationales providing its insight on presented patient data and the reasoning path towards the diagnosis, namely Clinical Chain-of-Thought (Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical reasoning via extensive experiments and analyses on both rationale generation and disease diagnosis in various s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#39588;&#24037;&#20316;&#27969;&#21160;&#20316;&#39044;&#27979;&#30340;&#26032;&#38382;&#39064;&#65292;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#27493;&#39588;&#65292;&#23454;&#29616;&#23545;&#20219;&#21153;&#30340;&#22810;&#36718;&#33258;&#21160;&#21270;&#65292;&#33410;&#30465;&#26102;&#38388;&#12290;&#25552;&#20986;&#20102;&#19977;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#27493;&#39588;&#21160;&#20316;&#39044;&#27979;&#25552;&#39640;&#23545;&#35805;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#27493;&#39588;&#33258;&#21160;&#21270;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2311.09593</link><description>&lt;p&gt;
&#22810;&#27493;&#39588;&#23545;&#35805;&#24037;&#20316;&#27969;&#21160;&#20316;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Step Dialogue Workflow Action Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#39588;&#24037;&#20316;&#27969;&#21160;&#20316;&#39044;&#27979;&#30340;&#26032;&#38382;&#39064;&#65292;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#27493;&#39588;&#65292;&#23454;&#29616;&#23545;&#20219;&#21153;&#30340;&#22810;&#36718;&#33258;&#21160;&#21270;&#65292;&#33410;&#30465;&#26102;&#38388;&#12290;&#25552;&#20986;&#20102;&#19977;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#27493;&#39588;&#21160;&#20316;&#39044;&#27979;&#25552;&#39640;&#23545;&#35805;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#27493;&#39588;&#33258;&#21160;&#21270;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#65292;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#36981;&#24490;&#19968;&#31995;&#21015;&#21160;&#20316;&#30340;&#39034;&#24207;&#65292;&#31216;&#20026;&#24037;&#20316;&#27969;&#65292;&#20197;&#20415;&#26681;&#25454;&#19968;&#32452;&#20934;&#21017;&#23436;&#25104;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#39588;&#24037;&#20316;&#27969;&#21160;&#20316;&#39044;&#27979;&#30340;&#26032;&#38382;&#39064;&#65292;&#31995;&#32479;&#39044;&#27979;&#26410;&#26469;&#30340;&#22810;&#20010;&#24037;&#20316;&#27969;&#21160;&#20316;&#12290;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#27493;&#39588;&#21487;&#20197;&#23454;&#29616;&#22810;&#36718;&#33258;&#21160;&#21270;&#65292;&#20174;&#32780;&#33021;&#22815;&#33410;&#30465;&#26102;&#38388;&#19987;&#27880;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#31616;&#21333;&#26131;&#34892;&#19988;&#33021;&#25552;&#39640;&#21160;&#20316;&#33258;&#21160;&#21270;&#30340;&#24314;&#27169;&#26041;&#27861;&#65306;1&#65289;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;2&#65289;&#20351;&#29992;&#26816;&#32034;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#21450;3&#65289;&#38646;&#26679;&#26412;&#22270;&#36941;&#21382;&#65292;&#23558;&#21382;&#21490;&#21160;&#20316;&#24207;&#21015;&#27719;&#24635;&#25104;&#22270;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27493;&#39588;&#21160;&#20316;&#39044;&#27979;&#20135;&#29983;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#23545;&#35805;&#20219;&#21153;&#65288;&#22914;&#39044;&#27979;&#20219;&#21153;&#25104;&#21151;&#65289;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22826;&#22810;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#23558;&#27493;&#39588;&#30340;&#33258;&#21160;&#21270;&#25552;&#39640;20%&#12290;
&lt;/p&gt;
&lt;p&gt;
In task-oriented dialogue, a system often needs to follow a sequence of actions, called a workflow, that complies with a set of guidelines in order to complete a task. In this paper, we propose the novel problem of multi-step workflow action prediction, in which the system predicts multiple future workflow actions. Accurate prediction of multiple steps allows for multi-turn automation, which can free up time to focus on more complex tasks. We propose three modeling approaches that are simple to implement yet lead to more action automation: 1) fine-tuning on a training dataset, 2) few-shot in-context learning leveraging retrieval and large language model prompting, and 3) zero-shot graph traversal, which aggregates historical action sequences into a graph for prediction. We show that multi-step action prediction produces features that improve accuracy on downstream dialogue tasks like predicting task success, and can increase automation of steps by 20% without requiring as much feedback
&lt;/p&gt;</description></item><item><title>LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.19791</link><description>&lt;p&gt;
LILO&#65306;&#36890;&#36807;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#23398;&#20064;&#21487;&#35299;&#37322;&#24211;
&lt;/p&gt;
&lt;p&gt;
LILO: Learning Interpretable Libraries by Compressing and Documenting Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19791
&lt;/p&gt;
&lt;p&gt;
LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#26041;&#38754;&#26159;&#37325;&#26500;&#30340;&#33402;&#26415;&#65306;&#23558;&#20195;&#30721;&#25972;&#21512;&#21040;&#21487;&#37325;&#29992;&#21644;&#21487;&#35835;&#30340;&#31243;&#24207;&#24211;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LILO&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#36866;&#21512;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#24211;&#12290;LILO&#23558;LLM&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#19982;Stitch&#33258;&#21160;&#37325;&#26500;&#30340;&#36817;&#26399;&#31639;&#27861;&#36827;&#23637;&#30456;&#32467;&#21512;&#65306;Stitch&#26159;&#19968;&#20010;&#31526;&#21495;&#21387;&#32553;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#22823;&#22411;&#20195;&#30721;&#35821;&#26009;&#24211;&#20013;&#30340;&#26368;&#20339;lambda&#25277;&#35937;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#25277;&#35937;&#21487;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#25991;&#26723;&#65288;AutoDoc&#65289;&#36807;&#31243;&#65292;&#23427;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#30340;&#20351;&#29992;&#31034;&#20363;&#25512;&#26029;&#20986;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#21644;&#25991;&#26723;&#23383;&#31526;&#20018;&#12290;&#38500;&#20102;&#25552;&#39640;&#20154;&#31867;&#21487;&#35835;&#24615;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;AutoDoc&#36890;&#36807;&#24110;&#21161;LILO&#30340;&#21512;&#25104;&#22120;&#35299;&#37322;&#21644;&#37096;&#32626;&#23398;&#20064;&#21040;&#30340;&#25277;&#35937;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LILO&#36827;&#34892;&#20102;&#19977;&#20010;&#24402;&#32435;&#24335;&#31243;&#24207;&#32508;&#21512;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synth
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;PeTailor&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#20998;&#22359;&#35780;&#20998;&#22120;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#20998;&#22359;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#24182;&#23558;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2310.18463</link><description>&lt;p&gt;
PeTailor&#65306;&#36890;&#36807;&#23450;&#21046;&#30340;&#20998;&#22359;&#35780;&#20998;&#22120;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PeTailor: Improving Large Language Model by Tailored Chunk Scorer in Biomedical Triple Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18463
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PeTailor&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#20998;&#22359;&#35780;&#20998;&#22120;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#20998;&#22359;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#24182;&#23558;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#31995;&#32479;&#26088;&#22312;&#33258;&#21160;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#32479;&#19968;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#29702;&#35299;&#22797;&#26434;&#29983;&#29289;&#21307;&#23398;&#21477;&#23376;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#25968;&#25454;&#38598;&#38459;&#30861;&#20102;&#31283;&#20581;&#30340;&#19977;&#20803;&#32452;&#25552;&#21462;&#31995;&#32479;&#30340;&#24320;&#21457;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;PeTailor&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23450;&#21046;&#20998;&#22359;&#35780;&#20998;&#22120;&#20174;&#25105;&#20204;&#39044;&#20808;&#26500;&#24314;&#30340;&#22810;&#26679;&#20998;&#22359;&#25968;&#25454;&#24211;&#20013;&#26174;&#24335;&#22320;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#24182;&#23558;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36755;&#20837;&#20013;&#65292;&#20026;&#36755;&#20837;&#30340;&#21477;&#23376;&#29983;&#25104;&#30456;&#24212;&#30340;&#19977;&#20803;&#32452;&#65288;&#22836;&#23454;&#20307;&#65292;&#20851;&#31995;&#65292;&#23614;&#23454;&#20307;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;GM-CIHT&#65292;&#19968;&#31181;&#19987;&#23478;&#26631;&#27880;&#30340;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical triple extraction systems aim to automatically extract biomedical entities and relations between entities. While current unified information extraction models showcase state-of-the-art performance, they face challenges in understanding relationships between entities within intricate biomedical sentences. Furthermore, the absence of a high-quality biomedical triple extraction dataset impedes the progress in developing robust triple extraction systems. To tackle these challenges, we propose a novel retrieval-based framework for biomedical triple extraction, namely PeTailor, which explicitly retrieves the relevant document from our pre-built diverse chunk database using a novel tailored chunk scorer and integrates the retrieved information into the input of a Large Language Model (LLM) to generate the corresponding triple (head entity, relation, tail entity) for the input sentence. Additionally, we present GM-CIHT, an expert-annotated biomedical triple extraction dataset that c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;mBERT&#12289;XLM-R&#21644;GPT-3&#31561;&#19977;&#31181;&#27969;&#34892;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#36164;&#28304;&#21487;&#29992;&#24615;&#12289;&#35821;&#35328;&#23478;&#26063;&#12289;&#33050;&#26412;&#31867;&#22411;&#21644;&#35789;&#24207;&#31561;&#22240;&#32032;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#29305;&#23450;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#20854;&#20182;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2310.05404</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#35821;&#35328;&#24314;&#27169;&#30340;&#36855;&#23467;
&lt;/p&gt;
&lt;p&gt;
Exploring the Maze of Multilingual Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;mBERT&#12289;XLM-R&#21644;GPT-3&#31561;&#19977;&#31181;&#27969;&#34892;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#36164;&#28304;&#21487;&#29992;&#24615;&#12289;&#35821;&#35328;&#23478;&#26063;&#12289;&#33050;&#26412;&#31867;&#22411;&#21644;&#35789;&#24207;&#31561;&#22240;&#32032;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#29305;&#23450;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#20854;&#20182;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#21487;&#20197;&#24320;&#21457;&#36866;&#24212;&#19981;&#21516;&#35821;&#35328;&#29615;&#22659;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#23545;&#19977;&#31181;&#27969;&#34892;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;mBERT&#12289;XLM-R&#21644;GPT-3&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#36164;&#28304;&#21487;&#29992;&#24615;&#65288;&#36890;&#29992;&#21644;&#27169;&#22411;&#29305;&#23450;&#65289;&#12289;&#35821;&#35328;&#23478;&#26063;&#12289;&#33050;&#26412;&#31867;&#22411;&#21644;&#35789;&#24207;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#38024;&#23545;&#20004;&#31181;&#19981;&#21516;&#20219;&#21153;-&#25991;&#26412;&#20998;&#31867;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#29305;&#23450;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#22312;&#27169;&#22411;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21516;&#26102;&#25105;&#20204;&#20063;&#30830;&#23450;&#20102;&#20854;&#20182;&#22240;&#32032;&#65292;&#22914;&#36890;&#29992;&#36164;&#28304;&#21487;&#29992;&#24615;&#12289;&#35821;&#35328;&#23478;&#26063;&#21644;&#33050;&#26412;&#31867;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#33021;&#22815;&#21152;&#28145;&#23545;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual language models have gained significant attention in recent years, enabling the development of applications that meet diverse linguistic contexts. In this paper, we present a comprehensive evaluation of three popular multilingual language models: mBERT, XLM-R, and GPT-3. We assess their performance across a diverse set of languages, with a focus on understanding the impact of resource availability (general and model-specific), language family, script type, and word order on model performance, under two distinct tasks - text classification and text generation. Our findings reveal that while the amount of language-specific pretraining data plays a crucial role in model performance, we also identify other factors such as general resource availability, language family, and script type, as important features. We hope that our study contributes to a deeper understanding of multilingual language models to enhance their performance across languages and linguistic contexts.
&lt;/p&gt;</description></item><item><title>CCPrefix&#26159;&#19968;&#31181;&#38024;&#23545;&#22810;&#31867;&#20998;&#31867;&#30340;&#26032;&#22411;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#21453;&#20107;&#23454;&#23545;&#27604;&#26469;&#35299;&#20915;&#35821;&#35328;&#34920;&#31034;&#22120;&#27169;&#31946;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2211.05987</link><description>&lt;p&gt;
CCPrefix:&#21453;&#20107;&#23454;&#23545;&#27604;&#21069;&#32512;&#35843;&#25972;&#29992;&#20110;&#22810;&#31867;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CCPrefix: Counterfactual Contrastive Prefix-Tuning for Many-Class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.05987
&lt;/p&gt;
&lt;p&gt;
CCPrefix&#26159;&#19968;&#31181;&#38024;&#23545;&#22810;&#31867;&#20998;&#31867;&#30340;&#26032;&#22411;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#21453;&#20107;&#23454;&#23545;&#27604;&#26469;&#35299;&#20915;&#35821;&#35328;&#34920;&#31034;&#22120;&#27169;&#31946;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21069;&#32512;&#35843;&#25972;&#20197;&#26377;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#12290;&#23427;&#21033;&#29992;&#36719;&#21069;&#32512;&#20316;&#20026;&#20219;&#21153;&#29305;&#23450;&#30340;&#25351;&#31034;&#22120;&#21644;&#35821;&#35328;&#34920;&#31034;&#22120;&#20316;&#20026;&#20998;&#31867;&#26631;&#31614;&#30340;&#25552;&#21450;&#65292;&#20197;&#20943;&#23569;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21040;&#29305;&#23450;&#20219;&#21153;&#30340;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#24403;&#26631;&#31614;&#31354;&#38388;&#22823;&#24133;&#22686;&#21152;&#26102;&#65288;&#21363;&#22810;&#31867;&#20998;&#31867;&#65289;&#65292;&#36825;&#31181;&#35843;&#25972;&#25216;&#26415;&#20250;&#38754;&#20020;&#35821;&#35328;&#34920;&#31034;&#22120;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#30701;&#35821;&#35328;&#30701;&#21477;&#20013;&#30340;&#31867;&#21035;&#26631;&#31614;&#30001;&#35821;&#20041;&#30456;&#20284;&#30340;&#35821;&#35328;&#34920;&#31034;&#22120;&#34920;&#31034;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#21463;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#21363;&#27599;&#20010;&#23454;&#20363;&#37117;&#20250;&#32771;&#34385;&#26368;&#27169;&#31946;&#30340;&#31867;&#21035;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#26032;&#30340;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#65292;&#21363;&#21453;&#20107;&#23454;&#23545;&#27604;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#65288;CCPrefix&#65289;&#65292;&#29992;&#20110;&#22810;&#31867;&#20998;&#31867;&#12290;&#22522;&#26412;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#26631;&#31614;&#31354;&#38388;&#20013;&#30340;&#20107;&#23454;-&#21453;&#20107;&#23454;&#23545;&#26469;&#24471;&#21040;&#20381;&#36182;&#20110;&#23454;&#20363;&#30340;&#36719;&#21069;&#32512;&#65292;&#20197;&#34917;&#20805;&#35821;&#35328;&#34920;&#31034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, prefix-tuning was proposed to efficiently adapt pre-trained language models to a broad spectrum of natural language classification tasks. It leverages soft prefix as task-specific indicators and language verbalizers as categorical-label mentions to narrow the formulation gap from pre-training language models. However, when the label space increases considerably (i.e., many-class classification), such a tuning technique suffers from a verbalizer ambiguity problem since the many-class labels are represented by semantic-similar verbalizers in short language phrases. To overcome this, inspired by the human-decision process that the most ambiguous classes would be mulled over for each instance, we propose a brand-new prefix-tuning method, Counterfactual Contrastive Prefix-tuning (CCPrefix), for many-class classification. Basically, an instance-dependent soft prefix, derived from fact-counterfactual pairs in the label space, is leveraged to complement the language verbalizers in ma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#24615;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#30456;&#27604;&#20110;GPT-3.5&#65292;GPT-4&#33021;&#22815;&#23454;&#29616;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#32534;&#30721;&#19968;&#33268;&#24615;&#12290;&#26080;&#35770;&#27169;&#22411;&#35268;&#27169;&#22823;&#23567;&#65292;&#21482;&#35201;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#27169;&#22411;&#37117;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32534;&#30721;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15170</link><description>&lt;p&gt;
LLMs&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#23450;&#24615;&#32534;&#30721;&#65306;&#24605;&#32500;&#38142;&#25512;&#29702;&#22312;&#26576;&#20123;&#35299;&#37322;&#23398;&#20219;&#21153;&#20013;&#33021;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks. (arXiv:2401.15170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#24615;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#30456;&#27604;&#20110;GPT-3.5&#65292;GPT-4&#33021;&#22815;&#23454;&#29616;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#32534;&#30721;&#19968;&#33268;&#24615;&#12290;&#26080;&#35770;&#27169;&#22411;&#35268;&#27169;&#22823;&#23567;&#65292;&#21482;&#35201;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#27169;&#22411;&#37117;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32534;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#24615;&#32534;&#30721;&#25110;&#20869;&#23481;&#20998;&#26512;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21547;&#20041;&#65292;&#20197;&#35782;&#21035;&#36328;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#23450;&#37327;&#27169;&#24335;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#37322;&#33021;&#21147;&#26041;&#38754;&#30340;&#36827;&#23637;&#20026;&#33258;&#21160;&#21270;&#32534;&#30721;&#36807;&#31243;&#65288;&#23545;&#25991;&#26412;&#24212;&#29992;&#31867;&#21035;&#26631;&#31614;&#65289;&#25552;&#20379;&#20102;&#28508;&#21147;&#65292;&#20174;&#32780;&#20351;&#20154;&#31867;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#19987;&#27880;&#20110;&#26356;&#26377;&#21019;&#36896;&#21147;&#30340;&#30740;&#31350;&#26041;&#38754;&#65292;&#21516;&#26102;&#23558;&#36825;&#20123;&#35299;&#37322;&#20219;&#21153;&#22996;&#25176;&#32473;&#20154;&#24037;&#26234;&#33021;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#21253;&#25324;&#23545;&#20154;&#25991;&#23398;&#30740;&#31350;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#23494;&#38598;&#27573;&#33853;&#30340;&#19968;&#32452;&#31038;&#20250;&#21382;&#21490;&#32534;&#30721;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-4&#33021;&#22815;&#36798;&#21040;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#35299;&#37322;&#65292;&#32780;GPT-3.5&#21017;&#19981;&#33021;&#12290;&#19982;&#25105;&#20204;&#30001;&#20154;&#31867;&#33719;&#24471;&#30340;&#37329;&#26631;&#20934;&#30456;&#27604;&#65292;GPT-4&#22312;3&#20010;&#32534;&#30721;&#20013;&#20855;&#26377;&#20248;&#31168;&#30340;&#32534;&#30721;&#19968;&#33268;&#24615;&#65288;Cohen's &#954; &#8805; 0.79&#65289;&#65292;&#22312;9&#20010;&#32534;&#30721;&#20013;&#26377;8&#20010;&#20855;&#26377;&#26174;&#33879;&#30340;&#19968;&#33268;&#24615;&#65288;&#954; &#8805; 0.6&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;GPT-3.5&#22312;&#25152;&#26377;&#32534;&#30721;&#20013;&#34920;&#29616;&#19981;&#20339;&#65288;mean(&#954;) = 0.34&#65307;max(&#954;) = 0.55&#65289;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#32534;&#30721;&#30340;&#20934;&#30830;&#24615;&#19981;&#21463;&#27169;&#22411;&#35268;&#27169;&#24433;&#21709;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32534;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Qualitative coding, or content analysis, extracts meaning from text to discern quantitative patterns across a corpus of texts. Recently, advances in the interpretive abilities of large language models (LLMs) offer potential for automating the coding process (applying category labels to texts), thereby enabling human researchers to concentrate on more creative research aspects, while delegating these interpretive tasks to AI. Our case study comprises a set of socio-historical codes on dense, paragraph-long passages representative of a humanistic study. We show that GPT-4 is capable of human-equivalent interpretations, whereas GPT-3.5 is not. Compared to our human-derived gold standard, GPT-4 delivers excellent intercoder reliability (Cohen's $\kappa \geq 0.79$) for 3 of 9 codes, and substantial reliability ($\kappa \geq 0.6$) for 8 of 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes ($mean(\kappa) = 0.34$; $max(\kappa) = 0.55$). Importantly, we find that coding fidelity
&lt;/p&gt;</description></item><item><title>PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.15042</link><description>&lt;p&gt;
PROXYQA&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models. (arXiv:2401.15042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15042
&lt;/p&gt;
&lt;p&gt;
PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38271;&#31687;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#29983;&#25104;&#38271;&#31687;&#20869;&#23481;&#65288;&#22914;&#25253;&#21578;&#21644;&#25991;&#31456;&#65289;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#24403;&#21069;&#30340;&#22522;&#20934;&#19981;&#36275;&#20197;&#20805;&#20998;&#35780;&#20272;LLMs&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#19988;&#20840;&#38754;&#30340;&#20869;&#23481;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#31181;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc{ProxyQA}&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#65292;&#21253;&#25324;&#28145;&#20837;&#20154;&#24037;&#31574;&#21010;&#30340;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#30340;&#8220;&#20803;&#38382;&#39064;&#8221;&#12290;&#27599;&#20010;&#20803;&#38382;&#39064;&#37117;&#21253;&#21547;&#30456;&#24212;&#30340;&#24102;&#27880;&#37322;&#31572;&#26696;&#30340;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#12290;LLMs&#34987;&#35201;&#27714;&#26681;&#25454;&#36825;&#20123;&#20803;&#38382;&#39064;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#12290;&#21033;&#29992;&#35780;&#20272;&#22120;&#24182;&#23558;&#29983;&#25104;&#30340;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;\textsc{ProxyQA}&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#30340;&#34920;&#29616;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#22810;&#20010;LLMs&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks. However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored. Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach. In this study, we introduce \textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \textit{meta-questions} spanning various domains. Each meta-question contains corresponding \textit{proxy-questions} with annotated answers. LLMs are prompted to generate extensive content in response to these meta-questions. Utilizing an evaluator and incorporating generated content as background context, \textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \textit{proxy-questions}. We examine multiple LLMs, emphasizing \t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21644;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#32452;&#20214;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#39046;&#22495;&#65292;&#22312;&#26377;&#25928;&#30340;RAG&#30340;&#25552;&#31034;&#34920;&#36848;&#20013;&#65292;&#19981;&#30456;&#20851;&#25991;&#26723;&#30340;&#21253;&#21547;&#21487;&#33021;&#20250;&#23545;&#31995;&#32479;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.14887</link><description>&lt;p&gt;
&#22122;&#22768;&#30340;&#21147;&#37327;&#65306;&#37325;&#26032;&#23450;&#20041;RAG&#31995;&#32479;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
The Power of Noise: Redefining Retrieval for RAG Systems. (arXiv:2401.14887v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21644;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#32452;&#20214;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#39046;&#22495;&#65292;&#22312;&#26377;&#25928;&#30340;RAG&#30340;&#25552;&#31034;&#34920;&#36848;&#20013;&#65292;&#19981;&#30456;&#20851;&#25991;&#26723;&#30340;&#21253;&#21547;&#21487;&#33021;&#20250;&#23545;&#31995;&#32479;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#36827;&#27493;&#12290;RAG&#31995;&#32479;&#36890;&#36807;&#25972;&#21512;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#38454;&#27573;&#26816;&#32034;&#30340;&#22806;&#37096;&#25968;&#25454;&#26469;&#22686;&#24378;&#20854;&#29983;&#25104;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;LLMs&#30340;&#38480;&#21046;&#65292;&#21518;&#32773;&#20165;&#38480;&#20110;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#21644;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;RAG&#31995;&#32479;&#20869;LLMs&#30340;&#29983;&#25104;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#20840;&#38754;&#32780;&#25209;&#21028;&#24615;&#22320;&#20998;&#26512;IR&#32452;&#20214;&#23545;RAG&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#20010;&#26816;&#32034;&#22120;&#22312;&#26377;&#25928;&#30340;RAG&#30340;&#25552;&#31034;&#34920;&#36848;&#20013;&#24212;&#35813;&#20855;&#22791;&#30340;&#29305;&#24449;&#65292;&#37325;&#28857;&#20851;&#27880;&#24212;&#35813;&#26816;&#32034;&#21738;&#31181;&#31867;&#22411;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#22240;&#32032;&#65292;&#22914;&#25991;&#26723;&#19982;&#25552;&#31034;&#30340;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#30340;&#20301;&#32622;&#20197;&#21450;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20986;&#65292;&#21253;&#21547;&#19981;&#30456;&#20851;&#30340;&#25991;&#26723;&#21487;&#33021;&#20250;&#8230;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation (RAG) systems represent a significant advancement over traditional Large Language Models (LLMs). RAG systems enhance their generation ability by incorporating external data retrieved through an Information Retrieval (IR) phase, overcoming the limitations of standard LLMs, which are restricted to their pre-trained knowledge and limited context window. Most research in this area has predominantly concentrated on the generative aspect of LLMs within RAG systems. Our study fills this gap by thoroughly and critically analyzing the influence of IR components on RAG systems. This paper analyzes which characteristics a retriever should possess for an effective RAG's prompt formulation, focusing on the type of documents that should be retrieved. We evaluate various elements, such as the relevance of the documents to the prompt, their position, and the number included in the context. Our findings reveal, among other insights, that including irrelevant documents can
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#65292;&#24341;&#20837;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#35780;&#20272;&#35282;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19987;&#38376;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#20316;&#20026;&#22522;&#20934;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#24314;&#31435;&#20102;&#22522;&#30784;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.09002</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models. (arXiv:2401.09002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#65292;&#24341;&#20837;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#35780;&#20272;&#35282;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19987;&#38376;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#20316;&#20026;&#22522;&#20934;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#24314;&#31435;&#20102;&#22522;&#30784;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21019;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#30340;&#26032;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#20581;&#22766;&#24615;&#35780;&#20272;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#35780;&#20272;&#26694;&#26550;&#65306;&#31895;&#31890;&#24230;&#35780;&#20272;&#21644;&#32454;&#31890;&#24230;&#35780;&#20272;&#12290;&#27599;&#20010;&#26694;&#26550;&#37117;&#20351;&#29992;&#20174;0&#21040;1&#30340;&#35780;&#20998;&#33539;&#22260;&#65292;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#21644;&#32454;&#33268;&#22320;&#35780;&#20272;&#25915;&#20987;&#25928;&#26524;&#65292;&#24182;&#24110;&#21161;&#25915;&#20987;&#32773;&#26356;&#22909;&#22320;&#20248;&#21270;&#25915;&#20987;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#36234;&#29425;&#20219;&#21153;&#30340;&#20840;&#38754;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#19981;&#20165;&#26159;&#25105;&#20204;&#24403;&#21069;&#30740;&#31350;&#30340;&#20851;&#38190;&#22522;&#20934;&#65292;&#20063;&#20026;&#26410;&#26469;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#30784;&#36164;&#28304;&#65292;&#21487;&#20197;&#22312;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#36827;&#34892;&#19968;&#33268;&#21644;&#27604;&#36739;&#30340;&#20998;&#26512;&#12290;&#36890;&#36807;&#19982;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#30340;&#31934;&#24515;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#19982;&#20043;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our research, we pioneer a novel approach to evaluate the effectiveness of jailbreak attacks on Large Language Models (LLMs), such as GPT-4 and LLaMa2, diverging from traditional robustness-focused binary evaluations. Our study introduces two distinct evaluation frameworks: a coarse-grained evaluation and a fine-grained evaluation. Each framework, using a scoring range from 0 to 1, offers a unique perspective, enabling a more comprehensive and nuanced evaluation of attack effectiveness and empowering attackers to refine their attack prompts with greater understanding. Furthermore, we have developed a comprehensive ground truth dataset specifically tailored for jailbreak tasks. This dataset not only serves as a crucial benchmark for our current study but also establishes a foundational resource for future research, enabling consistent and comparative analyses in this evolving field. Upon meticulous comparison with traditional evaluation methods, we discovered that our evaluation alig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.01854</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#37319;&#32435;&#65292;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36890;&#36807;&#22312;&#21478;&#19968;&#31181;&#35821;&#35328;&#19978;&#24494;&#35843;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#31181;&#35821;&#35328;&#19978;&#33719;&#24471;&#29305;&#23450;&#30340;&#21151;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLM&#22312;&#25351;&#20196;&#35843;&#20248;&#36807;&#31243;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33521;&#35821;&#35843;&#20248;&#38598;&#21512;&#20013;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#65292;&#22312;&#35843;&#20248;&#36807;&#31243;&#20013;&#19981;&#35770;&#26159;&#24050;&#35265;&#35821;&#35328;&#36824;&#26159;&#26410;&#35265;&#35821;&#35328;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.01335</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#21487;&#20197;&#23558;&#20854;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. (arXiv:2401.01335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#32454;&#35843;&#65288;SFT&#65289;&#21033;&#29992;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#21147;&#37327;&#23545;&#20110;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#38656;&#35201;&#33719;&#21462;&#39069;&#22806;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#25104;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26032;&#30340;&#32454;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#32454;&#35843;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;SPIN&#30340;&#26680;&#24515;&#26159;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#20854;&#20013;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#23454;&#20363;&#23545;&#24328;&#26469;&#25552;&#21319;&#33258;&#24049;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#33258;&#24049;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20248;&#21270;&#33258;&#36523;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#33258;&#25105;&#29983;&#25104;&#30340;&#22238;&#24212;&#19982;&#26469;&#33258;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#22238;&#24212;&#26469;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20026;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20805;&#20998;&#21457;&#25496;&#20154;&#31867;&#26631;&#27880;&#31034;&#33539;&#25968;&#25454;&#22312;SFT&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#35757;&#32451;&#30446;&#26631;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#26159;&#21487;&#20197;&#36798;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achiev
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25552;&#39640;&#23398;&#26415;&#20889;&#20316;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#21407;&#21017;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#26694;&#26550;&#12289;&#26377;&#25928;&#30340;&#25552;&#31034;&#25216;&#26415;&#21644;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#35748;&#30693;&#21368;&#36733;&#21644;&#24819;&#35937;&#21050;&#28608;&#30340;AI&#36741;&#21161;&#20889;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.17143</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#25512;&#21160;&#23398;&#26415;&#20889;&#20316;&#65306;&#26694;&#26550;&#12289;&#25216;&#26415;&#21644;&#27880;&#24847;&#20107;&#39033;
&lt;/p&gt;
&lt;p&gt;
Supercharging academic writing with generative AI: framework, techniques, and caveats. (arXiv:2310.17143v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25552;&#39640;&#23398;&#26415;&#20889;&#20316;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#21407;&#21017;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#26694;&#26550;&#12289;&#26377;&#25928;&#30340;&#25552;&#31034;&#25216;&#26415;&#21644;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#35748;&#30693;&#21368;&#36733;&#21644;&#24819;&#35937;&#21050;&#28608;&#30340;AI&#36741;&#21161;&#20889;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#20889;&#20316;&#26159;&#30740;&#31350;&#39033;&#30446;&#20013;&#19981;&#21487;&#25110;&#32570;&#20294;&#36153;&#26102;&#36153;&#21147;&#30340;&#37096;&#20998;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#39640;&#23398;&#26415;&#20889;&#20316;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#21407;&#21017;&#21644;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#26694;&#26550;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;AI&#22312;&#20889;&#20316;&#20013;&#30340;&#29702;&#35770;&#22522;&#30784;&#65288;&#20026;&#20160;&#20040;&#65289;&#12289;&#36807;&#31243;&#65288;&#22914;&#20309;&#65289;&#21644;&#24615;&#36136;&#65288;&#20160;&#20040;&#65289;&#12290;&#35813;&#26694;&#26550;&#25351;&#20986;&#20102;&#30701;&#26399;&#21644;&#38271;&#26399;&#21442;&#19982;AI&#20889;&#20316;&#30340;&#21407;&#22240;&#21450;&#20854;&#22522;&#26412;&#26426;&#21046;&#65288;&#22914;&#35748;&#30693;&#21368;&#36733;&#21644;&#24819;&#35937;&#21050;&#28608;&#65289;&#12290;&#23427;&#25581;&#31034;&#20102;AI&#22312;&#25972;&#20010;&#20889;&#20316;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#20889;&#20316;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#21644;&#20889;&#20316;&#36741;&#21161;&#31867;&#22411;&#21644;&#32423;&#21035;&#30340;&#27169;&#22411;&#34920;&#31034;&#20102;AI&#22312;&#20889;&#20316;&#20013;&#30340;&#24110;&#21161;&#26041;&#24335;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;&#20889;&#20316;&#24120;&#35268;&#20013;&#25972;&#21512;AI&#30340;&#26377;&#25928;&#25552;&#31034;&#25216;&#26415;&#65288;&#22823;&#32434;&#12289;&#36215;&#33609;&#21644;&#32534;&#36753;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Academic writing is an indispensable yet laborious part of the research enterprise. This Perspective maps out principles and methods for using generative artificial intelligence (AI), specifically large language models (LLMs), to elevate the quality and efficiency of academic writing. We introduce a human-AI collaborative framework that delineates the rationale (why), process (how), and nature (what) of AI engagement in writing. The framework pinpoints both short-term and long-term reasons for engagement and their underlying mechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals the role of AI throughout the writing process, conceptualized through a two-stage model for human-AI collaborative writing, and the nature of AI assistance in writing, represented through a model of writing-assistance types and levels. Building on this framework, we describe effective prompting techniques for incorporating AI into the writing routine (outlining, drafting, and editing) a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17022</link><description>&lt;p&gt;
&#21463;&#25511;&#35299;&#30721;&#26469;&#33258;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#65292;&#29992;&#20110;&#25511;&#21046;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#33719;&#24471;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#20540;&#20989;&#25968;&#26469;&#35299;&#20915;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#20540;&#20989;&#25968;&#34987;&#31216;&#20026;&#21069;&#32512;&#35780;&#20998;&#22120;&#12290;&#21069;&#32512;&#35780;&#20998;&#22120;&#22312;&#25512;&#29702;&#26102;&#29992;&#20110;&#24341;&#23548;&#29983;&#25104;&#21521;&#26356;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21069;&#32512;&#35780;&#20998;&#22120;&#21487;&#20197;&#20174;&#65288;&#21487;&#33021;&#26159;&#65289;&#31163;&#31574;&#30053;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#20174;&#37096;&#20998;&#35299;&#30721;&#30340;&#21709;&#24212;&#32487;&#32493;&#35299;&#30721;&#26102;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;Reddit&#23545;&#35805;&#35821;&#26009;&#24211;&#19978;&#32463;&#39564;&#35777;&#26126;&#65292;CD&#20316;&#20026;&#19968;&#31181;&#25511;&#21046;&#26426;&#21046;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CD&#35774;&#35745;&#30340;&#27169;&#22359;&#21270;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#20219;&#20309;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CD&#21487;&#20197;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#22359;&#26041;&#24335;&#22312;&#25512;&#29702;&#26102;&#24212;&#29992;&#65292;&#21516;&#26679;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09518</link><description>&lt;p&gt;
&#20154;&#31867;&#35838;&#31243;&#25351;&#23548;&#19979;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#20027;&#27969;&#33539;&#24335;&#26159;&#38543;&#26426;&#27927;&#29260;&#35757;&#32451;&#26368;&#22823;&#22810;&#26679;&#21270;&#25351;&#20196;-&#21709;&#24212;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#19982;&#20197;&#24448;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#25351;&#20196;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#25945;&#32946;&#30340;&#28176;&#36827;&#24615;&#21644;&#26377;&#32452;&#32455;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#19982;&#25945;&#32946;&#26694;&#26550;&#23545;&#40784;&#26469;&#31574;&#21010;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#27599;&#20010;&#26679;&#26412;&#21253;&#25324;&#20027;&#39064;&#21644;&#35748;&#30693;&#20005;&#35880;&#31243;&#24230;&#31561;&#20803;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#20013;&#23398;&#21040;&#30740;&#31350;&#29983;&#38454;&#27573;&#30340;&#20840;&#38754;&#32454;&#31890;&#24230;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#37117;&#26377;&#21508;&#31181;&#38382;&#39064;&#65292;&#20197;&#21033;&#29992;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#32423;&#27861;&#25552;&#39640;&#27010;&#24565;&#28145;&#24230;&#65292;&#35813;&#20998;&#32423;&#27861;&#29992;&#20110;&#21306;&#20998;&#27599;&#20010;&#27010;&#24565;&#30340;&#19981;&#21516;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27491;&#30830;&#19981;&#21464;&#37327;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#20943;&#23569;&#31243;&#24207;&#39564;&#35777;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.09342</link><description>&lt;p&gt;
&#20026;&#31243;&#24207;&#39564;&#35777;&#23545;LLM&#29983;&#25104;&#30340;&#24490;&#29615;&#19981;&#21464;&#24335;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking LLM-Generated Loop Invariants for Program Verification. (arXiv:2310.09342v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27491;&#30830;&#19981;&#21464;&#37327;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#20943;&#23569;&#31243;&#24207;&#39564;&#35777;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#24402;&#32435;&#24490;&#29615;&#19981;&#21464;&#37327;&#26159;&#33258;&#21160;&#21270;&#31243;&#24207;&#39564;&#35777;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;gpt-3.5&#25110;gpt-4&#65289;&#33021;&#22815;&#22312;0-shot&#29615;&#22659;&#19979;&#20026;&#19968;&#31867;&#31243;&#24207;&#21512;&#25104;&#24490;&#29615;&#19981;&#21464;&#37327;&#65292;&#20294;&#38656;&#35201;&#22810;&#20010;&#26679;&#26412;&#25165;&#33021;&#29983;&#25104;&#27491;&#30830;&#30340;&#19981;&#21464;&#37327;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22823;&#37327;&#35843;&#29992;&#31243;&#24207;&#39564;&#35777;&#22120;&#26469;&#24314;&#31435;&#19981;&#21464;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25490;&#21517;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#38382;&#39064;&#23450;&#20041;&#21306;&#20998;&#27491;&#30830;&#30340;&#24402;&#32435;&#19981;&#21464;&#37327;&#21644;&#38169;&#35823;&#30340;&#23581;&#35797;&#12290;&#35813;&#25490;&#21517;&#22120;&#32463;&#36807;&#23545;&#27604;&#25490;&#21517;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#37325;&#26032;&#25490;&#21517;&#26426;&#21046;&#26174;&#33879;&#25552;&#39640;&#20102;&#27491;&#30830;&#19981;&#21464;&#37327;&#22312;&#29983;&#25104;&#30340;&#20505;&#36873;&#39033;&#20013;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#22823;&#24133;&#20943;&#23569;&#20102;&#23545;&#39564;&#35777;&#22120;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing inductive loop invariants is fundamental to automating program verification. In this work, we observe that Large Language Models (such as gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of programs in a 0-shot setting, yet require several samples to generate the correct invariants. This can lead to a large number of calls to a program verifier to establish an invariant. To address this issue, we propose a {\it re-ranking} approach for the generated results of LLMs. We have designed a ranker that can distinguish between correct inductive invariants and incorrect attempts based on the problem definition. The ranker is optimized as a contrastive ranker. Experimental results demonstrate that this re-ranking mechanism significantly improves the ranking of correct invariants among the generated candidates, leading to a notable reduction in the number of calls to a verifier.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#22686;&#21152;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#23454;&#20363;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#38382;&#31572;&#27169;&#22411;&#22312;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#21644;&#27169;&#22411;&#26657;&#20934;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24615;&#33021;&#25913;&#36827;&#19982;&#21453;&#20107;&#23454;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.07822</link><description>&lt;p&gt;
CATfOOD&#65306;&#21453;&#20107;&#23454;&#22686;&#24378;&#35757;&#32451;&#20197;&#25552;&#39640;&#39046;&#22495;&#22806;&#24615;&#33021;&#21644;&#26657;&#20934;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration. (arXiv:2309.07822v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#22686;&#21152;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#23454;&#20363;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#38382;&#31572;&#27169;&#22411;&#22312;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#21644;&#27169;&#22411;&#26657;&#20934;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24615;&#33021;&#25913;&#36827;&#19982;&#21453;&#20107;&#23454;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35268;&#27169;&#26041;&#38754;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32473;&#23450;&#25552;&#31034;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#25991;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;LLM&#26469;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#65288;CF&#65289;&#23454;&#20363;&#65288;&#21363;&#26368;&#23567;&#31243;&#24230;&#30340;&#25913;&#21464;&#36755;&#20837;&#65289;&#65292;&#20197;&#25552;&#39640;SLM&#22312;&#25688;&#35201;&#38382;&#31572;&#65288;QA&#65289;&#35774;&#32622;&#19979;&#30340;&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;LLM&#29983;&#25104;&#22120;&#20013;&#65292;&#36825;&#31181;&#25968;&#25454;&#22686;&#24378;&#22987;&#32456;&#33021;&#22815;&#25552;&#39640;OOD&#24615;&#33021;&#65292;&#24182;&#25913;&#36827;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#21644;&#22522;&#20110;&#29702;&#24615;&#22686;&#24378;&#30340;&#26657;&#20934;&#27169;&#22411;&#30340;&#27169;&#22411;&#26657;&#20934;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#24615;&#33021;&#25552;&#21319;&#19982;CF&#23454;&#20363;&#22312;&#22806;&#35266;&#24418;&#24335;&#21644;&#35821;&#20041;&#20869;&#23481;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#21576;&#27491;&#30456;&#20851;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26657;&#20934;&#26356;&#23481;&#26131;&#30340;CF&#22686;&#24378;&#27169;&#22411;&#22312;&#20998;&#37197;&#37325;&#35201;&#24615;&#26102;&#30340;&#29109;&#20063;&#36739;&#20302;&#65292;&#36825;&#34920;&#26126;&#29702;&#24615;&#22686;&#24378;&#30340;&#26657;&#20934;&#22120;&#26356;&#20559;&#22909;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have shown remarkable capabilities at scale, particularly at generating text conditioned on a prompt. In our work, we investigate the use of LLMs to augment training data of small language models~(SLMs) with automatically generated counterfactual~(CF) instances -- i.e. minimally altered inputs -- in order to improve out-of-domain~(OOD) performance of SLMs in the extractive question answering~(QA) setup. We show that, across various LLM generators, such data augmentation consistently enhances OOD performance and improves model calibration for both confidence-based and rationale-augmented calibrator models. Furthermore, these performance improvements correlate with higher diversity of CF instances in terms of their surface form and semantic content. Finally, we show that CF augmented models which are easier to calibrate also exhibit much lower entropy when assigning importance, indicating that rationale-augmented calibrators prefer concise ex
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35780;&#20272;&#22120;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;&#22810;&#35821;&#35328;&#35780;&#20272;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#33021;&#22815;&#23545;&#21508;&#31181;&#35821;&#35328;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#26377;&#25928;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.07462</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#22120;&#26159;&#21542;&#26159;&#25193;&#23637;&#22810;&#35821;&#35328;&#35780;&#20272;&#30340;&#35299;&#20915;&#26041;&#26696;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?. (arXiv:2309.07462v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07462
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35780;&#20272;&#22120;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;&#22810;&#35821;&#35328;&#35780;&#20272;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#33021;&#22815;&#23545;&#21508;&#31181;&#35821;&#35328;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#26377;&#25928;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#22914;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#20998;&#31867;&#12290;&#23558;LLMs&#29992;&#20316;&#35780;&#20272;&#22120;&#65292;&#21487;&#20197;&#23545;&#20854;&#20182;&#27169;&#22411;&#65288;&#36890;&#24120;&#20026;LLMs&#65289;&#30340;&#36755;&#20986;&#36827;&#34892;&#25490;&#24207;&#25110;&#35780;&#20998;&#65292;&#22240;&#20026;&#24403;&#21069;&#35780;&#20272;&#25216;&#26415;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#65292;&#21253;&#25324;&#32570;&#20047;&#36866;&#24403;&#30340;&#22522;&#20934;&#12289;&#25351;&#26631;&#12289;&#25104;&#26412;&#21644;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#35775;&#38382;&#24615;&#12290;&#34429;&#28982;LLMs&#33021;&#22815;&#22788;&#29702;&#22823;&#32422;100&#31181;&#35821;&#35328;&#65292;&#20294;&#22823;&#22810;&#25968;&#35821;&#35328;&#22312;&#21508;&#31181;&#20219;&#21153;&#12289;&#25351;&#26631;&#21644;&#22522;&#20934;&#19978;&#32570;&#20047;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;&#36825;&#23601;&#36843;&#20999;&#38656;&#35201;&#25193;&#23637;&#22810;&#35821;&#35328;&#35780;&#20272;&#65292;&#20197;&#30830;&#20445;&#23545;LLM&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#26377;&#20934;&#30830;&#30340;&#29702;&#35299;&#12290;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#20284;&#20046;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#23436;&#32654;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#32773;&#12289;&#20154;&#24037;&#21019;&#24314;&#30340;&#21442;&#32771;&#21644;&#22522;&#20934;&#65292;&#24182;&#19988;&#29702;&#35770;&#19978;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#20219;&#20309;&#34987;&#35206;&#30422;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive performance on Natural Language Processing (NLP) tasks, such as Question Answering, Summarization, and Classification. The use of LLMs as evaluators, that can rank or score the output of other models (usually LLMs) has become increasingly popular, due to the limitations of current evaluation techniques including the lack of appropriate benchmarks, metrics, cost, and access to human annotators. While LLMs are capable of handling approximately 100 languages, the majority of languages beyond the top 20 lack systematic evaluation across various tasks, metrics, and benchmarks. This creates an urgent need to scale up multilingual evaluation to ensure a precise understanding of LLM performance across diverse languages. LLM-based evaluators seem like the perfect solution to this problem, as they do not require human annotators, human-created references, or benchmarks and can theoretically be used to evaluate any language covered by the 
&lt;/p&gt;</description></item><item><title>LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.01413</link><description>&lt;p&gt;
LaFiCMIL&#65306;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22823;&#25991;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning. (arXiv:2308.01413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01413
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#31361;&#30772;&#12290;&#30452;&#35266;&#19978;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#26399;&#26395;&#25991;&#26412;&#20998;&#31867;&#65292;&#20316;&#20026;&#19981;&#38656;&#35201;&#20687;&#29983;&#25104;&#20219;&#21153;&#37027;&#26679;&#35768;&#22810;&#39640;&#32423;&#34920;&#31034;&#30340;&#20219;&#21153;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;Transformer&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#36827;&#34892;&#32508;&#21512;&#24615;&#30340;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#65292;&#22312;&#22810;&#31867;&#21035;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#38271;&#25991;&#26412;&#25991;&#26723;&#21644;&#20854;&#20182;&#22823;&#25991;&#20214;&#30340;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#36739;&#22823;&#30340;&#25913;&#36827;&#28508;&#21147;&#12290;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#20027;&#35201;&#21463;&#21040;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#30340;&#38459;&#30861;&#65306;&#26377;&#38480;&#30340;&#36755;&#20837;&#38271;&#24230;&#65292;&#27604;&#22914;BERT&#30340;512&#20010;&#26631;&#35760;&#12290;&#34429;&#28982;&#22686;&#21152;GPU&#20869;&#23384;&#21487;&#20197;&#31245;&#24494;&#25193;&#23637;&#36825;&#20010;&#38480;&#21046;&#65292;&#20294;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;GPU&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#36755;&#20837;&#38480;&#21046;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;LaFiCMIL&#65292;&#20316;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have revolutionized the performance of a wide range of language tasks. Intuitively, one might expect text classification, which does not necessitate as many high-level representations as generative tasks, to be comprehensively addressed with the powerful representation capabilities of Transformers. However, in reality, there remains significant potential for enhancement, particularly in the areas of multi-class and multi-label classification of lengthy textual documents and other large files. The performance of Transformer-based models is mainly hindered by a major limitation: a restricted input length, e.g., 512 tokens for BERT. While an increase in GPU memory can marginally extend this limit, practical real-world applications often operate under constrained GPU resources. In this work, we tackle the input limit problem from the perspective of correlated multiple instance learning. The proposed approach, LaFiCMIL, serves as a versatile framework applicable to 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#27700;&#21360;&#20197;&#36827;&#34892;AI&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#24863;&#30693;&#27700;&#21360;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20445;&#25345;&#26816;&#27979;&#33021;&#21147;&#30340;&#21516;&#26102;&#65292;&#22312;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.13808</link><description>&lt;p&gt;
&#27700;&#21360;&#25216;&#26415;&#29992;&#20110;AI&#26816;&#27979;&#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65306;&#25581;&#31034;&#25361;&#25112;&#21450;&#35821;&#20041;&#24863;&#30693;&#27700;&#21360;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy. (arXiv:2307.13808v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#27700;&#21360;&#20197;&#36827;&#34892;AI&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#24863;&#30693;&#27700;&#21360;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20445;&#25345;&#26816;&#27979;&#33021;&#21147;&#30340;&#21516;&#26102;&#65292;&#22312;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#26368;&#36817;&#30340;AI&#26816;&#27979;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#38543;&#26426;&#38480;&#21046;&#35789;&#27719;&#24182;&#21033;&#29992;&#27492;&#20449;&#24687;&#36827;&#34892;&#26816;&#27979;&#30340;&#26041;&#24335;&#23558;&#27700;&#21360;&#23884;&#20837;&#21040;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#12290;&#34429;&#28982;&#36825;&#20123;&#27700;&#21360;&#21482;&#20250;&#23548;&#33268;&#22256;&#24785;&#24230;&#36731;&#24494;&#19979;&#38477;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#35777;&#35843;&#26597;&#25581;&#31034;&#20102;&#23545;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#24615;&#33021;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35821;&#20041;&#24863;&#30693;&#27700;&#21360;&#31639;&#27861;&#65292;&#32771;&#34385;&#20102;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#30340;&#29305;&#24615;&#21644;&#36755;&#20837;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65288;&#21253;&#25324;BART&#21644;Flan-T5&#65289;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#22312;&#25688;&#35201;&#29983;&#25104;&#21644;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#20173;&#20445;&#25345;&#20102;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To mitigate potential risks associated with language models, recent AI detection research proposes incorporating watermarks into machine-generated text through random vocabulary restrictions and utilizing this information for detection. While these watermarks only induce a slight deterioration in perplexity, our empirical investigation reveals a significant detriment to the performance of conditional text generation. To address this issue, we introduce a simple yet effective semantic-aware watermarking algorithm that considers the characteristics of conditional text generation and the input context. Experimental results demonstrate that our proposed method yields substantial improvements across various text generation models, including BART and Flan-T5, in tasks such as summarization and data-to-text generation while maintaining detection ability.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31579;&#36873;&#31574;&#30053;AlpaGasus&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#36807;&#28388;&#25481;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;&#23427;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#21407;&#22987;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.08701</link><description>&lt;p&gt;
AlpaGasus: &#29992;&#26356;&#23569;&#25968;&#25454;&#35757;&#32451;&#26356;&#22909;&#30340;&#32650;&#39548;
&lt;/p&gt;
&lt;p&gt;
AlpaGasus: Training A Better Alpaca with Fewer Data. (arXiv:2307.08701v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08701
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31579;&#36873;&#31574;&#30053;AlpaGasus&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#36807;&#28388;&#25481;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;&#23427;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#21407;&#22987;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#26377;&#30417;&#30563;&#30340;&#25351;&#20196;/&#22238;&#22797;&#25968;&#25454;&#19978;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65288;IFT&#65289;&#26469;&#22686;&#24378;&#20854;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;IFT&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#65306;Alpaca&#30340;52k&#25968;&#25454;&#65289;&#20986;&#20046;&#24847;&#26009;&#22320;&#21253;&#21547;&#35768;&#22810;&#20855;&#26377;&#19981;&#27491;&#30830;&#25110;&#19981;&#30456;&#20851;&#22238;&#22797;&#30340;&#20302;&#36136;&#37327;&#23454;&#20363;&#65292;&#36825;&#20123;&#23454;&#20363;&#20250;&#35823;&#23548;&#21644;&#23545;IFT&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;&#65306;ChatGPT&#65289;&#33258;&#21160;&#35782;&#21035;&#24182;&#36807;&#28388;&#25481;&#20302;&#36136;&#37327;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AlpaGasus&#65292;&#23427;&#20165;&#22312;&#20174;52k Alpaca&#25968;&#25454;&#20013;&#36807;&#28388;&#24471;&#21040;&#30340;9k&#39640;&#36136;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;AlpaGasus&#22312;&#22810;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#21407;&#22987;&#30340;Alpaca&#65292;&#30001;GPT-4&#36827;&#34892;&#35780;&#20272;&#12290;&#20854;13B&#21464;&#31181;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19982;&#20854;&#25945;&#24072;&#27169;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#29983;&#25104;52k&#25968;&#25454;&#30340;Text-Davinci-003&#65289;&#30340;&#24615;&#33021;&#21305;&#37197;&#29575;&#36229;&#36807;90&#65285;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;5.7&#20493;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#23558;7B&#21464;&#31181;&#30340;&#35757;&#32451;&#26102;&#38388;&#20174;80&#20998;&#38047;&#20943;&#23569;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large language models~(LLMs) strengthen instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT. In this paper, we propose a simple and effective data selection strategy that automatically identifies and filters out low-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data. AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and the controlled human evaluation. Its 13B variant matches $&gt;90\%$ performance of its teacher LLM (i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#30340;&#27010;&#24565;&#65292;&#24182;&#20197;&#39046;&#22303;&#20105;&#31471;&#20026;&#20363;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#12289;&#22810;&#36873;&#39064;&#30340;&#25968;&#25454;&#38598;BorderLines&#21644;&#20960;&#20010;&#23450;&#37327;&#25351;&#26631;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#20013;&#30340;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.14610</link><description>&lt;p&gt;
&#36825;&#29255;&#22303;&#22320;&#26159;&#20320;&#25105;&#30340;&#22303;&#22320;&#65306;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models. (arXiv:2305.14610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#30340;&#27010;&#24565;&#65292;&#24182;&#20197;&#39046;&#22303;&#20105;&#31471;&#20026;&#20363;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#12289;&#22810;&#36873;&#39064;&#30340;&#25968;&#25454;&#38598;BorderLines&#21644;&#20960;&#20010;&#23450;&#37327;&#25351;&#26631;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#20013;&#30340;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#30340;&#27010;&#24565;&#8212;&#8212;&#21363;&#26681;&#25454;&#35821;&#35328;&#29615;&#22659;&#25253;&#36947;&#19981;&#21516;&#30340;&#22320;&#32536;&#25919;&#27835;&#30693;&#35782;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#20197;&#39046;&#22303;&#20105;&#31471;&#20026;&#26696;&#20363;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#34987;&#24191;&#27867;&#20105;&#35758;&#30340;&#21335;&#27801;&#32676;&#23707;&#65292;&#22914;&#26524;&#29992;&#20013;&#25991;&#38382;&#65292;LM&#26159;&#21542;&#26356;&#26377;&#21487;&#33021;&#35828;&#23427;&#20204;&#23646;&#20110;&#20013;&#22269;&#65292;&#32780;&#22914;&#26524;&#29992;&#22612;&#21152;&#27931;&#35821;&#38382;&#65292;&#21017;&#26356;&#26377;&#21487;&#33021;&#35828;&#23427;&#20204;&#23646;&#20110;&#33778;&#24459;&#23486;&#65311;&#20026;&#20102;&#35780;&#20272;&#26159;&#21542;&#23384;&#22312;&#36825;&#31181;&#20559;&#35265;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#32500;&#22522;&#30334;&#31185;&#19978;&#25910;&#38598;&#20102;&#19968;&#32452;&#39046;&#22303;&#20105;&#31471;&#25968;&#25454;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#39046;&#22303;&#19982;&#19968;&#32452;&#22810;&#35821;&#35328;&#12289;&#22810;&#36873;&#39064;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#34987;&#31216;&#20026;BorderLines&#65292;&#23427;&#21253;&#25324;250&#20010;&#39046;&#22303;&#21644;45&#31181;&#35821;&#35328;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#38382;&#39064;&#38598;&#25552;&#20132;&#32473;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#25552;&#20986;&#30340;&#23450;&#37327;&#25351;&#26631;&#20998;&#26512;&#23427;&#20204;&#30340;&#21709;&#24212;&#20013;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#12290;&#36825;&#20123;&#25351;&#26631;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#30340;&#22238;&#31572;&#20197;&#21450;&#23454;&#38469;&#30340;&#22320;&#32536;&#25919;&#27835;&#24773;&#20917;&#12290;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#29616;&#35937;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;&#36328;&#35821;&#35328;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the notion of geopolitical bias -- a tendency to report different geopolitical knowledge depending on the linguistic context. As a case study, we consider territorial disputes between countries. For example, for the widely contested Spratly Islands, would an LM be more likely to say they belong to China if asked in Chinese, vs. to the Philippines if asked in Tagalog? To evaluate if such biases exist, we first collect a dataset of territorial disputes from Wikipedia, then associate each territory with a set of multilingual, multiple-choice questions. This dataset, termed BorderLines, consists of 250 territories with questions in 45 languages. We pose these question sets to language models, and analyze geopolitical bias in their responses through several proposed quantitative metrics. The metrics compare between responses in different question languages as well as to the actual geopolitical situation. The phenomenon of geopolitical bias is a uniquely cross-lingual evaluation
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23569;&#37327;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#30340;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#25913;&#21892;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#29983;&#25104;&#20989;&#25968;&#21512;&#25104;&#25968;&#25454;&#65292;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#21442;&#25968;&#37327;&#21482;&#26377;&#20043;&#21069;&#27169;&#22411;&#30340;&#19977;&#20998;&#20043;&#19968;&#65292;&#36798;&#21040;&#20102;&#19982;&#20043;&#21069;&#27169;&#22411;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13691</link><description>&lt;p&gt;
&#20351;&#29992;&#23569;&#37327;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#30340;&#24320;&#25918;&#39046;&#22495;&#8220;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#8221;
&lt;/p&gt;
&lt;p&gt;
Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis. (arXiv:2305.13691v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13691
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23569;&#37327;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#30340;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#25913;&#21892;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#29983;&#25104;&#20989;&#25968;&#21512;&#25104;&#25968;&#25454;&#65292;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#21442;&#25968;&#37327;&#21482;&#26377;&#20043;&#21069;&#27169;&#22411;&#30340;&#19977;&#20998;&#20043;&#19968;&#65292;&#36798;&#21040;&#20102;&#19982;&#20043;&#21069;&#27169;&#22411;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24320;&#25918;&#39046;&#22495;&#8220;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#8221;&#65292;&#23569;&#37327;&#25968;&#25454;&#30340;&#23398;&#20064;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#34429;&#28982;&#24378;&#22823;&#65292;&#20294;&#26159;LLMs&#22312;&#25512;&#26029;&#26102;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#8220;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#8221;&#65292;&#21487;&#20351;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#20110;10&#20010;&#20154;&#31867;&#27880;&#37322;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#26041;&#38754;&#24471;&#21040;&#25913;&#21892;&#12290;&#35813;&#26694;&#26550;&#24314;&#31435;&#22312;&#30001;LLMs&#21644;&#25552;&#31034;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#29983;&#25104;&#20989;&#25968;&#20043;&#19978;&#65292;&#36825;&#20165;&#38656;&#35201;&#23569;&#37327;&#30340;&#25163;&#24037;&#29305;&#24449;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#25968;&#30334;&#19975;&#20010;&#22810;&#36339;&#38382;&#39064;&#21644;&#22768;&#31216;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21518;&#65292;&#25105;&#20204;&#22312;&#27969;&#34892;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#21644;&#20107;&#23454;&#39564;&#35777;&#22522;&#20934;&#19978;&#35780;&#20272;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#20351;&#25105;&#20204;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;&#21442;&#25968;&#37327;&#19978;&#20165;&#20026;&#20043;&#21069;&#27169;&#22411;&#30340;&#19977;&#20998;&#20043;&#19968;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning for open domain multi-hop question answering typically relies on large language models (LLMs). While powerful, LLMs are inefficient at the inference time. We propose a data synthesis framework for multi-hop question answering that allows for improving smaller language models with less than 10 human-annotated question answer pairs. The framework is built upon the data generation functions parameterized by LLMs and prompts, which requires minimal hand-crafted features. Empirically, we synthesize millions of multi-hop questions and claims. After finetuning language models on the synthetic data, we evaluate the models on popular benchmarks on multi-hop question answering and fact verification. Our experimental results show that finetuning on the synthetic data improves model performance significantly, allowing our finetuned models to be competitive with prior models while being almost one-third the size in terms of parameter counts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"gisting"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23558;&#25552;&#31034;&#21387;&#32553;&#20026;&#26356;&#23567;&#30340;"&#35201;&#28857;"&#26631;&#35760;&#38598;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;26&#20493;&#30340;&#25552;&#31034;&#21387;&#32553;&#65292;&#20943;&#23569;40&#65285;&#30340;FLOPs&#12289;4.2&#65285;&#30340;&#22681;&#26102;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#33410;&#30465;&#23384;&#20648;&#31354;&#38388;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#36755;&#20986;&#36136;&#37327;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2304.08467</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#35201;&#28857;&#26631;&#35760;&#21387;&#32553;&#25552;&#31034;&#35821;
&lt;/p&gt;
&lt;p&gt;
Learning to Compress Prompts with Gist Tokens. (arXiv:2304.08467v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08467
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"gisting"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23558;&#25552;&#31034;&#21387;&#32553;&#20026;&#26356;&#23567;&#30340;"&#35201;&#28857;"&#26631;&#35760;&#38598;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;26&#20493;&#30340;&#25552;&#31034;&#21387;&#32553;&#65292;&#20943;&#23569;40&#65285;&#30340;FLOPs&#12289;4.2&#65285;&#30340;&#22681;&#26102;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#33410;&#30465;&#23384;&#20648;&#31354;&#38388;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#36755;&#20986;&#36136;&#37327;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26159;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#33021;&#21147;&#30340;&#20027;&#35201;&#26041;&#24335;&#65292;&#20294;&#26159;&#25552;&#31034;&#21344;&#25454;&#20102;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#23453;&#36149;&#30340;&#31354;&#38388;&#65292;&#37325;&#22797;&#32534;&#30721;&#30456;&#21516;&#30340;&#25552;&#31034;&#22312;&#35745;&#31639;&#19978;&#26159;&#20302;&#25928;&#30340;&#12290;&#24494;&#35843;&#21644;&#33976;&#39311;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#19987;&#38376;&#21270;&#65292;&#20294;&#38656;&#35201;&#20026;&#27599;&#20010;&#20219;&#21153;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#20102;&#23436;&#20840;&#36991;&#20813;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;gist&#65292;&#23427;&#35757;&#32451;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#23558;&#25552;&#31034;&#21387;&#32553;&#25104;&#26356;&#23567;&#30340;&#8220;&#35201;&#28857;&#8221;&#26631;&#35760;&#38598;&#65292;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#25928;&#29575;&#30340;&#32531;&#23384;&#21644;&#37325;&#29992;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#20462;&#25913;Transformer&#30340;&#27880;&#24847;&#21147;&#25513;&#30721;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#39069;&#22806;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#23545;gist&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25552;&#31034;&#30340;&#39640;&#36798;26&#20493;&#30340;&#21387;&#32553;&#65292;&#20174;&#32780;&#20943;&#23569;&#39640;&#36798;40&#65285;&#30340;FLOPs&#12289;4.2&#65285;&#30340;&#22681;&#26102;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#33410;&#30465;&#23384;&#20648;&#31354;&#38388;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#36755;&#20986;&#36136;&#37327;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting is the primary way to utilize the multitask capabilities of language models (LMs), but prompts occupy valuable space in the input context window, and repeatedly encoding the same prompt is computationally inefficient. Finetuning and distillation methods allow for specialization of LMs without prompting, but require retraining the model for each task. To avoid this trade-off entirely, we present gisting, which trains an LM to compress prompts into smaller sets of "gist" tokens which can be cached and reused for compute efficiency. Gist models can be trained with no additional cost over standard instruction finetuning by simply modifying Transformer attention masks to encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings, all with minimal loss in output quality.
&lt;/p&gt;</description></item><item><title>&#28145;&#23618;&#28508;&#22312;&#20301;&#32622;&#20027;&#39064;&#27169;&#22411;&#29992;&#20110;&#32593;&#32476;&#32858;&#31867;&#21644;&#34920;&#31034;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#31574;&#30053;&#21644;&#27010;&#29575;&#27169;&#22411;&#23545;&#33410;&#28857;&#21644;&#36793;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#36827;&#34892;&#21442;&#25968;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2304.08242</link><description>&lt;p&gt;
&#28145;&#23618;&#28508;&#22312;&#20301;&#32622;&#20027;&#39064;&#27169;&#22411;&#29992;&#20110;&#24102;&#26377;&#25991;&#26412;&#36793;&#30340;&#32593;&#32476;&#32858;&#31867;&#21644;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
The Deep Latent Position Topic Model for Clustering and Representation of Networks with Textual Edges. (arXiv:2304.08242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08242
&lt;/p&gt;
&lt;p&gt;
&#28145;&#23618;&#28508;&#22312;&#20301;&#32622;&#20027;&#39064;&#27169;&#22411;&#29992;&#20110;&#32593;&#32476;&#32858;&#31867;&#21644;&#34920;&#31034;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#31574;&#30053;&#21644;&#27010;&#29575;&#27169;&#22411;&#23545;&#33410;&#28857;&#21644;&#36793;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#36827;&#34892;&#21442;&#25968;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#20132;&#20114;&#23548;&#33268;&#29992;&#25143;&#20849;&#20139;&#20854;&#20182;&#20154;&#21457;&#24067;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#36825;&#20123;&#20869;&#23481;&#33258;&#28982;&#22320;&#30001;&#23558;&#20010;&#20307;&#19982;&#33410;&#28857;&#20851;&#32852;&#21644;&#20132;&#25442;&#30340;&#25991;&#26412;&#23450;&#20041;&#20026;&#36793;&#30340;&#32593;&#32476;&#26469;&#34920;&#31034;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#20123;&#24322;&#26500;&#21644;&#22797;&#26434;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#23558;&#33410;&#28857;&#32858;&#31867;&#20026;&#21516;&#31867;&#32676;&#32452;&#20197;&#21450;&#21576;&#29616;&#21487;&#29702;&#35299;&#30340;&#25968;&#25454;&#21487;&#35270;&#21270;&#26159;&#24517;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Deep-LPTM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#31574;&#30053;&#65292;&#20381;&#36182;&#20110;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#20197;&#21450;&#27010;&#29575;&#27169;&#22411;&#26469;&#25551;&#36848;&#35752;&#35770;&#30340;&#20027;&#39064;&#12290;Deep-LPTM&#20801;&#35768;&#22312;&#20004;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#26500;&#24314;&#33410;&#28857;&#21644;&#36793;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#21442;&#25968;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;IC2L&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36873;&#25321;&#20855;&#26377;&#30456;&#20851;&#32858;&#31867;&#21644;&#21487;&#35270;&#21270;&#23646;&#24615;&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#12290;&#29305;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Numerical interactions leading to users sharing textual content published by others are naturally represented by a network where the individuals are associated with the nodes and the exchanged texts with the edges. To understand those heterogeneous and complex data structures, clustering nodes into homogeneous groups as well as rendering a comprehensible visualisation of the data is mandatory. To address both issues, we introduce Deep-LPTM, a model-based clustering strategy relying on a variational graph auto-encoder approach as well as a probabilistic model to characterise the topics of discussion. Deep-LPTM allows to build a joint representation of the nodes and of the edges in two embeddings spaces. The parameters are inferred using a variational inference algorithm. We also introduce IC2L, a model selection criterion specifically designed to choose models with relevant clustering and visualisation properties. An extensive benchmark study on synthetic data is provided. In particular
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11916</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#65306;&#35299;&#37322;&#21644;&#23547;&#25214;&#22909;&#30340;&#31034;&#33539;&#20197;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#22312;&#25512;&#29702;&#26102;&#23454;&#29616;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#30340;&#26174;&#33879;&#25928;&#29575;&#65292;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290; &#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#24378;&#35843;&#36825;&#31181;&#33021;&#21147;&#23545;&#23569;&#37327;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#24456;&#25935;&#24863;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#36125;&#21494;&#26031;&#35270;&#35282;&#30740;&#31350;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#20174;&#31034;&#33539;&#20013;&#38544;&#21547;&#22320;&#25512;&#26029;&#20986;&#30456;&#20851;&#20449;&#24687;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;&#22312;&#27492;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#19968;&#32452;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#38543;&#26426;&#36873;&#25321;&#22522;&#32447;&#30340;&#24179;&#22343;&#20540;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#27599;&#20010; GPT2 &#21644; GPT3 &#27169;&#22411;&#26377;&#26174;&#30528;&#30340; 12.5% &#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that la
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#20851;&#20110;&#26234;&#33021;&#12289;&#20154;&#31867;&#35821;&#35328;&#21644;&#20154;&#31867;&#25968;&#23398;&#30340;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#20154;&#31867;&#35821;&#35328;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#33021;&#21542;&#23545;&#25105;&#20204;&#26080;&#27861;&#24819;&#35937;&#30340;&#20107;&#29289;&#26377;&#20219;&#20309;&#20102;&#35299;&#12290;</title><link>http://arxiv.org/abs/2208.03886</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#20102;&#35299;&#29978;&#33267;&#26080;&#27861;&#24819;&#35937;&#30340;&#20107;&#29289;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
What can we know about that which we cannot even imagine?. (arXiv:2208.03886v3 [physics.hist-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03886
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#20851;&#20110;&#26234;&#33021;&#12289;&#20154;&#31867;&#35821;&#35328;&#21644;&#20154;&#31867;&#25968;&#23398;&#30340;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#20154;&#31867;&#35821;&#35328;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#33021;&#21542;&#23545;&#25105;&#20204;&#26080;&#27861;&#24819;&#35937;&#30340;&#20107;&#29289;&#26377;&#20219;&#20309;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#23558;&#32771;&#34385;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#38382;&#39064;&#28041;&#21450;&#21040;&#26234;&#33021;&#30340;&#29983;&#29289;&#23398;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#35748;&#30693;&#20041;&#32930;&#12290;&#36825;&#23558;&#24341;&#20986;&#20851;&#20110;&#20154;&#31867;&#35821;&#35328;&#30340;&#38382;&#39064;&#65292;&#20063;&#35768;&#26159;&#20154;&#31867;&#36804;&#20170;&#20026;&#27490;&#24320;&#21457;&#30340;&#26368;&#37325;&#35201;&#30340;&#35748;&#30693;&#20041;&#32930;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#23545;&#20154;&#31867;&#35821;&#35328;&#25152;&#21253;&#21547;&#30340;&#35748;&#30693;&#33021;&#21147;&#36827;&#34892;&#36190;&#32654;&#65292;&#20294;&#25105;&#23558;&#24378;&#35843;&#20154;&#31867;&#35821;&#35328;&#22810;&#20040;&#26377;&#38480;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#35748;&#30693;&#33021;&#21147;&#20063;&#26159;&#26377;&#38480;&#30340;&#65292;&#23613;&#31649;&#35821;&#35328;&#23545;&#20854;&#36827;&#34892;&#20102;&#22686;&#24378;&#12290;&#36825;&#23558;&#24341;&#20986;&#20851;&#20110;&#20154;&#31867;&#25968;&#23398;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26368;&#32456;&#26159;&#20197;&#20154;&#31867;&#35821;&#35328;&#30340;&#24418;&#24335;&#26469;&#34920;&#36848;&#30340;&#65292;&#25152;&#20197;&#20063;&#23384;&#22312;&#28145;&#23618;&#27425;&#30340;&#38480;&#21046;&#12290;&#28982;&#21518;&#65292;&#25105;&#23558;&#32467;&#21512;&#36825;&#20123;&#38382;&#39064;&#65292;&#23545;&#36825;&#31687;&#25991;&#31456;&#30340;&#26680;&#24515;&#38382;&#39064;&#25552;&#20986;&#19968;&#20010;&#37096;&#20998;&#24615;&#30340;&#12289;&#26377;&#28857;&#20391;&#38754;&#30340;&#31572;&#26696;&#65306;&#25105;&#20204;&#33021;&#22815;&#23545;&#25105;&#20204;&#29978;&#33267;&#26080;&#27861;&#26500;&#24819;&#30340;&#20107;&#29289;&#26377;&#20309;&#20102;&#35299;&#65311;
&lt;/p&gt;
&lt;p&gt;
In this essay I will consider a sequence of questions. The first questions concern the biological function of intelligence in general, and cognitive prostheses of human intelligence in particular. These will lead into questions concerning human language, perhaps the most important cognitive prosthesis humanity has ever developed. While it is traditional to rhapsodize about the cognitive power encapsulated in human language, I will emphasize how horribly limited human language is -- and therefore how limited our cognitive abilities are, despite their being augmented with language. This will lead to questions of whether human mathematics, being ultimately formulated in terms of human language, is also deeply limited. I will then combine these questions to pose a partial, sort-of, sideways answer to the guiding concern of this essay: what we can ever discern about that we cannot even conceive?
&lt;/p&gt;</description></item></channel></rss>